diff a/src/hotspot/share/opto/library_call.cpp b/src/hotspot/share/opto/library_call.cpp
--- a/src/hotspot/share/opto/library_call.cpp
+++ b/src/hotspot/share/opto/library_call.cpp
@@ -256,10 +256,12 @@
   bool inline_unsafe_newArray(bool uninitialized);
   bool inline_unsafe_writeback0();
   bool inline_unsafe_writebackSync0(bool is_pre);
   bool inline_unsafe_copyMemory();
   bool inline_native_currentThread();
+  bool inline_native_scopedCache();
+  bool inline_native_setScopedCache();
 
   bool inline_native_time_funcs(address method, const char* funcName);
 #ifdef JFR_HAVE_INTRINSICS
   bool inline_native_classID();
   // bool inline_native_getEventWriter();
@@ -756,10 +758,13 @@
 
   case vmIntrinsics::_onSpinWait:               return inline_onspinwait();
 
   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
 
+  case vmIntrinsics::_scopedCache:              return inline_native_scopedCache();
+  case vmIntrinsics::_setScopedCache:           return inline_native_setScopedCache();
+
 #ifdef JFR_HAVE_INTRINSICS
   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, JFR_TIME_FUNCTION), "counterTime");
   case vmIntrinsics::_getClassId:               return inline_native_classID();
   // case vmIntrinsics::_getEventWriter:           return inline_native_getEventWriter();
 #endif
@@ -3043,10 +3048,44 @@
   Node* junk = NULL;
   set_result(generate_current_thread(junk));
   return true;
 }
 
+//------------------------inline_native_scopedCache------------------
+bool LibraryCallKit::inline_native_scopedCache() {
+  ciKlass *objects_klass = ciObjArrayKlass::make(env()->Object_klass());
+  const TypeOopPtr *etype = TypeOopPtr::make_from_klass(env()->Object_klass());
+
+  // It might be nice to eliminate the bounds check on the cache array
+  // by replacing TypeInt::POS here with
+  // TypeInt::make(ScopedCacheSize*2), but this causes a performance
+  // regression in some test cases.
+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);
+  bool xk = etype->klass_is_exact();
+
+  // Because we create the scoped cache lazily we have to make the
+  // type of the result BotPTR.
+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);
+  Node* thread = _gvn.transform(new ThreadLocalNode());
+  Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::scopedCache_offset()));
+  Node* threadObj = make_load(NULL, p, objects_type, T_OBJECT, MemNode::unordered);
+  set_result(threadObj);
+
+  return true;
+}
+
+//------------------------inline_native_setScopedCache------------------
+bool LibraryCallKit::inline_native_setScopedCache() {
+  Node* arr = argument(0);
+  Node* thread = _gvn.transform(new ThreadLocalNode());
+  Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::scopedCache_offset()));
+  const TypePtr *adr_type = _gvn.type(p)->isa_ptr();
+  store_to_memory(control(), p, arr, T_OBJECT, adr_type, MemNode::unordered);
+
+  return true;
+}
+
 //---------------------------load_mirror_from_klass----------------------------
 // Given a klass oop, load its java mirror (a java.lang.Class oop).
 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
   Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
