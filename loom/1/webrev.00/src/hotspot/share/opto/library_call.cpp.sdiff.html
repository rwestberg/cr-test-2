<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/library_call.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c2compiler.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../prims/jvm.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/library_call.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 241   bool inline_math_negateExactL();
 242   bool inline_math_subtractExactI(bool is_decrement);
 243   bool inline_math_subtractExactL(bool is_decrement);
 244   bool inline_min_max(vmIntrinsics::ID id);
 245   bool inline_notify(vmIntrinsics::ID id);
 246   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 247   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 248   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset, BasicType type);
 249   Node* make_unsafe_address(Node*&amp; base, Node* offset, DecoratorSet decorators, BasicType type = T_ILLEGAL, bool can_cast = false);
 250 
 251   typedef enum { Relaxed, Opaque, Volatile, Acquire, Release } AccessKind;
 252   DecoratorSet mo_decorator_for_access_kind(AccessKind kind);
 253   bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned);
 254   static bool klass_needs_init_guard(Node* kls);
 255   bool inline_unsafe_allocate();
 256   bool inline_unsafe_newArray(bool uninitialized);
 257   bool inline_unsafe_writeback0();
 258   bool inline_unsafe_writebackSync0(bool is_pre);
 259   bool inline_unsafe_copyMemory();
 260   bool inline_native_currentThread();


 261 
 262   bool inline_native_time_funcs(address method, const char* funcName);
 263 #ifdef JFR_HAVE_INTRINSICS
 264   bool inline_native_classID();
 265   // bool inline_native_getEventWriter();
 266 #endif
 267   bool inline_native_Class_query(vmIntrinsics::ID id);
 268   bool inline_native_subtype_check();
 269   bool inline_native_getLength();
 270   bool inline_array_copyOf(bool is_copyOfRange);
 271   bool inline_array_equals(StrIntrinsicNode::ArgEnc ae);
 272   bool inline_preconditions_checkIndex();
 273   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array);
 274   bool inline_native_clone(bool is_virtual);
 275   bool inline_native_Reflection_getCallerClass();
 276   // Helper function for inlining native object hash method
 277   bool inline_native_hashcode(bool is_virtual, bool is_static);
 278   bool inline_native_getClass();
 279 
 280   // Helper functions for inlining arraycopy
</pre>
<hr />
<pre>
 741 
 742   case vmIntrinsics::_getAndAddByte:                    return inline_unsafe_load_store(T_BYTE,   LS_get_add,       Volatile);
 743   case vmIntrinsics::_getAndAddShort:                   return inline_unsafe_load_store(T_SHORT,  LS_get_add,       Volatile);
 744   case vmIntrinsics::_getAndAddInt:                     return inline_unsafe_load_store(T_INT,    LS_get_add,       Volatile);
 745   case vmIntrinsics::_getAndAddLong:                    return inline_unsafe_load_store(T_LONG,   LS_get_add,       Volatile);
 746 
 747   case vmIntrinsics::_getAndSetByte:                    return inline_unsafe_load_store(T_BYTE,   LS_get_set,       Volatile);
 748   case vmIntrinsics::_getAndSetShort:                   return inline_unsafe_load_store(T_SHORT,  LS_get_set,       Volatile);
 749   case vmIntrinsics::_getAndSetInt:                     return inline_unsafe_load_store(T_INT,    LS_get_set,       Volatile);
 750   case vmIntrinsics::_getAndSetLong:                    return inline_unsafe_load_store(T_LONG,   LS_get_set,       Volatile);
 751   case vmIntrinsics::_getAndSetReference:               return inline_unsafe_load_store(T_OBJECT, LS_get_set,       Volatile);
 752 
 753   case vmIntrinsics::_loadFence:
 754   case vmIntrinsics::_storeFence:
 755   case vmIntrinsics::_fullFence:                return inline_unsafe_fence(intrinsic_id());
 756 
 757   case vmIntrinsics::_onSpinWait:               return inline_onspinwait();
 758 
 759   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
 760 



 761 #ifdef JFR_HAVE_INTRINSICS
 762   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, JFR_TIME_FUNCTION), &quot;counterTime&quot;);
 763   case vmIntrinsics::_getClassId:               return inline_native_classID();
 764   // case vmIntrinsics::_getEventWriter:           return inline_native_getEventWriter();
 765 #endif
 766   case vmIntrinsics::_currentTimeMillis:        return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeMillis), &quot;currentTimeMillis&quot;);
 767   case vmIntrinsics::_nanoTime:                 return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeNanos), &quot;nanoTime&quot;);
 768   case vmIntrinsics::_writeback0:               return inline_unsafe_writeback0();
 769   case vmIntrinsics::_writebackPreSync0:        return inline_unsafe_writebackSync0(true);
 770   case vmIntrinsics::_writebackPostSync0:       return inline_unsafe_writebackSync0(false);
 771   case vmIntrinsics::_allocateInstance:         return inline_unsafe_allocate();
 772   case vmIntrinsics::_copyMemory:               return inline_unsafe_copyMemory();
 773   case vmIntrinsics::_getLength:                return inline_native_getLength();
 774   case vmIntrinsics::_copyOf:                   return inline_array_copyOf(false);
 775   case vmIntrinsics::_copyOfRange:              return inline_array_copyOf(true);
 776   case vmIntrinsics::_equalsB:                  return inline_array_equals(StrIntrinsicNode::LL);
 777   case vmIntrinsics::_equalsC:                  return inline_array_equals(StrIntrinsicNode::UU);
 778   case vmIntrinsics::_Preconditions_checkIndex: return inline_preconditions_checkIndex();
 779   case vmIntrinsics::_clone:                    return inline_native_clone(intrinsic()-&gt;is_virtual());
 780 
</pre>
<hr />
<pre>
3028                           IN_NATIVE | C2_CONTROL_DEPENDENT_LOAD);
3029   result_rgn-&gt;init_req(_normal_path, control());
3030   result_val-&gt;init_req(_normal_path, res);
3031 
3032   set_result(result_rgn, result_val);
3033 
3034   return true;
3035 }
3036 
3037 */
3038 
3039 #endif // JFR_HAVE_INTRINSICS
3040 
3041 //------------------------inline_native_currentThread------------------
3042 bool LibraryCallKit::inline_native_currentThread() {
3043   Node* junk = NULL;
3044   set_result(generate_current_thread(junk));
3045   return true;
3046 }
3047 


































3048 //---------------------------load_mirror_from_klass----------------------------
3049 // Given a klass oop, load its java mirror (a java.lang.Class oop).
3050 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
3051   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
3052   Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3053   // mirror = ((OopHandle)mirror)-&gt;resolve();
3054   return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);
3055 }
3056 
3057 //-----------------------load_klass_from_mirror_common-------------------------
3058 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3059 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3060 // and branch to the given path on the region.
3061 // If never_see_null, take an uncommon trap on null, so we can optimistically
3062 // compile for the non-null case.
3063 // If the region is NULL, force never_see_null = true.
3064 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3065                                                     bool never_see_null,
3066                                                     RegionNode* region,
3067                                                     int null_path,
</pre>
</td>
<td>
<hr />
<pre>
 241   bool inline_math_negateExactL();
 242   bool inline_math_subtractExactI(bool is_decrement);
 243   bool inline_math_subtractExactL(bool is_decrement);
 244   bool inline_min_max(vmIntrinsics::ID id);
 245   bool inline_notify(vmIntrinsics::ID id);
 246   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 247   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 248   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset, BasicType type);
 249   Node* make_unsafe_address(Node*&amp; base, Node* offset, DecoratorSet decorators, BasicType type = T_ILLEGAL, bool can_cast = false);
 250 
 251   typedef enum { Relaxed, Opaque, Volatile, Acquire, Release } AccessKind;
 252   DecoratorSet mo_decorator_for_access_kind(AccessKind kind);
 253   bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned);
 254   static bool klass_needs_init_guard(Node* kls);
 255   bool inline_unsafe_allocate();
 256   bool inline_unsafe_newArray(bool uninitialized);
 257   bool inline_unsafe_writeback0();
 258   bool inline_unsafe_writebackSync0(bool is_pre);
 259   bool inline_unsafe_copyMemory();
 260   bool inline_native_currentThread();
<span class="line-added"> 261   bool inline_native_scopedCache();</span>
<span class="line-added"> 262   bool inline_native_setScopedCache();</span>
 263 
 264   bool inline_native_time_funcs(address method, const char* funcName);
 265 #ifdef JFR_HAVE_INTRINSICS
 266   bool inline_native_classID();
 267   // bool inline_native_getEventWriter();
 268 #endif
 269   bool inline_native_Class_query(vmIntrinsics::ID id);
 270   bool inline_native_subtype_check();
 271   bool inline_native_getLength();
 272   bool inline_array_copyOf(bool is_copyOfRange);
 273   bool inline_array_equals(StrIntrinsicNode::ArgEnc ae);
 274   bool inline_preconditions_checkIndex();
 275   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array);
 276   bool inline_native_clone(bool is_virtual);
 277   bool inline_native_Reflection_getCallerClass();
 278   // Helper function for inlining native object hash method
 279   bool inline_native_hashcode(bool is_virtual, bool is_static);
 280   bool inline_native_getClass();
 281 
 282   // Helper functions for inlining arraycopy
</pre>
<hr />
<pre>
 743 
 744   case vmIntrinsics::_getAndAddByte:                    return inline_unsafe_load_store(T_BYTE,   LS_get_add,       Volatile);
 745   case vmIntrinsics::_getAndAddShort:                   return inline_unsafe_load_store(T_SHORT,  LS_get_add,       Volatile);
 746   case vmIntrinsics::_getAndAddInt:                     return inline_unsafe_load_store(T_INT,    LS_get_add,       Volatile);
 747   case vmIntrinsics::_getAndAddLong:                    return inline_unsafe_load_store(T_LONG,   LS_get_add,       Volatile);
 748 
 749   case vmIntrinsics::_getAndSetByte:                    return inline_unsafe_load_store(T_BYTE,   LS_get_set,       Volatile);
 750   case vmIntrinsics::_getAndSetShort:                   return inline_unsafe_load_store(T_SHORT,  LS_get_set,       Volatile);
 751   case vmIntrinsics::_getAndSetInt:                     return inline_unsafe_load_store(T_INT,    LS_get_set,       Volatile);
 752   case vmIntrinsics::_getAndSetLong:                    return inline_unsafe_load_store(T_LONG,   LS_get_set,       Volatile);
 753   case vmIntrinsics::_getAndSetReference:               return inline_unsafe_load_store(T_OBJECT, LS_get_set,       Volatile);
 754 
 755   case vmIntrinsics::_loadFence:
 756   case vmIntrinsics::_storeFence:
 757   case vmIntrinsics::_fullFence:                return inline_unsafe_fence(intrinsic_id());
 758 
 759   case vmIntrinsics::_onSpinWait:               return inline_onspinwait();
 760 
 761   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
 762 
<span class="line-added"> 763   case vmIntrinsics::_scopedCache:              return inline_native_scopedCache();</span>
<span class="line-added"> 764   case vmIntrinsics::_setScopedCache:           return inline_native_setScopedCache();</span>
<span class="line-added"> 765 </span>
 766 #ifdef JFR_HAVE_INTRINSICS
 767   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, JFR_TIME_FUNCTION), &quot;counterTime&quot;);
 768   case vmIntrinsics::_getClassId:               return inline_native_classID();
 769   // case vmIntrinsics::_getEventWriter:           return inline_native_getEventWriter();
 770 #endif
 771   case vmIntrinsics::_currentTimeMillis:        return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeMillis), &quot;currentTimeMillis&quot;);
 772   case vmIntrinsics::_nanoTime:                 return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeNanos), &quot;nanoTime&quot;);
 773   case vmIntrinsics::_writeback0:               return inline_unsafe_writeback0();
 774   case vmIntrinsics::_writebackPreSync0:        return inline_unsafe_writebackSync0(true);
 775   case vmIntrinsics::_writebackPostSync0:       return inline_unsafe_writebackSync0(false);
 776   case vmIntrinsics::_allocateInstance:         return inline_unsafe_allocate();
 777   case vmIntrinsics::_copyMemory:               return inline_unsafe_copyMemory();
 778   case vmIntrinsics::_getLength:                return inline_native_getLength();
 779   case vmIntrinsics::_copyOf:                   return inline_array_copyOf(false);
 780   case vmIntrinsics::_copyOfRange:              return inline_array_copyOf(true);
 781   case vmIntrinsics::_equalsB:                  return inline_array_equals(StrIntrinsicNode::LL);
 782   case vmIntrinsics::_equalsC:                  return inline_array_equals(StrIntrinsicNode::UU);
 783   case vmIntrinsics::_Preconditions_checkIndex: return inline_preconditions_checkIndex();
 784   case vmIntrinsics::_clone:                    return inline_native_clone(intrinsic()-&gt;is_virtual());
 785 
</pre>
<hr />
<pre>
3033                           IN_NATIVE | C2_CONTROL_DEPENDENT_LOAD);
3034   result_rgn-&gt;init_req(_normal_path, control());
3035   result_val-&gt;init_req(_normal_path, res);
3036 
3037   set_result(result_rgn, result_val);
3038 
3039   return true;
3040 }
3041 
3042 */
3043 
3044 #endif // JFR_HAVE_INTRINSICS
3045 
3046 //------------------------inline_native_currentThread------------------
3047 bool LibraryCallKit::inline_native_currentThread() {
3048   Node* junk = NULL;
3049   set_result(generate_current_thread(junk));
3050   return true;
3051 }
3052 
<span class="line-added">3053 //------------------------inline_native_scopedCache------------------</span>
<span class="line-added">3054 bool LibraryCallKit::inline_native_scopedCache() {</span>
<span class="line-added">3055   ciKlass *objects_klass = ciObjArrayKlass::make(env()-&gt;Object_klass());</span>
<span class="line-added">3056   const TypeOopPtr *etype = TypeOopPtr::make_from_klass(env()-&gt;Object_klass());</span>
<span class="line-added">3057 </span>
<span class="line-added">3058   // It might be nice to eliminate the bounds check on the cache array</span>
<span class="line-added">3059   // by replacing TypeInt::POS here with</span>
<span class="line-added">3060   // TypeInt::make(ScopedCacheSize*2), but this causes a performance</span>
<span class="line-added">3061   // regression in some test cases.</span>
<span class="line-added">3062   const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);</span>
<span class="line-added">3063   bool xk = etype-&gt;klass_is_exact();</span>
<span class="line-added">3064 </span>
<span class="line-added">3065   // Because we create the scoped cache lazily we have to make the</span>
<span class="line-added">3066   // type of the result BotPTR.</span>
<span class="line-added">3067   const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);</span>
<span class="line-added">3068   Node* thread = _gvn.transform(new ThreadLocalNode());</span>
<span class="line-added">3069   Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::scopedCache_offset()));</span>
<span class="line-added">3070   Node* threadObj = make_load(NULL, p, objects_type, T_OBJECT, MemNode::unordered);</span>
<span class="line-added">3071   set_result(threadObj);</span>
<span class="line-added">3072 </span>
<span class="line-added">3073   return true;</span>
<span class="line-added">3074 }</span>
<span class="line-added">3075 </span>
<span class="line-added">3076 //------------------------inline_native_setScopedCache------------------</span>
<span class="line-added">3077 bool LibraryCallKit::inline_native_setScopedCache() {</span>
<span class="line-added">3078   Node* arr = argument(0);</span>
<span class="line-added">3079   Node* thread = _gvn.transform(new ThreadLocalNode());</span>
<span class="line-added">3080   Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::scopedCache_offset()));</span>
<span class="line-added">3081   const TypePtr *adr_type = _gvn.type(p)-&gt;isa_ptr();</span>
<span class="line-added">3082   store_to_memory(control(), p, arr, T_OBJECT, adr_type, MemNode::unordered);</span>
<span class="line-added">3083 </span>
<span class="line-added">3084   return true;</span>
<span class="line-added">3085 }</span>
<span class="line-added">3086 </span>
3087 //---------------------------load_mirror_from_klass----------------------------
3088 // Given a klass oop, load its java mirror (a java.lang.Class oop).
3089 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
3090   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
3091   Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3092   // mirror = ((OopHandle)mirror)-&gt;resolve();
3093   return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);
3094 }
3095 
3096 //-----------------------load_klass_from_mirror_common-------------------------
3097 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3098 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3099 // and branch to the given path on the region.
3100 // If never_see_null, take an uncommon trap on null, so we can optimistically
3101 // compile for the non-null case.
3102 // If the region is NULL, force never_see_null = true.
3103 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3104                                                     bool never_see_null,
3105                                                     RegionNode* region,
3106                                                     int null_path,
</pre>
</td>
</tr>
</table>
<center><a href="c2compiler.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../prims/jvm.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>