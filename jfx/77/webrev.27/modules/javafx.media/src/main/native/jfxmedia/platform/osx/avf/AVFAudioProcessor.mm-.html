<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.media/src/main/native/jfxmedia/platform/osx/avf/AVFAudioProcessor.mm</title>
    <link rel="stylesheet" href="../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.  Oracle designates this
  8  * particular file as subject to the &quot;Classpath&quot; exception as provided
  9  * by Oracle in the LICENSE file that accompanied this code.
 10  *
 11  * This code is distributed in the hope that it will be useful, but WITHOUT
 12  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 13  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 14  * version 2 for more details (a copy is included in the LICENSE file that
 15  * accompanied this code).
 16  *
 17  * You should have received a copy of the GNU General Public License version
 18  * 2 along with this work; if not, write to the Free Software Foundation,
 19  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 20  *
 21  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 22  * or visit www.oracle.com if you need additional information or have any
 23  * questions.
 24  */
 25 
 26 #import &quot;AVFAudioProcessor.h&quot;
 27 #import &quot;AVFMediaPlayer.h&quot;
 28 
 29 #import &lt;AVFoundation/AVFoundation.h&gt;
 30 #import &lt;MediaToolbox/MediaToolbox.h&gt;
 31 
 32 #import &quot;AVFKernelProcessor.h&quot;
 33 #import &lt;CoreFoundation/CoreFoundation.h&gt;
 34 
 35 #import &lt;pthread.h&gt;
 36 #import &lt;objc/message.h&gt;
 37 
 38 static void InitAudioTap(MTAudioProcessingTapRef tapRef, void *clientInfo, void **tapStorageOut);
 39 static void FinalizeAudioTap(MTAudioProcessingTapRef tapRef);
 40 static void PrepareAudioTap(MTAudioProcessingTapRef tapRef,
 41                             CMItemCount maxFrames,
 42                             const AudioStreamBasicDescription *processingFormat);
 43 static void UnprepareAudioTap(MTAudioProcessingTapRef tapRef);
 44 static void ProcessAudioTap(MTAudioProcessingTapRef tapRef, CMItemCount numberFrames,
 45                             MTAudioProcessingTapFlags flags,
 46                             AudioBufferList *bufferListInOut,
 47                             CMItemCount *numberFramesOut,
 48                             MTAudioProcessingTapFlags *flagsOut);
 49 
 50 static OSStatus AVFTapRenderCallback(void *inRefCon,
 51                                      AudioUnitRenderActionFlags *ioActionFlags,
 52                                      const AudioTimeStamp *inTimeStamp,
 53                                      UInt32 inBusNumber,
 54                                      UInt32 inNumberFrames,
 55                                      AudioBufferList *ioData);
 56 
 57 class AVFTapContext {
 58 public:
 59     AVFTapContext(AVFSoundLevelUnitPtr slu, AVFAudioSpectrumUnitPtr spectrum, AVFAudioEqualizerPtr eq) :
 60         audioSLU(slu),
 61         audioSpectrum(spectrum),
 62         audioEQ(eq)
 63     {
 64     }
 65 
 66     ~AVFTapContext() {
 67         // AudioUnits have already been deallocated by now
 68         // shared_ptrs get freed automatically
 69     }
 70 
 71     AudioUnit spectrumUnit;
 72     AudioUnit volumeUnit;
 73     AudioUnit eqUnit;
 74 
 75     AudioUnit renderUnit; // the last unit in our chain
 76     CMItemCount totalFrames;
 77     
 78     // Hold on to these while we&#39;re running
 79     AVFSoundLevelUnitPtr audioSLU;
 80     AVFAudioSpectrumUnitPtr audioSpectrum;
 81     AVFAudioEqualizerPtr audioEQ;
 82 };
 83 
 84 @implementation AVFAudioProcessor
 85 
 86 - (id) init {
 87     if ((self = [super init]) != nil) {
 88         _soundLevelUnit = AVFSoundLevelUnitPtr(new AVFSoundLevelUnit());
 89         _audioSpectrum = AVFAudioSpectrumUnitPtr(new AVFAudioSpectrumUnit());
 90         _audioEqualizer = AVFAudioEqualizerPtr(new AVFAudioEqualizer());
 91 
 92         _volume = 1.0f;
 93         _balance = 0.0f;
 94         _audioDelay = 0LL;
 95     }
 96     return self;
 97 }
 98 
 99 - (void) dealloc {
100     _soundLevelUnit = nullptr;
101     _audioSpectrum = nullptr;
102     _audioEqualizer = nullptr;
103 }
104 
105 - (void) setAudioTrack:(AVAssetTrack *)track {
106     if (track != _audioTrack) {
107         // reset the audio mixer if it&#39;s already been created
108         // this theoretically should never happen...
109         _mixer = nil;
110     }
111     _audioTrack = track;
112 }
113 
114 - (AVAudioMix*) mixer {
115     if (!self.audioTrack) {
116         return nil;
117     }
118 
119     if (!_mixer) {
120         AVMutableAudioMix *mixer = [AVMutableAudioMix audioMix];
121         if (mixer) {
122             AVMutableAudioMixInputParameters *audioMixInputParameters =
123                 [AVMutableAudioMixInputParameters audioMixInputParametersWithTrack:self.audioTrack];
124             if (audioMixInputParameters &amp;&amp;
125                 [audioMixInputParameters respondsToSelector:@selector(setAudioTapProcessor:)]) {
126                 MTAudioProcessingTapCallbacks callbacks;
127 
128                 callbacks.version = kMTAudioProcessingTapCallbacksVersion_0;
129                 callbacks.clientInfo = (__bridge void *)self;
130                 callbacks.init = InitAudioTap;
131                 callbacks.finalize = FinalizeAudioTap;
132                 callbacks.prepare = PrepareAudioTap;
133                 callbacks.unprepare = UnprepareAudioTap;
134                 callbacks.process = ProcessAudioTap;
135 
136                 MTAudioProcessingTapRef audioProcessingTap;
137                 if (noErr == MTAudioProcessingTapCreate(kCFAllocatorDefault, &amp;callbacks,
138                                              kMTAudioProcessingTapCreationFlag_PreEffects,
139                                              &amp;audioProcessingTap))
140                 {
141                     objc_msgSend(audioMixInputParameters,
142                                  @selector(setAudioTapProcessor:),
143                                  audioProcessingTap);
144 
145                     CFRelease(audioProcessingTap); // owned by the mixer now
146                     mixer.inputParameters = @[audioMixInputParameters];
147 
148                     _mixer = mixer;
149                 }
150             }
151         }
152     }
153     return _mixer;
154 }
155 
156 - (void) setVolume:(float)volume {
157     _volume = volume;
158     if (_soundLevelUnit != nullptr) {
159         _soundLevelUnit-&gt;setVolume(volume);
160     }
161 }
162 
163 - (void) setBalance:(float)balance {
164     _balance = balance;
165     if (_soundLevelUnit != nullptr) {
166         _soundLevelUnit-&gt;setBalance(balance);
167     }
168 }
169 
170 @end
171 
172 void InitAudioTap(MTAudioProcessingTapRef tapRef, void *clientInfo, void **tapStorageOut)
173 {
174     // retain the AU kernels so they don&#39;t get freed while we&#39;re running
175     AVFAudioProcessor *processor = (__bridge AVFAudioProcessor *)clientInfo;
176     if (processor) {
177         AVFTapContext *context = new AVFTapContext(processor.soundLevelUnit,
178                                                    processor.audioSpectrum,
179                                                    processor.audioEqualizer);
180         *tapStorageOut = context;
181     }
182 }
183 
184 void FinalizeAudioTap(MTAudioProcessingTapRef tapRef)
185 {
186     AVFTapContext *context = (AVFTapContext*)MTAudioProcessingTapGetStorage(tapRef);
187 
188     if (context) {
189         delete context;
190     }
191 }
192 
193 static OSStatus SetupAudioUnit(AudioUnit unit,
194                                const AudioStreamBasicDescription *processingFormat,
195                                UInt32 maxFrames) {
196     OSStatus status = noErr;
197     if (noErr == status) {
198         status = AudioUnitSetProperty(unit,
199                                       kAudioUnitProperty_StreamFormat,
200                                       kAudioUnitScope_Input, 0,
201                                       processingFormat, sizeof(AudioStreamBasicDescription));
202     }
203     if (noErr == status) {
204         status = AudioUnitSetProperty(unit,
205                                       kAudioUnitProperty_StreamFormat,
206                                       kAudioUnitScope_Output, 0,
207                                       processingFormat, sizeof(AudioStreamBasicDescription));
208     }
209     if (noErr == status) {
210         status = AudioUnitSetProperty(unit,
211                                       kAudioUnitProperty_MaximumFramesPerSlice,
212                                       kAudioUnitScope_Global, 0,
213                                       &amp;maxFrames, sizeof(UInt32));
214     }
215     if (noErr == status) {
216         status = AudioUnitInitialize(unit);
217     }
218     return status;
219 }
220 
221 static OSStatus ConnectAudioUnits(AudioUnit source, AudioUnit sink) {
222     AudioUnitConnection connection;
223     connection.sourceAudioUnit = source;
224     connection.sourceOutputNumber = 0;
225     connection.destInputNumber = 0;
226     return AudioUnitSetProperty(sink, kAudioUnitProperty_MakeConnection,
227                                 kAudioUnitScope_Input, 0,
228                                 &amp;connection, sizeof(connection));
229 }
230 
231 AudioUnit FindAudioUnit(OSType type, OSType subType, OSType manu) {
232     AudioUnit audioUnit = NULL;
233 
234     AudioComponentDescription audioComponentDescription;
235     audioComponentDescription.componentType = type;
236     audioComponentDescription.componentSubType = subType;
237     audioComponentDescription.componentManufacturer = manu;
238     audioComponentDescription.componentFlags = 0;
239     audioComponentDescription.componentFlagsMask = 0;
240 
241     AudioComponent audioComponent = AudioComponentFindNext(NULL, &amp;audioComponentDescription);
242     if (audioComponent) {
243         AudioComponentInstanceNew(audioComponent, &amp;audioUnit);
244     }
245     return audioUnit;
246 }
247 
248 void PrepareAudioTap(MTAudioProcessingTapRef tapRef,
249                      CMItemCount maxFrames,
250                      const AudioStreamBasicDescription *processingFormat)
251 {
252     AVFTapContext *context = (AVFTapContext*)MTAudioProcessingTapGetStorage(tapRef);
253 
254     // Validate the audio format before we enable the processor
255 
256     // Failures here should rarely, if ever, happen so leave the NSLogs in for
257     // easier diagnosis in the field
258     if (processingFormat-&gt;mFormatID != kAudioFormatLinearPCM) {
259         NSLog(@&quot;AVFAudioProcessor needs linear PCM&quot;);
260         return;
261     }
262 
263     // Use the convenient kAudioFormatFlagsNativeFloatPacked to check if we can
264     // process the incoming audio
265     if ((processingFormat-&gt;mFormatFlags &amp; kAudioFormatFlagsNativeFloatPacked)
266         != kAudioFormatFlagsNativeFloatPacked) {
267         NSLog(@&quot;AVFAudioProcessor needs native endian packed float samples!!&quot;);
268         return;
269     }
270 
271     // Get an instance of our sound level unit
272     context-&gt;eqUnit = NULL;
273     if (context-&gt;audioEQ != nullptr) {
274         context-&gt;eqUnit = NewKernelProcessorUnit(static_pointer_cast&lt;AVFKernelProcessor&gt;(context-&gt;audioEQ));
275         if (context-&gt;eqUnit) {
276             OSStatus status = SetupAudioUnit(context-&gt;eqUnit,
277                                              processingFormat,
278                                              (UInt32)maxFrames);
279             if (noErr != status) {
280                 NSLog(@&quot;Error creating audio equalizer unit: %d&quot;, status);
281                 AudioComponentInstanceDispose(context-&gt;eqUnit);
282                 context-&gt;eqUnit = NULL;
283             }
284         }
285     }
286 
287     context-&gt;spectrumUnit = NULL;
288     if (context-&gt;audioSpectrum != nullptr) {
289         context-&gt;spectrumUnit = NewKernelProcessorUnit(static_pointer_cast&lt;AVFKernelProcessor&gt;(context-&gt;audioSpectrum));
290         if (context-&gt;spectrumUnit) {
291             OSStatus status = SetupAudioUnit(context-&gt;spectrumUnit,
292                                              processingFormat,
293                                              (UInt32)maxFrames);
294             if (noErr != status) {
295                 NSLog(@&quot;Error creating audio spectrum unit: %d&quot;, status);
296                 AudioComponentInstanceDispose(context-&gt;spectrumUnit);
297                 context-&gt;spectrumUnit = NULL;
298             }
299         }
300     }
301 
302     context-&gt;volumeUnit = NULL;
303     if (context-&gt;audioSLU != nullptr) {
304         context-&gt;volumeUnit = NewKernelProcessorUnit(static_pointer_cast&lt;AVFKernelProcessor&gt;(context-&gt;audioSLU));
305         if (context-&gt;volumeUnit) {
306             OSStatus status = SetupAudioUnit(context-&gt;volumeUnit,
307                                              processingFormat,
308                                              (UInt32)maxFrames);
309             if (noErr != status) {
310                 NSLog(@&quot;Error setting up Sound Level Unit: %d&quot;, status);
311                 AudioComponentInstanceDispose(context-&gt;volumeUnit);
312                 context-&gt;volumeUnit = NULL;
313             }
314         }
315     }
316 
317     /*
318      * Use AudioUnitConnections to build a processing graph
319      * The last unit in the chain will be the unit we call to render, it will
320      * pull through the graph until we get to the first, which will fetch samples
321      * via the render proc we install.
322      *
323      * The graph will look like this:
324      *    (render proc) -&gt; eqUnit -&gt; spectrumUnit -&gt; volUnit
325      *
326      * This will allow the EQ settings to affect the spectrum output, but not
327      * the volume or balance.
328      */
329     AudioUnit firstUnit = NULL;
330     context-&gt;renderUnit = NULL;
331 
332     // Set initial settings
333     if (context-&gt;eqUnit) {
334         if (context-&gt;renderUnit) {
335             ConnectAudioUnits(context-&gt;renderUnit, context-&gt;eqUnit);
336         }
337         context-&gt;renderUnit = context-&gt;eqUnit;
338         if (!firstUnit) {
339             firstUnit = context-&gt;eqUnit;
340         }
341     }
342     if (context-&gt;spectrumUnit) {
343         if (context-&gt;renderUnit) {
344             ConnectAudioUnits(context-&gt;renderUnit, context-&gt;spectrumUnit);
345         }
346         context-&gt;renderUnit = context-&gt;spectrumUnit;
347         if (!firstUnit) {
348             firstUnit = context-&gt;spectrumUnit;
349         }
350     }
351     if (context-&gt;volumeUnit) {
352         if (context-&gt;renderUnit) {
353             ConnectAudioUnits(context-&gt;renderUnit, context-&gt;volumeUnit);
354         }
355         context-&gt;renderUnit = context-&gt;volumeUnit;
356         if (!firstUnit) {
357             firstUnit = context-&gt;volumeUnit;
358         }
359     }
360 
361     // Set up a render callback on our first unit
362     if (firstUnit) {
363         AURenderCallbackStruct renderCB;
364         renderCB.inputProc = (AURenderCallback)AVFTapRenderCallback;
365         renderCB.inputProcRefCon = (void*)tapRef;
366         AudioUnitSetProperty(firstUnit,
367                              kAudioUnitProperty_SetRenderCallback,
368                              kAudioUnitScope_Input, 0,
369                              &amp;renderCB, sizeof(renderCB));
370     }
371     context-&gt;totalFrames = 0;
372 }
373 
374 void UnprepareAudioTap(MTAudioProcessingTapRef tapRef)
375 {
376     AVFTapContext *context = (AVFTapContext*)MTAudioProcessingTapGetStorage(tapRef);
377     context-&gt;renderUnit = NULL;
378 
379     if (context-&gt;spectrumUnit) {
380         AudioUnitUninitialize(context-&gt;spectrumUnit);
381         AudioComponentInstanceDispose(context-&gt;spectrumUnit);
382         context-&gt;spectrumUnit = NULL;
383     }
384     if (context-&gt;volumeUnit) {
385         AudioUnitUninitialize(context-&gt;volumeUnit);
386         AudioComponentInstanceDispose(context-&gt;volumeUnit);
387         context-&gt;volumeUnit = NULL;
388     }
389     if (context-&gt;eqUnit) {
390         AudioUnitUninitialize(context-&gt;eqUnit);
391         AudioComponentInstanceDispose(context-&gt;eqUnit);
392         context-&gt;eqUnit = NULL;
393     }
394 }
395 
396 void ProcessAudioTap(MTAudioProcessingTapRef tapRef,
397                      CMItemCount numberFrames,
398                      uint32_t flags,
399                      AudioBufferList *bufferListInOut,
400                      CMItemCount *numberFramesOut,
401                      uint32_t *flagsOut)
402 {
403     AVFTapContext *context = (AVFTapContext*)MTAudioProcessingTapGetStorage(tapRef);
404     OSStatus status = noErr;
405 
406     if (context-&gt;renderUnit) {
407         AudioTimeStamp audioTimeStamp;
408         audioTimeStamp.mSampleTime = context-&gt;totalFrames;
409         audioTimeStamp.mFlags = kAudioTimeStampSampleTimeValid;
410 
411         status = AudioUnitRender(context-&gt;renderUnit,
412                                  0,
413                                  &amp;audioTimeStamp,
414                                  0,
415                                  (UInt32)numberFrames,
416                                  bufferListInOut);
417         if (noErr != status) {
418             return;
419         }
420         context-&gt;totalFrames += numberFrames;
421         *numberFramesOut = numberFrames;
422     } else {
423         MTAudioProcessingTapGetSourceAudio(tapRef, numberFrames, bufferListInOut,
424                                 flagsOut, NULL, numberFramesOut);
425     }
426 }
427 
428 static OSStatus AVFTapRenderCallback(void *inRefCon,
429                                      AudioUnitRenderActionFlags *ioActionFlags,
430                                      const AudioTimeStamp *inTimeStamp,
431                                      UInt32 inBusNumber,
432                                      UInt32 inNumberFrames,
433                                      AudioBufferList *ioData)
434 {
435     MTAudioProcessingTapRef tapRef = static_cast&lt;MTAudioProcessingTapRef&gt;(inRefCon);
436     return MTAudioProcessingTapGetSourceAudio(tapRef, inNumberFrames, ioData, NULL, NULL, NULL);
437 }
    </pre>
  </body>
</html>