<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/Heap.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="Gigacage.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="Heap.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/Heap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 37 #include &quot;Scavenger.h&quot;
 38 #include &quot;SmallLine.h&quot;
 39 #include &quot;SmallPage.h&quot;
 40 #include &quot;VMHeap.h&quot;
 41 #include &quot;bmalloc.h&quot;
 42 #include &lt;thread&gt;
 43 #include &lt;vector&gt;
 44 
 45 namespace bmalloc {
 46 
 47 Heap::Heap(HeapKind kind, std::lock_guard&lt;Mutex&gt;&amp;)
 48     : m_kind(kind)
 49     , m_vmPageSizePhysical(vmPageSizePhysical())
 50 {
 51     RELEASE_BASSERT(vmPageSizePhysical() &gt;= smallPageSize);
 52     RELEASE_BASSERT(vmPageSize() &gt;= vmPageSizePhysical());
 53 
 54     initializeLineMetadata();
 55     initializePageMetadata();
 56 
<span class="line-modified"> 57     BASSERT(!PerProcess&lt;Environment&gt;::get()-&gt;isDebugHeapEnabled());</span>
 58 
<span class="line-modified"> 59         Gigacage::ensureGigacage();</span>
 60 #if GIGACAGE_ENABLED
<span class="line-modified"> 61         if (usingGigacage()) {</span>
<span class="line-modified"> 62             RELEASE_BASSERT(gigacageBasePtr());</span>
<span class="line-modified"> 63             uint64_t random[2];</span>
<span class="line-modified"> 64             cryptoRandom(reinterpret_cast&lt;unsigned char*&gt;(random), sizeof(random));</span>
<span class="line-modified"> 65             size_t size = roundDownToMultipleOf(vmPageSize(), gigacageSize() - (random[0] % Gigacage::maximumCageSizeReductionForSlide));</span>
<span class="line-modified"> 66             ptrdiff_t offset = roundDownToMultipleOf(vmPageSize(), random[1] % (gigacageSize() - size));</span>
<span class="line-modified"> 67             void* base = reinterpret_cast&lt;unsigned char*&gt;(gigacageBasePtr()) + offset;</span>
<span class="line-modified"> 68             m_largeFree.add(LargeRange(base, size, 0, 0));</span>
<span class="line-modified"> 69         }</span>
 70 #endif
 71 
<span class="line-modified"> 72     m_scavenger = PerProcess&lt;Scavenger&gt;::get();</span>
 73 }
 74 
 75 bool Heap::usingGigacage()
 76 {
 77     return isGigacage(m_kind) &amp;&amp; gigacageBasePtr();
 78 }
 79 
 80 void* Heap::gigacageBasePtr()
 81 {
 82     return Gigacage::basePtr(gigacageKind(m_kind));
 83 }
 84 
 85 size_t Heap::gigacageSize()
 86 {
 87     return Gigacage::size(gigacageKind(m_kind));
 88 }
 89 
 90 void Heap::initializeLineMetadata()
 91 {
 92     size_t sizeClassCount = bmalloc::sizeClass(smallLineSize);
</pre>
<hr />
<pre>
158     m_largeFree.markAllAsEligibile();
159     m_hasPendingDecommits = false;
160     m_condition.notify_all();
161 }
162 
163 void Heap::decommitLargeRange(std::lock_guard&lt;Mutex&gt;&amp;, LargeRange&amp; range, BulkDecommit&amp; decommitter)
164 {
165     m_footprint -= range.totalPhysicalSize();
166     m_freeableMemory -= range.totalPhysicalSize();
167     decommitter.addLazy(range.begin(), range.size());
168     m_hasPendingDecommits = true;
169     range.setStartPhysicalSize(0);
170     range.setTotalPhysicalSize(0);
171     BASSERT(range.isEligibile());
172     range.setEligible(false);
173 #if ENABLE_PHYSICAL_PAGE_MAP
174     m_physicalPageMap.decommit(range.begin(), range.size());
175 #endif
176 }
177 
<span class="line-modified">178 void Heap::scavenge(std::lock_guard&lt;Mutex&gt;&amp; lock, BulkDecommit&amp; decommitter)</span>
179 {
180     for (auto&amp; list : m_freePages) {
181         for (auto* chunk : list) {
182             for (auto* page : chunk-&gt;freePages()) {
183                 if (!page-&gt;hasPhysicalPages())
184                     continue;





185 
186                 size_t pageSize = bmalloc::pageSize(&amp;list - &amp;m_freePages[0]);
187                 size_t decommitSize = physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize);
188                 m_freeableMemory -= decommitSize;
189                 m_footprint -= decommitSize;
190                 decommitter.addEager(page-&gt;begin()-&gt;begin(), pageSize);
191                 page-&gt;setHasPhysicalPages(false);
192 #if ENABLE_PHYSICAL_PAGE_MAP
193                 m_physicalPageMap.decommit(page-&gt;begin()-&gt;begin(), pageSize);
194 #endif
195             }
196         }
197     }
198 
199     for (auto&amp; list : m_chunkCache) {
200         while (!list.isEmpty())
201             deallocateSmallChunk(list.pop(), &amp;list - &amp;m_chunkCache[0]);
202     }
203 
204     for (LargeRange&amp; range : m_largeFree) {
<span class="line-modified">205         m_highWatermark = std::min(m_highWatermark, static_cast&lt;void*&gt;(range.begin()));</span>




206         decommitLargeRange(lock, range, decommitter);
207     }
<span class="line-removed">208 </span>
<span class="line-removed">209     m_freeableMemory = 0;</span>
<span class="line-removed">210 }</span>
<span class="line-removed">211 </span>
<span class="line-removed">212 void Heap::scavengeToHighWatermark(std::lock_guard&lt;Mutex&gt;&amp; lock, BulkDecommit&amp; decommitter)</span>
<span class="line-removed">213 {</span>
<span class="line-removed">214     void* newHighWaterMark = nullptr;</span>
<span class="line-removed">215     for (LargeRange&amp; range : m_largeFree) {</span>
<span class="line-removed">216         if (range.begin() &lt;= m_highWatermark)</span>
<span class="line-removed">217             newHighWaterMark = std::min(newHighWaterMark, static_cast&lt;void*&gt;(range.begin()));</span>
<span class="line-removed">218         else</span>
<span class="line-removed">219             decommitLargeRange(lock, range, decommitter);</span>
<span class="line-removed">220     }</span>
<span class="line-removed">221     m_highWatermark = newHighWaterMark;</span>
222 }
223 
224 void Heap::deallocateLineCache(std::unique_lock&lt;Mutex&gt;&amp;, LineCache&amp; lineCache)
225 {
226     for (auto&amp; list : lineCache) {
227         while (!list.isEmpty()) {
228             size_t sizeClass = &amp;list - &amp;lineCache[0];
229             m_lineCache[sizeClass].push(list.popFront());
230         }
231     }
232 }
233 
234 void Heap::allocateSmallChunk(std::unique_lock&lt;Mutex&gt;&amp; lock, size_t pageClass)
235 {
236     RELEASE_BASSERT(isActiveHeapKind(m_kind));
237 
238     size_t pageSize = bmalloc::pageSize(pageClass);
239 
240     Chunk* chunk = [&amp;]() {
241         if (!m_chunkCache[pageClass].isEmpty())
242             return m_chunkCache[pageClass].pop();
243 
244         void* memory = allocateLarge(lock, chunkSize, chunkSize);
245 
246         Chunk* chunk = new (memory) Chunk(pageSize);
247 
248         m_objectTypes.set(chunk, ObjectType::Small);
249 
250         forEachPage(chunk, pageSize, [&amp;](SmallPage* page) {
251             page-&gt;setHasPhysicalPages(true);

252             page-&gt;setHasFreeLines(lock, true);
253             chunk-&gt;freePages().push(page);
254         });
255 
256         m_freeableMemory += chunkSize;
257 
258         m_scavenger-&gt;schedule(0);
259 
260         return chunk;
261     }();
262 
263     m_freePages[pageClass].push(chunk);
264 }
265 
266 void Heap::deallocateSmallChunk(Chunk* chunk, size_t pageClass)
267 {
268     m_objectTypes.set(chunk, ObjectType::Large);
269 
270     size_t size = m_largeAllocated.remove(chunk);
271     size_t totalPhysicalSize = size;
</pre>
<hr />
<pre>
311 
312         chunk-&gt;ref();
313 
314         SmallPage* page = chunk-&gt;freePages().pop();
315         if (chunk-&gt;freePages().isEmpty())
316             m_freePages[pageClass].remove(chunk);
317 
318         size_t pageSize = bmalloc::pageSize(pageClass);
319         size_t physicalSize = physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize);
320         if (page-&gt;hasPhysicalPages())
321             m_freeableMemory -= physicalSize;
322         else {
323             m_scavenger-&gt;scheduleIfUnderMemoryPressure(pageSize);
324             m_footprint += physicalSize;
325             vmAllocatePhysicalPagesSloppy(page-&gt;begin()-&gt;begin(), pageSize);
326             page-&gt;setHasPhysicalPages(true);
327 #if ENABLE_PHYSICAL_PAGE_MAP
328             m_physicalPageMap.commit(page-&gt;begin()-&gt;begin(), pageSize);
329 #endif
330         }

331 
332         return page;
333     }();
334 
335     page-&gt;setSizeClass(sizeClass);
336     return page;
337 }
338 
339 void Heap::deallocateSmallLine(std::unique_lock&lt;Mutex&gt;&amp; lock, Object object, LineCache&amp; lineCache)
340 {
341     BASSERT(!object.line()-&gt;refCount(lock));
342     SmallPage* page = object.page();
343     page-&gt;deref(lock);
344 
345     if (!page-&gt;hasFreeLines(lock)) {
346         page-&gt;setHasFreeLines(lock, true);
347         lineCache[page-&gt;sizeClass()].push(page);
348     }
349 
350     if (page-&gt;refCount(lock))
</pre>
<hr />
<pre>
557     if (roundedSize &lt; size) // Check for overflow
558         return nullptr;
559     size = roundedSize;
560 
561     size_t roundedAlignment = roundUpToMultipleOf&lt;largeAlignment&gt;(alignment);
562     if (roundedAlignment &lt; alignment) // Check for overflow
563         return nullptr;
564     alignment = roundedAlignment;
565 
566     LargeRange range = m_largeFree.remove(alignment, size);
567     if (!range) {
568         if (m_hasPendingDecommits) {
569             m_condition.wait(lock, [&amp;]() { return !m_hasPendingDecommits; });
570             // Now we&#39;re guaranteed we&#39;re looking at all available memory.
571             return tryAllocateLarge(lock, alignment, size);
572         }
573 
574         if (usingGigacage())
575             return nullptr;
576 
<span class="line-modified">577         range = PerProcess&lt;VMHeap&gt;::get()-&gt;tryAllocateLargeChunk(alignment, size);</span>
578         if (!range)
579             return nullptr;
580 
581         m_largeFree.add(range);
582         range = m_largeFree.remove(alignment, size);
583     }
584 
585     m_freeableMemory -= range.totalPhysicalSize();
586 
587     void* result = splitAndAllocate(lock, range, alignment, size).begin();
<span class="line-removed">588     m_highWatermark = std::max(m_highWatermark, result);</span>
589     return result;
590 }
591 
592 void* Heap::allocateLarge(std::unique_lock&lt;Mutex&gt;&amp; lock, size_t alignment, size_t size)
593 {
594     void* result = tryAllocateLarge(lock, alignment, size);
595     RELEASE_BASSERT(result);
596     return result;
597 }
598 
599 bool Heap::isLarge(std::unique_lock&lt;Mutex&gt;&amp;, void* object)
600 {
601     return m_objectTypes.get(Object(object).chunk()) == ObjectType::Large;
602 }
603 
604 size_t Heap::largeSize(std::unique_lock&lt;Mutex&gt;&amp;, void* object)
605 {
606     return m_largeAllocated.get(object);
607 }
608 
</pre>
</td>
<td>
<hr />
<pre>
 37 #include &quot;Scavenger.h&quot;
 38 #include &quot;SmallLine.h&quot;
 39 #include &quot;SmallPage.h&quot;
 40 #include &quot;VMHeap.h&quot;
 41 #include &quot;bmalloc.h&quot;
 42 #include &lt;thread&gt;
 43 #include &lt;vector&gt;
 44 
 45 namespace bmalloc {
 46 
 47 Heap::Heap(HeapKind kind, std::lock_guard&lt;Mutex&gt;&amp;)
 48     : m_kind(kind)
 49     , m_vmPageSizePhysical(vmPageSizePhysical())
 50 {
 51     RELEASE_BASSERT(vmPageSizePhysical() &gt;= smallPageSize);
 52     RELEASE_BASSERT(vmPageSize() &gt;= vmPageSizePhysical());
 53 
 54     initializeLineMetadata();
 55     initializePageMetadata();
 56 
<span class="line-modified"> 57     BASSERT(!Environment::get()-&gt;isDebugHeapEnabled());</span>
 58 
<span class="line-modified"> 59     Gigacage::ensureGigacage();</span>
 60 #if GIGACAGE_ENABLED
<span class="line-modified"> 61     if (usingGigacage()) {</span>
<span class="line-modified"> 62         RELEASE_BASSERT(gigacageBasePtr());</span>
<span class="line-modified"> 63         uint64_t random[2];</span>
<span class="line-modified"> 64         cryptoRandom(reinterpret_cast&lt;unsigned char*&gt;(random), sizeof(random));</span>
<span class="line-modified"> 65         size_t size = roundDownToMultipleOf(vmPageSize(), gigacageSize() - (random[0] % Gigacage::maximumCageSizeReductionForSlide));</span>
<span class="line-modified"> 66         ptrdiff_t offset = roundDownToMultipleOf(vmPageSize(), random[1] % (gigacageSize() - size));</span>
<span class="line-modified"> 67         void* base = reinterpret_cast&lt;unsigned char*&gt;(gigacageBasePtr()) + offset;</span>
<span class="line-modified"> 68         m_largeFree.add(LargeRange(base, size, 0, 0));</span>
<span class="line-modified"> 69     }</span>
 70 #endif
 71 
<span class="line-modified"> 72     m_scavenger = Scavenger::get();</span>
 73 }
 74 
 75 bool Heap::usingGigacage()
 76 {
 77     return isGigacage(m_kind) &amp;&amp; gigacageBasePtr();
 78 }
 79 
 80 void* Heap::gigacageBasePtr()
 81 {
 82     return Gigacage::basePtr(gigacageKind(m_kind));
 83 }
 84 
 85 size_t Heap::gigacageSize()
 86 {
 87     return Gigacage::size(gigacageKind(m_kind));
 88 }
 89 
 90 void Heap::initializeLineMetadata()
 91 {
 92     size_t sizeClassCount = bmalloc::sizeClass(smallLineSize);
</pre>
<hr />
<pre>
158     m_largeFree.markAllAsEligibile();
159     m_hasPendingDecommits = false;
160     m_condition.notify_all();
161 }
162 
163 void Heap::decommitLargeRange(std::lock_guard&lt;Mutex&gt;&amp;, LargeRange&amp; range, BulkDecommit&amp; decommitter)
164 {
165     m_footprint -= range.totalPhysicalSize();
166     m_freeableMemory -= range.totalPhysicalSize();
167     decommitter.addLazy(range.begin(), range.size());
168     m_hasPendingDecommits = true;
169     range.setStartPhysicalSize(0);
170     range.setTotalPhysicalSize(0);
171     BASSERT(range.isEligibile());
172     range.setEligible(false);
173 #if ENABLE_PHYSICAL_PAGE_MAP
174     m_physicalPageMap.decommit(range.begin(), range.size());
175 #endif
176 }
177 
<span class="line-modified">178 void Heap::scavenge(std::lock_guard&lt;Mutex&gt;&amp; lock, BulkDecommit&amp; decommitter, size_t&amp; deferredDecommits)</span>
179 {
180     for (auto&amp; list : m_freePages) {
181         for (auto* chunk : list) {
182             for (auto* page : chunk-&gt;freePages()) {
183                 if (!page-&gt;hasPhysicalPages())
184                     continue;
<span class="line-added">185                 if (page-&gt;usedSinceLastScavenge()) {</span>
<span class="line-added">186                     page-&gt;clearUsedSinceLastScavenge();</span>
<span class="line-added">187                     deferredDecommits++;</span>
<span class="line-added">188                     continue;</span>
<span class="line-added">189                 }</span>
190 
191                 size_t pageSize = bmalloc::pageSize(&amp;list - &amp;m_freePages[0]);
192                 size_t decommitSize = physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize);
193                 m_freeableMemory -= decommitSize;
194                 m_footprint -= decommitSize;
195                 decommitter.addEager(page-&gt;begin()-&gt;begin(), pageSize);
196                 page-&gt;setHasPhysicalPages(false);
197 #if ENABLE_PHYSICAL_PAGE_MAP
198                 m_physicalPageMap.decommit(page-&gt;begin()-&gt;begin(), pageSize);
199 #endif
200             }
201         }
202     }
203 
204     for (auto&amp; list : m_chunkCache) {
205         while (!list.isEmpty())
206             deallocateSmallChunk(list.pop(), &amp;list - &amp;m_chunkCache[0]);
207     }
208 
209     for (LargeRange&amp; range : m_largeFree) {
<span class="line-modified">210         if (range.usedSinceLastScavenge()) {</span>
<span class="line-added">211             range.clearUsedSinceLastScavenge();</span>
<span class="line-added">212             deferredDecommits++;</span>
<span class="line-added">213             continue;</span>
<span class="line-added">214         }</span>
215         decommitLargeRange(lock, range, decommitter);
216     }














217 }
218 
219 void Heap::deallocateLineCache(std::unique_lock&lt;Mutex&gt;&amp;, LineCache&amp; lineCache)
220 {
221     for (auto&amp; list : lineCache) {
222         while (!list.isEmpty()) {
223             size_t sizeClass = &amp;list - &amp;lineCache[0];
224             m_lineCache[sizeClass].push(list.popFront());
225         }
226     }
227 }
228 
229 void Heap::allocateSmallChunk(std::unique_lock&lt;Mutex&gt;&amp; lock, size_t pageClass)
230 {
231     RELEASE_BASSERT(isActiveHeapKind(m_kind));
232 
233     size_t pageSize = bmalloc::pageSize(pageClass);
234 
235     Chunk* chunk = [&amp;]() {
236         if (!m_chunkCache[pageClass].isEmpty())
237             return m_chunkCache[pageClass].pop();
238 
239         void* memory = allocateLarge(lock, chunkSize, chunkSize);
240 
241         Chunk* chunk = new (memory) Chunk(pageSize);
242 
243         m_objectTypes.set(chunk, ObjectType::Small);
244 
245         forEachPage(chunk, pageSize, [&amp;](SmallPage* page) {
246             page-&gt;setHasPhysicalPages(true);
<span class="line-added">247             page-&gt;setUsedSinceLastScavenge();</span>
248             page-&gt;setHasFreeLines(lock, true);
249             chunk-&gt;freePages().push(page);
250         });
251 
252         m_freeableMemory += chunkSize;
253 
254         m_scavenger-&gt;schedule(0);
255 
256         return chunk;
257     }();
258 
259     m_freePages[pageClass].push(chunk);
260 }
261 
262 void Heap::deallocateSmallChunk(Chunk* chunk, size_t pageClass)
263 {
264     m_objectTypes.set(chunk, ObjectType::Large);
265 
266     size_t size = m_largeAllocated.remove(chunk);
267     size_t totalPhysicalSize = size;
</pre>
<hr />
<pre>
307 
308         chunk-&gt;ref();
309 
310         SmallPage* page = chunk-&gt;freePages().pop();
311         if (chunk-&gt;freePages().isEmpty())
312             m_freePages[pageClass].remove(chunk);
313 
314         size_t pageSize = bmalloc::pageSize(pageClass);
315         size_t physicalSize = physicalPageSizeSloppy(page-&gt;begin()-&gt;begin(), pageSize);
316         if (page-&gt;hasPhysicalPages())
317             m_freeableMemory -= physicalSize;
318         else {
319             m_scavenger-&gt;scheduleIfUnderMemoryPressure(pageSize);
320             m_footprint += physicalSize;
321             vmAllocatePhysicalPagesSloppy(page-&gt;begin()-&gt;begin(), pageSize);
322             page-&gt;setHasPhysicalPages(true);
323 #if ENABLE_PHYSICAL_PAGE_MAP
324             m_physicalPageMap.commit(page-&gt;begin()-&gt;begin(), pageSize);
325 #endif
326         }
<span class="line-added">327         page-&gt;setUsedSinceLastScavenge();</span>
328 
329         return page;
330     }();
331 
332     page-&gt;setSizeClass(sizeClass);
333     return page;
334 }
335 
336 void Heap::deallocateSmallLine(std::unique_lock&lt;Mutex&gt;&amp; lock, Object object, LineCache&amp; lineCache)
337 {
338     BASSERT(!object.line()-&gt;refCount(lock));
339     SmallPage* page = object.page();
340     page-&gt;deref(lock);
341 
342     if (!page-&gt;hasFreeLines(lock)) {
343         page-&gt;setHasFreeLines(lock, true);
344         lineCache[page-&gt;sizeClass()].push(page);
345     }
346 
347     if (page-&gt;refCount(lock))
</pre>
<hr />
<pre>
554     if (roundedSize &lt; size) // Check for overflow
555         return nullptr;
556     size = roundedSize;
557 
558     size_t roundedAlignment = roundUpToMultipleOf&lt;largeAlignment&gt;(alignment);
559     if (roundedAlignment &lt; alignment) // Check for overflow
560         return nullptr;
561     alignment = roundedAlignment;
562 
563     LargeRange range = m_largeFree.remove(alignment, size);
564     if (!range) {
565         if (m_hasPendingDecommits) {
566             m_condition.wait(lock, [&amp;]() { return !m_hasPendingDecommits; });
567             // Now we&#39;re guaranteed we&#39;re looking at all available memory.
568             return tryAllocateLarge(lock, alignment, size);
569         }
570 
571         if (usingGigacage())
572             return nullptr;
573 
<span class="line-modified">574         range = VMHeap::get()-&gt;tryAllocateLargeChunk(alignment, size);</span>
575         if (!range)
576             return nullptr;
577 
578         m_largeFree.add(range);
579         range = m_largeFree.remove(alignment, size);
580     }
581 
582     m_freeableMemory -= range.totalPhysicalSize();
583 
584     void* result = splitAndAllocate(lock, range, alignment, size).begin();

585     return result;
586 }
587 
588 void* Heap::allocateLarge(std::unique_lock&lt;Mutex&gt;&amp; lock, size_t alignment, size_t size)
589 {
590     void* result = tryAllocateLarge(lock, alignment, size);
591     RELEASE_BASSERT(result);
592     return result;
593 }
594 
595 bool Heap::isLarge(std::unique_lock&lt;Mutex&gt;&amp;, void* object)
596 {
597     return m_objectTypes.get(Object(object).chunk()) == ObjectType::Large;
598 }
599 
600 size_t Heap::largeSize(std::unique_lock&lt;Mutex&gt;&amp;, void* object)
601 {
602     return m_largeAllocated.get(object);
603 }
604 
</pre>
</td>
</tr>
</table>
<center><a href="Gigacage.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="Heap.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>