<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/Heap.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  *  Copyright (C) 2003-2019 Apple Inc. All rights reserved.
   3  *  Copyright (C) 2007 Eric Seidel &lt;eric@webkit.org&gt;
   4  *
   5  *  This library is free software; you can redistribute it and/or
   6  *  modify it under the terms of the GNU Lesser General Public
   7  *  License as published by the Free Software Foundation; either
   8  *  version 2 of the License, or (at your option) any later version.
   9  *
  10  *  This library is distributed in the hope that it will be useful,
  11  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
  12  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
  13  *  Lesser General Public License for more details.
  14  *
  15  *  You should have received a copy of the GNU Lesser General Public
  16  *  License along with this library; if not, write to the Free Software
  17  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
  18  *
  19  */
  20 
  21 #include &quot;config.h&quot;
  22 #include &quot;Heap.h&quot;
  23 
  24 #include &quot;BlockDirectoryInlines.h&quot;
<a name="1" id="anc1"></a><span class="line-added">  25 #include &quot;BuiltinExecutables.h&quot;</span>
  26 #include &quot;CodeBlock.h&quot;
  27 #include &quot;CodeBlockSetInlines.h&quot;
  28 #include &quot;CollectingScope.h&quot;
  29 #include &quot;ConservativeRoots.h&quot;
  30 #include &quot;DFGWorklistInlines.h&quot;
  31 #include &quot;EdenGCActivityCallback.h&quot;
  32 #include &quot;Exception.h&quot;
  33 #include &quot;FullGCActivityCallback.h&quot;
<a name="2" id="anc2"></a><span class="line-added">  34 #include &quot;FunctionExecutableInlines.h&quot;</span>
  35 #include &quot;GCActivityCallback.h&quot;
  36 #include &quot;GCIncomingRefCountedSetInlines.h&quot;
  37 #include &quot;GCSegmentedArrayInlines.h&quot;
  38 #include &quot;GCTypeMap.h&quot;
  39 #include &quot;HasOwnPropertyCache.h&quot;
  40 #include &quot;HeapHelperPool.h&quot;
  41 #include &quot;HeapIterationScope.h&quot;
  42 #include &quot;HeapProfiler.h&quot;
  43 #include &quot;HeapSnapshot.h&quot;
  44 #include &quot;HeapVerifier.h&quot;
  45 #include &quot;IncrementalSweeper.h&quot;
  46 #include &quot;InferredValueInlines.h&quot;
  47 #include &quot;Interpreter.h&quot;
  48 #include &quot;IsoCellSetInlines.h&quot;
  49 #include &quot;JITStubRoutineSet.h&quot;
  50 #include &quot;JITWorklist.h&quot;
  51 #include &quot;JSCInlines.h&quot;
  52 #include &quot;JSGlobalObject.h&quot;
  53 #include &quot;JSLock.h&quot;
  54 #include &quot;JSVirtualMachineInternal.h&quot;
  55 #include &quot;JSWeakMap.h&quot;
<a name="3" id="anc3"></a><span class="line-added">  56 #include &quot;JSWeakObjectRef.h&quot;</span>
  57 #include &quot;JSWeakSet.h&quot;
  58 #include &quot;JSWebAssemblyCodeBlock.h&quot;
  59 #include &quot;MachineStackMarker.h&quot;
  60 #include &quot;MarkStackMergingConstraint.h&quot;
  61 #include &quot;MarkedSpaceInlines.h&quot;
  62 #include &quot;MarkingConstraintSet.h&quot;
  63 #include &quot;PreventCollectionScope.h&quot;
  64 #include &quot;SamplingProfiler.h&quot;
  65 #include &quot;ShadowChicken.h&quot;
  66 #include &quot;SpaceTimeMutatorScheduler.h&quot;
  67 #include &quot;StochasticSpaceTimeMutatorScheduler.h&quot;
  68 #include &quot;StopIfNecessaryTimer.h&quot;
  69 #include &quot;SubspaceInlines.h&quot;
  70 #include &quot;SuperSampler.h&quot;
  71 #include &quot;SweepingScope.h&quot;
<a name="4" id="anc4"></a><span class="line-added">  72 #include &quot;SymbolTableInlines.h&quot;</span>
  73 #include &quot;SynchronousStopTheWorldMutatorScheduler.h&quot;
  74 #include &quot;TypeProfiler.h&quot;
  75 #include &quot;TypeProfilerLog.h&quot;
  76 #include &quot;UnlinkedCodeBlock.h&quot;
  77 #include &quot;VM.h&quot;
  78 #include &quot;VisitCounter.h&quot;
  79 #include &quot;WasmMemory.h&quot;
  80 #include &quot;WeakMapImplInlines.h&quot;
  81 #include &quot;WeakSetInlines.h&quot;
  82 #include &lt;algorithm&gt;
  83 #include &lt;wtf/ListDump.h&gt;
  84 #include &lt;wtf/MainThread.h&gt;
  85 #include &lt;wtf/ParallelVectorIterator.h&gt;
  86 #include &lt;wtf/ProcessID.h&gt;
  87 #include &lt;wtf/RAMSize.h&gt;
  88 #include &lt;wtf/SimpleStats.h&gt;
  89 #include &lt;wtf/Threading.h&gt;
  90 
  91 #if PLATFORM(IOS_FAMILY)
  92 #include &lt;bmalloc/bmalloc.h&gt;
  93 #endif
  94 
  95 #if USE(FOUNDATION)
  96 #include &lt;wtf/spi/cocoa/objcSPI.h&gt;
  97 #endif
  98 
<a name="5" id="anc5"></a><span class="line-modified">  99 #ifdef JSC_GLIB_API_ENABLED</span>
 100 #include &quot;JSCGLibWrapperObject.h&quot;
 101 #endif
 102 
 103 namespace JSC {
 104 
 105 namespace {
 106 
 107 bool verboseStop = false;
 108 
 109 double maxPauseMS(double thisPauseMS)
 110 {
 111     static double maxPauseMS;
 112     maxPauseMS = std::max(thisPauseMS, maxPauseMS);
 113     return maxPauseMS;
 114 }
 115 
 116 size_t minHeapSize(HeapType heapType, size_t ramSize)
 117 {
 118     if (heapType == LargeHeap) {
 119         double result = std::min(
 120             static_cast&lt;double&gt;(Options::largeHeapSize()),
 121             ramSize * Options::smallHeapRAMFraction());
 122         return static_cast&lt;size_t&gt;(result);
 123     }
 124     return Options::smallHeapSize();
 125 }
 126 
 127 size_t proportionalHeapSize(size_t heapSize, size_t ramSize)
 128 {
 129     if (VM::isInMiniMode())
 130         return Options::miniVMHeapGrowthFactor() * heapSize;
 131 
 132 #if PLATFORM(IOS_FAMILY)
 133     size_t memoryFootprint = bmalloc::api::memoryFootprint();
 134     if (memoryFootprint &lt; ramSize * Options::smallHeapRAMFraction())
 135         return Options::smallHeapGrowthFactor() * heapSize;
 136     if (memoryFootprint &lt; ramSize * Options::mediumHeapRAMFraction())
 137         return Options::mediumHeapGrowthFactor() * heapSize;
 138 #else
 139     if (heapSize &lt; ramSize * Options::smallHeapRAMFraction())
 140         return Options::smallHeapGrowthFactor() * heapSize;
 141     if (heapSize &lt; ramSize * Options::mediumHeapRAMFraction())
 142         return Options::mediumHeapGrowthFactor() * heapSize;
 143 #endif
 144     return Options::largeHeapGrowthFactor() * heapSize;
 145 }
 146 
<a name="6" id="anc6"></a><span class="line-modified"> 147 bool isValidSharedInstanceThreadState(VM&amp; vm)</span>
 148 {
<a name="7" id="anc7"></a><span class="line-modified"> 149     return vm.currentThreadIsHoldingAPILock();</span>
 150 }
 151 
<a name="8" id="anc8"></a><span class="line-modified"> 152 bool isValidThreadState(VM&amp; vm)</span>
 153 {
<a name="9" id="anc9"></a><span class="line-modified"> 154     if (vm.atomStringTable() != Thread::current().atomStringTable())</span>
 155         return false;
 156 
<a name="10" id="anc10"></a><span class="line-modified"> 157     if (vm.isSharedInstance() &amp;&amp; !isValidSharedInstanceThreadState(vm))</span>
 158         return false;
 159 
 160     return true;
 161 }
 162 
 163 void recordType(VM&amp; vm, TypeCountSet&amp; set, JSCell* cell)
 164 {
 165     const char* typeName = &quot;[unknown]&quot;;
 166     const ClassInfo* info = cell-&gt;classInfo(vm);
 167     if (info &amp;&amp; info-&gt;className)
 168         typeName = info-&gt;className;
 169     set.add(typeName);
 170 }
 171 
 172 bool measurePhaseTiming()
 173 {
 174     return false;
 175 }
 176 
 177 HashMap&lt;const char*, GCTypeMap&lt;SimpleStats&gt;&gt;&amp; timingStats()
 178 {
 179     static HashMap&lt;const char*, GCTypeMap&lt;SimpleStats&gt;&gt;* result;
 180     static std::once_flag once;
 181     std::call_once(
 182         once,
 183         [] {
 184             result = new HashMap&lt;const char*, GCTypeMap&lt;SimpleStats&gt;&gt;();
 185         });
 186     return *result;
 187 }
 188 
 189 SimpleStats&amp; timingStats(const char* name, CollectionScope scope)
 190 {
 191     return timingStats().add(name, GCTypeMap&lt;SimpleStats&gt;()).iterator-&gt;value[scope];
 192 }
 193 
 194 class TimingScope {
 195 public:
 196     TimingScope(Optional&lt;CollectionScope&gt; scope, const char* name)
 197         : m_scope(scope)
 198         , m_name(name)
 199     {
 200         if (measurePhaseTiming())
 201             m_before = MonotonicTime::now();
 202     }
 203 
 204     TimingScope(Heap&amp; heap, const char* name)
 205         : TimingScope(heap.collectionScope(), name)
 206     {
 207     }
 208 
 209     void setScope(Optional&lt;CollectionScope&gt; scope)
 210     {
 211         m_scope = scope;
 212     }
 213 
 214     void setScope(Heap&amp; heap)
 215     {
 216         setScope(heap.collectionScope());
 217     }
 218 
 219     ~TimingScope()
 220     {
 221         if (measurePhaseTiming()) {
 222             MonotonicTime after = MonotonicTime::now();
 223             Seconds timing = after - m_before;
 224             SimpleStats&amp; stats = timingStats(m_name, *m_scope);
 225             stats.add(timing.milliseconds());
 226             dataLog(&quot;[GC:&quot;, *m_scope, &quot;] &quot;, m_name, &quot; took: &quot;, timing.milliseconds(), &quot;ms (average &quot;, stats.mean(), &quot;ms).\n&quot;);
 227         }
 228     }
 229 private:
 230     Optional&lt;CollectionScope&gt; m_scope;
 231     MonotonicTime m_before;
 232     const char* m_name;
 233 };
 234 
 235 } // anonymous namespace
 236 
<a name="11" id="anc11"></a><span class="line-modified"> 237 class Heap::HeapThread : public AutomaticThread {</span>
 238 public:
<a name="12" id="anc12"></a><span class="line-modified"> 239     HeapThread(const AbstractLocker&amp; locker, Heap&amp; heap)</span>
 240         : AutomaticThread(locker, heap.m_threadLock, heap.m_threadCondition.copyRef())
 241         , m_heap(heap)
 242     {
 243     }
 244 
 245     const char* name() const override
 246     {
 247         return &quot;JSC Heap Collector Thread&quot;;
 248     }
 249 
 250 protected:
 251     PollResult poll(const AbstractLocker&amp; locker) override
 252     {
 253         if (m_heap.m_threadShouldStop) {
 254             m_heap.notifyThreadStopping(locker);
 255             return PollResult::Stop;
 256         }
<a name="13" id="anc13"></a><span class="line-modified"> 257         if (m_heap.shouldCollectInCollectorThread(locker)) {</span>
<span class="line-added"> 258             m_heap.m_collectorThreadIsRunning = true;</span>
 259             return PollResult::Work;
<a name="14" id="anc14"></a><span class="line-added"> 260         }</span>
<span class="line-added"> 261         m_heap.m_collectorThreadIsRunning = false;</span>
 262         return PollResult::Wait;
 263     }
 264 
 265     WorkResult work() override
 266     {
 267         m_heap.collectInCollectorThread();
 268         return WorkResult::Continue;
 269     }
 270 
 271     void threadDidStart() override
 272     {
<a name="15" id="anc15"></a><span class="line-modified"> 273         Thread::registerGCThread(GCThreadType::Main);</span>
<span class="line-added"> 274     }</span>
<span class="line-added"> 275 </span>
<span class="line-added"> 276     void threadIsStopping(const AbstractLocker&amp;) override</span>
<span class="line-added"> 277     {</span>
<span class="line-added"> 278         m_heap.m_collectorThreadIsRunning = false;</span>
 279     }
 280 
 281 private:
 282     Heap&amp; m_heap;
 283 };
 284 
<a name="16" id="anc16"></a><span class="line-modified"> 285 Heap::Heap(VM&amp; vm, HeapType heapType)</span>
 286     : m_heapType(heapType)
 287     , m_ramSize(Options::forceRAMSize() ? Options::forceRAMSize() : ramSize())
 288     , m_minBytesPerCycle(minHeapSize(m_heapType, m_ramSize))
 289     , m_maxEdenSize(m_minBytesPerCycle)
 290     , m_maxHeapSize(m_minBytesPerCycle)
 291     , m_objectSpace(this)
<a name="17" id="anc17"></a><span class="line-modified"> 292     , m_machineThreads(makeUnique&lt;MachineThreads&gt;())</span>
<span class="line-modified"> 293     , m_collectorSlotVisitor(makeUnique&lt;SlotVisitor&gt;(*this, &quot;C&quot;))</span>
<span class="line-modified"> 294     , m_mutatorSlotVisitor(makeUnique&lt;SlotVisitor&gt;(*this, &quot;M&quot;))</span>
<span class="line-modified"> 295     , m_mutatorMarkStack(makeUnique&lt;MarkStackArray&gt;())</span>
<span class="line-modified"> 296     , m_raceMarkStack(makeUnique&lt;MarkStackArray&gt;())</span>
<span class="line-modified"> 297     , m_constraintSet(makeUnique&lt;MarkingConstraintSet&gt;(*this))</span>
 298     , m_handleSet(vm)
<a name="18" id="anc18"></a><span class="line-modified"> 299     , m_codeBlocks(makeUnique&lt;CodeBlockSet&gt;())</span>
<span class="line-modified"> 300     , m_jitStubRoutines(makeUnique&lt;JITStubRoutineSet&gt;())</span>
 301     , m_vm(vm)
 302     // We seed with 10ms so that GCActivityCallback::didAllocate doesn&#39;t continuously
 303     // schedule the timer if we&#39;ve never done a collection.
 304     , m_fullActivityCallback(GCActivityCallback::tryCreateFullTimer(this))
 305     , m_edenActivityCallback(GCActivityCallback::tryCreateEdenTimer(this))
 306     , m_sweeper(adoptRef(*new IncrementalSweeper(this)))
 307     , m_stopIfNecessaryTimer(adoptRef(*new StopIfNecessaryTimer(vm)))
<a name="19" id="anc19"></a><span class="line-modified"> 308     , m_sharedCollectorMarkStack(makeUnique&lt;MarkStackArray&gt;())</span>
<span class="line-modified"> 309     , m_sharedMutatorMarkStack(makeUnique&lt;MarkStackArray&gt;())</span>
 310     , m_helperClient(&amp;heapHelperPool())
 311     , m_threadLock(Box&lt;Lock&gt;::create())
 312     , m_threadCondition(AutomaticThreadCondition::create())
 313 {
 314     m_worldState.store(0);
 315 
<a name="20" id="anc20"></a><span class="line-added"> 316     for (unsigned i = 0, numberOfParallelThreads = heapHelperPool().numberOfThreads(); i &lt; numberOfParallelThreads; ++i) {</span>
<span class="line-added"> 317         std::unique_ptr&lt;SlotVisitor&gt; visitor = makeUnique&lt;SlotVisitor&gt;(*this, toCString(&quot;P&quot;, i + 1));</span>
<span class="line-added"> 318         if (Options::optimizeParallelSlotVisitorsForStoppedMutator())</span>
<span class="line-added"> 319             visitor-&gt;optimizeForStoppedMutator();</span>
<span class="line-added"> 320         m_availableParallelSlotVisitors.append(visitor.get());</span>
<span class="line-added"> 321         m_parallelSlotVisitors.append(WTFMove(visitor));</span>
<span class="line-added"> 322     }</span>
<span class="line-added"> 323 </span>
 324     if (Options::useConcurrentGC()) {
 325         if (Options::useStochasticMutatorScheduler())
<a name="21" id="anc21"></a><span class="line-modified"> 326             m_scheduler = makeUnique&lt;StochasticSpaceTimeMutatorScheduler&gt;(*this);</span>
 327         else
<a name="22" id="anc22"></a><span class="line-modified"> 328             m_scheduler = makeUnique&lt;SpaceTimeMutatorScheduler&gt;(*this);</span>
 329     } else {
 330         // We simulate turning off concurrent GC by making the scheduler say that the world
 331         // should always be stopped when the collector is running.
<a name="23" id="anc23"></a><span class="line-modified"> 332         m_scheduler = makeUnique&lt;SynchronousStopTheWorldMutatorScheduler&gt;();</span>
 333     }
 334 
 335     if (Options::verifyHeap())
<a name="24" id="anc24"></a><span class="line-modified"> 336         m_verifier = makeUnique&lt;HeapVerifier&gt;(this, Options::numberOfGCCyclesToRecordForVerification());</span>
 337 
 338     m_collectorSlotVisitor-&gt;optimizeForStoppedMutator();
 339 
 340     // When memory is critical, allow allocating 25% of the amount above the critical threshold before collecting.
 341     size_t memoryAboveCriticalThreshold = static_cast&lt;size_t&gt;(static_cast&lt;double&gt;(m_ramSize) * (1.0 - Options::criticalGCMemoryThreshold()));
 342     m_maxEdenSizeWhenCritical = memoryAboveCriticalThreshold / 4;
 343 
 344     LockHolder locker(*m_threadLock);
<a name="25" id="anc25"></a><span class="line-modified"> 345     m_thread = adoptRef(new HeapThread(locker, *this));</span>
 346 }
 347 
 348 Heap::~Heap()
 349 {
<a name="26" id="anc26"></a><span class="line-added"> 350     // Scribble m_worldState to make it clear that the heap has already been destroyed if we crash in checkConn</span>
<span class="line-added"> 351     m_worldState.store(0xbadbeeffu);</span>
<span class="line-added"> 352 </span>
 353     forEachSlotVisitor(
 354         [&amp;] (SlotVisitor&amp; visitor) {
 355             visitor.clearMarkStacks();
 356         });
 357     m_mutatorMarkStack-&gt;clear();
 358     m_raceMarkStack-&gt;clear();
 359 
 360     for (WeakBlock* block : m_logicallyEmptyWeakBlocks)
 361         WeakBlock::destroy(*this, block);
 362 }
 363 
 364 bool Heap::isPagedOut(MonotonicTime deadline)
 365 {
 366     return m_objectSpace.isPagedOut(deadline);
 367 }
 368 
<a name="27" id="anc27"></a><span class="line-added"> 369 void Heap::dumpHeapStatisticsAtVMDestruction()</span>
<span class="line-added"> 370 {</span>
<span class="line-added"> 371     unsigned counter = 0;</span>
<span class="line-added"> 372     m_objectSpace.forEachBlock([&amp;] (MarkedBlock::Handle* block) {</span>
<span class="line-added"> 373         unsigned live = 0;</span>
<span class="line-added"> 374         block-&gt;forEachCell([&amp;] (HeapCell* cell, HeapCell::Kind) {</span>
<span class="line-added"> 375             if (cell-&gt;isLive())</span>
<span class="line-added"> 376                 live++;</span>
<span class="line-added"> 377             return IterationStatus::Continue;</span>
<span class="line-added"> 378         });</span>
<span class="line-added"> 379         dataLogLn(&quot;[&quot;, counter++, &quot;] &quot;, block-&gt;cellSize(), &quot;, &quot;, live, &quot; / &quot;, block-&gt;cellsPerBlock(), &quot; &quot;, static_cast&lt;double&gt;(live) / block-&gt;cellsPerBlock() * 100, &quot;% &quot;, block-&gt;attributes(), &quot; &quot;, block-&gt;subspace()-&gt;name());</span>
<span class="line-added"> 380         block-&gt;forEachCell([&amp;] (HeapCell* heapCell, HeapCell::Kind kind) {</span>
<span class="line-added"> 381             if (heapCell-&gt;isLive() &amp;&amp; kind == HeapCell::Kind::JSCell) {</span>
<span class="line-added"> 382                 auto* cell = static_cast&lt;JSCell*&gt;(heapCell);</span>
<span class="line-added"> 383                 if (cell-&gt;isObject())</span>
<span class="line-added"> 384                     dataLogLn(&quot;    &quot;, JSValue((JSObject*)cell));</span>
<span class="line-added"> 385                 else</span>
<span class="line-added"> 386                     dataLogLn(&quot;    &quot;, *cell);</span>
<span class="line-added"> 387             }</span>
<span class="line-added"> 388             return IterationStatus::Continue;</span>
<span class="line-added"> 389         });</span>
<span class="line-added"> 390     });</span>
<span class="line-added"> 391 }</span>
<span class="line-added"> 392 </span>
 393 // The VM is being destroyed and the collector will never run again.
 394 // Run all pending finalizers now because we won&#39;t get another chance.
 395 void Heap::lastChanceToFinalize()
 396 {
 397     MonotonicTime before;
 398     if (Options::logGC()) {
 399         before = MonotonicTime::now();
 400         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: shutdown &quot;);
 401     }
 402 
 403     m_isShuttingDown = true;
 404 
<a name="28" id="anc28"></a><span class="line-modified"> 405     RELEASE_ASSERT(!m_vm.entryScope);</span>
 406     RELEASE_ASSERT(m_mutatorState == MutatorState::Running);
 407 
 408     if (m_collectContinuouslyThread) {
 409         {
 410             LockHolder locker(m_collectContinuouslyLock);
 411             m_shouldStopCollectingContinuously = true;
 412             m_collectContinuouslyCondition.notifyOne();
 413         }
 414         m_collectContinuouslyThread-&gt;waitForCompletion();
 415     }
 416 
 417     if (Options::logGC())
 418         dataLog(&quot;1&quot;);
 419 
 420     // Prevent new collections from being started. This is probably not even necessary, since we&#39;re not
 421     // going to call into anything that starts collections. Still, this makes the algorithm more
 422     // obviously sound.
 423     m_isSafeToCollect = false;
 424 
 425     if (Options::logGC())
 426         dataLog(&quot;2&quot;);
 427 
 428     bool isCollecting;
 429     {
 430         auto locker = holdLock(*m_threadLock);
 431         RELEASE_ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
 432         isCollecting = m_lastServedTicket &lt; m_lastGrantedTicket;
 433     }
 434     if (isCollecting) {
 435         if (Options::logGC())
 436             dataLog(&quot;...]\n&quot;);
 437 
 438         // Wait for the current collection to finish.
 439         waitForCollector(
 440             [&amp;] (const AbstractLocker&amp;) -&gt; bool {
 441                 RELEASE_ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
 442                 return m_lastServedTicket == m_lastGrantedTicket;
 443             });
 444 
 445         if (Options::logGC())
 446             dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: shutdown &quot;);
 447     }
 448     if (Options::logGC())
 449         dataLog(&quot;3&quot;);
 450 
 451     RELEASE_ASSERT(m_requests.isEmpty());
 452     RELEASE_ASSERT(m_lastServedTicket == m_lastGrantedTicket);
 453 
 454     // Carefully bring the thread down.
 455     bool stopped = false;
 456     {
 457         LockHolder locker(*m_threadLock);
 458         stopped = m_thread-&gt;tryStop(locker);
 459         m_threadShouldStop = true;
 460         if (!stopped)
 461             m_threadCondition-&gt;notifyOne(locker);
 462     }
 463 
 464     if (Options::logGC())
 465         dataLog(&quot;4&quot;);
 466 
 467     if (!stopped)
 468         m_thread-&gt;join();
 469 
 470     if (Options::logGC())
 471         dataLog(&quot;5 &quot;);
 472 
<a name="29" id="anc29"></a><span class="line-added"> 473     if (UNLIKELY(Options::dumpHeapStatisticsAtVMDestruction()))</span>
<span class="line-added"> 474         dumpHeapStatisticsAtVMDestruction();</span>
<span class="line-added"> 475 </span>
 476     m_arrayBuffers.lastChanceToFinalize();
 477     m_objectSpace.stopAllocatingForGood();
 478     m_objectSpace.lastChanceToFinalize();
 479     releaseDelayedReleasedObjects();
 480 
 481     sweepAllLogicallyEmptyWeakBlocks();
 482 
 483     m_objectSpace.freeMemory();
 484 
 485     if (Options::logGC())
 486         dataLog((MonotonicTime::now() - before).milliseconds(), &quot;ms]\n&quot;);
 487 }
 488 
 489 void Heap::releaseDelayedReleasedObjects()
 490 {
<a name="30" id="anc30"></a><span class="line-modified"> 491 #if USE(FOUNDATION) || defined(JSC_GLIB_API_ENABLED)</span>
 492     // We need to guard against the case that releasing an object can create more objects due to the
 493     // release calling into JS. When those JS call(s) exit and all locks are being dropped we end up
 494     // back here and could try to recursively release objects. We guard that with a recursive entry
 495     // count. Only the initial call will release objects, recursive calls simple return and let the
 496     // the initial call to the function take care of any objects created during release time.
 497     // This also means that we need to loop until there are no objects in m_delayedReleaseObjects
 498     // and use a temp Vector for the actual releasing.
 499     if (!m_delayedReleaseRecursionCount++) {
 500         while (!m_delayedReleaseObjects.isEmpty()) {
<a name="31" id="anc31"></a><span class="line-modified"> 501             ASSERT(m_vm.currentThreadIsHoldingAPILock());</span>
 502 
 503             auto objectsToRelease = WTFMove(m_delayedReleaseObjects);
 504 
 505             {
 506                 // We need to drop locks before calling out to arbitrary code.
 507                 JSLock::DropAllLocks dropAllLocks(m_vm);
 508 
 509 #if USE(FOUNDATION)
 510                 void* context = objc_autoreleasePoolPush();
 511 #endif
 512                 objectsToRelease.clear();
 513 #if USE(FOUNDATION)
 514                 objc_autoreleasePoolPop(context);
 515 #endif
 516             }
 517         }
 518     }
 519     m_delayedReleaseRecursionCount--;
 520 #endif
 521 }
 522 
 523 void Heap::reportExtraMemoryAllocatedSlowCase(size_t size)
 524 {
 525     didAllocate(size);
 526     collectIfNecessaryOrDefer();
 527 }
 528 
 529 void Heap::deprecatedReportExtraMemorySlowCase(size_t size)
 530 {
 531     // FIXME: Change this to use SaturatedArithmetic when available.
 532     // https://bugs.webkit.org/show_bug.cgi?id=170411
 533     Checked&lt;size_t, RecordOverflow&gt; checkedNewSize = m_deprecatedExtraMemorySize;
 534     checkedNewSize += size;
 535     m_deprecatedExtraMemorySize = UNLIKELY(checkedNewSize.hasOverflowed()) ? std::numeric_limits&lt;size_t&gt;::max() : checkedNewSize.unsafeGet();
 536     reportExtraMemoryAllocatedSlowCase(size);
 537 }
 538 
 539 bool Heap::overCriticalMemoryThreshold(MemoryThresholdCallType memoryThresholdCallType)
 540 {
 541 #if PLATFORM(IOS_FAMILY)
 542     if (memoryThresholdCallType == MemoryThresholdCallType::Direct || ++m_precentAvailableMemoryCachedCallCount &gt;= 100) {
 543         m_overCriticalMemoryThreshold = bmalloc::api::percentAvailableMemoryInUse() &gt; Options::criticalGCMemoryThreshold();
 544         m_precentAvailableMemoryCachedCallCount = 0;
 545     }
 546 
 547     return m_overCriticalMemoryThreshold;
 548 #else
 549     UNUSED_PARAM(memoryThresholdCallType);
 550     return false;
 551 #endif
 552 }
 553 
 554 void Heap::reportAbandonedObjectGraph()
 555 {
 556     // Our clients don&#39;t know exactly how much memory they
 557     // are abandoning so we just guess for them.
 558     size_t abandonedBytes = static_cast&lt;size_t&gt;(0.1 * capacity());
 559 
 560     // We want to accelerate the next collection. Because memory has just
 561     // been abandoned, the next collection has the potential to
 562     // be more profitable. Since allocation is the trigger for collection,
 563     // we hasten the next collection by pretending that we&#39;ve allocated more memory.
 564     if (m_fullActivityCallback) {
 565         m_fullActivityCallback-&gt;didAllocate(*this,
 566             m_sizeAfterLastCollect - m_sizeAfterLastFullCollect + m_bytesAllocatedThisCycle + m_bytesAbandonedSinceLastFullCollect);
 567     }
 568     m_bytesAbandonedSinceLastFullCollect += abandonedBytes;
 569 }
 570 
 571 void Heap::protect(JSValue k)
 572 {
 573     ASSERT(k);
<a name="32" id="anc32"></a><span class="line-modified"> 574     ASSERT(m_vm.currentThreadIsHoldingAPILock());</span>
 575 
 576     if (!k.isCell())
 577         return;
 578 
 579     m_protectedValues.add(k.asCell());
 580 }
 581 
 582 bool Heap::unprotect(JSValue k)
 583 {
 584     ASSERT(k);
<a name="33" id="anc33"></a><span class="line-modified"> 585     ASSERT(m_vm.currentThreadIsHoldingAPILock());</span>
 586 
 587     if (!k.isCell())
 588         return false;
 589 
 590     return m_protectedValues.remove(k.asCell());
 591 }
 592 
 593 void Heap::addReference(JSCell* cell, ArrayBuffer* buffer)
 594 {
 595     if (m_arrayBuffers.addReference(cell, buffer)) {
 596         collectIfNecessaryOrDefer();
 597         didAllocate(buffer-&gt;gcSizeEstimateInBytes());
 598     }
 599 }
 600 
 601 template&lt;typename CellType, typename CellSet&gt;
 602 void Heap::finalizeMarkedUnconditionalFinalizers(CellSet&amp; cellSet)
 603 {
 604     cellSet.forEachMarkedCell(
 605         [&amp;] (HeapCell* cell, HeapCell::Kind) {
<a name="34" id="anc34"></a><span class="line-modified"> 606             static_cast&lt;CellType*&gt;(cell)-&gt;finalizeUnconditionally(vm());</span>
 607         });
 608 }
 609 
 610 void Heap::finalizeUnconditionalFinalizers()
 611 {
<a name="35" id="anc35"></a><span class="line-modified"> 612     vm().builtinExecutables()-&gt;finalizeUnconditionally();</span>
<span class="line-modified"> 613     finalizeMarkedUnconditionalFinalizers&lt;FunctionExecutable&gt;(vm().functionExecutableSpace.space);</span>
<span class="line-modified"> 614     finalizeMarkedUnconditionalFinalizers&lt;SymbolTable&gt;(vm().symbolTableSpace);</span>
<span class="line-added"> 615     vm().forEachCodeBlockSpace(</span>
 616         [&amp;] (auto&amp; space) {
 617             this-&gt;finalizeMarkedUnconditionalFinalizers&lt;CodeBlock&gt;(space.set);
 618         });
<a name="36" id="anc36"></a><span class="line-modified"> 619     finalizeMarkedUnconditionalFinalizers&lt;ExecutableToCodeBlockEdge&gt;(vm().executableToCodeBlockEdgesWithFinalizers);</span>
<span class="line-modified"> 620     finalizeMarkedUnconditionalFinalizers&lt;StructureRareData&gt;(vm().structureRareDataSpace);</span>
<span class="line-modified"> 621     finalizeMarkedUnconditionalFinalizers&lt;UnlinkedFunctionExecutable&gt;(vm().unlinkedFunctionExecutableSpace.set);</span>
<span class="line-modified"> 622     if (vm().m_weakSetSpace)</span>
<span class="line-modified"> 623         finalizeMarkedUnconditionalFinalizers&lt;JSWeakSet&gt;(*vm().m_weakSetSpace);</span>
<span class="line-modified"> 624     if (vm().m_weakMapSpace)</span>
<span class="line-modified"> 625         finalizeMarkedUnconditionalFinalizers&lt;JSWeakMap&gt;(*vm().m_weakMapSpace);</span>
<span class="line-added"> 626     if (vm().m_weakObjectRefSpace)</span>
<span class="line-added"> 627         finalizeMarkedUnconditionalFinalizers&lt;JSWeakObjectRef&gt;(*vm().m_weakObjectRefSpace);</span>
<span class="line-added"> 628     if (vm().m_errorInstanceSpace)</span>
<span class="line-added"> 629         finalizeMarkedUnconditionalFinalizers&lt;ErrorInstance&gt;(*vm().m_errorInstanceSpace);</span>
 630 
 631 #if ENABLE(WEBASSEMBLY)
<a name="37" id="anc37"></a><span class="line-modified"> 632     if (vm().m_webAssemblyCodeBlockSpace)</span>
<span class="line-modified"> 633         finalizeMarkedUnconditionalFinalizers&lt;JSWebAssemblyCodeBlock&gt;(*vm().m_webAssemblyCodeBlockSpace);</span>
 634 #endif
 635 }
 636 
 637 void Heap::willStartIterating()
 638 {
 639     m_objectSpace.willStartIterating();
 640 }
 641 
 642 void Heap::didFinishIterating()
 643 {
 644     m_objectSpace.didFinishIterating();
 645 }
 646 
 647 void Heap::completeAllJITPlans()
 648 {
 649     if (!VM::canUseJIT())
 650         return;
 651 #if ENABLE(JIT)
<a name="38" id="anc38"></a><span class="line-modified"> 652     JITWorklist::ensureGlobalWorklist().completeAllForVM(m_vm);</span>
 653 #endif // ENABLE(JIT)
<a name="39" id="anc39"></a><span class="line-modified"> 654     DFG::completeAllPlansForVM(m_vm);</span>
 655 }
 656 
 657 template&lt;typename Func&gt;
 658 void Heap::iterateExecutingAndCompilingCodeBlocks(const Func&amp; func)
 659 {
 660     m_codeBlocks-&gt;iterateCurrentlyExecuting(func);
 661     if (VM::canUseJIT())
<a name="40" id="anc40"></a><span class="line-modified"> 662         DFG::iterateCodeBlocksForGC(m_vm, func);</span>
 663 }
 664 
 665 template&lt;typename Func&gt;
 666 void Heap::iterateExecutingAndCompilingCodeBlocksWithoutHoldingLocks(const Func&amp; func)
 667 {
 668     Vector&lt;CodeBlock*, 256&gt; codeBlocks;
 669     iterateExecutingAndCompilingCodeBlocks(
 670         [&amp;] (CodeBlock* codeBlock) {
 671             codeBlocks.append(codeBlock);
 672         });
 673     for (CodeBlock* codeBlock : codeBlocks)
 674         func(codeBlock);
 675 }
 676 
 677 void Heap::assertMarkStacksEmpty()
 678 {
 679     bool ok = true;
 680 
 681     if (!m_sharedCollectorMarkStack-&gt;isEmpty()) {
 682         dataLog(&quot;FATAL: Shared collector mark stack not empty! It has &quot;, m_sharedCollectorMarkStack-&gt;size(), &quot; elements.\n&quot;);
 683         ok = false;
 684     }
 685 
 686     if (!m_sharedMutatorMarkStack-&gt;isEmpty()) {
 687         dataLog(&quot;FATAL: Shared mutator mark stack not empty! It has &quot;, m_sharedMutatorMarkStack-&gt;size(), &quot; elements.\n&quot;);
 688         ok = false;
 689     }
 690 
 691     forEachSlotVisitor(
 692         [&amp;] (SlotVisitor&amp; visitor) {
 693             if (visitor.isEmpty())
 694                 return;
 695 
 696             dataLog(&quot;FATAL: Visitor &quot;, RawPointer(&amp;visitor), &quot; is not empty!\n&quot;);
 697             ok = false;
 698         });
 699 
 700     RELEASE_ASSERT(ok);
 701 }
 702 
 703 void Heap::gatherStackRoots(ConservativeRoots&amp; roots)
 704 {
 705     m_machineThreads-&gt;gatherConservativeRoots(roots, *m_jitStubRoutines, *m_codeBlocks, m_currentThreadState, m_currentThread);
 706 }
 707 
 708 void Heap::gatherJSStackRoots(ConservativeRoots&amp; roots)
 709 {
 710 #if ENABLE(C_LOOP)
<a name="41" id="anc41"></a><span class="line-modified"> 711     m_vm.interpreter-&gt;cloopStack().gatherConservativeRoots(roots, *m_jitStubRoutines, *m_codeBlocks);</span>
 712 #else
 713     UNUSED_PARAM(roots);
 714 #endif
 715 }
 716 
 717 void Heap::gatherScratchBufferRoots(ConservativeRoots&amp; roots)
 718 {
 719 #if ENABLE(DFG_JIT)
 720     if (!VM::canUseJIT())
 721         return;
<a name="42" id="anc42"></a><span class="line-modified"> 722     m_vm.gatherScratchBufferRoots(roots);</span>
 723 #else
 724     UNUSED_PARAM(roots);
 725 #endif
 726 }
 727 
 728 void Heap::beginMarking()
 729 {
 730     TimingScope timingScope(*this, &quot;Heap::beginMarking&quot;);
 731     m_jitStubRoutines-&gt;clearMarks();
 732     m_objectSpace.beginMarking();
 733     setMutatorShouldBeFenced(true);
 734 }
 735 
 736 void Heap::removeDeadCompilerWorklistEntries()
 737 {
 738 #if ENABLE(DFG_JIT)
 739     if (!VM::canUseJIT())
 740         return;
 741     for (unsigned i = DFG::numberOfWorklists(); i--;)
<a name="43" id="anc43"></a><span class="line-modified"> 742         DFG::existingWorklistForIndex(i).removeDeadPlans(m_vm);</span>
 743 #endif
 744 }
 745 
<a name="44" id="anc44"></a><span class="line-modified"> 746 bool Heap::isAnalyzingHeap() const</span>
 747 {
<a name="45" id="anc45"></a><span class="line-modified"> 748     HeapProfiler* heapProfiler = m_vm.heapProfiler();</span>
 749     if (UNLIKELY(heapProfiler))
<a name="46" id="anc46"></a><span class="line-modified"> 750         return heapProfiler-&gt;activeHeapAnalyzer();</span>
 751     return false;
 752 }
 753 
<a name="47" id="anc47"></a><span class="line-modified"> 754 struct GatherExtraHeapData : MarkedBlock::CountFunctor {</span>
<span class="line-modified"> 755     GatherExtraHeapData(VM&amp; vm, HeapAnalyzer&amp; analyzer)</span>
 756         : m_vm(vm)
<a name="48" id="anc48"></a><span class="line-modified"> 757         , m_analyzer(analyzer)</span>
 758     {
 759     }
 760 
 761     IterationStatus operator()(HeapCell* heapCell, HeapCell::Kind kind) const
 762     {
 763         if (isJSCellKind(kind)) {
 764             JSCell* cell = static_cast&lt;JSCell*&gt;(heapCell);
<a name="49" id="anc49"></a><span class="line-modified"> 765             cell-&gt;methodTable(m_vm)-&gt;analyzeHeap(cell, m_analyzer);</span>
 766         }
 767         return IterationStatus::Continue;
 768     }
 769 
 770     VM&amp; m_vm;
<a name="50" id="anc50"></a><span class="line-modified"> 771     HeapAnalyzer&amp; m_analyzer;</span>
 772 };
 773 
<a name="51" id="anc51"></a><span class="line-modified"> 774 void Heap::gatherExtraHeapData(HeapProfiler&amp; heapProfiler)</span>
 775 {
<a name="52" id="anc52"></a><span class="line-modified"> 776     if (auto* analyzer = heapProfiler.activeHeapAnalyzer()) {</span>
 777         HeapIterationScope heapIterationScope(*this);
<a name="53" id="anc53"></a><span class="line-modified"> 778         GatherExtraHeapData functor(m_vm, *analyzer);</span>
 779         m_objectSpace.forEachLiveCell(heapIterationScope, functor);
 780     }
 781 }
 782 
 783 struct RemoveDeadHeapSnapshotNodes : MarkedBlock::CountFunctor {
 784     RemoveDeadHeapSnapshotNodes(HeapSnapshot&amp; snapshot)
 785         : m_snapshot(snapshot)
 786     {
 787     }
 788 
 789     IterationStatus operator()(HeapCell* cell, HeapCell::Kind kind) const
 790     {
 791         if (isJSCellKind(kind))
 792             m_snapshot.sweepCell(static_cast&lt;JSCell*&gt;(cell));
 793         return IterationStatus::Continue;
 794     }
 795 
 796     HeapSnapshot&amp; m_snapshot;
 797 };
 798 
 799 void Heap::removeDeadHeapSnapshotNodes(HeapProfiler&amp; heapProfiler)
 800 {
 801     if (HeapSnapshot* snapshot = heapProfiler.mostRecentSnapshot()) {
 802         HeapIterationScope heapIterationScope(*this);
 803         RemoveDeadHeapSnapshotNodes functor(*snapshot);
 804         m_objectSpace.forEachDeadCell(heapIterationScope, functor);
 805         snapshot-&gt;shrinkToFit();
 806     }
 807 }
 808 
 809 void Heap::updateObjectCounts()
 810 {
 811     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full)
 812         m_totalBytesVisited = 0;
 813 
 814     m_totalBytesVisitedThisCycle = bytesVisited();
 815 
 816     m_totalBytesVisited += m_totalBytesVisitedThisCycle;
 817 }
 818 
 819 void Heap::endMarking()
 820 {
 821     forEachSlotVisitor(
 822         [&amp;] (SlotVisitor&amp; visitor) {
 823             visitor.reset();
 824         });
 825 
 826     assertMarkStacksEmpty();
 827 
 828     RELEASE_ASSERT(m_raceMarkStack-&gt;isEmpty());
 829 
 830     m_objectSpace.endMarking();
 831     setMutatorShouldBeFenced(Options::forceFencedBarrier());
 832 }
 833 
 834 size_t Heap::objectCount()
 835 {
 836     return m_objectSpace.objectCount();
 837 }
 838 
 839 size_t Heap::extraMemorySize()
 840 {
 841     // FIXME: Change this to use SaturatedArithmetic when available.
 842     // https://bugs.webkit.org/show_bug.cgi?id=170411
 843     Checked&lt;size_t, RecordOverflow&gt; checkedTotal = m_extraMemorySize;
 844     checkedTotal += m_deprecatedExtraMemorySize;
 845     checkedTotal += m_arrayBuffers.size();
 846     size_t total = UNLIKELY(checkedTotal.hasOverflowed()) ? std::numeric_limits&lt;size_t&gt;::max() : checkedTotal.unsafeGet();
 847 
 848     ASSERT(m_objectSpace.capacity() &gt;= m_objectSpace.size());
 849     return std::min(total, std::numeric_limits&lt;size_t&gt;::max() - m_objectSpace.capacity());
 850 }
 851 
 852 size_t Heap::size()
 853 {
 854     return m_objectSpace.size() + extraMemorySize();
 855 }
 856 
 857 size_t Heap::capacity()
 858 {
 859     return m_objectSpace.capacity() + extraMemorySize();
 860 }
 861 
 862 size_t Heap::protectedGlobalObjectCount()
 863 {
 864     size_t result = 0;
 865     forEachProtectedCell(
 866         [&amp;] (JSCell* cell) {
 867             if (cell-&gt;isObject() &amp;&amp; asObject(cell)-&gt;isGlobalObject())
 868                 result++;
 869         });
 870     return result;
 871 }
 872 
 873 size_t Heap::globalObjectCount()
 874 {
 875     HeapIterationScope iterationScope(*this);
 876     size_t result = 0;
 877     m_objectSpace.forEachLiveCell(
 878         iterationScope,
 879         [&amp;] (HeapCell* heapCell, HeapCell::Kind kind) -&gt; IterationStatus {
 880             if (!isJSCellKind(kind))
 881                 return IterationStatus::Continue;
 882             JSCell* cell = static_cast&lt;JSCell*&gt;(heapCell);
 883             if (cell-&gt;isObject() &amp;&amp; asObject(cell)-&gt;isGlobalObject())
 884                 result++;
 885             return IterationStatus::Continue;
 886         });
 887     return result;
 888 }
 889 
 890 size_t Heap::protectedObjectCount()
 891 {
 892     size_t result = 0;
 893     forEachProtectedCell(
 894         [&amp;] (JSCell*) {
 895             result++;
 896         });
 897     return result;
 898 }
 899 
 900 std::unique_ptr&lt;TypeCountSet&gt; Heap::protectedObjectTypeCounts()
 901 {
<a name="54" id="anc54"></a><span class="line-modified"> 902     std::unique_ptr&lt;TypeCountSet&gt; result = makeUnique&lt;TypeCountSet&gt;();</span>
 903     forEachProtectedCell(
 904         [&amp;] (JSCell* cell) {
<a name="55" id="anc55"></a><span class="line-modified"> 905             recordType(vm(), *result, cell);</span>
 906         });
 907     return result;
 908 }
 909 
 910 std::unique_ptr&lt;TypeCountSet&gt; Heap::objectTypeCounts()
 911 {
<a name="56" id="anc56"></a><span class="line-modified"> 912     std::unique_ptr&lt;TypeCountSet&gt; result = makeUnique&lt;TypeCountSet&gt;();</span>
 913     HeapIterationScope iterationScope(*this);
 914     m_objectSpace.forEachLiveCell(
 915         iterationScope,
 916         [&amp;] (HeapCell* cell, HeapCell::Kind kind) -&gt; IterationStatus {
 917             if (isJSCellKind(kind))
<a name="57" id="anc57"></a><span class="line-modified"> 918                 recordType(vm(), *result, static_cast&lt;JSCell*&gt;(cell));</span>
 919             return IterationStatus::Continue;
 920         });
 921     return result;
 922 }
 923 
 924 void Heap::deleteAllCodeBlocks(DeleteAllCodeEffort effort)
 925 {
 926     if (m_collectionScope &amp;&amp; effort == DeleteAllCodeIfNotCollecting)
 927         return;
 928 
<a name="58" id="anc58"></a><span class="line-modified"> 929     VM&amp; vm = m_vm;</span>
 930     PreventCollectionScope preventCollectionScope(*this);
 931 
 932     // If JavaScript is running, it&#39;s not safe to delete all JavaScript code, since
 933     // we&#39;ll end up returning to deleted code.
 934     RELEASE_ASSERT(!vm.entryScope);
 935     RELEASE_ASSERT(!m_collectionScope);
 936 
 937     completeAllJITPlans();
 938 
 939     vm.forEachScriptExecutableSpace(
 940         [&amp;] (auto&amp; spaceAndSet) {
 941             HeapIterationScope heapIterationScope(*this);
 942             auto&amp; set = spaceAndSet.set;
 943             set.forEachLiveCell(
 944                 [&amp;] (HeapCell* cell, HeapCell::Kind) {
 945                     ScriptExecutable* executable = static_cast&lt;ScriptExecutable*&gt;(cell);
 946                     executable-&gt;clearCode(set);
 947                 });
 948         });
 949 
 950 #if ENABLE(WEBASSEMBLY)
 951     {
 952         // We must ensure that we clear the JS call ICs from Wasm. Otherwise, Wasm will
 953         // have no idea that we cleared the code from all of the Executables in the
 954         // VM. This could leave Wasm in an inconsistent state where it has an IC that
 955         // points into a CodeBlock that could be dead. The IC will still succeed because
 956         // it uses a callee check, but then it will call into dead code.
 957         HeapIterationScope heapIterationScope(*this);
 958         if (vm.m_webAssemblyCodeBlockSpace) {
 959             vm.m_webAssemblyCodeBlockSpace-&gt;forEachLiveCell([&amp;] (HeapCell* cell, HeapCell::Kind kind) {
 960                 ASSERT_UNUSED(kind, kind == HeapCell::JSCell);
 961                 JSWebAssemblyCodeBlock* codeBlock = static_cast&lt;JSWebAssemblyCodeBlock*&gt;(cell);
 962                 codeBlock-&gt;clearJSCallICs(vm);
 963             });
 964         }
 965     }
 966 #endif
 967 }
 968 
 969 void Heap::deleteAllUnlinkedCodeBlocks(DeleteAllCodeEffort effort)
 970 {
 971     if (m_collectionScope &amp;&amp; effort == DeleteAllCodeIfNotCollecting)
 972         return;
 973 
<a name="59" id="anc59"></a><span class="line-modified"> 974     VM&amp; vm = m_vm;</span>
 975     PreventCollectionScope preventCollectionScope(*this);
 976 
 977     RELEASE_ASSERT(!m_collectionScope);
 978 
 979     HeapIterationScope heapIterationScope(*this);
 980     vm.unlinkedFunctionExecutableSpace.set.forEachLiveCell(
 981         [&amp;] (HeapCell* cell, HeapCell::Kind) {
 982             UnlinkedFunctionExecutable* executable = static_cast&lt;UnlinkedFunctionExecutable*&gt;(cell);
 983             executable-&gt;clearCode(vm);
 984         });
 985 }
 986 
 987 void Heap::deleteUnmarkedCompiledCode()
 988 {
<a name="60" id="anc60"></a><span class="line-modified"> 989     vm().forEachScriptExecutableSpace([] (auto&amp; space) { space.space.sweep(); });</span>
<span class="line-modified"> 990     vm().forEachCodeBlockSpace([] (auto&amp; space) { space.space.sweep(); }); // Sweeping must occur before deleting stubs, otherwise the stubs might still think they&#39;re alive as they get deleted.</span>
 991     m_jitStubRoutines-&gt;deleteUnmarkedJettisonedStubRoutines();
 992 }
 993 
 994 void Heap::addToRememberedSet(const JSCell* constCell)
 995 {
 996     JSCell* cell = const_cast&lt;JSCell*&gt;(constCell);
 997     ASSERT(cell);
 998     ASSERT(!Options::useConcurrentJIT() || !isCompilationThread());
 999     m_barriersExecuted++;
1000     if (m_mutatorShouldBeFenced) {
1001         WTF::loadLoadFence();
1002         if (!isMarked(cell)) {
1003             // During a full collection a store into an unmarked object that had surivived past
1004             // collections will manifest as a store to an unmarked PossiblyBlack object. If the
1005             // object gets marked at some time after this then it will go down the normal marking
1006             // path. So, we don&#39;t have to remember this object. We could return here. But we go
1007             // further and attempt to re-white the object.
1008 
1009             RELEASE_ASSERT(m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full);
1010 
1011             if (cell-&gt;atomicCompareExchangeCellStateStrong(CellState::PossiblyBlack, CellState::DefinitelyWhite) == CellState::PossiblyBlack) {
1012                 // Now we protect against this race:
1013                 //
1014                 //     1) Object starts out black + unmarked.
1015                 //     --&gt; We do isMarked here.
1016                 //     2) Object is marked and greyed.
1017                 //     3) Object is scanned and blacked.
1018                 //     --&gt; We do atomicCompareExchangeCellStateStrong here.
1019                 //
1020                 // In this case we would have made the object white again, even though it should
1021                 // be black. This check lets us correct our mistake. This relies on the fact that
1022                 // isMarked converges monotonically to true.
1023                 if (isMarked(cell)) {
1024                     // It&#39;s difficult to work out whether the object should be grey or black at
1025                     // this point. We say black conservatively.
1026                     cell-&gt;setCellState(CellState::PossiblyBlack);
1027                 }
1028 
1029                 // Either way, we can return. Most likely, the object was not marked, and so the
1030                 // object is now labeled white. This means that future barrier executions will not
1031                 // fire. In the unlikely event that the object had become marked, we can still
1032                 // return anyway, since we proved that the object was not marked at the time that
1033                 // we executed this slow path.
1034             }
1035 
1036             return;
1037         }
1038     } else
<a name="61" id="anc61"></a><span class="line-modified">1039         ASSERT(isMarked(cell));</span>
1040     // It could be that the object was *just* marked. This means that the collector may set the
1041     // state to DefinitelyGrey and then to PossiblyOldOrBlack at any time. It&#39;s OK for us to
1042     // race with the collector here. If we win then this is accurate because the object _will_
1043     // get scanned again. If we lose then someone else will barrier the object again. That would
1044     // be unfortunate but not the end of the world.
1045     cell-&gt;setCellState(CellState::PossiblyGrey);
1046     m_mutatorMarkStack-&gt;append(cell);
1047 }
1048 
1049 void Heap::sweepSynchronously()
1050 {
1051     MonotonicTime before { };
1052     if (Options::logGC()) {
1053         dataLog(&quot;Full sweep: &quot;, capacity() / 1024, &quot;kb &quot;);
1054         before = MonotonicTime::now();
1055     }
1056     m_objectSpace.sweep();
1057     m_objectSpace.shrink();
1058     if (Options::logGC()) {
1059         MonotonicTime after = MonotonicTime::now();
1060         dataLog(&quot;=&gt; &quot;, capacity() / 1024, &quot;kb, &quot;, (after - before).milliseconds(), &quot;ms&quot;);
1061     }
1062 }
1063 
1064 void Heap::collect(Synchronousness synchronousness, GCRequest request)
1065 {
1066     switch (synchronousness) {
1067     case Async:
1068         collectAsync(request);
1069         return;
1070     case Sync:
1071         collectSync(request);
1072         return;
1073     }
1074     RELEASE_ASSERT_NOT_REACHED();
1075 }
1076 
1077 void Heap::collectNow(Synchronousness synchronousness, GCRequest request)
1078 {
1079     if (validateDFGDoesGC)
1080         RELEASE_ASSERT(expectDoesGC());
1081 
1082     switch (synchronousness) {
1083     case Async: {
1084         collectAsync(request);
1085         stopIfNecessary();
1086         return;
1087     }
1088 
1089     case Sync: {
1090         collectSync(request);
1091 
1092         DeferGCForAWhile deferGC(*this);
1093         if (UNLIKELY(Options::useImmortalObjects()))
1094             sweeper().stopSweeping();
1095 
1096         bool alreadySweptInCollectSync = shouldSweepSynchronously();
1097         if (!alreadySweptInCollectSync) {
1098             if (Options::logGC())
1099                 dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: &quot;);
1100             sweepSynchronously();
1101             if (Options::logGC())
1102                 dataLog(&quot;]\n&quot;);
1103         }
1104         m_objectSpace.assertNoUnswept();
1105 
1106         sweepAllLogicallyEmptyWeakBlocks();
1107         return;
1108     } }
1109     RELEASE_ASSERT_NOT_REACHED();
1110 }
1111 
1112 void Heap::collectAsync(GCRequest request)
1113 {
1114     if (validateDFGDoesGC)
1115         RELEASE_ASSERT(expectDoesGC());
1116 
1117     if (!m_isSafeToCollect)
1118         return;
1119 
1120     bool alreadyRequested = false;
1121     {
1122         LockHolder locker(*m_threadLock);
1123         for (const GCRequest&amp; previousRequest : m_requests) {
1124             if (request.subsumedBy(previousRequest)) {
1125                 alreadyRequested = true;
1126                 break;
1127             }
1128         }
1129     }
1130     if (alreadyRequested)
1131         return;
1132 
1133     requestCollection(request);
1134 }
1135 
1136 void Heap::collectSync(GCRequest request)
1137 {
1138     if (validateDFGDoesGC)
1139         RELEASE_ASSERT(expectDoesGC());
1140 
1141     if (!m_isSafeToCollect)
1142         return;
1143 
1144     waitForCollection(requestCollection(request));
1145 }
1146 
1147 bool Heap::shouldCollectInCollectorThread(const AbstractLocker&amp;)
1148 {
1149     RELEASE_ASSERT(m_requests.isEmpty() == (m_lastServedTicket == m_lastGrantedTicket));
1150     RELEASE_ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
1151 
1152     if (false)
1153         dataLog(&quot;Mutator has the conn = &quot;, !!(m_worldState.load() &amp; mutatorHasConnBit), &quot;\n&quot;);
1154 
1155     return !m_requests.isEmpty() &amp;&amp; !(m_worldState.load() &amp; mutatorHasConnBit);
1156 }
1157 
1158 void Heap::collectInCollectorThread()
1159 {
1160     for (;;) {
1161         RunCurrentPhaseResult result = runCurrentPhase(GCConductor::Collector, nullptr);
1162         switch (result) {
1163         case RunCurrentPhaseResult::Finished:
1164             return;
1165         case RunCurrentPhaseResult::Continue:
1166             break;
1167         case RunCurrentPhaseResult::NeedCurrentThreadState:
1168             RELEASE_ASSERT_NOT_REACHED();
1169             break;
1170         }
1171     }
1172 }
1173 
1174 ALWAYS_INLINE int asInt(CollectorPhase phase)
1175 {
1176     return static_cast&lt;int&gt;(phase);
1177 }
1178 
1179 void Heap::checkConn(GCConductor conn)
1180 {
1181     unsigned worldState = m_worldState.load();
1182     switch (conn) {
1183     case GCConductor::Mutator:
<a name="62" id="anc62"></a><span class="line-modified">1184         RELEASE_ASSERT(worldState &amp; mutatorHasConnBit, worldState, asInt(m_lastPhase), asInt(m_currentPhase), asInt(m_nextPhase), vm().id(), VM::numberOfIDs(), vm().isEntered());</span>
1185         return;
1186     case GCConductor::Collector:
<a name="63" id="anc63"></a><span class="line-modified">1187         RELEASE_ASSERT(!(worldState &amp; mutatorHasConnBit), worldState, asInt(m_lastPhase), asInt(m_currentPhase), asInt(m_nextPhase), vm().id(), VM::numberOfIDs(), vm().isEntered());</span>
1188         return;
1189     }
1190     RELEASE_ASSERT_NOT_REACHED();
1191 }
1192 
1193 auto Heap::runCurrentPhase(GCConductor conn, CurrentThreadState* currentThreadState) -&gt; RunCurrentPhaseResult
1194 {
1195     checkConn(conn);
1196     m_currentThreadState = currentThreadState;
<a name="64" id="anc64"></a><span class="line-modified">1197     m_currentThread = &amp;Thread::current();</span>
1198 
1199     if (conn == GCConductor::Mutator)
1200         sanitizeStackForVM(vm());
1201 
1202     // If the collector transfers the conn to the mutator, it leaves us in between phases.
1203     if (!finishChangingPhase(conn)) {
1204         // A mischevious mutator could repeatedly relinquish the conn back to us. We try to avoid doing
1205         // this, but it&#39;s probably not the end of the world if it did happen.
1206         if (false)
1207             dataLog(&quot;Conn bounce-back.\n&quot;);
1208         return RunCurrentPhaseResult::Finished;
1209     }
1210 
1211     bool result = false;
1212     switch (m_currentPhase) {
1213     case CollectorPhase::NotRunning:
1214         result = runNotRunningPhase(conn);
1215         break;
1216 
1217     case CollectorPhase::Begin:
1218         result = runBeginPhase(conn);
1219         break;
1220 
1221     case CollectorPhase::Fixpoint:
1222         if (!currentThreadState &amp;&amp; conn == GCConductor::Mutator)
1223             return RunCurrentPhaseResult::NeedCurrentThreadState;
1224 
1225         result = runFixpointPhase(conn);
1226         break;
1227 
1228     case CollectorPhase::Concurrent:
1229         result = runConcurrentPhase(conn);
1230         break;
1231 
1232     case CollectorPhase::Reloop:
1233         result = runReloopPhase(conn);
1234         break;
1235 
1236     case CollectorPhase::End:
1237         result = runEndPhase(conn);
1238         break;
1239     }
1240 
1241     return result ? RunCurrentPhaseResult::Continue : RunCurrentPhaseResult::Finished;
1242 }
1243 
1244 NEVER_INLINE bool Heap::runNotRunningPhase(GCConductor conn)
1245 {
1246     // Check m_requests since the mutator calls this to poll what&#39;s going on.
1247     {
1248         auto locker = holdLock(*m_threadLock);
1249         if (m_requests.isEmpty())
1250             return false;
1251     }
1252 
1253     return changePhase(conn, CollectorPhase::Begin);
1254 }
1255 
1256 NEVER_INLINE bool Heap::runBeginPhase(GCConductor conn)
1257 {
1258     m_currentGCStartTime = MonotonicTime::now();
1259 
1260     {
1261         LockHolder locker(*m_threadLock);
1262         RELEASE_ASSERT(!m_requests.isEmpty());
1263         m_currentRequest = m_requests.first();
1264     }
1265 
1266     if (Options::logGC())
1267         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: START &quot;, gcConductorShortName(conn), &quot; &quot;, capacity() / 1024, &quot;kb &quot;);
1268 
1269     m_beforeGC = MonotonicTime::now();
1270 
1271     if (m_collectionScope) {
1272         dataLog(&quot;Collection scope already set during GC: &quot;, *m_collectionScope, &quot;\n&quot;);
1273         RELEASE_ASSERT_NOT_REACHED();
1274     }
1275 
1276     willStartCollection();
1277 
1278     if (UNLIKELY(m_verifier)) {
1279         // Verify that live objects from the last GC cycle haven&#39;t been corrupted by
1280         // mutators before we begin this new GC cycle.
1281         m_verifier-&gt;verify(HeapVerifier::Phase::BeforeGC);
1282 
1283         m_verifier-&gt;startGC();
1284         m_verifier-&gt;gatherLiveCells(HeapVerifier::Phase::BeforeMarking);
1285     }
1286 
1287     prepareForMarking();
1288 
1289     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
1290         m_opaqueRoots.clear();
1291         m_collectorSlotVisitor-&gt;clearMarkStacks();
1292         m_mutatorMarkStack-&gt;clear();
1293     }
1294 
1295     RELEASE_ASSERT(m_raceMarkStack-&gt;isEmpty());
1296 
1297     beginMarking();
1298 
1299     forEachSlotVisitor(
1300         [&amp;] (SlotVisitor&amp; visitor) {
1301             visitor.didStartMarking();
1302         });
1303 
1304     m_parallelMarkersShouldExit = false;
1305 
1306     m_helperClient.setFunction(
1307         [this] () {
1308             SlotVisitor* slotVisitor;
1309             {
1310                 LockHolder locker(m_parallelSlotVisitorLock);
<a name="65" id="anc65"></a><span class="line-modified">1311                 RELEASE_ASSERT_WITH_MESSAGE(!m_availableParallelSlotVisitors.isEmpty(), &quot;Parallel SlotVisitors are allocated apriori&quot;);</span>











1312                 slotVisitor = m_availableParallelSlotVisitors.takeLast();
1313             }
1314 
<a name="66" id="anc66"></a><span class="line-modified">1315             Thread::registerGCThread(GCThreadType::Helper);</span>
1316 
1317             {
1318                 ParallelModeEnabler parallelModeEnabler(*slotVisitor);
1319                 slotVisitor-&gt;drainFromShared(SlotVisitor::SlaveDrain);
1320             }
1321 
1322             {
1323                 LockHolder locker(m_parallelSlotVisitorLock);
1324                 m_availableParallelSlotVisitors.append(slotVisitor);
1325             }
1326         });
1327 
1328     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1329 
1330     m_constraintSet-&gt;didStartMarking();
1331 
1332     m_scheduler-&gt;beginCollection();
1333     if (Options::logGC())
1334         m_scheduler-&gt;log();
1335 
1336     // After this, we will almost certainly fall through all of the &quot;slotVisitor.isEmpty()&quot;
1337     // checks because bootstrap would have put things into the visitor. So, we should fall
1338     // through to draining.
1339 
1340     if (!slotVisitor.didReachTermination()) {
1341         dataLog(&quot;Fatal: SlotVisitor should think that GC should terminate before constraint solving, but it does not think this.\n&quot;);
1342         dataLog(&quot;slotVisitor.isEmpty(): &quot;, slotVisitor.isEmpty(), &quot;\n&quot;);
1343         dataLog(&quot;slotVisitor.collectorMarkStack().isEmpty(): &quot;, slotVisitor.collectorMarkStack().isEmpty(), &quot;\n&quot;);
1344         dataLog(&quot;slotVisitor.mutatorMarkStack().isEmpty(): &quot;, slotVisitor.mutatorMarkStack().isEmpty(), &quot;\n&quot;);
1345         dataLog(&quot;m_numberOfActiveParallelMarkers: &quot;, m_numberOfActiveParallelMarkers, &quot;\n&quot;);
1346         dataLog(&quot;m_sharedCollectorMarkStack-&gt;isEmpty(): &quot;, m_sharedCollectorMarkStack-&gt;isEmpty(), &quot;\n&quot;);
1347         dataLog(&quot;m_sharedMutatorMarkStack-&gt;isEmpty(): &quot;, m_sharedMutatorMarkStack-&gt;isEmpty(), &quot;\n&quot;);
1348         dataLog(&quot;slotVisitor.didReachTermination(): &quot;, slotVisitor.didReachTermination(), &quot;\n&quot;);
1349         RELEASE_ASSERT_NOT_REACHED();
1350     }
1351 
1352     return changePhase(conn, CollectorPhase::Fixpoint);
1353 }
1354 
1355 NEVER_INLINE bool Heap::runFixpointPhase(GCConductor conn)
1356 {
1357     RELEASE_ASSERT(conn == GCConductor::Collector || m_currentThreadState);
1358 
1359     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1360 
1361     if (Options::logGC()) {
1362         HashMap&lt;const char*, size_t&gt; visitMap;
1363         forEachSlotVisitor(
1364             [&amp;] (SlotVisitor&amp; slotVisitor) {
1365                 visitMap.add(slotVisitor.codeName(), slotVisitor.bytesVisited() / 1024);
1366             });
1367 
1368         auto perVisitorDump = sortedMapDump(
1369             visitMap,
1370             [] (const char* a, const char* b) -&gt; bool {
1371                 return strcmp(a, b) &lt; 0;
1372             },
1373             &quot;:&quot;, &quot; &quot;);
1374 
1375         dataLog(&quot;v=&quot;, bytesVisited() / 1024, &quot;kb (&quot;, perVisitorDump, &quot;) o=&quot;, m_opaqueRoots.size(), &quot; b=&quot;, m_barriersExecuted, &quot; &quot;);
1376     }
1377 
1378     if (slotVisitor.didReachTermination()) {
1379         m_opaqueRoots.deleteOldTables();
1380 
1381         m_scheduler-&gt;didReachTermination();
1382 
1383         assertMarkStacksEmpty();
1384 
1385         // FIXME: Take m_mutatorDidRun into account when scheduling constraints. Most likely,
1386         // we don&#39;t have to execute root constraints again unless the mutator did run. At a
1387         // minimum, we could use this for work estimates - but it&#39;s probably more than just an
1388         // estimate.
1389         // https://bugs.webkit.org/show_bug.cgi?id=166828
1390 
1391         // Wondering what this does? Look at Heap::addCoreConstraints(). The DOM and others can also
1392         // add their own using Heap::addMarkingConstraint().
1393         bool converged = m_constraintSet-&gt;executeConvergence(slotVisitor);
1394 
1395         // FIXME: The slotVisitor.isEmpty() check is most likely not needed.
1396         // https://bugs.webkit.org/show_bug.cgi?id=180310
1397         if (converged &amp;&amp; slotVisitor.isEmpty()) {
1398             assertMarkStacksEmpty();
1399             return changePhase(conn, CollectorPhase::End);
1400         }
1401 
1402         m_scheduler-&gt;didExecuteConstraints();
1403     }
1404 
1405     if (Options::logGC())
1406         dataLog(slotVisitor.collectorMarkStack().size(), &quot;+&quot;, m_mutatorMarkStack-&gt;size() + slotVisitor.mutatorMarkStack().size(), &quot; &quot;);
1407 
1408     {
1409         ParallelModeEnabler enabler(slotVisitor);
1410         slotVisitor.drainInParallel(m_scheduler-&gt;timeToResume());
1411     }
1412 
1413     m_scheduler-&gt;synchronousDrainingDidStall();
1414 
1415     // This is kinda tricky. The termination check looks at:
1416     //
1417     // - Whether the marking threads are active. If they are not, this means that the marking threads&#39;
1418     //   SlotVisitors are empty.
1419     // - Whether the collector&#39;s slot visitor is empty.
1420     // - Whether the shared mark stacks are empty.
1421     //
1422     // This doesn&#39;t have to check the mutator SlotVisitor because that one becomes empty after every GC
1423     // work increment, so it must be empty now.
1424     if (slotVisitor.didReachTermination())
1425         return true; // This is like relooping to the top if runFixpointPhase().
1426 
1427     if (!m_scheduler-&gt;shouldResume())
1428         return true;
1429 
1430     m_scheduler-&gt;willResume();
1431 
1432     if (Options::logGC()) {
1433         double thisPauseMS = (MonotonicTime::now() - m_stopTime).milliseconds();
1434         dataLog(&quot;p=&quot;, thisPauseMS, &quot;ms (max &quot;, maxPauseMS(thisPauseMS), &quot;)...]\n&quot;);
1435     }
1436 
1437     // Forgive the mutator for its past failures to keep up.
1438     // FIXME: Figure out if moving this to different places results in perf changes.
1439     m_incrementBalance = 0;
1440 
1441     return changePhase(conn, CollectorPhase::Concurrent);
1442 }
1443 
1444 NEVER_INLINE bool Heap::runConcurrentPhase(GCConductor conn)
1445 {
1446     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1447 
1448     switch (conn) {
1449     case GCConductor::Mutator: {
1450         // When the mutator has the conn, we poll runConcurrentPhase() on every time someone says
1451         // stopIfNecessary(), so on every allocation slow path. When that happens we poll if it&#39;s time
1452         // to stop and do some work.
1453         if (slotVisitor.didReachTermination()
1454             || m_scheduler-&gt;shouldStop())
1455             return changePhase(conn, CollectorPhase::Reloop);
1456 
1457         // We could be coming from a collector phase that stuffed our SlotVisitor, so make sure we donate
1458         // everything. This is super cheap if the SlotVisitor is already empty.
1459         slotVisitor.donateAll();
1460         return false;
1461     }
1462     case GCConductor::Collector: {
1463         {
1464             ParallelModeEnabler enabler(slotVisitor);
1465             slotVisitor.drainInParallelPassively(m_scheduler-&gt;timeToStop());
1466         }
1467         return changePhase(conn, CollectorPhase::Reloop);
1468     } }
1469 
1470     RELEASE_ASSERT_NOT_REACHED();
1471     return false;
1472 }
1473 
1474 NEVER_INLINE bool Heap::runReloopPhase(GCConductor conn)
1475 {
1476     if (Options::logGC())
1477         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: &quot;, gcConductorShortName(conn), &quot; &quot;);
1478 
1479     m_scheduler-&gt;didStop();
1480 
1481     if (Options::logGC())
1482         m_scheduler-&gt;log();
1483 
1484     return changePhase(conn, CollectorPhase::Fixpoint);
1485 }
1486 
1487 NEVER_INLINE bool Heap::runEndPhase(GCConductor conn)
1488 {
1489     m_scheduler-&gt;endCollection();
1490 
1491     {
1492         auto locker = holdLock(m_markingMutex);
1493         m_parallelMarkersShouldExit = true;
1494         m_markingConditionVariable.notifyAll();
1495     }
1496     m_helperClient.finish();
1497 
1498     iterateExecutingAndCompilingCodeBlocks(
1499         [&amp;] (CodeBlock* codeBlock) {
1500             writeBarrier(codeBlock);
1501         });
1502 
1503     updateObjectCounts();
1504     endMarking();
1505 
1506     if (UNLIKELY(m_verifier)) {
1507         m_verifier-&gt;gatherLiveCells(HeapVerifier::Phase::AfterMarking);
1508         m_verifier-&gt;verify(HeapVerifier::Phase::AfterMarking);
1509     }
1510 
<a name="67" id="anc67"></a><span class="line-modified">1511     if (vm().typeProfiler())</span>
<span class="line-modified">1512         vm().typeProfiler()-&gt;invalidateTypeSetCache(vm());</span>



1513 
1514     reapWeakHandles();
1515     pruneStaleEntriesFromWeakGCMaps();
1516     sweepArrayBuffers();
1517     snapshotUnswept();
1518     finalizeUnconditionalFinalizers();
1519     removeDeadCompilerWorklistEntries();
1520     notifyIncrementalSweeper();
1521 
1522     m_codeBlocks-&gt;iterateCurrentlyExecuting(
1523         [&amp;] (CodeBlock* codeBlock) {
1524             writeBarrier(codeBlock);
1525         });
1526     m_codeBlocks-&gt;clearCurrentlyExecuting();
1527 
1528     m_objectSpace.prepareForAllocation();
1529     updateAllocationLimits();
1530 
1531     if (UNLIKELY(m_verifier)) {
1532         m_verifier-&gt;trimDeadCells();
1533         m_verifier-&gt;verify(HeapVerifier::Phase::AfterGC);
1534     }
1535 
1536     didFinishCollection();
1537 
1538     if (m_currentRequest.didFinishEndPhase)
1539         m_currentRequest.didFinishEndPhase-&gt;run();
1540 
1541     if (false) {
1542         dataLog(&quot;Heap state after GC:\n&quot;);
1543         m_objectSpace.dumpBits();
1544     }
1545 
1546     if (Options::logGC()) {
1547         double thisPauseMS = (m_afterGC - m_stopTime).milliseconds();
1548         dataLog(&quot;p=&quot;, thisPauseMS, &quot;ms (max &quot;, maxPauseMS(thisPauseMS), &quot;), cycle &quot;, (m_afterGC - m_beforeGC).milliseconds(), &quot;ms END]\n&quot;);
1549     }
1550 
1551     {
1552         auto locker = holdLock(*m_threadLock);
1553         m_requests.removeFirst();
1554         m_lastServedTicket++;
1555         clearMutatorWaiting();
1556     }
1557     ParkingLot::unparkAll(&amp;m_worldState);
1558 
1559     if (false)
1560         dataLog(&quot;GC END!\n&quot;);
1561 
1562     setNeedFinalize();
1563 
1564     m_lastGCStartTime = m_currentGCStartTime;
1565     m_lastGCEndTime = MonotonicTime::now();
1566     m_totalGCTime += m_lastGCEndTime - m_lastGCStartTime;
1567 
1568     return changePhase(conn, CollectorPhase::NotRunning);
1569 }
1570 
1571 bool Heap::changePhase(GCConductor conn, CollectorPhase nextPhase)
1572 {
1573     checkConn(conn);
1574 
1575     m_lastPhase = m_currentPhase;
1576     m_nextPhase = nextPhase;
1577 
1578     return finishChangingPhase(conn);
1579 }
1580 
1581 NEVER_INLINE bool Heap::finishChangingPhase(GCConductor conn)
1582 {
1583     checkConn(conn);
1584 
1585     if (m_nextPhase == m_currentPhase)
1586         return true;
1587 
1588     if (false)
1589         dataLog(conn, &quot;: Going to phase: &quot;, m_nextPhase, &quot; (from &quot;, m_currentPhase, &quot;)\n&quot;);
1590 
1591     m_phaseVersion++;
1592 
1593     bool suspendedBefore = worldShouldBeSuspended(m_currentPhase);
1594     bool suspendedAfter = worldShouldBeSuspended(m_nextPhase);
1595 
1596     if (suspendedBefore != suspendedAfter) {
1597         if (suspendedBefore) {
1598             RELEASE_ASSERT(!suspendedAfter);
1599 
1600             resumeThePeriphery();
1601             if (conn == GCConductor::Collector)
1602                 resumeTheMutator();
1603             else
1604                 handleNeedFinalize();
1605         } else {
1606             RELEASE_ASSERT(!suspendedBefore);
1607             RELEASE_ASSERT(suspendedAfter);
1608 
1609             if (conn == GCConductor::Collector) {
1610                 waitWhileNeedFinalize();
1611                 if (!stopTheMutator()) {
1612                     if (false)
1613                         dataLog(&quot;Returning false.\n&quot;);
1614                     return false;
1615                 }
1616             } else {
1617                 sanitizeStackForVM(m_vm);
1618                 handleNeedFinalize();
1619             }
1620             stopThePeriphery(conn);
1621         }
1622     }
1623 
1624     m_currentPhase = m_nextPhase;
1625     return true;
1626 }
1627 
1628 void Heap::stopThePeriphery(GCConductor conn)
1629 {
1630     if (m_worldIsStopped) {
1631         dataLog(&quot;FATAL: world already stopped.\n&quot;);
1632         RELEASE_ASSERT_NOT_REACHED();
1633     }
1634 
1635     if (m_mutatorDidRun)
1636         m_mutatorExecutionVersion++;
1637 
1638     m_mutatorDidRun = false;
1639 
1640     suspendCompilerThreads();
1641     m_worldIsStopped = true;
1642 
1643     forEachSlotVisitor(
1644         [&amp;] (SlotVisitor&amp; slotVisitor) {
1645             slotVisitor.updateMutatorIsStopped(NoLockingNecessary);
1646         });
1647 
1648 #if ENABLE(JIT)
1649     if (VM::canUseJIT()) {
1650         DeferGCForAWhile awhile(*this);
<a name="68" id="anc68"></a><span class="line-modified">1651         if (JITWorklist::ensureGlobalWorklist().completeAllForVM(m_vm)</span>
1652             &amp;&amp; conn == GCConductor::Collector)
1653             setGCDidJIT();
1654     }
1655 #endif // ENABLE(JIT)
1656     UNUSED_PARAM(conn);
1657 
<a name="69" id="anc69"></a><span class="line-modified">1658     if (auto* shadowChicken = vm().shadowChicken())</span>
<span class="line-modified">1659         shadowChicken-&gt;update(vm(), vm().topCallFrame);</span>
1660 
1661     m_structureIDTable.flushOldTables();
1662     m_objectSpace.stopAllocating();
1663 
1664     m_stopTime = MonotonicTime::now();
1665 }
1666 
1667 NEVER_INLINE void Heap::resumeThePeriphery()
1668 {
1669     // Calling resumeAllocating does the Right Thing depending on whether this is the end of a
1670     // collection cycle or this is just a concurrent phase within a collection cycle:
1671     // - At end of collection cycle: it&#39;s a no-op because prepareForAllocation already cleared the
1672     //   last active block.
1673     // - During collection cycle: it reinstates the last active block.
1674     m_objectSpace.resumeAllocating();
1675 
1676     m_barriersExecuted = 0;
1677 
1678     if (!m_worldIsStopped) {
1679         dataLog(&quot;Fatal: collector does not believe that the world is stopped.\n&quot;);
1680         RELEASE_ASSERT_NOT_REACHED();
1681     }
1682     m_worldIsStopped = false;
1683 
1684     // FIXME: This could be vastly improved: we want to grab the locks in the order in which they
1685     // become available. We basically want a lockAny() method that will lock whatever lock is available
1686     // and tell you which one it locked. That would require teaching ParkingLot how to park on multiple
1687     // queues at once, which is totally achievable - it would just require memory allocation, which is
1688     // suboptimal but not a disaster. Alternatively, we could replace the SlotVisitor rightToRun lock
1689     // with a DLG-style handshake mechanism, but that seems not as general.
1690     Vector&lt;SlotVisitor*, 8&gt; slotVisitorsToUpdate;
1691 
1692     forEachSlotVisitor(
1693         [&amp;] (SlotVisitor&amp; slotVisitor) {
1694             slotVisitorsToUpdate.append(&amp;slotVisitor);
1695         });
1696 
1697     for (unsigned countdown = 40; !slotVisitorsToUpdate.isEmpty() &amp;&amp; countdown--;) {
1698         for (unsigned index = 0; index &lt; slotVisitorsToUpdate.size(); ++index) {
1699             SlotVisitor&amp; slotVisitor = *slotVisitorsToUpdate[index];
1700             bool remove = false;
1701             if (slotVisitor.hasAcknowledgedThatTheMutatorIsResumed())
1702                 remove = true;
1703             else if (auto locker = tryHoldLock(slotVisitor.rightToRun())) {
1704                 slotVisitor.updateMutatorIsStopped(locker);
1705                 remove = true;
1706             }
1707             if (remove) {
1708                 slotVisitorsToUpdate[index--] = slotVisitorsToUpdate.last();
1709                 slotVisitorsToUpdate.takeLast();
1710             }
1711         }
<a name="70" id="anc70"></a><span class="line-modified">1712         Thread::yield();</span>
1713     }
1714 
1715     for (SlotVisitor* slotVisitor : slotVisitorsToUpdate)
1716         slotVisitor-&gt;updateMutatorIsStopped();
1717 
1718     resumeCompilerThreads();
1719 }
1720 
1721 bool Heap::stopTheMutator()
1722 {
1723     for (;;) {
1724         unsigned oldState = m_worldState.load();
1725         if (oldState &amp; stoppedBit) {
1726             RELEASE_ASSERT(!(oldState &amp; hasAccessBit));
1727             RELEASE_ASSERT(!(oldState &amp; mutatorWaitingBit));
1728             RELEASE_ASSERT(!(oldState &amp; mutatorHasConnBit));
1729             return true;
1730         }
1731 
1732         if (oldState &amp; mutatorHasConnBit) {
1733             RELEASE_ASSERT(!(oldState &amp; hasAccessBit));
1734             RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1735             return false;
1736         }
1737 
1738         if (!(oldState &amp; hasAccessBit)) {
1739             RELEASE_ASSERT(!(oldState &amp; mutatorHasConnBit));
1740             RELEASE_ASSERT(!(oldState &amp; mutatorWaitingBit));
1741             // We can stop the world instantly.
1742             if (m_worldState.compareExchangeWeak(oldState, oldState | stoppedBit))
1743                 return true;
1744             continue;
1745         }
1746 
1747         // Transfer the conn to the mutator and bail.
1748         RELEASE_ASSERT(oldState &amp; hasAccessBit);
1749         RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1750         unsigned newState = (oldState | mutatorHasConnBit) &amp; ~mutatorWaitingBit;
1751         if (m_worldState.compareExchangeWeak(oldState, newState)) {
1752             if (false)
1753                 dataLog(&quot;Handed off the conn.\n&quot;);
1754             m_stopIfNecessaryTimer-&gt;scheduleSoon();
1755             ParkingLot::unparkAll(&amp;m_worldState);
1756             return false;
1757         }
1758     }
1759 }
1760 
1761 NEVER_INLINE void Heap::resumeTheMutator()
1762 {
1763     if (false)
1764         dataLog(&quot;Resuming the mutator.\n&quot;);
1765     for (;;) {
1766         unsigned oldState = m_worldState.load();
1767         if (!!(oldState &amp; hasAccessBit) != !(oldState &amp; stoppedBit)) {
1768             dataLog(&quot;Fatal: hasAccess = &quot;, !!(oldState &amp; hasAccessBit), &quot;, stopped = &quot;, !!(oldState &amp; stoppedBit), &quot;\n&quot;);
1769             RELEASE_ASSERT_NOT_REACHED();
1770         }
1771         if (oldState &amp; mutatorHasConnBit) {
1772             dataLog(&quot;Fatal: mutator has the conn.\n&quot;);
1773             RELEASE_ASSERT_NOT_REACHED();
1774         }
1775 
1776         if (!(oldState &amp; stoppedBit)) {
1777             if (false)
1778                 dataLog(&quot;Returning because not stopped.\n&quot;);
1779             return;
1780         }
1781 
1782         if (m_worldState.compareExchangeWeak(oldState, oldState &amp; ~stoppedBit)) {
1783             if (false)
1784                 dataLog(&quot;CASing and returning.\n&quot;);
1785             ParkingLot::unparkAll(&amp;m_worldState);
1786             return;
1787         }
1788     }
1789 }
1790 
1791 void Heap::stopIfNecessarySlow()
1792 {
1793     if (validateDFGDoesGC)
1794         RELEASE_ASSERT(expectDoesGC());
1795 
1796     while (stopIfNecessarySlow(m_worldState.load())) { }
1797 
1798     RELEASE_ASSERT(m_worldState.load() &amp; hasAccessBit);
1799     RELEASE_ASSERT(!(m_worldState.load() &amp; stoppedBit));
1800 
1801     handleGCDidJIT();
1802     handleNeedFinalize();
1803     m_mutatorDidRun = true;
1804 }
1805 
1806 bool Heap::stopIfNecessarySlow(unsigned oldState)
1807 {
1808     if (validateDFGDoesGC)
1809         RELEASE_ASSERT(expectDoesGC());
1810 
1811     RELEASE_ASSERT(oldState &amp; hasAccessBit);
1812     RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1813 
1814     // It&#39;s possible for us to wake up with finalization already requested but the world not yet
1815     // resumed. If that happens, we can&#39;t run finalization yet.
1816     if (handleNeedFinalize(oldState))
1817         return true;
1818 
1819     // FIXME: When entering the concurrent phase, we could arrange for this branch not to fire, and then
1820     // have the SlotVisitor do things to the m_worldState to make this branch fire again. That would
1821     // prevent us from polling this so much. Ideally, stopIfNecessary would ignore the mutatorHasConnBit
1822     // and there would be some other bit indicating whether we were in some GC phase other than the
1823     // NotRunning or Concurrent ones.
1824     if (oldState &amp; mutatorHasConnBit)
1825         collectInMutatorThread();
1826 
1827     return false;
1828 }
1829 
1830 NEVER_INLINE void Heap::collectInMutatorThread()
1831 {
1832     CollectingScope collectingScope(*this);
1833     for (;;) {
1834         RunCurrentPhaseResult result = runCurrentPhase(GCConductor::Mutator, nullptr);
1835         switch (result) {
1836         case RunCurrentPhaseResult::Finished:
1837             return;
1838         case RunCurrentPhaseResult::Continue:
1839             break;
1840         case RunCurrentPhaseResult::NeedCurrentThreadState:
1841             sanitizeStackForVM(m_vm);
1842             auto lambda = [&amp;] (CurrentThreadState&amp; state) {
1843                 for (;;) {
1844                     RunCurrentPhaseResult result = runCurrentPhase(GCConductor::Mutator, &amp;state);
1845                     switch (result) {
1846                     case RunCurrentPhaseResult::Finished:
1847                         return;
1848                     case RunCurrentPhaseResult::Continue:
1849                         break;
1850                     case RunCurrentPhaseResult::NeedCurrentThreadState:
1851                         RELEASE_ASSERT_NOT_REACHED();
1852                         break;
1853                     }
1854                 }
1855             };
1856             callWithCurrentThreadState(scopedLambda&lt;void(CurrentThreadState&amp;)&gt;(WTFMove(lambda)));
1857             return;
1858         }
1859     }
1860 }
1861 
1862 template&lt;typename Func&gt;
1863 void Heap::waitForCollector(const Func&amp; func)
1864 {
1865     for (;;) {
1866         bool done;
1867         {
1868             LockHolder locker(*m_threadLock);
1869             done = func(locker);
1870             if (!done) {
1871                 setMutatorWaiting();
1872 
1873                 // At this point, the collector knows that we intend to wait, and he will clear the
1874                 // waiting bit and then unparkAll when the GC cycle finishes. Clearing the bit
1875                 // prevents us from parking except if there is also stop-the-world. Unparking after
1876                 // clearing means that if the clearing happens after we park, then we will unpark.
1877             }
1878         }
1879 
1880         // If we&#39;re in a stop-the-world scenario, we need to wait for that even if done is true.
1881         unsigned oldState = m_worldState.load();
1882         if (stopIfNecessarySlow(oldState))
1883             continue;
1884 
1885         // FIXME: We wouldn&#39;t need this if stopIfNecessarySlow() had a mode where it knew to just
1886         // do the collection.
1887         relinquishConn();
1888 
1889         if (done) {
1890             clearMutatorWaiting(); // Clean up just in case.
1891             return;
1892         }
1893 
1894         // If mutatorWaitingBit is still set then we want to wait.
1895         ParkingLot::compareAndPark(&amp;m_worldState, oldState | mutatorWaitingBit);
1896     }
1897 }
1898 
1899 void Heap::acquireAccessSlow()
1900 {
1901     for (;;) {
1902         unsigned oldState = m_worldState.load();
1903         RELEASE_ASSERT(!(oldState &amp; hasAccessBit));
1904 
1905         if (oldState &amp; stoppedBit) {
1906             if (verboseStop) {
1907                 dataLog(&quot;Stopping in acquireAccess!\n&quot;);
1908                 WTFReportBacktrace();
1909             }
1910             // Wait until we&#39;re not stopped anymore.
1911             ParkingLot::compareAndPark(&amp;m_worldState, oldState);
1912             continue;
1913         }
1914 
1915         RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1916         unsigned newState = oldState | hasAccessBit;
1917         if (m_worldState.compareExchangeWeak(oldState, newState)) {
1918             handleGCDidJIT();
1919             handleNeedFinalize();
1920             m_mutatorDidRun = true;
1921             stopIfNecessary();
1922             return;
1923         }
1924     }
1925 }
1926 
1927 void Heap::releaseAccessSlow()
1928 {
1929     for (;;) {
1930         unsigned oldState = m_worldState.load();
1931         if (!(oldState &amp; hasAccessBit)) {
1932             dataLog(&quot;FATAL: Attempting to release access but the mutator does not have access.\n&quot;);
1933             RELEASE_ASSERT_NOT_REACHED();
1934         }
1935         if (oldState &amp; stoppedBit) {
1936             dataLog(&quot;FATAL: Attempting to release access but the mutator is stopped.\n&quot;);
1937             RELEASE_ASSERT_NOT_REACHED();
1938         }
1939 
1940         if (handleNeedFinalize(oldState))
1941             continue;
1942 
1943         unsigned newState = oldState &amp; ~(hasAccessBit | mutatorHasConnBit);
1944 
1945         if ((oldState &amp; mutatorHasConnBit)
1946             &amp;&amp; m_nextPhase != m_currentPhase) {
1947             // This means that the collector thread had given us the conn so that we would do something
1948             // for it. Stop ourselves as we release access. This ensures that acquireAccess blocks. In
1949             // the meantime, since we&#39;re handing the conn over, the collector will be awoken and it is
1950             // sure to have work to do.
1951             newState |= stoppedBit;
1952         }
1953 
1954         if (m_worldState.compareExchangeWeak(oldState, newState)) {
1955             if (oldState &amp; mutatorHasConnBit)
1956                 finishRelinquishingConn();
1957             return;
1958         }
1959     }
1960 }
1961 
1962 bool Heap::relinquishConn(unsigned oldState)
1963 {
1964     RELEASE_ASSERT(oldState &amp; hasAccessBit);
1965     RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1966 
1967     if (!(oldState &amp; mutatorHasConnBit))
1968         return false; // Done.
1969 
1970     if (m_threadShouldStop)
1971         return false;
1972 
1973     if (!m_worldState.compareExchangeWeak(oldState, oldState &amp; ~mutatorHasConnBit))
1974         return true; // Loop around.
1975 
1976     finishRelinquishingConn();
1977     return true;
1978 }
1979 
1980 void Heap::finishRelinquishingConn()
1981 {
1982     if (false)
1983         dataLog(&quot;Relinquished the conn.\n&quot;);
1984 
1985     sanitizeStackForVM(m_vm);
1986 
1987     auto locker = holdLock(*m_threadLock);
1988     if (!m_requests.isEmpty())
1989         m_threadCondition-&gt;notifyOne(locker);
1990     ParkingLot::unparkAll(&amp;m_worldState);
1991 }
1992 
1993 void Heap::relinquishConn()
1994 {
1995     while (relinquishConn(m_worldState.load())) { }
1996 }
1997 
1998 bool Heap::handleGCDidJIT(unsigned oldState)
1999 {
2000     RELEASE_ASSERT(oldState &amp; hasAccessBit);
2001     if (!(oldState &amp; gcDidJITBit))
2002         return false;
2003     if (m_worldState.compareExchangeWeak(oldState, oldState &amp; ~gcDidJITBit)) {
2004         WTF::crossModifyingCodeFence();
2005         return true;
2006     }
2007     return true;
2008 }
2009 
2010 NEVER_INLINE bool Heap::handleNeedFinalize(unsigned oldState)
2011 {
2012     RELEASE_ASSERT(oldState &amp; hasAccessBit);
2013     RELEASE_ASSERT(!(oldState &amp; stoppedBit));
2014 
2015     if (!(oldState &amp; needFinalizeBit))
2016         return false;
2017     if (m_worldState.compareExchangeWeak(oldState, oldState &amp; ~needFinalizeBit)) {
2018         finalize();
2019         // Wake up anyone waiting for us to finalize. Note that they may have woken up already, in
2020         // which case they would be waiting for us to release heap access.
2021         ParkingLot::unparkAll(&amp;m_worldState);
2022         return true;
2023     }
2024     return true;
2025 }
2026 
2027 void Heap::handleGCDidJIT()
2028 {
2029     while (handleGCDidJIT(m_worldState.load())) { }
2030 }
2031 
2032 void Heap::handleNeedFinalize()
2033 {
2034     while (handleNeedFinalize(m_worldState.load())) { }
2035 }
2036 
2037 void Heap::setGCDidJIT()
2038 {
2039     m_worldState.transaction(
2040         [&amp;] (unsigned&amp; state) -&gt; bool {
2041             RELEASE_ASSERT(state &amp; stoppedBit);
2042             state |= gcDidJITBit;
2043             return true;
2044         });
2045 }
2046 
2047 void Heap::setNeedFinalize()
2048 {
2049     m_worldState.exchangeOr(needFinalizeBit);
2050     ParkingLot::unparkAll(&amp;m_worldState);
2051     m_stopIfNecessaryTimer-&gt;scheduleSoon();
2052 }
2053 
2054 void Heap::waitWhileNeedFinalize()
2055 {
2056     for (;;) {
2057         unsigned oldState = m_worldState.load();
2058         if (!(oldState &amp; needFinalizeBit)) {
2059             // This means that either there was no finalize request or the main thread will finalize
2060             // with heap access, so a subsequent call to stopTheWorld() will return only when
2061             // finalize finishes.
2062             return;
2063         }
2064         ParkingLot::compareAndPark(&amp;m_worldState, oldState);
2065     }
2066 }
2067 
2068 void Heap::setMutatorWaiting()
2069 {
2070     m_worldState.exchangeOr(mutatorWaitingBit);
2071 }
2072 
2073 void Heap::clearMutatorWaiting()
2074 {
2075     m_worldState.exchangeAnd(~mutatorWaitingBit);
2076 }
2077 
2078 void Heap::notifyThreadStopping(const AbstractLocker&amp;)
2079 {
2080     m_threadIsStopping = true;
2081     clearMutatorWaiting();
2082     ParkingLot::unparkAll(&amp;m_worldState);
2083 }
2084 
2085 void Heap::finalize()
2086 {
2087     MonotonicTime before;
2088     if (Options::logGC()) {
2089         before = MonotonicTime::now();
2090         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: finalize &quot;);
2091     }
2092 
2093     {
2094         SweepingScope sweepingScope(*this);
2095         deleteUnmarkedCompiledCode();
2096         deleteSourceProviderCaches();
2097         sweepInFinalize();
2098     }
2099 
<a name="71" id="anc71"></a><span class="line-modified">2100     if (HasOwnPropertyCache* cache = vm().hasOwnPropertyCache())</span>
2101         cache-&gt;clear();
2102 
2103     immutableButterflyToStringCache.clear();
2104 
2105     for (const HeapFinalizerCallback&amp; callback : m_heapFinalizerCallbacks)
<a name="72" id="anc72"></a><span class="line-modified">2106         callback.run(vm());</span>
2107 
2108     if (shouldSweepSynchronously())
2109         sweepSynchronously();
2110 
2111     if (Options::logGC()) {
2112         MonotonicTime after = MonotonicTime::now();
2113         dataLog((after - before).milliseconds(), &quot;ms]\n&quot;);
2114     }
2115 }
2116 
2117 Heap::Ticket Heap::requestCollection(GCRequest request)
2118 {
2119     stopIfNecessary();
2120 
<a name="73" id="anc73"></a><span class="line-modified">2121     ASSERT(vm().currentThreadIsHoldingAPILock());</span>
<span class="line-modified">2122     RELEASE_ASSERT(vm().atomStringTable() == Thread::current().atomStringTable());</span>
2123 
2124     LockHolder locker(*m_threadLock);
2125     // We may be able to steal the conn. That only works if the collector is definitely not running
2126     // right now. This is an optimization that prevents the collector thread from ever starting in most
2127     // cases.
2128     ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
<a name="74" id="anc74"></a><span class="line-modified">2129     if ((m_lastServedTicket == m_lastGrantedTicket) &amp;&amp; !m_collectorThreadIsRunning) {</span>
2130         if (false)
2131             dataLog(&quot;Taking the conn.\n&quot;);
2132         m_worldState.exchangeOr(mutatorHasConnBit);
2133     }
2134 
2135     m_requests.append(request);
2136     m_lastGrantedTicket++;
2137     if (!(m_worldState.load() &amp; mutatorHasConnBit))
2138         m_threadCondition-&gt;notifyOne(locker);
2139     return m_lastGrantedTicket;
2140 }
2141 
2142 void Heap::waitForCollection(Ticket ticket)
2143 {
2144     waitForCollector(
2145         [&amp;] (const AbstractLocker&amp;) -&gt; bool {
2146             return m_lastServedTicket &gt;= ticket;
2147         });
2148 }
2149 
2150 void Heap::sweepInFinalize()
2151 {
2152     m_objectSpace.sweepLargeAllocations();
<a name="75" id="anc75"></a><span class="line-modified">2153     vm().eagerlySweptDestructibleObjectSpace.sweep();</span>
2154 }
2155 
2156 void Heap::suspendCompilerThreads()
2157 {
2158 #if ENABLE(DFG_JIT)
2159     // We ensure the worklists so that it&#39;s not possible for the mutator to start a new worklist
2160     // after we have suspended the ones that he had started before. That&#39;s not very expensive since
2161     // the worklists use AutomaticThreads anyway.
2162     if (!VM::canUseJIT())
2163         return;
2164     for (unsigned i = DFG::numberOfWorklists(); i--;)
2165         DFG::ensureWorklistForIndex(i).suspendAllThreads();
2166 #endif
2167 }
2168 
2169 void Heap::willStartCollection()
2170 {
2171     if (Options::logGC())
2172         dataLog(&quot;=&gt; &quot;);
2173 
2174     if (shouldDoFullCollection()) {
2175         m_collectionScope = CollectionScope::Full;
2176         m_shouldDoFullCollection = false;
2177         if (Options::logGC())
2178             dataLog(&quot;FullCollection, &quot;);
2179         if (false)
2180             dataLog(&quot;Full collection!\n&quot;);
2181     } else {
2182         m_collectionScope = CollectionScope::Eden;
2183         if (Options::logGC())
2184             dataLog(&quot;EdenCollection, &quot;);
2185         if (false)
2186             dataLog(&quot;Eden collection!\n&quot;);
2187     }
2188     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2189         m_sizeBeforeLastFullCollect = m_sizeAfterLastCollect + m_bytesAllocatedThisCycle;
2190         m_extraMemorySize = 0;
2191         m_deprecatedExtraMemorySize = 0;
2192 #if ENABLE(RESOURCE_USAGE)
2193         m_externalMemorySize = 0;
2194 #endif
2195 
2196         if (m_fullActivityCallback)
2197             m_fullActivityCallback-&gt;willCollect();
2198     } else {
2199         ASSERT(m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Eden);
2200         m_sizeBeforeLastEdenCollect = m_sizeAfterLastCollect + m_bytesAllocatedThisCycle;
2201     }
2202 
2203     if (m_edenActivityCallback)
2204         m_edenActivityCallback-&gt;willCollect();
2205 
2206     for (auto* observer : m_observers)
2207         observer-&gt;willGarbageCollect();
2208 }
2209 
2210 void Heap::prepareForMarking()
2211 {
2212     m_objectSpace.prepareForMarking();
2213 }
2214 
2215 void Heap::reapWeakHandles()
2216 {
2217     m_objectSpace.reapWeakSets();
2218 }
2219 
2220 void Heap::pruneStaleEntriesFromWeakGCMaps()
2221 {
2222     if (!m_collectionScope || m_collectionScope.value() != CollectionScope::Full)
2223         return;
2224     for (WeakGCMapBase* weakGCMap : m_weakGCMaps)
2225         weakGCMap-&gt;pruneStaleEntries();
2226 }
2227 
2228 void Heap::sweepArrayBuffers()
2229 {
<a name="76" id="anc76"></a><span class="line-modified">2230     m_arrayBuffers.sweep(vm());</span>
2231 }
2232 
2233 void Heap::snapshotUnswept()
2234 {
2235     TimingScope timingScope(*this, &quot;Heap::snapshotUnswept&quot;);
2236     m_objectSpace.snapshotUnswept();
2237 }
2238 
2239 void Heap::deleteSourceProviderCaches()
2240 {
2241     if (m_lastCollectionScope &amp;&amp; m_lastCollectionScope.value() == CollectionScope::Full)
<a name="77" id="anc77"></a><span class="line-modified">2242         m_vm.clearSourceProviderCaches();</span>
2243 }
2244 
2245 void Heap::notifyIncrementalSweeper()
2246 {
2247     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2248         if (!m_logicallyEmptyWeakBlocks.isEmpty())
2249             m_indexOfNextLogicallyEmptyWeakBlockToSweep = 0;
2250     }
2251 
2252     m_sweeper-&gt;startSweeping(*this);
2253 }
2254 
2255 void Heap::updateAllocationLimits()
2256 {
2257     static const bool verbose = false;
2258 
2259     if (verbose) {
2260         dataLog(&quot;\n&quot;);
2261         dataLog(&quot;bytesAllocatedThisCycle = &quot;, m_bytesAllocatedThisCycle, &quot;\n&quot;);
2262     }
2263 
2264     // Calculate our current heap size threshold for the purpose of figuring out when we should
2265     // run another collection. This isn&#39;t the same as either size() or capacity(), though it should
2266     // be somewhere between the two. The key is to match the size calculations involved calls to
2267     // didAllocate(), while never dangerously underestimating capacity(). In extreme cases of
2268     // fragmentation, we may have size() much smaller than capacity().
2269     size_t currentHeapSize = 0;
2270 
2271     // For marked space, we use the total number of bytes visited. This matches the logic for
2272     // BlockDirectory&#39;s calls to didAllocate(), which effectively accounts for the total size of
2273     // objects allocated rather than blocks used. This will underestimate capacity(), and in case
2274     // of fragmentation, this may be substantial. Fortunately, marked space rarely fragments because
2275     // cells usually have a narrow range of sizes. So, the underestimation is probably OK.
2276     currentHeapSize += m_totalBytesVisited;
2277     if (verbose)
2278         dataLog(&quot;totalBytesVisited = &quot;, m_totalBytesVisited, &quot;, currentHeapSize = &quot;, currentHeapSize, &quot;\n&quot;);
2279 
2280     // It&#39;s up to the user to ensure that extraMemorySize() ends up corresponding to allocation-time
2281     // extra memory reporting.
2282     currentHeapSize += extraMemorySize();
2283     if (!ASSERT_DISABLED) {
2284         Checked&lt;size_t, RecordOverflow&gt; checkedCurrentHeapSize = m_totalBytesVisited;
2285         checkedCurrentHeapSize += extraMemorySize();
2286         ASSERT(!checkedCurrentHeapSize.hasOverflowed() &amp;&amp; checkedCurrentHeapSize.unsafeGet() == currentHeapSize);
2287     }
2288 
2289     if (verbose)
2290         dataLog(&quot;extraMemorySize() = &quot;, extraMemorySize(), &quot;, currentHeapSize = &quot;, currentHeapSize, &quot;\n&quot;);
2291 
2292     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2293         // To avoid pathological GC churn in very small and very large heaps, we set
2294         // the new allocation limit based on the current size of the heap, with a
2295         // fixed minimum.
2296         m_maxHeapSize = std::max(minHeapSize(m_heapType, m_ramSize), proportionalHeapSize(currentHeapSize, m_ramSize));
2297         if (verbose)
2298             dataLog(&quot;Full: maxHeapSize = &quot;, m_maxHeapSize, &quot;\n&quot;);
2299         m_maxEdenSize = m_maxHeapSize - currentHeapSize;
2300         if (verbose)
2301             dataLog(&quot;Full: maxEdenSize = &quot;, m_maxEdenSize, &quot;\n&quot;);
2302         m_sizeAfterLastFullCollect = currentHeapSize;
2303         if (verbose)
2304             dataLog(&quot;Full: sizeAfterLastFullCollect = &quot;, currentHeapSize, &quot;\n&quot;);
2305         m_bytesAbandonedSinceLastFullCollect = 0;
2306         if (verbose)
2307             dataLog(&quot;Full: bytesAbandonedSinceLastFullCollect = &quot;, 0, &quot;\n&quot;);
2308     } else {
2309         ASSERT(currentHeapSize &gt;= m_sizeAfterLastCollect);
2310         // Theoretically, we shouldn&#39;t ever scan more memory than the heap size we planned to have.
2311         // But we are sloppy, so we have to defend against the overflow.
2312         m_maxEdenSize = currentHeapSize &gt; m_maxHeapSize ? 0 : m_maxHeapSize - currentHeapSize;
2313         if (verbose)
2314             dataLog(&quot;Eden: maxEdenSize = &quot;, m_maxEdenSize, &quot;\n&quot;);
2315         m_sizeAfterLastEdenCollect = currentHeapSize;
2316         if (verbose)
2317             dataLog(&quot;Eden: sizeAfterLastEdenCollect = &quot;, currentHeapSize, &quot;\n&quot;);
2318         double edenToOldGenerationRatio = (double)m_maxEdenSize / (double)m_maxHeapSize;
2319         double minEdenToOldGenerationRatio = 1.0 / 3.0;
2320         if (edenToOldGenerationRatio &lt; minEdenToOldGenerationRatio)
2321             m_shouldDoFullCollection = true;
2322         // This seems suspect at first, but what it does is ensure that the nursery size is fixed.
2323         m_maxHeapSize += currentHeapSize - m_sizeAfterLastCollect;
2324         if (verbose)
2325             dataLog(&quot;Eden: maxHeapSize = &quot;, m_maxHeapSize, &quot;\n&quot;);
2326         m_maxEdenSize = m_maxHeapSize - currentHeapSize;
2327         if (verbose)
2328             dataLog(&quot;Eden: maxEdenSize = &quot;, m_maxEdenSize, &quot;\n&quot;);
2329         if (m_fullActivityCallback) {
2330             ASSERT(currentHeapSize &gt;= m_sizeAfterLastFullCollect);
2331             m_fullActivityCallback-&gt;didAllocate(*this, currentHeapSize - m_sizeAfterLastFullCollect);
2332         }
2333     }
2334 
2335 #if PLATFORM(IOS_FAMILY)
2336     // Get critical memory threshold for next cycle.
2337     overCriticalMemoryThreshold(MemoryThresholdCallType::Direct);
2338 #endif
2339 
2340     m_sizeAfterLastCollect = currentHeapSize;
2341     if (verbose)
2342         dataLog(&quot;sizeAfterLastCollect = &quot;, m_sizeAfterLastCollect, &quot;\n&quot;);
2343     m_bytesAllocatedThisCycle = 0;
2344 
2345     if (Options::logGC())
2346         dataLog(&quot;=&gt; &quot;, currentHeapSize / 1024, &quot;kb, &quot;);
2347 }
2348 
2349 void Heap::didFinishCollection()
2350 {
2351     m_afterGC = MonotonicTime::now();
2352     CollectionScope scope = *m_collectionScope;
2353     if (scope == CollectionScope::Full)
2354         m_lastFullGCLength = m_afterGC - m_beforeGC;
2355     else
2356         m_lastEdenGCLength = m_afterGC - m_beforeGC;
2357 
2358 #if ENABLE(RESOURCE_USAGE)
2359     ASSERT(externalMemorySize() &lt;= extraMemorySize());
2360 #endif
2361 
<a name="78" id="anc78"></a><span class="line-modified">2362     if (HeapProfiler* heapProfiler = m_vm.heapProfiler()) {</span>
<span class="line-modified">2363         gatherExtraHeapData(*heapProfiler);</span>
2364         removeDeadHeapSnapshotNodes(*heapProfiler);
2365     }
2366 
2367     if (UNLIKELY(m_verifier))
2368         m_verifier-&gt;endGC();
2369 
2370     RELEASE_ASSERT(m_collectionScope);
2371     m_lastCollectionScope = m_collectionScope;
2372     m_collectionScope = WTF::nullopt;
2373 
2374     for (auto* observer : m_observers)
2375         observer-&gt;didGarbageCollect(scope);
2376 }
2377 
2378 void Heap::resumeCompilerThreads()
2379 {
2380 #if ENABLE(DFG_JIT)
2381     if (!VM::canUseJIT())
2382         return;
2383     for (unsigned i = DFG::numberOfWorklists(); i--;)
2384         DFG::existingWorklistForIndex(i).resumeAllThreads();
2385 #endif
2386 }
2387 
2388 GCActivityCallback* Heap::fullActivityCallback()
2389 {
2390     return m_fullActivityCallback.get();
2391 }
2392 
2393 GCActivityCallback* Heap::edenActivityCallback()
2394 {
2395     return m_edenActivityCallback.get();
2396 }
2397 
2398 IncrementalSweeper&amp; Heap::sweeper()
2399 {
2400     return m_sweeper.get();
2401 }
2402 
2403 void Heap::setGarbageCollectionTimerEnabled(bool enable)
2404 {
2405     if (m_fullActivityCallback)
2406         m_fullActivityCallback-&gt;setEnabled(enable);
2407     if (m_edenActivityCallback)
2408         m_edenActivityCallback-&gt;setEnabled(enable);
2409 }
2410 
2411 void Heap::didAllocate(size_t bytes)
2412 {
2413     if (m_edenActivityCallback)
2414         m_edenActivityCallback-&gt;didAllocate(*this, m_bytesAllocatedThisCycle + m_bytesAbandonedSinceLastFullCollect);
2415     m_bytesAllocatedThisCycle += bytes;
2416     performIncrement(bytes);
2417 }
2418 
2419 bool Heap::isValidAllocation(size_t)
2420 {
2421     if (!isValidThreadState(m_vm))
2422         return false;
2423 
2424     if (isCurrentThreadBusy())
2425         return false;
2426 
2427     return true;
2428 }
2429 
2430 void Heap::addFinalizer(JSCell* cell, Finalizer finalizer)
2431 {
2432     WeakSet::allocate(cell, &amp;m_finalizerOwner, reinterpret_cast&lt;void*&gt;(finalizer)); // Balanced by FinalizerOwner::finalize().
2433 }
2434 
2435 void Heap::FinalizerOwner::finalize(Handle&lt;Unknown&gt; handle, void* context)
2436 {
2437     HandleSlot slot = handle.slot();
2438     Finalizer finalizer = reinterpret_cast&lt;Finalizer&gt;(context);
2439     finalizer(slot-&gt;asCell());
2440     WeakSet::deallocate(WeakImpl::asWeakImpl(slot));
2441 }
2442 
2443 void Heap::collectNowFullIfNotDoneRecently(Synchronousness synchronousness)
2444 {
2445     if (!m_fullActivityCallback) {
2446         collectNow(synchronousness, CollectionScope::Full);
2447         return;
2448     }
2449 
2450     if (m_fullActivityCallback-&gt;didGCRecently()) {
2451         // A synchronous GC was already requested recently so we merely accelerate next collection.
2452         reportAbandonedObjectGraph();
2453         return;
2454     }
2455 
2456     m_fullActivityCallback-&gt;setDidGCRecently();
2457     collectNow(synchronousness, CollectionScope::Full);
2458 }
2459 
2460 bool Heap::useGenerationalGC()
2461 {
2462     return Options::useGenerationalGC() &amp;&amp; !VM::isInMiniMode();
2463 }
2464 
2465 bool Heap::shouldSweepSynchronously()
2466 {
2467     return Options::sweepSynchronously() || VM::isInMiniMode();
2468 }
2469 
2470 bool Heap::shouldDoFullCollection()
2471 {
2472     if (!useGenerationalGC())
2473         return true;
2474 
2475     if (!m_currentRequest.scope)
2476         return m_shouldDoFullCollection || overCriticalMemoryThreshold();
2477     return *m_currentRequest.scope == CollectionScope::Full;
2478 }
2479 
2480 void Heap::addLogicallyEmptyWeakBlock(WeakBlock* block)
2481 {
2482     m_logicallyEmptyWeakBlocks.append(block);
2483 }
2484 
2485 void Heap::sweepAllLogicallyEmptyWeakBlocks()
2486 {
2487     if (m_logicallyEmptyWeakBlocks.isEmpty())
2488         return;
2489 
2490     m_indexOfNextLogicallyEmptyWeakBlockToSweep = 0;
2491     while (sweepNextLogicallyEmptyWeakBlock()) { }
2492 }
2493 
2494 bool Heap::sweepNextLogicallyEmptyWeakBlock()
2495 {
2496     if (m_indexOfNextLogicallyEmptyWeakBlockToSweep == WTF::notFound)
2497         return false;
2498 
2499     WeakBlock* block = m_logicallyEmptyWeakBlocks[m_indexOfNextLogicallyEmptyWeakBlockToSweep];
2500 
2501     block-&gt;sweep();
2502     if (block-&gt;isEmpty()) {
2503         std::swap(m_logicallyEmptyWeakBlocks[m_indexOfNextLogicallyEmptyWeakBlockToSweep], m_logicallyEmptyWeakBlocks.last());
2504         m_logicallyEmptyWeakBlocks.removeLast();
2505         WeakBlock::destroy(*this, block);
2506     } else
2507         m_indexOfNextLogicallyEmptyWeakBlockToSweep++;
2508 
2509     if (m_indexOfNextLogicallyEmptyWeakBlockToSweep &gt;= m_logicallyEmptyWeakBlocks.size()) {
2510         m_indexOfNextLogicallyEmptyWeakBlockToSweep = WTF::notFound;
2511         return false;
2512     }
2513 
2514     return true;
2515 }
2516 
2517 size_t Heap::visitCount()
2518 {
2519     size_t result = 0;
2520     forEachSlotVisitor(
2521         [&amp;] (SlotVisitor&amp; visitor) {
2522             result += visitor.visitCount();
2523         });
2524     return result;
2525 }
2526 
2527 size_t Heap::bytesVisited()
2528 {
2529     size_t result = 0;
2530     forEachSlotVisitor(
2531         [&amp;] (SlotVisitor&amp; visitor) {
2532             result += visitor.bytesVisited();
2533         });
2534     return result;
2535 }
2536 
2537 void Heap::forEachCodeBlockImpl(const ScopedLambda&lt;void(CodeBlock*)&gt;&amp; func)
2538 {
2539     // We don&#39;t know the full set of CodeBlocks until compilation has terminated.
2540     completeAllJITPlans();
2541 
2542     return m_codeBlocks-&gt;iterate(func);
2543 }
2544 
2545 void Heap::forEachCodeBlockIgnoringJITPlansImpl(const AbstractLocker&amp; locker, const ScopedLambda&lt;void(CodeBlock*)&gt;&amp; func)
2546 {
2547     return m_codeBlocks-&gt;iterate(locker, func);
2548 }
2549 
2550 void Heap::writeBarrierSlowPath(const JSCell* from)
2551 {
2552     if (UNLIKELY(mutatorShouldBeFenced())) {
2553         // In this case, the barrierThreshold is the tautological threshold, so from could still be
2554         // not black. But we can&#39;t know for sure until we fire off a fence.
2555         WTF::storeLoadFence();
2556         if (from-&gt;cellState() != CellState::PossiblyBlack)
2557             return;
2558     }
2559 
2560     addToRememberedSet(from);
2561 }
2562 
2563 bool Heap::isCurrentThreadBusy()
2564 {
<a name="79" id="anc79"></a><span class="line-modified">2565     return Thread::mayBeGCThread() || mutatorState() != MutatorState::Running;</span>
2566 }
2567 
2568 void Heap::reportExtraMemoryVisited(size_t size)
2569 {
2570     size_t* counter = &amp;m_extraMemorySize;
2571 
2572     for (;;) {
2573         size_t oldSize = *counter;
2574         // FIXME: Change this to use SaturatedArithmetic when available.
2575         // https://bugs.webkit.org/show_bug.cgi?id=170411
2576         Checked&lt;size_t, RecordOverflow&gt; checkedNewSize = oldSize;
2577         checkedNewSize += size;
2578         size_t newSize = UNLIKELY(checkedNewSize.hasOverflowed()) ? std::numeric_limits&lt;size_t&gt;::max() : checkedNewSize.unsafeGet();
2579         if (WTF::atomicCompareExchangeWeakRelaxed(counter, oldSize, newSize))
2580             return;
2581     }
2582 }
2583 
2584 #if ENABLE(RESOURCE_USAGE)
2585 void Heap::reportExternalMemoryVisited(size_t size)
2586 {
2587     size_t* counter = &amp;m_externalMemorySize;
2588 
2589     for (;;) {
2590         size_t oldSize = *counter;
2591         if (WTF::atomicCompareExchangeWeakRelaxed(counter, oldSize, oldSize + size))
2592             return;
2593     }
2594 }
2595 #endif
2596 
2597 void Heap::collectIfNecessaryOrDefer(GCDeferralContext* deferralContext)
2598 {
2599     ASSERT(deferralContext || isDeferred() || !DisallowGC::isInEffectOnCurrentThread());
2600     if (validateDFGDoesGC)
2601         RELEASE_ASSERT(expectDoesGC());
2602 
2603     if (!m_isSafeToCollect)
2604         return;
2605 
2606     switch (mutatorState()) {
2607     case MutatorState::Running:
2608     case MutatorState::Allocating:
2609         break;
2610     case MutatorState::Sweeping:
2611     case MutatorState::Collecting:
2612         return;
2613     }
2614     if (!Options::useGC())
2615         return;
2616 
2617     if (mayNeedToStop()) {
2618         if (deferralContext)
2619             deferralContext-&gt;m_shouldGC = true;
2620         else if (isDeferred())
2621             m_didDeferGCWork = true;
2622         else
2623             stopIfNecessary();
2624     }
2625 
2626     if (UNLIKELY(Options::gcMaxHeapSize())) {
2627         if (m_bytesAllocatedThisCycle &lt;= Options::gcMaxHeapSize())
2628             return;
2629     } else {
2630         size_t bytesAllowedThisCycle = m_maxEdenSize;
2631 
2632 #if PLATFORM(IOS_FAMILY)
2633         if (overCriticalMemoryThreshold())
2634             bytesAllowedThisCycle = std::min(m_maxEdenSizeWhenCritical, bytesAllowedThisCycle);
2635 #endif
2636 
2637         if (m_bytesAllocatedThisCycle &lt;= bytesAllowedThisCycle)
2638             return;
2639     }
2640 
2641     if (deferralContext)
2642         deferralContext-&gt;m_shouldGC = true;
2643     else if (isDeferred())
2644         m_didDeferGCWork = true;
2645     else {
2646         collectAsync();
2647         stopIfNecessary(); // This will immediately start the collection if we have the conn.
2648     }
2649 }
2650 
2651 void Heap::decrementDeferralDepthAndGCIfNeededSlow()
2652 {
2653     // Can&#39;t do anything if we&#39;re still deferred.
2654     if (m_deferralDepth)
2655         return;
2656 
2657     ASSERT(!isDeferred());
2658 
2659     m_didDeferGCWork = false;
2660     // FIXME: Bring back something like the DeferGCProbability mode.
2661     // https://bugs.webkit.org/show_bug.cgi?id=166627
2662     collectIfNecessaryOrDefer();
2663 }
2664 
2665 void Heap::registerWeakGCMap(WeakGCMapBase* weakGCMap)
2666 {
2667     m_weakGCMaps.add(weakGCMap);
2668 }
2669 
2670 void Heap::unregisterWeakGCMap(WeakGCMapBase* weakGCMap)
2671 {
2672     m_weakGCMaps.remove(weakGCMap);
2673 }
2674 
2675 void Heap::didAllocateBlock(size_t capacity)
2676 {
2677 #if ENABLE(RESOURCE_USAGE)
2678     m_blockBytesAllocated += capacity;
2679 #else
2680     UNUSED_PARAM(capacity);
2681 #endif
2682 }
2683 
2684 void Heap::didFreeBlock(size_t capacity)
2685 {
2686 #if ENABLE(RESOURCE_USAGE)
2687     m_blockBytesAllocated -= capacity;
2688 #else
2689     UNUSED_PARAM(capacity);
2690 #endif
2691 }
2692 
2693 void Heap::addCoreConstraints()
2694 {
2695     m_constraintSet-&gt;add(
2696         &quot;Cs&quot;, &quot;Conservative Scan&quot;,
2697         [this, lastVersion = static_cast&lt;uint64_t&gt;(0)] (SlotVisitor&amp; slotVisitor) mutable {
2698             bool shouldNotProduceWork = lastVersion == m_phaseVersion;
2699             if (shouldNotProduceWork)
2700                 return;
2701 
2702             TimingScope preConvergenceTimingScope(*this, &quot;Constraint: conservative scan&quot;);
2703             m_objectSpace.prepareForConservativeScan();
<a name="80" id="anc80"></a><span class="line-added">2704             m_jitStubRoutines-&gt;prepareForConservativeScan();</span>
2705 
2706             {
2707                 ConservativeRoots conservativeRoots(*this);
2708                 SuperSamplerScope superSamplerScope(false);
2709 
2710                 gatherStackRoots(conservativeRoots);
2711                 gatherJSStackRoots(conservativeRoots);
2712                 gatherScratchBufferRoots(conservativeRoots);
2713 
2714                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ConservativeScan);
2715                 slotVisitor.append(conservativeRoots);
2716             }
2717             if (VM::canUseJIT()) {
2718                 // JITStubRoutines must be visited after scanning ConservativeRoots since JITStubRoutines depend on the hook executed during gathering ConservativeRoots.
2719                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::JITStubRoutines);
2720                 m_jitStubRoutines-&gt;traceMarkedStubRoutines(slotVisitor);
2721             }
2722 
2723             lastVersion = m_phaseVersion;
2724         },
2725         ConstraintVolatility::GreyedByExecution);
2726 
2727     m_constraintSet-&gt;add(
2728         &quot;Msr&quot;, &quot;Misc Small Roots&quot;,
2729         [this] (SlotVisitor&amp; slotVisitor) {
2730 
2731 #if JSC_OBJC_API_ENABLED
<a name="81" id="anc81"></a><span class="line-modified">2732             scanExternalRememberedSet(m_vm, slotVisitor);</span>
2733 #endif
<a name="82" id="anc82"></a><span class="line-modified">2734             if (m_vm.smallStrings.needsToBeVisited(*m_collectionScope)) {</span>
2735                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::StrongReferences);
<a name="83" id="anc83"></a><span class="line-modified">2736                 m_vm.smallStrings.visitStrongReferences(slotVisitor);</span>
2737             }
2738 
2739             {
2740                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ProtectedValues);
2741                 for (auto&amp; pair : m_protectedValues)
2742                     slotVisitor.appendUnbarriered(pair.key);
2743             }
2744 
2745             if (m_markListSet &amp;&amp; m_markListSet-&gt;size()) {
2746                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ConservativeScan);
2747                 MarkedArgumentBuffer::markLists(slotVisitor, *m_markListSet);
2748             }
2749 
2750             {
2751                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::VMExceptions);
<a name="84" id="anc84"></a><span class="line-modified">2752                 slotVisitor.appendUnbarriered(m_vm.exception());</span>
<span class="line-modified">2753                 slotVisitor.appendUnbarriered(m_vm.lastException());</span>
2754             }
2755         },
2756         ConstraintVolatility::GreyedByExecution);
2757 
2758     m_constraintSet-&gt;add(
2759         &quot;Sh&quot;, &quot;Strong Handles&quot;,
2760         [this] (SlotVisitor&amp; slotVisitor) {
2761             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::StrongHandles);
2762             m_handleSet.visitStrongHandles(slotVisitor);
2763         },
2764         ConstraintVolatility::GreyedByExecution);
2765 
2766     m_constraintSet-&gt;add(
2767         &quot;D&quot;, &quot;Debugger&quot;,
2768         [this] (SlotVisitor&amp; slotVisitor) {
2769             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::Debugger);
2770 
2771 #if ENABLE(SAMPLING_PROFILER)
<a name="85" id="anc85"></a><span class="line-modified">2772             if (SamplingProfiler* samplingProfiler = m_vm.samplingProfiler()) {</span>
2773                 LockHolder locker(samplingProfiler-&gt;getLock());
2774                 samplingProfiler-&gt;processUnverifiedStackTraces();
2775                 samplingProfiler-&gt;visit(slotVisitor);
2776                 if (Options::logGC() == GCLogging::Verbose)
2777                     dataLog(&quot;Sampling Profiler data:\n&quot;, slotVisitor);
2778             }
2779 #endif // ENABLE(SAMPLING_PROFILER)
2780 
<a name="86" id="anc86"></a><span class="line-modified">2781             if (m_vm.typeProfiler())</span>
<span class="line-modified">2782                 m_vm.typeProfilerLog()-&gt;visit(slotVisitor);</span>
2783 
<a name="87" id="anc87"></a><span class="line-modified">2784             if (auto* shadowChicken = m_vm.shadowChicken())</span>
2785                 shadowChicken-&gt;visitChildren(slotVisitor);
2786         },
2787         ConstraintVolatility::GreyedByExecution);
2788 
2789     m_constraintSet-&gt;add(
2790         &quot;Ws&quot;, &quot;Weak Sets&quot;,
2791         [this] (SlotVisitor&amp; slotVisitor) {
2792             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::WeakSets);
2793             m_objectSpace.visitWeakSets(slotVisitor);
2794         },
2795         ConstraintVolatility::GreyedByMarking);
2796 
2797     m_constraintSet-&gt;add(
2798         &quot;O&quot;, &quot;Output&quot;,
2799         [] (SlotVisitor&amp; slotVisitor) {
2800             VM&amp; vm = slotVisitor.vm();
2801 
2802             auto callOutputConstraint = [] (SlotVisitor&amp; slotVisitor, HeapCell* heapCell, HeapCell::Kind) {
2803                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::Output);
2804                 VM&amp; vm = slotVisitor.vm();
2805                 JSCell* cell = static_cast&lt;JSCell*&gt;(heapCell);
2806                 cell-&gt;methodTable(vm)-&gt;visitOutputConstraints(cell, slotVisitor);
2807             };
2808 
2809             auto add = [&amp;] (auto&amp; set) {
2810                 slotVisitor.addParallelConstraintTask(set.forEachMarkedCellInParallel(callOutputConstraint));
2811             };
2812 
2813             add(vm.executableToCodeBlockEdgesWithConstraints);
2814             if (vm.m_weakMapSpace)
2815                 add(*vm.m_weakMapSpace);
2816         },
2817         ConstraintVolatility::GreyedByMarking,
2818         ConstraintParallelism::Parallel);
2819 
2820 #if ENABLE(DFG_JIT)
2821     if (VM::canUseJIT()) {
2822         m_constraintSet-&gt;add(
2823             &quot;Dw&quot;, &quot;DFG Worklists&quot;,
2824             [this] (SlotVisitor&amp; slotVisitor) {
2825                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::DFGWorkLists);
2826 
2827                 for (unsigned i = DFG::numberOfWorklists(); i--;)
2828                     DFG::existingWorklistForIndex(i).visitWeakReferences(slotVisitor);
2829 
2830                 // FIXME: This is almost certainly unnecessary.
2831                 // https://bugs.webkit.org/show_bug.cgi?id=166829
2832                 DFG::iterateCodeBlocksForGC(
<a name="88" id="anc88"></a><span class="line-modified">2833                     m_vm,</span>
2834                     [&amp;] (CodeBlock* codeBlock) {
2835                         slotVisitor.appendUnbarriered(codeBlock);
2836                     });
2837 
2838                 if (Options::logGC() == GCLogging::Verbose)
2839                     dataLog(&quot;DFG Worklists:\n&quot;, slotVisitor);
2840             },
2841             ConstraintVolatility::GreyedByMarking);
2842     }
2843 #endif
2844 
2845     m_constraintSet-&gt;add(
2846         &quot;Cb&quot;, &quot;CodeBlocks&quot;,
2847         [this] (SlotVisitor&amp; slotVisitor) {
2848             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::CodeBlocks);
2849             iterateExecutingAndCompilingCodeBlocksWithoutHoldingLocks(
2850                 [&amp;] (CodeBlock* codeBlock) {
2851                     // Visit the CodeBlock as a constraint only if it&#39;s black.
<a name="89" id="anc89"></a><span class="line-modified">2852                     if (isMarked(codeBlock)</span>
2853                         &amp;&amp; codeBlock-&gt;cellState() == CellState::PossiblyBlack)
2854                         slotVisitor.visitAsConstraint(codeBlock);
2855                 });
2856         },
2857         ConstraintVolatility::SeldomGreyed);
2858 
<a name="90" id="anc90"></a><span class="line-modified">2859     m_constraintSet-&gt;add(makeUnique&lt;MarkStackMergingConstraint&gt;(*this));</span>
2860 }
2861 
2862 void Heap::addMarkingConstraint(std::unique_ptr&lt;MarkingConstraint&gt; constraint)
2863 {
2864     PreventCollectionScope preventCollectionScope(*this);
2865     m_constraintSet-&gt;add(WTFMove(constraint));
2866 }
2867 
2868 void Heap::notifyIsSafeToCollect()
2869 {
2870     MonotonicTime before;
2871     if (Options::logGC()) {
2872         before = MonotonicTime::now();
2873         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: starting &quot;);
2874     }
2875 
2876     addCoreConstraints();
2877 
2878     m_isSafeToCollect = true;
2879 
2880     if (Options::collectContinuously()) {
<a name="91" id="anc91"></a><span class="line-modified">2881         m_collectContinuouslyThread = Thread::create(</span>
2882             &quot;JSC DEBUG Continuous GC&quot;,
2883             [this] () {
2884                 MonotonicTime initialTime = MonotonicTime::now();
2885                 Seconds period = Seconds::fromMilliseconds(Options::collectContinuouslyPeriodMS());
2886                 while (!m_shouldStopCollectingContinuously) {
2887                     {
2888                         LockHolder locker(*m_threadLock);
2889                         if (m_requests.isEmpty()) {
2890                             m_requests.append(WTF::nullopt);
2891                             m_lastGrantedTicket++;
2892                             m_threadCondition-&gt;notifyOne(locker);
2893                         }
2894                     }
2895 
2896                     {
2897                         LockHolder locker(m_collectContinuouslyLock);
2898                         Seconds elapsed = MonotonicTime::now() - initialTime;
2899                         Seconds elapsedInPeriod = elapsed % period;
2900                         MonotonicTime timeToWakeUp =
2901                             initialTime + elapsed - elapsedInPeriod + period;
2902                         while (!hasElapsed(timeToWakeUp) &amp;&amp; !m_shouldStopCollectingContinuously) {
2903                             m_collectContinuouslyCondition.waitUntil(
2904                                 m_collectContinuouslyLock, timeToWakeUp);
2905                         }
2906                     }
2907                 }
2908             });
2909     }
2910 
2911     if (Options::logGC())
2912         dataLog((MonotonicTime::now() - before).milliseconds(), &quot;ms]\n&quot;);
2913 }
2914 
2915 void Heap::preventCollection()
2916 {
2917     if (!m_isSafeToCollect)
2918         return;
2919 
2920     // This prevents the collectContinuously thread from starting a collection.
2921     m_collectContinuouslyLock.lock();
2922 
2923     // Wait for all collections to finish.
2924     waitForCollector(
2925         [&amp;] (const AbstractLocker&amp;) -&gt; bool {
2926             ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
2927             return m_lastServedTicket == m_lastGrantedTicket;
2928         });
2929 
2930     // Now a collection can only start if this thread starts it.
2931     RELEASE_ASSERT(!m_collectionScope);
2932 }
2933 
2934 void Heap::allowCollection()
2935 {
2936     if (!m_isSafeToCollect)
2937         return;
2938 
2939     m_collectContinuouslyLock.unlock();
2940 }
2941 
2942 void Heap::setMutatorShouldBeFenced(bool value)
2943 {
2944     m_mutatorShouldBeFenced = value;
2945     m_barrierThreshold = value ? tautologicalThreshold : blackThreshold;
2946 }
2947 
2948 void Heap::performIncrement(size_t bytes)
2949 {
2950     if (!m_objectSpace.isMarking())
2951         return;
2952 
2953     if (isDeferred())
2954         return;
2955 
2956     m_incrementBalance += bytes * Options::gcIncrementScale();
2957 
2958     // Save ourselves from crazy. Since this is an optimization, it&#39;s OK to go back to any consistent
2959     // state when the double goes wild.
2960     if (std::isnan(m_incrementBalance) || std::isinf(m_incrementBalance))
2961         m_incrementBalance = 0;
2962 
2963     if (m_incrementBalance &lt; static_cast&lt;double&gt;(Options::gcIncrementBytes()))
2964         return;
2965 
2966     double targetBytes = m_incrementBalance;
2967     if (targetBytes &lt;= 0)
2968         return;
2969     targetBytes = std::min(targetBytes, Options::gcIncrementMaxBytes());
2970 
2971     SlotVisitor&amp; slotVisitor = *m_mutatorSlotVisitor;
2972     ParallelModeEnabler parallelModeEnabler(slotVisitor);
2973     size_t bytesVisited = slotVisitor.performIncrementOfDraining(static_cast&lt;size_t&gt;(targetBytes));
2974     // incrementBalance may go negative here because it&#39;ll remember how many bytes we overshot.
2975     m_incrementBalance -= bytesVisited;
2976 }
2977 
2978 void Heap::addHeapFinalizerCallback(const HeapFinalizerCallback&amp; callback)
2979 {
2980     m_heapFinalizerCallbacks.append(callback);
2981 }
2982 
2983 void Heap::removeHeapFinalizerCallback(const HeapFinalizerCallback&amp; callback)
2984 {
2985     m_heapFinalizerCallbacks.removeFirst(callback);
2986 }
2987 
2988 void Heap::setBonusVisitorTask(RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt; task)
2989 {
2990     auto locker = holdLock(m_markingMutex);
2991     m_bonusVisitorTask = task;
2992     m_markingConditionVariable.notifyAll();
2993 }
2994 
2995 void Heap::runTaskInParallel(RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt; task)
2996 {
2997     unsigned initialRefCount = task-&gt;refCount();
2998     setBonusVisitorTask(task);
2999     task-&gt;run(*m_collectorSlotVisitor);
3000     setBonusVisitorTask(nullptr);
3001     // The constraint solver expects return of this function to imply termination of the task in all
3002     // threads. This ensures that property.
3003     {
3004         auto locker = holdLock(m_markingMutex);
3005         while (task-&gt;refCount() &gt; initialRefCount)
3006             m_markingConditionVariable.wait(m_markingMutex);
3007     }
3008 }
3009 
3010 } // namespace JSC
<a name="92" id="anc92"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="92" type="hidden" />
</body>
</html>