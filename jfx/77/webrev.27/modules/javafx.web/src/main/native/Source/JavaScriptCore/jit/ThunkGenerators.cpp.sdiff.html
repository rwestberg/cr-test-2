<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/ThunkGenerators.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="ThunkGenerator.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="ThunkGenerators.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/ThunkGenerators.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  36 #include &quot;MathCommon.h&quot;
  37 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;
  38 #include &quot;SpecializedThunkJIT.h&quot;
  39 #include &lt;wtf/InlineASM.h&gt;
  40 #include &lt;wtf/StringPrintStream.h&gt;
  41 #include &lt;wtf/text/StringImpl.h&gt;
  42 
  43 #if ENABLE(JIT)
  44 
  45 namespace JSC {
  46 
  47 template&lt;typename TagType&gt;
  48 inline void emitPointerValidation(CCallHelpers&amp; jit, GPRReg pointerGPR, TagType tag)
  49 {
  50     if (ASSERT_DISABLED)
  51         return;
  52     CCallHelpers::Jump isNonZero = jit.branchTestPtr(CCallHelpers::NonZero, pointerGPR);
  53     jit.abortWithReason(TGInvalidPointer);
  54     isNonZero.link(&amp;jit);
  55     jit.pushToSave(pointerGPR);
<span class="line-modified">  56     jit.untagPtr(pointerGPR, tag);</span>
  57     jit.load8(pointerGPR, pointerGPR);
  58     jit.popToRestore(pointerGPR);
  59 }
  60 
  61 // We will jump here if the JIT code tries to make a call, but the
  62 // linking helper (C++ code) decides to throw an exception instead.
<span class="line-modified">  63 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; throwExceptionFromCallSlowPathGenerator(VM* vm)</span>
  64 {
  65     CCallHelpers jit;
  66 
  67     // The call pushed a return address, so we need to pop it back off to re-align the stack,
  68     // even though we won&#39;t use it.
  69     jit.preserveReturnAddressAfterCall(GPRInfo::nonPreservedNonReturnGPR);
  70 
<span class="line-modified">  71     jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm-&gt;topEntryFrame);</span>
  72 
<span class="line-modified">  73     jit.setupArguments&lt;decltype(lookupExceptionHandler)&gt;(CCallHelpers::TrustedImmPtr(vm), GPRInfo::callFrameRegister);</span>
  74     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandler)), GPRInfo::nonArgGPR0);
  75     emitPointerValidation(jit, GPRInfo::nonArgGPR0, OperationPtrTag);
  76     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
<span class="line-modified">  77     jit.jumpToExceptionHandler(*vm);</span>
  78 
  79     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
  80     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;Throw exception from call slow path thunk&quot;);
  81 }
  82 
<span class="line-modified">  83 static void slowPathFor(CCallHelpers&amp; jit, VM* vm, Sprt_JITOperation_ECli slowPathFunction)</span>
  84 {
<span class="line-modified">  85     jit.sanitizeStackInline(*vm, GPRInfo::nonArgGPR0);</span>
  86     jit.emitFunctionPrologue();
<span class="line-modified">  87     jit.storePtr(GPRInfo::callFrameRegister, &amp;vm-&gt;topCallFrame);</span>
  88 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
  89     // Windows X86_64 needs some space pointed to by arg0 for return types larger than 64 bits.
  90     // Other argument values are shift by 1. Use space on the stack for our two return values.
  91     // Moving the stack down maxFrameExtentForSlowPathCall bytes gives us room for our 3 arguments
  92     // and space for the 16 byte return area.
  93     jit.addPtr(CCallHelpers::TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), CCallHelpers::stackPointerRegister);
  94     jit.move(GPRInfo::regT2, GPRInfo::argumentGPR2);
  95     jit.addPtr(CCallHelpers::TrustedImm32(32), CCallHelpers::stackPointerRegister, GPRInfo::argumentGPR0);
  96     jit.move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
  97     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(slowPathFunction)), GPRInfo::nonArgGPR0);
  98     emitPointerValidation(jit, GPRInfo::nonArgGPR0, OperationPtrTag);
  99     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
 100     jit.loadPtr(CCallHelpers::Address(GPRInfo::returnValueGPR, 8), GPRInfo::returnValueGPR2);
 101     jit.loadPtr(CCallHelpers::Address(GPRInfo::returnValueGPR), GPRInfo::returnValueGPR);
 102     jit.addPtr(CCallHelpers::TrustedImm32(maxFrameExtentForSlowPathCall), CCallHelpers::stackPointerRegister);
 103 #else
 104     if (maxFrameExtentForSlowPathCall)
 105         jit.addPtr(CCallHelpers::TrustedImm32(-maxFrameExtentForSlowPathCall), CCallHelpers::stackPointerRegister);
 106     jit.setupArguments&lt;decltype(slowPathFunction)&gt;(GPRInfo::regT2);
 107     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(slowPathFunction)), GPRInfo::nonArgGPR0);
</pre>
<hr />
<pre>
 111         jit.addPtr(CCallHelpers::TrustedImm32(maxFrameExtentForSlowPathCall), CCallHelpers::stackPointerRegister);
 112 #endif
 113 
 114     // This slow call will return the address of one of the following:
 115     // 1) Exception throwing thunk.
 116     // 2) Host call return value returner thingy.
 117     // 3) The function to call.
 118     // The second return value GPR will hold a non-zero value for tail calls.
 119 
 120     emitPointerValidation(jit, GPRInfo::returnValueGPR, JSEntryPtrTag);
 121     jit.emitFunctionEpilogue();
 122     jit.untagReturnAddress();
 123 
 124     RELEASE_ASSERT(reinterpret_cast&lt;void*&gt;(KeepTheFrame) == reinterpret_cast&lt;void*&gt;(0));
 125     CCallHelpers::Jump doNotTrash = jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::returnValueGPR2);
 126 
 127     jit.preserveReturnAddressAfterCall(GPRInfo::nonPreservedNonReturnGPR);
 128     jit.prepareForTailCallSlow(GPRInfo::returnValueGPR);
 129 
 130     doNotTrash.link(&amp;jit);
<span class="line-modified"> 131     jit.jump(GPRInfo::returnValueGPR, JSEntryPtrTag);</span>
 132 }
 133 
<span class="line-modified"> 134 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; linkCallThunkGenerator(VM* vm)</span>
 135 {
 136     // The return address is on the stack or in the link register. We will hence
 137     // save the return address to the call frame while we make a C++ function call
 138     // to perform linking and lazy compilation if necessary. We expect the callee
 139     // to be in regT0/regT1 (payload/tag), the CallFrame to have already
 140     // been adjusted, and all other registers to be available for use.
 141     CCallHelpers jit;
 142 
 143     slowPathFor(jit, vm, operationLinkCall);
 144 
 145     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 146     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;Link call slow path thunk&quot;);
 147 }
 148 
 149 // For closure optimizations, we only include calls, since if you&#39;re using closures for
 150 // object construction then you&#39;re going to lose big time anyway.
<span class="line-modified"> 151 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; linkPolymorphicCallThunkGenerator(VM* vm)</span>
 152 {
 153     CCallHelpers jit;
 154 
 155     slowPathFor(jit, vm, operationLinkPolymorphicCall);
 156 
 157     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 158     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;Link polymorphic call slow path thunk&quot;);
 159 }
 160 
 161 // FIXME: We should distinguish between a megamorphic virtual call vs. a slow
 162 // path virtual call so that we can enable fast tail calls for megamorphic
 163 // virtual calls by using the shuffler.
 164 // https://bugs.webkit.org/show_bug.cgi?id=148831
<span class="line-modified"> 165 MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; virtualThunkFor(VM* vm, CallLinkInfo&amp; callLinkInfo)</span>
 166 {
 167     // The callee is in regT0 (for JSVALUE32_64, the tag is in regT1).
 168     // The return address is on the stack, or in the link register. We will hence
 169     // jump to the callee, or save the return address to the call frame while we
 170     // make a C++ function call to the appropriate JIT operation.
 171 
 172     CCallHelpers jit;
 173 
 174     CCallHelpers::JumpList slowCase;
 175 
 176     // This is a slow path execution, and regT2 contains the CallLinkInfo. Count the
 177     // slow path execution for the profiler.
 178     jit.add32(
 179         CCallHelpers::TrustedImm32(1),
 180         CCallHelpers::Address(GPRInfo::regT2, CallLinkInfo::offsetOfSlowPathCount()));
 181 
 182     // FIXME: we should have a story for eliminating these checks. In many cases,
 183     // the DFG knows that the value is definitely a cell, or definitely a function.
 184 
 185 #if USE(JSVALUE64)
<span class="line-removed"> 186     GPRReg tagMaskRegister = GPRInfo::tagMaskRegister;</span>
 187     if (callLinkInfo.isTailCall()) {
 188         // Tail calls could have clobbered the GPRInfo::tagMaskRegister because they
 189         // restore callee saved registers before getthing here. So, let&#39;s materialize
 190         // the TagMask in a temp register and use the temp instead.
<span class="line-modified"> 191         tagMaskRegister = GPRInfo::regT4;</span>
<span class="line-modified"> 192         jit.move(CCallHelpers::TrustedImm64(TagMask), tagMaskRegister);</span>
<span class="line-modified"> 193     }</span>
<span class="line-removed"> 194     slowCase.append(</span>
<span class="line-removed"> 195         jit.branchTest64(CCallHelpers::NonZero, GPRInfo::regT0, tagMaskRegister));</span>
 196 #else
 197     slowCase.append(jit.branchIfNotCell(GPRInfo::regT1));
 198 #endif
<span class="line-modified"> 199     auto notJSFunction = jit.branchIfNotType(GPRInfo::regT0, JSFunctionType);</span>
 200 
 201     // Now we know we have a JSFunction.
 202 
 203     jit.loadPtr(
 204         CCallHelpers::Address(GPRInfo::regT0, JSFunction::offsetOfExecutable()),
 205         GPRInfo::regT4);
 206     jit.loadPtr(
 207         CCallHelpers::Address(
 208             GPRInfo::regT4, ExecutableBase::offsetOfJITCodeWithArityCheckFor(
 209                 callLinkInfo.specializationKind())),
 210         GPRInfo::regT4);
 211     slowCase.append(jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::regT4));
 212 
 213     // Now we know that we have a CodeBlock, and we&#39;re committed to making a fast
 214     // call.
 215 
 216     // Make a tail call. This will return back to JIT code.
 217     JSInterfaceJIT::Label callCode(jit.label());
 218     emitPointerValidation(jit, GPRInfo::regT4, JSEntryPtrTag);
 219     if (callLinkInfo.isTailCall()) {
 220         jit.preserveReturnAddressAfterCall(GPRInfo::regT0);
 221         jit.prepareForTailCallSlow(GPRInfo::regT4);
 222     }
<span class="line-modified"> 223     jit.jump(GPRInfo::regT4, JSEntryPtrTag);</span>
 224 
 225     notJSFunction.link(&amp;jit);
 226     slowCase.append(jit.branchIfNotType(GPRInfo::regT0, InternalFunctionType));
<span class="line-modified"> 227     void* executableAddress = vm-&gt;getCTIInternalFunctionTrampolineFor(callLinkInfo.specializationKind()).executableAddress();</span>
 228     jit.move(CCallHelpers::TrustedImmPtr(executableAddress), GPRInfo::regT4);
 229     jit.jump().linkTo(callCode, &amp;jit);
 230 
 231     slowCase.link(&amp;jit);
 232 
 233     // Here we don&#39;t know anything, so revert to the full slow path.
 234     slowPathFor(jit, vm, operationVirtualCall);
 235 
 236     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 237     return FINALIZE_CODE(
 238         patchBuffer, JITStubRoutinePtrTag,
 239         &quot;Virtual %s slow path thunk&quot;,
 240         callLinkInfo.callMode() == CallMode::Regular ? &quot;call&quot; : callLinkInfo.callMode() == CallMode::Tail ? &quot;tail call&quot; : &quot;construct&quot;);
 241 }
 242 
 243 enum ThunkEntryType { EnterViaCall, EnterViaJumpWithSavedTags, EnterViaJumpWithoutSavedTags };
 244 enum class ThunkFunctionType { JSFunction, InternalFunction };
 245 
<span class="line-modified"> 246 static MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeForGenerator(VM* vm, ThunkFunctionType thunkFunctionType, CodeSpecializationKind kind, ThunkEntryType entryType = EnterViaCall)</span>
 247 {
 248     // FIXME: This should be able to log ShadowChicken prologue packets.
 249     // https://bugs.webkit.org/show_bug.cgi?id=155689
 250 
 251     int executableOffsetToFunction = NativeExecutable::offsetOfNativeFunctionFor(kind);
 252 
<span class="line-modified"> 253     JSInterfaceJIT jit(vm);</span>
 254 
 255     switch (entryType) {
 256     case EnterViaCall:
 257         jit.emitFunctionPrologue();
 258         break;
 259     case EnterViaJumpWithSavedTags:
 260 #if USE(JSVALUE64)
 261         // We&#39;re coming from a specialized thunk that has saved the prior tag registers&#39; contents.
 262         // Restore them now.
 263         jit.popPair(JSInterfaceJIT::tagTypeNumberRegister, JSInterfaceJIT::tagMaskRegister);
 264 #endif
 265         break;
 266     case EnterViaJumpWithoutSavedTags:
 267         jit.move(JSInterfaceJIT::framePointerRegister, JSInterfaceJIT::stackPointerRegister);
 268         break;
 269     }
 270 
 271     jit.emitPutToCallFrameHeader(0, CallFrameSlot::codeBlock);
<span class="line-modified"> 272     jit.storePtr(JSInterfaceJIT::callFrameRegister, &amp;vm-&gt;topCallFrame);</span>
 273 
 274 #if CPU(X86)
 275     // Calling convention:      f(ecx, edx, ...);
 276     // Host function signature: f(ExecState*);
 277     jit.move(JSInterfaceJIT::callFrameRegister, X86Registers::ecx);
 278 
 279     jit.subPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::stackPointerRegister); // Align stack after prologue.
 280 
 281     // call the function
 282     jit.emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, JSInterfaceJIT::regT1);
 283     if (thunkFunctionType == ThunkFunctionType::JSFunction) {
 284         jit.loadPtr(JSInterfaceJIT::Address(JSInterfaceJIT::regT1, JSFunction::offsetOfExecutable()), JSInterfaceJIT::regT1);
 285         jit.call(JSInterfaceJIT::Address(JSInterfaceJIT::regT1, executableOffsetToFunction), JSEntryPtrTag);
 286     } else
 287         jit.call(JSInterfaceJIT::Address(JSInterfaceJIT::regT1, InternalFunction::offsetOfNativeFunctionFor(kind)), JSEntryPtrTag);
 288 
 289     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::stackPointerRegister);
 290 
 291 #elif CPU(X86_64)
 292 #if !OS(WINDOWS)
</pre>
<hr />
<pre>
 349 
 350     jit.emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, JSInterfaceJIT::argumentGPR1);
 351     if (thunkFunctionType == ThunkFunctionType::JSFunction) {
 352         jit.loadPtr(JSInterfaceJIT::Address(JSInterfaceJIT::argumentGPR1, JSFunction::offsetOfExecutable()), JSInterfaceJIT::regT2);
 353         jit.call(JSInterfaceJIT::Address(JSInterfaceJIT::regT2, executableOffsetToFunction), JSEntryPtrTag);
 354     } else
 355         jit.call(JSInterfaceJIT::Address(JSInterfaceJIT::argumentGPR1, InternalFunction::offsetOfNativeFunctionFor(kind)), JSEntryPtrTag);
 356 
 357 #if CPU(MIPS)
 358     // Restore stack space
 359     jit.addPtr(JSInterfaceJIT::TrustedImm32(16), JSInterfaceJIT::stackPointerRegister);
 360 #endif
 361 #else
 362 #error &quot;JIT not supported on this platform.&quot;
 363     UNUSED_PARAM(executableOffsetToFunction);
 364     abortWithReason(TGNotSupported);
 365 #endif
 366 
 367     // Check for an exception
 368 #if USE(JSVALUE64)
<span class="line-modified"> 369     jit.load64(vm-&gt;addressOfException(), JSInterfaceJIT::regT2);</span>
 370     JSInterfaceJIT::Jump exceptionHandler = jit.branchTest64(JSInterfaceJIT::NonZero, JSInterfaceJIT::regT2);
 371 #else
 372     JSInterfaceJIT::Jump exceptionHandler = jit.branch32(
 373         JSInterfaceJIT::NotEqual,
<span class="line-modified"> 374         JSInterfaceJIT::AbsoluteAddress(vm-&gt;addressOfException()),</span>
 375         JSInterfaceJIT::TrustedImm32(0));
 376 #endif
 377 
 378     jit.emitFunctionEpilogue();
 379     // Return.
 380     jit.ret();
 381 
 382     // Handle an exception
 383     exceptionHandler.link(&amp;jit);
 384 
<span class="line-modified"> 385     jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm-&gt;topEntryFrame);</span>
<span class="line-modified"> 386     jit.storePtr(JSInterfaceJIT::callFrameRegister, &amp;vm-&gt;topCallFrame);</span>
 387 
 388 #if CPU(X86) &amp;&amp; USE(JSVALUE32_64)
 389     jit.subPtr(JSInterfaceJIT::TrustedImm32(4), JSInterfaceJIT::stackPointerRegister);
 390     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::regT0);
 391     jit.push(JSInterfaceJIT::regT0);
 392 #else
 393 #if OS(WINDOWS)
 394     // Allocate space on stack for the 4 parameter registers.
 395     jit.subPtr(JSInterfaceJIT::TrustedImm32(4 * sizeof(int64_t)), JSInterfaceJIT::stackPointerRegister);
 396 #endif
 397     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::argumentGPR0);
 398 #endif
 399     jit.move(JSInterfaceJIT::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationVMHandleException)), JSInterfaceJIT::regT3);
 400     jit.call(JSInterfaceJIT::regT3, OperationPtrTag);
 401 #if CPU(X86) &amp;&amp; USE(JSVALUE32_64)
 402     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::stackPointerRegister);
 403 #elif OS(WINDOWS)
 404     jit.addPtr(JSInterfaceJIT::TrustedImm32(4 * sizeof(int64_t)), JSInterfaceJIT::stackPointerRegister);
 405 #endif
 406 
<span class="line-modified"> 407     jit.jumpToExceptionHandler(*vm);</span>
 408 
 409     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 410     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;%s %s%s trampoline&quot;, thunkFunctionType == ThunkFunctionType::JSFunction ? &quot;native&quot; : &quot;internal&quot;, entryType == EnterViaJumpWithSavedTags ? &quot;Tail With Saved Tags &quot; : entryType == EnterViaJumpWithoutSavedTags ? &quot;Tail Without Saved Tags &quot; : &quot;&quot;, toCString(kind).data());
 411 }
 412 
<span class="line-modified"> 413 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeCallGenerator(VM* vm)</span>
 414 {
 415     return nativeForGenerator(vm, ThunkFunctionType::JSFunction, CodeForCall);
 416 }
 417 
<span class="line-modified"> 418 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeTailCallGenerator(VM* vm)</span>
 419 {
 420     return nativeForGenerator(vm, ThunkFunctionType::JSFunction, CodeForCall, EnterViaJumpWithSavedTags);
 421 }
 422 
<span class="line-modified"> 423 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeTailCallWithoutSavedTagsGenerator(VM* vm)</span>
 424 {
 425     return nativeForGenerator(vm, ThunkFunctionType::JSFunction, CodeForCall, EnterViaJumpWithoutSavedTags);
 426 }
 427 
<span class="line-modified"> 428 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeConstructGenerator(VM* vm)</span>
 429 {
 430     return nativeForGenerator(vm, ThunkFunctionType::JSFunction, CodeForConstruct);
 431 }
 432 
<span class="line-modified"> 433 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; internalFunctionCallGenerator(VM* vm)</span>
 434 {
 435     return nativeForGenerator(vm, ThunkFunctionType::InternalFunction, CodeForCall);
 436 }
 437 
<span class="line-modified"> 438 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; internalFunctionConstructGenerator(VM* vm)</span>
 439 {
 440     return nativeForGenerator(vm, ThunkFunctionType::InternalFunction, CodeForConstruct);
 441 }
 442 
<span class="line-modified"> 443 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; arityFixupGenerator(VM* vm)</span>
 444 {
<span class="line-modified"> 445     JSInterfaceJIT jit(vm);</span>
 446 
 447     // We enter with fixup count in argumentGPR0
 448     // We have the guarantee that a0, a1, a2, t3, t4 and t5 (or t0 for Windows) are all distinct :-)
 449 #if USE(JSVALUE64)
 450 #if OS(WINDOWS)
 451     const GPRReg extraTemp = JSInterfaceJIT::regT0;
 452 #else
 453     const GPRReg extraTemp = JSInterfaceJIT::regT5;
 454 #endif
 455 #  if CPU(X86_64)
 456     jit.pop(JSInterfaceJIT::regT4);
 457 #  endif
 458     jit.tagReturnAddress();
<span class="line-modified"> 459 #if CPU(ARM64) &amp;&amp; USE(POINTER_PROFILING)</span>
 460     jit.loadPtr(JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()), GPRInfo::regT3);
 461     jit.addPtr(JSInterfaceJIT::TrustedImm32(sizeof(CallerFrameAndPC)), GPRInfo::callFrameRegister, extraTemp);
<span class="line-modified"> 462     jit.untagPtr(GPRInfo::regT3, extraTemp);</span>
 463     PtrTag tempReturnPCTag = static_cast&lt;PtrTag&gt;(random());
 464     jit.move(JSInterfaceJIT::TrustedImmPtr(tempReturnPCTag), extraTemp);
<span class="line-modified"> 465     jit.tagPtr(GPRInfo::regT3, extraTemp);</span>
 466     jit.storePtr(GPRInfo::regT3, JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()));
 467 #endif
 468     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::regT3);
 469     jit.load32(JSInterfaceJIT::addressFor(CallFrameSlot::argumentCount), JSInterfaceJIT::argumentGPR2);
 470     jit.add32(JSInterfaceJIT::TrustedImm32(CallFrame::headerSizeInRegisters), JSInterfaceJIT::argumentGPR2);
 471 
 472     // Check to see if we have extra slots we can use
 473     jit.move(JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::argumentGPR1);
 474     jit.and32(JSInterfaceJIT::TrustedImm32(stackAlignmentRegisters() - 1), JSInterfaceJIT::argumentGPR1);
 475     JSInterfaceJIT::Jump noExtraSlot = jit.branchTest32(MacroAssembler::Zero, JSInterfaceJIT::argumentGPR1);
 476     jit.move(JSInterfaceJIT::TrustedImm64(ValueUndefined), extraTemp);
 477     JSInterfaceJIT::Label fillExtraSlots(jit.label());
 478     jit.store64(extraTemp, MacroAssembler::BaseIndex(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::argumentGPR2, JSInterfaceJIT::TimesEight));
 479     jit.add32(JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2);
 480     jit.branchSub32(JSInterfaceJIT::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR1).linkTo(fillExtraSlots, &amp;jit);
 481     jit.and32(JSInterfaceJIT::TrustedImm32(-stackAlignmentRegisters()), JSInterfaceJIT::argumentGPR0);
 482     JSInterfaceJIT::Jump done = jit.branchTest32(MacroAssembler::Zero, JSInterfaceJIT::argumentGPR0);
 483     noExtraSlot.link(&amp;jit);
 484 
 485     jit.neg64(JSInterfaceJIT::argumentGPR0);
</pre>
<hr />
<pre>
 495     jit.addPtr(extraTemp, JSInterfaceJIT::stackPointerRegister);
 496     jit.tagReturnAddress();
 497 
 498     // Move current frame down argumentGPR0 number of slots
 499     JSInterfaceJIT::Label copyLoop(jit.label());
 500     jit.load64(JSInterfaceJIT::regT3, extraTemp);
 501     jit.store64(extraTemp, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight));
 502     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::regT3);
 503     jit.branchSub32(MacroAssembler::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2).linkTo(copyLoop, &amp;jit);
 504 
 505     // Fill in argumentGPR0 missing arg slots with undefined
 506     jit.move(JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::argumentGPR2);
 507     jit.move(JSInterfaceJIT::TrustedImm64(ValueUndefined), extraTemp);
 508     JSInterfaceJIT::Label fillUndefinedLoop(jit.label());
 509     jit.store64(extraTemp, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight));
 510     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::regT3);
 511     jit.branchAdd32(MacroAssembler::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2).linkTo(fillUndefinedLoop, &amp;jit);
 512 
 513     done.link(&amp;jit);
 514 
<span class="line-modified"> 515 #if CPU(ARM64) &amp;&amp; USE(POINTER_PROFILING)</span>
 516     jit.loadPtr(JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()), GPRInfo::regT3);
 517     jit.move(JSInterfaceJIT::TrustedImmPtr(tempReturnPCTag), extraTemp);
<span class="line-modified"> 518     jit.untagPtr(GPRInfo::regT3, extraTemp);</span>
 519     jit.addPtr(JSInterfaceJIT::TrustedImm32(sizeof(CallerFrameAndPC)), GPRInfo::callFrameRegister, extraTemp);
<span class="line-modified"> 520     jit.tagPtr(GPRInfo::regT3, extraTemp);</span>
 521     jit.storePtr(GPRInfo::regT3, JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()));
 522 #endif
 523 
 524 #  if CPU(X86_64)
 525     jit.push(JSInterfaceJIT::regT4);
 526 #  endif
 527     jit.ret();
 528 #else // USE(JSVALUE64) section above, USE(JSVALUE32_64) section below.
 529 #  if CPU(X86)
 530     jit.pop(JSInterfaceJIT::regT4);
 531 #  endif
 532     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::regT3);
 533     jit.load32(JSInterfaceJIT::addressFor(CallFrameSlot::argumentCount), JSInterfaceJIT::argumentGPR2);
 534     jit.add32(JSInterfaceJIT::TrustedImm32(CallFrame::headerSizeInRegisters), JSInterfaceJIT::argumentGPR2);
 535 
 536     // Check to see if we have extra slots we can use
 537     jit.move(JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::argumentGPR1);
 538     jit.and32(JSInterfaceJIT::TrustedImm32(stackAlignmentRegisters() - 1), JSInterfaceJIT::argumentGPR1);
 539     JSInterfaceJIT::Jump noExtraSlot = jit.branchTest32(MacroAssembler::Zero, JSInterfaceJIT::argumentGPR1);
 540     JSInterfaceJIT::Label fillExtraSlots(jit.label());
</pre>
<hr />
<pre>
 576     jit.move(JSInterfaceJIT::TrustedImm32(0), JSInterfaceJIT::regT5);
 577     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight, PayloadOffset));
 578     jit.move(JSInterfaceJIT::TrustedImm32(JSValue::UndefinedTag), JSInterfaceJIT::regT5);
 579     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight, TagOffset));
 580 
 581     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::regT3);
 582     jit.branchAdd32(MacroAssembler::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2).linkTo(fillUndefinedLoop, &amp;jit);
 583 
 584     done.link(&amp;jit);
 585 
 586 #  if CPU(X86)
 587     jit.push(JSInterfaceJIT::regT4);
 588 #  endif
 589     jit.ret();
 590 #endif // End of USE(JSVALUE32_64) section.
 591 
 592     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 593     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;fixup arity&quot;);
 594 }
 595 
<span class="line-modified"> 596 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; unreachableGenerator(VM* vm)</span>
 597 {
<span class="line-modified"> 598     JSInterfaceJIT jit(vm);</span>
 599 
 600     jit.breakpoint();
 601 
 602     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 603     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;unreachable thunk&quot;);
 604 }
 605 
<span class="line-modified"> 606 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; stringGetByValGenerator(VM* vm)</span>
 607 {
 608     // regT0 is JSString*, and regT1 (64bit) or regT2 (32bit) is int index.
 609     // Return regT0 = result JSString* if succeeds. Otherwise, return regT0 = 0.
 610 #if USE(JSVALUE64)
 611     GPRReg stringGPR = GPRInfo::regT0;
 612     GPRReg indexGPR = GPRInfo::regT1;
 613     GPRReg scratchGPR = GPRInfo::regT2;
 614 #else
 615     GPRReg stringGPR = GPRInfo::regT0;
 616     GPRReg indexGPR = GPRInfo::regT2;
 617     GPRReg scratchGPR = GPRInfo::regT1;
 618 #endif
 619 
<span class="line-modified"> 620     JSInterfaceJIT jit(vm);</span>
 621     JSInterfaceJIT::JumpList failures;
 622     jit.tagReturnAddress();
 623 
 624     // Load string length to regT2, and start the process of loading the data pointer into regT0
 625     jit.loadPtr(JSInterfaceJIT::Address(stringGPR, JSString::offsetOfValue()), stringGPR);
 626     failures.append(jit.branchIfRopeStringImpl(stringGPR));
 627     jit.load32(JSInterfaceJIT::Address(stringGPR, StringImpl::lengthMemoryOffset()), scratchGPR);
 628 
 629     // Do an unsigned compare to simultaneously filter negative indices as well as indices that are too large
 630     failures.append(jit.branch32(JSInterfaceJIT::AboveOrEqual, indexGPR, scratchGPR));
 631 
 632     // Load the character
 633     JSInterfaceJIT::JumpList is16Bit;
 634     JSInterfaceJIT::JumpList cont8Bit;
 635     // Load the string flags
 636     jit.load32(JSInterfaceJIT::Address(stringGPR, StringImpl::flagsOffset()), scratchGPR);
 637     jit.loadPtr(JSInterfaceJIT::Address(stringGPR, StringImpl::dataOffset()), stringGPR);
 638     is16Bit.append(jit.branchTest32(JSInterfaceJIT::Zero, scratchGPR, JSInterfaceJIT::TrustedImm32(StringImpl::flagIs8Bit())));
 639     jit.load8(JSInterfaceJIT::BaseIndex(stringGPR, indexGPR, JSInterfaceJIT::TimesOne, 0), stringGPR);
 640     cont8Bit.append(jit.jump());
 641     is16Bit.link(&amp;jit);
 642     jit.load16(JSInterfaceJIT::BaseIndex(stringGPR, indexGPR, JSInterfaceJIT::TimesTwo, 0), stringGPR);
 643     cont8Bit.link(&amp;jit);
 644 
 645     failures.append(jit.branch32(JSInterfaceJIT::Above, stringGPR, JSInterfaceJIT::TrustedImm32(maxSingleCharacterString)));
<span class="line-modified"> 646     jit.move(JSInterfaceJIT::TrustedImmPtr(vm-&gt;smallStrings.singleCharacterStrings()), indexGPR);</span>
 647     jit.loadPtr(JSInterfaceJIT::BaseIndex(indexGPR, stringGPR, JSInterfaceJIT::ScalePtr, 0), stringGPR);
 648     jit.ret();
 649 
 650     failures.link(&amp;jit);
 651     jit.move(JSInterfaceJIT::TrustedImm32(0), stringGPR);
 652     jit.ret();
 653 
 654     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 655     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;String get_by_val stub&quot;);
 656 }
 657 
 658 static void stringCharLoad(SpecializedThunkJIT&amp; jit)
 659 {
 660     // load string
 661     jit.loadJSStringArgument(SpecializedThunkJIT::ThisArgument, SpecializedThunkJIT::regT0);
 662 
 663     // Load string length to regT2, and start the process of loading the data pointer into regT0
 664     jit.loadPtr(MacroAssembler::Address(SpecializedThunkJIT::regT0, JSString::offsetOfValue()), SpecializedThunkJIT::regT0);
 665     jit.appendFailure(jit.branchIfRopeStringImpl(SpecializedThunkJIT::regT0));
 666     jit.load32(MacroAssembler::Address(SpecializedThunkJIT::regT0, StringImpl::lengthMemoryOffset()), SpecializedThunkJIT::regT2);
</pre>
<hr />
<pre>
 668     // load index
 669     jit.loadInt32Argument(0, SpecializedThunkJIT::regT1); // regT1 contains the index
 670 
 671     // Do an unsigned compare to simultaneously filter negative indices as well as indices that are too large
 672     jit.appendFailure(jit.branch32(MacroAssembler::AboveOrEqual, SpecializedThunkJIT::regT1, SpecializedThunkJIT::regT2));
 673 
 674     // Load the character
 675     SpecializedThunkJIT::JumpList is16Bit;
 676     SpecializedThunkJIT::JumpList cont8Bit;
 677     // Load the string flags
 678     jit.loadPtr(MacroAssembler::Address(SpecializedThunkJIT::regT0, StringImpl::flagsOffset()), SpecializedThunkJIT::regT2);
 679     jit.loadPtr(MacroAssembler::Address(SpecializedThunkJIT::regT0, StringImpl::dataOffset()), SpecializedThunkJIT::regT0);
 680     is16Bit.append(jit.branchTest32(MacroAssembler::Zero, SpecializedThunkJIT::regT2, MacroAssembler::TrustedImm32(StringImpl::flagIs8Bit())));
 681     jit.load8(MacroAssembler::BaseIndex(SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1, MacroAssembler::TimesOne, 0), SpecializedThunkJIT::regT0);
 682     cont8Bit.append(jit.jump());
 683     is16Bit.link(&amp;jit);
 684     jit.load16(MacroAssembler::BaseIndex(SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1, MacroAssembler::TimesTwo, 0), SpecializedThunkJIT::regT0);
 685     cont8Bit.link(&amp;jit);
 686 }
 687 
<span class="line-modified"> 688 static void charToString(SpecializedThunkJIT&amp; jit, VM* vm, MacroAssembler::RegisterID src, MacroAssembler::RegisterID dst, MacroAssembler::RegisterID scratch)</span>
 689 {
 690     jit.appendFailure(jit.branch32(MacroAssembler::Above, src, MacroAssembler::TrustedImm32(maxSingleCharacterString)));
<span class="line-modified"> 691     jit.move(MacroAssembler::TrustedImmPtr(vm-&gt;smallStrings.singleCharacterStrings()), scratch);</span>
 692     jit.loadPtr(MacroAssembler::BaseIndex(scratch, src, MacroAssembler::ScalePtr, 0), dst);
 693     jit.appendFailure(jit.branchTestPtr(MacroAssembler::Zero, dst));
 694 }
 695 
<span class="line-modified"> 696 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; charCodeAtThunkGenerator(VM* vm)</span>
 697 {
 698     SpecializedThunkJIT jit(vm, 1);
 699     stringCharLoad(jit);
 700     jit.returnInt32(SpecializedThunkJIT::regT0);
<span class="line-modified"> 701     return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;charCodeAt&quot;);</span>
 702 }
 703 
<span class="line-modified"> 704 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; charAtThunkGenerator(VM* vm)</span>
 705 {
 706     SpecializedThunkJIT jit(vm, 1);
 707     stringCharLoad(jit);
 708     charToString(jit, vm, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1);
 709     jit.returnJSCell(SpecializedThunkJIT::regT0);
<span class="line-modified"> 710     return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;charAt&quot;);</span>
 711 }
 712 
<span class="line-modified"> 713 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; fromCharCodeThunkGenerator(VM* vm)</span>
 714 {
 715     SpecializedThunkJIT jit(vm, 1);
 716     // load char code
 717     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0);
 718     charToString(jit, vm, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1);
 719     jit.returnJSCell(SpecializedThunkJIT::regT0);
<span class="line-modified"> 720     return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;fromCharCode&quot;);</span>
 721 }
 722 
<span class="line-modified"> 723 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; clz32ThunkGenerator(VM* vm)</span>
 724 {
 725     SpecializedThunkJIT jit(vm, 1);
 726     MacroAssembler::Jump nonIntArgJump;
 727     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntArgJump);
 728 
 729     SpecializedThunkJIT::Label convertedArgumentReentry(&amp;jit);
 730     jit.countLeadingZeros32(SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1);
 731     jit.returnInt32(SpecializedThunkJIT::regT1);
 732 
 733     if (jit.supportsFloatingPointTruncate()) {
 734         nonIntArgJump.link(&amp;jit);
 735         jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
 736         jit.branchTruncateDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, SpecializedThunkJIT::BranchIfTruncateSuccessful).linkTo(convertedArgumentReentry, &amp;jit);
 737         jit.appendFailure(jit.jump());
 738     } else
 739         jit.appendFailure(nonIntArgJump);
 740 
<span class="line-modified"> 741     return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;clz32&quot;);</span>
 742 }
 743 
<span class="line-modified"> 744 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; sqrtThunkGenerator(VM* vm)</span>
 745 {
 746     SpecializedThunkJIT jit(vm, 1);
 747     if (!jit.supportsFloatingPointSqrt())
<span class="line-modified"> 748         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm-&gt;jitStubs-&gt;ctiNativeCall(vm));</span>
 749 
 750     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
 751     jit.sqrtDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT0);
 752     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified"> 753     return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;sqrt&quot;);</span>
 754 }
 755 
 756 
 757 #define UnaryDoubleOpWrapper(function) function##Wrapper
 758 enum MathThunkCallingConvention { };
 759 typedef MathThunkCallingConvention(*MathThunk)(MathThunkCallingConvention);
 760 
 761 #if CPU(X86_64) &amp;&amp; COMPILER(GCC_COMPATIBLE) &amp;&amp; (OS(DARWIN) || OS(LINUX))
 762 
 763 #define defineUnaryDoubleOpWrapper(function) \
 764     asm( \
 765         &quot;.text\n&quot; \
 766         &quot;.globl &quot; SYMBOL_STRING(function##Thunk) &quot;\n&quot; \
 767         HIDE_SYMBOL(function##Thunk) &quot;\n&quot; \
 768         SYMBOL_STRING(function##Thunk) &quot;:&quot; &quot;\n&quot; \
 769         &quot;pushq %rax\n&quot; \
 770         &quot;call &quot; GLOBAL_REFERENCE(function) &quot;\n&quot; \
 771         &quot;popq %rcx\n&quot; \
 772         &quot;ret\n&quot; \
 773     );\
</pre>
<hr />
<pre>
 884         __asm ret \
 885         } \
 886     } \
 887     static MathThunk UnaryDoubleOpWrapper(function) = &amp;function##Thunk;
 888 
 889 #else
 890 
 891 #define defineUnaryDoubleOpWrapper(function) \
 892     static MathThunk UnaryDoubleOpWrapper(function) = 0
 893 #endif
 894 
 895 defineUnaryDoubleOpWrapper(jsRound);
 896 defineUnaryDoubleOpWrapper(exp);
 897 defineUnaryDoubleOpWrapper(log);
 898 defineUnaryDoubleOpWrapper(floor);
 899 defineUnaryDoubleOpWrapper(ceil);
 900 defineUnaryDoubleOpWrapper(trunc);
 901 
 902 static const double halfConstant = 0.5;
 903 
<span class="line-modified"> 904 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; floorThunkGenerator(VM* vm)</span>
 905 {
 906     SpecializedThunkJIT jit(vm, 1);
 907     MacroAssembler::Jump nonIntJump;
 908     if (!UnaryDoubleOpWrapper(floor) || !jit.supportsFloatingPoint())
<span class="line-modified"> 909         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm-&gt;jitStubs-&gt;ctiNativeCall(vm));</span>
 910     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntJump);
 911     jit.returnInt32(SpecializedThunkJIT::regT0);
 912     nonIntJump.link(&amp;jit);
 913     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
 914 
 915     if (jit.supportsFloatingPointRounding()) {
 916         SpecializedThunkJIT::JumpList doubleResult;
 917         jit.floorDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT0);
 918         jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
 919         jit.returnInt32(SpecializedThunkJIT::regT0);
 920         doubleResult.link(&amp;jit);
 921         jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified"> 922         return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;floor&quot;);</span>
 923     }
 924 
 925     SpecializedThunkJIT::Jump intResult;
 926     SpecializedThunkJIT::JumpList doubleResult;
 927     if (jit.supportsFloatingPointTruncate()) {
 928         jit.moveZeroToDouble(SpecializedThunkJIT::fpRegT1);
 929         doubleResult.append(jit.branchDouble(MacroAssembler::DoubleEqual, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1));
 930         SpecializedThunkJIT::JumpList slowPath;
 931         // Handle the negative doubles in the slow path for now.
 932         slowPath.append(jit.branchDouble(MacroAssembler::DoubleLessThanOrUnordered, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1));
 933         slowPath.append(jit.branchTruncateDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0));
 934         intResult = jit.jump();
 935         slowPath.link(&amp;jit);
 936     }
 937     jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(floor));
 938     jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
 939     if (jit.supportsFloatingPointTruncate())
 940         intResult.link(&amp;jit);
 941     jit.returnInt32(SpecializedThunkJIT::regT0);
 942     doubleResult.link(&amp;jit);
 943     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified"> 944     return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;floor&quot;);</span>
 945 }
 946 
<span class="line-modified"> 947 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; ceilThunkGenerator(VM* vm)</span>
 948 {
 949     SpecializedThunkJIT jit(vm, 1);
 950     if (!UnaryDoubleOpWrapper(ceil) || !jit.supportsFloatingPoint())
<span class="line-modified"> 951         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm-&gt;jitStubs-&gt;ctiNativeCall(vm));</span>
 952     MacroAssembler::Jump nonIntJump;
 953     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntJump);
 954     jit.returnInt32(SpecializedThunkJIT::regT0);
 955     nonIntJump.link(&amp;jit);
 956     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
 957     if (jit.supportsFloatingPointRounding())
 958         jit.ceilDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT0);
 959     else
 960         jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(ceil));
 961 
 962     SpecializedThunkJIT::JumpList doubleResult;
 963     jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
 964     jit.returnInt32(SpecializedThunkJIT::regT0);
 965     doubleResult.link(&amp;jit);
 966     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified"> 967     return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;ceil&quot;);</span>
 968 }
 969 
<span class="line-modified"> 970 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; truncThunkGenerator(VM* vm)</span>
 971 {
 972     SpecializedThunkJIT jit(vm, 1);
 973     if (!UnaryDoubleOpWrapper(trunc) || !jit.supportsFloatingPoint())
<span class="line-modified"> 974         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm-&gt;jitStubs-&gt;ctiNativeCall(vm));</span>
 975     MacroAssembler::Jump nonIntJump;
 976     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntJump);
 977     jit.returnInt32(SpecializedThunkJIT::regT0);
 978     nonIntJump.link(&amp;jit);
 979     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
 980     if (jit.supportsFloatingPointRounding())
 981         jit.roundTowardZeroDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT0);
 982     else
 983         jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(trunc));
 984 
 985     SpecializedThunkJIT::JumpList doubleResult;
 986     jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
 987     jit.returnInt32(SpecializedThunkJIT::regT0);
 988     doubleResult.link(&amp;jit);
 989     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified"> 990     return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;trunc&quot;);</span>
 991 }
 992 
<span class="line-modified"> 993 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; roundThunkGenerator(VM* vm)</span>
 994 {
 995     SpecializedThunkJIT jit(vm, 1);
 996     if (!UnaryDoubleOpWrapper(jsRound) || !jit.supportsFloatingPoint())
<span class="line-modified"> 997         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm-&gt;jitStubs-&gt;ctiNativeCall(vm));</span>
 998     MacroAssembler::Jump nonIntJump;
 999     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntJump);
1000     jit.returnInt32(SpecializedThunkJIT::regT0);
1001     nonIntJump.link(&amp;jit);
1002     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
1003     SpecializedThunkJIT::Jump intResult;
1004     SpecializedThunkJIT::JumpList doubleResult;
1005     if (jit.supportsFloatingPointTruncate()) {
1006         jit.moveZeroToDouble(SpecializedThunkJIT::fpRegT1);
1007         doubleResult.append(jit.branchDouble(MacroAssembler::DoubleEqual, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1));
1008         SpecializedThunkJIT::JumpList slowPath;
1009         // Handle the negative doubles in the slow path for now.
1010         slowPath.append(jit.branchDouble(MacroAssembler::DoubleLessThanOrUnordered, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1));
1011         jit.loadDouble(MacroAssembler::TrustedImmPtr(&amp;halfConstant), SpecializedThunkJIT::fpRegT1);
1012         jit.addDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1);
1013         slowPath.append(jit.branchTruncateDoubleToInt32(SpecializedThunkJIT::fpRegT1, SpecializedThunkJIT::regT0));
1014         intResult = jit.jump();
1015         slowPath.link(&amp;jit);
1016     }
1017     jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(jsRound));
1018     jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
1019     if (jit.supportsFloatingPointTruncate())
1020         intResult.link(&amp;jit);
1021     jit.returnInt32(SpecializedThunkJIT::regT0);
1022     doubleResult.link(&amp;jit);
1023     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified">1024     return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;round&quot;);</span>
1025 }
1026 
<span class="line-modified">1027 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; expThunkGenerator(VM* vm)</span>
1028 {
1029     if (!UnaryDoubleOpWrapper(exp))
<span class="line-modified">1030         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm-&gt;jitStubs-&gt;ctiNativeCall(vm));</span>
1031     SpecializedThunkJIT jit(vm, 1);
1032     if (!jit.supportsFloatingPoint())
<span class="line-modified">1033         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm-&gt;jitStubs-&gt;ctiNativeCall(vm));</span>
1034     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
1035     jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(exp));
1036     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified">1037     return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;exp&quot;);</span>
1038 }
1039 
<span class="line-modified">1040 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; logThunkGenerator(VM* vm)</span>
1041 {
1042     if (!UnaryDoubleOpWrapper(log))
<span class="line-modified">1043         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm-&gt;jitStubs-&gt;ctiNativeCall(vm));</span>
1044     SpecializedThunkJIT jit(vm, 1);
1045     if (!jit.supportsFloatingPoint())
<span class="line-modified">1046         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm-&gt;jitStubs-&gt;ctiNativeCall(vm));</span>
1047     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
1048     jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(log));
1049     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified">1050     return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;log&quot;);</span>
1051 }
1052 
<span class="line-modified">1053 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; absThunkGenerator(VM* vm)</span>
1054 {
1055     SpecializedThunkJIT jit(vm, 1);
1056     if (!jit.supportsFloatingPointAbs())
<span class="line-modified">1057         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm-&gt;jitStubs-&gt;ctiNativeCall(vm));</span>
1058 
1059 #if USE(JSVALUE64)
1060     unsigned virtualRegisterIndex = CallFrame::argumentOffset(0);
1061     jit.load64(AssemblyHelpers::addressFor(virtualRegisterIndex), GPRInfo::regT0);
1062     auto notInteger = jit.branchIfNotInt32(GPRInfo::regT0);
1063 
1064     // Abs Int32.
1065     jit.rshift32(GPRInfo::regT0, MacroAssembler::TrustedImm32(31), GPRInfo::regT1);
1066     jit.add32(GPRInfo::regT1, GPRInfo::regT0);
1067     jit.xor32(GPRInfo::regT1, GPRInfo::regT0);
1068 
1069     // IntMin cannot be inverted.
1070     MacroAssembler::Jump integerIsIntMin = jit.branchTest32(MacroAssembler::Signed, GPRInfo::regT0);
1071 
1072     // Box and finish.
1073     jit.or64(GPRInfo::tagTypeNumberRegister, GPRInfo::regT0);
1074     MacroAssembler::Jump doneWithIntegers = jit.jump();
1075 
1076     // Handle Doubles.
1077     notInteger.link(&amp;jit);
</pre>
<hr />
<pre>
1087 
1088     // We know the value of regT0 is IntMin. We could load that value from memory but
1089     // it is simpler to just convert it.
1090     integerIsIntMin.link(&amp;jit);
1091     jit.convertInt32ToDouble(GPRInfo::regT0, FPRInfo::fpRegT0);
1092     jit.jump().linkTo(absFPR0Label, &amp;jit);
1093 #else
1094     MacroAssembler::Jump nonIntJump;
1095     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntJump);
1096     jit.rshift32(SpecializedThunkJIT::regT0, MacroAssembler::TrustedImm32(31), SpecializedThunkJIT::regT1);
1097     jit.add32(SpecializedThunkJIT::regT1, SpecializedThunkJIT::regT0);
1098     jit.xor32(SpecializedThunkJIT::regT1, SpecializedThunkJIT::regT0);
1099     jit.appendFailure(jit.branchTest32(MacroAssembler::Signed, SpecializedThunkJIT::regT0));
1100     jit.returnInt32(SpecializedThunkJIT::regT0);
1101     nonIntJump.link(&amp;jit);
1102     // Shame about the double int conversion here.
1103     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
1104     jit.absDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1);
1105     jit.returnDouble(SpecializedThunkJIT::fpRegT1);
1106 #endif
<span class="line-modified">1107     return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;abs&quot;);</span>
1108 }
1109 
<span class="line-modified">1110 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; imulThunkGenerator(VM* vm)</span>
1111 {
1112     SpecializedThunkJIT jit(vm, 2);
1113     MacroAssembler::Jump nonIntArg0Jump;
1114     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntArg0Jump);
1115     SpecializedThunkJIT::Label doneLoadingArg0(&amp;jit);
1116     MacroAssembler::Jump nonIntArg1Jump;
1117     jit.loadInt32Argument(1, SpecializedThunkJIT::regT1, nonIntArg1Jump);
1118     SpecializedThunkJIT::Label doneLoadingArg1(&amp;jit);
1119     jit.mul32(SpecializedThunkJIT::regT1, SpecializedThunkJIT::regT0);
1120     jit.returnInt32(SpecializedThunkJIT::regT0);
1121 
1122     if (jit.supportsFloatingPointTruncate()) {
1123         nonIntArg0Jump.link(&amp;jit);
1124         jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
1125         jit.branchTruncateDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, SpecializedThunkJIT::BranchIfTruncateSuccessful).linkTo(doneLoadingArg0, &amp;jit);
1126         jit.appendFailure(jit.jump());
1127     } else
1128         jit.appendFailure(nonIntArg0Jump);
1129 
1130     if (jit.supportsFloatingPointTruncate()) {
1131         nonIntArg1Jump.link(&amp;jit);
1132         jit.loadDoubleArgument(1, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT1);
1133         jit.branchTruncateDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT1, SpecializedThunkJIT::BranchIfTruncateSuccessful).linkTo(doneLoadingArg1, &amp;jit);
1134         jit.appendFailure(jit.jump());
1135     } else
1136         jit.appendFailure(nonIntArg1Jump);
1137 
<span class="line-modified">1138     return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;imul&quot;);</span>
1139 }
1140 
<span class="line-modified">1141 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; randomThunkGenerator(VM* vm)</span>
1142 {
1143     SpecializedThunkJIT jit(vm, 0);
1144     if (!jit.supportsFloatingPoint())
<span class="line-modified">1145         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm-&gt;jitStubs-&gt;ctiNativeCall(vm));</span>
1146 
1147 #if USE(JSVALUE64)
<span class="line-modified">1148     jit.emitRandomThunk(*vm, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1, SpecializedThunkJIT::regT2, SpecializedThunkJIT::regT3, SpecializedThunkJIT::fpRegT0);</span>
1149     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
1150 
<span class="line-modified">1151     return jit.finalize(vm-&gt;jitStubs-&gt;ctiNativeTailCall(vm), &quot;random&quot;);</span>
1152 #else
<span class="line-modified">1153     return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm-&gt;jitStubs-&gt;ctiNativeCall(vm));</span>
1154 #endif
1155 }
1156 
<span class="line-modified">1157 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; boundThisNoArgsFunctionCallGenerator(VM* vm)</span>
1158 {
1159     CCallHelpers jit;
1160 
1161     jit.emitFunctionPrologue();
1162 
1163     // Set up our call frame.
1164     jit.storePtr(CCallHelpers::TrustedImmPtr(nullptr), CCallHelpers::addressFor(CallFrameSlot::codeBlock));
1165     jit.store32(CCallHelpers::TrustedImm32(0), CCallHelpers::tagFor(CallFrameSlot::argumentCount));
1166 
1167     unsigned extraStackNeeded = 0;
1168     if (unsigned stackMisalignment = sizeof(CallerFrameAndPC) % stackAlignmentBytes())
1169         extraStackNeeded = stackAlignmentBytes() - stackMisalignment;
1170 
1171     // We need to forward all of the arguments that we were passed. We aren&#39;t allowed to do a tail
1172     // call here as far as I can tell. At least not so long as the generic path doesn&#39;t do a tail
1173     // call, since that would be way too weird.
1174 
1175     // The formula for the number of stack bytes needed given some number of parameters (including
1176     // this) is:
1177     //
1178     //     stackAlign((numParams + CallFrameHeaderSize) * sizeof(Register) - sizeof(CallerFrameAndPC))
1179     //
1180     // Probably we want to write this as:
1181     //
1182     //     stackAlign((numParams + (CallFrameHeaderSize - CallerFrameAndPCSize)) * sizeof(Register))
1183     //
1184     // That&#39;s really all there is to this. We have all the registers we need to do it.
1185 
1186     jit.load32(CCallHelpers::payloadFor(CallFrameSlot::argumentCount), GPRInfo::regT1);
1187     jit.add32(CCallHelpers::TrustedImm32(CallFrame::headerSizeInRegisters - CallerFrameAndPC::sizeInRegisters), GPRInfo::regT1, GPRInfo::regT2);
1188     jit.lshift32(CCallHelpers::TrustedImm32(3), GPRInfo::regT2);
1189     jit.add32(CCallHelpers::TrustedImm32(stackAlignmentBytes() - 1), GPRInfo::regT2);
1190     jit.and32(CCallHelpers::TrustedImm32(-stackAlignmentBytes()), GPRInfo::regT2);
1191 
1192     if (extraStackNeeded)
1193         jit.add32(CCallHelpers::TrustedImm32(extraStackNeeded), GPRInfo::regT2);
1194 
<span class="line-modified">1195     // At this point regT1 has the actual argument count and regT2 has the amount of stack we will</span>
<span class="line-modified">1196     // need.</span>












1197 
<span class="line-modified">1198     jit.subPtr(GPRInfo::regT2, CCallHelpers::stackPointerRegister);</span>

1199 
1200     // Do basic callee frame setup, including &#39;this&#39;.
1201 
1202     jit.loadCell(CCallHelpers::addressFor(CallFrameSlot::callee), GPRInfo::regT3);
1203 
1204     jit.store32(GPRInfo::regT1, CCallHelpers::calleeFramePayloadSlot(CallFrameSlot::argumentCount));
1205 
1206     JSValueRegs valueRegs = JSValueRegs::withTwoAvailableRegs(GPRInfo::regT0, GPRInfo::regT2);
1207     jit.loadValue(CCallHelpers::Address(GPRInfo::regT3, JSBoundFunction::offsetOfBoundThis()), valueRegs);
1208     jit.storeValue(valueRegs, CCallHelpers::calleeArgumentSlot(0));
1209 
1210     jit.loadPtr(CCallHelpers::Address(GPRInfo::regT3, JSBoundFunction::offsetOfTargetFunction()), GPRInfo::regT3);
1211     jit.storeCell(GPRInfo::regT3, CCallHelpers::calleeFrameSlot(CallFrameSlot::callee));
1212 
1213     // OK, now we can start copying. This is a simple matter of copying parameters from the caller&#39;s
1214     // frame to the callee&#39;s frame. Note that we know that regT1 (the argument count) must be at
1215     // least 1.
1216     jit.sub32(CCallHelpers::TrustedImm32(1), GPRInfo::regT1);
1217     CCallHelpers::Jump done = jit.branchTest32(CCallHelpers::Zero, GPRInfo::regT1);
1218 
</pre>
<hr />
<pre>
1223     jit.branchTest32(CCallHelpers::NonZero, GPRInfo::regT1).linkTo(loop, &amp;jit);
1224 
1225     done.link(&amp;jit);
1226 
1227     jit.loadPtr(
1228         CCallHelpers::Address(GPRInfo::regT3, JSFunction::offsetOfExecutable()),
1229         GPRInfo::regT0);
1230     jit.loadPtr(
1231         CCallHelpers::Address(
1232             GPRInfo::regT0, ExecutableBase::offsetOfJITCodeWithArityCheckFor(CodeForCall)),
1233         GPRInfo::regT0);
1234     CCallHelpers::Jump noCode = jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::regT0);
1235 
1236     emitPointerValidation(jit, GPRInfo::regT0, JSEntryPtrTag);
1237     jit.call(GPRInfo::regT0, JSEntryPtrTag);
1238 
1239     jit.emitFunctionEpilogue();
1240     jit.ret();
1241 
1242     LinkBuffer linkBuffer(jit, GLOBAL_THUNK_ID);
<span class="line-modified">1243     linkBuffer.link(noCode, CodeLocationLabel&lt;JITThunkPtrTag&gt;(vm-&gt;jitStubs-&gt;ctiNativeTailCallWithoutSavedTags(vm)));</span>
1244     return FINALIZE_CODE(
1245         linkBuffer, JITThunkPtrTag, &quot;Specialized thunk for bound function calls with no arguments&quot;);
1246 }
1247 
1248 } // namespace JSC
1249 
1250 #endif // ENABLE(JIT)
</pre>
</td>
<td>
<hr />
<pre>
  36 #include &quot;MathCommon.h&quot;
  37 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;
  38 #include &quot;SpecializedThunkJIT.h&quot;
  39 #include &lt;wtf/InlineASM.h&gt;
  40 #include &lt;wtf/StringPrintStream.h&gt;
  41 #include &lt;wtf/text/StringImpl.h&gt;
  42 
  43 #if ENABLE(JIT)
  44 
  45 namespace JSC {
  46 
  47 template&lt;typename TagType&gt;
  48 inline void emitPointerValidation(CCallHelpers&amp; jit, GPRReg pointerGPR, TagType tag)
  49 {
  50     if (ASSERT_DISABLED)
  51         return;
  52     CCallHelpers::Jump isNonZero = jit.branchTestPtr(CCallHelpers::NonZero, pointerGPR);
  53     jit.abortWithReason(TGInvalidPointer);
  54     isNonZero.link(&amp;jit);
  55     jit.pushToSave(pointerGPR);
<span class="line-modified">  56     jit.untagPtr(tag, pointerGPR);</span>
  57     jit.load8(pointerGPR, pointerGPR);
  58     jit.popToRestore(pointerGPR);
  59 }
  60 
  61 // We will jump here if the JIT code tries to make a call, but the
  62 // linking helper (C++ code) decides to throw an exception instead.
<span class="line-modified">  63 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; throwExceptionFromCallSlowPathGenerator(VM&amp; vm)</span>
  64 {
  65     CCallHelpers jit;
  66 
  67     // The call pushed a return address, so we need to pop it back off to re-align the stack,
  68     // even though we won&#39;t use it.
  69     jit.preserveReturnAddressAfterCall(GPRInfo::nonPreservedNonReturnGPR);
  70 
<span class="line-modified">  71     jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm.topEntryFrame);</span>
  72 
<span class="line-modified">  73     jit.setupArguments&lt;decltype(lookupExceptionHandler)&gt;(CCallHelpers::TrustedImmPtr(&amp;vm), GPRInfo::callFrameRegister);</span>
  74     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandler)), GPRInfo::nonArgGPR0);
  75     emitPointerValidation(jit, GPRInfo::nonArgGPR0, OperationPtrTag);
  76     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
<span class="line-modified">  77     jit.jumpToExceptionHandler(vm);</span>
  78 
  79     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
  80     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;Throw exception from call slow path thunk&quot;);
  81 }
  82 
<span class="line-modified">  83 static void slowPathFor(CCallHelpers&amp; jit, VM&amp; vm, Sprt_JITOperation_ECli slowPathFunction)</span>
  84 {
<span class="line-modified">  85     jit.sanitizeStackInline(vm, GPRInfo::nonArgGPR0);</span>
  86     jit.emitFunctionPrologue();
<span class="line-modified">  87     jit.storePtr(GPRInfo::callFrameRegister, &amp;vm.topCallFrame);</span>
  88 #if OS(WINDOWS) &amp;&amp; CPU(X86_64)
  89     // Windows X86_64 needs some space pointed to by arg0 for return types larger than 64 bits.
  90     // Other argument values are shift by 1. Use space on the stack for our two return values.
  91     // Moving the stack down maxFrameExtentForSlowPathCall bytes gives us room for our 3 arguments
  92     // and space for the 16 byte return area.
  93     jit.addPtr(CCallHelpers::TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), CCallHelpers::stackPointerRegister);
  94     jit.move(GPRInfo::regT2, GPRInfo::argumentGPR2);
  95     jit.addPtr(CCallHelpers::TrustedImm32(32), CCallHelpers::stackPointerRegister, GPRInfo::argumentGPR0);
  96     jit.move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
  97     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(slowPathFunction)), GPRInfo::nonArgGPR0);
  98     emitPointerValidation(jit, GPRInfo::nonArgGPR0, OperationPtrTag);
  99     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
 100     jit.loadPtr(CCallHelpers::Address(GPRInfo::returnValueGPR, 8), GPRInfo::returnValueGPR2);
 101     jit.loadPtr(CCallHelpers::Address(GPRInfo::returnValueGPR), GPRInfo::returnValueGPR);
 102     jit.addPtr(CCallHelpers::TrustedImm32(maxFrameExtentForSlowPathCall), CCallHelpers::stackPointerRegister);
 103 #else
 104     if (maxFrameExtentForSlowPathCall)
 105         jit.addPtr(CCallHelpers::TrustedImm32(-maxFrameExtentForSlowPathCall), CCallHelpers::stackPointerRegister);
 106     jit.setupArguments&lt;decltype(slowPathFunction)&gt;(GPRInfo::regT2);
 107     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(slowPathFunction)), GPRInfo::nonArgGPR0);
</pre>
<hr />
<pre>
 111         jit.addPtr(CCallHelpers::TrustedImm32(maxFrameExtentForSlowPathCall), CCallHelpers::stackPointerRegister);
 112 #endif
 113 
 114     // This slow call will return the address of one of the following:
 115     // 1) Exception throwing thunk.
 116     // 2) Host call return value returner thingy.
 117     // 3) The function to call.
 118     // The second return value GPR will hold a non-zero value for tail calls.
 119 
 120     emitPointerValidation(jit, GPRInfo::returnValueGPR, JSEntryPtrTag);
 121     jit.emitFunctionEpilogue();
 122     jit.untagReturnAddress();
 123 
 124     RELEASE_ASSERT(reinterpret_cast&lt;void*&gt;(KeepTheFrame) == reinterpret_cast&lt;void*&gt;(0));
 125     CCallHelpers::Jump doNotTrash = jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::returnValueGPR2);
 126 
 127     jit.preserveReturnAddressAfterCall(GPRInfo::nonPreservedNonReturnGPR);
 128     jit.prepareForTailCallSlow(GPRInfo::returnValueGPR);
 129 
 130     doNotTrash.link(&amp;jit);
<span class="line-modified"> 131     jit.farJump(GPRInfo::returnValueGPR, JSEntryPtrTag);</span>
 132 }
 133 
<span class="line-modified"> 134 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; linkCallThunkGenerator(VM&amp; vm)</span>
 135 {
 136     // The return address is on the stack or in the link register. We will hence
 137     // save the return address to the call frame while we make a C++ function call
 138     // to perform linking and lazy compilation if necessary. We expect the callee
 139     // to be in regT0/regT1 (payload/tag), the CallFrame to have already
 140     // been adjusted, and all other registers to be available for use.
 141     CCallHelpers jit;
 142 
 143     slowPathFor(jit, vm, operationLinkCall);
 144 
 145     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 146     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;Link call slow path thunk&quot;);
 147 }
 148 
 149 // For closure optimizations, we only include calls, since if you&#39;re using closures for
 150 // object construction then you&#39;re going to lose big time anyway.
<span class="line-modified"> 151 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; linkPolymorphicCallThunkGenerator(VM&amp; vm)</span>
 152 {
 153     CCallHelpers jit;
 154 
 155     slowPathFor(jit, vm, operationLinkPolymorphicCall);
 156 
 157     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 158     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;Link polymorphic call slow path thunk&quot;);
 159 }
 160 
 161 // FIXME: We should distinguish between a megamorphic virtual call vs. a slow
 162 // path virtual call so that we can enable fast tail calls for megamorphic
 163 // virtual calls by using the shuffler.
 164 // https://bugs.webkit.org/show_bug.cgi?id=148831
<span class="line-modified"> 165 MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; virtualThunkFor(VM&amp; vm, CallLinkInfo&amp; callLinkInfo)</span>
 166 {
 167     // The callee is in regT0 (for JSVALUE32_64, the tag is in regT1).
 168     // The return address is on the stack, or in the link register. We will hence
 169     // jump to the callee, or save the return address to the call frame while we
 170     // make a C++ function call to the appropriate JIT operation.
 171 
 172     CCallHelpers jit;
 173 
 174     CCallHelpers::JumpList slowCase;
 175 
 176     // This is a slow path execution, and regT2 contains the CallLinkInfo. Count the
 177     // slow path execution for the profiler.
 178     jit.add32(
 179         CCallHelpers::TrustedImm32(1),
 180         CCallHelpers::Address(GPRInfo::regT2, CallLinkInfo::offsetOfSlowPathCount()));
 181 
 182     // FIXME: we should have a story for eliminating these checks. In many cases,
 183     // the DFG knows that the value is definitely a cell, or definitely a function.
 184 
 185 #if USE(JSVALUE64)

 186     if (callLinkInfo.isTailCall()) {
 187         // Tail calls could have clobbered the GPRInfo::tagMaskRegister because they
 188         // restore callee saved registers before getthing here. So, let&#39;s materialize
 189         // the TagMask in a temp register and use the temp instead.
<span class="line-modified"> 190         slowCase.append(jit.branchIfNotCell(GPRInfo::regT0, DoNotHaveTagRegisters));</span>
<span class="line-modified"> 191     } else</span>
<span class="line-modified"> 192         slowCase.append(jit.branchIfNotCell(GPRInfo::regT0));</span>


 193 #else
 194     slowCase.append(jit.branchIfNotCell(GPRInfo::regT1));
 195 #endif
<span class="line-modified"> 196     auto notJSFunction = jit.branchIfNotFunction(GPRInfo::regT0);</span>
 197 
 198     // Now we know we have a JSFunction.
 199 
 200     jit.loadPtr(
 201         CCallHelpers::Address(GPRInfo::regT0, JSFunction::offsetOfExecutable()),
 202         GPRInfo::regT4);
 203     jit.loadPtr(
 204         CCallHelpers::Address(
 205             GPRInfo::regT4, ExecutableBase::offsetOfJITCodeWithArityCheckFor(
 206                 callLinkInfo.specializationKind())),
 207         GPRInfo::regT4);
 208     slowCase.append(jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::regT4));
 209 
 210     // Now we know that we have a CodeBlock, and we&#39;re committed to making a fast
 211     // call.
 212 
 213     // Make a tail call. This will return back to JIT code.
 214     JSInterfaceJIT::Label callCode(jit.label());
 215     emitPointerValidation(jit, GPRInfo::regT4, JSEntryPtrTag);
 216     if (callLinkInfo.isTailCall()) {
 217         jit.preserveReturnAddressAfterCall(GPRInfo::regT0);
 218         jit.prepareForTailCallSlow(GPRInfo::regT4);
 219     }
<span class="line-modified"> 220     jit.farJump(GPRInfo::regT4, JSEntryPtrTag);</span>
 221 
 222     notJSFunction.link(&amp;jit);
 223     slowCase.append(jit.branchIfNotType(GPRInfo::regT0, InternalFunctionType));
<span class="line-modified"> 224     void* executableAddress = vm.getCTIInternalFunctionTrampolineFor(callLinkInfo.specializationKind()).executableAddress();</span>
 225     jit.move(CCallHelpers::TrustedImmPtr(executableAddress), GPRInfo::regT4);
 226     jit.jump().linkTo(callCode, &amp;jit);
 227 
 228     slowCase.link(&amp;jit);
 229 
 230     // Here we don&#39;t know anything, so revert to the full slow path.
 231     slowPathFor(jit, vm, operationVirtualCall);
 232 
 233     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 234     return FINALIZE_CODE(
 235         patchBuffer, JITStubRoutinePtrTag,
 236         &quot;Virtual %s slow path thunk&quot;,
 237         callLinkInfo.callMode() == CallMode::Regular ? &quot;call&quot; : callLinkInfo.callMode() == CallMode::Tail ? &quot;tail call&quot; : &quot;construct&quot;);
 238 }
 239 
 240 enum ThunkEntryType { EnterViaCall, EnterViaJumpWithSavedTags, EnterViaJumpWithoutSavedTags };
 241 enum class ThunkFunctionType { JSFunction, InternalFunction };
 242 
<span class="line-modified"> 243 static MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeForGenerator(VM&amp; vm, ThunkFunctionType thunkFunctionType, CodeSpecializationKind kind, ThunkEntryType entryType = EnterViaCall)</span>
 244 {
 245     // FIXME: This should be able to log ShadowChicken prologue packets.
 246     // https://bugs.webkit.org/show_bug.cgi?id=155689
 247 
 248     int executableOffsetToFunction = NativeExecutable::offsetOfNativeFunctionFor(kind);
 249 
<span class="line-modified"> 250     JSInterfaceJIT jit(&amp;vm);</span>
 251 
 252     switch (entryType) {
 253     case EnterViaCall:
 254         jit.emitFunctionPrologue();
 255         break;
 256     case EnterViaJumpWithSavedTags:
 257 #if USE(JSVALUE64)
 258         // We&#39;re coming from a specialized thunk that has saved the prior tag registers&#39; contents.
 259         // Restore them now.
 260         jit.popPair(JSInterfaceJIT::tagTypeNumberRegister, JSInterfaceJIT::tagMaskRegister);
 261 #endif
 262         break;
 263     case EnterViaJumpWithoutSavedTags:
 264         jit.move(JSInterfaceJIT::framePointerRegister, JSInterfaceJIT::stackPointerRegister);
 265         break;
 266     }
 267 
 268     jit.emitPutToCallFrameHeader(0, CallFrameSlot::codeBlock);
<span class="line-modified"> 269     jit.storePtr(JSInterfaceJIT::callFrameRegister, &amp;vm.topCallFrame);</span>
 270 
 271 #if CPU(X86)
 272     // Calling convention:      f(ecx, edx, ...);
 273     // Host function signature: f(ExecState*);
 274     jit.move(JSInterfaceJIT::callFrameRegister, X86Registers::ecx);
 275 
 276     jit.subPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::stackPointerRegister); // Align stack after prologue.
 277 
 278     // call the function
 279     jit.emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, JSInterfaceJIT::regT1);
 280     if (thunkFunctionType == ThunkFunctionType::JSFunction) {
 281         jit.loadPtr(JSInterfaceJIT::Address(JSInterfaceJIT::regT1, JSFunction::offsetOfExecutable()), JSInterfaceJIT::regT1);
 282         jit.call(JSInterfaceJIT::Address(JSInterfaceJIT::regT1, executableOffsetToFunction), JSEntryPtrTag);
 283     } else
 284         jit.call(JSInterfaceJIT::Address(JSInterfaceJIT::regT1, InternalFunction::offsetOfNativeFunctionFor(kind)), JSEntryPtrTag);
 285 
 286     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::stackPointerRegister);
 287 
 288 #elif CPU(X86_64)
 289 #if !OS(WINDOWS)
</pre>
<hr />
<pre>
 346 
 347     jit.emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, JSInterfaceJIT::argumentGPR1);
 348     if (thunkFunctionType == ThunkFunctionType::JSFunction) {
 349         jit.loadPtr(JSInterfaceJIT::Address(JSInterfaceJIT::argumentGPR1, JSFunction::offsetOfExecutable()), JSInterfaceJIT::regT2);
 350         jit.call(JSInterfaceJIT::Address(JSInterfaceJIT::regT2, executableOffsetToFunction), JSEntryPtrTag);
 351     } else
 352         jit.call(JSInterfaceJIT::Address(JSInterfaceJIT::argumentGPR1, InternalFunction::offsetOfNativeFunctionFor(kind)), JSEntryPtrTag);
 353 
 354 #if CPU(MIPS)
 355     // Restore stack space
 356     jit.addPtr(JSInterfaceJIT::TrustedImm32(16), JSInterfaceJIT::stackPointerRegister);
 357 #endif
 358 #else
 359 #error &quot;JIT not supported on this platform.&quot;
 360     UNUSED_PARAM(executableOffsetToFunction);
 361     abortWithReason(TGNotSupported);
 362 #endif
 363 
 364     // Check for an exception
 365 #if USE(JSVALUE64)
<span class="line-modified"> 366     jit.load64(vm.addressOfException(), JSInterfaceJIT::regT2);</span>
 367     JSInterfaceJIT::Jump exceptionHandler = jit.branchTest64(JSInterfaceJIT::NonZero, JSInterfaceJIT::regT2);
 368 #else
 369     JSInterfaceJIT::Jump exceptionHandler = jit.branch32(
 370         JSInterfaceJIT::NotEqual,
<span class="line-modified"> 371         JSInterfaceJIT::AbsoluteAddress(vm.addressOfException()),</span>
 372         JSInterfaceJIT::TrustedImm32(0));
 373 #endif
 374 
 375     jit.emitFunctionEpilogue();
 376     // Return.
 377     jit.ret();
 378 
 379     // Handle an exception
 380     exceptionHandler.link(&amp;jit);
 381 
<span class="line-modified"> 382     jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm.topEntryFrame);</span>
<span class="line-modified"> 383     jit.storePtr(JSInterfaceJIT::callFrameRegister, &amp;vm.topCallFrame);</span>
 384 
 385 #if CPU(X86) &amp;&amp; USE(JSVALUE32_64)
 386     jit.subPtr(JSInterfaceJIT::TrustedImm32(4), JSInterfaceJIT::stackPointerRegister);
 387     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::regT0);
 388     jit.push(JSInterfaceJIT::regT0);
 389 #else
 390 #if OS(WINDOWS)
 391     // Allocate space on stack for the 4 parameter registers.
 392     jit.subPtr(JSInterfaceJIT::TrustedImm32(4 * sizeof(int64_t)), JSInterfaceJIT::stackPointerRegister);
 393 #endif
 394     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::argumentGPR0);
 395 #endif
 396     jit.move(JSInterfaceJIT::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationVMHandleException)), JSInterfaceJIT::regT3);
 397     jit.call(JSInterfaceJIT::regT3, OperationPtrTag);
 398 #if CPU(X86) &amp;&amp; USE(JSVALUE32_64)
 399     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::stackPointerRegister);
 400 #elif OS(WINDOWS)
 401     jit.addPtr(JSInterfaceJIT::TrustedImm32(4 * sizeof(int64_t)), JSInterfaceJIT::stackPointerRegister);
 402 #endif
 403 
<span class="line-modified"> 404     jit.jumpToExceptionHandler(vm);</span>
 405 
 406     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 407     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;%s %s%s trampoline&quot;, thunkFunctionType == ThunkFunctionType::JSFunction ? &quot;native&quot; : &quot;internal&quot;, entryType == EnterViaJumpWithSavedTags ? &quot;Tail With Saved Tags &quot; : entryType == EnterViaJumpWithoutSavedTags ? &quot;Tail Without Saved Tags &quot; : &quot;&quot;, toCString(kind).data());
 408 }
 409 
<span class="line-modified"> 410 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeCallGenerator(VM&amp; vm)</span>
 411 {
 412     return nativeForGenerator(vm, ThunkFunctionType::JSFunction, CodeForCall);
 413 }
 414 
<span class="line-modified"> 415 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeTailCallGenerator(VM&amp; vm)</span>
 416 {
 417     return nativeForGenerator(vm, ThunkFunctionType::JSFunction, CodeForCall, EnterViaJumpWithSavedTags);
 418 }
 419 
<span class="line-modified"> 420 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeTailCallWithoutSavedTagsGenerator(VM&amp; vm)</span>
 421 {
 422     return nativeForGenerator(vm, ThunkFunctionType::JSFunction, CodeForCall, EnterViaJumpWithoutSavedTags);
 423 }
 424 
<span class="line-modified"> 425 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; nativeConstructGenerator(VM&amp; vm)</span>
 426 {
 427     return nativeForGenerator(vm, ThunkFunctionType::JSFunction, CodeForConstruct);
 428 }
 429 
<span class="line-modified"> 430 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; internalFunctionCallGenerator(VM&amp; vm)</span>
 431 {
 432     return nativeForGenerator(vm, ThunkFunctionType::InternalFunction, CodeForCall);
 433 }
 434 
<span class="line-modified"> 435 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; internalFunctionConstructGenerator(VM&amp; vm)</span>
 436 {
 437     return nativeForGenerator(vm, ThunkFunctionType::InternalFunction, CodeForConstruct);
 438 }
 439 
<span class="line-modified"> 440 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; arityFixupGenerator(VM&amp; vm)</span>
 441 {
<span class="line-modified"> 442     JSInterfaceJIT jit(&amp;vm);</span>
 443 
 444     // We enter with fixup count in argumentGPR0
 445     // We have the guarantee that a0, a1, a2, t3, t4 and t5 (or t0 for Windows) are all distinct :-)
 446 #if USE(JSVALUE64)
 447 #if OS(WINDOWS)
 448     const GPRReg extraTemp = JSInterfaceJIT::regT0;
 449 #else
 450     const GPRReg extraTemp = JSInterfaceJIT::regT5;
 451 #endif
 452 #  if CPU(X86_64)
 453     jit.pop(JSInterfaceJIT::regT4);
 454 #  endif
 455     jit.tagReturnAddress();
<span class="line-modified"> 456 #if CPU(ARM64E)</span>
 457     jit.loadPtr(JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()), GPRInfo::regT3);
 458     jit.addPtr(JSInterfaceJIT::TrustedImm32(sizeof(CallerFrameAndPC)), GPRInfo::callFrameRegister, extraTemp);
<span class="line-modified"> 459     jit.untagPtr(extraTemp, GPRInfo::regT3);</span>
 460     PtrTag tempReturnPCTag = static_cast&lt;PtrTag&gt;(random());
 461     jit.move(JSInterfaceJIT::TrustedImmPtr(tempReturnPCTag), extraTemp);
<span class="line-modified"> 462     jit.tagPtr(extraTemp, GPRInfo::regT3);</span>
 463     jit.storePtr(GPRInfo::regT3, JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()));
 464 #endif
 465     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::regT3);
 466     jit.load32(JSInterfaceJIT::addressFor(CallFrameSlot::argumentCount), JSInterfaceJIT::argumentGPR2);
 467     jit.add32(JSInterfaceJIT::TrustedImm32(CallFrame::headerSizeInRegisters), JSInterfaceJIT::argumentGPR2);
 468 
 469     // Check to see if we have extra slots we can use
 470     jit.move(JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::argumentGPR1);
 471     jit.and32(JSInterfaceJIT::TrustedImm32(stackAlignmentRegisters() - 1), JSInterfaceJIT::argumentGPR1);
 472     JSInterfaceJIT::Jump noExtraSlot = jit.branchTest32(MacroAssembler::Zero, JSInterfaceJIT::argumentGPR1);
 473     jit.move(JSInterfaceJIT::TrustedImm64(ValueUndefined), extraTemp);
 474     JSInterfaceJIT::Label fillExtraSlots(jit.label());
 475     jit.store64(extraTemp, MacroAssembler::BaseIndex(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::argumentGPR2, JSInterfaceJIT::TimesEight));
 476     jit.add32(JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2);
 477     jit.branchSub32(JSInterfaceJIT::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR1).linkTo(fillExtraSlots, &amp;jit);
 478     jit.and32(JSInterfaceJIT::TrustedImm32(-stackAlignmentRegisters()), JSInterfaceJIT::argumentGPR0);
 479     JSInterfaceJIT::Jump done = jit.branchTest32(MacroAssembler::Zero, JSInterfaceJIT::argumentGPR0);
 480     noExtraSlot.link(&amp;jit);
 481 
 482     jit.neg64(JSInterfaceJIT::argumentGPR0);
</pre>
<hr />
<pre>
 492     jit.addPtr(extraTemp, JSInterfaceJIT::stackPointerRegister);
 493     jit.tagReturnAddress();
 494 
 495     // Move current frame down argumentGPR0 number of slots
 496     JSInterfaceJIT::Label copyLoop(jit.label());
 497     jit.load64(JSInterfaceJIT::regT3, extraTemp);
 498     jit.store64(extraTemp, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight));
 499     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::regT3);
 500     jit.branchSub32(MacroAssembler::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2).linkTo(copyLoop, &amp;jit);
 501 
 502     // Fill in argumentGPR0 missing arg slots with undefined
 503     jit.move(JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::argumentGPR2);
 504     jit.move(JSInterfaceJIT::TrustedImm64(ValueUndefined), extraTemp);
 505     JSInterfaceJIT::Label fillUndefinedLoop(jit.label());
 506     jit.store64(extraTemp, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight));
 507     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::regT3);
 508     jit.branchAdd32(MacroAssembler::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2).linkTo(fillUndefinedLoop, &amp;jit);
 509 
 510     done.link(&amp;jit);
 511 
<span class="line-modified"> 512 #if CPU(ARM64E)</span>
 513     jit.loadPtr(JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()), GPRInfo::regT3);
 514     jit.move(JSInterfaceJIT::TrustedImmPtr(tempReturnPCTag), extraTemp);
<span class="line-modified"> 515     jit.untagPtr(extraTemp, GPRInfo::regT3);</span>
 516     jit.addPtr(JSInterfaceJIT::TrustedImm32(sizeof(CallerFrameAndPC)), GPRInfo::callFrameRegister, extraTemp);
<span class="line-modified"> 517     jit.tagPtr(extraTemp, GPRInfo::regT3);</span>
 518     jit.storePtr(GPRInfo::regT3, JSInterfaceJIT::Address(GPRInfo::callFrameRegister, CallFrame::returnPCOffset()));
 519 #endif
 520 
 521 #  if CPU(X86_64)
 522     jit.push(JSInterfaceJIT::regT4);
 523 #  endif
 524     jit.ret();
 525 #else // USE(JSVALUE64) section above, USE(JSVALUE32_64) section below.
 526 #  if CPU(X86)
 527     jit.pop(JSInterfaceJIT::regT4);
 528 #  endif
 529     jit.move(JSInterfaceJIT::callFrameRegister, JSInterfaceJIT::regT3);
 530     jit.load32(JSInterfaceJIT::addressFor(CallFrameSlot::argumentCount), JSInterfaceJIT::argumentGPR2);
 531     jit.add32(JSInterfaceJIT::TrustedImm32(CallFrame::headerSizeInRegisters), JSInterfaceJIT::argumentGPR2);
 532 
 533     // Check to see if we have extra slots we can use
 534     jit.move(JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::argumentGPR1);
 535     jit.and32(JSInterfaceJIT::TrustedImm32(stackAlignmentRegisters() - 1), JSInterfaceJIT::argumentGPR1);
 536     JSInterfaceJIT::Jump noExtraSlot = jit.branchTest32(MacroAssembler::Zero, JSInterfaceJIT::argumentGPR1);
 537     JSInterfaceJIT::Label fillExtraSlots(jit.label());
</pre>
<hr />
<pre>
 573     jit.move(JSInterfaceJIT::TrustedImm32(0), JSInterfaceJIT::regT5);
 574     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight, PayloadOffset));
 575     jit.move(JSInterfaceJIT::TrustedImm32(JSValue::UndefinedTag), JSInterfaceJIT::regT5);
 576     jit.store32(JSInterfaceJIT::regT5, MacroAssembler::BaseIndex(JSInterfaceJIT::regT3, JSInterfaceJIT::argumentGPR0, JSInterfaceJIT::TimesEight, TagOffset));
 577 
 578     jit.addPtr(JSInterfaceJIT::TrustedImm32(8), JSInterfaceJIT::regT3);
 579     jit.branchAdd32(MacroAssembler::NonZero, JSInterfaceJIT::TrustedImm32(1), JSInterfaceJIT::argumentGPR2).linkTo(fillUndefinedLoop, &amp;jit);
 580 
 581     done.link(&amp;jit);
 582 
 583 #  if CPU(X86)
 584     jit.push(JSInterfaceJIT::regT4);
 585 #  endif
 586     jit.ret();
 587 #endif // End of USE(JSVALUE32_64) section.
 588 
 589     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 590     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;fixup arity&quot;);
 591 }
 592 
<span class="line-modified"> 593 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; unreachableGenerator(VM&amp; vm)</span>
 594 {
<span class="line-modified"> 595     JSInterfaceJIT jit(&amp;vm);</span>
 596 
 597     jit.breakpoint();
 598 
 599     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 600     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;unreachable thunk&quot;);
 601 }
 602 
<span class="line-modified"> 603 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; stringGetByValGenerator(VM&amp; vm)</span>
 604 {
 605     // regT0 is JSString*, and regT1 (64bit) or regT2 (32bit) is int index.
 606     // Return regT0 = result JSString* if succeeds. Otherwise, return regT0 = 0.
 607 #if USE(JSVALUE64)
 608     GPRReg stringGPR = GPRInfo::regT0;
 609     GPRReg indexGPR = GPRInfo::regT1;
 610     GPRReg scratchGPR = GPRInfo::regT2;
 611 #else
 612     GPRReg stringGPR = GPRInfo::regT0;
 613     GPRReg indexGPR = GPRInfo::regT2;
 614     GPRReg scratchGPR = GPRInfo::regT1;
 615 #endif
 616 
<span class="line-modified"> 617     JSInterfaceJIT jit(&amp;vm);</span>
 618     JSInterfaceJIT::JumpList failures;
 619     jit.tagReturnAddress();
 620 
 621     // Load string length to regT2, and start the process of loading the data pointer into regT0
 622     jit.loadPtr(JSInterfaceJIT::Address(stringGPR, JSString::offsetOfValue()), stringGPR);
 623     failures.append(jit.branchIfRopeStringImpl(stringGPR));
 624     jit.load32(JSInterfaceJIT::Address(stringGPR, StringImpl::lengthMemoryOffset()), scratchGPR);
 625 
 626     // Do an unsigned compare to simultaneously filter negative indices as well as indices that are too large
 627     failures.append(jit.branch32(JSInterfaceJIT::AboveOrEqual, indexGPR, scratchGPR));
 628 
 629     // Load the character
 630     JSInterfaceJIT::JumpList is16Bit;
 631     JSInterfaceJIT::JumpList cont8Bit;
 632     // Load the string flags
 633     jit.load32(JSInterfaceJIT::Address(stringGPR, StringImpl::flagsOffset()), scratchGPR);
 634     jit.loadPtr(JSInterfaceJIT::Address(stringGPR, StringImpl::dataOffset()), stringGPR);
 635     is16Bit.append(jit.branchTest32(JSInterfaceJIT::Zero, scratchGPR, JSInterfaceJIT::TrustedImm32(StringImpl::flagIs8Bit())));
 636     jit.load8(JSInterfaceJIT::BaseIndex(stringGPR, indexGPR, JSInterfaceJIT::TimesOne, 0), stringGPR);
 637     cont8Bit.append(jit.jump());
 638     is16Bit.link(&amp;jit);
 639     jit.load16(JSInterfaceJIT::BaseIndex(stringGPR, indexGPR, JSInterfaceJIT::TimesTwo, 0), stringGPR);
 640     cont8Bit.link(&amp;jit);
 641 
 642     failures.append(jit.branch32(JSInterfaceJIT::Above, stringGPR, JSInterfaceJIT::TrustedImm32(maxSingleCharacterString)));
<span class="line-modified"> 643     jit.move(JSInterfaceJIT::TrustedImmPtr(vm.smallStrings.singleCharacterStrings()), indexGPR);</span>
 644     jit.loadPtr(JSInterfaceJIT::BaseIndex(indexGPR, stringGPR, JSInterfaceJIT::ScalePtr, 0), stringGPR);
 645     jit.ret();
 646 
 647     failures.link(&amp;jit);
 648     jit.move(JSInterfaceJIT::TrustedImm32(0), stringGPR);
 649     jit.ret();
 650 
 651     LinkBuffer patchBuffer(jit, GLOBAL_THUNK_ID);
 652     return FINALIZE_CODE(patchBuffer, JITThunkPtrTag, &quot;String get_by_val stub&quot;);
 653 }
 654 
 655 static void stringCharLoad(SpecializedThunkJIT&amp; jit)
 656 {
 657     // load string
 658     jit.loadJSStringArgument(SpecializedThunkJIT::ThisArgument, SpecializedThunkJIT::regT0);
 659 
 660     // Load string length to regT2, and start the process of loading the data pointer into regT0
 661     jit.loadPtr(MacroAssembler::Address(SpecializedThunkJIT::regT0, JSString::offsetOfValue()), SpecializedThunkJIT::regT0);
 662     jit.appendFailure(jit.branchIfRopeStringImpl(SpecializedThunkJIT::regT0));
 663     jit.load32(MacroAssembler::Address(SpecializedThunkJIT::regT0, StringImpl::lengthMemoryOffset()), SpecializedThunkJIT::regT2);
</pre>
<hr />
<pre>
 665     // load index
 666     jit.loadInt32Argument(0, SpecializedThunkJIT::regT1); // regT1 contains the index
 667 
 668     // Do an unsigned compare to simultaneously filter negative indices as well as indices that are too large
 669     jit.appendFailure(jit.branch32(MacroAssembler::AboveOrEqual, SpecializedThunkJIT::regT1, SpecializedThunkJIT::regT2));
 670 
 671     // Load the character
 672     SpecializedThunkJIT::JumpList is16Bit;
 673     SpecializedThunkJIT::JumpList cont8Bit;
 674     // Load the string flags
 675     jit.loadPtr(MacroAssembler::Address(SpecializedThunkJIT::regT0, StringImpl::flagsOffset()), SpecializedThunkJIT::regT2);
 676     jit.loadPtr(MacroAssembler::Address(SpecializedThunkJIT::regT0, StringImpl::dataOffset()), SpecializedThunkJIT::regT0);
 677     is16Bit.append(jit.branchTest32(MacroAssembler::Zero, SpecializedThunkJIT::regT2, MacroAssembler::TrustedImm32(StringImpl::flagIs8Bit())));
 678     jit.load8(MacroAssembler::BaseIndex(SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1, MacroAssembler::TimesOne, 0), SpecializedThunkJIT::regT0);
 679     cont8Bit.append(jit.jump());
 680     is16Bit.link(&amp;jit);
 681     jit.load16(MacroAssembler::BaseIndex(SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1, MacroAssembler::TimesTwo, 0), SpecializedThunkJIT::regT0);
 682     cont8Bit.link(&amp;jit);
 683 }
 684 
<span class="line-modified"> 685 static void charToString(SpecializedThunkJIT&amp; jit, VM&amp; vm, MacroAssembler::RegisterID src, MacroAssembler::RegisterID dst, MacroAssembler::RegisterID scratch)</span>
 686 {
 687     jit.appendFailure(jit.branch32(MacroAssembler::Above, src, MacroAssembler::TrustedImm32(maxSingleCharacterString)));
<span class="line-modified"> 688     jit.move(MacroAssembler::TrustedImmPtr(vm.smallStrings.singleCharacterStrings()), scratch);</span>
 689     jit.loadPtr(MacroAssembler::BaseIndex(scratch, src, MacroAssembler::ScalePtr, 0), dst);
 690     jit.appendFailure(jit.branchTestPtr(MacroAssembler::Zero, dst));
 691 }
 692 
<span class="line-modified"> 693 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; charCodeAtThunkGenerator(VM&amp; vm)</span>
 694 {
 695     SpecializedThunkJIT jit(vm, 1);
 696     stringCharLoad(jit);
 697     jit.returnInt32(SpecializedThunkJIT::regT0);
<span class="line-modified"> 698     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;charCodeAt&quot;);</span>
 699 }
 700 
<span class="line-modified"> 701 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; charAtThunkGenerator(VM&amp; vm)</span>
 702 {
 703     SpecializedThunkJIT jit(vm, 1);
 704     stringCharLoad(jit);
 705     charToString(jit, vm, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1);
 706     jit.returnJSCell(SpecializedThunkJIT::regT0);
<span class="line-modified"> 707     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;charAt&quot;);</span>
 708 }
 709 
<span class="line-modified"> 710 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; fromCharCodeThunkGenerator(VM&amp; vm)</span>
 711 {
 712     SpecializedThunkJIT jit(vm, 1);
 713     // load char code
 714     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0);
 715     charToString(jit, vm, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1);
 716     jit.returnJSCell(SpecializedThunkJIT::regT0);
<span class="line-modified"> 717     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;fromCharCode&quot;);</span>
 718 }
 719 
<span class="line-modified"> 720 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; clz32ThunkGenerator(VM&amp; vm)</span>
 721 {
 722     SpecializedThunkJIT jit(vm, 1);
 723     MacroAssembler::Jump nonIntArgJump;
 724     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntArgJump);
 725 
 726     SpecializedThunkJIT::Label convertedArgumentReentry(&amp;jit);
 727     jit.countLeadingZeros32(SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1);
 728     jit.returnInt32(SpecializedThunkJIT::regT1);
 729 
 730     if (jit.supportsFloatingPointTruncate()) {
 731         nonIntArgJump.link(&amp;jit);
 732         jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
 733         jit.branchTruncateDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, SpecializedThunkJIT::BranchIfTruncateSuccessful).linkTo(convertedArgumentReentry, &amp;jit);
 734         jit.appendFailure(jit.jump());
 735     } else
 736         jit.appendFailure(nonIntArgJump);
 737 
<span class="line-modified"> 738     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;clz32&quot;);</span>
 739 }
 740 
<span class="line-modified"> 741 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; sqrtThunkGenerator(VM&amp; vm)</span>
 742 {
 743     SpecializedThunkJIT jit(vm, 1);
 744     if (!jit.supportsFloatingPointSqrt())
<span class="line-modified"> 745         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));</span>
 746 
 747     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
 748     jit.sqrtDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT0);
 749     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified"> 750     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;sqrt&quot;);</span>
 751 }
 752 
 753 
 754 #define UnaryDoubleOpWrapper(function) function##Wrapper
 755 enum MathThunkCallingConvention { };
 756 typedef MathThunkCallingConvention(*MathThunk)(MathThunkCallingConvention);
 757 
 758 #if CPU(X86_64) &amp;&amp; COMPILER(GCC_COMPATIBLE) &amp;&amp; (OS(DARWIN) || OS(LINUX))
 759 
 760 #define defineUnaryDoubleOpWrapper(function) \
 761     asm( \
 762         &quot;.text\n&quot; \
 763         &quot;.globl &quot; SYMBOL_STRING(function##Thunk) &quot;\n&quot; \
 764         HIDE_SYMBOL(function##Thunk) &quot;\n&quot; \
 765         SYMBOL_STRING(function##Thunk) &quot;:&quot; &quot;\n&quot; \
 766         &quot;pushq %rax\n&quot; \
 767         &quot;call &quot; GLOBAL_REFERENCE(function) &quot;\n&quot; \
 768         &quot;popq %rcx\n&quot; \
 769         &quot;ret\n&quot; \
 770     );\
</pre>
<hr />
<pre>
 881         __asm ret \
 882         } \
 883     } \
 884     static MathThunk UnaryDoubleOpWrapper(function) = &amp;function##Thunk;
 885 
 886 #else
 887 
 888 #define defineUnaryDoubleOpWrapper(function) \
 889     static MathThunk UnaryDoubleOpWrapper(function) = 0
 890 #endif
 891 
 892 defineUnaryDoubleOpWrapper(jsRound);
 893 defineUnaryDoubleOpWrapper(exp);
 894 defineUnaryDoubleOpWrapper(log);
 895 defineUnaryDoubleOpWrapper(floor);
 896 defineUnaryDoubleOpWrapper(ceil);
 897 defineUnaryDoubleOpWrapper(trunc);
 898 
 899 static const double halfConstant = 0.5;
 900 
<span class="line-modified"> 901 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; floorThunkGenerator(VM&amp; vm)</span>
 902 {
 903     SpecializedThunkJIT jit(vm, 1);
 904     MacroAssembler::Jump nonIntJump;
 905     if (!UnaryDoubleOpWrapper(floor) || !jit.supportsFloatingPoint())
<span class="line-modified"> 906         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));</span>
 907     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntJump);
 908     jit.returnInt32(SpecializedThunkJIT::regT0);
 909     nonIntJump.link(&amp;jit);
 910     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
 911 
 912     if (jit.supportsFloatingPointRounding()) {
 913         SpecializedThunkJIT::JumpList doubleResult;
 914         jit.floorDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT0);
 915         jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
 916         jit.returnInt32(SpecializedThunkJIT::regT0);
 917         doubleResult.link(&amp;jit);
 918         jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified"> 919         return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;floor&quot;);</span>
 920     }
 921 
 922     SpecializedThunkJIT::Jump intResult;
 923     SpecializedThunkJIT::JumpList doubleResult;
 924     if (jit.supportsFloatingPointTruncate()) {
 925         jit.moveZeroToDouble(SpecializedThunkJIT::fpRegT1);
 926         doubleResult.append(jit.branchDouble(MacroAssembler::DoubleEqual, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1));
 927         SpecializedThunkJIT::JumpList slowPath;
 928         // Handle the negative doubles in the slow path for now.
 929         slowPath.append(jit.branchDouble(MacroAssembler::DoubleLessThanOrUnordered, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1));
 930         slowPath.append(jit.branchTruncateDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0));
 931         intResult = jit.jump();
 932         slowPath.link(&amp;jit);
 933     }
 934     jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(floor));
 935     jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
 936     if (jit.supportsFloatingPointTruncate())
 937         intResult.link(&amp;jit);
 938     jit.returnInt32(SpecializedThunkJIT::regT0);
 939     doubleResult.link(&amp;jit);
 940     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified"> 941     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;floor&quot;);</span>
 942 }
 943 
<span class="line-modified"> 944 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; ceilThunkGenerator(VM&amp; vm)</span>
 945 {
 946     SpecializedThunkJIT jit(vm, 1);
 947     if (!UnaryDoubleOpWrapper(ceil) || !jit.supportsFloatingPoint())
<span class="line-modified"> 948         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));</span>
 949     MacroAssembler::Jump nonIntJump;
 950     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntJump);
 951     jit.returnInt32(SpecializedThunkJIT::regT0);
 952     nonIntJump.link(&amp;jit);
 953     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
 954     if (jit.supportsFloatingPointRounding())
 955         jit.ceilDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT0);
 956     else
 957         jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(ceil));
 958 
 959     SpecializedThunkJIT::JumpList doubleResult;
 960     jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
 961     jit.returnInt32(SpecializedThunkJIT::regT0);
 962     doubleResult.link(&amp;jit);
 963     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified"> 964     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;ceil&quot;);</span>
 965 }
 966 
<span class="line-modified"> 967 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; truncThunkGenerator(VM&amp; vm)</span>
 968 {
 969     SpecializedThunkJIT jit(vm, 1);
 970     if (!UnaryDoubleOpWrapper(trunc) || !jit.supportsFloatingPoint())
<span class="line-modified"> 971         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));</span>
 972     MacroAssembler::Jump nonIntJump;
 973     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntJump);
 974     jit.returnInt32(SpecializedThunkJIT::regT0);
 975     nonIntJump.link(&amp;jit);
 976     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
 977     if (jit.supportsFloatingPointRounding())
 978         jit.roundTowardZeroDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT0);
 979     else
 980         jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(trunc));
 981 
 982     SpecializedThunkJIT::JumpList doubleResult;
 983     jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
 984     jit.returnInt32(SpecializedThunkJIT::regT0);
 985     doubleResult.link(&amp;jit);
 986     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified"> 987     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;trunc&quot;);</span>
 988 }
 989 
<span class="line-modified"> 990 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; roundThunkGenerator(VM&amp; vm)</span>
 991 {
 992     SpecializedThunkJIT jit(vm, 1);
 993     if (!UnaryDoubleOpWrapper(jsRound) || !jit.supportsFloatingPoint())
<span class="line-modified"> 994         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));</span>
 995     MacroAssembler::Jump nonIntJump;
 996     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntJump);
 997     jit.returnInt32(SpecializedThunkJIT::regT0);
 998     nonIntJump.link(&amp;jit);
 999     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
1000     SpecializedThunkJIT::Jump intResult;
1001     SpecializedThunkJIT::JumpList doubleResult;
1002     if (jit.supportsFloatingPointTruncate()) {
1003         jit.moveZeroToDouble(SpecializedThunkJIT::fpRegT1);
1004         doubleResult.append(jit.branchDouble(MacroAssembler::DoubleEqual, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1));
1005         SpecializedThunkJIT::JumpList slowPath;
1006         // Handle the negative doubles in the slow path for now.
1007         slowPath.append(jit.branchDouble(MacroAssembler::DoubleLessThanOrUnordered, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1));
1008         jit.loadDouble(MacroAssembler::TrustedImmPtr(&amp;halfConstant), SpecializedThunkJIT::fpRegT1);
1009         jit.addDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1);
1010         slowPath.append(jit.branchTruncateDoubleToInt32(SpecializedThunkJIT::fpRegT1, SpecializedThunkJIT::regT0));
1011         intResult = jit.jump();
1012         slowPath.link(&amp;jit);
1013     }
1014     jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(jsRound));
1015     jit.branchConvertDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, doubleResult, SpecializedThunkJIT::fpRegT1);
1016     if (jit.supportsFloatingPointTruncate())
1017         intResult.link(&amp;jit);
1018     jit.returnInt32(SpecializedThunkJIT::regT0);
1019     doubleResult.link(&amp;jit);
1020     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified">1021     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;round&quot;);</span>
1022 }
1023 
<span class="line-modified">1024 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; expThunkGenerator(VM&amp; vm)</span>
1025 {
1026     if (!UnaryDoubleOpWrapper(exp))
<span class="line-modified">1027         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));</span>
1028     SpecializedThunkJIT jit(vm, 1);
1029     if (!jit.supportsFloatingPoint())
<span class="line-modified">1030         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));</span>
1031     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
1032     jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(exp));
1033     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified">1034     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;exp&quot;);</span>
1035 }
1036 
<span class="line-modified">1037 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; logThunkGenerator(VM&amp; vm)</span>
1038 {
1039     if (!UnaryDoubleOpWrapper(log))
<span class="line-modified">1040         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));</span>
1041     SpecializedThunkJIT jit(vm, 1);
1042     if (!jit.supportsFloatingPoint())
<span class="line-modified">1043         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));</span>
1044     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
1045     jit.callDoubleToDoublePreservingReturn(UnaryDoubleOpWrapper(log));
1046     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
<span class="line-modified">1047     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;log&quot;);</span>
1048 }
1049 
<span class="line-modified">1050 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; absThunkGenerator(VM&amp; vm)</span>
1051 {
1052     SpecializedThunkJIT jit(vm, 1);
1053     if (!jit.supportsFloatingPointAbs())
<span class="line-modified">1054         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));</span>
1055 
1056 #if USE(JSVALUE64)
1057     unsigned virtualRegisterIndex = CallFrame::argumentOffset(0);
1058     jit.load64(AssemblyHelpers::addressFor(virtualRegisterIndex), GPRInfo::regT0);
1059     auto notInteger = jit.branchIfNotInt32(GPRInfo::regT0);
1060 
1061     // Abs Int32.
1062     jit.rshift32(GPRInfo::regT0, MacroAssembler::TrustedImm32(31), GPRInfo::regT1);
1063     jit.add32(GPRInfo::regT1, GPRInfo::regT0);
1064     jit.xor32(GPRInfo::regT1, GPRInfo::regT0);
1065 
1066     // IntMin cannot be inverted.
1067     MacroAssembler::Jump integerIsIntMin = jit.branchTest32(MacroAssembler::Signed, GPRInfo::regT0);
1068 
1069     // Box and finish.
1070     jit.or64(GPRInfo::tagTypeNumberRegister, GPRInfo::regT0);
1071     MacroAssembler::Jump doneWithIntegers = jit.jump();
1072 
1073     // Handle Doubles.
1074     notInteger.link(&amp;jit);
</pre>
<hr />
<pre>
1084 
1085     // We know the value of regT0 is IntMin. We could load that value from memory but
1086     // it is simpler to just convert it.
1087     integerIsIntMin.link(&amp;jit);
1088     jit.convertInt32ToDouble(GPRInfo::regT0, FPRInfo::fpRegT0);
1089     jit.jump().linkTo(absFPR0Label, &amp;jit);
1090 #else
1091     MacroAssembler::Jump nonIntJump;
1092     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntJump);
1093     jit.rshift32(SpecializedThunkJIT::regT0, MacroAssembler::TrustedImm32(31), SpecializedThunkJIT::regT1);
1094     jit.add32(SpecializedThunkJIT::regT1, SpecializedThunkJIT::regT0);
1095     jit.xor32(SpecializedThunkJIT::regT1, SpecializedThunkJIT::regT0);
1096     jit.appendFailure(jit.branchTest32(MacroAssembler::Signed, SpecializedThunkJIT::regT0));
1097     jit.returnInt32(SpecializedThunkJIT::regT0);
1098     nonIntJump.link(&amp;jit);
1099     // Shame about the double int conversion here.
1100     jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
1101     jit.absDouble(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::fpRegT1);
1102     jit.returnDouble(SpecializedThunkJIT::fpRegT1);
1103 #endif
<span class="line-modified">1104     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;abs&quot;);</span>
1105 }
1106 
<span class="line-modified">1107 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; imulThunkGenerator(VM&amp; vm)</span>
1108 {
1109     SpecializedThunkJIT jit(vm, 2);
1110     MacroAssembler::Jump nonIntArg0Jump;
1111     jit.loadInt32Argument(0, SpecializedThunkJIT::regT0, nonIntArg0Jump);
1112     SpecializedThunkJIT::Label doneLoadingArg0(&amp;jit);
1113     MacroAssembler::Jump nonIntArg1Jump;
1114     jit.loadInt32Argument(1, SpecializedThunkJIT::regT1, nonIntArg1Jump);
1115     SpecializedThunkJIT::Label doneLoadingArg1(&amp;jit);
1116     jit.mul32(SpecializedThunkJIT::regT1, SpecializedThunkJIT::regT0);
1117     jit.returnInt32(SpecializedThunkJIT::regT0);
1118 
1119     if (jit.supportsFloatingPointTruncate()) {
1120         nonIntArg0Jump.link(&amp;jit);
1121         jit.loadDoubleArgument(0, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0);
1122         jit.branchTruncateDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT0, SpecializedThunkJIT::BranchIfTruncateSuccessful).linkTo(doneLoadingArg0, &amp;jit);
1123         jit.appendFailure(jit.jump());
1124     } else
1125         jit.appendFailure(nonIntArg0Jump);
1126 
1127     if (jit.supportsFloatingPointTruncate()) {
1128         nonIntArg1Jump.link(&amp;jit);
1129         jit.loadDoubleArgument(1, SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT1);
1130         jit.branchTruncateDoubleToInt32(SpecializedThunkJIT::fpRegT0, SpecializedThunkJIT::regT1, SpecializedThunkJIT::BranchIfTruncateSuccessful).linkTo(doneLoadingArg1, &amp;jit);
1131         jit.appendFailure(jit.jump());
1132     } else
1133         jit.appendFailure(nonIntArg1Jump);
1134 
<span class="line-modified">1135     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;imul&quot;);</span>
1136 }
1137 
<span class="line-modified">1138 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; randomThunkGenerator(VM&amp; vm)</span>
1139 {
1140     SpecializedThunkJIT jit(vm, 0);
1141     if (!jit.supportsFloatingPoint())
<span class="line-modified">1142         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));</span>
1143 
1144 #if USE(JSVALUE64)
<span class="line-modified">1145     jit.emitRandomThunk(vm, SpecializedThunkJIT::regT0, SpecializedThunkJIT::regT1, SpecializedThunkJIT::regT2, SpecializedThunkJIT::regT3, SpecializedThunkJIT::fpRegT0);</span>
1146     jit.returnDouble(SpecializedThunkJIT::fpRegT0);
1147 
<span class="line-modified">1148     return jit.finalize(vm.jitStubs-&gt;ctiNativeTailCall(vm), &quot;random&quot;);</span>
1149 #else
<span class="line-modified">1150     return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(vm.jitStubs-&gt;ctiNativeCall(vm));</span>
1151 #endif
1152 }
1153 
<span class="line-modified">1154 MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; boundThisNoArgsFunctionCallGenerator(VM&amp; vm)</span>
1155 {
1156     CCallHelpers jit;
1157 
1158     jit.emitFunctionPrologue();
1159 
1160     // Set up our call frame.
1161     jit.storePtr(CCallHelpers::TrustedImmPtr(nullptr), CCallHelpers::addressFor(CallFrameSlot::codeBlock));
1162     jit.store32(CCallHelpers::TrustedImm32(0), CCallHelpers::tagFor(CallFrameSlot::argumentCount));
1163 
1164     unsigned extraStackNeeded = 0;
1165     if (unsigned stackMisalignment = sizeof(CallerFrameAndPC) % stackAlignmentBytes())
1166         extraStackNeeded = stackAlignmentBytes() - stackMisalignment;
1167 
1168     // We need to forward all of the arguments that we were passed. We aren&#39;t allowed to do a tail
1169     // call here as far as I can tell. At least not so long as the generic path doesn&#39;t do a tail
1170     // call, since that would be way too weird.
1171 
1172     // The formula for the number of stack bytes needed given some number of parameters (including
1173     // this) is:
1174     //
1175     //     stackAlign((numParams + CallFrameHeaderSize) * sizeof(Register) - sizeof(CallerFrameAndPC))
1176     //
1177     // Probably we want to write this as:
1178     //
1179     //     stackAlign((numParams + (CallFrameHeaderSize - CallerFrameAndPCSize)) * sizeof(Register))
1180     //
1181     // That&#39;s really all there is to this. We have all the registers we need to do it.
1182 
1183     jit.load32(CCallHelpers::payloadFor(CallFrameSlot::argumentCount), GPRInfo::regT1);
1184     jit.add32(CCallHelpers::TrustedImm32(CallFrame::headerSizeInRegisters - CallerFrameAndPC::sizeInRegisters), GPRInfo::regT1, GPRInfo::regT2);
1185     jit.lshift32(CCallHelpers::TrustedImm32(3), GPRInfo::regT2);
1186     jit.add32(CCallHelpers::TrustedImm32(stackAlignmentBytes() - 1), GPRInfo::regT2);
1187     jit.and32(CCallHelpers::TrustedImm32(-stackAlignmentBytes()), GPRInfo::regT2);
1188 
1189     if (extraStackNeeded)
1190         jit.add32(CCallHelpers::TrustedImm32(extraStackNeeded), GPRInfo::regT2);
1191 
<span class="line-modified">1192     // At this point regT1 has the actual argument count and regT2 has the amount of stack we will need.</span>
<span class="line-modified">1193     // Check to see if we have enough stack space.</span>
<span class="line-added">1194 </span>
<span class="line-added">1195     jit.negPtr(GPRInfo::regT2);</span>
<span class="line-added">1196     jit.addPtr(CCallHelpers::stackPointerRegister, GPRInfo::regT2);</span>
<span class="line-added">1197     CCallHelpers::Jump haveStackSpace = jit.branchPtr(CCallHelpers::BelowOrEqual, CCallHelpers::AbsoluteAddress(vm.addressOfSoftStackLimit()), GPRInfo::regT2);</span>
<span class="line-added">1198 </span>
<span class="line-added">1199     // Throw Stack Overflow exception</span>
<span class="line-added">1200     jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm.topEntryFrame);</span>
<span class="line-added">1201     jit.setupArguments&lt;decltype(throwStackOverflowErrorFromThunk)&gt;(CCallHelpers::TrustedImmPtr(&amp;vm), GPRInfo::callFrameRegister);</span>
<span class="line-added">1202     jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(throwStackOverflowErrorFromThunk)), GPRInfo::nonArgGPR0);</span>
<span class="line-added">1203     emitPointerValidation(jit, GPRInfo::nonArgGPR0, OperationPtrTag);</span>
<span class="line-added">1204     jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);</span>
<span class="line-added">1205     jit.jumpToExceptionHandler(vm);</span>
1206 
<span class="line-modified">1207     haveStackSpace.link(&amp;jit);</span>
<span class="line-added">1208     jit.move(GPRInfo::regT2, CCallHelpers::stackPointerRegister);</span>
1209 
1210     // Do basic callee frame setup, including &#39;this&#39;.
1211 
1212     jit.loadCell(CCallHelpers::addressFor(CallFrameSlot::callee), GPRInfo::regT3);
1213 
1214     jit.store32(GPRInfo::regT1, CCallHelpers::calleeFramePayloadSlot(CallFrameSlot::argumentCount));
1215 
1216     JSValueRegs valueRegs = JSValueRegs::withTwoAvailableRegs(GPRInfo::regT0, GPRInfo::regT2);
1217     jit.loadValue(CCallHelpers::Address(GPRInfo::regT3, JSBoundFunction::offsetOfBoundThis()), valueRegs);
1218     jit.storeValue(valueRegs, CCallHelpers::calleeArgumentSlot(0));
1219 
1220     jit.loadPtr(CCallHelpers::Address(GPRInfo::regT3, JSBoundFunction::offsetOfTargetFunction()), GPRInfo::regT3);
1221     jit.storeCell(GPRInfo::regT3, CCallHelpers::calleeFrameSlot(CallFrameSlot::callee));
1222 
1223     // OK, now we can start copying. This is a simple matter of copying parameters from the caller&#39;s
1224     // frame to the callee&#39;s frame. Note that we know that regT1 (the argument count) must be at
1225     // least 1.
1226     jit.sub32(CCallHelpers::TrustedImm32(1), GPRInfo::regT1);
1227     CCallHelpers::Jump done = jit.branchTest32(CCallHelpers::Zero, GPRInfo::regT1);
1228 
</pre>
<hr />
<pre>
1233     jit.branchTest32(CCallHelpers::NonZero, GPRInfo::regT1).linkTo(loop, &amp;jit);
1234 
1235     done.link(&amp;jit);
1236 
1237     jit.loadPtr(
1238         CCallHelpers::Address(GPRInfo::regT3, JSFunction::offsetOfExecutable()),
1239         GPRInfo::regT0);
1240     jit.loadPtr(
1241         CCallHelpers::Address(
1242             GPRInfo::regT0, ExecutableBase::offsetOfJITCodeWithArityCheckFor(CodeForCall)),
1243         GPRInfo::regT0);
1244     CCallHelpers::Jump noCode = jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::regT0);
1245 
1246     emitPointerValidation(jit, GPRInfo::regT0, JSEntryPtrTag);
1247     jit.call(GPRInfo::regT0, JSEntryPtrTag);
1248 
1249     jit.emitFunctionEpilogue();
1250     jit.ret();
1251 
1252     LinkBuffer linkBuffer(jit, GLOBAL_THUNK_ID);
<span class="line-modified">1253     linkBuffer.link(noCode, CodeLocationLabel&lt;JITThunkPtrTag&gt;(vm.jitStubs-&gt;ctiNativeTailCallWithoutSavedTags(vm)));</span>
1254     return FINALIZE_CODE(
1255         linkBuffer, JITThunkPtrTag, &quot;Specialized thunk for bound function calls with no arguments&quot;);
1256 }
1257 
1258 } // namespace JSC
1259 
1260 #endif // ENABLE(JIT)
</pre>
</td>
</tr>
</table>
<center><a href="ThunkGenerator.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="ThunkGenerators.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>