<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/SlotVisitor.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2012-2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
 14  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 15  * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
 17  * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 18  * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 19  * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 20  * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 21  * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 22  * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 23  * THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;SlotVisitor.h&quot;
 28 
 29 #include &quot;CPU.h&quot;
 30 #include &quot;ConservativeRoots.h&quot;
 31 #include &quot;GCSegmentedArrayInlines.h&quot;
 32 #include &quot;HeapCellInlines.h&quot;
 33 #include &quot;HeapProfiler.h&quot;
 34 #include &quot;HeapSnapshotBuilder.h&quot;
 35 #include &quot;JSArray.h&quot;
 36 #include &quot;JSDestructibleObject.h&quot;
 37 #include &quot;JSObject.h&quot;
 38 #include &quot;JSString.h&quot;
 39 #include &quot;JSCInlines.h&quot;
 40 #include &quot;MarkingConstraintSolver.h&quot;
 41 #include &quot;SlotVisitorInlines.h&quot;
 42 #include &quot;StopIfNecessaryTimer.h&quot;
 43 #include &quot;SuperSampler.h&quot;
 44 #include &quot;VM.h&quot;
 45 #include &lt;wtf/ListDump.h&gt;
 46 #include &lt;wtf/Lock.h&gt;
 47 
 48 namespace JSC {
 49 
 50 #if ENABLE(GC_VALIDATION)
 51 static void validate(JSCell* cell)
 52 {
 53     RELEASE_ASSERT(cell);
 54 
 55     if (!cell-&gt;structure()) {
 56         dataLogF(&quot;cell at %p has a null structure\n&quot; , cell);
 57         CRASH();
 58     }
 59 
 60     // Both the cell&#39;s structure, and the cell&#39;s structure&#39;s structure should be the Structure Structure.
 61     // I hate this sentence.
 62     VM&amp; vm = *cell-&gt;vm();
 63     if (cell-&gt;structure()-&gt;structure()-&gt;JSCell::classInfo(vm) != cell-&gt;structure()-&gt;JSCell::classInfo(vm)) {
 64         const char* parentClassName = 0;
 65         const char* ourClassName = 0;
 66         if (cell-&gt;structure()-&gt;structure() &amp;&amp; cell-&gt;structure()-&gt;structure()-&gt;JSCell::classInfo(vm))
 67             parentClassName = cell-&gt;structure()-&gt;structure()-&gt;JSCell::classInfo(vm)-&gt;className;
 68         if (cell-&gt;structure()-&gt;JSCell::classInfo(vm))
 69             ourClassName = cell-&gt;structure()-&gt;JSCell::classInfo(vm)-&gt;className;
 70         dataLogF(&quot;parent structure (%p &lt;%s&gt;) of cell at %p doesn&#39;t match cell&#39;s structure (%p &lt;%s&gt;)\n&quot;,
 71             cell-&gt;structure()-&gt;structure(), parentClassName, cell, cell-&gt;structure(), ourClassName);
 72         CRASH();
 73     }
 74 
 75     // Make sure we can walk the ClassInfo chain
 76     const ClassInfo* info = cell-&gt;classInfo(vm);
 77     do { } while ((info = info-&gt;parentClass));
 78 }
 79 #endif
 80 
 81 SlotVisitor::SlotVisitor(Heap&amp; heap, CString codeName)
 82     : m_bytesVisited(0)
 83     , m_visitCount(0)
 84     , m_isInParallelMode(false)
 85     , m_markingVersion(MarkedSpace::initialVersion)
 86     , m_heap(heap)
 87     , m_codeName(codeName)
 88 #if !ASSERT_DISABLED
 89     , m_isCheckingForDefaultMarkViolation(false)
 90     , m_isDraining(false)
 91 #endif
 92 {
 93 }
 94 
 95 SlotVisitor::~SlotVisitor()
 96 {
 97     clearMarkStacks();
 98 }
 99 
100 void SlotVisitor::didStartMarking()
101 {
102     auto scope = heap()-&gt;collectionScope();
103     if (scope) {
104         switch (*scope) {
105         case CollectionScope::Eden:
106             reset();
107             break;
108         case CollectionScope::Full:
109             m_extraMemorySize = 0;
110             break;
111         }
112     }
113 
114     if (HeapProfiler* heapProfiler = vm().heapProfiler())
115         m_heapSnapshotBuilder = heapProfiler-&gt;activeSnapshotBuilder();
116 
117     m_markingVersion = heap()-&gt;objectSpace().markingVersion();
118 }
119 
120 void SlotVisitor::reset()
121 {
122     m_bytesVisited = 0;
123     m_visitCount = 0;
124     m_heapSnapshotBuilder = nullptr;
125     RELEASE_ASSERT(!m_currentCell);
126 }
127 
128 void SlotVisitor::clearMarkStacks()
129 {
130     forEachMarkStack(
131         [&amp;] (MarkStackArray&amp; stack) -&gt; IterationStatus {
132             stack.clear();
133             return IterationStatus::Continue;
134         });
135 }
136 
137 void SlotVisitor::append(const ConservativeRoots&amp; conservativeRoots)
138 {
139     HeapCell** roots = conservativeRoots.roots();
140     size_t size = conservativeRoots.size();
141     for (size_t i = 0; i &lt; size; ++i)
142         appendJSCellOrAuxiliary(roots[i]);
143 }
144 
145 void SlotVisitor::appendJSCellOrAuxiliary(HeapCell* heapCell)
146 {
147     if (!heapCell)
148         return;
149 
150     ASSERT(!m_isCheckingForDefaultMarkViolation);
151 
152     auto validateCell = [&amp;] (JSCell* jsCell) {
153         StructureID structureID = jsCell-&gt;structureID();
154 
155         auto die = [&amp;] (const char* text) {
156             WTF::dataFile().atomically(
157                 [&amp;] (PrintStream&amp; out) {
158                     out.print(text);
159                     out.print(&quot;GC type: &quot;, heap()-&gt;collectionScope(), &quot;\n&quot;);
160                     out.print(&quot;Object at: &quot;, RawPointer(jsCell), &quot;\n&quot;);
161 #if USE(JSVALUE64)
162                     out.print(&quot;Structure ID: &quot;, structureID, &quot; (0x&quot;, format(&quot;%x&quot;, structureID), &quot;)\n&quot;);
163                     out.print(&quot;Structure ID table size: &quot;, heap()-&gt;structureIDTable().size(), &quot;\n&quot;);
164 #else
165                     out.print(&quot;Structure: &quot;, RawPointer(structureID), &quot;\n&quot;);
166 #endif
167                     out.print(&quot;Object contents:&quot;);
168                     for (unsigned i = 0; i &lt; 2; ++i)
169                         out.print(&quot; &quot;, format(&quot;0x%016llx&quot;, bitwise_cast&lt;uint64_t*&gt;(jsCell)[i]));
170                     out.print(&quot;\n&quot;);
171                     CellContainer container = jsCell-&gt;cellContainer();
172                     out.print(&quot;Is marked: &quot;, container.isMarked(jsCell), &quot;\n&quot;);
173                     out.print(&quot;Is newly allocated: &quot;, container.isNewlyAllocated(jsCell), &quot;\n&quot;);
174                     if (container.isMarkedBlock()) {
175                         MarkedBlock&amp; block = container.markedBlock();
176                         out.print(&quot;Block: &quot;, RawPointer(&amp;block), &quot;\n&quot;);
177                         block.handle().dumpState(out);
178                         out.print(&quot;\n&quot;);
179                         out.print(&quot;Is marked raw: &quot;, block.isMarkedRaw(jsCell), &quot;\n&quot;);
180                         out.print(&quot;Marking version: &quot;, block.markingVersion(), &quot;\n&quot;);
181                         out.print(&quot;Heap marking version: &quot;, heap()-&gt;objectSpace().markingVersion(), &quot;\n&quot;);
182                         out.print(&quot;Is newly allocated raw: &quot;, block.isNewlyAllocated(jsCell), &quot;\n&quot;);
183                         out.print(&quot;Newly allocated version: &quot;, block.newlyAllocatedVersion(), &quot;\n&quot;);
184                         out.print(&quot;Heap newly allocated version: &quot;, heap()-&gt;objectSpace().newlyAllocatedVersion(), &quot;\n&quot;);
185                     }
186                     UNREACHABLE_FOR_PLATFORM();
187                 });
188         };
189 
190         // It&#39;s not OK for the structure to be null at any GC scan point. We must not GC while
191         // an object is not fully initialized.
192         if (!structureID)
193             die(&quot;GC scan found corrupt object: structureID is zero!\n&quot;);
194 
195         // It&#39;s not OK for the structure to be nuked at any GC scan point.
196         if (isNuked(structureID))
197             die(&quot;GC scan found object in bad state: structureID is nuked!\n&quot;);
198 
199 #if USE(JSVALUE64)
200         // This detects the worst of the badness.
201         if (!heap()-&gt;structureIDTable().isValid(structureID))
202             die(&quot;GC scan found corrupt object: structureID is invalid!\n&quot;);
203 #endif
204     };
205 
206     // In debug mode, we validate before marking since this makes it clearer what the problem
207     // was. It&#39;s also slower, so we don&#39;t do it normally.
208     if (!ASSERT_DISABLED &amp;&amp; isJSCellKind(heapCell-&gt;cellKind()))
209         validateCell(static_cast&lt;JSCell*&gt;(heapCell));
210 
211     if (Heap::testAndSetMarked(m_markingVersion, heapCell))
212         return;
213 
214     switch (heapCell-&gt;cellKind()) {
215     case HeapCell::JSCell:
216     case HeapCell::JSCellWithInteriorPointers: {
217         // We have ample budget to perform validation here.
218 
219         JSCell* jsCell = static_cast&lt;JSCell*&gt;(heapCell);
220         validateCell(jsCell);
221 
222         jsCell-&gt;setCellState(CellState::PossiblyGrey);
223 
224         appendToMarkStack(jsCell);
225         return;
226     }
227 
228     case HeapCell::Auxiliary: {
229         noteLiveAuxiliaryCell(heapCell);
230         return;
231     } }
232 }
233 
234 void SlotVisitor::appendSlow(JSCell* cell, Dependency dependency)
235 {
236     if (UNLIKELY(m_heapSnapshotBuilder))
237         m_heapSnapshotBuilder-&gt;appendEdge(m_currentCell, cell, m_rootMarkReason);
238 
239     appendHiddenSlowImpl(cell, dependency);
240 }
241 
242 void SlotVisitor::appendHiddenSlow(JSCell* cell, Dependency dependency)
243 {
244     appendHiddenSlowImpl(cell, dependency);
245 }
246 
247 ALWAYS_INLINE void SlotVisitor::appendHiddenSlowImpl(JSCell* cell, Dependency dependency)
248 {
249     ASSERT(!m_isCheckingForDefaultMarkViolation);
250 
251 #if ENABLE(GC_VALIDATION)
252     validate(cell);
253 #endif
254 
255     if (cell-&gt;isLargeAllocation())
256         setMarkedAndAppendToMarkStack(cell-&gt;largeAllocation(), cell, dependency);
257     else
258         setMarkedAndAppendToMarkStack(cell-&gt;markedBlock(), cell, dependency);
259 }
260 
261 template&lt;typename ContainerType&gt;
262 ALWAYS_INLINE void SlotVisitor::setMarkedAndAppendToMarkStack(ContainerType&amp; container, JSCell* cell, Dependency dependency)
263 {
264     if (container.testAndSetMarked(cell, dependency))
265         return;
266 
267     ASSERT(cell-&gt;structure());
268 
269     // Indicate that the object is grey and that:
270     // In case of concurrent GC: it&#39;s the first time it is grey in this GC cycle.
271     // In case of eden collection: it&#39;s a new object that became grey rather than an old remembered object.
272     cell-&gt;setCellState(CellState::PossiblyGrey);
273 
274     appendToMarkStack(container, cell);
275 }
276 
277 void SlotVisitor::appendToMarkStack(JSCell* cell)
278 {
279     if (cell-&gt;isLargeAllocation())
280         appendToMarkStack(cell-&gt;largeAllocation(), cell);
281     else
282         appendToMarkStack(cell-&gt;markedBlock(), cell);
283 }
284 
285 template&lt;typename ContainerType&gt;
286 ALWAYS_INLINE void SlotVisitor::appendToMarkStack(ContainerType&amp; container, JSCell* cell)
287 {
288     ASSERT(Heap::isMarked(cell));
289     ASSERT(!cell-&gt;isZapped());
290 
291     container.noteMarked();
292 
293     m_visitCount++;
294     m_bytesVisited += container.cellSize();
295 
296     m_collectorStack.append(cell);
297 }
298 
299 void SlotVisitor::appendToMutatorMarkStack(const JSCell* cell)
300 {
301     m_mutatorStack.append(cell);
302 }
303 
304 void SlotVisitor::markAuxiliary(const void* base)
305 {
306     HeapCell* cell = bitwise_cast&lt;HeapCell*&gt;(base);
307 
308     ASSERT(cell-&gt;heap() == heap());
309 
310     if (Heap::testAndSetMarked(m_markingVersion, cell))
311         return;
312 
313     noteLiveAuxiliaryCell(cell);
314 }
315 
316 void SlotVisitor::noteLiveAuxiliaryCell(HeapCell* cell)
317 {
318     // We get here once per GC under these circumstances:
319     //
320     // Eden collection: if the cell was allocated since the last collection and is live somehow.
321     //
322     // Full collection: if the cell is live somehow.
323 
324     CellContainer container = cell-&gt;cellContainer();
325 
326     container.assertValidCell(vm(), cell);
327     container.noteMarked();
328 
329     m_visitCount++;
330 
331     size_t cellSize = container.cellSize();
332     m_bytesVisited += cellSize;
333     m_nonCellVisitCount += cellSize;
334 }
335 
336 class SetCurrentCellScope {
337 public:
338     SetCurrentCellScope(SlotVisitor&amp; visitor, const JSCell* cell)
339         : m_visitor(visitor)
340     {
341         ASSERT(!m_visitor.m_currentCell);
342         m_visitor.m_currentCell = const_cast&lt;JSCell*&gt;(cell);
343     }
344 
345     ~SetCurrentCellScope()
346     {
347         ASSERT(m_visitor.m_currentCell);
348         m_visitor.m_currentCell = nullptr;
349     }
350 
351 private:
352     SlotVisitor&amp; m_visitor;
353 };
354 
355 ALWAYS_INLINE void SlotVisitor::visitChildren(const JSCell* cell)
356 {
357     ASSERT(Heap::isMarked(cell));
358 
359     SetCurrentCellScope currentCellScope(*this, cell);
360 
361     if (false) {
362         dataLog(&quot;Visiting &quot;, RawPointer(cell));
363         if (!m_isFirstVisit)
364             dataLog(&quot; (subsequent)&quot;);
365         dataLog(&quot;\n&quot;);
366     }
367 
368     // Funny story: it&#39;s possible for the object to be black already, if we barrier the object at
369     // about the same time that it&#39;s marked. That&#39;s fine. It&#39;s a gnarly and super-rare race. It&#39;s
370     // not clear to me that it would be correct or profitable to bail here if the object is already
371     // black.
372 
373     cell-&gt;setCellState(CellState::PossiblyBlack);
374 
375     WTF::storeLoadFence();
376 
377     switch (cell-&gt;type()) {
378     case StringType:
379         JSString::visitChildren(const_cast&lt;JSCell*&gt;(cell), *this);
380         break;
381 
382     case FinalObjectType:
383         JSFinalObject::visitChildren(const_cast&lt;JSCell*&gt;(cell), *this);
384         break;
385 
386     case ArrayType:
387         JSArray::visitChildren(const_cast&lt;JSCell*&gt;(cell), *this);
388         break;
389 
390     default:
391         // FIXME: This could be so much better.
392         // https://bugs.webkit.org/show_bug.cgi?id=162462
393         cell-&gt;methodTable(vm())-&gt;visitChildren(const_cast&lt;JSCell*&gt;(cell), *this);
394         break;
395     }
396 
397     if (UNLIKELY(m_heapSnapshotBuilder)) {
398         if (m_isFirstVisit)
399             m_heapSnapshotBuilder-&gt;appendNode(const_cast&lt;JSCell*&gt;(cell));
400     }
401 }
402 
403 void SlotVisitor::visitAsConstraint(const JSCell* cell)
404 {
405     m_isFirstVisit = false;
406     visitChildren(cell);
407 }
408 
409 inline void SlotVisitor::propagateExternalMemoryVisitedIfNecessary()
410 {
411     if (m_isFirstVisit) {
412         if (m_extraMemorySize.hasOverflowed())
413             heap()-&gt;reportExtraMemoryVisited(std::numeric_limits&lt;size_t&gt;::max());
414         else if (m_extraMemorySize)
415             heap()-&gt;reportExtraMemoryVisited(m_extraMemorySize.unsafeGet());
416         m_extraMemorySize = 0;
417     }
418 }
419 
420 void SlotVisitor::donateKnownParallel(MarkStackArray&amp; from, MarkStackArray&amp; to)
421 {
422     // NOTE: Because we re-try often, we can afford to be conservative, and
423     // assume that donating is not profitable.
424 
425     // Avoid locking when a thread reaches a dead end in the object graph.
426     if (from.size() &lt; 2)
427         return;
428 
429     // If there&#39;s already some shared work queued up, be conservative and assume
430     // that donating more is not profitable.
431     if (to.size())
432         return;
433 
434     // If we&#39;re contending on the lock, be conservative and assume that another
435     // thread is already donating.
436     std::unique_lock&lt;Lock&gt; lock(m_heap.m_markingMutex, std::try_to_lock);
437     if (!lock.owns_lock())
438         return;
439 
440     // Otherwise, assume that a thread will go idle soon, and donate.
441     from.donateSomeCellsTo(to);
442 
443     m_heap.m_markingConditionVariable.notifyAll();
444 }
445 
446 void SlotVisitor::donateKnownParallel()
447 {
448     forEachMarkStack(
449         [&amp;] (MarkStackArray&amp; stack) -&gt; IterationStatus {
450             donateKnownParallel(stack, correspondingGlobalStack(stack));
451             return IterationStatus::Continue;
452         });
453 }
454 
455 void SlotVisitor::updateMutatorIsStopped(const AbstractLocker&amp;)
456 {
457     m_mutatorIsStopped = (m_heap.worldIsStopped() &amp; m_canOptimizeForStoppedMutator);
458 }
459 
460 void SlotVisitor::updateMutatorIsStopped()
461 {
462     if (mutatorIsStoppedIsUpToDate())
463         return;
464     updateMutatorIsStopped(holdLock(m_rightToRun));
465 }
466 
467 bool SlotVisitor::hasAcknowledgedThatTheMutatorIsResumed() const
468 {
469     return !m_mutatorIsStopped;
470 }
471 
472 bool SlotVisitor::mutatorIsStoppedIsUpToDate() const
473 {
474     return m_mutatorIsStopped == (m_heap.worldIsStopped() &amp; m_canOptimizeForStoppedMutator);
475 }
476 
477 void SlotVisitor::optimizeForStoppedMutator()
478 {
479     m_canOptimizeForStoppedMutator = true;
480 }
481 
482 NEVER_INLINE void SlotVisitor::drain(MonotonicTime timeout)
483 {
484     if (!m_isInParallelMode) {
485         dataLog(&quot;FATAL: attempting to drain when not in parallel mode.\n&quot;);
486         RELEASE_ASSERT_NOT_REACHED();
487     }
488 
489     auto locker = holdLock(m_rightToRun);
490 
491     while (!hasElapsed(timeout)) {
492         updateMutatorIsStopped(locker);
493         IterationStatus status = forEachMarkStack(
494             [&amp;] (MarkStackArray&amp; stack) -&gt; IterationStatus {
495                 if (stack.isEmpty())
496                     return IterationStatus::Continue;
497 
498                 stack.refill();
499 
500                 m_isFirstVisit = (&amp;stack == &amp;m_collectorStack);
501 
502                 for (unsigned countdown = Options::minimumNumberOfScansBetweenRebalance(); stack.canRemoveLast() &amp;&amp; countdown--;)
503                     visitChildren(stack.removeLast());
504                 return IterationStatus::Done;
505             });
506         propagateExternalMemoryVisitedIfNecessary();
507         if (status == IterationStatus::Continue)
508             break;
509 
510         m_rightToRun.safepoint();
511         donateKnownParallel();
512     }
513 }
514 
515 size_t SlotVisitor::performIncrementOfDraining(size_t bytesRequested)
516 {
517     RELEASE_ASSERT(m_isInParallelMode);
518 
519     size_t cellsRequested = bytesRequested / MarkedBlock::atomSize;
520     {
521         auto locker = holdLock(m_heap.m_markingMutex);
522         forEachMarkStack(
523             [&amp;] (MarkStackArray&amp; stack) -&gt; IterationStatus {
524                 cellsRequested -= correspondingGlobalStack(stack).transferTo(stack, cellsRequested);
525                 return cellsRequested ? IterationStatus::Continue : IterationStatus::Done;
526             });
527     }
528 
529     size_t cellBytesVisited = 0;
530     m_nonCellVisitCount = 0;
531 
532     auto bytesVisited = [&amp;] () -&gt; size_t {
533         return cellBytesVisited + m_nonCellVisitCount;
534     };
535 
536     auto isDone = [&amp;] () -&gt; bool {
537         return bytesVisited() &gt;= bytesRequested;
538     };
539 
540     {
541         auto locker = holdLock(m_rightToRun);
542 
543         while (!isDone()) {
544             updateMutatorIsStopped(locker);
545             IterationStatus status = forEachMarkStack(
546                 [&amp;] (MarkStackArray&amp; stack) -&gt; IterationStatus {
547                     if (stack.isEmpty() || isDone())
548                         return IterationStatus::Continue;
549 
550                     stack.refill();
551 
552                     m_isFirstVisit = (&amp;stack == &amp;m_collectorStack);
553 
554                     unsigned countdown = Options::minimumNumberOfScansBetweenRebalance();
555                     while (countdown &amp;&amp; stack.canRemoveLast() &amp;&amp; !isDone()) {
556                         const JSCell* cell = stack.removeLast();
557                         cellBytesVisited += cell-&gt;cellSize();
558                         visitChildren(cell);
559                         countdown--;
560                     }
561                     return IterationStatus::Done;
562                 });
563             propagateExternalMemoryVisitedIfNecessary();
564             if (status == IterationStatus::Continue)
565                 break;
566             m_rightToRun.safepoint();
567             donateKnownParallel();
568         }
569     }
570 
571     donateAll();
572 
573     return bytesVisited();
574 }
575 
576 bool SlotVisitor::didReachTermination()
577 {
578     LockHolder locker(m_heap.m_markingMutex);
579     return didReachTermination(locker);
580 }
581 
582 bool SlotVisitor::didReachTermination(const AbstractLocker&amp; locker)
583 {
584     return !m_heap.m_numberOfActiveParallelMarkers
585         &amp;&amp; !hasWork(locker);
586 }
587 
588 bool SlotVisitor::hasWork(const AbstractLocker&amp;)
589 {
590     return !isEmpty()
591         || !m_heap.m_sharedCollectorMarkStack-&gt;isEmpty()
592         || !m_heap.m_sharedMutatorMarkStack-&gt;isEmpty();
593 }
594 
595 NEVER_INLINE SlotVisitor::SharedDrainResult SlotVisitor::drainFromShared(SharedDrainMode sharedDrainMode, MonotonicTime timeout)
596 {
597     ASSERT(m_isInParallelMode);
598 
599     ASSERT(Options::numberOfGCMarkers());
600 
601     bool isActive = false;
602     while (true) {
603         RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt; bonusTask;
604 
605         {
606             auto locker = holdLock(m_heap.m_markingMutex);
607             if (isActive)
608                 m_heap.m_numberOfActiveParallelMarkers--;
609             m_heap.m_numberOfWaitingParallelMarkers++;
610 
611             if (sharedDrainMode == MasterDrain) {
612                 while (true) {
613                     if (hasElapsed(timeout))
614                         return SharedDrainResult::TimedOut;
615 
616                     if (didReachTermination(locker)) {
617                         m_heap.m_markingConditionVariable.notifyAll();
618                         return SharedDrainResult::Done;
619                     }
620 
621                     if (hasWork(locker))
622                         break;
623 
624                     m_heap.m_markingConditionVariable.waitUntil(m_heap.m_markingMutex, timeout);
625                 }
626             } else {
627                 ASSERT(sharedDrainMode == SlaveDrain);
628 
629                 if (hasElapsed(timeout))
630                     return SharedDrainResult::TimedOut;
631 
632                 if (didReachTermination(locker)) {
633                     m_heap.m_markingConditionVariable.notifyAll();
634 
635                     // If we&#39;re in concurrent mode, then we know that the mutator will eventually do
636                     // the right thing because:
637                     // - It&#39;s possible that the collector has the conn. In that case, the collector will
638                     //   wake up from the notification above. This will happen if the app released heap
639                     //   access. Native apps can spend a lot of time with heap access released.
640                     // - It&#39;s possible that the mutator will allocate soon. Then it will check if we
641                     //   reached termination. This is the most likely outcome in programs that allocate
642                     //   a lot.
643                     // - WebCore never releases access. But WebCore has a runloop. The runloop will check
644                     //   if we reached termination.
645                     // So, this tells the runloop that it&#39;s got things to do.
646                     m_heap.m_stopIfNecessaryTimer-&gt;scheduleSoon();
647                 }
648 
649                 auto isReady = [&amp;] () -&gt; bool {
650                     return hasWork(locker)
651                         || m_heap.m_bonusVisitorTask
652                         || m_heap.m_parallelMarkersShouldExit;
653                 };
654 
655                 m_heap.m_markingConditionVariable.waitUntil(m_heap.m_markingMutex, timeout, isReady);
656 
657                 if (!hasWork(locker)
658                     &amp;&amp; m_heap.m_bonusVisitorTask)
659                     bonusTask = m_heap.m_bonusVisitorTask;
660 
661                 if (m_heap.m_parallelMarkersShouldExit)
662                     return SharedDrainResult::Done;
663             }
664 
665             if (!bonusTask &amp;&amp; isEmpty()) {
666                 forEachMarkStack(
667                     [&amp;] (MarkStackArray&amp; stack) -&gt; IterationStatus {
668                         stack.stealSomeCellsFrom(
669                             correspondingGlobalStack(stack),
670                             m_heap.m_numberOfWaitingParallelMarkers);
671                         return IterationStatus::Continue;
672                     });
673             }
674 
675             m_heap.m_numberOfActiveParallelMarkers++;
676             m_heap.m_numberOfWaitingParallelMarkers--;
677         }
678 
679         if (bonusTask) {
680             bonusTask-&gt;run(*this);
681 
682             // The main thread could still be running, and may run for a while. Unless we clear the task
683             // ourselves, we will keep looping around trying to run the task.
684             {
685                 auto locker = holdLock(m_heap.m_markingMutex);
686                 if (m_heap.m_bonusVisitorTask == bonusTask)
687                     m_heap.m_bonusVisitorTask = nullptr;
688                 bonusTask = nullptr;
689                 m_heap.m_markingConditionVariable.notifyAll();
690             }
691         } else {
692             RELEASE_ASSERT(!isEmpty());
693             drain(timeout);
694         }
695 
696         isActive = true;
697     }
698 }
699 
700 SlotVisitor::SharedDrainResult SlotVisitor::drainInParallel(MonotonicTime timeout)
701 {
702     donateAndDrain(timeout);
703     return drainFromShared(MasterDrain, timeout);
704 }
705 
706 SlotVisitor::SharedDrainResult SlotVisitor::drainInParallelPassively(MonotonicTime timeout)
707 {
708     ASSERT(m_isInParallelMode);
709 
710     ASSERT(Options::numberOfGCMarkers());
711 
712     if (Options::numberOfGCMarkers() == 1
713         || (m_heap.m_worldState.load() &amp; Heap::mutatorWaitingBit)
714         || !m_heap.hasHeapAccess()
715         || m_heap.worldIsStopped()) {
716         // This is an optimization over drainInParallel() when we have a concurrent mutator but
717         // otherwise it is not profitable.
718         return drainInParallel(timeout);
719     }
720 
721     donateAll(holdLock(m_heap.m_markingMutex));
722     return waitForTermination(timeout);
723 }
724 
725 SlotVisitor::SharedDrainResult SlotVisitor::waitForTermination(MonotonicTime timeout)
726 {
727     auto locker = holdLock(m_heap.m_markingMutex);
728     for (;;) {
729         if (hasElapsed(timeout))
730             return SharedDrainResult::TimedOut;
731 
732         if (didReachTermination(locker)) {
733             m_heap.m_markingConditionVariable.notifyAll();
734             return SharedDrainResult::Done;
735         }
736 
737         m_heap.m_markingConditionVariable.waitUntil(m_heap.m_markingMutex, timeout);
738     }
739 }
740 
741 void SlotVisitor::donateAll()
742 {
743     if (isEmpty())
744         return;
745 
746     donateAll(holdLock(m_heap.m_markingMutex));
747 }
748 
749 void SlotVisitor::donateAll(const AbstractLocker&amp;)
750 {
751     forEachMarkStack(
752         [&amp;] (MarkStackArray&amp; stack) -&gt; IterationStatus {
753             stack.transferTo(correspondingGlobalStack(stack));
754             return IterationStatus::Continue;
755         });
756 
757     m_heap.m_markingConditionVariable.notifyAll();
758 }
759 
760 void SlotVisitor::donate()
761 {
762     if (!m_isInParallelMode) {
763         dataLog(&quot;FATAL: Attempting to donate when not in parallel mode.\n&quot;);
764         RELEASE_ASSERT_NOT_REACHED();
765     }
766 
767     if (Options::numberOfGCMarkers() == 1)
768         return;
769 
770     donateKnownParallel();
771 }
772 
773 void SlotVisitor::donateAndDrain(MonotonicTime timeout)
774 {
775     donate();
776     drain(timeout);
777 }
778 
779 void SlotVisitor::didRace(const VisitRaceKey&amp; race)
780 {
781     if (Options::verboseVisitRace())
782         dataLog(toCString(&quot;GC visit race: &quot;, race, &quot;\n&quot;));
783 
784     auto locker = holdLock(heap()-&gt;m_raceMarkStackLock);
785     JSCell* cell = race.cell();
786     cell-&gt;setCellState(CellState::PossiblyGrey);
787     heap()-&gt;m_raceMarkStack-&gt;append(cell);
788 }
789 
790 void SlotVisitor::dump(PrintStream&amp; out) const
791 {
792     out.print(&quot;Collector: [&quot;, pointerListDump(collectorMarkStack()), &quot;], Mutator: [&quot;, pointerListDump(mutatorMarkStack()), &quot;]&quot;);
793 }
794 
795 MarkStackArray&amp; SlotVisitor::correspondingGlobalStack(MarkStackArray&amp; stack)
796 {
797     if (&amp;stack == &amp;m_collectorStack)
798         return *m_heap.m_sharedCollectorMarkStack;
799     RELEASE_ASSERT(&amp;stack == &amp;m_mutatorStack);
800     return *m_heap.m_sharedMutatorMarkStack;
801 }
802 
803 void SlotVisitor::addParallelConstraintTask(RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt; task)
804 {
805     RELEASE_ASSERT(m_currentSolver);
806     RELEASE_ASSERT(m_currentConstraint);
807     RELEASE_ASSERT(task);
808 
809     m_currentSolver-&gt;addParallelTask(task, *m_currentConstraint);
810 }
811 
812 } // namespace JSC
    </pre>
  </body>
</html>