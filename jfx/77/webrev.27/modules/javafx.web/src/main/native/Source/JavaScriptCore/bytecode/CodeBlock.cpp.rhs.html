<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/CodeBlock.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.
   3  * Copyright (C) 2008 Cameron Zwarich &lt;cwzwarich@uwaterloo.ca&gt;
   4  *
   5  * Redistribution and use in source and binary forms, with or without
   6  * modification, are permitted provided that the following conditions
   7  * are met:
   8  *
   9  * 1.  Redistributions of source code must retain the above copyright
  10  *     notice, this list of conditions and the following disclaimer.
  11  * 2.  Redistributions in binary form must reproduce the above copyright
  12  *     notice, this list of conditions and the following disclaimer in the
  13  *     documentation and/or other materials provided with the distribution.
  14  * 3.  Neither the name of Apple Inc. (&quot;Apple&quot;) nor the names of
  15  *     its contributors may be used to endorse or promote products derived
  16  *     from this software without specific prior written permission.
  17  *
  18  * THIS SOFTWARE IS PROVIDED BY APPLE AND ITS CONTRIBUTORS &quot;AS IS&quot; AND ANY
  19  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
  20  * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  21  * DISCLAIMED. IN NO EVENT SHALL APPLE OR ITS CONTRIBUTORS BE LIABLE FOR ANY
  22  * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
  23  * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
  24  * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
  25  * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  26  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
  27  * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  28  */
  29 
  30 #include &quot;config.h&quot;
  31 #include &quot;CodeBlock.h&quot;
  32 
  33 #include &quot;ArithProfile.h&quot;
  34 #include &quot;BasicBlockLocation.h&quot;
  35 #include &quot;BytecodeDumper.h&quot;
  36 #include &quot;BytecodeGenerator.h&quot;
  37 #include &quot;BytecodeLivenessAnalysis.h&quot;
  38 #include &quot;BytecodeStructs.h&quot;
  39 #include &quot;BytecodeUseDef.h&quot;
  40 #include &quot;CallLinkStatus.h&quot;
  41 #include &quot;CodeBlockInlines.h&quot;
  42 #include &quot;CodeBlockSet.h&quot;
  43 #include &quot;DFGCapabilities.h&quot;
  44 #include &quot;DFGCommon.h&quot;
  45 #include &quot;DFGDriver.h&quot;
  46 #include &quot;DFGJITCode.h&quot;
  47 #include &quot;DFGWorklist.h&quot;
  48 #include &quot;Debugger.h&quot;
  49 #include &quot;EvalCodeBlock.h&quot;
  50 #include &quot;FullCodeOrigin.h&quot;
  51 #include &quot;FunctionCodeBlock.h&quot;
  52 #include &quot;FunctionExecutableDump.h&quot;
  53 #include &quot;GetPutInfo.h&quot;
  54 #include &quot;InlineCallFrame.h&quot;
  55 #include &quot;Instruction.h&quot;
  56 #include &quot;InstructionStream.h&quot;
  57 #include &quot;InterpreterInlines.h&quot;
  58 #include &quot;IsoCellSetInlines.h&quot;
  59 #include &quot;JIT.h&quot;
  60 #include &quot;JITMathIC.h&quot;
  61 #include &quot;JSBigInt.h&quot;
  62 #include &quot;JSCInlines.h&quot;
  63 #include &quot;JSCJSValue.h&quot;
  64 #include &quot;JSFunction.h&quot;
  65 #include &quot;JSLexicalEnvironment.h&quot;
  66 #include &quot;JSModuleEnvironment.h&quot;
  67 #include &quot;JSSet.h&quot;
  68 #include &quot;JSString.h&quot;
  69 #include &quot;JSTemplateObjectDescriptor.h&quot;
  70 #include &quot;LLIntData.h&quot;
  71 #include &quot;LLIntEntrypoint.h&quot;
  72 #include &quot;LLIntPrototypeLoadAdaptiveStructureWatchpoint.h&quot;
  73 #include &quot;LowLevelInterpreter.h&quot;
  74 #include &quot;MetadataTable.h&quot;
  75 #include &quot;ModuleProgramCodeBlock.h&quot;
  76 #include &quot;ObjectAllocationProfileInlines.h&quot;
  77 #include &quot;OpcodeInlines.h&quot;
  78 #include &quot;PCToCodeOriginMap.h&quot;
  79 #include &quot;PolymorphicAccess.h&quot;
  80 #include &quot;ProfilerDatabase.h&quot;
  81 #include &quot;ProgramCodeBlock.h&quot;
  82 #include &quot;ReduceWhitespace.h&quot;
  83 #include &quot;Repatch.h&quot;
  84 #include &quot;SlotVisitorInlines.h&quot;
  85 #include &quot;StackVisitor.h&quot;
  86 #include &quot;StructureStubInfo.h&quot;
  87 #include &quot;TypeLocationCache.h&quot;
  88 #include &quot;TypeProfiler.h&quot;
  89 #include &quot;VMInlines.h&quot;
  90 #include &lt;wtf/BagToHashMap.h&gt;
  91 #include &lt;wtf/CommaPrinter.h&gt;
  92 #include &lt;wtf/Forward.h&gt;
  93 #include &lt;wtf/SimpleStats.h&gt;
  94 #include &lt;wtf/StringPrintStream.h&gt;
  95 #include &lt;wtf/text/StringConcatenateNumbers.h&gt;
  96 #include &lt;wtf/text/UniquedStringImpl.h&gt;
  97 
  98 #if ENABLE(ASSEMBLER)
  99 #include &quot;RegisterAtOffsetList.h&quot;
 100 #endif
 101 
 102 #if ENABLE(DFG_JIT)
 103 #include &quot;DFGOperations.h&quot;
 104 #endif
 105 
 106 #if ENABLE(FTL_JIT)
 107 #include &quot;FTLJITCode.h&quot;
 108 #endif
 109 
 110 namespace JSC {
<a name="1" id="anc1"></a>


 111 
 112 const ClassInfo CodeBlock::s_info = {
 113     &quot;CodeBlock&quot;, nullptr, nullptr, nullptr,
 114     CREATE_METHOD_TABLE(CodeBlock)
 115 };
 116 
 117 CString CodeBlock::inferredName() const
 118 {
 119     switch (codeType()) {
 120     case GlobalCode:
 121         return &quot;&lt;global&gt;&quot;;
 122     case EvalCode:
 123         return &quot;&lt;eval&gt;&quot;;
 124     case FunctionCode:
<a name="2" id="anc2"></a><span class="line-modified"> 125         return jsCast&lt;FunctionExecutable*&gt;(ownerExecutable())-&gt;ecmaName().utf8();</span>
 126     case ModuleCode:
 127         return &quot;&lt;module&gt;&quot;;
 128     default:
 129         CRASH();
 130         return CString(&quot;&quot;, 0);
 131     }
 132 }
 133 
 134 bool CodeBlock::hasHash() const
 135 {
 136     return !!m_hash;
 137 }
 138 
 139 bool CodeBlock::isSafeToComputeHash() const
 140 {
 141     return !isCompilationThread();
 142 }
 143 
 144 CodeBlockHash CodeBlock::hash() const
 145 {
 146     if (!m_hash) {
 147         RELEASE_ASSERT(isSafeToComputeHash());
 148         m_hash = CodeBlockHash(ownerExecutable()-&gt;source(), specializationKind());
 149     }
 150     return m_hash;
 151 }
 152 
 153 CString CodeBlock::sourceCodeForTools() const
 154 {
 155     if (codeType() != FunctionCode)
 156         return ownerExecutable()-&gt;source().toUTF8();
 157 
 158     SourceProvider* provider = source().provider();
 159     FunctionExecutable* executable = jsCast&lt;FunctionExecutable*&gt;(ownerExecutable());
 160     UnlinkedFunctionExecutable* unlinked = executable-&gt;unlinkedExecutable();
 161     unsigned unlinkedStartOffset = unlinked-&gt;startOffset();
 162     unsigned linkedStartOffset = executable-&gt;source().startOffset();
 163     int delta = linkedStartOffset - unlinkedStartOffset;
 164     unsigned rangeStart = delta + unlinked-&gt;unlinkedFunctionNameStart();
 165     unsigned rangeEnd = delta + unlinked-&gt;startOffset() + unlinked-&gt;sourceLength();
 166     return toCString(
 167         &quot;function &quot;,
 168         provider-&gt;source().substring(rangeStart, rangeEnd - rangeStart).utf8());
 169 }
 170 
 171 CString CodeBlock::sourceCodeOnOneLine() const
 172 {
 173     return reduceWhitespace(sourceCodeForTools());
 174 }
 175 
 176 CString CodeBlock::hashAsStringIfPossible() const
 177 {
 178     if (hasHash() || isSafeToComputeHash())
 179         return toCString(hash());
 180     return &quot;&lt;no-hash&gt;&quot;;
 181 }
 182 
<a name="3" id="anc3"></a><span class="line-modified"> 183 void CodeBlock::dumpAssumingJITType(PrintStream&amp; out, JITType jitType) const</span>
 184 {
 185     out.print(inferredName(), &quot;#&quot;, hashAsStringIfPossible());
 186     out.print(&quot;:[&quot;, RawPointer(this), &quot;-&gt;&quot;);
 187     if (!!m_alternative)
 188         out.print(RawPointer(alternative()), &quot;-&gt;&quot;);
 189     out.print(RawPointer(ownerExecutable()), &quot;, &quot;, jitType, codeType());
 190 
 191     if (codeType() == FunctionCode)
 192         out.print(specializationKind());
<a name="4" id="anc4"></a><span class="line-modified"> 193     out.print(&quot;, &quot;, instructionsSize());</span>
<span class="line-modified"> 194     if (this-&gt;jitType() == JITType::BaselineJIT &amp;&amp; m_shouldAlwaysBeInlined)</span>
 195         out.print(&quot; (ShouldAlwaysBeInlined)&quot;);
 196     if (ownerExecutable()-&gt;neverInline())
 197         out.print(&quot; (NeverInline)&quot;);
 198     if (ownerExecutable()-&gt;neverOptimize())
 199         out.print(&quot; (NeverOptimize)&quot;);
 200     else if (ownerExecutable()-&gt;neverFTLOptimize())
 201         out.print(&quot; (NeverFTLOptimize)&quot;);
 202     if (ownerExecutable()-&gt;didTryToEnterInLoop())
 203         out.print(&quot; (DidTryToEnterInLoop)&quot;);
 204     if (ownerExecutable()-&gt;isStrictMode())
 205         out.print(&quot; (StrictMode)&quot;);
 206     if (m_didFailJITCompilation)
 207         out.print(&quot; (JITFail)&quot;);
<a name="5" id="anc5"></a><span class="line-modified"> 208     if (this-&gt;jitType() == JITType::BaselineJIT &amp;&amp; m_didFailFTLCompilation)</span>
 209         out.print(&quot; (FTLFail)&quot;);
<a name="6" id="anc6"></a><span class="line-modified"> 210     if (this-&gt;jitType() == JITType::BaselineJIT &amp;&amp; m_hasBeenCompiledWithFTL)</span>
 211         out.print(&quot; (HadFTLReplacement)&quot;);
 212     out.print(&quot;]&quot;);
 213 }
 214 
 215 void CodeBlock::dump(PrintStream&amp; out) const
 216 {
 217     dumpAssumingJITType(out, jitType());
 218 }
 219 
 220 void CodeBlock::dumpSource()
 221 {
 222     dumpSource(WTF::dataFile());
 223 }
 224 
 225 void CodeBlock::dumpSource(PrintStream&amp; out)
 226 {
 227     ScriptExecutable* executable = ownerExecutable();
 228     if (executable-&gt;isFunctionExecutable()) {
 229         FunctionExecutable* functionExecutable = reinterpret_cast&lt;FunctionExecutable*&gt;(executable);
 230         StringView source = functionExecutable-&gt;source().provider()-&gt;getRange(
 231             functionExecutable-&gt;parametersStartOffset(),
<a name="7" id="anc7"></a><span class="line-modified"> 232             functionExecutable-&gt;typeProfilingEndOffset(vm()) + 1); // Type profiling end offset is the character before the &#39;}&#39;.</span>
 233 
 234         out.print(&quot;function &quot;, inferredName(), source);
 235         return;
 236     }
 237     out.print(executable-&gt;source().view());
 238 }
 239 
 240 void CodeBlock::dumpBytecode()
 241 {
 242     dumpBytecode(WTF::dataFile());
 243 }
 244 
 245 void CodeBlock::dumpBytecode(PrintStream&amp; out)
 246 {
 247     ICStatusMap statusMap;
 248     getICStatusMap(statusMap);
 249     BytecodeDumper&lt;CodeBlock&gt;::dumpBlock(this, instructions(), out, statusMap);
 250 }
 251 
 252 void CodeBlock::dumpBytecode(PrintStream&amp; out, const InstructionStream::Ref&amp; it, const ICStatusMap&amp; statusMap)
 253 {
 254     BytecodeDumper&lt;CodeBlock&gt;::dumpBytecode(this, out, it, statusMap);
 255 }
 256 
 257 void CodeBlock::dumpBytecode(PrintStream&amp; out, unsigned bytecodeOffset, const ICStatusMap&amp; statusMap)
 258 {
 259     const auto it = instructions().at(bytecodeOffset);
 260     dumpBytecode(out, it, statusMap);
 261 }
 262 
 263 namespace {
 264 
 265 class PutToScopeFireDetail : public FireDetail {
 266 public:
 267     PutToScopeFireDetail(CodeBlock* codeBlock, const Identifier&amp; ident)
 268         : m_codeBlock(codeBlock)
 269         , m_ident(ident)
 270     {
 271     }
 272 
 273     void dump(PrintStream&amp; out) const override
 274     {
 275         out.print(&quot;Linking put_to_scope in &quot;, FunctionExecutableDump(jsCast&lt;FunctionExecutable*&gt;(m_codeBlock-&gt;ownerExecutable())), &quot; for &quot;, m_ident);
 276     }
 277 
 278 private:
 279     CodeBlock* m_codeBlock;
 280     const Identifier&amp; m_ident;
 281 };
 282 
 283 } // anonymous namespace
 284 
<a name="8" id="anc8"></a><span class="line-modified"> 285 CodeBlock::CodeBlock(VM&amp; vm, Structure* structure, CopyParsedBlockTag, CodeBlock&amp; other)</span>
<span class="line-modified"> 286     : JSCell(vm, structure)</span>
 287     , m_globalObject(other.m_globalObject)
 288     , m_shouldAlwaysBeInlined(true)
 289 #if ENABLE(JIT)
 290     , m_capabilityLevelState(DFG::CapabilityLevelNotSet)
 291 #endif
 292     , m_didFailJITCompilation(false)
 293     , m_didFailFTLCompilation(false)
 294     , m_hasBeenCompiledWithFTL(false)
 295     , m_numCalleeLocals(other.m_numCalleeLocals)
 296     , m_numVars(other.m_numVars)
 297     , m_numberOfArgumentsToSkip(other.m_numberOfArgumentsToSkip)
 298     , m_hasDebuggerStatement(false)
 299     , m_steppingMode(SteppingModeDisabled)
 300     , m_numBreakpoints(0)
<a name="9" id="anc9"></a><span class="line-modified"> 301     , m_bytecodeCost(other.m_bytecodeCost)</span>
 302     , m_scopeRegister(other.m_scopeRegister)
 303     , m_hash(other.m_hash)
<a name="10" id="anc10"></a><span class="line-modified"> 304     , m_unlinkedCode(other.vm(), this, other.m_unlinkedCode.get())</span>
<span class="line-modified"> 305     , m_ownerExecutable(other.vm(), this, other.m_ownerExecutable.get())</span>
 306     , m_vm(other.m_vm)
 307     , m_instructionsRawPointer(other.m_instructionsRawPointer)
 308     , m_constantRegisters(other.m_constantRegisters)
 309     , m_constantsSourceCodeRepresentation(other.m_constantsSourceCodeRepresentation)
 310     , m_functionDecls(other.m_functionDecls)
 311     , m_functionExprs(other.m_functionExprs)
 312     , m_osrExitCounter(0)
 313     , m_optimizationDelayCounter(0)
 314     , m_reoptimizationRetryCounter(0)
 315     , m_metadata(other.m_metadata)
 316     , m_creationTime(MonotonicTime::now())
 317 {
 318     ASSERT(heap()-&gt;isDeferred());
 319     ASSERT(m_scopeRegister.isLocal());
 320 
 321     ASSERT(source().provider());
 322     setNumParameters(other.numParameters());
 323 
<a name="11" id="anc11"></a><span class="line-modified"> 324     vm.heap.codeBlockSet().add(this);</span>
 325 }
 326 
 327 void CodeBlock::finishCreation(VM&amp; vm, CopyParsedBlockTag, CodeBlock&amp; other)
 328 {
 329     Base::finishCreation(vm);
 330     finishCreationCommon(vm);
 331 
 332     optimizeAfterWarmUp();
 333     jitAfterWarmUp();
 334 
 335     if (other.m_rareData) {
 336         createRareDataIfNecessary();
 337 
 338         m_rareData-&gt;m_exceptionHandlers = other.m_rareData-&gt;m_exceptionHandlers;
 339         m_rareData-&gt;m_switchJumpTables = other.m_rareData-&gt;m_switchJumpTables;
 340         m_rareData-&gt;m_stringSwitchJumpTables = other.m_rareData-&gt;m_stringSwitchJumpTables;
 341     }
 342 }
 343 
<a name="12" id="anc12"></a><span class="line-modified"> 344 CodeBlock::CodeBlock(VM&amp; vm, Structure* structure, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock* unlinkedCodeBlock, JSScope* scope)</span>
<span class="line-modified"> 345     : JSCell(vm, structure)</span>
<span class="line-modified"> 346     , m_globalObject(vm, this, scope-&gt;globalObject(vm))</span>
 347     , m_shouldAlwaysBeInlined(true)
 348 #if ENABLE(JIT)
 349     , m_capabilityLevelState(DFG::CapabilityLevelNotSet)
 350 #endif
 351     , m_didFailJITCompilation(false)
 352     , m_didFailFTLCompilation(false)
 353     , m_hasBeenCompiledWithFTL(false)
 354     , m_numCalleeLocals(unlinkedCodeBlock-&gt;numCalleeLocals())
 355     , m_numVars(unlinkedCodeBlock-&gt;numVars())
 356     , m_hasDebuggerStatement(false)
 357     , m_steppingMode(SteppingModeDisabled)
 358     , m_numBreakpoints(0)
 359     , m_scopeRegister(unlinkedCodeBlock-&gt;scopeRegister())
<a name="13" id="anc13"></a><span class="line-modified"> 360     , m_unlinkedCode(vm, this, unlinkedCodeBlock)</span>
<span class="line-modified"> 361     , m_ownerExecutable(vm, this, ownerExecutable)</span>
<span class="line-modified"> 362     , m_vm(&amp;vm)</span>
 363     , m_instructionsRawPointer(unlinkedCodeBlock-&gt;instructions().rawPointer())
 364     , m_osrExitCounter(0)
 365     , m_optimizationDelayCounter(0)
 366     , m_reoptimizationRetryCounter(0)
 367     , m_metadata(unlinkedCodeBlock-&gt;metadata().link())
 368     , m_creationTime(MonotonicTime::now())
 369 {
 370     ASSERT(heap()-&gt;isDeferred());
 371     ASSERT(m_scopeRegister.isLocal());
 372 
 373     ASSERT(source().provider());
 374     setNumParameters(unlinkedCodeBlock-&gt;numParameters());
 375 
<a name="14" id="anc14"></a><span class="line-modified"> 376     vm.heap.codeBlockSet().add(this);</span>
 377 }
 378 
 379 // The main purpose of this function is to generate linked bytecode from unlinked bytecode. The process
 380 // of linking is taking an abstract representation of bytecode and tying it to a GlobalObject and scope
 381 // chain. For example, this process allows us to cache the depth of lexical environment reads that reach
 382 // outside of this CodeBlock&#39;s compilation unit. It also allows us to generate particular constants that
 383 // we can&#39;t generate during unlinked bytecode generation. This process is not allowed to generate control
 384 // flow or introduce new locals. The reason for this is we rely on liveness analysis to be the same for
 385 // all the CodeBlocks of an UnlinkedCodeBlock. We rely on this fact by caching the liveness analysis
 386 // inside UnlinkedCodeBlock.
 387 bool CodeBlock::finishCreation(VM&amp; vm, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock* unlinkedCodeBlock,
 388     JSScope* scope)
 389 {
 390     Base::finishCreation(vm);
 391     finishCreationCommon(vm);
 392 
 393     auto throwScope = DECLARE_THROW_SCOPE(vm);
 394 
<a name="15" id="anc15"></a><span class="line-modified"> 395     if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes() || m_unlinkedCode-&gt;wasCompiledWithControlFlowProfilerOpcodes())</span>
 396         vm.functionHasExecutedCache()-&gt;removeUnexecutedRange(ownerExecutable-&gt;sourceID(), ownerExecutable-&gt;typeProfilingStartOffset(vm), ownerExecutable-&gt;typeProfilingEndOffset(vm));
 397 
<a name="16" id="anc16"></a><span class="line-modified"> 398     ScriptExecutable* topLevelExecutable = ownerExecutable-&gt;topLevelExecutable();</span>
<span class="line-added"> 399     setConstantRegisters(unlinkedCodeBlock-&gt;constantRegisters(), unlinkedCodeBlock-&gt;constantsSourceCodeRepresentation(), topLevelExecutable);</span>
 400     RETURN_IF_EXCEPTION(throwScope, false);
 401 
 402     for (unsigned i = 0; i &lt; LinkTimeConstantCount; i++) {
 403         LinkTimeConstant type = static_cast&lt;LinkTimeConstant&gt;(i);
 404         if (unsigned registerIndex = unlinkedCodeBlock-&gt;registerIndexForLinkTimeConstant(type))
 405             m_constantRegisters[registerIndex].set(vm, this, m_globalObject-&gt;jsCellForLinkTimeConstant(type));
 406     }
 407 
 408     // We already have the cloned symbol table for the module environment since we need to instantiate
 409     // the module environments before linking the code block. We replace the stored symbol table with the already cloned one.
 410     if (UnlinkedModuleProgramCodeBlock* unlinkedModuleProgramCodeBlock = jsDynamicCast&lt;UnlinkedModuleProgramCodeBlock*&gt;(vm, unlinkedCodeBlock)) {
 411         SymbolTable* clonedSymbolTable = jsCast&lt;ModuleProgramExecutable*&gt;(ownerExecutable)-&gt;moduleEnvironmentSymbolTable();
<a name="17" id="anc17"></a><span class="line-modified"> 412         if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes()) {</span>
 413             ConcurrentJSLocker locker(clonedSymbolTable-&gt;m_lock);
 414             clonedSymbolTable-&gt;prepareForTypeProfiling(locker);
 415         }
 416         replaceConstant(unlinkedModuleProgramCodeBlock-&gt;moduleEnvironmentSymbolTableConstantRegisterOffset(), clonedSymbolTable);
 417     }
 418 
<a name="18" id="anc18"></a><span class="line-modified"> 419     bool shouldUpdateFunctionHasExecutedCache = m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes() || m_unlinkedCode-&gt;wasCompiledWithControlFlowProfilerOpcodes();</span>
 420     m_functionDecls = RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt;(unlinkedCodeBlock-&gt;numberOfFunctionDecls());
 421     for (size_t count = unlinkedCodeBlock-&gt;numberOfFunctionDecls(), i = 0; i &lt; count; ++i) {
 422         UnlinkedFunctionExecutable* unlinkedExecutable = unlinkedCodeBlock-&gt;functionDecl(i);
 423         if (shouldUpdateFunctionHasExecutedCache)
 424             vm.functionHasExecutedCache()-&gt;insertUnexecutedRange(ownerExecutable-&gt;sourceID(), unlinkedExecutable-&gt;typeProfilingStartOffset(), unlinkedExecutable-&gt;typeProfilingEndOffset());
<a name="19" id="anc19"></a><span class="line-modified"> 425         m_functionDecls[i].set(vm, this, unlinkedExecutable-&gt;link(vm, topLevelExecutable, ownerExecutable-&gt;source()));</span>
 426     }
 427 
 428     m_functionExprs = RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt;(unlinkedCodeBlock-&gt;numberOfFunctionExprs());
 429     for (size_t count = unlinkedCodeBlock-&gt;numberOfFunctionExprs(), i = 0; i &lt; count; ++i) {
 430         UnlinkedFunctionExecutable* unlinkedExecutable = unlinkedCodeBlock-&gt;functionExpr(i);
 431         if (shouldUpdateFunctionHasExecutedCache)
 432             vm.functionHasExecutedCache()-&gt;insertUnexecutedRange(ownerExecutable-&gt;sourceID(), unlinkedExecutable-&gt;typeProfilingStartOffset(), unlinkedExecutable-&gt;typeProfilingEndOffset());
<a name="20" id="anc20"></a><span class="line-modified"> 433         m_functionExprs[i].set(vm, this, unlinkedExecutable-&gt;link(vm, topLevelExecutable, ownerExecutable-&gt;source()));</span>
 434     }
 435 
 436     if (unlinkedCodeBlock-&gt;hasRareData()) {
 437         createRareDataIfNecessary();
 438 
 439         setConstantIdentifierSetRegisters(vm, unlinkedCodeBlock-&gt;constantIdentifierSets());
 440         RETURN_IF_EXCEPTION(throwScope, false);
 441 
 442         if (size_t count = unlinkedCodeBlock-&gt;numberOfExceptionHandlers()) {
 443             m_rareData-&gt;m_exceptionHandlers.resizeToFit(count);
 444             for (size_t i = 0; i &lt; count; i++) {
 445                 const UnlinkedHandlerInfo&amp; unlinkedHandler = unlinkedCodeBlock-&gt;exceptionHandler(i);
 446                 HandlerInfo&amp; handler = m_rareData-&gt;m_exceptionHandlers[i];
 447 #if ENABLE(JIT)
<a name="21" id="anc21"></a><span class="line-modified"> 448                 auto instruction = instructions().at(unlinkedHandler.target);</span>
<span class="line-modified"> 449                 MacroAssemblerCodePtr&lt;BytecodePtrTag&gt; codePtr;</span>
<span class="line-modified"> 450                 if (instruction-&gt;isWide32())</span>
<span class="line-added"> 451                     codePtr = LLInt::getWide32CodePtr&lt;BytecodePtrTag&gt;(op_catch);</span>
<span class="line-added"> 452                 else if (instruction-&gt;isWide16())</span>
<span class="line-added"> 453                     codePtr = LLInt::getWide16CodePtr&lt;BytecodePtrTag&gt;(op_catch);</span>
<span class="line-added"> 454                 else</span>
<span class="line-added"> 455                     codePtr = LLInt::getCodePtr&lt;BytecodePtrTag&gt;(op_catch);</span>
 456                 handler.initialize(unlinkedHandler, CodeLocationLabel&lt;ExceptionHandlerPtrTag&gt;(codePtr.retagged&lt;ExceptionHandlerPtrTag&gt;()));
 457 #else
 458                 handler.initialize(unlinkedHandler);
 459 #endif
 460             }
 461         }
 462 
 463         if (size_t count = unlinkedCodeBlock-&gt;numberOfStringSwitchJumpTables()) {
 464             m_rareData-&gt;m_stringSwitchJumpTables.grow(count);
 465             for (size_t i = 0; i &lt; count; i++) {
 466                 UnlinkedStringJumpTable::StringOffsetTable::iterator ptr = unlinkedCodeBlock-&gt;stringSwitchJumpTable(i).offsetTable.begin();
 467                 UnlinkedStringJumpTable::StringOffsetTable::iterator end = unlinkedCodeBlock-&gt;stringSwitchJumpTable(i).offsetTable.end();
 468                 for (; ptr != end; ++ptr) {
 469                     OffsetLocation offset;
 470                     offset.branchOffset = ptr-&gt;value.branchOffset;
 471                     m_rareData-&gt;m_stringSwitchJumpTables[i].offsetTable.add(ptr-&gt;key, offset);
 472                 }
 473             }
 474         }
 475 
 476         if (size_t count = unlinkedCodeBlock-&gt;numberOfSwitchJumpTables()) {
 477             m_rareData-&gt;m_switchJumpTables.grow(count);
 478             for (size_t i = 0; i &lt; count; i++) {
 479                 UnlinkedSimpleJumpTable&amp; sourceTable = unlinkedCodeBlock-&gt;switchJumpTable(i);
 480                 SimpleJumpTable&amp; destTable = m_rareData-&gt;m_switchJumpTables[i];
 481                 destTable.branchOffsets = sourceTable.branchOffsets;
 482                 destTable.min = sourceTable.min;
 483             }
 484         }
 485     }
 486 
 487     // Bookkeep the strongly referenced module environments.
 488     HashSet&lt;JSModuleEnvironment*&gt; stronglyReferencedModuleEnvironments;
 489 
<a name="22" id="anc22"></a><span class="line-modified"> 490     auto link_profile = [&amp;](const auto&amp; /*instruction*/, auto /*bytecode*/, auto&amp; /*metadata*/) {</span>
 491         m_numberOfNonArgumentValueProfiles++;
<a name="23" id="anc23"></a>




 492     };
 493 
 494     auto link_objectAllocationProfile = [&amp;](const auto&amp; /*instruction*/, auto bytecode, auto&amp; metadata) {
 495         metadata.m_objectAllocationProfile.initializeProfile(vm, m_globalObject.get(), this, m_globalObject-&gt;objectPrototype(), bytecode.m_inlineCapacity);
 496     };
 497 
 498     auto link_arrayAllocationProfile = [&amp;](const auto&amp; /*instruction*/, auto bytecode, auto&amp; metadata) {
 499         metadata.m_arrayAllocationProfile.initializeIndexingMode(bytecode.m_recommendedIndexingType);
 500     };
 501 
<a name="24" id="anc24"></a>



 502 #define LINK_FIELD(__field) \
 503     WTF_LAZY_JOIN(link_, __field)(instruction, bytecode, metadata);
 504 
 505 #define INITIALIZE_METADATA(__op) \
 506     auto bytecode = instruction-&gt;as&lt;__op&gt;(); \
 507     auto&amp; metadata = bytecode.metadata(this); \
 508     new (&amp;metadata) __op::Metadata { bytecode }; \
 509 
 510 #define CASE(__op) case __op::opcodeID
 511 
 512 #define LINK(...) \
 513     CASE(WTF_LAZY_FIRST(__VA_ARGS__)): { \
 514         INITIALIZE_METADATA(WTF_LAZY_FIRST(__VA_ARGS__)) \
 515         WTF_LAZY_HAS_REST(__VA_ARGS__)({ \
 516             WTF_LAZY_FOR_EACH_TERM(LINK_FIELD,  WTF_LAZY_REST_(__VA_ARGS__)) \
 517         }) \
 518         break; \
 519     }
 520 
 521     const InstructionStream&amp; instructionStream = instructions();
 522     for (const auto&amp; instruction : instructionStream) {
 523         OpcodeID opcodeID = instruction-&gt;opcodeID();
<a name="25" id="anc25"></a><span class="line-modified"> 524         m_bytecodeCost += opcodeLengths[opcodeID];</span>
 525         switch (opcodeID) {
<a name="26" id="anc26"></a><span class="line-modified"> 526         LINK(OpHasIndexedProperty)</span>
 527 
<a name="27" id="anc27"></a><span class="line-modified"> 528         LINK(OpCallVarargs, profile)</span>
<span class="line-modified"> 529         LINK(OpTailCallVarargs, profile)</span>
<span class="line-modified"> 530         LINK(OpTailCallForwardArguments, profile)</span>
<span class="line-modified"> 531         LINK(OpConstructVarargs, profile)</span>
<span class="line-modified"> 532         LINK(OpGetByVal, profile)</span>
 533 
 534         LINK(OpGetDirectPname, profile)
 535         LINK(OpGetByIdWithThis, profile)
 536         LINK(OpTryGetById, profile)
 537         LINK(OpGetByIdDirect, profile)
 538         LINK(OpGetByValWithThis, profile)
 539         LINK(OpGetFromArguments, profile)
 540         LINK(OpToNumber, profile)
 541         LINK(OpToObject, profile)
 542         LINK(OpGetArgument, profile)
 543         LINK(OpToThis, profile)
 544         LINK(OpBitand, profile)
 545         LINK(OpBitor, profile)
 546         LINK(OpBitnot, profile)
 547         LINK(OpBitxor, profile)
<a name="28" id="anc28"></a><span class="line-added"> 548         LINK(OpLshift, profile)</span>
 549 
<a name="29" id="anc29"></a><span class="line-modified"> 550         LINK(OpGetById, profile)</span>
 551 
<a name="30" id="anc30"></a><span class="line-modified"> 552         LINK(OpCall, profile)</span>
<span class="line-modified"> 553         LINK(OpTailCall, profile)</span>
<span class="line-modified"> 554         LINK(OpCallEval, profile)</span>
<span class="line-modified"> 555         LINK(OpConstruct, profile)</span>
 556 
<a name="31" id="anc31"></a><span class="line-modified"> 557         LINK(OpInByVal)</span>
<span class="line-modified"> 558         LINK(OpPutByVal)</span>
<span class="line-modified"> 559         LINK(OpPutByValDirect)</span>
 560 
 561         LINK(OpNewArray)
 562         LINK(OpNewArrayWithSize)
 563         LINK(OpNewArrayBuffer, arrayAllocationProfile)
 564 
 565         LINK(OpNewObject, objectAllocationProfile)
 566 
 567         LINK(OpPutById)
 568         LINK(OpCreateThis)
 569 
 570         LINK(OpAdd)
 571         LINK(OpMul)
 572         LINK(OpDiv)
 573         LINK(OpSub)
 574 
 575         LINK(OpNegate)
 576 
 577         LINK(OpJneqPtr)
 578 
 579         LINK(OpCatch)
 580         LINK(OpProfileControlFlow)
 581 
 582         case op_resolve_scope: {
 583             INITIALIZE_METADATA(OpResolveScope)
 584 
 585             const Identifier&amp; ident = identifier(bytecode.m_var);
 586             RELEASE_ASSERT(bytecode.m_resolveType != LocalClosureVar);
 587 
 588             ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), bytecode.m_localScopeDepth, scope, ident, Get, bytecode.m_resolveType, InitializationMode::NotInitialization);
 589             RETURN_IF_EXCEPTION(throwScope, false);
 590 
 591             metadata.m_resolveType = op.type;
 592             metadata.m_localScopeDepth = op.depth;
 593             if (op.lexicalEnvironment) {
 594                 if (op.type == ModuleVar) {
 595                     // Keep the linked module environment strongly referenced.
 596                     if (stronglyReferencedModuleEnvironments.add(jsCast&lt;JSModuleEnvironment*&gt;(op.lexicalEnvironment)).isNewEntry)
 597                         addConstant(op.lexicalEnvironment);
 598                     metadata.m_lexicalEnvironment.set(vm, this, op.lexicalEnvironment);
 599                 } else
 600                     metadata.m_symbolTable.set(vm, this, op.lexicalEnvironment-&gt;symbolTable());
 601             } else if (JSScope* constantScope = JSScope::constantScopeForCodeBlock(op.type, this)) {
 602                 metadata.m_constantScope.set(vm, this, constantScope);
 603                 if (op.type == GlobalProperty || op.type == GlobalPropertyWithVarInjectionChecks)
 604                     metadata.m_globalLexicalBindingEpoch = m_globalObject-&gt;globalLexicalBindingEpoch();
 605             } else
<a name="32" id="anc32"></a><span class="line-modified"> 606                 metadata.m_globalObject.clear();</span>
 607             break;
 608         }
 609 
 610         case op_get_from_scope: {
 611             INITIALIZE_METADATA(OpGetFromScope)
 612 
 613             link_profile(instruction, bytecode, metadata);
 614             metadata.m_watchpointSet = nullptr;
 615 
 616             ASSERT(!isInitialization(bytecode.m_getPutInfo.initializationMode()));
 617             if (bytecode.m_getPutInfo.resolveType() == LocalClosureVar) {
 618                 metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), ClosureVar, bytecode.m_getPutInfo.initializationMode());
 619                 break;
 620             }
 621 
 622             const Identifier&amp; ident = identifier(bytecode.m_var);
 623             ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), bytecode.m_localScopeDepth, scope, ident, Get, bytecode.m_getPutInfo.resolveType(), InitializationMode::NotInitialization);
 624             RETURN_IF_EXCEPTION(throwScope, false);
 625 
 626             metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), op.type, bytecode.m_getPutInfo.initializationMode());
 627             if (op.type == ModuleVar)
 628                 metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), ClosureVar, bytecode.m_getPutInfo.initializationMode());
 629             if (op.type == GlobalVar || op.type == GlobalVarWithVarInjectionChecks || op.type == GlobalLexicalVar || op.type == GlobalLexicalVarWithVarInjectionChecks)
 630                 metadata.m_watchpointSet = op.watchpointSet;
 631             else if (op.structure)
 632                 metadata.m_structure.set(vm, this, op.structure);
 633             metadata.m_operand = op.operand;
 634             break;
 635         }
 636 
 637         case op_put_to_scope: {
 638             INITIALIZE_METADATA(OpPutToScope)
 639 
 640             if (bytecode.m_getPutInfo.resolveType() == LocalClosureVar) {
 641                 // Only do watching if the property we&#39;re putting to is not anonymous.
 642                 if (bytecode.m_var != UINT_MAX) {
<a name="33" id="anc33"></a><span class="line-modified"> 643                     SymbolTable* symbolTable = jsCast&lt;SymbolTable*&gt;(getConstant(bytecode.m_symbolTableOrScopeDepth.symbolTable().offset()));</span>
 644                     const Identifier&amp; ident = identifier(bytecode.m_var);
 645                     ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 646                     auto iter = symbolTable-&gt;find(locker, ident.impl());
 647                     ASSERT(iter != symbolTable-&gt;end(locker));
 648                     iter-&gt;value.prepareToWatch();
 649                     metadata.m_watchpointSet = iter-&gt;value.watchpointSet();
 650                 } else
 651                     metadata.m_watchpointSet = nullptr;
 652                 break;
 653             }
 654 
 655             const Identifier&amp; ident = identifier(bytecode.m_var);
 656             metadata.m_watchpointSet = nullptr;
<a name="34" id="anc34"></a><span class="line-modified"> 657             ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), bytecode.m_symbolTableOrScopeDepth.scopeDepth(), scope, ident, Put, bytecode.m_getPutInfo.resolveType(), bytecode.m_getPutInfo.initializationMode());</span>
 658             RETURN_IF_EXCEPTION(throwScope, false);
 659 
 660             metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), op.type, bytecode.m_getPutInfo.initializationMode());
 661             if (op.type == GlobalVar || op.type == GlobalVarWithVarInjectionChecks || op.type == GlobalLexicalVar || op.type == GlobalLexicalVarWithVarInjectionChecks)
 662                 metadata.m_watchpointSet = op.watchpointSet;
 663             else if (op.type == ClosureVar || op.type == ClosureVarWithVarInjectionChecks) {
 664                 if (op.watchpointSet)
 665                     op.watchpointSet-&gt;invalidate(vm, PutToScopeFireDetail(this, ident));
 666             } else if (op.structure)
 667                 metadata.m_structure.set(vm, this, op.structure);
 668             metadata.m_operand = op.operand;
 669             break;
 670         }
 671 
 672         case op_profile_type: {
<a name="35" id="anc35"></a><span class="line-modified"> 673             RELEASE_ASSERT(m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes());</span>
 674 
 675             INITIALIZE_METADATA(OpProfileType)
 676 
 677             size_t instructionOffset = instruction.offset() + instruction-&gt;size() - 1;
 678             unsigned divotStart, divotEnd;
 679             GlobalVariableID globalVariableID = 0;
 680             RefPtr&lt;TypeSet&gt; globalTypeSet;
 681             bool shouldAnalyze = m_unlinkedCode-&gt;typeProfilerExpressionInfoForBytecodeOffset(instructionOffset, divotStart, divotEnd);
 682             SymbolTable* symbolTable = nullptr;
 683 
 684             switch (bytecode.m_flag) {
 685             case ProfileTypeBytecodeClosureVar: {
 686                 const Identifier&amp; ident = identifier(bytecode.m_identifier);
<a name="36" id="anc36"></a><span class="line-modified"> 687                 unsigned localScopeDepth = bytecode.m_symbolTableOrScopeDepth.scopeDepth();</span>
 688                 // Even though type profiling may be profiling either a Get or a Put, we can always claim a Get because
 689                 // we&#39;re abstractly &quot;read&quot;ing from a JSScope.
 690                 ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), localScopeDepth, scope, ident, Get, bytecode.m_resolveType, InitializationMode::NotInitialization);
 691                 RETURN_IF_EXCEPTION(throwScope, false);
 692 
 693                 if (op.type == ClosureVar || op.type == ModuleVar)
 694                     symbolTable = op.lexicalEnvironment-&gt;symbolTable();
 695                 else if (op.type == GlobalVar)
 696                     symbolTable = m_globalObject.get()-&gt;symbolTable();
 697 
 698                 UniquedStringImpl* impl = (op.type == ModuleVar) ? op.importedName.get() : ident.impl();
 699                 if (symbolTable) {
 700                     ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 701                     // If our parent scope was created while profiling was disabled, it will not have prepared for profiling yet.
 702                     symbolTable-&gt;prepareForTypeProfiling(locker);
 703                     globalVariableID = symbolTable-&gt;uniqueIDForVariable(locker, impl, vm);
 704                     globalTypeSet = symbolTable-&gt;globalTypeSetForVariable(locker, impl, vm);
 705                 } else
 706                     globalVariableID = TypeProfilerNoGlobalIDExists;
 707 
 708                 break;
 709             }
 710             case ProfileTypeBytecodeLocallyResolved: {
<a name="37" id="anc37"></a><span class="line-modified"> 711                 int symbolTableIndex = bytecode.m_symbolTableOrScopeDepth.symbolTable().offset();</span>
 712                 SymbolTable* symbolTable = jsCast&lt;SymbolTable*&gt;(getConstant(symbolTableIndex));
 713                 const Identifier&amp; ident = identifier(bytecode.m_identifier);
 714                 ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 715                 // If our parent scope was created while profiling was disabled, it will not have prepared for profiling yet.
 716                 globalVariableID = symbolTable-&gt;uniqueIDForVariable(locker, ident.impl(), vm);
 717                 globalTypeSet = symbolTable-&gt;globalTypeSetForVariable(locker, ident.impl(), vm);
 718 
 719                 break;
 720             }
 721             case ProfileTypeBytecodeDoesNotHaveGlobalID:
 722             case ProfileTypeBytecodeFunctionArgument: {
 723                 globalVariableID = TypeProfilerNoGlobalIDExists;
 724                 break;
 725             }
 726             case ProfileTypeBytecodeFunctionReturnStatement: {
 727                 RELEASE_ASSERT(ownerExecutable-&gt;isFunctionExecutable());
 728                 globalTypeSet = jsCast&lt;FunctionExecutable*&gt;(ownerExecutable)-&gt;returnStatementTypeSet();
 729                 globalVariableID = TypeProfilerReturnStatement;
 730                 if (!shouldAnalyze) {
 731                     // Because a return statement can be added implicitly to return undefined at the end of a function,
 732                     // and these nodes don&#39;t emit expression ranges because they aren&#39;t in the actual source text of
 733                     // the user&#39;s program, give the type profiler some range to identify these return statements.
 734                     // Currently, the text offset that is used as identification is &quot;f&quot; in the function keyword
 735                     // and is stored on TypeLocation&#39;s m_divotForFunctionOffsetIfReturnStatement member variable.
 736                     divotStart = divotEnd = ownerExecutable-&gt;typeProfilingStartOffset(vm);
 737                     shouldAnalyze = true;
 738                 }
 739                 break;
 740             }
 741             }
 742 
 743             std::pair&lt;TypeLocation*, bool&gt; locationPair = vm.typeProfiler()-&gt;typeLocationCache()-&gt;getTypeLocation(globalVariableID,
 744                 ownerExecutable-&gt;sourceID(), divotStart, divotEnd, WTFMove(globalTypeSet), &amp;vm);
 745             TypeLocation* location = locationPair.first;
 746             bool isNewLocation = locationPair.second;
 747 
 748             if (bytecode.m_flag == ProfileTypeBytecodeFunctionReturnStatement)
 749                 location-&gt;m_divotForFunctionOffsetIfReturnStatement = ownerExecutable-&gt;typeProfilingStartOffset(vm);
 750 
 751             if (shouldAnalyze &amp;&amp; isNewLocation)
 752                 vm.typeProfiler()-&gt;insertNewLocation(location);
 753 
 754             metadata.m_typeLocation = location;
 755             break;
 756         }
 757 
 758         case op_debug: {
 759             if (instruction-&gt;as&lt;OpDebug&gt;().m_debugHookType == DidReachBreakpoint)
 760                 m_hasDebuggerStatement = true;
 761             break;
 762         }
 763 
 764         case op_create_rest: {
 765             int numberOfArgumentsToSkip = instruction-&gt;as&lt;OpCreateRest&gt;().m_numParametersToSkip;
 766             ASSERT_UNUSED(numberOfArgumentsToSkip, numberOfArgumentsToSkip &gt;= 0);
 767             // This is used when rematerializing the rest parameter during OSR exit in the FTL JIT.&quot;);
 768             m_numberOfArgumentsToSkip = numberOfArgumentsToSkip;
 769             break;
 770         }
 771 
 772         default:
 773             break;
 774         }
 775     }
 776 
 777 #undef CASE
 778 #undef INITIALIZE_METADATA
 779 #undef LINK_FIELD
 780 #undef LINK
 781 
<a name="38" id="anc38"></a><span class="line-modified"> 782     if (m_unlinkedCode-&gt;wasCompiledWithControlFlowProfilerOpcodes())</span>
 783         insertBasicBlockBoundariesForControlFlowProfiler();
 784 
 785     // Set optimization thresholds only after instructions is initialized, since these
 786     // rely on the instruction count (and are in theory permitted to also inspect the
 787     // instruction stream to more accurate assess the cost of tier-up).
 788     optimizeAfterWarmUp();
 789     jitAfterWarmUp();
 790 
 791     // If the concurrent thread will want the code block&#39;s hash, then compute it here
 792     // synchronously.
 793     if (Options::alwaysComputeHash())
 794         hash();
 795 
 796     if (Options::dumpGeneratedBytecodes())
 797         dumpBytecode();
 798 
 799     if (m_metadata)
 800         vm.heap.reportExtraMemoryAllocated(m_metadata-&gt;sizeInBytes());
 801 
 802     return true;
 803 }
 804 
 805 void CodeBlock::finishCreationCommon(VM&amp; vm)
 806 {
 807     m_ownerEdge.set(vm, this, ExecutableToCodeBlockEdge::create(vm, this));
 808 }
 809 
 810 CodeBlock::~CodeBlock()
 811 {
 812     VM&amp; vm = *m_vm;
 813 
<a name="39" id="anc39"></a><span class="line-added"> 814 #if ENABLE(DFG_JIT)</span>
<span class="line-added"> 815     // The JITCode (and its corresponding DFG::CommonData) may outlive the CodeBlock by</span>
<span class="line-added"> 816     // a short amount of time after the CodeBlock is destructed. For example, the</span>
<span class="line-added"> 817     // Interpreter::execute methods will ref JITCode before invoking it. This can</span>
<span class="line-added"> 818     // result in the JITCode having a non-zero refCount when its owner CodeBlock is</span>
<span class="line-added"> 819     // destructed.</span>
<span class="line-added"> 820     //</span>
<span class="line-added"> 821     // Hence, we cannot rely on DFG::CommonData destruction to clear these now invalid</span>
<span class="line-added"> 822     // watchpoints in a timely manner. We&#39;ll ensure they are cleared here eagerly.</span>
<span class="line-added"> 823     //</span>
<span class="line-added"> 824     // We only need to do this for a DFG/FTL CodeBlock because only these will have a</span>
<span class="line-added"> 825     // DFG:CommonData. Hence, the LLInt and Baseline will not have any of these watchpoints.</span>
<span class="line-added"> 826     //</span>
<span class="line-added"> 827     // Note also that the LLIntPrototypeLoadAdaptiveStructureWatchpoint is also related</span>
<span class="line-added"> 828     // to the CodeBlock. However, its lifecycle is tied directly to the CodeBlock, and</span>
<span class="line-added"> 829     // will be automatically cleared when the CodeBlock destructs.</span>
<span class="line-added"> 830 </span>
<span class="line-added"> 831     if (JITCode::isOptimizingJIT(jitType()))</span>
<span class="line-added"> 832         jitCode()-&gt;dfgCommon()-&gt;clearWatchpoints();</span>
<span class="line-added"> 833 #endif</span>
 834     vm.heap.codeBlockSet().remove(this);
 835 
 836     if (UNLIKELY(vm.m_perBytecodeProfiler))
 837         vm.m_perBytecodeProfiler-&gt;notifyDestruction(this);
 838 
 839     if (!vm.heap.isShuttingDown() &amp;&amp; unlinkedCodeBlock()-&gt;didOptimize() == MixedTriState)
 840         unlinkedCodeBlock()-&gt;setDidOptimize(FalseTriState);
 841 
 842 #if ENABLE(VERBOSE_VALUE_PROFILE)
 843     dumpValueProfiles();
 844 #endif
 845 
 846     // We may be destroyed before any CodeBlocks that refer to us are destroyed.
 847     // Consider that two CodeBlocks become unreachable at the same time. There
 848     // is no guarantee about the order in which the CodeBlocks are destroyed.
 849     // So, if we don&#39;t remove incoming calls, and get destroyed before the
 850     // CodeBlock(s) that have calls into us, then the CallLinkInfo vector&#39;s
 851     // destructor will try to remove nodes from our (no longer valid) linked list.
 852     unlinkIncomingCalls();
 853 
 854     // Note that our outgoing calls will be removed from other CodeBlocks&#39;
 855     // m_incomingCalls linked lists through the execution of the ~CallLinkInfo
 856     // destructors.
 857 
 858 #if ENABLE(JIT)
 859     if (auto* jitData = m_jitData.get()) {
 860         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
 861             stubInfo-&gt;aboutToDie();
 862             stubInfo-&gt;deref();
 863         }
 864     }
 865 #endif // ENABLE(JIT)
 866 }
 867 
 868 void CodeBlock::setConstantIdentifierSetRegisters(VM&amp; vm, const Vector&lt;ConstantIdentifierSetEntry&gt;&amp; constants)
 869 {
 870     auto scope = DECLARE_THROW_SCOPE(vm);
 871     JSGlobalObject* globalObject = m_globalObject.get();
 872     ExecState* exec = globalObject-&gt;globalExec();
 873 
 874     for (const auto&amp; entry : constants) {
 875         const IdentifierSet&amp; set = entry.first;
 876 
 877         Structure* setStructure = globalObject-&gt;setStructure();
 878         RETURN_IF_EXCEPTION(scope, void());
 879         JSSet* jsSet = JSSet::create(exec, vm, setStructure, set.size());
 880         RETURN_IF_EXCEPTION(scope, void());
 881 
 882         for (auto setEntry : set) {
<a name="40" id="anc40"></a><span class="line-modified"> 883             JSString* jsString = jsOwnedString(vm, setEntry.get());</span>
 884             jsSet-&gt;add(exec, jsString);
 885             RETURN_IF_EXCEPTION(scope, void());
 886         }
 887         m_constantRegisters[entry.second].set(vm, this, jsSet);
 888     }
 889 }
 890 
<a name="41" id="anc41"></a><span class="line-modified"> 891 void CodeBlock::setConstantRegisters(const Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants, const Vector&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation, ScriptExecutable* topLevelExecutable)</span>
 892 {
 893     VM&amp; vm = *m_vm;
 894     auto scope = DECLARE_THROW_SCOPE(vm);
 895     JSGlobalObject* globalObject = m_globalObject.get();
 896     ExecState* exec = globalObject-&gt;globalExec();
 897 
 898     ASSERT(constants.size() == constantsSourceCodeRepresentation.size());
 899     size_t count = constants.size();
 900     m_constantRegisters.resizeToFit(count);
<a name="42" id="anc42"></a>
 901     for (size_t i = 0; i &lt; count; i++) {
 902         JSValue constant = constants[i].get();
 903 
 904         if (!constant.isEmpty()) {
 905             if (constant.isCell()) {
 906                 JSCell* cell = constant.asCell();
 907                 if (SymbolTable* symbolTable = jsDynamicCast&lt;SymbolTable*&gt;(vm, cell)) {
<a name="43" id="anc43"></a><span class="line-modified"> 908                     if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes()) {</span>
 909                         ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 910                         symbolTable-&gt;prepareForTypeProfiling(locker);
 911                     }
 912 
 913                     SymbolTable* clone = symbolTable-&gt;cloneScopePart(vm);
 914                     if (wasCompiledWithDebuggingOpcodes())
 915                         clone-&gt;setRareDataCodeBlock(this);
 916 
 917                     constant = clone;
 918                 } else if (auto* descriptor = jsDynamicCast&lt;JSTemplateObjectDescriptor*&gt;(vm, cell)) {
<a name="44" id="anc44"></a><span class="line-modified"> 919                     auto* templateObject = topLevelExecutable-&gt;createTemplateObject(exec, descriptor);</span>
 920                     RETURN_IF_EXCEPTION(scope, void());
 921                     constant = templateObject;
 922                 }
 923             }
 924         }
 925 
 926         m_constantRegisters[i].set(vm, this, constant);
 927     }
 928 
 929     m_constantsSourceCodeRepresentation = constantsSourceCodeRepresentation;
 930 }
 931 
 932 void CodeBlock::setAlternative(VM&amp; vm, CodeBlock* alternative)
 933 {
 934     RELEASE_ASSERT(alternative);
 935     RELEASE_ASSERT(alternative-&gt;jitCode());
 936     m_alternative.set(vm, this, alternative);
 937 }
 938 
 939 void CodeBlock::setNumParameters(int newValue)
 940 {
 941     m_numParameters = newValue;
 942 
<a name="45" id="anc45"></a><span class="line-modified"> 943     m_argumentValueProfiles = RefCountedArray&lt;ValueProfile&gt;(vm().canUseJIT() ? newValue : 0);</span>
 944 }
 945 
 946 CodeBlock* CodeBlock::specialOSREntryBlockOrNull()
 947 {
 948 #if ENABLE(FTL_JIT)
<a name="46" id="anc46"></a><span class="line-modified"> 949     if (jitType() != JITType::DFGJIT)</span>
 950         return 0;
 951     DFG::JITCode* jitCode = m_jitCode-&gt;dfg();
 952     return jitCode-&gt;osrEntryBlock();
 953 #else // ENABLE(FTL_JIT)
 954     return 0;
 955 #endif // ENABLE(FTL_JIT)
 956 }
 957 
 958 size_t CodeBlock::estimatedSize(JSCell* cell, VM&amp; vm)
 959 {
 960     CodeBlock* thisObject = jsCast&lt;CodeBlock*&gt;(cell);
 961     size_t extraMemoryAllocated = 0;
 962     if (thisObject-&gt;m_metadata)
 963         extraMemoryAllocated += thisObject-&gt;m_metadata-&gt;sizeInBytes();
<a name="47" id="anc47"></a><span class="line-modified"> 964     RefPtr&lt;JITCode&gt; jitCode = thisObject-&gt;m_jitCode;</span>
<span class="line-modified"> 965     if (jitCode &amp;&amp; !jitCode-&gt;isShared())</span>
<span class="line-added"> 966         extraMemoryAllocated += jitCode-&gt;size();</span>
 967     return Base::estimatedSize(cell, vm) + extraMemoryAllocated;
 968 }
 969 
 970 void CodeBlock::visitChildren(JSCell* cell, SlotVisitor&amp; visitor)
 971 {
 972     CodeBlock* thisObject = jsCast&lt;CodeBlock*&gt;(cell);
 973     ASSERT_GC_OBJECT_INHERITS(thisObject, info());
 974     Base::visitChildren(cell, visitor);
 975     visitor.append(thisObject-&gt;m_ownerEdge);
 976     thisObject-&gt;visitChildren(visitor);
 977 }
 978 
 979 void CodeBlock::visitChildren(SlotVisitor&amp; visitor)
 980 {
 981     ConcurrentJSLocker locker(m_lock);
 982     if (CodeBlock* otherBlock = specialOSREntryBlockOrNull())
 983         visitor.appendUnbarriered(otherBlock);
 984 
 985     size_t extraMemory = 0;
 986     if (m_metadata)
 987         extraMemory += m_metadata-&gt;sizeInBytes();
<a name="48" id="anc48"></a><span class="line-modified"> 988     if (m_jitCode &amp;&amp; !m_jitCode-&gt;isShared())</span>
 989         extraMemory += m_jitCode-&gt;size();
 990     visitor.reportExtraMemoryVisited(extraMemory);
 991 
 992     stronglyVisitStrongReferences(locker, visitor);
 993     stronglyVisitWeakReferences(locker, visitor);
 994 
 995     VM::SpaceAndSet::setFor(*subspace()).add(this);
 996 }
 997 
 998 bool CodeBlock::shouldVisitStrongly(const ConcurrentJSLocker&amp; locker)
 999 {
1000     if (Options::forceCodeBlockLiveness())
1001         return true;
1002 
1003     if (shouldJettisonDueToOldAge(locker))
1004         return false;
1005 
1006     // Interpreter and Baseline JIT CodeBlocks don&#39;t need to be jettisoned when
1007     // their weak references go stale. So if a basline JIT CodeBlock gets
1008     // scanned, we can assume that this means that it&#39;s live.
1009     if (!JITCode::isOptimizingJIT(jitType()))
1010         return true;
1011 
1012     return false;
1013 }
1014 
<a name="49" id="anc49"></a><span class="line-modified">1015 bool CodeBlock::shouldJettisonDueToWeakReference(VM&amp; vm)</span>
1016 {
1017     if (!JITCode::isOptimizingJIT(jitType()))
1018         return false;
<a name="50" id="anc50"></a><span class="line-modified">1019     return !vm.heap.isMarked(this);</span>
1020 }
1021 
<a name="51" id="anc51"></a><span class="line-modified">1022 static Seconds timeToLive(JITType jitType)</span>
1023 {
1024     if (UNLIKELY(Options::useEagerCodeBlockJettisonTiming())) {
1025         switch (jitType) {
<a name="52" id="anc52"></a><span class="line-modified">1026         case JITType::InterpreterThunk:</span>
1027             return 10_ms;
<a name="53" id="anc53"></a><span class="line-modified">1028         case JITType::BaselineJIT:</span>
1029             return 30_ms;
<a name="54" id="anc54"></a><span class="line-modified">1030         case JITType::DFGJIT:</span>
1031             return 40_ms;
<a name="55" id="anc55"></a><span class="line-modified">1032         case JITType::FTLJIT:</span>
1033             return 120_ms;
1034         default:
1035             return Seconds::infinity();
1036         }
1037     }
1038 
1039     switch (jitType) {
<a name="56" id="anc56"></a><span class="line-modified">1040     case JITType::InterpreterThunk:</span>
1041         return 5_s;
<a name="57" id="anc57"></a><span class="line-modified">1042     case JITType::BaselineJIT:</span>
1043         // Effectively 10 additional seconds, since BaselineJIT and
1044         // InterpreterThunk share a CodeBlock.
1045         return 15_s;
<a name="58" id="anc58"></a><span class="line-modified">1046     case JITType::DFGJIT:</span>
1047         return 20_s;
<a name="59" id="anc59"></a><span class="line-modified">1048     case JITType::FTLJIT:</span>
1049         return 60_s;
1050     default:
1051         return Seconds::infinity();
1052     }
1053 }
1054 
1055 bool CodeBlock::shouldJettisonDueToOldAge(const ConcurrentJSLocker&amp;)
1056 {
<a name="60" id="anc60"></a><span class="line-modified">1057     if (m_vm-&gt;heap.isMarked(this))</span>
1058         return false;
1059 
1060     if (UNLIKELY(Options::forceCodeBlockToJettisonDueToOldAge()))
1061         return true;
1062 
1063     if (timeSinceCreation() &lt; timeToLive(jitType()))
1064         return false;
1065 
1066     return true;
1067 }
1068 
1069 #if ENABLE(DFG_JIT)
<a name="61" id="anc61"></a><span class="line-modified">1070 static bool shouldMarkTransition(VM&amp; vm, DFG::WeakReferenceTransition&amp; transition)</span>
1071 {
<a name="62" id="anc62"></a><span class="line-modified">1072     if (transition.m_codeOrigin &amp;&amp; !vm.heap.isMarked(transition.m_codeOrigin.get()))</span>
1073         return false;
1074 
<a name="63" id="anc63"></a><span class="line-modified">1075     if (!vm.heap.isMarked(transition.m_from.get()))</span>
1076         return false;
1077 
1078     return true;
1079 }
1080 #endif // ENABLE(DFG_JIT)
1081 
1082 void CodeBlock::propagateTransitions(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1083 {
1084     UNUSED_PARAM(visitor);
1085 
1086     VM&amp; vm = *m_vm;
1087 
<a name="64" id="anc64"></a><span class="line-modified">1088     if (jitType() == JITType::InterpreterThunk) {</span>
1089         const Vector&lt;InstructionStream::Offset&gt;&amp; propertyAccessInstructions = m_unlinkedCode-&gt;propertyAccessInstructions();
1090         const InstructionStream&amp; instructionStream = instructions();
1091         for (size_t i = 0; i &lt; propertyAccessInstructions.size(); ++i) {
1092             auto instruction = instructionStream.at(propertyAccessInstructions[i]);
1093             if (instruction-&gt;is&lt;OpPutById&gt;()) {
1094                 auto&amp; metadata = instruction-&gt;as&lt;OpPutById&gt;().metadata(this);
1095                 StructureID oldStructureID = metadata.m_oldStructureID;
1096                 StructureID newStructureID = metadata.m_newStructureID;
1097                 if (!oldStructureID || !newStructureID)
1098                     continue;
1099                 Structure* oldStructure =
1100                     vm.heap.structureIDTable().get(oldStructureID);
1101                 Structure* newStructure =
1102                     vm.heap.structureIDTable().get(newStructureID);
<a name="65" id="anc65"></a><span class="line-modified">1103                 if (vm.heap.isMarked(oldStructure))</span>
1104                     visitor.appendUnbarriered(newStructure);
1105                 continue;
1106             }
1107         }
1108     }
1109 
1110 #if ENABLE(JIT)
1111     if (JITCode::isJIT(jitType())) {
1112         if (auto* jitData = m_jitData.get()) {
1113             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1114                 stubInfo-&gt;propagateTransitions(visitor);
1115         }
1116     }
1117 #endif // ENABLE(JIT)
1118 
1119 #if ENABLE(DFG_JIT)
1120     if (JITCode::isOptimizingJIT(jitType())) {
1121         DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1122 
1123         dfgCommon-&gt;recordedStatuses.markIfCheap(visitor);
1124 
1125         for (auto&amp; weakReference : dfgCommon-&gt;weakStructureReferences)
1126             weakReference-&gt;markIfCheap(visitor);
1127 
1128         for (auto&amp; transition : dfgCommon-&gt;transitions) {
<a name="66" id="anc66"></a><span class="line-modified">1129             if (shouldMarkTransition(vm, transition)) {</span>
1130                 // If the following three things are live, then the target of the
1131                 // transition is also live:
1132                 //
1133                 // - This code block. We know it&#39;s live already because otherwise
1134                 //   we wouldn&#39;t be scanning ourselves.
1135                 //
1136                 // - The code origin of the transition. Transitions may arise from
1137                 //   code that was inlined. They are not relevant if the user&#39;s
1138                 //   object that is required for the inlinee to run is no longer
1139                 //   live.
1140                 //
1141                 // - The source of the transition. The transition checks if some
1142                 //   heap location holds the source, and if so, stores the target.
1143                 //   Hence the source must be live for the transition to be live.
1144                 //
1145                 // We also short-circuit the liveness if the structure is harmless
1146                 // to mark (i.e. its global object and prototype are both already
1147                 // live).
1148 
1149                 visitor.append(transition.m_to);
1150             }
1151         }
1152     }
1153 #endif // ENABLE(DFG_JIT)
1154 }
1155 
1156 void CodeBlock::determineLiveness(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1157 {
1158     UNUSED_PARAM(visitor);
1159 
1160 #if ENABLE(DFG_JIT)
<a name="67" id="anc67"></a><span class="line-modified">1161     VM&amp; vm = *m_vm;</span>
<span class="line-added">1162     if (vm.heap.isMarked(this))</span>
1163         return;
1164 
1165     // In rare and weird cases, this could be called on a baseline CodeBlock. One that I found was
1166     // that we might decide that the CodeBlock should be jettisoned due to old age, so the
1167     // isMarked check doesn&#39;t protect us.
1168     if (!JITCode::isOptimizingJIT(jitType()))
1169         return;
1170 
1171     DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1172     // Now check all of our weak references. If all of them are live, then we
1173     // have proved liveness and so we scan our strong references. If at end of
1174     // GC we still have not proved liveness, then this code block is toast.
1175     bool allAreLiveSoFar = true;
1176     for (unsigned i = 0; i &lt; dfgCommon-&gt;weakReferences.size(); ++i) {
1177         JSCell* reference = dfgCommon-&gt;weakReferences[i].get();
<a name="68" id="anc68"></a><span class="line-modified">1178         ASSERT(!jsDynamicCast&lt;CodeBlock*&gt;(vm, reference));</span>
<span class="line-modified">1179         if (!vm.heap.isMarked(reference)) {</span>
1180             allAreLiveSoFar = false;
1181             break;
1182         }
1183     }
1184     if (allAreLiveSoFar) {
1185         for (unsigned i = 0; i &lt; dfgCommon-&gt;weakStructureReferences.size(); ++i) {
<a name="69" id="anc69"></a><span class="line-modified">1186             if (!vm.heap.isMarked(dfgCommon-&gt;weakStructureReferences[i].get())) {</span>
1187                 allAreLiveSoFar = false;
1188                 break;
1189             }
1190         }
1191     }
1192 
1193     // If some weak references are dead, then this fixpoint iteration was
1194     // unsuccessful.
1195     if (!allAreLiveSoFar)
1196         return;
1197 
1198     // All weak references are live. Record this information so we don&#39;t
1199     // come back here again, and scan the strong references.
1200     visitor.appendUnbarriered(this);
1201 #endif // ENABLE(DFG_JIT)
1202 }
1203 
1204 void CodeBlock::finalizeLLIntInlineCaches()
1205 {
1206     VM&amp; vm = *m_vm;
1207     const Vector&lt;InstructionStream::Offset&gt;&amp; propertyAccessInstructions = m_unlinkedCode-&gt;propertyAccessInstructions();
1208 
<a name="70" id="anc70"></a><span class="line-modified">1209     auto handleGetPutFromScope = [&amp;] (auto&amp; metadata) {</span>
1210         GetPutInfo getPutInfo = metadata.m_getPutInfo;
1211         if (getPutInfo.resolveType() == GlobalVar || getPutInfo.resolveType() == GlobalVarWithVarInjectionChecks
1212             || getPutInfo.resolveType() == LocalClosureVar || getPutInfo.resolveType() == GlobalLexicalVar || getPutInfo.resolveType() == GlobalLexicalVarWithVarInjectionChecks)
1213             return;
1214         WriteBarrierBase&lt;Structure&gt;&amp; structure = metadata.m_structure;
<a name="71" id="anc71"></a><span class="line-modified">1215         if (!structure || vm.heap.isMarked(structure.get()))</span>
1216             return;
1217         if (Options::verboseOSR())
1218             dataLogF(&quot;Clearing scope access with structure %p.\n&quot;, structure.get());
1219         structure.clear();
1220     };
1221 
1222     const InstructionStream&amp; instructionStream = instructions();
1223     for (size_t size = propertyAccessInstructions.size(), i = 0; i &lt; size; ++i) {
1224         const auto curInstruction = instructionStream.at(propertyAccessInstructions[i]);
1225         switch (curInstruction-&gt;opcodeID()) {
1226         case op_get_by_id: {
1227             auto&amp; metadata = curInstruction-&gt;as&lt;OpGetById&gt;().metadata(this);
<a name="72" id="anc72"></a><span class="line-modified">1228             if (metadata.m_modeMetadata.mode != GetByIdMode::Default)</span>
1229                 break;
1230             StructureID oldStructureID = metadata.m_modeMetadata.defaultMode.structureID;
<a name="73" id="anc73"></a><span class="line-modified">1231             if (!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))</span>
1232                 break;
1233             if (Options::verboseOSR())
1234                 dataLogF(&quot;Clearing LLInt property access.\n&quot;);
1235             LLIntPrototypeLoadAdaptiveStructureWatchpoint::clearLLIntGetByIdCache(metadata);
1236             break;
1237         }
1238         case op_get_by_id_direct: {
1239             auto&amp; metadata = curInstruction-&gt;as&lt;OpGetByIdDirect&gt;().metadata(this);
1240             StructureID oldStructureID = metadata.m_structureID;
<a name="74" id="anc74"></a><span class="line-modified">1241             if (!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))</span>
1242                 break;
1243             if (Options::verboseOSR())
1244                 dataLogF(&quot;Clearing LLInt property access.\n&quot;);
1245             metadata.m_structureID = 0;
1246             metadata.m_offset = 0;
1247             break;
1248         }
1249         case op_put_by_id: {
1250             auto&amp; metadata = curInstruction-&gt;as&lt;OpPutById&gt;().metadata(this);
1251             StructureID oldStructureID = metadata.m_oldStructureID;
1252             StructureID newStructureID = metadata.m_newStructureID;
1253             StructureChain* chain = metadata.m_structureChain.get();
<a name="75" id="anc75"></a><span class="line-modified">1254             if ((!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))</span>
<span class="line-modified">1255                 &amp;&amp; (!newStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(newStructureID)))</span>
<span class="line-modified">1256                 &amp;&amp; (!chain || vm.heap.isMarked(chain)))</span>
1257                 break;
1258             if (Options::verboseOSR())
1259                 dataLogF(&quot;Clearing LLInt put transition.\n&quot;);
1260             metadata.m_oldStructureID = 0;
1261             metadata.m_offset = 0;
1262             metadata.m_newStructureID = 0;
1263             metadata.m_structureChain.clear();
1264             break;
1265         }
1266         // FIXME: https://bugs.webkit.org/show_bug.cgi?id=166418
1267         // We need to add optimizations for op_resolve_scope_for_hoisting_func_decl_in_eval to do link time scope resolution.
1268         case op_resolve_scope_for_hoisting_func_decl_in_eval:
1269             break;
1270         case op_to_this: {
1271             auto&amp; metadata = curInstruction-&gt;as&lt;OpToThis&gt;().metadata(this);
<a name="76" id="anc76"></a><span class="line-modified">1272             if (!metadata.m_cachedStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(metadata.m_cachedStructureID)))</span>
1273                 break;
<a name="77" id="anc77"></a><span class="line-modified">1274             if (Options::verboseOSR()) {</span>
<span class="line-modified">1275                 Structure* structure = vm.heap.structureIDTable().get(metadata.m_cachedStructureID);</span>
<span class="line-modified">1276                 dataLogF(&quot;Clearing LLInt to_this with structure %p.\n&quot;, structure);</span>
<span class="line-added">1277             }</span>
<span class="line-added">1278             metadata.m_cachedStructureID = 0;</span>
1279             metadata.m_toThisStatus = merge(metadata.m_toThisStatus, ToThisClearedByGC);
1280             break;
1281         }
1282         case op_create_this: {
1283             auto&amp; metadata = curInstruction-&gt;as&lt;OpCreateThis&gt;().metadata(this);
1284             auto&amp; cacheWriteBarrier = metadata.m_cachedCallee;
1285             if (!cacheWriteBarrier || cacheWriteBarrier.unvalidatedGet() == JSCell::seenMultipleCalleeObjects())
1286                 break;
1287             JSCell* cachedFunction = cacheWriteBarrier.get();
<a name="78" id="anc78"></a><span class="line-modified">1288             if (vm.heap.isMarked(cachedFunction))</span>
1289                 break;
1290             if (Options::verboseOSR())
1291                 dataLogF(&quot;Clearing LLInt create_this with cached callee %p.\n&quot;, cachedFunction);
1292             cacheWriteBarrier.clear();
1293             break;
1294         }
1295         case op_resolve_scope: {
1296             // Right now this isn&#39;t strictly necessary. Any symbol tables that this will refer to
1297             // are for outer functions, and we refer to those functions strongly, and they refer
1298             // to the symbol table strongly. But it&#39;s nice to be on the safe side.
1299             auto&amp; metadata = curInstruction-&gt;as&lt;OpResolveScope&gt;().metadata(this);
1300             WriteBarrierBase&lt;SymbolTable&gt;&amp; symbolTable = metadata.m_symbolTable;
<a name="79" id="anc79"></a><span class="line-modified">1301             if (!symbolTable || vm.heap.isMarked(symbolTable.get()))</span>
1302                 break;
1303             if (Options::verboseOSR())
1304                 dataLogF(&quot;Clearing dead symbolTable %p.\n&quot;, symbolTable.get());
1305             symbolTable.clear();
1306             break;
1307         }
1308         case op_get_from_scope:
1309             handleGetPutFromScope(curInstruction-&gt;as&lt;OpGetFromScope&gt;().metadata(this));
1310             break;
1311         case op_put_to_scope:
1312             handleGetPutFromScope(curInstruction-&gt;as&lt;OpPutToScope&gt;().metadata(this));
1313             break;
1314         default:
1315             OpcodeID opcodeID = curInstruction-&gt;opcodeID();
1316             ASSERT_WITH_MESSAGE_UNUSED(opcodeID, false, &quot;Unhandled opcode in CodeBlock::finalizeUnconditionally, %s(%d) at bc %u&quot;, opcodeNames[opcodeID], opcodeID, propertyAccessInstructions[i]);
1317         }
1318     }
1319 
1320     // We can&#39;t just remove all the sets when we clear the caches since we might have created a watchpoint set
1321     // then cleared the cache without GCing in between.
1322     m_llintGetByIdWatchpointMap.removeIf([&amp;] (const StructureWatchpointMap::KeyValuePairType&amp; pair) -&gt; bool {
1323         auto clear = [&amp;] () {
<a name="80" id="anc80"></a><span class="line-modified">1324             auto&amp; instruction = instructions().at(std::get&lt;1&gt;(pair.key));</span>
1325             OpcodeID opcode = instruction-&gt;opcodeID();
1326             if (opcode == op_get_by_id) {
1327                 if (Options::verboseOSR())
1328                     dataLogF(&quot;Clearing LLInt property access.\n&quot;);
1329                 LLIntPrototypeLoadAdaptiveStructureWatchpoint::clearLLIntGetByIdCache(instruction-&gt;as&lt;OpGetById&gt;().metadata(this));
1330             }
1331             return true;
1332         };
1333 
<a name="81" id="anc81"></a><span class="line-modified">1334         if (!vm.heap.isMarked(vm.heap.structureIDTable().get(std::get&lt;0&gt;(pair.key))))</span>
1335             return clear();
1336 
<a name="82" id="anc82"></a><span class="line-modified">1337         for (const LLIntPrototypeLoadAdaptiveStructureWatchpoint&amp; watchpoint : pair.value) {</span>
<span class="line-modified">1338             if (!watchpoint.key().isStillLive(vm))</span>
1339                 return clear();
1340         }
1341 
1342         return false;
1343     });
1344 
1345     forEachLLIntCallLinkInfo([&amp;](LLIntCallLinkInfo&amp; callLinkInfo) {
<a name="83" id="anc83"></a><span class="line-modified">1346         if (callLinkInfo.isLinked() &amp;&amp; !vm.heap.isMarked(callLinkInfo.callee())) {</span>
1347             if (Options::verboseOSR())
1348                 dataLog(&quot;Clearing LLInt call from &quot;, *this, &quot;\n&quot;);
1349             callLinkInfo.unlink();
1350         }
<a name="84" id="anc84"></a><span class="line-modified">1351         if (callLinkInfo.lastSeenCallee() &amp;&amp; !vm.heap.isMarked(callLinkInfo.lastSeenCallee()))</span>
<span class="line-modified">1352             callLinkInfo.clearLastSeenCallee();</span>
1353     });
1354 }
1355 
1356 #if ENABLE(JIT)
1357 CodeBlock::JITData&amp; CodeBlock::ensureJITDataSlow(const ConcurrentJSLocker&amp;)
1358 {
1359     ASSERT(!m_jitData);
<a name="85" id="anc85"></a><span class="line-modified">1360     m_jitData = makeUnique&lt;JITData&gt;();</span>
1361     return *m_jitData;
1362 }
1363 
1364 void CodeBlock::finalizeBaselineJITInlineCaches()
1365 {
1366     if (auto* jitData = m_jitData.get()) {
1367         for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos)
<a name="86" id="anc86"></a><span class="line-modified">1368             callLinkInfo-&gt;visitWeak(vm());</span>
1369 
1370         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1371             stubInfo-&gt;visitWeakReferences(this);
1372     }
1373 }
1374 #endif
1375 
<a name="87" id="anc87"></a><span class="line-modified">1376 void CodeBlock::finalizeUnconditionally(VM&amp; vm)</span>
1377 {
<a name="88" id="anc88"></a><span class="line-added">1378     UNUSED_PARAM(vm);</span>
<span class="line-added">1379 </span>
1380     updateAllPredictions();
1381 
1382     if (JITCode::couldBeInterpreted(jitType()))
1383         finalizeLLIntInlineCaches();
1384 
1385 #if ENABLE(JIT)
1386     if (!!jitCode())
1387         finalizeBaselineJITInlineCaches();
1388 #endif
1389 
1390 #if ENABLE(DFG_JIT)
1391     if (JITCode::isOptimizingJIT(jitType())) {
1392         DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
<a name="89" id="anc89"></a><span class="line-modified">1393         dfgCommon-&gt;recordedStatuses.finalize(vm);</span>
1394     }
1395 #endif // ENABLE(DFG_JIT)
1396 
<a name="90" id="anc90"></a><span class="line-added">1397     auto updateActivity = [&amp;] {</span>
<span class="line-added">1398         if (!VM::useUnlinkedCodeBlockJettisoning())</span>
<span class="line-added">1399             return;</span>
<span class="line-added">1400         JITCode* jitCode = m_jitCode.get();</span>
<span class="line-added">1401         double count = 0;</span>
<span class="line-added">1402         bool alwaysActive = false;</span>
<span class="line-added">1403         switch (JITCode::jitTypeFor(jitCode)) {</span>
<span class="line-added">1404         case JITType::None:</span>
<span class="line-added">1405         case JITType::HostCallThunk:</span>
<span class="line-added">1406             return;</span>
<span class="line-added">1407         case JITType::InterpreterThunk:</span>
<span class="line-added">1408             count = m_llintExecuteCounter.count();</span>
<span class="line-added">1409             break;</span>
<span class="line-added">1410         case JITType::BaselineJIT:</span>
<span class="line-added">1411             count = m_jitExecuteCounter.count();</span>
<span class="line-added">1412             break;</span>
<span class="line-added">1413         case JITType::DFGJIT:</span>
<span class="line-added">1414 #if ENABLE(FTL_JIT)</span>
<span class="line-added">1415             count = static_cast&lt;DFG::JITCode*&gt;(jitCode)-&gt;tierUpCounter.count();</span>
<span class="line-added">1416 #else</span>
<span class="line-added">1417             alwaysActive = true;</span>
<span class="line-added">1418 #endif</span>
<span class="line-added">1419             break;</span>
<span class="line-added">1420         case JITType::FTLJIT:</span>
<span class="line-added">1421             alwaysActive = true;</span>
<span class="line-added">1422             break;</span>
<span class="line-added">1423         }</span>
<span class="line-added">1424         if (alwaysActive || m_previousCounter &lt; count) {</span>
<span class="line-added">1425             // CodeBlock is active right now, so resetting UnlinkedCodeBlock&#39;s age.</span>
<span class="line-added">1426             m_unlinkedCode-&gt;resetAge();</span>
<span class="line-added">1427         }</span>
<span class="line-added">1428         m_previousCounter = count;</span>
<span class="line-added">1429     };</span>
<span class="line-added">1430     updateActivity();</span>
<span class="line-added">1431 </span>
1432     VM::SpaceAndSet::setFor(*subspace()).remove(this);
1433 }
1434 
1435 void CodeBlock::destroy(JSCell* cell)
1436 {
1437     static_cast&lt;CodeBlock*&gt;(cell)-&gt;~CodeBlock();
1438 }
1439 
1440 void CodeBlock::getICStatusMap(const ConcurrentJSLocker&amp;, ICStatusMap&amp; result)
1441 {
1442 #if ENABLE(JIT)
1443     if (JITCode::isJIT(jitType())) {
1444         if (auto* jitData = m_jitData.get()) {
1445             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1446                 result.add(stubInfo-&gt;codeOrigin, ICStatus()).iterator-&gt;value.stubInfo = stubInfo;
1447             for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos)
1448                 result.add(callLinkInfo-&gt;codeOrigin(), ICStatus()).iterator-&gt;value.callLinkInfo = callLinkInfo;
1449             for (ByValInfo* byValInfo : jitData-&gt;m_byValInfos)
1450                 result.add(CodeOrigin(byValInfo-&gt;bytecodeIndex), ICStatus()).iterator-&gt;value.byValInfo = byValInfo;
1451         }
1452 #if ENABLE(DFG_JIT)
1453         if (JITCode::isOptimizingJIT(jitType())) {
1454             DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1455             for (auto&amp; pair : dfgCommon-&gt;recordedStatuses.calls)
1456                 result.add(pair.first, ICStatus()).iterator-&gt;value.callStatus = pair.second.get();
1457             for (auto&amp; pair : dfgCommon-&gt;recordedStatuses.gets)
1458                 result.add(pair.first, ICStatus()).iterator-&gt;value.getStatus = pair.second.get();
1459             for (auto&amp; pair : dfgCommon-&gt;recordedStatuses.puts)
1460                 result.add(pair.first, ICStatus()).iterator-&gt;value.putStatus = pair.second.get();
1461             for (auto&amp; pair : dfgCommon-&gt;recordedStatuses.ins)
1462                 result.add(pair.first, ICStatus()).iterator-&gt;value.inStatus = pair.second.get();
1463         }
1464 #endif
1465     }
1466 #else
1467     UNUSED_PARAM(result);
1468 #endif
1469 }
1470 
1471 void CodeBlock::getICStatusMap(ICStatusMap&amp; result)
1472 {
1473     ConcurrentJSLocker locker(m_lock);
1474     getICStatusMap(locker, result);
1475 }
1476 
1477 #if ENABLE(JIT)
1478 StructureStubInfo* CodeBlock::addStubInfo(AccessType accessType)
1479 {
1480     ConcurrentJSLocker locker(m_lock);
1481     return ensureJITData(locker).m_stubInfos.add(accessType);
1482 }
1483 
<a name="91" id="anc91"></a><span class="line-modified">1484 JITAddIC* CodeBlock::addJITAddIC(ArithProfile* arithProfile)</span>
1485 {
1486     ConcurrentJSLocker locker(m_lock);
<a name="92" id="anc92"></a><span class="line-modified">1487     return ensureJITData(locker).m_addICs.add(arithProfile);</span>
1488 }
1489 
<a name="93" id="anc93"></a><span class="line-modified">1490 JITMulIC* CodeBlock::addJITMulIC(ArithProfile* arithProfile)</span>
1491 {
1492     ConcurrentJSLocker locker(m_lock);
<a name="94" id="anc94"></a><span class="line-modified">1493     return ensureJITData(locker).m_mulICs.add(arithProfile);</span>
1494 }
1495 
<a name="95" id="anc95"></a><span class="line-modified">1496 JITSubIC* CodeBlock::addJITSubIC(ArithProfile* arithProfile)</span>
1497 {
1498     ConcurrentJSLocker locker(m_lock);
<a name="96" id="anc96"></a><span class="line-modified">1499     return ensureJITData(locker).m_subICs.add(arithProfile);</span>
1500 }
1501 
<a name="97" id="anc97"></a><span class="line-modified">1502 JITNegIC* CodeBlock::addJITNegIC(ArithProfile* arithProfile)</span>
1503 {
1504     ConcurrentJSLocker locker(m_lock);
<a name="98" id="anc98"></a><span class="line-modified">1505     return ensureJITData(locker).m_negICs.add(arithProfile);</span>
1506 }
1507 
1508 StructureStubInfo* CodeBlock::findStubInfo(CodeOrigin codeOrigin)
1509 {
1510     ConcurrentJSLocker locker(m_lock);
1511     if (auto* jitData = m_jitData.get()) {
1512         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
1513             if (stubInfo-&gt;codeOrigin == codeOrigin)
1514                 return stubInfo;
1515         }
1516     }
1517     return nullptr;
1518 }
1519 
1520 ByValInfo* CodeBlock::addByValInfo()
1521 {
1522     ConcurrentJSLocker locker(m_lock);
1523     return ensureJITData(locker).m_byValInfos.add();
1524 }
1525 
1526 CallLinkInfo* CodeBlock::addCallLinkInfo()
1527 {
1528     ConcurrentJSLocker locker(m_lock);
1529     return ensureJITData(locker).m_callLinkInfos.add();
1530 }
1531 
1532 CallLinkInfo* CodeBlock::getCallLinkInfoForBytecodeIndex(unsigned index)
1533 {
1534     ConcurrentJSLocker locker(m_lock);
1535     if (auto* jitData = m_jitData.get()) {
1536         for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos) {
1537             if (callLinkInfo-&gt;codeOrigin() == CodeOrigin(index))
1538                 return callLinkInfo;
1539         }
1540     }
1541     return nullptr;
1542 }
1543 
1544 RareCaseProfile* CodeBlock::addRareCaseProfile(int bytecodeOffset)
1545 {
1546     ConcurrentJSLocker locker(m_lock);
1547     auto&amp; jitData = ensureJITData(locker);
1548     jitData.m_rareCaseProfiles.append(RareCaseProfile(bytecodeOffset));
1549     return &amp;jitData.m_rareCaseProfiles.last();
1550 }
1551 
1552 RareCaseProfile* CodeBlock::rareCaseProfileForBytecodeOffset(const ConcurrentJSLocker&amp;, int bytecodeOffset)
1553 {
1554     if (auto* jitData = m_jitData.get()) {
1555         return tryBinarySearch&lt;RareCaseProfile, int&gt;(
1556             jitData-&gt;m_rareCaseProfiles, jitData-&gt;m_rareCaseProfiles.size(), bytecodeOffset,
1557             getRareCaseProfileBytecodeOffset);
1558     }
1559     return nullptr;
1560 }
1561 
1562 unsigned CodeBlock::rareCaseProfileCountForBytecodeOffset(const ConcurrentJSLocker&amp; locker, int bytecodeOffset)
1563 {
1564     RareCaseProfile* profile = rareCaseProfileForBytecodeOffset(locker, bytecodeOffset);
1565     if (profile)
1566         return profile-&gt;m_counter;
1567     return 0;
1568 }
1569 
1570 void CodeBlock::setCalleeSaveRegisters(RegisterSet calleeSaveRegisters)
1571 {
1572     ConcurrentJSLocker locker(m_lock);
<a name="99" id="anc99"></a><span class="line-modified">1573     ensureJITData(locker).m_calleeSaveRegisters = makeUnique&lt;RegisterAtOffsetList&gt;(calleeSaveRegisters);</span>
1574 }
1575 
1576 void CodeBlock::setCalleeSaveRegisters(std::unique_ptr&lt;RegisterAtOffsetList&gt; registerAtOffsetList)
1577 {
1578     ConcurrentJSLocker locker(m_lock);
1579     ensureJITData(locker).m_calleeSaveRegisters = WTFMove(registerAtOffsetList);
1580 }
1581 
1582 void CodeBlock::resetJITData()
1583 {
1584     RELEASE_ASSERT(!JITCode::isJIT(jitType()));
1585     ConcurrentJSLocker locker(m_lock);
1586 
1587     if (auto* jitData = m_jitData.get()) {
1588         // We can clear these because no other thread will have references to any stub infos, call
1589         // link infos, or by val infos if we don&#39;t have JIT code. Attempts to query these data
1590         // structures using the concurrent API (getICStatusMap and friends) will return nothing if we
1591         // don&#39;t have JIT code.
1592         jitData-&gt;m_stubInfos.clear();
1593         jitData-&gt;m_callLinkInfos.clear();
1594         jitData-&gt;m_byValInfos.clear();
1595         // We can clear this because the DFG&#39;s queries to these data structures are guarded by whether
1596         // there is JIT code.
1597         jitData-&gt;m_rareCaseProfiles.clear();
1598     }
1599 }
1600 #endif
1601 
1602 void CodeBlock::visitOSRExitTargets(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1603 {
1604     // We strongly visit OSR exits targets because we don&#39;t want to deal with
1605     // the complexity of generating an exit target CodeBlock on demand and
1606     // guaranteeing that it matches the details of the CodeBlock we compiled
1607     // the OSR exit against.
1608 
1609     visitor.append(m_alternative);
1610 
1611 #if ENABLE(DFG_JIT)
1612     DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1613     if (dfgCommon-&gt;inlineCallFrames) {
1614         for (auto* inlineCallFrame : *dfgCommon-&gt;inlineCallFrames) {
1615             ASSERT(inlineCallFrame-&gt;baselineCodeBlock);
1616             visitor.append(inlineCallFrame-&gt;baselineCodeBlock);
1617         }
1618     }
1619 #endif
1620 }
1621 
1622 void CodeBlock::stronglyVisitStrongReferences(const ConcurrentJSLocker&amp; locker, SlotVisitor&amp; visitor)
1623 {
1624     UNUSED_PARAM(locker);
1625 
1626     visitor.append(m_globalObject);
1627     visitor.append(m_ownerExecutable); // This is extra important since it causes the ExecutableToCodeBlockEdge to be marked.
1628     visitor.append(m_unlinkedCode);
1629     if (m_rareData)
1630         m_rareData-&gt;m_directEvalCodeCache.visitAggregate(visitor);
1631     visitor.appendValues(m_constantRegisters.data(), m_constantRegisters.size());
1632     for (auto&amp; functionExpr : m_functionExprs)
1633         visitor.append(functionExpr);
1634     for (auto&amp; functionDecl : m_functionDecls)
1635         visitor.append(functionDecl);
1636     forEachObjectAllocationProfile([&amp;](ObjectAllocationProfile&amp; objectAllocationProfile) {
1637         objectAllocationProfile.visitAggregate(visitor);
1638     });
1639 
1640 #if ENABLE(JIT)
1641     if (auto* jitData = m_jitData.get()) {
1642         for (ByValInfo* byValInfo : jitData-&gt;m_byValInfos)
1643             visitor.append(byValInfo-&gt;cachedSymbol);
1644     }
1645 #endif
1646 
1647 #if ENABLE(DFG_JIT)
1648     if (JITCode::isOptimizingJIT(jitType()))
1649         visitOSRExitTargets(locker, visitor);
1650 #endif
1651 }
1652 
1653 void CodeBlock::stronglyVisitWeakReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1654 {
1655     UNUSED_PARAM(visitor);
1656 
1657 #if ENABLE(DFG_JIT)
1658     if (!JITCode::isOptimizingJIT(jitType()))
1659         return;
1660 
1661     DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1662 
1663     for (auto&amp; transition : dfgCommon-&gt;transitions) {
1664         if (!!transition.m_codeOrigin)
1665             visitor.append(transition.m_codeOrigin); // Almost certainly not necessary, since the code origin should also be a weak reference. Better to be safe, though.
1666         visitor.append(transition.m_from);
1667         visitor.append(transition.m_to);
1668     }
1669 
1670     for (auto&amp; weakReference : dfgCommon-&gt;weakReferences)
1671         visitor.append(weakReference);
1672 
1673     for (auto&amp; weakStructureReference : dfgCommon-&gt;weakStructureReferences)
1674         visitor.append(weakStructureReference);
1675 
1676     dfgCommon-&gt;livenessHasBeenProved = true;
1677 #endif
1678 }
1679 
1680 CodeBlock* CodeBlock::baselineAlternative()
1681 {
1682 #if ENABLE(JIT)
1683     CodeBlock* result = this;
1684     while (result-&gt;alternative())
1685         result = result-&gt;alternative();
1686     RELEASE_ASSERT(result);
<a name="100" id="anc100"></a><span class="line-modified">1687     RELEASE_ASSERT(JITCode::isBaselineCode(result-&gt;jitType()) || result-&gt;jitType() == JITType::None);</span>
1688     return result;
1689 #else
1690     return this;
1691 #endif
1692 }
1693 
1694 CodeBlock* CodeBlock::baselineVersion()
1695 {
1696 #if ENABLE(JIT)
<a name="101" id="anc101"></a><span class="line-modified">1697     JITType selfJITType = jitType();</span>
1698     if (JITCode::isBaselineCode(selfJITType))
1699         return this;
1700     CodeBlock* result = replacement();
1701     if (!result) {
1702         if (JITCode::isOptimizingJIT(selfJITType)) {
1703             // The replacement can be null if we&#39;ve had a memory clean up and the executable
1704             // has been purged of its codeBlocks (see ExecutableBase::clearCode()). Regardless,
1705             // the current codeBlock is still live on the stack, and as an optimizing JIT
1706             // codeBlock, it will keep its baselineAlternative() alive for us to fetch below.
1707             result = this;
1708         } else {
1709             // This can happen if we&#39;re creating the original CodeBlock for an executable.
1710             // Assume that we&#39;re the baseline CodeBlock.
<a name="102" id="anc102"></a><span class="line-modified">1711             RELEASE_ASSERT(selfJITType == JITType::None);</span>
1712             return this;
1713         }
1714     }
1715     result = result-&gt;baselineAlternative();
1716     ASSERT(result);
1717     return result;
1718 #else
1719     return this;
1720 #endif
1721 }
1722 
1723 #if ENABLE(JIT)
<a name="103" id="anc103"></a><span class="line-modified">1724 bool CodeBlock::hasOptimizedReplacement(JITType typeToReplace)</span>
1725 {
1726     CodeBlock* replacement = this-&gt;replacement();
1727     return replacement &amp;&amp; JITCode::isHigherTier(replacement-&gt;jitType(), typeToReplace);
1728 }
1729 
1730 bool CodeBlock::hasOptimizedReplacement()
1731 {
1732     return hasOptimizedReplacement(jitType());
1733 }
1734 #endif
1735 
1736 HandlerInfo* CodeBlock::handlerForBytecodeOffset(unsigned bytecodeOffset, RequiredHandler requiredHandler)
1737 {
1738     RELEASE_ASSERT(bytecodeOffset &lt; instructions().size());
1739     return handlerForIndex(bytecodeOffset, requiredHandler);
1740 }
1741 
1742 HandlerInfo* CodeBlock::handlerForIndex(unsigned index, RequiredHandler requiredHandler)
1743 {
1744     if (!m_rareData)
1745         return 0;
1746     return HandlerInfo::handlerForIndex(m_rareData-&gt;m_exceptionHandlers, index, requiredHandler);
1747 }
1748 
1749 DisposableCallSiteIndex CodeBlock::newExceptionHandlingCallSiteIndex(CallSiteIndex originalCallSite)
1750 {
1751 #if ENABLE(DFG_JIT)
1752     RELEASE_ASSERT(JITCode::isOptimizingJIT(jitType()));
1753     RELEASE_ASSERT(canGetCodeOrigin(originalCallSite));
1754     ASSERT(!!handlerForIndex(originalCallSite.bits()));
1755     CodeOrigin originalOrigin = codeOrigin(originalCallSite);
1756     return m_jitCode-&gt;dfgCommon()-&gt;addDisposableCallSiteIndex(originalOrigin);
1757 #else
1758     // We never create new on-the-fly exception handling
1759     // call sites outside the DFG/FTL inline caches.
1760     UNUSED_PARAM(originalCallSite);
1761     RELEASE_ASSERT_NOT_REACHED();
1762     return DisposableCallSiteIndex(0u);
1763 #endif
1764 }
1765 
1766 
1767 
1768 void CodeBlock::ensureCatchLivenessIsComputedForBytecodeOffset(InstructionStream::Offset bytecodeOffset)
1769 {
1770     auto&amp; instruction = instructions().at(bytecodeOffset);
1771     OpCatch op = instruction-&gt;as&lt;OpCatch&gt;();
1772     auto&amp; metadata = op.metadata(this);
1773     if (!!metadata.m_buffer) {
1774 #if !ASSERT_DISABLED
1775         ConcurrentJSLocker locker(m_lock);
1776         bool found = false;
1777         auto* rareData = m_rareData.get();
1778         ASSERT(rareData);
1779         for (auto&amp; profile : rareData-&gt;m_catchProfiles) {
1780             if (profile.get() == metadata.m_buffer) {
1781                 found = true;
1782                 break;
1783             }
1784         }
1785         ASSERT(found);
1786 #endif
1787         return;
1788     }
1789 
1790     ensureCatchLivenessIsComputedForBytecodeOffsetSlow(op, bytecodeOffset);
1791 }
1792 
1793 void CodeBlock::ensureCatchLivenessIsComputedForBytecodeOffsetSlow(const OpCatch&amp; op, InstructionStream::Offset bytecodeOffset)
1794 {
1795     BytecodeLivenessAnalysis&amp; bytecodeLiveness = livenessAnalysis();
1796 
1797     // We get the live-out set of variables at op_catch, not the live-in. This
1798     // is because the variables that the op_catch defines might be dead, and
1799     // we can avoid profiling them and extracting them when doing OSR entry
1800     // into the DFG.
1801 
1802     auto nextOffset = instructions().at(bytecodeOffset).next().offset();
1803     FastBitVector liveLocals = bytecodeLiveness.getLivenessInfoAtBytecodeOffset(this, nextOffset);
1804     Vector&lt;VirtualRegister&gt; liveOperands;
1805     liveOperands.reserveInitialCapacity(liveLocals.bitCount());
1806     liveLocals.forEachSetBit([&amp;] (unsigned liveLocal) {
1807         liveOperands.append(virtualRegisterForLocal(liveLocal));
1808     });
1809 
1810     for (int i = 0; i &lt; numParameters(); ++i)
1811         liveOperands.append(virtualRegisterForArgument(i));
1812 
<a name="104" id="anc104"></a><span class="line-modified">1813     auto profiles = makeUnique&lt;ValueProfileAndOperandBuffer&gt;(liveOperands.size());</span>
1814     RELEASE_ASSERT(profiles-&gt;m_size == liveOperands.size());
1815     for (unsigned i = 0; i &lt; profiles-&gt;m_size; ++i)
1816         profiles-&gt;m_buffer.get()[i].m_operand = liveOperands[i].offset();
1817 
1818     createRareDataIfNecessary();
1819 
1820     // The compiler thread will read this pointer value and then proceed to dereference it
1821     // if it is not null. We need to make sure all above stores happen before this store so
1822     // the compiler thread reads fully initialized data.
1823     WTF::storeStoreFence();
1824 
1825     op.metadata(this).m_buffer = profiles.get();
1826     {
1827         ConcurrentJSLocker locker(m_lock);
1828         m_rareData-&gt;m_catchProfiles.append(WTFMove(profiles));
1829     }
1830 }
1831 
1832 void CodeBlock::removeExceptionHandlerForCallSite(DisposableCallSiteIndex callSiteIndex)
1833 {
1834     RELEASE_ASSERT(m_rareData);
1835     Vector&lt;HandlerInfo&gt;&amp; exceptionHandlers = m_rareData-&gt;m_exceptionHandlers;
1836     unsigned index = callSiteIndex.bits();
1837     for (size_t i = 0; i &lt; exceptionHandlers.size(); ++i) {
1838         HandlerInfo&amp; handler = exceptionHandlers[i];
1839         if (handler.start &lt;= index &amp;&amp; handler.end &gt; index) {
1840             exceptionHandlers.remove(i);
1841             return;
1842         }
1843     }
1844 
1845     RELEASE_ASSERT_NOT_REACHED();
1846 }
1847 
1848 unsigned CodeBlock::lineNumberForBytecodeOffset(unsigned bytecodeOffset)
1849 {
1850     RELEASE_ASSERT(bytecodeOffset &lt; instructions().size());
1851     return ownerExecutable()-&gt;firstLine() + m_unlinkedCode-&gt;lineNumberForBytecodeOffset(bytecodeOffset);
1852 }
1853 
1854 unsigned CodeBlock::columnNumberForBytecodeOffset(unsigned bytecodeOffset)
1855 {
1856     int divot;
1857     int startOffset;
1858     int endOffset;
1859     unsigned line;
1860     unsigned column;
1861     expressionRangeForBytecodeOffset(bytecodeOffset, divot, startOffset, endOffset, line, column);
1862     return column;
1863 }
1864 
1865 void CodeBlock::expressionRangeForBytecodeOffset(unsigned bytecodeOffset, int&amp; divot, int&amp; startOffset, int&amp; endOffset, unsigned&amp; line, unsigned&amp; column) const
1866 {
1867     m_unlinkedCode-&gt;expressionRangeForBytecodeOffset(bytecodeOffset, divot, startOffset, endOffset, line, column);
1868     divot += sourceOffset();
1869     column += line ? 1 : firstLineColumnOffset();
1870     line += ownerExecutable()-&gt;firstLine();
1871 }
1872 
1873 bool CodeBlock::hasOpDebugForLineAndColumn(unsigned line, unsigned column)
1874 {
1875     const InstructionStream&amp; instructionStream = instructions();
1876     for (const auto&amp; it : instructionStream) {
1877         if (it-&gt;is&lt;OpDebug&gt;()) {
1878             int unused;
1879             unsigned opDebugLine;
1880             unsigned opDebugColumn;
1881             expressionRangeForBytecodeOffset(it.offset(), unused, unused, unused, opDebugLine, opDebugColumn);
1882             if (line == opDebugLine &amp;&amp; (column == Breakpoint::unspecifiedColumn || column == opDebugColumn))
1883                 return true;
1884         }
1885     }
1886     return false;
1887 }
1888 
1889 void CodeBlock::shrinkToFit(ShrinkMode shrinkMode)
1890 {
1891     ConcurrentJSLocker locker(m_lock);
1892 
1893 #if ENABLE(JIT)
1894     if (auto* jitData = m_jitData.get())
1895         jitData-&gt;m_rareCaseProfiles.shrinkToFit();
1896 #endif
1897 
1898     if (shrinkMode == EarlyShrink) {
1899         m_constantRegisters.shrinkToFit();
1900         m_constantsSourceCodeRepresentation.shrinkToFit();
1901 
1902         if (m_rareData) {
1903             m_rareData-&gt;m_switchJumpTables.shrinkToFit();
1904             m_rareData-&gt;m_stringSwitchJumpTables.shrinkToFit();
1905         }
1906     } // else don&#39;t shrink these, because we would have already pointed pointers into these tables.
1907 }
1908 
1909 #if ENABLE(JIT)
1910 void CodeBlock::linkIncomingCall(ExecState* callerFrame, CallLinkInfo* incoming)
1911 {
1912     noticeIncomingCall(callerFrame);
1913     ConcurrentJSLocker locker(m_lock);
1914     ensureJITData(locker).m_incomingCalls.push(incoming);
1915 }
1916 
1917 void CodeBlock::linkIncomingPolymorphicCall(ExecState* callerFrame, PolymorphicCallNode* incoming)
1918 {
1919     noticeIncomingCall(callerFrame);
1920     {
1921         ConcurrentJSLocker locker(m_lock);
1922         ensureJITData(locker).m_incomingPolymorphicCalls.push(incoming);
1923     }
1924 }
1925 #endif // ENABLE(JIT)
1926 
1927 void CodeBlock::unlinkIncomingCalls()
1928 {
1929     while (m_incomingLLIntCalls.begin() != m_incomingLLIntCalls.end())
1930         m_incomingLLIntCalls.begin()-&gt;unlink();
1931 #if ENABLE(JIT)
1932     JITData* jitData = nullptr;
1933     {
1934         ConcurrentJSLocker locker(m_lock);
1935         jitData = m_jitData.get();
1936     }
1937     if (jitData) {
1938         while (jitData-&gt;m_incomingCalls.begin() != jitData-&gt;m_incomingCalls.end())
<a name="105" id="anc105"></a><span class="line-modified">1939             jitData-&gt;m_incomingCalls.begin()-&gt;unlink(vm());</span>
1940         while (jitData-&gt;m_incomingPolymorphicCalls.begin() != jitData-&gt;m_incomingPolymorphicCalls.end())
<a name="106" id="anc106"></a><span class="line-modified">1941             jitData-&gt;m_incomingPolymorphicCalls.begin()-&gt;unlink(vm());</span>
1942     }
1943 #endif // ENABLE(JIT)
1944 }
1945 
1946 void CodeBlock::linkIncomingCall(ExecState* callerFrame, LLIntCallLinkInfo* incoming)
1947 {
1948     noticeIncomingCall(callerFrame);
1949     m_incomingLLIntCalls.push(incoming);
1950 }
1951 
1952 CodeBlock* CodeBlock::newReplacement()
1953 {
1954     return ownerExecutable()-&gt;newReplacementCodeBlockFor(specializationKind());
1955 }
1956 
1957 #if ENABLE(JIT)
1958 CodeBlock* CodeBlock::replacement()
1959 {
<a name="107" id="anc107"></a><span class="line-modified">1960     const ClassInfo* classInfo = this-&gt;classInfo(vm());</span>
1961 
1962     if (classInfo == FunctionCodeBlock::info())
1963         return jsCast&lt;FunctionExecutable*&gt;(ownerExecutable())-&gt;codeBlockFor(isConstructor() ? CodeForConstruct : CodeForCall);
1964 
1965     if (classInfo == EvalCodeBlock::info())
1966         return jsCast&lt;EvalExecutable*&gt;(ownerExecutable())-&gt;codeBlock();
1967 
1968     if (classInfo == ProgramCodeBlock::info())
1969         return jsCast&lt;ProgramExecutable*&gt;(ownerExecutable())-&gt;codeBlock();
1970 
1971     if (classInfo == ModuleProgramCodeBlock::info())
1972         return jsCast&lt;ModuleProgramExecutable*&gt;(ownerExecutable())-&gt;codeBlock();
1973 
1974     RELEASE_ASSERT_NOT_REACHED();
1975     return nullptr;
1976 }
1977 
1978 DFG::CapabilityLevel CodeBlock::computeCapabilityLevel()
1979 {
<a name="108" id="anc108"></a><span class="line-modified">1980     const ClassInfo* classInfo = this-&gt;classInfo(vm());</span>
1981 
1982     if (classInfo == FunctionCodeBlock::info()) {
1983         if (isConstructor())
1984             return DFG::functionForConstructCapabilityLevel(this);
1985         return DFG::functionForCallCapabilityLevel(this);
1986     }
1987 
1988     if (classInfo == EvalCodeBlock::info())
1989         return DFG::evalCapabilityLevel(this);
1990 
1991     if (classInfo == ProgramCodeBlock::info())
1992         return DFG::programCapabilityLevel(this);
1993 
1994     if (classInfo == ModuleProgramCodeBlock::info())
1995         return DFG::programCapabilityLevel(this);
1996 
1997     RELEASE_ASSERT_NOT_REACHED();
1998     return DFG::CannotCompile;
1999 }
2000 
2001 #endif // ENABLE(JIT)
2002 
2003 void CodeBlock::jettison(Profiler::JettisonReason reason, ReoptimizationMode mode, const FireDetail* detail)
2004 {
2005 #if !ENABLE(DFG_JIT)
2006     UNUSED_PARAM(mode);
2007     UNUSED_PARAM(detail);
2008 #endif
2009 
<a name="109" id="anc109"></a><span class="line-modified">2010     VM&amp; vm = *m_vm;</span>
<span class="line-added">2011 </span>
<span class="line-added">2012     CodeBlock* codeBlock = this; // Placate GCC for use in CODEBLOCK_LOG_EVENT  (does not like this).</span>
<span class="line-added">2013     CODEBLOCK_LOG_EVENT(codeBlock, &quot;jettison&quot;, (&quot;due to &quot;, reason, &quot;, counting = &quot;, mode == CountReoptimization, &quot;, detail = &quot;, pointerDump(detail)));</span>
2014 
2015     RELEASE_ASSERT(reason != Profiler::NotJettisoned);
2016 
2017 #if ENABLE(DFG_JIT)
2018     if (DFG::shouldDumpDisassembly()) {
2019         dataLog(&quot;Jettisoning &quot;, *this);
2020         if (mode == CountReoptimization)
2021             dataLog(&quot; and counting reoptimization&quot;);
2022         dataLog(&quot; due to &quot;, reason);
2023         if (detail)
2024             dataLog(&quot;, &quot;, *detail);
2025         dataLog(&quot;.\n&quot;);
2026     }
2027 
2028     if (reason == Profiler::JettisonDueToWeakReference) {
2029         if (DFG::shouldDumpDisassembly()) {
2030             dataLog(*this, &quot; will be jettisoned because of the following dead references:\n&quot;);
2031             DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
2032             for (auto&amp; transition : dfgCommon-&gt;transitions) {
2033                 JSCell* origin = transition.m_codeOrigin.get();
2034                 JSCell* from = transition.m_from.get();
2035                 JSCell* to = transition.m_to.get();
<a name="110" id="anc110"></a><span class="line-modified">2036                 if ((!origin || vm.heap.isMarked(origin)) &amp;&amp; vm.heap.isMarked(from))</span>
2037                     continue;
2038                 dataLog(&quot;    Transition under &quot;, RawPointer(origin), &quot;, &quot;, RawPointer(from), &quot; -&gt; &quot;, RawPointer(to), &quot;.\n&quot;);
2039             }
2040             for (unsigned i = 0; i &lt; dfgCommon-&gt;weakReferences.size(); ++i) {
2041                 JSCell* weak = dfgCommon-&gt;weakReferences[i].get();
<a name="111" id="anc111"></a><span class="line-modified">2042                 if (vm.heap.isMarked(weak))</span>
2043                     continue;
2044                 dataLog(&quot;    Weak reference &quot;, RawPointer(weak), &quot;.\n&quot;);
2045             }
2046         }
2047     }
2048 #endif // ENABLE(DFG_JIT)
2049 
<a name="112" id="anc112"></a>
2050     DeferGCForAWhile deferGC(*heap());
2051 
2052     // We want to accomplish two things here:
2053     // 1) Make sure that if this CodeBlock is on the stack right now, then if we return to it
2054     //    we should OSR exit at the top of the next bytecode instruction after the return.
2055     // 2) Make sure that if we call the owner executable, then we shouldn&#39;t call this CodeBlock.
2056 
2057 #if ENABLE(DFG_JIT)
2058     if (JITCode::isOptimizingJIT(jitType()))
2059         jitCode()-&gt;dfgCommon()-&gt;clearWatchpoints();
2060 
2061     if (reason != Profiler::JettisonDueToOldAge) {
2062         Profiler::Compilation* compilation = jitCode()-&gt;dfgCommon()-&gt;compilation.get();
2063         if (UNLIKELY(compilation))
2064             compilation-&gt;setJettisonReason(reason, detail);
2065 
2066         // This accomplishes (1), and does its own book-keeping about whether it has already happened.
2067         if (!jitCode()-&gt;dfgCommon()-&gt;invalidate()) {
2068             // We&#39;ve already been invalidated.
<a name="113" id="anc113"></a><span class="line-modified">2069             RELEASE_ASSERT(this != replacement() || (vm.heap.isCurrentThreadBusy() &amp;&amp; !vm.heap.isMarked(ownerExecutable())));</span>
2070             return;
2071         }
2072     }
2073 
2074     if (DFG::shouldDumpDisassembly())
2075         dataLog(&quot;    Did invalidate &quot;, *this, &quot;\n&quot;);
2076 
2077     // Count the reoptimization if that&#39;s what the user wanted.
2078     if (mode == CountReoptimization) {
2079         // FIXME: Maybe this should call alternative().
2080         // https://bugs.webkit.org/show_bug.cgi?id=123677
2081         baselineAlternative()-&gt;countReoptimization();
2082         if (DFG::shouldDumpDisassembly())
2083             dataLog(&quot;    Did count reoptimization for &quot;, *this, &quot;\n&quot;);
2084     }
2085 
2086     if (this != replacement()) {
2087         // This means that we were never the entrypoint. This can happen for OSR entry code
2088         // blocks.
2089         return;
2090     }
2091 
2092     if (alternative())
2093         alternative()-&gt;optimizeAfterWarmUp();
2094 
2095     if (reason != Profiler::JettisonDueToOldAge &amp;&amp; reason != Profiler::JettisonDueToVMTraps)
2096         tallyFrequentExitSites();
2097 #endif // ENABLE(DFG_JIT)
2098 
2099     // Jettison can happen during GC. We don&#39;t want to install code to a dead executable
2100     // because that would add a dead object to the remembered set.
<a name="114" id="anc114"></a><span class="line-modified">2101     if (vm.heap.isCurrentThreadBusy() &amp;&amp; !vm.heap.isMarked(ownerExecutable()))</span>
2102         return;
2103 
<a name="115" id="anc115"></a><span class="line-added">2104 #if ENABLE(JIT)</span>
<span class="line-added">2105     {</span>
<span class="line-added">2106         ConcurrentJSLocker locker(m_lock);</span>
<span class="line-added">2107         if (JITData* jitData = m_jitData.get()) {</span>
<span class="line-added">2108             for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos)</span>
<span class="line-added">2109                 callLinkInfo-&gt;setClearedByJettison();</span>
<span class="line-added">2110         }</span>
<span class="line-added">2111     }</span>
<span class="line-added">2112 #endif</span>
<span class="line-added">2113 </span>
2114     // This accomplishes (2).
2115     ownerExecutable()-&gt;installCode(vm, alternative(), codeType(), specializationKind());
2116 
2117 #if ENABLE(DFG_JIT)
2118     if (DFG::shouldDumpDisassembly())
2119         dataLog(&quot;    Did install baseline version of &quot;, *this, &quot;\n&quot;);
2120 #endif // ENABLE(DFG_JIT)
2121 }
2122 
2123 JSGlobalObject* CodeBlock::globalObjectFor(CodeOrigin codeOrigin)
2124 {
<a name="116" id="anc116"></a><span class="line-modified">2125     auto* inlineCallFrame = codeOrigin.inlineCallFrame();</span>
<span class="line-added">2126     if (!inlineCallFrame)</span>
2127         return globalObject();
<a name="117" id="anc117"></a><span class="line-modified">2128     return inlineCallFrame-&gt;baselineCodeBlock-&gt;globalObject();</span>
2129 }
2130 
2131 class RecursionCheckFunctor {
2132 public:
2133     RecursionCheckFunctor(CallFrame* startCallFrame, CodeBlock* codeBlock, unsigned depthToCheck)
2134         : m_startCallFrame(startCallFrame)
2135         , m_codeBlock(codeBlock)
2136         , m_depthToCheck(depthToCheck)
2137         , m_foundStartCallFrame(false)
2138         , m_didRecurse(false)
2139     { }
2140 
2141     StackVisitor::Status operator()(StackVisitor&amp; visitor) const
2142     {
2143         CallFrame* currentCallFrame = visitor-&gt;callFrame();
2144 
2145         if (currentCallFrame == m_startCallFrame)
2146             m_foundStartCallFrame = true;
2147 
2148         if (m_foundStartCallFrame) {
2149             if (visitor-&gt;callFrame()-&gt;codeBlock() == m_codeBlock) {
2150                 m_didRecurse = true;
2151                 return StackVisitor::Done;
2152             }
2153 
2154             if (!m_depthToCheck--)
2155                 return StackVisitor::Done;
2156         }
2157 
2158         return StackVisitor::Continue;
2159     }
2160 
2161     bool didRecurse() const { return m_didRecurse; }
2162 
2163 private:
2164     CallFrame* m_startCallFrame;
2165     CodeBlock* m_codeBlock;
2166     mutable unsigned m_depthToCheck;
2167     mutable bool m_foundStartCallFrame;
2168     mutable bool m_didRecurse;
2169 };
2170 
2171 void CodeBlock::noticeIncomingCall(ExecState* callerFrame)
2172 {
2173     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
2174 
2175     if (Options::verboseCallLink())
2176         dataLog(&quot;Noticing call link from &quot;, pointerDump(callerCodeBlock), &quot; to &quot;, *this, &quot;\n&quot;);
2177 
2178 #if ENABLE(DFG_JIT)
2179     if (!m_shouldAlwaysBeInlined)
2180         return;
2181 
2182     if (!callerCodeBlock) {
2183         m_shouldAlwaysBeInlined = false;
2184         if (Options::verboseCallLink())
2185             dataLog(&quot;    Clearing SABI because caller is native.\n&quot;);
2186         return;
2187     }
2188 
2189     if (!hasBaselineJITProfiling())
2190         return;
2191 
2192     if (!DFG::mightInlineFunction(this))
2193         return;
2194 
2195     if (!canInline(capabilityLevelState()))
2196         return;
2197 
2198     if (!DFG::isSmallEnoughToInlineCodeInto(callerCodeBlock)) {
2199         m_shouldAlwaysBeInlined = false;
2200         if (Options::verboseCallLink())
2201             dataLog(&quot;    Clearing SABI because caller is too large.\n&quot;);
2202         return;
2203     }
2204 
<a name="118" id="anc118"></a><span class="line-modified">2205     if (callerCodeBlock-&gt;jitType() == JITType::InterpreterThunk) {</span>
2206         // If the caller is still in the interpreter, then we can&#39;t expect inlining to
2207         // happen anytime soon. Assume it&#39;s profitable to optimize it separately. This
2208         // ensures that a function is SABI only if it is called no more frequently than
2209         // any of its callers.
2210         m_shouldAlwaysBeInlined = false;
2211         if (Options::verboseCallLink())
2212             dataLog(&quot;    Clearing SABI because caller is in LLInt.\n&quot;);
2213         return;
2214     }
2215 
2216     if (JITCode::isOptimizingJIT(callerCodeBlock-&gt;jitType())) {
2217         m_shouldAlwaysBeInlined = false;
2218         if (Options::verboseCallLink())
2219             dataLog(&quot;    Clearing SABI bcause caller was already optimized.\n&quot;);
2220         return;
2221     }
2222 
2223     if (callerCodeBlock-&gt;codeType() != FunctionCode) {
2224         // If the caller is either eval or global code, assume that that won&#39;t be
2225         // optimized anytime soon. For eval code this is particularly true since we
2226         // delay eval optimization by a *lot*.
2227         m_shouldAlwaysBeInlined = false;
2228         if (Options::verboseCallLink())
2229             dataLog(&quot;    Clearing SABI because caller is not a function.\n&quot;);
2230         return;
2231     }
2232 
2233     // Recursive calls won&#39;t be inlined.
2234     RecursionCheckFunctor functor(callerFrame, this, Options::maximumInliningDepth());
<a name="119" id="anc119"></a><span class="line-modified">2235     vm().topCallFrame-&gt;iterate(functor);</span>
2236 
2237     if (functor.didRecurse()) {
2238         if (Options::verboseCallLink())
2239             dataLog(&quot;    Clearing SABI because recursion was detected.\n&quot;);
2240         m_shouldAlwaysBeInlined = false;
2241         return;
2242     }
2243 
2244     if (callerCodeBlock-&gt;capabilityLevelState() == DFG::CapabilityLevelNotSet) {
2245         dataLog(&quot;In call from &quot;, FullCodeOrigin(callerCodeBlock, callerFrame-&gt;codeOrigin()), &quot; to &quot;, *this, &quot;: caller&#39;s DFG capability level is not set.\n&quot;);
2246         CRASH();
2247     }
2248 
2249     if (canCompile(callerCodeBlock-&gt;capabilityLevelState()))
2250         return;
2251 
2252     if (Options::verboseCallLink())
2253         dataLog(&quot;    Clearing SABI because the caller is not a DFG candidate.\n&quot;);
2254 
2255     m_shouldAlwaysBeInlined = false;
2256 #endif
2257 }
2258 
2259 unsigned CodeBlock::reoptimizationRetryCounter() const
2260 {
2261 #if ENABLE(JIT)
2262     ASSERT(m_reoptimizationRetryCounter &lt;= Options::reoptimizationRetryCounterMax());
2263     return m_reoptimizationRetryCounter;
2264 #else
2265     return 0;
2266 #endif // ENABLE(JIT)
2267 }
2268 
2269 #if !ENABLE(C_LOOP)
2270 const RegisterAtOffsetList* CodeBlock::calleeSaveRegisters() const
2271 {
2272 #if ENABLE(JIT)
2273     if (auto* jitData = m_jitData.get()) {
2274         if (const RegisterAtOffsetList* registers = jitData-&gt;m_calleeSaveRegisters.get())
2275             return registers;
2276     }
2277 #endif
2278     return &amp;RegisterAtOffsetList::llintBaselineCalleeSaveRegisters();
2279 }
2280 
2281 
2282 static size_t roundCalleeSaveSpaceAsVirtualRegisters(size_t calleeSaveRegisters)
2283 {
2284 
2285     return (WTF::roundUpToMultipleOf(sizeof(Register), calleeSaveRegisters * sizeof(CPURegister)) / sizeof(Register));
2286 
2287 }
2288 
2289 size_t CodeBlock::llintBaselineCalleeSaveSpaceAsVirtualRegisters()
2290 {
2291     return roundCalleeSaveSpaceAsVirtualRegisters(numberOfLLIntBaselineCalleeSaveRegisters());
2292 }
2293 
2294 size_t CodeBlock::calleeSaveSpaceAsVirtualRegisters()
2295 {
2296     return roundCalleeSaveSpaceAsVirtualRegisters(calleeSaveRegisters()-&gt;size());
2297 }
2298 #endif
2299 
2300 #if ENABLE(JIT)
2301 
2302 void CodeBlock::countReoptimization()
2303 {
2304     m_reoptimizationRetryCounter++;
2305     if (m_reoptimizationRetryCounter &gt; Options::reoptimizationRetryCounterMax())
2306         m_reoptimizationRetryCounter = Options::reoptimizationRetryCounterMax();
2307 }
2308 
2309 unsigned CodeBlock::numberOfDFGCompiles()
2310 {
2311     ASSERT(JITCode::isBaselineCode(jitType()));
2312     if (Options::testTheFTL()) {
2313         if (m_didFailFTLCompilation)
2314             return 1000000;
2315         return (m_hasBeenCompiledWithFTL ? 1 : 0) + m_reoptimizationRetryCounter;
2316     }
2317     CodeBlock* replacement = this-&gt;replacement();
2318     return ((replacement &amp;&amp; JITCode::isOptimizingJIT(replacement-&gt;jitType())) ? 1 : 0) + m_reoptimizationRetryCounter;
2319 }
2320 
2321 int32_t CodeBlock::codeTypeThresholdMultiplier() const
2322 {
2323     if (codeType() == EvalCode)
2324         return Options::evalThresholdMultiplier();
2325 
2326     return 1;
2327 }
2328 
2329 double CodeBlock::optimizationThresholdScalingFactor()
2330 {
2331     // This expression arises from doing a least-squares fit of
2332     //
2333     // F[x_] =: a * Sqrt[x + b] + Abs[c * x] + d
2334     //
2335     // against the data points:
2336     //
2337     //    x       F[x_]
2338     //    10       0.9          (smallest reasonable code block)
2339     //   200       1.0          (typical small-ish code block)
2340     //   320       1.2          (something I saw in 3d-cube that I wanted to optimize)
2341     //  1268       5.0          (something I saw in 3d-cube that I didn&#39;t want to optimize)
2342     //  4000       5.5          (random large size, used to cause the function to converge to a shallow curve of some sort)
2343     // 10000       6.0          (similar to above)
2344     //
2345     // I achieve the minimization using the following Mathematica code:
2346     //
2347     // MyFunctionTemplate[x_, a_, b_, c_, d_] := a*Sqrt[x + b] + Abs[c*x] + d
2348     //
2349     // samples = {{10, 0.9}, {200, 1}, {320, 1.2}, {1268, 5}, {4000, 5.5}, {10000, 6}}
2350     //
2351     // solution =
2352     //     Minimize[Plus @@ ((MyFunctionTemplate[#[[1]], a, b, c, d] - #[[2]])^2 &amp; /@ samples),
2353     //         {a, b, c, d}][[2]]
2354     //
2355     // And the code below (to initialize a, b, c, d) is generated by:
2356     //
2357     // Print[&quot;const double &quot; &lt;&gt; ToString[#[[1]]] &lt;&gt; &quot; = &quot; &lt;&gt;
2358     //     If[#[[2]] &lt; 0.00001, &quot;0.0&quot;, ToString[#[[2]]]] &lt;&gt; &quot;;&quot;] &amp; /@ solution
2359     //
2360     // We&#39;ve long known the following to be true:
2361     // - Small code blocks are cheap to optimize and so we should do it sooner rather
2362     //   than later.
2363     // - Large code blocks are expensive to optimize and so we should postpone doing so,
2364     //   and sometimes have a large enough threshold that we never optimize them.
2365     // - The difference in cost is not totally linear because (a) just invoking the
2366     //   DFG incurs some base cost and (b) for large code blocks there is enough slop
2367     //   in the correlation between instruction count and the actual compilation cost
2368     //   that for those large blocks, the instruction count should not have a strong
2369     //   influence on our threshold.
2370     //
2371     // I knew the goals but I didn&#39;t know how to achieve them; so I picked an interesting
2372     // example where the heuristics were right (code block in 3d-cube with instruction
2373     // count 320, which got compiled early as it should have been) and one where they were
2374     // totally wrong (code block in 3d-cube with instruction count 1268, which was expensive
2375     // to compile and didn&#39;t run often enough to warrant compilation in my opinion), and
2376     // then threw in additional data points that represented my own guess of what our
2377     // heuristics should do for some round-numbered examples.
2378     //
2379     // The expression to which I decided to fit the data arose because I started with an
2380     // affine function, and then did two things: put the linear part in an Abs to ensure
2381     // that the fit didn&#39;t end up choosing a negative value of c (which would result in
2382     // the function turning over and going negative for large x) and I threw in a Sqrt
2383     // term because Sqrt represents my intution that the function should be more sensitive
2384     // to small changes in small values of x, but less sensitive when x gets large.
2385 
2386     // Note that the current fit essentially eliminates the linear portion of the
2387     // expression (c == 0.0).
2388     const double a = 0.061504;
2389     const double b = 1.02406;
2390     const double c = 0.0;
2391     const double d = 0.825914;
2392 
<a name="120" id="anc120"></a><span class="line-modified">2393     double bytecodeCost = this-&gt;bytecodeCost();</span>
2394 
<a name="121" id="anc121"></a><span class="line-modified">2395     ASSERT(bytecodeCost); // Make sure this is called only after we have an instruction stream; otherwise it&#39;ll just return the value of d, which makes no sense.</span>
2396 
<a name="122" id="anc122"></a><span class="line-modified">2397     double result = d + a * sqrt(bytecodeCost + b) + c * bytecodeCost;</span>
2398 
2399     result *= codeTypeThresholdMultiplier();
2400 
2401     if (Options::verboseOSR()) {
2402         dataLog(
<a name="123" id="anc123"></a><span class="line-modified">2403             *this, &quot;: bytecode cost is &quot;, bytecodeCost,</span>
2404             &quot;, scaling execution counter by &quot;, result, &quot; * &quot;, codeTypeThresholdMultiplier(),
2405             &quot;\n&quot;);
2406     }
2407     return result;
2408 }
2409 
2410 static int32_t clipThreshold(double threshold)
2411 {
2412     if (threshold &lt; 1.0)
2413         return 1;
2414 
2415     if (threshold &gt; static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::max()))
2416         return std::numeric_limits&lt;int32_t&gt;::max();
2417 
2418     return static_cast&lt;int32_t&gt;(threshold);
2419 }
2420 
2421 int32_t CodeBlock::adjustedCounterValue(int32_t desiredThreshold)
2422 {
2423     return clipThreshold(
2424         static_cast&lt;double&gt;(desiredThreshold) *
2425         optimizationThresholdScalingFactor() *
2426         (1 &lt;&lt; reoptimizationRetryCounter()));
2427 }
2428 
2429 bool CodeBlock::checkIfOptimizationThresholdReached()
2430 {
2431 #if ENABLE(DFG_JIT)
2432     if (DFG::Worklist* worklist = DFG::existingGlobalDFGWorklistOrNull()) {
2433         if (worklist-&gt;compilationState(DFG::CompilationKey(this, DFG::DFGMode))
2434             == DFG::Worklist::Compiled) {
2435             optimizeNextInvocation();
2436             return true;
2437         }
2438     }
2439 #endif
2440 
2441     return m_jitExecuteCounter.checkIfThresholdCrossedAndSet(this);
2442 }
2443 
2444 #if ENABLE(DFG_JIT)
2445 auto CodeBlock::updateOSRExitCounterAndCheckIfNeedToReoptimize(DFG::OSRExitState&amp; exitState) -&gt; OptimizeAction
2446 {
2447     DFG::OSRExitBase&amp; exit = exitState.exit;
2448     if (!exitKindMayJettison(exit.m_kind)) {
2449         // FIXME: We may want to notice that we&#39;re frequently exiting
2450         // at an op_catch that we didn&#39;t compile an entrypoint for, and
2451         // then trigger a reoptimization of this CodeBlock:
2452         // https://bugs.webkit.org/show_bug.cgi?id=175842
2453         return OptimizeAction::None;
2454     }
2455 
2456     exit.m_count++;
2457     m_osrExitCounter++;
2458 
2459     CodeBlock* baselineCodeBlock = exitState.baselineCodeBlock;
2460     ASSERT(baselineCodeBlock == baselineAlternative());
2461     if (UNLIKELY(baselineCodeBlock-&gt;jitExecuteCounter().hasCrossedThreshold()))
2462         return OptimizeAction::ReoptimizeNow;
2463 
2464     // We want to figure out if there&#39;s a possibility that we&#39;re in a loop. For the outermost
2465     // code block in the inline stack, we handle this appropriately by having the loop OSR trigger
2466     // check the exit count of the replacement of the CodeBlock from which we are OSRing. The
2467     // problem is the inlined functions, which might also have loops, but whose baseline versions
2468     // don&#39;t know where to look for the exit count. Figure out if those loops are severe enough
2469     // that we had tried to OSR enter. If so, then we should use the loop reoptimization trigger.
2470     // Otherwise, we should use the normal reoptimization trigger.
2471 
2472     bool didTryToEnterInLoop = false;
<a name="124" id="anc124"></a><span class="line-modified">2473     for (InlineCallFrame* inlineCallFrame = exit.m_codeOrigin.inlineCallFrame(); inlineCallFrame; inlineCallFrame = inlineCallFrame-&gt;directCaller.inlineCallFrame()) {</span>
2474         if (inlineCallFrame-&gt;baselineCodeBlock-&gt;ownerExecutable()-&gt;didTryToEnterInLoop()) {
2475             didTryToEnterInLoop = true;
2476             break;
2477         }
2478     }
2479 
2480     uint32_t exitCountThreshold = didTryToEnterInLoop
2481         ? exitCountThresholdForReoptimizationFromLoop()
2482         : exitCountThresholdForReoptimization();
2483 
2484     if (m_osrExitCounter &gt; exitCountThreshold)
2485         return OptimizeAction::ReoptimizeNow;
2486 
2487     // Too few fails. Adjust the execution counter such that the target is to only optimize after a while.
2488     baselineCodeBlock-&gt;m_jitExecuteCounter.setNewThresholdForOSRExit(exitState.activeThreshold, exitState.memoryUsageAdjustedThreshold);
2489     return OptimizeAction::None;
2490 }
2491 #endif
2492 
2493 void CodeBlock::optimizeNextInvocation()
2494 {
2495     if (Options::verboseOSR())
2496         dataLog(*this, &quot;: Optimizing next invocation.\n&quot;);
2497     m_jitExecuteCounter.setNewThreshold(0, this);
2498 }
2499 
2500 void CodeBlock::dontOptimizeAnytimeSoon()
2501 {
2502     if (Options::verboseOSR())
2503         dataLog(*this, &quot;: Not optimizing anytime soon.\n&quot;);
2504     m_jitExecuteCounter.deferIndefinitely();
2505 }
2506 
2507 void CodeBlock::optimizeAfterWarmUp()
2508 {
2509     if (Options::verboseOSR())
2510         dataLog(*this, &quot;: Optimizing after warm-up.\n&quot;);
2511 #if ENABLE(DFG_JIT)
2512     m_jitExecuteCounter.setNewThreshold(
2513         adjustedCounterValue(Options::thresholdForOptimizeAfterWarmUp()), this);
2514 #endif
2515 }
2516 
2517 void CodeBlock::optimizeAfterLongWarmUp()
2518 {
2519     if (Options::verboseOSR())
2520         dataLog(*this, &quot;: Optimizing after long warm-up.\n&quot;);
2521 #if ENABLE(DFG_JIT)
2522     m_jitExecuteCounter.setNewThreshold(
2523         adjustedCounterValue(Options::thresholdForOptimizeAfterLongWarmUp()), this);
2524 #endif
2525 }
2526 
2527 void CodeBlock::optimizeSoon()
2528 {
2529     if (Options::verboseOSR())
2530         dataLog(*this, &quot;: Optimizing soon.\n&quot;);
2531 #if ENABLE(DFG_JIT)
2532     m_jitExecuteCounter.setNewThreshold(
2533         adjustedCounterValue(Options::thresholdForOptimizeSoon()), this);
2534 #endif
2535 }
2536 
2537 void CodeBlock::forceOptimizationSlowPathConcurrently()
2538 {
2539     if (Options::verboseOSR())
2540         dataLog(*this, &quot;: Forcing slow path concurrently.\n&quot;);
2541     m_jitExecuteCounter.forceSlowPathConcurrently();
2542 }
2543 
2544 #if ENABLE(DFG_JIT)
2545 void CodeBlock::setOptimizationThresholdBasedOnCompilationResult(CompilationResult result)
2546 {
<a name="125" id="anc125"></a><span class="line-modified">2547     JITType type = jitType();</span>
<span class="line-modified">2548     if (type != JITType::BaselineJIT) {</span>
2549         dataLog(*this, &quot;: expected to have baseline code but have &quot;, type, &quot;\n&quot;);
<a name="126" id="anc126"></a><span class="line-modified">2550         CRASH_WITH_INFO(bitwise_cast&lt;uintptr_t&gt;(jitCode().get()), static_cast&lt;uint8_t&gt;(type));</span>
2551     }
2552 
2553     CodeBlock* replacement = this-&gt;replacement();
2554     bool hasReplacement = (replacement &amp;&amp; replacement != this);
2555     if ((result == CompilationSuccessful) != hasReplacement) {
2556         dataLog(*this, &quot;: we have result = &quot;, result, &quot; but &quot;);
2557         if (replacement == this)
2558             dataLog(&quot;we are our own replacement.\n&quot;);
2559         else
2560             dataLog(&quot;our replacement is &quot;, pointerDump(replacement), &quot;\n&quot;);
2561         RELEASE_ASSERT_NOT_REACHED();
2562     }
2563 
2564     switch (result) {
2565     case CompilationSuccessful:
2566         RELEASE_ASSERT(replacement &amp;&amp; JITCode::isOptimizingJIT(replacement-&gt;jitType()));
2567         optimizeNextInvocation();
2568         return;
2569     case CompilationFailed:
2570         dontOptimizeAnytimeSoon();
2571         return;
2572     case CompilationDeferred:
2573         // We&#39;d like to do dontOptimizeAnytimeSoon() but we cannot because
2574         // forceOptimizationSlowPathConcurrently() is inherently racy. It won&#39;t
2575         // necessarily guarantee anything. So, we make sure that even if that
2576         // function ends up being a no-op, we still eventually retry and realize
2577         // that we have optimized code ready.
2578         optimizeAfterWarmUp();
2579         return;
2580     case CompilationInvalidated:
2581         // Retry with exponential backoff.
2582         countReoptimization();
2583         optimizeAfterWarmUp();
2584         return;
2585     }
2586 
2587     dataLog(&quot;Unrecognized result: &quot;, static_cast&lt;int&gt;(result), &quot;\n&quot;);
2588     RELEASE_ASSERT_NOT_REACHED();
2589 }
2590 
2591 #endif
2592 
2593 uint32_t CodeBlock::adjustedExitCountThreshold(uint32_t desiredThreshold)
2594 {
2595     ASSERT(JITCode::isOptimizingJIT(jitType()));
2596     // Compute this the lame way so we don&#39;t saturate. This is called infrequently
2597     // enough that this loop won&#39;t hurt us.
2598     unsigned result = desiredThreshold;
2599     for (unsigned n = baselineVersion()-&gt;reoptimizationRetryCounter(); n--;) {
2600         unsigned newResult = result &lt;&lt; 1;
2601         if (newResult &lt; result)
2602             return std::numeric_limits&lt;uint32_t&gt;::max();
2603         result = newResult;
2604     }
2605     return result;
2606 }
2607 
2608 uint32_t CodeBlock::exitCountThresholdForReoptimization()
2609 {
2610     return adjustedExitCountThreshold(Options::osrExitCountForReoptimization() * codeTypeThresholdMultiplier());
2611 }
2612 
2613 uint32_t CodeBlock::exitCountThresholdForReoptimizationFromLoop()
2614 {
2615     return adjustedExitCountThreshold(Options::osrExitCountForReoptimizationFromLoop() * codeTypeThresholdMultiplier());
2616 }
2617 
2618 bool CodeBlock::shouldReoptimizeNow()
2619 {
2620     return osrExitCounter() &gt;= exitCountThresholdForReoptimization();
2621 }
2622 
2623 bool CodeBlock::shouldReoptimizeFromLoopNow()
2624 {
2625     return osrExitCounter() &gt;= exitCountThresholdForReoptimizationFromLoop();
2626 }
2627 #endif
2628 
2629 ArrayProfile* CodeBlock::getArrayProfile(const ConcurrentJSLocker&amp;, unsigned bytecodeOffset)
2630 {
2631     auto instruction = instructions().at(bytecodeOffset);
2632     switch (instruction-&gt;opcodeID()) {
<a name="127" id="anc127"></a><span class="line-modified">2633 #define CASE1(Op) \</span>
2634     case Op::opcodeID: \
2635         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_arrayProfile;
2636 
<a name="128" id="anc128"></a><span class="line-modified">2637 #define CASE2(Op) \</span>
<span class="line-modified">2638     case Op::opcodeID: \</span>
<span class="line-added">2639         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_callLinkInfo.m_arrayProfile;</span>
<span class="line-added">2640 </span>
<span class="line-added">2641     FOR_EACH_OPCODE_WITH_ARRAY_PROFILE(CASE1)</span>
<span class="line-added">2642     FOR_EACH_OPCODE_WITH_LLINT_CALL_LINK_INFO(CASE2)</span>
<span class="line-added">2643 </span>
<span class="line-added">2644 #undef CASE1</span>
<span class="line-added">2645 #undef CASE2</span>
2646 
2647     case OpGetById::opcodeID: {
2648         auto bytecode = instruction-&gt;as&lt;OpGetById&gt;();
2649         auto&amp; metadata = bytecode.metadata(this);
<a name="129" id="anc129"></a><span class="line-modified">2650         if (metadata.m_modeMetadata.mode == GetByIdMode::ArrayLength)</span>
2651             return &amp;metadata.m_modeMetadata.arrayLengthMode.arrayProfile;
2652         break;
2653     }
2654     default:
2655         break;
2656     }
2657 
2658     return nullptr;
2659 }
2660 
2661 ArrayProfile* CodeBlock::getArrayProfile(unsigned bytecodeOffset)
2662 {
2663     ConcurrentJSLocker locker(m_lock);
2664     return getArrayProfile(locker, bytecodeOffset);
2665 }
2666 
2667 #if ENABLE(DFG_JIT)
2668 Vector&lt;CodeOrigin, 0, UnsafeVectorOverflow&gt;&amp; CodeBlock::codeOrigins()
2669 {
2670     return m_jitCode-&gt;dfgCommon()-&gt;codeOrigins;
2671 }
2672 
2673 size_t CodeBlock::numberOfDFGIdentifiers() const
2674 {
2675     if (!JITCode::isOptimizingJIT(jitType()))
2676         return 0;
2677 
2678     return m_jitCode-&gt;dfgCommon()-&gt;dfgIdentifiers.size();
2679 }
2680 
2681 const Identifier&amp; CodeBlock::identifier(int index) const
2682 {
2683     size_t unlinkedIdentifiers = m_unlinkedCode-&gt;numberOfIdentifiers();
2684     if (static_cast&lt;unsigned&gt;(index) &lt; unlinkedIdentifiers)
2685         return m_unlinkedCode-&gt;identifier(index);
2686     ASSERT(JITCode::isOptimizingJIT(jitType()));
2687     return m_jitCode-&gt;dfgCommon()-&gt;dfgIdentifiers[index - unlinkedIdentifiers];
2688 }
2689 #endif // ENABLE(DFG_JIT)
2690 
<a name="130" id="anc130"></a><span class="line-modified">2691 void CodeBlock::updateAllValueProfilePredictionsAndCountLiveness(unsigned&amp; numberOfLiveNonArgumentValueProfiles, unsigned&amp; numberOfSamplesInProfiles)</span>
2692 {
2693     ConcurrentJSLocker locker(m_lock);
2694 
2695     numberOfLiveNonArgumentValueProfiles = 0;
2696     numberOfSamplesInProfiles = 0; // If this divided by ValueProfile::numberOfBuckets equals numberOfValueProfiles() then value profiles are full.
2697 
<a name="131" id="anc131"></a><span class="line-modified">2698     forEachValueProfile([&amp;](ValueProfile&amp; profile, bool isArgument) {</span>
2699         unsigned numSamples = profile.totalNumberOfSamples();
<a name="132" id="anc132"></a><span class="line-added">2700         static_assert(ValueProfile::numberOfBuckets == 1);</span>
2701         if (numSamples &gt; ValueProfile::numberOfBuckets)
2702             numSamples = ValueProfile::numberOfBuckets; // We don&#39;t want profiles that are extremely hot to be given more weight.
2703         numberOfSamplesInProfiles += numSamples;
<a name="133" id="anc133"></a><span class="line-modified">2704         if (isArgument) {</span>
2705             profile.computeUpdatedPrediction(locker);
2706             return;
2707         }
<a name="134" id="anc134"></a><span class="line-modified">2708         if (profile.numberOfSamples() || profile.isSampledBefore())</span>
2709             numberOfLiveNonArgumentValueProfiles++;
2710         profile.computeUpdatedPrediction(locker);
2711     });
2712 
2713     if (auto* rareData = m_rareData.get()) {
2714         for (auto&amp; profileBucket : rareData-&gt;m_catchProfiles) {
2715             profileBucket-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {
<a name="135" id="anc135"></a><span class="line-modified">2716                 profile.computeUpdatedPrediction(locker);</span>
2717             });
2718         }
2719     }
2720 
2721 #if ENABLE(DFG_JIT)
2722     lazyOperandValueProfiles(locker).computeUpdatedPredictions(locker);
2723 #endif
2724 }
2725 
2726 void CodeBlock::updateAllValueProfilePredictions()
2727 {
2728     unsigned ignoredValue1, ignoredValue2;
<a name="136" id="anc136"></a><span class="line-modified">2729     updateAllValueProfilePredictionsAndCountLiveness(ignoredValue1, ignoredValue2);</span>
2730 }
2731 
2732 void CodeBlock::updateAllArrayPredictions()
2733 {
2734     ConcurrentJSLocker locker(m_lock);
2735 
2736     forEachArrayProfile([&amp;](ArrayProfile&amp; profile) {
2737         profile.computeUpdatedPrediction(locker, this);
2738     });
2739 
2740     forEachArrayAllocationProfile([&amp;](ArrayAllocationProfile&amp; profile) {
2741         profile.updateProfile();
2742     });
2743 }
2744 
2745 void CodeBlock::updateAllPredictions()
2746 {
2747     updateAllValueProfilePredictions();
2748     updateAllArrayPredictions();
2749 }
2750 
2751 bool CodeBlock::shouldOptimizeNow()
2752 {
2753     if (Options::verboseOSR())
2754         dataLog(&quot;Considering optimizing &quot;, *this, &quot;...\n&quot;);
2755 
2756     if (m_optimizationDelayCounter &gt;= Options::maximumOptimizationDelay())
2757         return true;
2758 
2759     updateAllArrayPredictions();
2760 
2761     unsigned numberOfLiveNonArgumentValueProfiles;
2762     unsigned numberOfSamplesInProfiles;
<a name="137" id="anc137"></a><span class="line-modified">2763     updateAllValueProfilePredictionsAndCountLiveness(numberOfLiveNonArgumentValueProfiles, numberOfSamplesInProfiles);</span>
2764 
2765     if (Options::verboseOSR()) {
2766         dataLogF(
2767             &quot;Profile hotness: %lf (%u / %u), %lf (%u / %u)\n&quot;,
2768             (double)numberOfLiveNonArgumentValueProfiles / numberOfNonArgumentValueProfiles(),
2769             numberOfLiveNonArgumentValueProfiles, numberOfNonArgumentValueProfiles(),
2770             (double)numberOfSamplesInProfiles / ValueProfile::numberOfBuckets / numberOfNonArgumentValueProfiles(),
2771             numberOfSamplesInProfiles, ValueProfile::numberOfBuckets * numberOfNonArgumentValueProfiles());
2772     }
2773 
2774     if ((!numberOfNonArgumentValueProfiles() || (double)numberOfLiveNonArgumentValueProfiles / numberOfNonArgumentValueProfiles() &gt;= Options::desiredProfileLivenessRate())
2775         &amp;&amp; (!totalNumberOfValueProfiles() || (double)numberOfSamplesInProfiles / ValueProfile::numberOfBuckets / totalNumberOfValueProfiles() &gt;= Options::desiredProfileFullnessRate())
2776         &amp;&amp; static_cast&lt;unsigned&gt;(m_optimizationDelayCounter) + 1 &gt;= Options::minimumOptimizationDelay())
2777         return true;
2778 
2779     ASSERT(m_optimizationDelayCounter &lt; std::numeric_limits&lt;uint8_t&gt;::max());
2780     m_optimizationDelayCounter++;
2781     optimizeAfterWarmUp();
2782     return false;
2783 }
2784 
2785 #if ENABLE(DFG_JIT)
2786 void CodeBlock::tallyFrequentExitSites()
2787 {
2788     ASSERT(JITCode::isOptimizingJIT(jitType()));
<a name="138" id="anc138"></a><span class="line-modified">2789     ASSERT(alternative()-&gt;jitType() == JITType::BaselineJIT);</span>
2790 
2791     CodeBlock* profiledBlock = alternative();
2792 
2793     switch (jitType()) {
<a name="139" id="anc139"></a><span class="line-modified">2794     case JITType::DFGJIT: {</span>
2795         DFG::JITCode* jitCode = m_jitCode-&gt;dfg();
2796         for (auto&amp; exit : jitCode-&gt;osrExit)
2797             exit.considerAddingAsFrequentExitSite(profiledBlock);
2798         break;
2799     }
2800 
2801 #if ENABLE(FTL_JIT)
<a name="140" id="anc140"></a><span class="line-modified">2802     case JITType::FTLJIT: {</span>
2803         // There is no easy way to avoid duplicating this code since the FTL::JITCode::osrExit
2804         // vector contains a totally different type, that just so happens to behave like
2805         // DFG::JITCode::osrExit.
2806         FTL::JITCode* jitCode = m_jitCode-&gt;ftl();
2807         for (unsigned i = 0; i &lt; jitCode-&gt;osrExit.size(); ++i) {
2808             FTL::OSRExit&amp; exit = jitCode-&gt;osrExit[i];
2809             exit.considerAddingAsFrequentExitSite(profiledBlock);
2810         }
2811         break;
2812     }
2813 #endif
2814 
2815     default:
2816         RELEASE_ASSERT_NOT_REACHED();
2817         break;
2818     }
2819 }
2820 #endif // ENABLE(DFG_JIT)
2821 
2822 void CodeBlock::notifyLexicalBindingUpdate()
2823 {
2824     // FIXME: Currently, module code do not query to JSGlobalLexicalEnvironment. So this case should be removed once it is fixed.
2825     // https://bugs.webkit.org/show_bug.cgi?id=193347
2826     if (scriptMode() == JSParserScriptMode::Module)
2827         return;
2828     JSGlobalObject* globalObject = m_globalObject.get();
2829     JSGlobalLexicalEnvironment* globalLexicalEnvironment = jsCast&lt;JSGlobalLexicalEnvironment*&gt;(globalObject-&gt;globalScope());
2830     SymbolTable* symbolTable = globalLexicalEnvironment-&gt;symbolTable();
2831 
2832     ConcurrentJSLocker locker(m_lock);
2833 
2834     auto isShadowed = [&amp;] (UniquedStringImpl* uid) {
2835         ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
2836         return symbolTable-&gt;contains(locker, uid);
2837     };
2838 
2839     const InstructionStream&amp; instructionStream = instructions();
2840     for (const auto&amp; instruction : instructionStream) {
2841         OpcodeID opcodeID = instruction-&gt;opcodeID();
2842         switch (opcodeID) {
2843         case op_resolve_scope: {
2844             auto bytecode = instruction-&gt;as&lt;OpResolveScope&gt;();
2845             auto&amp; metadata = bytecode.metadata(this);
2846             ResolveType originalResolveType = metadata.m_resolveType;
2847             if (originalResolveType == GlobalProperty || originalResolveType == GlobalPropertyWithVarInjectionChecks) {
2848                 const Identifier&amp; ident = identifier(bytecode.m_var);
2849                 if (isShadowed(ident.impl()))
2850                     metadata.m_globalLexicalBindingEpoch = 0;
2851                 else
2852                     metadata.m_globalLexicalBindingEpoch = globalObject-&gt;globalLexicalBindingEpoch();
2853             }
2854             break;
2855         }
2856         default:
2857             break;
2858         }
2859     }
2860 }
2861 
2862 #if ENABLE(VERBOSE_VALUE_PROFILE)
2863 void CodeBlock::dumpValueProfiles()
2864 {
2865     dataLog(&quot;ValueProfile for &quot;, *this, &quot;:\n&quot;);
<a name="141" id="anc141"></a><span class="line-modified">2866     forEachValueProfile([](ValueProfile&amp; profile, bool isArgument) {</span>
<span class="line-modified">2867         if (isArgument)</span>
<span class="line-modified">2868             dataLogF(&quot;   arg: &quot;);</span>
<span class="line-modified">2869         else</span>
<span class="line-modified">2870             dataLogF(&quot;   bc: &quot;);</span>

2871         if (!profile.numberOfSamples() &amp;&amp; profile.m_prediction == SpecNone) {
2872             dataLogF(&quot;&lt;empty&gt;\n&quot;);
2873             continue;
2874         }
2875         profile.dump(WTF::dataFile());
2876         dataLogF(&quot;\n&quot;);
2877     });
2878     dataLog(&quot;RareCaseProfile for &quot;, *this, &quot;:\n&quot;);
2879     if (auto* jitData = m_jitData.get()) {
2880         for (RareCaseProfile* profile : jitData-&gt;m_rareCaseProfiles)
2881             dataLogF(&quot;   bc = %d: %u\n&quot;, profile-&gt;m_bytecodeOffset, profile-&gt;m_counter);
2882     }
2883 }
2884 #endif // ENABLE(VERBOSE_VALUE_PROFILE)
2885 
2886 unsigned CodeBlock::frameRegisterCount()
2887 {
2888     switch (jitType()) {
<a name="142" id="anc142"></a><span class="line-modified">2889     case JITType::InterpreterThunk:</span>
2890         return LLInt::frameRegisterCountFor(this);
2891 
2892 #if ENABLE(JIT)
<a name="143" id="anc143"></a><span class="line-modified">2893     case JITType::BaselineJIT:</span>
2894         return JIT::frameRegisterCountFor(this);
2895 #endif // ENABLE(JIT)
2896 
2897 #if ENABLE(DFG_JIT)
<a name="144" id="anc144"></a><span class="line-modified">2898     case JITType::DFGJIT:</span>
<span class="line-modified">2899     case JITType::FTLJIT:</span>
2900         return jitCode()-&gt;dfgCommon()-&gt;frameRegisterCount;
2901 #endif // ENABLE(DFG_JIT)
2902 
2903     default:
2904         RELEASE_ASSERT_NOT_REACHED();
2905         return 0;
2906     }
2907 }
2908 
2909 int CodeBlock::stackPointerOffset()
2910 {
2911     return virtualRegisterForLocal(frameRegisterCount() - 1).offset();
2912 }
2913 
2914 size_t CodeBlock::predictedMachineCodeSize()
2915 {
2916     VM* vm = m_vm;
2917     // This will be called from CodeBlock::CodeBlock before either m_vm or the
2918     // instructions have been initialized. It&#39;s OK to return 0 because what will really
2919     // matter is the recomputation of this value when the slow path is triggered.
2920     if (!vm)
2921         return 0;
2922 
2923     if (!*vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT)
2924         return 0; // It&#39;s as good of a prediction as we&#39;ll get.
2925 
2926     // Be conservative: return a size that will be an overestimation 84% of the time.
2927     double multiplier = vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;mean() +
2928         vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;standardDeviation();
2929 
2930     // Be paranoid: silently reject bogus multipiers. Silently doing the &quot;wrong&quot; thing
2931     // here is OK, since this whole method is just a heuristic.
2932     if (multiplier &lt; 0 || multiplier &gt; 1000)
2933         return 0;
2934 
<a name="145" id="anc145"></a><span class="line-modified">2935     double doubleResult = multiplier * bytecodeCost();</span>
2936 
2937     // Be even more paranoid: silently reject values that won&#39;t fit into a size_t. If
2938     // the function is so huge that we can&#39;t even fit it into virtual memory then we
2939     // should probably have some other guards in place to prevent us from even getting
2940     // to this point.
2941     if (doubleResult &gt; std::numeric_limits&lt;size_t&gt;::max())
2942         return 0;
2943 
2944     return static_cast&lt;size_t&gt;(doubleResult);
2945 }
2946 
2947 String CodeBlock::nameForRegister(VirtualRegister virtualRegister)
2948 {
2949     for (auto&amp; constantRegister : m_constantRegisters) {
2950         if (constantRegister.get().isEmpty())
2951             continue;
<a name="146" id="anc146"></a><span class="line-modified">2952         if (SymbolTable* symbolTable = jsDynamicCast&lt;SymbolTable*&gt;(vm(), constantRegister.get())) {</span>
2953             ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
2954             auto end = symbolTable-&gt;end(locker);
2955             for (auto ptr = symbolTable-&gt;begin(locker); ptr != end; ++ptr) {
2956                 if (ptr-&gt;value.varOffset() == VarOffset(virtualRegister)) {
2957                     // FIXME: This won&#39;t work from the compilation thread.
2958                     // https://bugs.webkit.org/show_bug.cgi?id=115300
2959                     return ptr-&gt;key.get();
2960                 }
2961             }
2962         }
2963     }
2964     if (virtualRegister == thisRegister())
2965         return &quot;this&quot;_s;
2966     if (virtualRegister.isArgument())
2967         return makeString(&quot;arguments[&quot;, pad(&#39; &#39;, 3, virtualRegister.toArgument()), &#39;]&#39;);
2968 
2969     return emptyString();
2970 }
2971 
2972 ValueProfile* CodeBlock::tryGetValueProfileForBytecodeOffset(int bytecodeOffset)
2973 {
2974     auto instruction = instructions().at(bytecodeOffset);
2975     switch (instruction-&gt;opcodeID()) {
2976 
2977 #define CASE(Op) \
2978     case Op::opcodeID: \
2979         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_profile;
2980 
2981         FOR_EACH_OPCODE_WITH_VALUE_PROFILE(CASE)
2982 
2983 #undef CASE
2984 
2985     default:
2986         return nullptr;
2987 
2988     }
2989 }
2990 
2991 SpeculatedType CodeBlock::valueProfilePredictionForBytecodeOffset(const ConcurrentJSLocker&amp; locker, int bytecodeOffset)
2992 {
2993     if (ValueProfile* valueProfile = tryGetValueProfileForBytecodeOffset(bytecodeOffset))
2994         return valueProfile-&gt;computeUpdatedPrediction(locker);
2995     return SpecNone;
2996 }
2997 
2998 ValueProfile&amp; CodeBlock::valueProfileForBytecodeOffset(int bytecodeOffset)
2999 {
3000     return *tryGetValueProfileForBytecodeOffset(bytecodeOffset);
3001 }
3002 
3003 void CodeBlock::validate()
3004 {
3005     BytecodeLivenessAnalysis liveness(this); // Compute directly from scratch so it doesn&#39;t effect CodeBlock footprint.
3006 
3007     FastBitVector liveAtHead = liveness.getLivenessInfoAtBytecodeOffset(this, 0);
3008 
3009     if (liveAtHead.numBits() != static_cast&lt;size_t&gt;(m_numCalleeLocals)) {
3010         beginValidationDidFail();
3011         dataLog(&quot;    Wrong number of bits in result!\n&quot;);
3012         dataLog(&quot;    Result: &quot;, liveAtHead, &quot;\n&quot;);
3013         dataLog(&quot;    Bit count: &quot;, liveAtHead.numBits(), &quot;\n&quot;);
3014         endValidationDidFail();
3015     }
3016 
3017     for (unsigned i = m_numCalleeLocals; i--;) {
3018         VirtualRegister reg = virtualRegisterForLocal(i);
3019 
3020         if (liveAtHead[i]) {
3021             beginValidationDidFail();
3022             dataLog(&quot;    Variable &quot;, reg, &quot; is expected to be dead.\n&quot;);
3023             dataLog(&quot;    Result: &quot;, liveAtHead, &quot;\n&quot;);
3024             endValidationDidFail();
3025         }
3026     }
3027 
3028     const InstructionStream&amp; instructionStream = instructions();
3029     for (const auto&amp; instruction : instructionStream) {
3030         OpcodeID opcode = instruction-&gt;opcodeID();
3031         if (!!baselineAlternative()-&gt;handlerForBytecodeOffset(instruction.offset())) {
3032             if (opcode == op_catch || opcode == op_enter) {
3033                 // op_catch/op_enter logically represent an entrypoint. Entrypoints are not allowed to be
3034                 // inside of a try block because they are responsible for bootstrapping state. And they
3035                 // are never allowed throw an exception because of this. We rely on this when compiling
3036                 // in the DFG. Because an entrypoint never throws, the bytecode generator will never
3037                 // allow once inside a try block.
3038                 beginValidationDidFail();
3039                 dataLog(&quot;    entrypoint not allowed inside a try block.&quot;);
3040                 endValidationDidFail();
3041             }
3042         }
3043     }
3044 }
3045 
3046 void CodeBlock::beginValidationDidFail()
3047 {
3048     dataLog(&quot;Validation failure in &quot;, *this, &quot;:\n&quot;);
3049     dataLog(&quot;\n&quot;);
3050 }
3051 
3052 void CodeBlock::endValidationDidFail()
3053 {
3054     dataLog(&quot;\n&quot;);
3055     dumpBytecode();
3056     dataLog(&quot;\n&quot;);
3057     dataLog(&quot;Validation failure.\n&quot;);
3058     RELEASE_ASSERT_NOT_REACHED();
3059 }
3060 
3061 void CodeBlock::addBreakpoint(unsigned numBreakpoints)
3062 {
3063     m_numBreakpoints += numBreakpoints;
3064     ASSERT(m_numBreakpoints);
3065     if (JITCode::isOptimizingJIT(jitType()))
3066         jettison(Profiler::JettisonDueToDebuggerBreakpoint);
3067 }
3068 
3069 void CodeBlock::setSteppingMode(CodeBlock::SteppingMode mode)
3070 {
3071     m_steppingMode = mode;
3072     if (mode == SteppingModeEnabled &amp;&amp; JITCode::isOptimizingJIT(jitType()))
3073         jettison(Profiler::JettisonDueToDebuggerStepping);
3074 }
3075 
3076 int CodeBlock::outOfLineJumpOffset(const Instruction* pc)
3077 {
3078     int offset = bytecodeOffset(pc);
3079     return m_unlinkedCode-&gt;outOfLineJumpOffset(offset);
3080 }
3081 
3082 const Instruction* CodeBlock::outOfLineJumpTarget(const Instruction* pc)
3083 {
3084     int offset = bytecodeOffset(pc);
3085     int target = m_unlinkedCode-&gt;outOfLineJumpOffset(offset);
3086     return instructions().at(offset + target).ptr();
3087 }
3088 
3089 ArithProfile* CodeBlock::arithProfileForBytecodeOffset(InstructionStream::Offset bytecodeOffset)
3090 {
3091     return arithProfileForPC(instructions().at(bytecodeOffset).ptr());
3092 }
3093 
3094 ArithProfile* CodeBlock::arithProfileForPC(const Instruction* pc)
3095 {
3096     switch (pc-&gt;opcodeID()) {
3097     case op_negate:
3098         return &amp;pc-&gt;as&lt;OpNegate&gt;().metadata(this).m_arithProfile;
3099     case op_add:
3100         return &amp;pc-&gt;as&lt;OpAdd&gt;().metadata(this).m_arithProfile;
3101     case op_mul:
3102         return &amp;pc-&gt;as&lt;OpMul&gt;().metadata(this).m_arithProfile;
3103     case op_sub:
3104         return &amp;pc-&gt;as&lt;OpSub&gt;().metadata(this).m_arithProfile;
3105     case op_div:
3106         return &amp;pc-&gt;as&lt;OpDiv&gt;().metadata(this).m_arithProfile;
3107     default:
3108         break;
3109     }
3110 
3111     return nullptr;
3112 }
3113 
3114 bool CodeBlock::couldTakeSpecialFastCase(InstructionStream::Offset bytecodeOffset)
3115 {
3116     if (!hasBaselineJITProfiling())
3117         return false;
3118     ArithProfile* profile = arithProfileForBytecodeOffset(bytecodeOffset);
3119     if (!profile)
3120         return false;
3121     return profile-&gt;tookSpecialFastPath();
3122 }
3123 
3124 #if ENABLE(JIT)
3125 DFG::CapabilityLevel CodeBlock::capabilityLevel()
3126 {
3127     DFG::CapabilityLevel result = computeCapabilityLevel();
3128     m_capabilityLevelState = result;
3129     return result;
3130 }
3131 #endif
3132 
3133 void CodeBlock::insertBasicBlockBoundariesForControlFlowProfiler()
3134 {
3135     if (!unlinkedCodeBlock()-&gt;hasOpProfileControlFlowBytecodeOffsets())
3136         return;
3137     const Vector&lt;InstructionStream::Offset&gt;&amp; bytecodeOffsets = unlinkedCodeBlock()-&gt;opProfileControlFlowBytecodeOffsets();
3138     for (size_t i = 0, offsetsLength = bytecodeOffsets.size(); i &lt; offsetsLength; i++) {
3139         // Because op_profile_control_flow is emitted at the beginning of every basic block, finding
3140         // the next op_profile_control_flow will give us the text range of a single basic block.
3141         size_t startIdx = bytecodeOffsets[i];
3142         auto instruction = instructions().at(startIdx);
3143         RELEASE_ASSERT(instruction-&gt;opcodeID() == op_profile_control_flow);
3144         auto bytecode = instruction-&gt;as&lt;OpProfileControlFlow&gt;();
3145         auto&amp; metadata = bytecode.metadata(this);
3146         int basicBlockStartOffset = bytecode.m_textOffset;
3147         int basicBlockEndOffset;
3148         if (i + 1 &lt; offsetsLength) {
3149             size_t endIdx = bytecodeOffsets[i + 1];
3150             auto endInstruction = instructions().at(endIdx);
3151             RELEASE_ASSERT(endInstruction-&gt;opcodeID() == op_profile_control_flow);
3152             basicBlockEndOffset = endInstruction-&gt;as&lt;OpProfileControlFlow&gt;().m_textOffset - 1;
3153         } else {
3154             basicBlockEndOffset = sourceOffset() + ownerExecutable()-&gt;source().length() - 1; // Offset before the closing brace.
3155             basicBlockStartOffset = std::min(basicBlockStartOffset, basicBlockEndOffset); // Some start offsets may be at the closing brace, ensure it is the offset before.
3156         }
3157 
3158         // The following check allows for the same textual JavaScript basic block to have its bytecode emitted more
3159         // than once and still play nice with the control flow profiler. When basicBlockStartOffset is larger than
3160         // basicBlockEndOffset, it indicates that the bytecode generator has emitted code for the same AST node
3161         // more than once (for example: ForInNode, Finally blocks in TryNode, etc). Though these are different
3162         // basic blocks at the bytecode level, they are generated from the same textual basic block in the JavaScript
3163         // program. The condition:
3164         // (basicBlockEndOffset &lt; basicBlockStartOffset)
3165         // is encountered when op_profile_control_flow lies across the boundary of these duplicated bytecode basic
3166         // blocks and the textual offset goes from the end of the duplicated block back to the beginning. These
3167         // ranges are dummy ranges and are ignored. The duplicated bytecode basic blocks point to the same
3168         // internal data structure, so if any of them execute, it will record the same textual basic block in the
3169         // JavaScript program as executing.
3170         // At the bytecode level, this situation looks like:
3171         // j: op_profile_control_flow (from j-&gt;k, we have basicBlockEndOffset &lt; basicBlockStartOffset)
3172         // ...
3173         // k: op_profile_control_flow (we want to skip over the j-&gt;k block and start fresh at offset k as the start of a new basic block k-&gt;m).
3174         // ...
3175         // m: op_profile_control_flow
3176         if (basicBlockEndOffset &lt; basicBlockStartOffset) {
3177             RELEASE_ASSERT(i + 1 &lt; offsetsLength); // We should never encounter dummy blocks at the end of a CodeBlock.
<a name="147" id="anc147"></a><span class="line-modified">3178             metadata.m_basicBlockLocation = vm().controlFlowProfiler()-&gt;dummyBasicBlock();</span>
3179             continue;
3180         }
3181 
<a name="148" id="anc148"></a><span class="line-modified">3182         BasicBlockLocation* basicBlockLocation = vm().controlFlowProfiler()-&gt;getBasicBlockLocation(ownerExecutable()-&gt;sourceID(), basicBlockStartOffset, basicBlockEndOffset);</span>
3183 
3184         // Find all functions that are enclosed within the range: [basicBlockStartOffset, basicBlockEndOffset]
3185         // and insert these functions&#39; start/end offsets as gaps in the current BasicBlockLocation.
3186         // This is necessary because in the original source text of a JavaScript program,
3187         // function literals form new basic blocks boundaries, but they aren&#39;t represented
3188         // inside the CodeBlock&#39;s instruction stream.
3189         auto insertFunctionGaps = [basicBlockLocation, basicBlockStartOffset, basicBlockEndOffset] (const WriteBarrier&lt;FunctionExecutable&gt;&amp; functionExecutable) {
3190             const UnlinkedFunctionExecutable* executable = functionExecutable-&gt;unlinkedExecutable();
3191             int functionStart = executable-&gt;typeProfilingStartOffset();
3192             int functionEnd = executable-&gt;typeProfilingEndOffset();
3193             if (functionStart &gt;= basicBlockStartOffset &amp;&amp; functionEnd &lt;= basicBlockEndOffset)
3194                 basicBlockLocation-&gt;insertGap(functionStart, functionEnd);
3195         };
3196 
3197         for (const WriteBarrier&lt;FunctionExecutable&gt;&amp; executable : m_functionDecls)
3198             insertFunctionGaps(executable);
3199         for (const WriteBarrier&lt;FunctionExecutable&gt;&amp; executable : m_functionExprs)
3200             insertFunctionGaps(executable);
3201 
3202         metadata.m_basicBlockLocation = basicBlockLocation;
3203     }
3204 }
3205 
3206 #if ENABLE(JIT)
3207 void CodeBlock::setPCToCodeOriginMap(std::unique_ptr&lt;PCToCodeOriginMap&gt;&amp;&amp; map)
3208 {
3209     ConcurrentJSLocker locker(m_lock);
3210     ensureJITData(locker).m_pcToCodeOriginMap = WTFMove(map);
3211 }
3212 
3213 Optional&lt;CodeOrigin&gt; CodeBlock::findPC(void* pc)
3214 {
3215     {
3216         ConcurrentJSLocker locker(m_lock);
3217         if (auto* jitData = m_jitData.get()) {
3218             if (jitData-&gt;m_pcToCodeOriginMap) {
3219                 if (Optional&lt;CodeOrigin&gt; codeOrigin = jitData-&gt;m_pcToCodeOriginMap-&gt;findPC(pc))
3220                     return codeOrigin;
3221             }
3222 
3223             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
3224                 if (stubInfo-&gt;containsPC(pc))
3225                     return Optional&lt;CodeOrigin&gt;(stubInfo-&gt;codeOrigin);
3226             }
3227         }
3228     }
3229 
3230     if (Optional&lt;CodeOrigin&gt; codeOrigin = m_jitCode-&gt;findPC(this, pc))
3231         return codeOrigin;
3232 
3233     return WTF::nullopt;
3234 }
3235 #endif // ENABLE(JIT)
3236 
3237 Optional&lt;unsigned&gt; CodeBlock::bytecodeOffsetFromCallSiteIndex(CallSiteIndex callSiteIndex)
3238 {
3239     Optional&lt;unsigned&gt; bytecodeOffset;
<a name="149" id="anc149"></a><span class="line-modified">3240     JITType jitType = this-&gt;jitType();</span>
<span class="line-modified">3241     if (jitType == JITType::InterpreterThunk || jitType == JITType::BaselineJIT) {</span>
3242 #if USE(JSVALUE64)
3243         bytecodeOffset = callSiteIndex.bits();
3244 #else
3245         Instruction* instruction = bitwise_cast&lt;Instruction*&gt;(callSiteIndex.bits());
3246         bytecodeOffset = this-&gt;bytecodeOffset(instruction);
3247 #endif
<a name="150" id="anc150"></a><span class="line-modified">3248     } else if (jitType == JITType::DFGJIT || jitType == JITType::FTLJIT) {</span>
3249 #if ENABLE(DFG_JIT)
3250         RELEASE_ASSERT(canGetCodeOrigin(callSiteIndex));
3251         CodeOrigin origin = codeOrigin(callSiteIndex);
<a name="151" id="anc151"></a><span class="line-modified">3252         bytecodeOffset = origin.bytecodeIndex();</span>
3253 #else
3254         RELEASE_ASSERT_NOT_REACHED();
3255 #endif
3256     }
3257 
3258     return bytecodeOffset;
3259 }
3260 
3261 int32_t CodeBlock::thresholdForJIT(int32_t threshold)
3262 {
3263     switch (unlinkedCodeBlock()-&gt;didOptimize()) {
3264     case MixedTriState:
3265         return threshold;
3266     case FalseTriState:
3267         return threshold * 4;
3268     case TrueTriState:
3269         return threshold / 2;
3270     }
3271     ASSERT_NOT_REACHED();
3272     return threshold;
3273 }
3274 
3275 void CodeBlock::jitAfterWarmUp()
3276 {
3277     m_llintExecuteCounter.setNewThreshold(thresholdForJIT(Options::thresholdForJITAfterWarmUp()), this);
3278 }
3279 
3280 void CodeBlock::jitSoon()
3281 {
3282     m_llintExecuteCounter.setNewThreshold(thresholdForJIT(Options::thresholdForJITSoon()), this);
3283 }
3284 
3285 bool CodeBlock::hasInstalledVMTrapBreakpoints() const
3286 {
3287 #if ENABLE(SIGNAL_BASED_VM_TRAPS)
3288     // This function may be called from a signal handler. We need to be
3289     // careful to not call anything that is not signal handler safe, e.g.
3290     // we should not perturb the refCount of m_jitCode.
3291     if (!JITCode::isOptimizingJIT(jitType()))
3292         return false;
3293     return m_jitCode-&gt;dfgCommon()-&gt;hasInstalledVMTrapsBreakpoints();
3294 #else
3295     return false;
3296 #endif
3297 }
3298 
3299 bool CodeBlock::installVMTrapBreakpoints()
3300 {
3301 #if ENABLE(SIGNAL_BASED_VM_TRAPS)
3302     // This function may be called from a signal handler. We need to be
3303     // careful to not call anything that is not signal handler safe, e.g.
3304     // we should not perturb the refCount of m_jitCode.
3305     if (!JITCode::isOptimizingJIT(jitType()))
3306         return false;
3307     auto&amp; commonData = *m_jitCode-&gt;dfgCommon();
3308     commonData.installVMTrapBreakpoints(this);
3309     return true;
3310 #else
3311     UNREACHABLE_FOR_PLATFORM();
3312     return false;
3313 #endif
3314 }
3315 
3316 void CodeBlock::dumpMathICStats()
3317 {
3318 #if ENABLE(MATH_IC_STATS)
3319     double numAdds = 0.0;
3320     double totalAddSize = 0.0;
3321     double numMuls = 0.0;
3322     double totalMulSize = 0.0;
3323     double numNegs = 0.0;
3324     double totalNegSize = 0.0;
3325     double numSubs = 0.0;
3326     double totalSubSize = 0.0;
3327 
3328     auto countICs = [&amp;] (CodeBlock* codeBlock) {
3329         if (auto* jitData = codeBlock-&gt;m_jitData.get()) {
3330             for (JITAddIC* addIC : jitData-&gt;m_addICs) {
3331                 numAdds++;
3332                 totalAddSize += addIC-&gt;codeSize();
3333             }
3334 
3335             for (JITMulIC* mulIC : jitData-&gt;m_mulICs) {
3336                 numMuls++;
3337                 totalMulSize += mulIC-&gt;codeSize();
3338             }
3339 
3340             for (JITNegIC* negIC : jitData-&gt;m_negICs) {
3341                 numNegs++;
3342                 totalNegSize += negIC-&gt;codeSize();
3343             }
3344 
3345             for (JITSubIC* subIC : jitData-&gt;m_subICs) {
3346                 numSubs++;
3347                 totalSubSize += subIC-&gt;codeSize();
3348             }
3349         }
3350     };
3351     heap()-&gt;forEachCodeBlock(countICs);
3352 
3353     dataLog(&quot;Num Adds: &quot;, numAdds, &quot;\n&quot;);
3354     dataLog(&quot;Total Add size in bytes: &quot;, totalAddSize, &quot;\n&quot;);
3355     dataLog(&quot;Average Add size: &quot;, totalAddSize / numAdds, &quot;\n&quot;);
3356     dataLog(&quot;\n&quot;);
3357     dataLog(&quot;Num Muls: &quot;, numMuls, &quot;\n&quot;);
3358     dataLog(&quot;Total Mul size in bytes: &quot;, totalMulSize, &quot;\n&quot;);
3359     dataLog(&quot;Average Mul size: &quot;, totalMulSize / numMuls, &quot;\n&quot;);
3360     dataLog(&quot;\n&quot;);
3361     dataLog(&quot;Num Negs: &quot;, numNegs, &quot;\n&quot;);
3362     dataLog(&quot;Total Neg size in bytes: &quot;, totalNegSize, &quot;\n&quot;);
3363     dataLog(&quot;Average Neg size: &quot;, totalNegSize / numNegs, &quot;\n&quot;);
3364     dataLog(&quot;\n&quot;);
3365     dataLog(&quot;Num Subs: &quot;, numSubs, &quot;\n&quot;);
3366     dataLog(&quot;Total Sub size in bytes: &quot;, totalSubSize, &quot;\n&quot;);
3367     dataLog(&quot;Average Sub size: &quot;, totalSubSize / numSubs, &quot;\n&quot;);
3368 
3369     dataLog(&quot;-----------------------\n&quot;);
3370 #endif
3371 }
3372 
3373 void setPrinter(Printer::PrintRecord&amp; record, CodeBlock* codeBlock)
3374 {
3375     Printer::setPrinter(record, toCString(codeBlock));
3376 }
3377 
3378 } // namespace JSC
3379 
3380 namespace WTF {
3381 
3382 void printInternal(PrintStream&amp; out, JSC::CodeBlock* codeBlock)
3383 {
3384     if (UNLIKELY(!codeBlock)) {
3385         out.print(&quot;&lt;null codeBlock&gt;&quot;);
3386         return;
3387     }
3388     out.print(*codeBlock);
3389 }
3390 
3391 } // namespace WTF
<a name="152" id="anc152"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="152" type="hidden" />
</body>
</html>