<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/wasm/WasmOperations.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;WasmOperations.h&quot;
 28 
 29 #if ENABLE(WEBASSEMBLY)
 30 
 31 #include &quot;ProbeContext.h&quot;
 32 #include &quot;WasmCallee.h&quot;
 33 #include &quot;WasmContextInlines.h&quot;
 34 #include &quot;WasmInstance.h&quot;
 35 #include &quot;WasmMemory.h&quot;
 36 #include &quot;WasmNameSection.h&quot;
 37 #include &quot;WasmOMGForOSREntryPlan.h&quot;
 38 #include &quot;WasmOMGPlan.h&quot;
 39 #include &quot;WasmOSREntryData.h&quot;
 40 #include &quot;WasmSignatureInlines.h&quot;
 41 #include &quot;WasmWorklist.h&quot;
 42 #include &lt;wtf/DataLog.h&gt;
 43 #include &lt;wtf/Locker.h&gt;
 44 #include &lt;wtf/MonotonicTime.h&gt;
 45 #include &lt;wtf/StdLibExtras.h&gt;
 46 
 47 namespace JSC { namespace Wasm {
 48 
 49 static bool shouldTriggerOMGCompile(TierUpCount&amp; tierUp, OMGCallee* replacement, uint32_t functionIndex)
 50 {
 51     if (!replacement &amp;&amp; !tierUp.checkIfOptimizationThresholdReached()) {
 52         dataLogLnIf(Options::verboseOSR(), &quot;delayOMGCompile counter = &quot;, tierUp, &quot; for &quot;, functionIndex);
 53         dataLogLnIf(Options::verboseOSR(), &quot;Choosing not to OMG-optimize &quot;, functionIndex, &quot; yet.&quot;);
 54         return false;
 55     }
 56     return true;
 57 }
 58 
 59 static void triggerOMGReplacementCompile(TierUpCount&amp; tierUp, OMGCallee* replacement, Instance* instance, Wasm::CodeBlock&amp; codeBlock, uint32_t functionIndex)
 60 {
 61     if (replacement) {
 62         tierUp.optimizeSoon(functionIndex);
 63         return;
 64     }
 65 
 66     bool compile = false;
 67     {
 68         auto locker = holdLock(tierUp.getLock());
 69         switch (tierUp.m_compilationStatusForOMG) {
 70         case TierUpCount::CompilationStatus::StartCompilation:
 71             tierUp.setOptimizationThresholdBasedOnCompilationResult(functionIndex, CompilationDeferred);
 72             return;
 73         case TierUpCount::CompilationStatus::NotCompiled:
 74             compile = true;
 75             tierUp.m_compilationStatusForOMG = TierUpCount::CompilationStatus::StartCompilation;
 76             break;
 77         default:
 78             break;
 79         }
 80     }
 81 
 82     if (compile) {
 83         dataLogLnIf(Options::verboseOSR(), &quot;triggerOMGReplacement for &quot;, functionIndex);
 84         // We need to compile the code.
 85         Ref&lt;Plan&gt; plan = adoptRef(*new OMGPlan(instance-&gt;context(), Ref&lt;Wasm::Module&gt;(instance-&gt;module()), functionIndex, codeBlock.mode(), Plan::dontFinalize()));
 86         ensureWorklist().enqueue(plan.copyRef());
 87         if (UNLIKELY(!Options::useConcurrentJIT()))
 88             plan-&gt;waitForCompletion();
 89         else
 90             tierUp.setOptimizationThresholdBasedOnCompilationResult(functionIndex, CompilationDeferred);
 91     }
 92 }
 93 
 94 SUPPRESS_ASAN
 95 static void doOSREntry(Instance* instance, Probe::Context&amp; context, BBQCallee&amp; callee, OMGForOSREntryCallee&amp; osrEntryCallee, OSREntryData&amp; osrEntryData)
 96 {
 97     auto returnWithoutOSREntry = [&amp;] {
 98         context.gpr(GPRInfo::argumentGPR0) = 0;
 99     };
100 
101     uint64_t* buffer = instance-&gt;context()-&gt;scratchBufferForSize(osrEntryCallee.osrEntryScratchBufferSize());
102     if (!buffer)
103         return returnWithoutOSREntry();
104 
105     dataLogLnIf(Options::verboseOSR(), osrEntryData.functionIndex(), &quot;:OMG OSR entry: got entry callee &quot;, RawPointer(&amp;osrEntryCallee));
106 
107     // 1. Place required values in scratch buffer.
108     for (unsigned index = 0; index &lt; osrEntryData.values().size(); ++index) {
109         const OSREntryValue&amp; value = osrEntryData.values()[index];
110         dataLogLnIf(Options::verboseOSR(), &quot;OMG OSR entry values[&quot;, index, &quot;] &quot;, value.type(), &quot; &quot;, value);
111         if (value.isGPR()) {
112             switch (value.type().kind()) {
113             case B3::Float:
114             case B3::Double:
115                 RELEASE_ASSERT_NOT_REACHED();
116             default:
117                 *bitwise_cast&lt;uint64_t*&gt;(buffer + index) = context.gpr(value.gpr());
118             }
119         } else if (value.isFPR()) {
120             switch (value.type().kind()) {
121             case B3::Float:
122             case B3::Double:
123                 *bitwise_cast&lt;double*&gt;(buffer + index) = context.fpr(value.fpr());
124                 break;
125             default:
126                 RELEASE_ASSERT_NOT_REACHED();
127             }
128         } else if (value.isConstant()) {
129             switch (value.type().kind()) {
130             case B3::Float:
131                 *bitwise_cast&lt;float*&gt;(buffer + index) = value.floatValue();
132                 break;
133             case B3::Double:
134                 *bitwise_cast&lt;double*&gt;(buffer + index) = value.doubleValue();
135                 break;
136             default:
137                 *bitwise_cast&lt;uint64_t*&gt;(buffer + index) = value.value();
138             }
139         } else if (value.isStack()) {
140             switch (value.type().kind()) {
141             case B3::Float:
142                 *bitwise_cast&lt;float*&gt;(buffer + index) = *bitwise_cast&lt;float*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.fp()) + value.offsetFromFP());
143                 break;
144             case B3::Double:
145                 *bitwise_cast&lt;double*&gt;(buffer + index) = *bitwise_cast&lt;double*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.fp()) + value.offsetFromFP());
146                 break;
147             default:
148                 *bitwise_cast&lt;uint64_t*&gt;(buffer + index) = *bitwise_cast&lt;uint64_t*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.fp()) + value.offsetFromFP());
149                 break;
150             }
151         } else if (value.isStackArgument()) {
152             switch (value.type().kind()) {
153             case B3::Float:
154                 *bitwise_cast&lt;float*&gt;(buffer + index) = *bitwise_cast&lt;float*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.sp()) + value.offsetFromSP());
155                 break;
156             case B3::Double:
157                 *bitwise_cast&lt;double*&gt;(buffer + index) = *bitwise_cast&lt;double*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.sp()) + value.offsetFromSP());
158                 break;
159             default:
160                 *bitwise_cast&lt;uint64_t*&gt;(buffer + index) = *bitwise_cast&lt;uint64_t*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.sp()) + value.offsetFromSP());
161                 break;
162             }
163         } else
164             RELEASE_ASSERT_NOT_REACHED();
165     }
166 
167     // 2. Restore callee saves.
168     RegisterSet dontRestoreRegisters = RegisterSet::stackRegisters();
169     for (const RegisterAtOffset&amp; entry : *callee.calleeSaveRegisters()) {
170         if (dontRestoreRegisters.get(entry.reg()))
171             continue;
172         if (entry.reg().isGPR())
173             context.gpr(entry.reg().gpr()) = *bitwise_cast&lt;UCPURegister*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.fp()) + entry.offset());
174         else
175             context.fpr(entry.reg().fpr()) = *bitwise_cast&lt;double*&gt;(bitwise_cast&lt;uint8_t*&gt;(context.fp()) + entry.offset());
176     }
177 
178     // 3. Function epilogue, like a tail-call.
179     UCPURegister* framePointer = bitwise_cast&lt;UCPURegister*&gt;(context.fp());
180 #if CPU(X86_64)
181     // move(framePointerRegister, stackPointerRegister);
182     // pop(framePointerRegister);
183     context.fp() = bitwise_cast&lt;UCPURegister*&gt;(*framePointer);
184     context.sp() = framePointer + 1;
185     static_assert(AssemblyHelpers::prologueStackPointerDelta() == sizeof(void*) * 1);
186 #elif CPU(ARM64E) || CPU(ARM64)
187     // move(framePointerRegister, stackPointerRegister);
188     // popPair(framePointerRegister, linkRegister);
189     context.fp() = bitwise_cast&lt;UCPURegister*&gt;(*framePointer);
190     context.gpr(ARM64Registers::lr) = bitwise_cast&lt;UCPURegister&gt;(*(framePointer + 1));
191     context.sp() = framePointer + 2;
192     static_assert(AssemblyHelpers::prologueStackPointerDelta() == sizeof(void*) * 2);
193 #if CPU(ARM64E)
194     // LR needs to be untagged since OSR entry function prologue will tag it with SP. This is similar to tail-call.
195     context.gpr(ARM64Registers::lr) = bitwise_cast&lt;UCPURegister&gt;(untagCodePtr(context.gpr&lt;void*&gt;(ARM64Registers::lr), bitwise_cast&lt;PtrTag&gt;(context.sp())));
196 #endif
197 #else
198 #error Unsupported architecture.
199 #endif
200     // 4. Configure argument registers to jump to OSR entry from the caller of this runtime function.
201     context.gpr(GPRInfo::argumentGPR0) = bitwise_cast&lt;UCPURegister&gt;(buffer);
202     context.gpr(GPRInfo::argumentGPR1) = bitwise_cast&lt;UCPURegister&gt;(osrEntryCallee.entrypoint().executableAddress&lt;&gt;());
203 }
204 
205 void JIT_OPERATION triggerOSREntryNow(Probe::Context&amp; context)
206 {
207     OSREntryData&amp; osrEntryData = *context.arg&lt;OSREntryData*&gt;();
208     uint32_t functionIndex = osrEntryData.functionIndex();
209     uint32_t loopIndex = osrEntryData.loopIndex();
210     Instance* instance = Wasm::Context::tryLoadInstanceFromTLS();
211     if (!instance)
212         instance = context.gpr&lt;Instance*&gt;(Wasm::PinnedRegisterInfo::get().wasmContextInstancePointer);
213 
214     auto returnWithoutOSREntry = [&amp;] {
215         context.gpr(GPRInfo::argumentGPR0) = 0;
216     };
217 
218     Wasm::CodeBlock&amp; codeBlock = *instance-&gt;codeBlock();
219     ASSERT(instance-&gt;memory()-&gt;mode() == codeBlock.mode());
220 
221     uint32_t functionIndexInSpace = functionIndex + codeBlock.functionImportCount();
222     ASSERT(codeBlock.wasmBBQCalleeFromFunctionIndexSpace(functionIndexInSpace).compilationMode() == Wasm::CompilationMode::BBQMode);
223     BBQCallee&amp; callee = static_cast&lt;BBQCallee&amp;&gt;(codeBlock.wasmBBQCalleeFromFunctionIndexSpace(functionIndexInSpace));
224     TierUpCount&amp; tierUp = *callee.tierUpCount();
225     dataLogLnIf(Options::verboseOSR(), &quot;Consider OMGForOSREntryPlan for [&quot;, functionIndex, &quot;] loopIndex#&quot;, loopIndex, &quot; with executeCounter = &quot;, tierUp, &quot; &quot;, RawPointer(callee.replacement()));
226 
227     if (!Options::useWebAssemblyOSR()) {
228         if (shouldTriggerOMGCompile(tierUp, callee.replacement(), functionIndex))
229             triggerOMGReplacementCompile(tierUp, callee.replacement(), instance, codeBlock, functionIndex);
230 
231         // We already have an OMG replacement.
232         if (callee.replacement()) {
233             // No OSR entry points. Just defer indefinitely.
234             if (tierUp.osrEntryTriggers().isEmpty()) {
235                 tierUp.dontOptimizeAnytimeSoon(functionIndex);
236                 return;
237             }
238 
239             // Found one OSR entry point. Since we do not have a way to jettison Wasm::Callee right now, this means that tierUp function is now meaningless.
240             // Not call it as much as possible.
241             if (callee.osrEntryCallee()) {
242                 tierUp.dontOptimizeAnytimeSoon(functionIndex);
243                 return;
244             }
245         }
246         return returnWithoutOSREntry();
247     }
248 
249     TierUpCount::CompilationStatus compilationStatus = TierUpCount::CompilationStatus::NotCompiled;
250     {
251         auto locker = holdLock(tierUp.getLock());
252         compilationStatus = tierUp.m_compilationStatusForOMGForOSREntry;
253     }
254 
255     bool triggeredSlowPathToStartCompilation = false;
256     switch (tierUp.osrEntryTriggers()[loopIndex]) {
257     case TierUpCount::TriggerReason::DontTrigger:
258         // The trigger isn&#39;t set, we entered because the counter reached its
259         // threshold.
260         break;
261     case TierUpCount::TriggerReason::CompilationDone:
262         // The trigger was set because compilation completed. Don&#39;t unset it
263         // so that further BBQ executions OSR enter as well.
264         break;
265     case TierUpCount::TriggerReason::StartCompilation: {
266         // We were asked to enter as soon as possible and start compiling an
267         // entry for the current loopIndex. Unset this trigger so we
268         // don&#39;t continually enter.
269         auto locker = holdLock(tierUp.getLock());
270         TierUpCount::TriggerReason reason = tierUp.osrEntryTriggers()[loopIndex];
271         if (reason == TierUpCount::TriggerReason::StartCompilation) {
272             tierUp.osrEntryTriggers()[loopIndex] = TierUpCount::TriggerReason::DontTrigger;
273             triggeredSlowPathToStartCompilation = true;
274         }
275         break;
276     }
277     }
278 
279     if (compilationStatus == TierUpCount::CompilationStatus::StartCompilation) {
280         dataLogLnIf(Options::verboseOSR(), &quot;delayOMGCompile still compiling for &quot;, functionIndex);
281         tierUp.setOptimizationThresholdBasedOnCompilationResult(functionIndex, CompilationDeferred);
282         return returnWithoutOSREntry();
283     }
284 
285     if (OMGForOSREntryCallee* osrEntryCallee = callee.osrEntryCallee()) {
286         if (osrEntryCallee-&gt;loopIndex() == loopIndex)
287             return doOSREntry(instance, context, callee, *osrEntryCallee, osrEntryData);
288     }
289 
290     if (!shouldTriggerOMGCompile(tierUp, callee.replacement(), functionIndex) &amp;&amp; !triggeredSlowPathToStartCompilation)
291         return returnWithoutOSREntry();
292 
293     if (!triggeredSlowPathToStartCompilation) {
294         triggerOMGReplacementCompile(tierUp, callee.replacement(), instance, codeBlock, functionIndex);
295 
296         if (!callee.replacement())
297             return returnWithoutOSREntry();
298     }
299 
300     if (OMGForOSREntryCallee* osrEntryCallee = callee.osrEntryCallee()) {
301         if (osrEntryCallee-&gt;loopIndex() == loopIndex)
302             return doOSREntry(instance, context, callee, *osrEntryCallee, osrEntryData);
303         tierUp.dontOptimizeAnytimeSoon(functionIndex);
304         return returnWithoutOSREntry();
305     }
306 
307     // Instead of triggering OSR entry compilation in inner loop, try outer loop&#39;s trigger immediately effective (setting TriggerReason::StartCompilation) and
308     // let outer loop attempt to compile.
309     if (!triggeredSlowPathToStartCompilation) {
310         // An inner loop didn&#39;t specifically ask for us to kick off a compilation. This means the counter
311         // crossed its threshold. We either fall through and kick off a compile for originBytecodeIndex,
312         // or we flag an outer loop to immediately try to compile itself. If there are outer loops,
313         // we first try to make them compile themselves. But we will eventually fall back to compiling
314         // a progressively inner loop if it takes too long for control to reach an outer loop.
315 
316         auto tryTriggerOuterLoopToCompile = [&amp;] {
317             // We start with the outermost loop and make our way inwards (hence why we iterate the vector in reverse).
318             // Our policy is that we will trigger an outer loop to compile immediately when program control reaches it.
319             // If program control is taking too long to reach that outer loop, we progressively move inwards, meaning,
320             // we&#39;ll eventually trigger some loop that is executing to compile. We start with trying to compile outer
321             // loops since we believe outer loop compilations reveal the best opportunities for optimizing code.
322             uint32_t currentLoopIndex = tierUp.outerLoops()[loopIndex];
323             auto locker = holdLock(tierUp.getLock());
324 
325             // We already started OMGForOSREntryPlan.
326             if (callee.didStartCompilingOSREntryCallee())
327                 return false;
328 
329             while (currentLoopIndex != UINT32_MAX) {
330                 if (tierUp.osrEntryTriggers()[currentLoopIndex] == TierUpCount::TriggerReason::StartCompilation) {
331                     // This means that we already asked this loop to compile. If we&#39;ve reached here, it
332                     // means program control has not yet reached that loop. So it&#39;s taking too long to compile.
333                     // So we move on to asking the inner loop of this loop to compile itself.
334                     currentLoopIndex = tierUp.outerLoops()[currentLoopIndex];
335                     continue;
336                 }
337 
338                 // This is where we ask the outer to loop to immediately compile itself if program
339                 // control reaches it.
340                 dataLogLnIf(Options::verboseOSR(), &quot;Inner-loop loopIndex#&quot;, loopIndex, &quot; in &quot;, functionIndex, &quot; setting parent loop loopIndex#&quot;, currentLoopIndex, &quot;&#39;s trigger and backing off.&quot;);
341                 tierUp.osrEntryTriggers()[currentLoopIndex] = TierUpCount::TriggerReason::StartCompilation;
342                 return true;
343             }
344             return false;
345         };
346 
347         if (tryTriggerOuterLoopToCompile()) {
348             tierUp.setOptimizationThresholdBasedOnCompilationResult(functionIndex, CompilationDeferred);
349             return returnWithoutOSREntry();
350         }
351     }
352 
353     bool startOSREntryCompilation = false;
354     {
355         auto locker = holdLock(tierUp.getLock());
356         if (tierUp.m_compilationStatusForOMGForOSREntry == TierUpCount::CompilationStatus::NotCompiled) {
357             tierUp.m_compilationStatusForOMGForOSREntry = TierUpCount::CompilationStatus::StartCompilation;
358             startOSREntryCompilation = true;
359             // Currently, we do not have a way to jettison wasm code. This means that once we decide to compile OSR entry code for a particular loopIndex,
360             // we cannot throw the compiled code so long as Wasm module is live. We immediately disable all the triggers.
361             for (auto&amp; trigger : tierUp.osrEntryTriggers())
362                 trigger = TierUpCount::TriggerReason::DontTrigger;
363         }
364     }
365 
366     if (startOSREntryCompilation) {
367         dataLogLnIf(Options::verboseOSR(), &quot;triggerOMGOSR for &quot;, functionIndex);
368         Ref&lt;Plan&gt; plan = adoptRef(*new OMGForOSREntryPlan(instance-&gt;context(), Ref&lt;Wasm::Module&gt;(instance-&gt;module()), Ref&lt;Wasm::BBQCallee&gt;(callee), functionIndex, loopIndex, codeBlock.mode(), Plan::dontFinalize()));
369         ensureWorklist().enqueue(plan.copyRef());
370         if (UNLIKELY(!Options::useConcurrentJIT()))
371             plan-&gt;waitForCompletion();
372         else
373             tierUp.setOptimizationThresholdBasedOnCompilationResult(functionIndex, CompilationDeferred);
374     }
375 
376     OMGForOSREntryCallee* osrEntryCallee = callee.osrEntryCallee();
377     if (!osrEntryCallee) {
378         tierUp.setOptimizationThresholdBasedOnCompilationResult(functionIndex, CompilationDeferred);
379         return returnWithoutOSREntry();
380     }
381 
382     if (osrEntryCallee-&gt;loopIndex() == loopIndex)
383         return doOSREntry(instance, context, callee, *osrEntryCallee, osrEntryData);
384 
385     tierUp.dontOptimizeAnytimeSoon(functionIndex);
386     return returnWithoutOSREntry();
387 }
388 
389 void JIT_OPERATION triggerTierUpNow(Instance* instance, uint32_t functionIndex)
390 {
391     Wasm::CodeBlock&amp; codeBlock = *instance-&gt;codeBlock();
392     ASSERT(instance-&gt;memory()-&gt;mode() == codeBlock.mode());
393 
394     uint32_t functionIndexInSpace = functionIndex + codeBlock.functionImportCount();
395     ASSERT(codeBlock.wasmBBQCalleeFromFunctionIndexSpace(functionIndexInSpace).compilationMode() == Wasm::CompilationMode::BBQMode);
396     BBQCallee&amp; callee = static_cast&lt;BBQCallee&amp;&gt;(codeBlock.wasmBBQCalleeFromFunctionIndexSpace(functionIndexInSpace));
397     TierUpCount&amp; tierUp = *callee.tierUpCount();
398     dataLogLnIf(Options::verboseOSR(), &quot;Consider OMGPlan for [&quot;, functionIndex, &quot;] with executeCounter = &quot;, tierUp, &quot; &quot;, RawPointer(callee.replacement()));
399 
400     if (shouldTriggerOMGCompile(tierUp, callee.replacement(), functionIndex))
401         triggerOMGReplacementCompile(tierUp, callee.replacement(), instance, codeBlock, functionIndex);
402 
403     // We already have an OMG replacement.
404     if (callee.replacement()) {
405         // No OSR entry points. Just defer indefinitely.
406         if (tierUp.osrEntryTriggers().isEmpty()) {
407             dataLogLnIf(Options::verboseOSR(), &quot;delayOMGCompile replacement in place, delaying indefinitely for &quot;, functionIndex);
408             tierUp.dontOptimizeAnytimeSoon(functionIndex);
409             return;
410         }
411 
412         // Found one OSR entry point. Since we do not have a way to jettison Wasm::Callee right now, this means that tierUp function is now meaningless.
413         // Not call it as much as possible.
414         if (callee.osrEntryCallee()) {
415             dataLogLnIf(Options::verboseOSR(), &quot;delayOMGCompile trigger in place, delaying indefinitely for &quot;, functionIndex);
416             tierUp.dontOptimizeAnytimeSoon(functionIndex);
417             return;
418         }
419     }
420 }
421 
422 } } // namespace JSC::Wasm
423 
424 #endif // ENABLE(WEBASSEMBLY)
    </pre>
  </body>
</html>