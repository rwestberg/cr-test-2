<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirAllocateRegistersAndStackAndGenerateCode.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;AirAllocateRegistersAndStackAndGenerateCode.h&quot;
 28 
 29 #if ENABLE(B3_JIT)
 30 
 31 #include &quot;AirBlockInsertionSet.h&quot;
 32 #include &quot;AirCode.h&quot;
 33 #include &quot;AirHandleCalleeSaves.h&quot;
 34 #include &quot;AirLowerStackArgs.h&quot;
 35 #include &quot;AirStackAllocation.h&quot;
 36 #include &quot;AirTmpMap.h&quot;
 37 #include &quot;CCallHelpers.h&quot;
 38 #include &quot;DisallowMacroScratchRegisterUsage.h&quot;
 39 
 40 namespace JSC { namespace B3 { namespace Air {
 41 
 42 GenerateAndAllocateRegisters::GenerateAndAllocateRegisters(Code&amp; code)
 43     : m_code(code)
 44     , m_map(code)
 45 { }
 46 
 47 void GenerateAndAllocateRegisters::buildLiveRanges(UnifiedTmpLiveness&amp; liveness)
 48 {
 49     m_liveRangeEnd = TmpMap&lt;size_t&gt;(m_code, 0);
 50 
 51     m_globalInstIndex = 0;
 52     for (BasicBlock* block : m_code) {
 53         for (Tmp tmp : liveness.liveAtHead(block)) {
 54             if (!tmp.isReg())
 55                 m_liveRangeEnd[tmp] = m_globalInstIndex;
 56         }
 57         for (Inst&amp; inst : *block) {
 58             inst.forEachTmpFast([&amp;] (Tmp tmp) {
 59                 if (!tmp.isReg())
 60                     m_liveRangeEnd[tmp] = m_globalInstIndex;
 61             });
 62             ++m_globalInstIndex;
 63         }
 64         for (Tmp tmp : liveness.liveAtTail(block)) {
 65             if (!tmp.isReg())
 66                 m_liveRangeEnd[tmp] = m_globalInstIndex;
 67         }
 68     }
 69 }
 70 
 71 void GenerateAndAllocateRegisters::insertBlocksForFlushAfterTerminalPatchpoints()
 72 {
 73     BlockInsertionSet blockInsertionSet(m_code);
 74     for (BasicBlock* block : m_code) {
 75         Inst&amp; inst = block-&gt;last();
 76         if (inst.kind.opcode != Patch)
 77             continue;
 78 
 79         HashMap&lt;Tmp, Arg*&gt; needToDef;
 80 
 81         inst.forEachArg([&amp;] (Arg&amp; arg, Arg::Role role, Bank, Width) {
 82             if (!arg.isTmp())
 83                 return;
 84             Tmp tmp = arg.tmp();
 85             if (Arg::isAnyDef(role) &amp;&amp; !tmp.isReg())
 86                 needToDef.add(tmp, &amp;arg);
 87         });
 88 
 89         if (needToDef.isEmpty())
 90             continue;
 91 
 92         for (FrequentedBlock&amp; frequentedSuccessor : block-&gt;successors()) {
 93             BasicBlock* successor = frequentedSuccessor.block();
 94             BasicBlock* newBlock = blockInsertionSet.insertBefore(successor, successor-&gt;frequency());
 95             newBlock-&gt;appendInst(Inst(Jump, inst.origin));
 96             newBlock-&gt;setSuccessors(successor);
 97             newBlock-&gt;addPredecessor(block);
 98             frequentedSuccessor.block() = newBlock;
 99             successor-&gt;replacePredecessor(block, newBlock);
100 
101             m_blocksAfterTerminalPatchForSpilling.add(newBlock, PatchSpillData { CCallHelpers::Jump(), CCallHelpers::Label(), needToDef });
102         }
103     }
104 
105     blockInsertionSet.execute();
106 }
107 
108 static ALWAYS_INLINE CCallHelpers::Address callFrameAddr(CCallHelpers&amp; jit, intptr_t offsetFromFP)
109 {
110     if (isX86()) {
111         ASSERT(Arg::addr(Air::Tmp(GPRInfo::callFrameRegister), offsetFromFP).isValidForm(Width64));
112         return CCallHelpers::Address(GPRInfo::callFrameRegister, offsetFromFP);
113     }
114 
115     ASSERT(pinnedExtendedOffsetAddrRegister());
116     auto addr = Arg::addr(Air::Tmp(GPRInfo::callFrameRegister), offsetFromFP);
117     if (addr.isValidForm(Width64))
118         return CCallHelpers::Address(GPRInfo::callFrameRegister, offsetFromFP);
119     GPRReg reg = *pinnedExtendedOffsetAddrRegister();
120     jit.move(CCallHelpers::TrustedImmPtr(offsetFromFP), reg);
121     jit.add64(GPRInfo::callFrameRegister, reg);
122     return CCallHelpers::Address(reg);
123 }
124 
125 ALWAYS_INLINE void GenerateAndAllocateRegisters::flush(Tmp tmp, Reg reg)
126 {
127     ASSERT(tmp);
128     intptr_t offset = m_map[tmp].spillSlot-&gt;offsetFromFP();
129     if (tmp.isGP())
130         m_jit-&gt;store64(reg.gpr(), callFrameAddr(*m_jit, offset));
131     else
132         m_jit-&gt;storeDouble(reg.fpr(), callFrameAddr(*m_jit, offset));
133 }
134 
135 ALWAYS_INLINE void GenerateAndAllocateRegisters::spill(Tmp tmp, Reg reg)
136 {
137     ASSERT(reg);
138     ASSERT(m_map[tmp].reg == reg);
139     m_availableRegs[tmp.bank()].set(reg);
140     m_currentAllocation-&gt;at(reg) = Tmp();
141     flush(tmp, reg);
142     m_map[tmp].reg = Reg();
143 }
144 
145 ALWAYS_INLINE void GenerateAndAllocateRegisters::alloc(Tmp tmp, Reg reg, bool isDef)
146 {
147     if (Tmp occupyingTmp = m_currentAllocation-&gt;at(reg))
148         spill(occupyingTmp, reg);
149     else {
150         ASSERT(!m_currentAllocation-&gt;at(reg));
151         ASSERT(m_availableRegs[tmp.bank()].get(reg));
152     }
153 
154     m_map[tmp].reg = reg;
155     m_availableRegs[tmp.bank()].clear(reg);
156     m_currentAllocation-&gt;at(reg) = tmp;
157 
158     if (!isDef) {
159         intptr_t offset = m_map[tmp].spillSlot-&gt;offsetFromFP();
160         if (tmp.bank() == GP)
161             m_jit-&gt;load64(callFrameAddr(*m_jit, offset), reg.gpr());
162         else
163             m_jit-&gt;loadDouble(callFrameAddr(*m_jit, offset), reg.fpr());
164     }
165 }
166 
167 ALWAYS_INLINE void GenerateAndAllocateRegisters::freeDeadTmpsIfNeeded()
168 {
169     if (m_didAlreadyFreeDeadSlots)
170         return;
171 
172     m_didAlreadyFreeDeadSlots = true;
173     for (size_t i = 0; i &lt; m_currentAllocation-&gt;size(); ++i) {
174         Tmp tmp = m_currentAllocation-&gt;at(i);
175         if (!tmp)
176             continue;
177         if (tmp.isReg())
178             continue;
179         if (m_liveRangeEnd[tmp] &gt;= m_globalInstIndex)
180             continue;
181 
182         Reg reg = Reg::fromIndex(i);
183         m_map[tmp].reg = Reg();
184         m_availableRegs[tmp.bank()].set(reg);
185         m_currentAllocation-&gt;at(i) = Tmp();
186     }
187 }
188 
189 ALWAYS_INLINE bool GenerateAndAllocateRegisters::assignTmp(Tmp&amp; tmp, Bank bank, bool isDef)
190 {
191     ASSERT(!tmp.isReg());
192     if (Reg reg = m_map[tmp].reg) {
193         ASSERT(!m_namedDefdRegs.contains(reg));
194         tmp = Tmp(reg);
195         m_namedUsedRegs.set(reg);
196         ASSERT(!m_availableRegs[bank].get(reg));
197         return true;
198     }
199 
200     if (!m_availableRegs[bank].numberOfSetRegisters())
201         freeDeadTmpsIfNeeded();
202 
203     if (m_availableRegs[bank].numberOfSetRegisters()) {
204         // We first take an available register.
205         for (Reg reg : m_registers[bank]) {
206             if (m_namedUsedRegs.contains(reg) || m_namedDefdRegs.contains(reg))
207                 continue;
208             if (!m_availableRegs[bank].contains(reg))
209                 continue;
210             m_namedUsedRegs.set(reg); // At this point, it doesn&#39;t matter if we add it to the m_namedUsedRegs or m_namedDefdRegs. We just need to mark that we can&#39;t use it again.
211             alloc(tmp, reg, isDef);
212             tmp = Tmp(reg);
213             return true;
214         }
215 
216         RELEASE_ASSERT_NOT_REACHED();
217     }
218 
219     // Nothing was available, let&#39;s make some room.
220     for (Reg reg : m_registers[bank]) {
221         if (m_namedUsedRegs.contains(reg) || m_namedDefdRegs.contains(reg))
222             continue;
223 
224         m_namedUsedRegs.set(reg);
225 
226         alloc(tmp, reg, isDef);
227         tmp = Tmp(reg);
228         return true;
229     }
230 
231     // This can happen if we have a #WarmAnys &gt; #Available registers
232     return false;
233 }
234 
235 ALWAYS_INLINE bool GenerateAndAllocateRegisters::isDisallowedRegister(Reg reg)
236 {
237     return !m_allowedRegisters.get(reg);
238 }
239 
240 void GenerateAndAllocateRegisters::prepareForGeneration()
241 {
242     // We pessimistically assume we use all callee saves.
243     handleCalleeSaves(m_code, RegisterSet::calleeSaveRegisters());
244     allocateEscapedStackSlots(m_code);
245 
246     // Each Tmp gets its own stack slot.
247     auto createStackSlot = [&amp;] (const Tmp&amp; tmp) {
248         TmpData data;
249         data.spillSlot = m_code.addStackSlot(8, StackSlotKind::Spill);
250         data.reg = Reg();
251         m_map[tmp] = data;
252 #if !ASSERT_DISABLED
253         m_allTmps[tmp.bank()].append(tmp);
254 #endif
255     };
256 
257     m_code.forEachTmp([&amp;] (Tmp tmp) {
258         ASSERT(!tmp.isReg());
259         createStackSlot(tmp);
260     });
261 
262     m_allowedRegisters = RegisterSet();
263 
264     forEachBank([&amp;] (Bank bank) {
265         m_registers[bank] = m_code.regsInPriorityOrder(bank);
266 
267         for (Reg reg : m_registers[bank]) {
268             m_allowedRegisters.set(reg);
269             createStackSlot(Tmp(reg));
270         }
271     });
272 
273     {
274         unsigned nextIndex = 0;
275         for (StackSlot* slot : m_code.stackSlots()) {
276             if (slot-&gt;isLocked())
277                 continue;
278             intptr_t offset = -static_cast&lt;intptr_t&gt;(m_code.frameSize()) - static_cast&lt;intptr_t&gt;(nextIndex) * 8 - 8;
279             ++nextIndex;
280             slot-&gt;setOffsetFromFP(offset);
281         }
282     }
283 
284     updateFrameSizeBasedOnStackSlots(m_code);
285     m_code.setStackIsAllocated(true);
286 
287     lowerStackArgs(m_code);
288 
289     // Verify none of these passes add any tmps.
290 #if !ASSERT_DISABLED
291     forEachBank([&amp;] (Bank bank) {
292         ASSERT(m_allTmps[bank].size() - m_registers[bank].size() == m_code.numTmps(bank));
293     });
294 #endif
295 }
296 
297 void GenerateAndAllocateRegisters::generate(CCallHelpers&amp; jit)
298 {
299     m_jit = &amp;jit;
300 
301     TimingScope timingScope(&quot;Air::generateAndAllocateRegisters&quot;);
302 
303     insertBlocksForFlushAfterTerminalPatchpoints();
304 
305     DisallowMacroScratchRegisterUsage disallowScratch(*m_jit);
306 
307     UnifiedTmpLiveness liveness(m_code);
308     buildLiveRanges(liveness);
309 
310     IndexMap&lt;BasicBlock*, IndexMap&lt;Reg, Tmp&gt;&gt; currentAllocationMap(m_code.size());
311     {
312         IndexMap&lt;Reg, Tmp&gt; defaultCurrentAllocation(Reg::maxIndex() + 1);
313         for (BasicBlock* block : m_code)
314             currentAllocationMap[block] = defaultCurrentAllocation;
315 
316         // The only things live that are in registers at the root blocks are
317         // the explicitly named registers that are live.
318 
319         for (unsigned i = m_code.numEntrypoints(); i--;) {
320             BasicBlock* entrypoint = m_code.entrypoint(i).block();
321             for (Tmp tmp : liveness.liveAtHead(entrypoint)) {
322                 if (tmp.isReg())
323                     currentAllocationMap[entrypoint][tmp.reg()] = tmp;
324             }
325         }
326     }
327 
328     // And now, we generate code.
329     GenerationContext context;
330     context.code = &amp;m_code;
331     context.blockLabels.resize(m_code.size());
332     for (BasicBlock* block : m_code)
333         context.blockLabels[block] = Box&lt;CCallHelpers::Label&gt;::create();
334     IndexMap&lt;BasicBlock*, CCallHelpers::JumpList&gt; blockJumps(m_code.size());
335 
336     auto link = [&amp;] (CCallHelpers::Jump jump, BasicBlock* target) {
337         if (context.blockLabels[target]-&gt;isSet()) {
338             jump.linkTo(*context.blockLabels[target], m_jit);
339             return;
340         }
341 
342         blockJumps[target].append(jump);
343     };
344 
345     Disassembler* disassembler = m_code.disassembler();
346 
347     m_globalInstIndex = 0;
348 
349     for (BasicBlock* block : m_code) {
350         context.currentBlock = block;
351         context.indexInBlock = UINT_MAX;
352         blockJumps[block].link(m_jit);
353         CCallHelpers::Label label = m_jit-&gt;label();
354         *context.blockLabels[block] = label;
355 
356         if (disassembler)
357             disassembler-&gt;startBlock(block, *m_jit);
358 
359         if (Optional&lt;unsigned&gt; entrypointIndex = m_code.entrypointIndex(block)) {
360             ASSERT(m_code.isEntrypoint(block));
361             if (disassembler)
362                 disassembler-&gt;startEntrypoint(*m_jit);
363 
364             m_code.prologueGeneratorForEntrypoint(*entrypointIndex)-&gt;run(*m_jit, m_code);
365 
366             if (disassembler)
367                 disassembler-&gt;endEntrypoint(*m_jit);
368         } else
369             ASSERT(!m_code.isEntrypoint(block));
370 
371         auto startLabel = m_jit-&gt;labelIgnoringWatchpoints();
372 
373         {
374             auto iter = m_blocksAfterTerminalPatchForSpilling.find(block);
375             if (iter != m_blocksAfterTerminalPatchForSpilling.end()) {
376                 auto&amp; data = iter-&gt;value;
377                 data.jump = m_jit-&gt;jump();
378                 data.continueLabel = m_jit-&gt;label();
379             }
380         }
381 
382         forEachBank([&amp;] (Bank bank) {
383 #if !ASSERT_DISABLED
384             // By default, everything is spilled at block boundaries. We do this after we process each block
385             // so we don&#39;t have to walk all Tmps, since #Tmps &gt;&gt; #Available regs. Instead, we walk the register file at
386             // each block boundary and clear entries in this map.
387             for (Tmp tmp : m_allTmps[bank])
388                 ASSERT(m_map[tmp].reg == Reg());
389 #endif
390 
391             RegisterSet availableRegisters;
392             for (Reg reg : m_registers[bank])
393                 availableRegisters.set(reg);
394             m_availableRegs[bank] = WTFMove(availableRegisters);
395         });
396 
397         IndexMap&lt;Reg, Tmp&gt;&amp; currentAllocation = currentAllocationMap[block];
398         m_currentAllocation = &amp;currentAllocation;
399 
400         for (unsigned i = 0; i &lt; currentAllocation.size(); ++i) {
401             Tmp tmp = currentAllocation[i];
402             if (!tmp)
403                 continue;
404             Reg reg = Reg::fromIndex(i);
405             m_map[tmp].reg = reg;
406             m_availableRegs[tmp.bank()].clear(reg);
407         }
408 
409         bool isReplayingSameInst = false;
410         for (size_t instIndex = 0; instIndex &lt; block-&gt;size(); ++instIndex) {
411             if (instIndex &amp;&amp; !isReplayingSameInst)
412                 startLabel = m_jit-&gt;labelIgnoringWatchpoints();
413 
414             context.indexInBlock = instIndex;
415 
416             Inst&amp; inst = block-&gt;at(instIndex);
417 
418             m_didAlreadyFreeDeadSlots = false;
419 
420             m_namedUsedRegs = RegisterSet();
421             m_namedDefdRegs = RegisterSet();
422 
423             inst.forEachArg([&amp;] (Arg&amp; arg, Arg::Role role, Bank, Width) {
424                 if (!arg.isTmp())
425                     return;
426 
427                 Tmp tmp = arg.tmp();
428                 if (tmp.isReg() &amp;&amp; isDisallowedRegister(tmp.reg()))
429                     return;
430 
431                 if (tmp.isReg()) {
432                     if (Arg::isAnyUse(role))
433                         m_namedUsedRegs.set(tmp.reg());
434                     if (Arg::isAnyDef(role))
435                         m_namedDefdRegs.set(tmp.reg());
436                 }
437 
438                 // We convert any cold uses that are already in the stack to just point to
439                 // the canonical stack location.
440                 if (!Arg::isColdUse(role))
441                     return;
442 
443                 if (!inst.admitsStack(arg))
444                     return;
445 
446                 auto&amp; entry = m_map[tmp];
447                 if (!entry.reg) {
448                     // We&#39;re a cold use, and our current location is already on the stack. Just use that.
449                     arg = Arg::addr(Tmp(GPRInfo::callFrameRegister), entry.spillSlot-&gt;offsetFromFP());
450                 }
451             });
452 
453             RegisterSet clobberedRegisters;
454             {
455                 Inst* nextInst = block-&gt;get(instIndex + 1);
456                 if (inst.kind.opcode == Patch || (nextInst &amp;&amp; nextInst-&gt;kind.opcode == Patch)) {
457                     if (inst.kind.opcode == Patch)
458                         clobberedRegisters.merge(inst.extraClobberedRegs());
459                     if (nextInst &amp;&amp; nextInst-&gt;kind.opcode == Patch)
460                         clobberedRegisters.merge(nextInst-&gt;extraEarlyClobberedRegs());
461 
462                     clobberedRegisters.filter(m_allowedRegisters);
463                     clobberedRegisters.exclude(m_namedDefdRegs);
464 
465                     m_namedDefdRegs.merge(clobberedRegisters);
466                 }
467             }
468 
469             auto allocNamed = [&amp;] (const RegisterSet&amp; named, bool isDef) {
470                 for (Reg reg : named) {
471                     if (Tmp occupyingTmp = currentAllocation[reg]) {
472                         if (occupyingTmp == Tmp(reg))
473                             continue;
474                     }
475 
476                     freeDeadTmpsIfNeeded(); // We don&#39;t want to spill a dead tmp.
477                     alloc(Tmp(reg), reg, isDef);
478                 }
479             };
480 
481             allocNamed(m_namedUsedRegs, false); // Must come before the defd registers since we may use and def the same register.
482             allocNamed(m_namedDefdRegs, true);
483 
484             {
485                 auto tryAllocate = [&amp;] {
486                     Vector&lt;Tmp*, 8&gt; usesToAlloc;
487                     Vector&lt;Tmp*, 8&gt; defsToAlloc;
488 
489                     inst.forEachTmp([&amp;] (Tmp&amp; tmp, Arg::Role role, Bank, Width) {
490                         if (tmp.isReg())
491                             return;
492 
493                         // We treat Use+Def as a use.
494                         if (Arg::isAnyUse(role))
495                             usesToAlloc.append(&amp;tmp);
496                         else if (Arg::isAnyDef(role))
497                             defsToAlloc.append(&amp;tmp);
498                     });
499 
500                     auto tryAllocateTmps = [&amp;] (auto&amp; vector, bool isDef) {
501                         bool success = true;
502                         for (Tmp* tmp : vector)
503                             success &amp;= assignTmp(*tmp, tmp-&gt;bank(), isDef);
504                         return success;
505                     };
506 
507                     // We first handle uses, then defs. We want to be able to tell the register allocator
508                     // which tmps need to be loaded from memory into their assigned register. Those such
509                     // tmps are uses. Defs don&#39;t need to be reloaded since we&#39;re defining them. However,
510                     // some tmps may both be used and defd. So we handle uses first since forEachTmp could
511                     // walk uses/defs in any order.
512                     bool success = true;
513                     success &amp;= tryAllocateTmps(usesToAlloc, false);
514                     success &amp;= tryAllocateTmps(defsToAlloc, true);
515                     return success;
516                 };
517 
518                 // We first allocate trying to give any Tmp a register. If that makes us exhaust the
519                 // available registers, we convert anything that accepts stack to be a stack addr
520                 // instead. This can happen for programs Insts that take in many args, but most
521                 // args can just be stack values.
522                 bool success = tryAllocate();
523                 if (!success) {
524                     RELEASE_ASSERT(!isReplayingSameInst); // We should only need to do the below at most once per inst.
525 
526                     // We need to capture the register state before we start spilling things
527                     // since we may have multiple arguments that are the same register.
528                     IndexMap&lt;Reg, Tmp&gt; allocationSnapshot = currentAllocation;
529 
530                     // We rewind this Inst to be in its previous state, however, if any arg admits stack,
531                     // we move to providing that arg in stack form. This will allow us to fully allocate
532                     // this inst when we rewind.
533                     inst.forEachArg([&amp;] (Arg&amp; arg, Arg::Role, Bank, Width) {
534                         if (!arg.isTmp())
535                             return;
536 
537                         Tmp tmp = arg.tmp();
538                         if (tmp.isReg() &amp;&amp; isDisallowedRegister(tmp.reg()))
539                             return;
540 
541                         if (tmp.isReg()) {
542                             Tmp originalTmp = allocationSnapshot[tmp.reg()];
543                             if (originalTmp.isReg()) {
544                                 ASSERT(tmp.reg() == originalTmp.reg());
545                                 // This means this Inst referred to this reg directly. We leave these as is.
546                                 return;
547                             }
548                             tmp = originalTmp;
549                         }
550 
551                         if (!inst.admitsStack(arg)) {
552                             arg = tmp;
553                             return;
554                         }
555 
556                         auto&amp; entry = m_map[tmp];
557                         if (Reg reg = entry.reg)
558                             spill(tmp, reg);
559 
560                         arg = Arg::addr(Tmp(GPRInfo::callFrameRegister), entry.spillSlot-&gt;offsetFromFP());
561                     });
562 
563                     --instIndex;
564                     isReplayingSameInst = true;
565                     continue;
566                 }
567 
568                 isReplayingSameInst = false;
569             }
570 
571             if (m_code.needsUsedRegisters() &amp;&amp; inst.kind.opcode == Patch) {
572                 freeDeadTmpsIfNeeded();
573                 RegisterSet registerSet;
574                 for (size_t i = 0; i &lt; currentAllocation.size(); ++i) {
575                     if (currentAllocation[i])
576                         registerSet.set(Reg::fromIndex(i));
577                 }
578                 inst.reportUsedRegisters(registerSet);
579             }
580 
581             if (inst.isTerminal() &amp;&amp; block-&gt;numSuccessors()) {
582                 // By default, we spill everything between block boundaries. However, we have a small
583                 // heuristic to pass along register state. We should eventually make this better.
584                 // What we do now is if we have a successor with a single predecessor (us), and we
585                 // haven&#39;t yet generated code for it, we give it our register state. If all our successors
586                 // can take on our register state, we don&#39;t flush at the end of this block.
587 
588                 bool everySuccessorGetsOurRegisterState = true;
589                 for (unsigned i = 0; i &lt; block-&gt;numSuccessors(); ++i) {
590                     BasicBlock* successor = block-&gt;successorBlock(i);
591                     if (successor-&gt;numPredecessors() == 1 &amp;&amp; !context.blockLabels[successor]-&gt;isSet())
592                         currentAllocationMap[successor] = currentAllocation;
593                     else
594                         everySuccessorGetsOurRegisterState = false;
595                 }
596                 if (!everySuccessorGetsOurRegisterState) {
597                     for (Tmp tmp : liveness.liveAtTail(block)) {
598                         if (tmp.isReg() &amp;&amp; isDisallowedRegister(tmp.reg()))
599                             continue;
600                         if (Reg reg = m_map[tmp].reg)
601                             flush(tmp, reg);
602                     }
603                 }
604             }
605 
606             if (!inst.isTerminal()) {
607                 CCallHelpers::Jump jump = inst.generate(*m_jit, context);
608                 ASSERT_UNUSED(jump, !jump.isSet());
609 
610                 for (Reg reg : clobberedRegisters) {
611                     Tmp tmp(reg);
612                     ASSERT(currentAllocation[reg] == tmp);
613                     m_availableRegs[tmp.bank()].set(reg);
614                     m_currentAllocation-&gt;at(reg) = Tmp();
615                     m_map[tmp].reg = Reg();
616                 }
617             } else {
618                 bool needsToGenerate = true;
619                 if (inst.kind.opcode == Jump &amp;&amp; block-&gt;successorBlock(0) == m_code.findNextBlock(block))
620                     needsToGenerate = false;
621 
622                 if (isReturn(inst.kind.opcode)) {
623                     needsToGenerate = false;
624 
625                     // We currently don&#39;t represent the full epilogue in Air, so we need to
626                     // have this override.
627                     if (m_code.frameSize()) {
628                         m_jit-&gt;emitRestore(m_code.calleeSaveRegisterAtOffsetList());
629                         m_jit-&gt;emitFunctionEpilogue();
630                     } else
631                         m_jit-&gt;emitFunctionEpilogueWithEmptyFrame();
632                     m_jit-&gt;ret();
633                 }
634 
635                 if (needsToGenerate) {
636                     CCallHelpers::Jump jump = inst.generate(*m_jit, context);
637 
638                     // The jump won&#39;t be set for patchpoints. It won&#39;t be set for Oops because then it won&#39;t have
639                     // any successors.
640                     if (jump.isSet()) {
641                         switch (block-&gt;numSuccessors()) {
642                         case 1:
643                             link(jump, block-&gt;successorBlock(0));
644                             break;
645                         case 2:
646                             link(jump, block-&gt;successorBlock(0));
647                             if (block-&gt;successorBlock(1) != m_code.findNextBlock(block))
648                                 link(m_jit-&gt;jump(), block-&gt;successorBlock(1));
649                             break;
650                         default:
651                             RELEASE_ASSERT_NOT_REACHED();
652                             break;
653                         }
654                     }
655                 }
656             }
657 
658             auto endLabel = m_jit-&gt;labelIgnoringWatchpoints();
659             if (disassembler)
660                 disassembler-&gt;addInst(&amp;inst, startLabel, endLabel);
661 
662             ++m_globalInstIndex;
663         }
664 
665         // Registers usually get spilled at block boundaries. We do it this way since we don&#39;t
666         // want to iterate the entire TmpMap, since usually #Tmps &gt;&gt; #Regs. We may not actually spill
667         // all registers, but at the top of this loop we handle that case by pre-populating register
668         // state. Here, we just clear this map. After this loop, this map should contain only
669         // null entries.
670         for (size_t i = 0; i &lt; currentAllocation.size(); ++i) {
671             if (Tmp tmp = currentAllocation[i])
672                 m_map[tmp].reg = Reg();
673         }
674     }
675 
676     for (auto&amp; entry : m_blocksAfterTerminalPatchForSpilling) {
677         entry.value.jump.linkTo(m_jit-&gt;label(), m_jit);
678         const HashMap&lt;Tmp, Arg*&gt;&amp; spills = entry.value.defdTmps;
679         for (auto&amp; entry : spills) {
680             Arg* arg = entry.value;
681             if (!arg-&gt;isTmp())
682                 continue;
683             Tmp originalTmp = entry.key;
684             Tmp currentTmp = arg-&gt;tmp();
685             ASSERT_WITH_MESSAGE(currentTmp.isReg(), &quot;We already did register allocation so we should have assigned this Tmp to a register.&quot;);
686             flush(originalTmp, currentTmp.reg());
687         }
688         m_jit-&gt;jump().linkTo(entry.value.continueLabel, m_jit);
689     }
690 
691     context.currentBlock = nullptr;
692     context.indexInBlock = UINT_MAX;
693 
694     Vector&lt;CCallHelpers::Label&gt; entrypointLabels(m_code.numEntrypoints());
695     for (unsigned i = m_code.numEntrypoints(); i--;)
696         entrypointLabels[i] = *context.blockLabels[m_code.entrypoint(i).block()];
697     m_code.setEntrypointLabels(WTFMove(entrypointLabels));
698 
699     if (disassembler)
700         disassembler-&gt;startLatePath(*m_jit);
701 
702     // FIXME: Make late paths have Origins: https://bugs.webkit.org/show_bug.cgi?id=153689
703     for (auto&amp; latePath : context.latePaths)
704         latePath-&gt;run(*m_jit, context);
705 
706     if (disassembler)
707         disassembler-&gt;endLatePath(*m_jit);
708 }
709 
710 } } } // namespace JSC::B3::Air
711 
712 #endif // ENABLE(B3_JIT)
    </pre>
  </body>
</html>