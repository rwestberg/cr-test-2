<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/CodeBlock.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.
   3  * Copyright (C) 2008 Cameron Zwarich &lt;cwzwarich@uwaterloo.ca&gt;
   4  *
   5  * Redistribution and use in source and binary forms, with or without
   6  * modification, are permitted provided that the following conditions
   7  * are met:
   8  *
   9  * 1.  Redistributions of source code must retain the above copyright
  10  *     notice, this list of conditions and the following disclaimer.
  11  * 2.  Redistributions in binary form must reproduce the above copyright
  12  *     notice, this list of conditions and the following disclaimer in the
  13  *     documentation and/or other materials provided with the distribution.
  14  * 3.  Neither the name of Apple Inc. (&quot;Apple&quot;) nor the names of
  15  *     its contributors may be used to endorse or promote products derived
  16  *     from this software without specific prior written permission.
  17  *
  18  * THIS SOFTWARE IS PROVIDED BY APPLE AND ITS CONTRIBUTORS &quot;AS IS&quot; AND ANY
  19  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
  20  * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  21  * DISCLAIMED. IN NO EVENT SHALL APPLE OR ITS CONTRIBUTORS BE LIABLE FOR ANY
  22  * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
  23  * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
  24  * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
  25  * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  26  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
  27  * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  28  */
  29 
  30 #pragma once
  31 
  32 #include &quot;ArrayProfile.h&quot;
  33 #include &quot;ByValInfo.h&quot;
  34 #include &quot;BytecodeConventions.h&quot;
  35 #include &quot;CallLinkInfo.h&quot;
  36 #include &quot;CodeBlockHash.h&quot;
  37 #include &quot;CodeOrigin.h&quot;
  38 #include &quot;CodeType.h&quot;
  39 #include &quot;CompilationResult.h&quot;
  40 #include &quot;ConcurrentJSLock.h&quot;
  41 #include &quot;DFGCommon.h&quot;
  42 #include &quot;DirectEvalCodeCache.h&quot;
  43 #include &quot;EvalExecutable.h&quot;
  44 #include &quot;ExecutionCounter.h&quot;
  45 #include &quot;ExpressionRangeInfo.h&quot;
  46 #include &quot;FunctionExecutable.h&quot;
  47 #include &quot;HandlerInfo.h&quot;
  48 #include &quot;ICStatusMap.h&quot;
  49 #include &quot;Instruction.h&quot;
  50 #include &quot;InstructionStream.h&quot;
  51 #include &quot;JITCode.h&quot;
  52 #include &quot;JITCodeMap.h&quot;
  53 #include &quot;JITMathICForwards.h&quot;
  54 #include &quot;JSCast.h&quot;
  55 #include &quot;JSGlobalObject.h&quot;
  56 #include &quot;JumpTable.h&quot;
  57 #include &quot;LLIntCallLinkInfo.h&quot;
  58 #include &quot;LazyOperandValueProfile.h&quot;
  59 #include &quot;MetadataTable.h&quot;
  60 #include &quot;ModuleProgramExecutable.h&quot;
  61 #include &quot;ObjectAllocationProfile.h&quot;
  62 #include &quot;Options.h&quot;
  63 #include &quot;Printer.h&quot;
  64 #include &quot;ProfilerJettisonReason.h&quot;
  65 #include &quot;ProgramExecutable.h&quot;
  66 #include &quot;PutPropertySlot.h&quot;
  67 #include &quot;ValueProfile.h&quot;
  68 #include &quot;VirtualRegister.h&quot;
  69 #include &quot;Watchpoint.h&quot;
  70 #include &lt;wtf/Bag.h&gt;
  71 #include &lt;wtf/FastMalloc.h&gt;
  72 #include &lt;wtf/RefCountedArray.h&gt;
  73 #include &lt;wtf/RefPtr.h&gt;
  74 #include &lt;wtf/SegmentedVector.h&gt;
  75 #include &lt;wtf/Vector.h&gt;
  76 #include &lt;wtf/text/WTFString.h&gt;
  77 
  78 namespace JSC {
  79 
  80 #if ENABLE(DFG_JIT)
  81 namespace DFG {
  82 struct OSRExitState;
  83 } // namespace DFG
  84 #endif
  85 
  86 class BytecodeLivenessAnalysis;
  87 class CodeBlockSet;
  88 class ExecutableToCodeBlockEdge;
  89 class JSModuleEnvironment;
  90 class LLIntOffsetsExtractor;
  91 class LLIntPrototypeLoadAdaptiveStructureWatchpoint;
  92 class MetadataTable;
  93 class PCToCodeOriginMap;
  94 class RegisterAtOffsetList;
  95 class StructureStubInfo;
  96 
  97 enum class AccessType : int8_t;
  98 
  99 struct ArithProfile;
 100 struct OpCatch;
 101 
 102 enum ReoptimizationMode { DontCountReoptimization, CountReoptimization };
 103 
 104 class CodeBlock : public JSCell {
 105     typedef JSCell Base;
 106     friend class BytecodeLivenessAnalysis;
 107     friend class JIT;
 108     friend class LLIntOffsetsExtractor;
 109 
 110 public:
 111 
 112     enum CopyParsedBlockTag { CopyParsedBlock };
 113 
 114     static const unsigned StructureFlags = Base::StructureFlags | StructureIsImmortal;
 115     static const bool needsDestruction = true;
 116 
 117     template&lt;typename, SubspaceAccess&gt;
 118     static void subspaceFor(VM&amp;) { }
 119 
 120     DECLARE_INFO;
 121 
 122 protected:
 123     CodeBlock(VM*, Structure*, CopyParsedBlockTag, CodeBlock&amp; other);
 124     CodeBlock(VM*, Structure*, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock*, JSScope*);
 125 
 126     void finishCreation(VM&amp;, CopyParsedBlockTag, CodeBlock&amp; other);
 127     bool finishCreation(VM&amp;, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock*, JSScope*);
 128 
 129     void finishCreationCommon(VM&amp;);
 130 
 131     WriteBarrier&lt;JSGlobalObject&gt; m_globalObject;
 132 
 133 public:
 134     JS_EXPORT_PRIVATE ~CodeBlock();
 135 
 136     UnlinkedCodeBlock* unlinkedCodeBlock() const { return m_unlinkedCode.get(); }
 137 
 138     CString inferredName() const;
 139     CodeBlockHash hash() const;
 140     bool hasHash() const;
 141     bool isSafeToComputeHash() const;
 142     CString hashAsStringIfPossible() const;
 143     CString sourceCodeForTools() const; // Not quite the actual source we parsed; this will do things like prefix the source for a function with a reified signature.
 144     CString sourceCodeOnOneLine() const; // As sourceCodeForTools(), but replaces all whitespace runs with a single space.
 145     void dumpAssumingJITType(PrintStream&amp;, JITCode::JITType) const;
 146     JS_EXPORT_PRIVATE void dump(PrintStream&amp;) const;
 147 
 148     int numParameters() const { return m_numParameters; }
 149     void setNumParameters(int newValue);
 150 
 151     int numberOfArgumentsToSkip() const { return m_numberOfArgumentsToSkip; }
 152 
 153     int numCalleeLocals() const { return m_numCalleeLocals; }
 154 
 155     int numVars() const { return m_numVars; }
 156 
 157     int* addressOfNumParameters() { return &amp;m_numParameters; }
 158     static ptrdiff_t offsetOfNumParameters() { return OBJECT_OFFSETOF(CodeBlock, m_numParameters); }
 159 
 160     CodeBlock* alternative() const { return static_cast&lt;CodeBlock*&gt;(m_alternative.get()); }
 161     void setAlternative(VM&amp;, CodeBlock*);
 162 
 163     template &lt;typename Functor&gt; void forEachRelatedCodeBlock(Functor&amp;&amp; functor)
 164     {
 165         Functor f(std::forward&lt;Functor&gt;(functor));
 166         Vector&lt;CodeBlock*, 4&gt; codeBlocks;
 167         codeBlocks.append(this);
 168 
 169         while (!codeBlocks.isEmpty()) {
 170             CodeBlock* currentCodeBlock = codeBlocks.takeLast();
 171             f(currentCodeBlock);
 172 
 173             if (CodeBlock* alternative = currentCodeBlock-&gt;alternative())
 174                 codeBlocks.append(alternative);
 175             if (CodeBlock* osrEntryBlock = currentCodeBlock-&gt;specialOSREntryBlockOrNull())
 176                 codeBlocks.append(osrEntryBlock);
 177         }
 178     }
 179 
 180     CodeSpecializationKind specializationKind() const
 181     {
 182         return specializationFromIsConstruct(isConstructor());
 183     }
 184 
 185     CodeBlock* alternativeForJettison();
 186     JS_EXPORT_PRIVATE CodeBlock* baselineAlternative();
 187 
 188     // FIXME: Get rid of this.
 189     // https://bugs.webkit.org/show_bug.cgi?id=123677
 190     CodeBlock* baselineVersion();
 191 
 192     static size_t estimatedSize(JSCell*, VM&amp;);
 193     static void visitChildren(JSCell*, SlotVisitor&amp;);
 194     static void destroy(JSCell*);
 195     void visitChildren(SlotVisitor&amp;);
 196     void finalizeUnconditionally(VM&amp;);
 197 
 198     void notifyLexicalBindingUpdate();
 199 
 200     void dumpSource();
 201     void dumpSource(PrintStream&amp;);
 202 
 203     void dumpBytecode();
 204     void dumpBytecode(PrintStream&amp;);
 205     void dumpBytecode(PrintStream&amp; out, const InstructionStream::Ref&amp; it, const ICStatusMap&amp; = ICStatusMap());
 206     void dumpBytecode(PrintStream&amp; out, unsigned bytecodeOffset, const ICStatusMap&amp; = ICStatusMap());
 207 
 208     void dumpExceptionHandlers(PrintStream&amp;);
 209     void printStructures(PrintStream&amp;, const Instruction*);
 210     void printStructure(PrintStream&amp;, const char* name, const Instruction*, int operand);
 211 
 212     void dumpMathICStats();
 213 
 214     bool isStrictMode() const { return m_unlinkedCode-&gt;isStrictMode(); }
 215     bool isConstructor() const { return m_unlinkedCode-&gt;isConstructor(); }
 216     ECMAMode ecmaMode() const { return isStrictMode() ? StrictMode : NotStrictMode; }
 217     CodeType codeType() const { return m_unlinkedCode-&gt;codeType(); }
 218 
 219     JSParserScriptMode scriptMode() const { return m_unlinkedCode-&gt;scriptMode(); }
 220 
 221     bool hasInstalledVMTrapBreakpoints() const;
 222     bool installVMTrapBreakpoints();
 223 
 224     inline bool isKnownNotImmediate(int index)
 225     {
 226         if (index == thisRegister().offset() &amp;&amp; !isStrictMode())
 227             return true;
 228 
 229         if (isConstantRegisterIndex(index))
 230             return getConstant(index).isCell();
 231 
 232         return false;
 233     }
 234 
 235     ALWAYS_INLINE bool isTemporaryRegisterIndex(int index)
 236     {
 237         return index &gt;= m_numVars;
 238     }
 239 
 240     HandlerInfo* handlerForBytecodeOffset(unsigned bytecodeOffset, RequiredHandler = RequiredHandler::AnyHandler);
 241     HandlerInfo* handlerForIndex(unsigned, RequiredHandler = RequiredHandler::AnyHandler);
 242     void removeExceptionHandlerForCallSite(DisposableCallSiteIndex);
 243     unsigned lineNumberForBytecodeOffset(unsigned bytecodeOffset);
 244     unsigned columnNumberForBytecodeOffset(unsigned bytecodeOffset);
 245     void expressionRangeForBytecodeOffset(unsigned bytecodeOffset, int&amp; divot,
 246         int&amp; startOffset, int&amp; endOffset, unsigned&amp; line, unsigned&amp; column) const;
 247 
 248     Optional&lt;unsigned&gt; bytecodeOffsetFromCallSiteIndex(CallSiteIndex);
 249 
 250     void getICStatusMap(const ConcurrentJSLocker&amp;, ICStatusMap&amp; result);
 251     void getICStatusMap(ICStatusMap&amp; result);
 252 
 253 #if ENABLE(JIT)
 254     struct JITData {
 255         WTF_MAKE_STRUCT_FAST_ALLOCATED;
 256 
 257         Bag&lt;StructureStubInfo&gt; m_stubInfos;
 258         Bag&lt;JITAddIC&gt; m_addICs;
 259         Bag&lt;JITMulIC&gt; m_mulICs;
 260         Bag&lt;JITNegIC&gt; m_negICs;
 261         Bag&lt;JITSubIC&gt; m_subICs;
 262         Bag&lt;ByValInfo&gt; m_byValInfos;
 263         Bag&lt;CallLinkInfo&gt; m_callLinkInfos;
 264         SentinelLinkedList&lt;CallLinkInfo, BasicRawSentinelNode&lt;CallLinkInfo&gt;&gt; m_incomingCalls;
 265         SentinelLinkedList&lt;PolymorphicCallNode, BasicRawSentinelNode&lt;PolymorphicCallNode&gt;&gt; m_incomingPolymorphicCalls;
 266         SegmentedVector&lt;RareCaseProfile, 8&gt; m_rareCaseProfiles;
 267         std::unique_ptr&lt;PCToCodeOriginMap&gt; m_pcToCodeOriginMap;
 268         std::unique_ptr&lt;RegisterAtOffsetList&gt; m_calleeSaveRegisters;
 269         JITCodeMap m_jitCodeMap;
 270     };
 271 
 272     JITData&amp; ensureJITData(const ConcurrentJSLocker&amp; locker)
 273     {
 274         if (LIKELY(m_jitData))
 275             return *m_jitData;
 276         return ensureJITDataSlow(locker);
 277     }
 278     JITData&amp; ensureJITDataSlow(const ConcurrentJSLocker&amp;);
 279 
 280     JITAddIC* addJITAddIC(ArithProfile*, const Instruction*);
 281     JITMulIC* addJITMulIC(ArithProfile*, const Instruction*);
 282     JITNegIC* addJITNegIC(ArithProfile*, const Instruction*);
 283     JITSubIC* addJITSubIC(ArithProfile*, const Instruction*);
 284 
 285     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITAddGenerator&gt;::value&gt;::type&gt;
 286     JITAddIC* addMathIC(ArithProfile* profile, const Instruction* instruction) { return addJITAddIC(profile, instruction); }
 287 
 288     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITMulGenerator&gt;::value&gt;::type&gt;
 289     JITMulIC* addMathIC(ArithProfile* profile, const Instruction* instruction) { return addJITMulIC(profile, instruction); }
 290 
 291     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITNegGenerator&gt;::value&gt;::type&gt;
 292     JITNegIC* addMathIC(ArithProfile* profile, const Instruction* instruction) { return addJITNegIC(profile, instruction); }
 293 
 294     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITSubGenerator&gt;::value&gt;::type&gt;
 295     JITSubIC* addMathIC(ArithProfile* profile, const Instruction* instruction) { return addJITSubIC(profile, instruction); }
 296 
 297     StructureStubInfo* addStubInfo(AccessType);
 298 
 299     // O(n) operation. Use getStubInfoMap() unless you really only intend to get one
 300     // stub info.
 301     StructureStubInfo* findStubInfo(CodeOrigin);
 302 
 303     ByValInfo* addByValInfo();
 304 
 305     CallLinkInfo* addCallLinkInfo();
 306 
 307     // This is a slow function call used primarily for compiling OSR exits in the case
 308     // that there had been inlining. Chances are if you want to use this, you&#39;re really
 309     // looking for a CallLinkInfoMap to amortize the cost of calling this.
 310     CallLinkInfo* getCallLinkInfoForBytecodeIndex(unsigned bytecodeIndex);
 311 
 312     void setJITCodeMap(JITCodeMap&amp;&amp; jitCodeMap)
 313     {
 314         ConcurrentJSLocker locker(m_lock);
 315         ensureJITData(locker).m_jitCodeMap = WTFMove(jitCodeMap);
 316     }
 317     const JITCodeMap&amp; jitCodeMap()
 318     {
 319         ConcurrentJSLocker locker(m_lock);
 320         return ensureJITData(locker).m_jitCodeMap;
 321     }
 322 
 323     void setPCToCodeOriginMap(std::unique_ptr&lt;PCToCodeOriginMap&gt;&amp;&amp;);
 324     Optional&lt;CodeOrigin&gt; findPC(void* pc);
 325 
 326     void setCalleeSaveRegisters(RegisterSet);
 327     void setCalleeSaveRegisters(std::unique_ptr&lt;RegisterAtOffsetList&gt;);
 328 
 329     RareCaseProfile* addRareCaseProfile(int bytecodeOffset);
 330     RareCaseProfile* rareCaseProfileForBytecodeOffset(const ConcurrentJSLocker&amp;, int bytecodeOffset);
 331     unsigned rareCaseProfileCountForBytecodeOffset(const ConcurrentJSLocker&amp;, int bytecodeOffset);
 332 
 333     bool likelyToTakeSlowCase(int bytecodeOffset)
 334     {
 335         if (!hasBaselineJITProfiling())
 336             return false;
 337         ConcurrentJSLocker locker(m_lock);
 338         unsigned value = rareCaseProfileCountForBytecodeOffset(locker, bytecodeOffset);
 339         return value &gt;= Options::likelyToTakeSlowCaseMinimumCount();
 340     }
 341 
 342     bool couldTakeSlowCase(int bytecodeOffset)
 343     {
 344         if (!hasBaselineJITProfiling())
 345             return false;
 346         ConcurrentJSLocker locker(m_lock);
 347         unsigned value = rareCaseProfileCountForBytecodeOffset(locker, bytecodeOffset);
 348         return value &gt;= Options::couldTakeSlowCaseMinimumCount();
 349     }
 350 
 351     // We call this when we want to reattempt compiling something with the baseline JIT. Ideally
 352     // the baseline JIT would not add data to CodeBlock, but instead it would put its data into
 353     // a newly created JITCode, which could be thrown away if we bail on JIT compilation. Then we
 354     // would be able to get rid of this silly function.
 355     // FIXME: https://bugs.webkit.org/show_bug.cgi?id=159061
 356     void resetJITData();
 357 #endif // ENABLE(JIT)
 358 
 359     void unlinkIncomingCalls();
 360 
 361 #if ENABLE(JIT)
 362     void linkIncomingCall(ExecState* callerFrame, CallLinkInfo*);
 363     void linkIncomingPolymorphicCall(ExecState* callerFrame, PolymorphicCallNode*);
 364 #endif // ENABLE(JIT)
 365 
 366     void linkIncomingCall(ExecState* callerFrame, LLIntCallLinkInfo*);
 367 
 368     const Instruction* outOfLineJumpTarget(const Instruction* pc);
 369     int outOfLineJumpOffset(const Instruction* pc);
 370     int outOfLineJumpOffset(const InstructionStream::Ref&amp; instruction)
 371     {
 372         return outOfLineJumpOffset(instruction.ptr());
 373     }
 374 
 375     inline unsigned bytecodeOffset(const Instruction* returnAddress)
 376     {
 377         const auto* instructionsBegin = instructions().at(0).ptr();
 378         const auto* instructionsEnd = reinterpret_cast&lt;const Instruction*&gt;(reinterpret_cast&lt;uintptr_t&gt;(instructionsBegin) + instructions().size());
 379         RELEASE_ASSERT(returnAddress &gt;= instructionsBegin &amp;&amp; returnAddress &lt; instructionsEnd);
 380         return returnAddress - instructionsBegin;
 381     }
 382 
 383     const InstructionStream&amp; instructions() const { return m_unlinkedCode-&gt;instructions(); }
 384 
 385     size_t predictedMachineCodeSize();
 386 
 387     unsigned instructionCount() const { return m_instructionCount; }
 388 
 389     // Exactly equivalent to codeBlock-&gt;ownerExecutable()-&gt;newReplacementCodeBlockFor(codeBlock-&gt;specializationKind())
 390     CodeBlock* newReplacement();
 391 
 392     void setJITCode(Ref&lt;JITCode&gt;&amp;&amp; code)
 393     {
 394         ASSERT(heap()-&gt;isDeferred());
 395         heap()-&gt;reportExtraMemoryAllocated(code-&gt;size());
 396         ConcurrentJSLocker locker(m_lock);
 397         WTF::storeStoreFence(); // This is probably not needed because the lock will also do something similar, but it&#39;s good to be paranoid.
 398         m_jitCode = WTFMove(code);
 399     }
 400     RefPtr&lt;JITCode&gt; jitCode() { return m_jitCode; }
 401     static ptrdiff_t jitCodeOffset() { return OBJECT_OFFSETOF(CodeBlock, m_jitCode); }
 402     JITCode::JITType jitType() const
 403     {
 404         JITCode* jitCode = m_jitCode.get();
 405         WTF::loadLoadFence();
 406         JITCode::JITType result = JITCode::jitTypeFor(jitCode);
 407         WTF::loadLoadFence(); // This probably isn&#39;t needed. Oh well, paranoia is good.
 408         return result;
 409     }
 410 
 411     bool hasBaselineJITProfiling() const
 412     {
 413         return jitType() == JITCode::BaselineJIT;
 414     }
 415 
 416 #if ENABLE(JIT)
 417     CodeBlock* replacement();
 418 
 419     DFG::CapabilityLevel computeCapabilityLevel();
 420     DFG::CapabilityLevel capabilityLevel();
 421     DFG::CapabilityLevel capabilityLevelState() { return static_cast&lt;DFG::CapabilityLevel&gt;(m_capabilityLevelState); }
 422 
 423     bool hasOptimizedReplacement(JITCode::JITType typeToReplace);
 424     bool hasOptimizedReplacement(); // the typeToReplace is my JITType
 425 #endif
 426 
 427     void jettison(Profiler::JettisonReason, ReoptimizationMode = DontCountReoptimization, const FireDetail* = nullptr);
 428 
 429     ScriptExecutable* ownerExecutable() const { return m_ownerExecutable.get(); }
 430 
 431     ExecutableToCodeBlockEdge* ownerEdge() const { return m_ownerEdge.get(); }
 432 
 433     VM* vm() const { return m_vm; }
 434 
 435     VirtualRegister thisRegister() const { return m_unlinkedCode-&gt;thisRegister(); }
 436 
 437     bool usesEval() const { return m_unlinkedCode-&gt;usesEval(); }
 438 
 439     void setScopeRegister(VirtualRegister scopeRegister)
 440     {
 441         ASSERT(scopeRegister.isLocal() || !scopeRegister.isValid());
 442         m_scopeRegister = scopeRegister;
 443     }
 444 
 445     VirtualRegister scopeRegister() const
 446     {
 447         return m_scopeRegister;
 448     }
 449 
 450     PutPropertySlot::Context putByIdContext() const
 451     {
 452         if (codeType() == EvalCode)
 453             return PutPropertySlot::PutByIdEval;
 454         return PutPropertySlot::PutById;
 455     }
 456 
 457     const SourceCode&amp; source() const { return m_ownerExecutable-&gt;source(); }
 458     unsigned sourceOffset() const { return m_ownerExecutable-&gt;source().startOffset(); }
 459     unsigned firstLineColumnOffset() const { return m_ownerExecutable-&gt;startColumn(); }
 460 
 461     size_t numberOfJumpTargets() const { return m_unlinkedCode-&gt;numberOfJumpTargets(); }
 462     unsigned jumpTarget(int index) const { return m_unlinkedCode-&gt;jumpTarget(index); }
 463 
 464     String nameForRegister(VirtualRegister);
 465 
 466     unsigned numberOfArgumentValueProfiles()
 467     {
 468         ASSERT(m_numParameters &gt;= 0);
 469         ASSERT(m_argumentValueProfiles.size() == static_cast&lt;unsigned&gt;(m_numParameters) || !vm()-&gt;canUseJIT());
 470         return m_argumentValueProfiles.size();
 471     }
 472 
 473     ValueProfile&amp; valueProfileForArgument(unsigned argumentIndex)
 474     {
 475         ASSERT(vm()-&gt;canUseJIT()); // This is only called from the various JIT compilers or places that first check numberOfArgumentValueProfiles before calling this.
 476         ValueProfile&amp; result = m_argumentValueProfiles[argumentIndex];
 477         ASSERT(result.m_bytecodeOffset == -1);
 478         return result;
 479     }
 480 
 481     ValueProfile&amp; valueProfileForBytecodeOffset(int bytecodeOffset);
 482     SpeculatedType valueProfilePredictionForBytecodeOffset(const ConcurrentJSLocker&amp;, int bytecodeOffset);
 483 
 484     template&lt;typename Functor&gt; void forEachValueProfile(const Functor&amp;);
 485     template&lt;typename Functor&gt; void forEachArrayProfile(const Functor&amp;);
 486     template&lt;typename Functor&gt; void forEachArrayAllocationProfile(const Functor&amp;);
 487     template&lt;typename Functor&gt; void forEachObjectAllocationProfile(const Functor&amp;);
 488     template&lt;typename Functor&gt; void forEachLLIntCallLinkInfo(const Functor&amp;);
 489 
 490     ArithProfile* arithProfileForBytecodeOffset(InstructionStream::Offset bytecodeOffset);
 491     ArithProfile* arithProfileForPC(const Instruction*);
 492 
 493     bool couldTakeSpecialFastCase(InstructionStream::Offset bytecodeOffset);
 494 
 495     ArrayProfile* getArrayProfile(const ConcurrentJSLocker&amp;, unsigned bytecodeOffset);
 496     ArrayProfile* getArrayProfile(unsigned bytecodeOffset);
 497 
 498     // Exception handling support
 499 
 500     size_t numberOfExceptionHandlers() const { return m_rareData ? m_rareData-&gt;m_exceptionHandlers.size() : 0; }
 501     HandlerInfo&amp; exceptionHandler(int index) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_exceptionHandlers[index]; }
 502 
 503     bool hasExpressionInfo() { return m_unlinkedCode-&gt;hasExpressionInfo(); }
 504 
 505 #if ENABLE(DFG_JIT)
 506     Vector&lt;CodeOrigin, 0, UnsafeVectorOverflow&gt;&amp; codeOrigins();
 507 
 508     // Having code origins implies that there has been some inlining.
 509     bool hasCodeOrigins()
 510     {
 511         return JITCode::isOptimizingJIT(jitType());
 512     }
 513 
 514     bool canGetCodeOrigin(CallSiteIndex index)
 515     {
 516         if (!hasCodeOrigins())
 517             return false;
 518         return index.bits() &lt; codeOrigins().size();
 519     }
 520 
 521     CodeOrigin codeOrigin(CallSiteIndex index)
 522     {
 523         return codeOrigins()[index.bits()];
 524     }
 525 
 526     CompressedLazyOperandValueProfileHolder&amp; lazyOperandValueProfiles(const ConcurrentJSLocker&amp;)
 527     {
 528         return m_lazyOperandValueProfiles;
 529     }
 530 #endif // ENABLE(DFG_JIT)
 531 
 532     // Constant Pool
 533 #if ENABLE(DFG_JIT)
 534     size_t numberOfIdentifiers() const { return m_unlinkedCode-&gt;numberOfIdentifiers() + numberOfDFGIdentifiers(); }
 535     size_t numberOfDFGIdentifiers() const;
 536     const Identifier&amp; identifier(int index) const;
 537 #else
 538     size_t numberOfIdentifiers() const { return m_unlinkedCode-&gt;numberOfIdentifiers(); }
 539     const Identifier&amp; identifier(int index) const { return m_unlinkedCode-&gt;identifier(index); }
 540 #endif
 541 
 542     Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants() { return m_constantRegisters; }
 543     Vector&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation() { return m_constantsSourceCodeRepresentation; }
 544     unsigned addConstant(JSValue v)
 545     {
 546         unsigned result = m_constantRegisters.size();
 547         m_constantRegisters.append(WriteBarrier&lt;Unknown&gt;());
 548         m_constantRegisters.last().set(*m_vm, this, v);
 549         m_constantsSourceCodeRepresentation.append(SourceCodeRepresentation::Other);
 550         return result;
 551     }
 552 
 553     unsigned addConstantLazily()
 554     {
 555         unsigned result = m_constantRegisters.size();
 556         m_constantRegisters.append(WriteBarrier&lt;Unknown&gt;());
 557         m_constantsSourceCodeRepresentation.append(SourceCodeRepresentation::Other);
 558         return result;
 559     }
 560 
 561     const Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constantRegisters() { return m_constantRegisters; }
 562     WriteBarrier&lt;Unknown&gt;&amp; constantRegister(int index) { return m_constantRegisters[index - FirstConstantRegisterIndex]; }
 563     static ALWAYS_INLINE bool isConstantRegisterIndex(int index) { return index &gt;= FirstConstantRegisterIndex; }
 564     ALWAYS_INLINE JSValue getConstant(int index) const { return m_constantRegisters[index - FirstConstantRegisterIndex].get(); }
 565     ALWAYS_INLINE SourceCodeRepresentation constantSourceCodeRepresentation(int index) const { return m_constantsSourceCodeRepresentation[index - FirstConstantRegisterIndex]; }
 566 
 567     FunctionExecutable* functionDecl(int index) { return m_functionDecls[index].get(); }
 568     int numberOfFunctionDecls() { return m_functionDecls.size(); }
 569     FunctionExecutable* functionExpr(int index) { return m_functionExprs[index].get(); }
 570 
 571     const BitVector&amp; bitVector(size_t i) { return m_unlinkedCode-&gt;bitVector(i); }
 572 
 573     Heap* heap() const { return &amp;m_vm-&gt;heap; }
 574     JSGlobalObject* globalObject() { return m_globalObject.get(); }
 575 
 576     JSGlobalObject* globalObjectFor(CodeOrigin);
 577 
 578     BytecodeLivenessAnalysis&amp; livenessAnalysis()
 579     {
 580         return m_unlinkedCode-&gt;livenessAnalysis(this);
 581     }
 582 
 583     void validate();
 584 
 585     // Jump Tables
 586 
 587     size_t numberOfSwitchJumpTables() const { return m_rareData ? m_rareData-&gt;m_switchJumpTables.size() : 0; }
 588     SimpleJumpTable&amp; addSwitchJumpTable() { createRareDataIfNecessary(); m_rareData-&gt;m_switchJumpTables.append(SimpleJumpTable()); return m_rareData-&gt;m_switchJumpTables.last(); }
 589     SimpleJumpTable&amp; switchJumpTable(int tableIndex) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_switchJumpTables[tableIndex]; }
 590     void clearSwitchJumpTables()
 591     {
 592         if (!m_rareData)
 593             return;
 594         m_rareData-&gt;m_switchJumpTables.clear();
 595     }
 596 
 597     size_t numberOfStringSwitchJumpTables() const { return m_rareData ? m_rareData-&gt;m_stringSwitchJumpTables.size() : 0; }
 598     StringJumpTable&amp; addStringSwitchJumpTable() { createRareDataIfNecessary(); m_rareData-&gt;m_stringSwitchJumpTables.append(StringJumpTable()); return m_rareData-&gt;m_stringSwitchJumpTables.last(); }
 599     StringJumpTable&amp; stringSwitchJumpTable(int tableIndex) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_stringSwitchJumpTables[tableIndex]; }
 600 
 601     DirectEvalCodeCache&amp; directEvalCodeCache() { createRareDataIfNecessary(); return m_rareData-&gt;m_directEvalCodeCache; }
 602 
 603     enum ShrinkMode {
 604         // Shrink prior to generating machine code that may point directly into vectors.
 605         EarlyShrink,
 606 
 607         // Shrink after generating machine code, and after possibly creating new vectors
 608         // and appending to others. At this time it is not safe to shrink certain vectors
 609         // because we would have generated machine code that references them directly.
 610         LateShrink
 611     };
 612     void shrinkToFit(ShrinkMode);
 613 
 614     // Functions for controlling when JITting kicks in, in a mixed mode
 615     // execution world.
 616 
 617     bool checkIfJITThresholdReached()
 618     {
 619         return m_llintExecuteCounter.checkIfThresholdCrossedAndSet(this);
 620     }
 621 
 622     void dontJITAnytimeSoon()
 623     {
 624         m_llintExecuteCounter.deferIndefinitely();
 625     }
 626 
 627     int32_t thresholdForJIT(int32_t threshold);
 628     void jitAfterWarmUp();
 629     void jitSoon();
 630 
 631     const BaselineExecutionCounter&amp; llintExecuteCounter() const
 632     {
 633         return m_llintExecuteCounter;
 634     }
 635 
 636     typedef HashMap&lt;std::tuple&lt;Structure*, const Instruction*&gt;, Bag&lt;LLIntPrototypeLoadAdaptiveStructureWatchpoint&gt;&gt; StructureWatchpointMap;
 637     StructureWatchpointMap&amp; llintGetByIdWatchpointMap() { return m_llintGetByIdWatchpointMap; }
 638 
 639     // Functions for controlling when tiered compilation kicks in. This
 640     // controls both when the optimizing compiler is invoked and when OSR
 641     // entry happens. Two triggers exist: the loop trigger and the return
 642     // trigger. In either case, when an addition to m_jitExecuteCounter
 643     // causes it to become non-negative, the optimizing compiler is
 644     // invoked. This includes a fast check to see if this CodeBlock has
 645     // already been optimized (i.e. replacement() returns a CodeBlock
 646     // that was optimized with a higher tier JIT than this one). In the
 647     // case of the loop trigger, if the optimized compilation succeeds
 648     // (or has already succeeded in the past) then OSR is attempted to
 649     // redirect program flow into the optimized code.
 650 
 651     // These functions are called from within the optimization triggers,
 652     // and are used as a single point at which we define the heuristics
 653     // for how much warm-up is mandated before the next optimization
 654     // trigger files. All CodeBlocks start out with optimizeAfterWarmUp(),
 655     // as this is called from the CodeBlock constructor.
 656 
 657     // When we observe a lot of speculation failures, we trigger a
 658     // reoptimization. But each time, we increase the optimization trigger
 659     // to avoid thrashing.
 660     JS_EXPORT_PRIVATE unsigned reoptimizationRetryCounter() const;
 661     void countReoptimization();
 662 
 663 #if !ENABLE(C_LOOP)
 664     const RegisterAtOffsetList* calleeSaveRegisters() const;
 665 
 666     static unsigned numberOfLLIntBaselineCalleeSaveRegisters() { return RegisterSet::llintBaselineCalleeSaveRegisters().numberOfSetRegisters(); }
 667     static size_t llintBaselineCalleeSaveSpaceAsVirtualRegisters();
 668     size_t calleeSaveSpaceAsVirtualRegisters();
 669 #else
 670     static unsigned numberOfLLIntBaselineCalleeSaveRegisters() { return 0; }
 671     static size_t llintBaselineCalleeSaveSpaceAsVirtualRegisters() { return 1; };
 672     size_t calleeSaveSpaceAsVirtualRegisters() { return 0; }
 673 #endif
 674 
 675 #if ENABLE(JIT)
 676     unsigned numberOfDFGCompiles();
 677 
 678     int32_t codeTypeThresholdMultiplier() const;
 679 
 680     int32_t adjustedCounterValue(int32_t desiredThreshold);
 681 
 682     int32_t* addressOfJITExecuteCounter()
 683     {
 684         return &amp;m_jitExecuteCounter.m_counter;
 685     }
 686 
 687     static ptrdiff_t offsetOfJITExecuteCounter() { return OBJECT_OFFSETOF(CodeBlock, m_jitExecuteCounter) + OBJECT_OFFSETOF(BaselineExecutionCounter, m_counter); }
 688     static ptrdiff_t offsetOfJITExecutionActiveThreshold() { return OBJECT_OFFSETOF(CodeBlock, m_jitExecuteCounter) + OBJECT_OFFSETOF(BaselineExecutionCounter, m_activeThreshold); }
 689     static ptrdiff_t offsetOfJITExecutionTotalCount() { return OBJECT_OFFSETOF(CodeBlock, m_jitExecuteCounter) + OBJECT_OFFSETOF(BaselineExecutionCounter, m_totalCount); }
 690 
 691     const BaselineExecutionCounter&amp; jitExecuteCounter() const { return m_jitExecuteCounter; }
 692 
 693     unsigned optimizationDelayCounter() const { return m_optimizationDelayCounter; }
 694 
 695     // Check if the optimization threshold has been reached, and if not,
 696     // adjust the heuristics accordingly. Returns true if the threshold has
 697     // been reached.
 698     bool checkIfOptimizationThresholdReached();
 699 
 700     // Call this to force the next optimization trigger to fire. This is
 701     // rarely wise, since optimization triggers are typically more
 702     // expensive than executing baseline code.
 703     void optimizeNextInvocation();
 704 
 705     // Call this to prevent optimization from happening again. Note that
 706     // optimization will still happen after roughly 2^29 invocations,
 707     // so this is really meant to delay that as much as possible. This
 708     // is called if optimization failed, and we expect it to fail in
 709     // the future as well.
 710     void dontOptimizeAnytimeSoon();
 711 
 712     // Call this to reinitialize the counter to its starting state,
 713     // forcing a warm-up to happen before the next optimization trigger
 714     // fires. This is called in the CodeBlock constructor. It also
 715     // makes sense to call this if an OSR exit occurred. Note that
 716     // OSR exit code is code generated, so the value of the execute
 717     // counter that this corresponds to is also available directly.
 718     void optimizeAfterWarmUp();
 719 
 720     // Call this to force an optimization trigger to fire only after
 721     // a lot of warm-up.
 722     void optimizeAfterLongWarmUp();
 723 
 724     // Call this to cause an optimization trigger to fire soon, but
 725     // not necessarily the next one. This makes sense if optimization
 726     // succeeds. Successful optimization means that all calls are
 727     // relinked to the optimized code, so this only affects call
 728     // frames that are still executing this CodeBlock. The value here
 729     // is tuned to strike a balance between the cost of OSR entry
 730     // (which is too high to warrant making every loop back edge to
 731     // trigger OSR immediately) and the cost of executing baseline
 732     // code (which is high enough that we don&#39;t necessarily want to
 733     // have a full warm-up). The intuition for calling this instead of
 734     // optimizeNextInvocation() is for the case of recursive functions
 735     // with loops. Consider that there may be N call frames of some
 736     // recursive function, for a reasonably large value of N. The top
 737     // one triggers optimization, and then returns, and then all of
 738     // the others return. We don&#39;t want optimization to be triggered on
 739     // each return, as that would be superfluous. It only makes sense
 740     // to trigger optimization if one of those functions becomes hot
 741     // in the baseline code.
 742     void optimizeSoon();
 743 
 744     void forceOptimizationSlowPathConcurrently();
 745 
 746     void setOptimizationThresholdBasedOnCompilationResult(CompilationResult);
 747 
 748     uint32_t osrExitCounter() const { return m_osrExitCounter; }
 749 
 750     void countOSRExit() { m_osrExitCounter++; }
 751 
 752     enum class OptimizeAction { None, ReoptimizeNow };
 753 #if ENABLE(DFG_JIT)
 754     OptimizeAction updateOSRExitCounterAndCheckIfNeedToReoptimize(DFG::OSRExitState&amp;);
 755 #endif
 756 
 757     static ptrdiff_t offsetOfOSRExitCounter() { return OBJECT_OFFSETOF(CodeBlock, m_osrExitCounter); }
 758 
 759     uint32_t adjustedExitCountThreshold(uint32_t desiredThreshold);
 760     uint32_t exitCountThresholdForReoptimization();
 761     uint32_t exitCountThresholdForReoptimizationFromLoop();
 762     bool shouldReoptimizeNow();
 763     bool shouldReoptimizeFromLoopNow();
 764 
 765 #else // No JIT
 766     void optimizeAfterWarmUp() { }
 767     unsigned numberOfDFGCompiles() { return 0; }
 768 #endif
 769 
 770     bool shouldOptimizeNow();
 771     void updateAllValueProfilePredictions();
 772     void updateAllArrayPredictions();
 773     void updateAllPredictions();
 774 
 775     unsigned frameRegisterCount();
 776     int stackPointerOffset();
 777 
 778     bool hasOpDebugForLineAndColumn(unsigned line, unsigned column);
 779 
 780     bool hasDebuggerRequests() const { return m_debuggerRequests; }
 781     void* debuggerRequestsAddress() { return &amp;m_debuggerRequests; }
 782 
 783     void addBreakpoint(unsigned numBreakpoints);
 784     void removeBreakpoint(unsigned numBreakpoints)
 785     {
 786         ASSERT(m_numBreakpoints &gt;= numBreakpoints);
 787         m_numBreakpoints -= numBreakpoints;
 788     }
 789 
 790     enum SteppingMode {
 791         SteppingModeDisabled,
 792         SteppingModeEnabled
 793     };
 794     void setSteppingMode(SteppingMode);
 795 
 796     void clearDebuggerRequests()
 797     {
 798         m_steppingMode = SteppingModeDisabled;
 799         m_numBreakpoints = 0;
 800     }
 801 
 802     bool wasCompiledWithDebuggingOpcodes() const { return m_unlinkedCode-&gt;wasCompiledWithDebuggingOpcodes(); }
 803 
 804     // This is intentionally public; it&#39;s the responsibility of anyone doing any
 805     // of the following to hold the lock:
 806     //
 807     // - Modifying any inline cache in this code block.
 808     //
 809     // - Quering any inline cache in this code block, from a thread other than
 810     //   the main thread.
 811     //
 812     // Additionally, it&#39;s only legal to modify the inline cache on the main
 813     // thread. This means that the main thread can query the inline cache without
 814     // locking. This is crucial since executing the inline cache is effectively
 815     // &quot;querying&quot; it.
 816     //
 817     // Another exception to the rules is that the GC can do whatever it wants
 818     // without holding any locks, because the GC is guaranteed to wait until any
 819     // concurrent compilation threads finish what they&#39;re doing.
 820     mutable ConcurrentJSLock m_lock;
 821 
 822     bool m_shouldAlwaysBeInlined; // Not a bitfield because the JIT wants to store to it.
 823 
 824 #if ENABLE(JIT)
 825     unsigned m_capabilityLevelState : 2; // DFG::CapabilityLevel
 826 #endif
 827 
 828     bool m_allTransitionsHaveBeenMarked : 1; // Initialized and used on every GC.
 829 
 830     bool m_didFailJITCompilation : 1;
 831     bool m_didFailFTLCompilation : 1;
 832     bool m_hasBeenCompiledWithFTL : 1;
 833 
 834     // Internal methods for use by validation code. It would be private if it wasn&#39;t
 835     // for the fact that we use it from anonymous namespaces.
 836     void beginValidationDidFail();
 837     NO_RETURN_DUE_TO_CRASH void endValidationDidFail();
 838 
 839     struct RareData {
 840         WTF_MAKE_FAST_ALLOCATED;
 841     public:
 842         Vector&lt;HandlerInfo&gt; m_exceptionHandlers;
 843 
 844         // Jump Tables
 845         Vector&lt;SimpleJumpTable&gt; m_switchJumpTables;
 846         Vector&lt;StringJumpTable&gt; m_stringSwitchJumpTables;
 847 
 848         Vector&lt;std::unique_ptr&lt;ValueProfileAndOperandBuffer&gt;&gt; m_catchProfiles;
 849 
 850         DirectEvalCodeCache m_directEvalCodeCache;
 851     };
 852 
 853     void clearExceptionHandlers()
 854     {
 855         if (m_rareData)
 856             m_rareData-&gt;m_exceptionHandlers.clear();
 857     }
 858 
 859     void appendExceptionHandler(const HandlerInfo&amp; handler)
 860     {
 861         createRareDataIfNecessary(); // We may be handling the exception of an inlined call frame.
 862         m_rareData-&gt;m_exceptionHandlers.append(handler);
 863     }
 864 
 865     DisposableCallSiteIndex newExceptionHandlingCallSiteIndex(CallSiteIndex originalCallSite);
 866 
 867     void ensureCatchLivenessIsComputedForBytecodeOffset(InstructionStream::Offset bytecodeOffset);
 868 
 869     bool hasTailCalls() const { return m_unlinkedCode-&gt;hasTailCalls(); }
 870 
 871     template&lt;typename Metadata&gt;
 872     Metadata&amp; metadata(OpcodeID opcodeID, unsigned metadataID)
 873     {
 874         ASSERT(m_metadata);
 875         return bitwise_cast&lt;Metadata*&gt;(m_metadata-&gt;get(opcodeID))[metadataID];
 876     }
 877 
 878     size_t metadataSizeInBytes()
 879     {
 880         return m_unlinkedCode-&gt;metadataSizeInBytes();
 881     }
 882 
 883 protected:
 884     void finalizeLLIntInlineCaches();
 885 #if ENABLE(JIT)
 886     void finalizeBaselineJITInlineCaches();
 887 #endif
 888 #if ENABLE(DFG_JIT)
 889     void tallyFrequentExitSites();
 890 #else
 891     void tallyFrequentExitSites() { }
 892 #endif
 893 
 894 private:
 895     friend class CodeBlockSet;
 896     friend class ExecutableToCodeBlockEdge;
 897 
 898     BytecodeLivenessAnalysis&amp; livenessAnalysisSlow();
 899 
 900     CodeBlock* specialOSREntryBlockOrNull();
 901 
 902     void noticeIncomingCall(ExecState* callerFrame);
 903 
 904     double optimizationThresholdScalingFactor();
 905 
 906     void updateAllPredictionsAndCountLiveness(unsigned&amp; numberOfLiveNonArgumentValueProfiles, unsigned&amp; numberOfSamplesInProfiles);
 907 
 908     void setConstantIdentifierSetRegisters(VM&amp;, const Vector&lt;ConstantIdentifierSetEntry&gt;&amp; constants);
 909 
 910     void setConstantRegisters(const Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants, const Vector&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation);
 911 
 912     void replaceConstant(int index, JSValue value)
 913     {
 914         ASSERT(isConstantRegisterIndex(index) &amp;&amp; static_cast&lt;size_t&gt;(index - FirstConstantRegisterIndex) &lt; m_constantRegisters.size());
 915         m_constantRegisters[index - FirstConstantRegisterIndex].set(*m_vm, this, value);
 916     }
 917 
 918     bool shouldVisitStrongly(const ConcurrentJSLocker&amp;);
 919     bool shouldJettisonDueToWeakReference();
 920     bool shouldJettisonDueToOldAge(const ConcurrentJSLocker&amp;);
 921 
 922     void propagateTransitions(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 923     void determineLiveness(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 924 
 925     void stronglyVisitStrongReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 926     void stronglyVisitWeakReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 927     void visitOSRExitTargets(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 928 
 929     unsigned numberOfNonArgumentValueProfiles() { return m_numberOfNonArgumentValueProfiles; }
 930     unsigned totalNumberOfValueProfiles() { return numberOfArgumentValueProfiles() + numberOfNonArgumentValueProfiles(); }
 931     ValueProfile* tryGetValueProfileForBytecodeOffset(int bytecodeOffset);
 932 
 933     Seconds timeSinceCreation()
 934     {
 935         return MonotonicTime::now() - m_creationTime;
 936     }
 937 
 938     void createRareDataIfNecessary()
 939     {
 940         if (!m_rareData) {
 941             auto rareData = std::make_unique&lt;RareData&gt;();
 942             WTF::storeStoreFence(); // m_catchProfiles can be touched from compiler threads.
 943             m_rareData = WTFMove(rareData);
 944         }
 945     }
 946 
 947     void insertBasicBlockBoundariesForControlFlowProfiler();
 948     void ensureCatchLivenessIsComputedForBytecodeOffsetSlow(const OpCatch&amp;, InstructionStream::Offset);
 949 
 950     int m_numCalleeLocals;
 951     int m_numVars;
 952     int m_numParameters;
 953     int m_numberOfArgumentsToSkip { 0 };
 954     unsigned m_numberOfNonArgumentValueProfiles { 0 };
 955     union {
 956         unsigned m_debuggerRequests;
 957         struct {
 958             unsigned m_hasDebuggerStatement : 1;
 959             unsigned m_steppingMode : 1;
 960             unsigned m_numBreakpoints : 30;
 961         };
 962     };
 963     unsigned m_instructionCount { 0 };
 964     VirtualRegister m_scopeRegister;
 965     mutable CodeBlockHash m_hash;
 966 
 967     WriteBarrier&lt;UnlinkedCodeBlock&gt; m_unlinkedCode;
 968     WriteBarrier&lt;ScriptExecutable&gt; m_ownerExecutable;
 969     WriteBarrier&lt;ExecutableToCodeBlockEdge&gt; m_ownerEdge;
 970     VM* m_vm;
 971 
 972     const void* m_instructionsRawPointer { nullptr };
 973     SentinelLinkedList&lt;LLIntCallLinkInfo, BasicRawSentinelNode&lt;LLIntCallLinkInfo&gt;&gt; m_incomingLLIntCalls;
 974     StructureWatchpointMap m_llintGetByIdWatchpointMap;
 975     RefPtr&lt;JITCode&gt; m_jitCode;
 976 #if ENABLE(JIT)
 977     std::unique_ptr&lt;JITData&gt; m_jitData;
 978 #endif
 979 #if ENABLE(DFG_JIT)
 980     // This is relevant to non-DFG code blocks that serve as the profiled code block
 981     // for DFG code blocks.
 982     CompressedLazyOperandValueProfileHolder m_lazyOperandValueProfiles;
 983 #endif
 984     RefCountedArray&lt;ValueProfile&gt; m_argumentValueProfiles;
 985 
 986     // Constant Pool
 987     COMPILE_ASSERT(sizeof(Register) == sizeof(WriteBarrier&lt;Unknown&gt;), Register_must_be_same_size_as_WriteBarrier_Unknown);
 988     // TODO: This could just be a pointer to m_unlinkedCodeBlock&#39;s data, but the DFG mutates
 989     // it, so we&#39;re stuck with it for now.
 990     Vector&lt;WriteBarrier&lt;Unknown&gt;&gt; m_constantRegisters;
 991     Vector&lt;SourceCodeRepresentation&gt; m_constantsSourceCodeRepresentation;
 992     RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt; m_functionDecls;
 993     RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt; m_functionExprs;
 994 
 995     WriteBarrier&lt;CodeBlock&gt; m_alternative;
 996 
 997     BaselineExecutionCounter m_llintExecuteCounter;
 998 
 999     BaselineExecutionCounter m_jitExecuteCounter;
1000     uint32_t m_osrExitCounter;
1001 
1002     uint16_t m_optimizationDelayCounter;
1003     uint16_t m_reoptimizationRetryCounter;
1004 
1005     RefPtr&lt;MetadataTable&gt; m_metadata;
1006 
1007     MonotonicTime m_creationTime;
1008 
1009     std::unique_ptr&lt;RareData&gt; m_rareData;
1010 };
1011 
1012 inline Register&amp; ExecState::r(int index)
1013 {
1014     CodeBlock* codeBlock = this-&gt;codeBlock();
1015     if (codeBlock-&gt;isConstantRegisterIndex(index))
1016         return *reinterpret_cast&lt;Register*&gt;(&amp;codeBlock-&gt;constantRegister(index));
1017     return this[index];
1018 }
1019 
1020 inline Register&amp; ExecState::r(VirtualRegister reg)
1021 {
1022     return r(reg.offset());
1023 }
1024 
1025 inline Register&amp; ExecState::uncheckedR(int index)
1026 {
1027     RELEASE_ASSERT(index &lt; FirstConstantRegisterIndex);
1028     return this[index];
1029 }
1030 
1031 inline Register&amp; ExecState::uncheckedR(VirtualRegister reg)
1032 {
1033     return uncheckedR(reg.offset());
1034 }
1035 
1036 template &lt;typename ExecutableType&gt;
1037 JSObject* ScriptExecutable::prepareForExecution(VM&amp; vm, JSFunction* function, JSScope* scope, CodeSpecializationKind kind, CodeBlock*&amp; resultCodeBlock)
1038 {
1039     if (hasJITCodeFor(kind)) {
1040         if (std::is_same&lt;ExecutableType, EvalExecutable&gt;::value)
1041             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;EvalExecutable*&gt;(this)-&gt;codeBlock());
1042         else if (std::is_same&lt;ExecutableType, ProgramExecutable&gt;::value)
1043             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;ProgramExecutable*&gt;(this)-&gt;codeBlock());
1044         else if (std::is_same&lt;ExecutableType, ModuleProgramExecutable&gt;::value)
1045             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;ModuleProgramExecutable*&gt;(this)-&gt;codeBlock());
1046         else if (std::is_same&lt;ExecutableType, FunctionExecutable&gt;::value)
1047             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;FunctionExecutable*&gt;(this)-&gt;codeBlockFor(kind));
1048         else
1049             RELEASE_ASSERT_NOT_REACHED();
1050         return nullptr;
1051     }
1052     return prepareForExecutionImpl(vm, function, scope, kind, resultCodeBlock);
1053 }
1054 
1055 #define CODEBLOCK_LOG_EVENT(codeBlock, summary, details) \
1056     (codeBlock-&gt;vm()-&gt;logEvent(codeBlock, summary, [&amp;] () { return toCString details; }))
1057 
1058 
1059 void setPrinter(Printer::PrintRecord&amp;, CodeBlock*);
1060 
1061 } // namespace JSC
1062 
1063 namespace WTF {
1064 
1065 JS_EXPORT_PRIVATE void printInternal(PrintStream&amp;, JSC::CodeBlock*);
1066 
1067 } // namespace WTF
    </pre>
  </body>
</html>