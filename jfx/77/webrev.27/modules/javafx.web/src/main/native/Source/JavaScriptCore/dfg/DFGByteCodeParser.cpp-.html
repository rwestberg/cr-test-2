<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGByteCodeParser.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2011-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;DFGByteCodeParser.h&quot;
  28 
  29 #if ENABLE(DFG_JIT)
  30 
  31 #include &quot;ArithProfile.h&quot;
  32 #include &quot;ArrayConstructor.h&quot;
  33 #include &quot;BasicBlockLocation.h&quot;
  34 #include &quot;BuiltinNames.h&quot;
  35 #include &quot;BytecodeStructs.h&quot;
  36 #include &quot;CallLinkStatus.h&quot;
  37 #include &quot;CodeBlock.h&quot;
  38 #include &quot;CodeBlockWithJITType.h&quot;
  39 #include &quot;CommonSlowPaths.h&quot;
  40 #include &quot;DFGAbstractHeap.h&quot;
  41 #include &quot;DFGArrayMode.h&quot;
  42 #include &quot;DFGCFG.h&quot;
  43 #include &quot;DFGCapabilities.h&quot;
  44 #include &quot;DFGClobberize.h&quot;
  45 #include &quot;DFGClobbersExitState.h&quot;
  46 #include &quot;DFGGraph.h&quot;
  47 #include &quot;DFGJITCode.h&quot;
  48 #include &quot;FunctionCodeBlock.h&quot;
  49 #include &quot;GetByIdStatus.h&quot;
  50 #include &quot;Heap.h&quot;
  51 #include &quot;InByIdStatus.h&quot;
  52 #include &quot;InstanceOfStatus.h&quot;
  53 #include &quot;JSCInlines.h&quot;
  54 #include &quot;JSFixedArray.h&quot;
  55 #include &quot;JSImmutableButterfly.h&quot;
  56 #include &quot;JSModuleEnvironment.h&quot;
  57 #include &quot;JSModuleNamespaceObject.h&quot;
  58 #include &quot;NumberConstructor.h&quot;
  59 #include &quot;ObjectConstructor.h&quot;
  60 #include &quot;OpcodeInlines.h&quot;
  61 #include &quot;PreciseJumpTargets.h&quot;
  62 #include &quot;PutByIdFlags.h&quot;
  63 #include &quot;PutByIdStatus.h&quot;
  64 #include &quot;RegExpPrototype.h&quot;
  65 #include &quot;StackAlignment.h&quot;
  66 #include &quot;StringConstructor.h&quot;
  67 #include &quot;StructureStubInfo.h&quot;
  68 #include &quot;SymbolConstructor.h&quot;
  69 #include &quot;Watchdog.h&quot;
  70 #include &lt;wtf/CommaPrinter.h&gt;
  71 #include &lt;wtf/HashMap.h&gt;
  72 #include &lt;wtf/MathExtras.h&gt;
  73 #include &lt;wtf/SetForScope.h&gt;
  74 #include &lt;wtf/StdLibExtras.h&gt;
  75 
  76 namespace JSC { namespace DFG {
  77 
  78 namespace DFGByteCodeParserInternal {
  79 #ifdef NDEBUG
  80 static const bool verbose = false;
  81 #else
  82 static const bool verbose = true;
  83 #endif
  84 } // namespace DFGByteCodeParserInternal
  85 
  86 #define VERBOSE_LOG(...) do { \
  87 if (DFGByteCodeParserInternal::verbose &amp;&amp; Options::verboseDFGBytecodeParsing()) \
  88 dataLog(__VA_ARGS__); \
  89 } while (false)
  90 
  91 // === ByteCodeParser ===
  92 //
  93 // This class is used to compile the dataflow graph from a CodeBlock.
  94 class ByteCodeParser {
  95 public:
  96     ByteCodeParser(Graph&amp; graph)
  97         : m_vm(&amp;graph.m_vm)
  98         , m_codeBlock(graph.m_codeBlock)
  99         , m_profiledBlock(graph.m_profiledBlock)
 100         , m_graph(graph)
 101         , m_currentBlock(0)
 102         , m_currentIndex(0)
 103         , m_constantUndefined(graph.freeze(jsUndefined()))
 104         , m_constantNull(graph.freeze(jsNull()))
 105         , m_constantNaN(graph.freeze(jsNumber(PNaN)))
 106         , m_constantOne(graph.freeze(jsNumber(1)))
 107         , m_numArguments(m_codeBlock-&gt;numParameters())
 108         , m_numLocals(m_codeBlock-&gt;numCalleeLocals())
 109         , m_parameterSlots(0)
 110         , m_numPassedVarArgs(0)
 111         , m_inlineStackTop(0)
 112         , m_currentInstruction(0)
 113         , m_hasDebuggerEnabled(graph.hasDebuggerEnabled())
 114     {
 115         ASSERT(m_profiledBlock);
 116     }
 117 
 118     // Parse a full CodeBlock of bytecode.
 119     void parse();
 120 
 121 private:
 122     struct InlineStackEntry;
 123 
 124     // Just parse from m_currentIndex to the end of the current CodeBlock.
 125     void parseCodeBlock();
 126 
 127     void ensureLocals(unsigned newNumLocals)
 128     {
 129         VERBOSE_LOG(&quot;   ensureLocals: trying to raise m_numLocals from &quot;, m_numLocals, &quot; to &quot;, newNumLocals, &quot;\n&quot;);
 130         if (newNumLocals &lt;= m_numLocals)
 131             return;
 132         m_numLocals = newNumLocals;
 133         for (size_t i = 0; i &lt; m_graph.numBlocks(); ++i)
 134             m_graph.block(i)-&gt;ensureLocals(newNumLocals);
 135     }
 136 
 137     // Helper for min and max.
 138     template&lt;typename ChecksFunctor&gt;
 139     bool handleMinMax(VirtualRegister result, NodeType op, int registerOffset, int argumentCountIncludingThis, const ChecksFunctor&amp; insertChecks);
 140 
 141     void refineStatically(CallLinkStatus&amp;, Node* callTarget);
 142     // Blocks can either be targetable (i.e. in the m_blockLinkingTargets of one InlineStackEntry) with a well-defined bytecodeBegin,
 143     // or they can be untargetable, with bytecodeBegin==UINT_MAX, to be managed manually and not by the linkBlock machinery.
 144     // This is used most notably when doing polyvariant inlining (it requires a fair bit of control-flow with no bytecode analog).
 145     // It is also used when doing an early return from an inlined callee: it is easier to fix the bytecode index later on if needed
 146     // than to move the right index all the way to the treatment of op_ret.
 147     BasicBlock* allocateTargetableBlock(unsigned bytecodeIndex);
 148     BasicBlock* allocateUntargetableBlock();
 149     // An untargetable block can be given a bytecodeIndex to be later managed by linkBlock, but only once, and it can never go in the other direction
 150     void makeBlockTargetable(BasicBlock*, unsigned bytecodeIndex);
 151     void addJumpTo(BasicBlock*);
 152     void addJumpTo(unsigned bytecodeIndex);
 153     // Handle calls. This resolves issues surrounding inlining and intrinsics.
 154     enum Terminality { Terminal, NonTerminal };
 155     Terminality handleCall(
 156         VirtualRegister result, NodeType op, InlineCallFrame::Kind, unsigned instructionSize,
 157         Node* callTarget, int argumentCountIncludingThis, int registerOffset, CallLinkStatus,
 158         SpeculatedType prediction);
 159     template&lt;typename CallOp&gt;
 160     Terminality handleCall(const Instruction* pc, NodeType op, CallMode);
 161     template&lt;typename CallOp&gt;
 162     Terminality handleVarargsCall(const Instruction* pc, NodeType op, CallMode);
 163     void emitFunctionChecks(CallVariant, Node* callTarget, VirtualRegister thisArgumnt);
 164     void emitArgumentPhantoms(int registerOffset, int argumentCountIncludingThis);
 165     Node* getArgumentCount();
 166     template&lt;typename ChecksFunctor&gt;
 167     bool handleRecursiveTailCall(Node* callTargetNode, CallVariant, int registerOffset, int argumentCountIncludingThis, const ChecksFunctor&amp; emitFunctionCheckIfNeeded);
 168     unsigned inliningCost(CallVariant, int argumentCountIncludingThis, InlineCallFrame::Kind); // Return UINT_MAX if it&#39;s not an inlining candidate. By convention, intrinsics have a cost of 1.
 169     // Handle inlining. Return true if it succeeded, false if we need to plant a call.
 170     bool handleVarargsInlining(Node* callTargetNode, VirtualRegister result, const CallLinkStatus&amp;, int registerOffset, VirtualRegister thisArgument, VirtualRegister argumentsArgument, unsigned argumentsOffset, NodeType callOp, InlineCallFrame::Kind);
 171     unsigned getInliningBalance(const CallLinkStatus&amp;, CodeSpecializationKind);
 172     enum class CallOptimizationResult { OptimizedToJump, Inlined, DidNothing };
 173     CallOptimizationResult handleCallVariant(Node* callTargetNode, VirtualRegister result, CallVariant, int registerOffset, VirtualRegister thisArgument, int argumentCountIncludingThis, unsigned nextOffset, InlineCallFrame::Kind, SpeculatedType prediction, unsigned&amp; inliningBalance, BasicBlock* continuationBlock, bool needsToCheckCallee);
 174     CallOptimizationResult handleInlining(Node* callTargetNode, VirtualRegister result, const CallLinkStatus&amp;, int registerOffset, VirtualRegister thisArgument, int argumentCountIncludingThis, unsigned nextOffset, NodeType callOp, InlineCallFrame::Kind, SpeculatedType prediction);
 175     template&lt;typename ChecksFunctor&gt;
 176     void inlineCall(Node* callTargetNode, VirtualRegister result, CallVariant, int registerOffset, int argumentCountIncludingThis, InlineCallFrame::Kind, BasicBlock* continuationBlock, const ChecksFunctor&amp; insertChecks);
 177     // Handle intrinsic functions. Return true if it succeeded, false if we need to plant a call.
 178     template&lt;typename ChecksFunctor&gt;
 179     bool handleIntrinsicCall(Node* callee, VirtualRegister result, Intrinsic, int registerOffset, int argumentCountIncludingThis, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks);
 180     template&lt;typename ChecksFunctor&gt;
 181     bool handleDOMJITCall(Node* callee, VirtualRegister result, const DOMJIT::Signature*, int registerOffset, int argumentCountIncludingThis, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks);
 182     template&lt;typename ChecksFunctor&gt;
 183     bool handleIntrinsicGetter(VirtualRegister result, SpeculatedType prediction, const GetByIdVariant&amp; intrinsicVariant, Node* thisNode, const ChecksFunctor&amp; insertChecks);
 184     template&lt;typename ChecksFunctor&gt;
 185     bool handleTypedArrayConstructor(VirtualRegister result, InternalFunction*, int registerOffset, int argumentCountIncludingThis, TypedArrayType, const ChecksFunctor&amp; insertChecks);
 186     template&lt;typename ChecksFunctor&gt;
 187     bool handleConstantInternalFunction(Node* callTargetNode, VirtualRegister result, InternalFunction*, int registerOffset, int argumentCountIncludingThis, CodeSpecializationKind, SpeculatedType, const ChecksFunctor&amp; insertChecks);
 188     Node* handlePutByOffset(Node* base, unsigned identifier, PropertyOffset, Node* value);
 189     Node* handleGetByOffset(SpeculatedType, Node* base, unsigned identifierNumber, PropertyOffset, NodeType = GetByOffset);
 190     bool handleDOMJITGetter(VirtualRegister result, const GetByIdVariant&amp;, Node* thisNode, unsigned identifierNumber, SpeculatedType prediction);
 191     bool handleModuleNamespaceLoad(VirtualRegister result, SpeculatedType, Node* base, GetByIdStatus);
 192 
 193     template&lt;typename Bytecode&gt;
 194     void handlePutByVal(Bytecode, unsigned instructionSize);
 195     template &lt;typename Bytecode&gt;
 196     void handlePutAccessorById(NodeType, Bytecode);
 197     template &lt;typename Bytecode&gt;
 198     void handlePutAccessorByVal(NodeType, Bytecode);
 199     template &lt;typename Bytecode&gt;
 200     void handleNewFunc(NodeType, Bytecode);
 201     template &lt;typename Bytecode&gt;
 202     void handleNewFuncExp(NodeType, Bytecode);
 203 
 204     // Create a presence ObjectPropertyCondition based on some known offset and structure set. Does not
 205     // check the validity of the condition, but it may return a null one if it encounters a contradiction.
 206     ObjectPropertyCondition presenceLike(
 207         JSObject* knownBase, UniquedStringImpl*, PropertyOffset, const StructureSet&amp;);
 208 
 209     // Attempt to watch the presence of a property. It will watch that the property is present in the same
 210     // way as in all of the structures in the set. It may emit code instead of just setting a watchpoint.
 211     // Returns true if this all works out.
 212     bool checkPresenceLike(JSObject* knownBase, UniquedStringImpl*, PropertyOffset, const StructureSet&amp;);
 213     void checkPresenceLike(Node* base, UniquedStringImpl*, PropertyOffset, const StructureSet&amp;);
 214 
 215     // Works with both GetByIdVariant and the setter form of PutByIdVariant.
 216     template&lt;typename VariantType&gt;
 217     Node* load(SpeculatedType, Node* base, unsigned identifierNumber, const VariantType&amp;);
 218 
 219     Node* store(Node* base, unsigned identifier, const PutByIdVariant&amp;, Node* value);
 220 
 221     template&lt;typename Op&gt;
 222     void parseGetById(const Instruction*);
 223     void handleGetById(
 224         VirtualRegister destination, SpeculatedType, Node* base, unsigned identifierNumber, GetByIdStatus, AccessType, unsigned instructionSize);
 225     void emitPutById(
 226         Node* base, unsigned identifierNumber, Node* value,  const PutByIdStatus&amp;, bool isDirect);
 227     void handlePutById(
 228         Node* base, unsigned identifierNumber, Node* value, const PutByIdStatus&amp;,
 229         bool isDirect, unsigned intructionSize);
 230 
 231     // Either register a watchpoint or emit a check for this condition. Returns false if the
 232     // condition no longer holds, and therefore no reasonable check can be emitted.
 233     bool check(const ObjectPropertyCondition&amp;);
 234 
 235     GetByOffsetMethod promoteToConstant(GetByOffsetMethod);
 236 
 237     // Either register a watchpoint or emit a check for this condition. It must be a Presence
 238     // condition. It will attempt to promote a Presence condition to an Equivalence condition.
 239     // Emits code for the loaded value that the condition guards, and returns a node containing
 240     // the loaded value. Returns null if the condition no longer holds.
 241     GetByOffsetMethod planLoad(const ObjectPropertyCondition&amp;);
 242     Node* load(SpeculatedType, unsigned identifierNumber, const GetByOffsetMethod&amp;, NodeType = GetByOffset);
 243     Node* load(SpeculatedType, const ObjectPropertyCondition&amp;, NodeType = GetByOffset);
 244 
 245     // Calls check() for each condition in the set: that is, it either emits checks or registers
 246     // watchpoints (or a combination of the two) to make the conditions hold. If any of those
 247     // conditions are no longer checkable, returns false.
 248     bool check(const ObjectPropertyConditionSet&amp;);
 249 
 250     // Calls check() for those conditions that aren&#39;t the slot base, and calls load() for the slot
 251     // base. Does a combination of watchpoint registration and check emission to guard the
 252     // conditions, and emits code to load the value from the slot base. Returns a node containing
 253     // the loaded value. Returns null if any of the conditions were no longer checkable.
 254     GetByOffsetMethod planLoad(const ObjectPropertyConditionSet&amp;);
 255     Node* load(SpeculatedType, const ObjectPropertyConditionSet&amp;, NodeType = GetByOffset);
 256 
 257     void prepareToParseBlock();
 258     void clearCaches();
 259 
 260     // Parse a single basic block of bytecode instructions.
 261     void parseBlock(unsigned limit);
 262     // Link block successors.
 263     void linkBlock(BasicBlock*, Vector&lt;BasicBlock*&gt;&amp; possibleTargets);
 264     void linkBlocks(Vector&lt;BasicBlock*&gt;&amp; unlinkedBlocks, Vector&lt;BasicBlock*&gt;&amp; possibleTargets);
 265 
 266     VariableAccessData* newVariableAccessData(VirtualRegister operand)
 267     {
 268         ASSERT(!operand.isConstant());
 269 
 270         m_graph.m_variableAccessData.append(VariableAccessData(operand));
 271         return &amp;m_graph.m_variableAccessData.last();
 272     }
 273 
 274     // Get/Set the operands/result of a bytecode instruction.
 275     Node* getDirect(VirtualRegister operand)
 276     {
 277         ASSERT(!operand.isConstant());
 278 
 279         // Is this an argument?
 280         if (operand.isArgument())
 281             return getArgument(operand);
 282 
 283         // Must be a local.
 284         return getLocal(operand);
 285     }
 286 
 287     Node* get(VirtualRegister operand)
 288     {
 289         if (operand.isConstant()) {
 290             unsigned constantIndex = operand.toConstantIndex();
 291             unsigned oldSize = m_constants.size();
 292             if (constantIndex &gt;= oldSize || !m_constants[constantIndex]) {
 293                 const CodeBlock&amp; codeBlock = *m_inlineStackTop-&gt;m_codeBlock;
 294                 JSValue value = codeBlock.getConstant(operand.offset());
 295                 SourceCodeRepresentation sourceCodeRepresentation = codeBlock.constantSourceCodeRepresentation(operand.offset());
 296                 if (constantIndex &gt;= oldSize) {
 297                     m_constants.grow(constantIndex + 1);
 298                     for (unsigned i = oldSize; i &lt; m_constants.size(); ++i)
 299                         m_constants[i] = nullptr;
 300                 }
 301 
 302                 Node* constantNode = nullptr;
 303                 if (sourceCodeRepresentation == SourceCodeRepresentation::Double)
 304                     constantNode = addToGraph(DoubleConstant, OpInfo(m_graph.freezeStrong(jsDoubleNumber(value.asNumber()))));
 305                 else
 306                     constantNode = addToGraph(JSConstant, OpInfo(m_graph.freezeStrong(value)));
 307                 m_constants[constantIndex] = constantNode;
 308             }
 309             ASSERT(m_constants[constantIndex]);
 310             return m_constants[constantIndex];
 311         }
 312 
 313         if (inlineCallFrame()) {
 314             if (!inlineCallFrame()-&gt;isClosureCall) {
 315                 JSFunction* callee = inlineCallFrame()-&gt;calleeConstant();
 316                 if (operand.offset() == CallFrameSlot::callee)
 317                     return weakJSConstant(callee);
 318             }
 319         } else if (operand.offset() == CallFrameSlot::callee) {
 320             // We have to do some constant-folding here because this enables CreateThis folding. Note
 321             // that we don&#39;t have such watchpoint-based folding for inlined uses of Callee, since in that
 322             // case if the function is a singleton then we already know it.
 323             if (FunctionExecutable* executable = jsDynamicCast&lt;FunctionExecutable*&gt;(*m_vm, m_codeBlock-&gt;ownerExecutable())) {
 324                 InferredValue* singleton = executable-&gt;singletonFunction();
 325                 if (JSValue value = singleton-&gt;inferredValue()) {
 326                     m_graph.watchpoints().addLazily(singleton);
 327                     JSFunction* function = jsCast&lt;JSFunction*&gt;(value);
 328                     return weakJSConstant(function);
 329                 }
 330             }
 331             return addToGraph(GetCallee);
 332         }
 333 
 334         return getDirect(m_inlineStackTop-&gt;remapOperand(operand));
 335     }
 336 
 337     enum SetMode {
 338         // A normal set which follows a two-phase commit that spans code origins. During
 339         // the current code origin it issues a MovHint, and at the start of the next
 340         // code origin there will be a SetLocal. If the local needs flushing, the second
 341         // SetLocal will be preceded with a Flush.
 342         NormalSet,
 343 
 344         // A set where the SetLocal happens immediately and there is still a Flush. This
 345         // is relevant when assigning to a local in tricky situations for the delayed
 346         // SetLocal logic but where we know that we have not performed any side effects
 347         // within this code origin. This is a safe replacement for NormalSet anytime we
 348         // know that we have not yet performed side effects in this code origin.
 349         ImmediateSetWithFlush,
 350 
 351         // A set where the SetLocal happens immediately and we do not Flush it even if
 352         // this is a local that is marked as needing it. This is relevant when
 353         // initializing locals at the top of a function.
 354         ImmediateNakedSet
 355     };
 356     Node* setDirect(VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 357     {
 358         addToGraph(MovHint, OpInfo(operand.offset()), value);
 359 
 360         // We can&#39;t exit anymore because our OSR exit state has changed.
 361         m_exitOK = false;
 362 
 363         DelayedSetLocal delayed(currentCodeOrigin(), operand, value, setMode);
 364 
 365         if (setMode == NormalSet) {
 366             m_setLocalQueue.append(delayed);
 367             return nullptr;
 368         }
 369 
 370         return delayed.execute(this);
 371     }
 372 
 373     void processSetLocalQueue()
 374     {
 375         for (unsigned i = 0; i &lt; m_setLocalQueue.size(); ++i)
 376             m_setLocalQueue[i].execute(this);
 377         m_setLocalQueue.shrink(0);
 378     }
 379 
 380     Node* set(VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 381     {
 382         return setDirect(m_inlineStackTop-&gt;remapOperand(operand), value, setMode);
 383     }
 384 
 385     Node* injectLazyOperandSpeculation(Node* node)
 386     {
 387         ASSERT(node-&gt;op() == GetLocal);
 388         ASSERT(node-&gt;origin.semantic.bytecodeIndex == m_currentIndex);
 389         ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
 390         LazyOperandValueProfileKey key(m_currentIndex, node-&gt;local());
 391         SpeculatedType prediction = m_inlineStackTop-&gt;m_lazyOperands.prediction(locker, key);
 392         node-&gt;variableAccessData()-&gt;predict(prediction);
 393         return node;
 394     }
 395 
 396     // Used in implementing get/set, above, where the operand is a local variable.
 397     Node* getLocal(VirtualRegister operand)
 398     {
 399         unsigned local = operand.toLocal();
 400 
 401         Node* node = m_currentBlock-&gt;variablesAtTail.local(local);
 402 
 403         // This has two goals: 1) link together variable access datas, and 2)
 404         // try to avoid creating redundant GetLocals. (1) is required for
 405         // correctness - no other phase will ensure that block-local variable
 406         // access data unification is done correctly. (2) is purely opportunistic
 407         // and is meant as an compile-time optimization only.
 408 
 409         VariableAccessData* variable;
 410 
 411         if (node) {
 412             variable = node-&gt;variableAccessData();
 413 
 414             switch (node-&gt;op()) {
 415             case GetLocal:
 416                 return node;
 417             case SetLocal:
 418                 return node-&gt;child1().node();
 419             default:
 420                 break;
 421             }
 422         } else
 423             variable = newVariableAccessData(operand);
 424 
 425         node = injectLazyOperandSpeculation(addToGraph(GetLocal, OpInfo(variable)));
 426         m_currentBlock-&gt;variablesAtTail.local(local) = node;
 427         return node;
 428     }
 429     Node* setLocal(const CodeOrigin&amp; semanticOrigin, VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 430     {
 431         SetForScope&lt;CodeOrigin&gt; originChange(m_currentSemanticOrigin, semanticOrigin);
 432 
 433         unsigned local = operand.toLocal();
 434 
 435         if (setMode != ImmediateNakedSet) {
 436             ArgumentPosition* argumentPosition = findArgumentPositionForLocal(operand);
 437             if (argumentPosition)
 438                 flushDirect(operand, argumentPosition);
 439             else if (m_graph.needsScopeRegister() &amp;&amp; operand == m_codeBlock-&gt;scopeRegister())
 440                 flush(operand);
 441         }
 442 
 443         VariableAccessData* variableAccessData = newVariableAccessData(operand);
 444         variableAccessData-&gt;mergeStructureCheckHoistingFailed(
 445             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex, BadCache));
 446         variableAccessData-&gt;mergeCheckArrayHoistingFailed(
 447             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex, BadIndexingType));
 448         Node* node = addToGraph(SetLocal, OpInfo(variableAccessData), value);
 449         m_currentBlock-&gt;variablesAtTail.local(local) = node;
 450         return node;
 451     }
 452 
 453     // Used in implementing get/set, above, where the operand is an argument.
 454     Node* getArgument(VirtualRegister operand)
 455     {
 456         unsigned argument = operand.toArgument();
 457         ASSERT(argument &lt; m_numArguments);
 458 
 459         Node* node = m_currentBlock-&gt;variablesAtTail.argument(argument);
 460 
 461         VariableAccessData* variable;
 462 
 463         if (node) {
 464             variable = node-&gt;variableAccessData();
 465 
 466             switch (node-&gt;op()) {
 467             case GetLocal:
 468                 return node;
 469             case SetLocal:
 470                 return node-&gt;child1().node();
 471             default:
 472                 break;
 473             }
 474         } else
 475             variable = newVariableAccessData(operand);
 476 
 477         node = injectLazyOperandSpeculation(addToGraph(GetLocal, OpInfo(variable)));
 478         m_currentBlock-&gt;variablesAtTail.argument(argument) = node;
 479         return node;
 480     }
 481     Node* setArgument(const CodeOrigin&amp; semanticOrigin, VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 482     {
 483         SetForScope&lt;CodeOrigin&gt; originChange(m_currentSemanticOrigin, semanticOrigin);
 484 
 485         unsigned argument = operand.toArgument();
 486         ASSERT(argument &lt; m_numArguments);
 487 
 488         VariableAccessData* variableAccessData = newVariableAccessData(operand);
 489 
 490         // Always flush arguments, except for &#39;this&#39;. If &#39;this&#39; is created by us,
 491         // then make sure that it&#39;s never unboxed.
 492         if (argument || m_graph.needsFlushedThis()) {
 493             if (setMode != ImmediateNakedSet)
 494                 flushDirect(operand);
 495         }
 496 
 497         if (!argument &amp;&amp; m_codeBlock-&gt;specializationKind() == CodeForConstruct)
 498             variableAccessData-&gt;mergeShouldNeverUnbox(true);
 499 
 500         variableAccessData-&gt;mergeStructureCheckHoistingFailed(
 501             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex, BadCache));
 502         variableAccessData-&gt;mergeCheckArrayHoistingFailed(
 503             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex, BadIndexingType));
 504         Node* node = addToGraph(SetLocal, OpInfo(variableAccessData), value);
 505         m_currentBlock-&gt;variablesAtTail.argument(argument) = node;
 506         return node;
 507     }
 508 
 509     ArgumentPosition* findArgumentPositionForArgument(int argument)
 510     {
 511         InlineStackEntry* stack = m_inlineStackTop;
 512         while (stack-&gt;m_inlineCallFrame)
 513             stack = stack-&gt;m_caller;
 514         return stack-&gt;m_argumentPositions[argument];
 515     }
 516 
 517     ArgumentPosition* findArgumentPositionForLocal(VirtualRegister operand)
 518     {
 519         for (InlineStackEntry* stack = m_inlineStackTop; ; stack = stack-&gt;m_caller) {
 520             InlineCallFrame* inlineCallFrame = stack-&gt;m_inlineCallFrame;
 521             if (!inlineCallFrame)
 522                 break;
 523             if (operand.offset() &lt; static_cast&lt;int&gt;(inlineCallFrame-&gt;stackOffset + CallFrame::headerSizeInRegisters))
 524                 continue;
 525             if (operand.offset() &gt;= static_cast&lt;int&gt;(inlineCallFrame-&gt;stackOffset + CallFrame::thisArgumentOffset() + inlineCallFrame-&gt;argumentsWithFixup.size()))
 526                 continue;
 527             int argument = VirtualRegister(operand.offset() - inlineCallFrame-&gt;stackOffset).toArgument();
 528             return stack-&gt;m_argumentPositions[argument];
 529         }
 530         return 0;
 531     }
 532 
 533     ArgumentPosition* findArgumentPosition(VirtualRegister operand)
 534     {
 535         if (operand.isArgument())
 536             return findArgumentPositionForArgument(operand.toArgument());
 537         return findArgumentPositionForLocal(operand);
 538     }
 539 
 540     template&lt;typename AddFlushDirectFunc&gt;
 541     void flushImpl(InlineCallFrame* inlineCallFrame, const AddFlushDirectFunc&amp; addFlushDirect)
 542     {
 543         int numArguments;
 544         if (inlineCallFrame) {
 545             ASSERT(!m_graph.hasDebuggerEnabled());
 546             numArguments = inlineCallFrame-&gt;argumentsWithFixup.size();
 547             if (inlineCallFrame-&gt;isClosureCall)
 548                 addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, VirtualRegister(CallFrameSlot::callee)));
 549             if (inlineCallFrame-&gt;isVarargs())
 550                 addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, VirtualRegister(CallFrameSlot::argumentCount)));
 551         } else
 552             numArguments = m_graph.baselineCodeBlockFor(inlineCallFrame)-&gt;numParameters();
 553 
 554         for (unsigned argument = numArguments; argument--;)
 555             addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, virtualRegisterForArgument(argument)));
 556 
 557         if (m_graph.needsScopeRegister())
 558             addFlushDirect(nullptr, m_graph.m_codeBlock-&gt;scopeRegister());
 559     }
 560 
 561     template&lt;typename AddFlushDirectFunc, typename AddPhantomLocalDirectFunc&gt;
 562     void flushForTerminalImpl(CodeOrigin origin, const AddFlushDirectFunc&amp; addFlushDirect, const AddPhantomLocalDirectFunc&amp; addPhantomLocalDirect)
 563     {
 564         origin.walkUpInlineStack(
 565             [&amp;] (CodeOrigin origin) {
 566                 unsigned bytecodeIndex = origin.bytecodeIndex;
 567                 InlineCallFrame* inlineCallFrame = origin.inlineCallFrame;
 568                 flushImpl(inlineCallFrame, addFlushDirect);
 569 
 570                 CodeBlock* codeBlock = m_graph.baselineCodeBlockFor(inlineCallFrame);
 571                 FullBytecodeLiveness&amp; fullLiveness = m_graph.livenessFor(codeBlock);
 572                 const FastBitVector&amp; livenessAtBytecode = fullLiveness.getLiveness(bytecodeIndex);
 573 
 574                 for (unsigned local = codeBlock-&gt;numCalleeLocals(); local--;) {
 575                     if (livenessAtBytecode[local])
 576                         addPhantomLocalDirect(inlineCallFrame, remapOperand(inlineCallFrame, virtualRegisterForLocal(local)));
 577                 }
 578             });
 579     }
 580 
 581     void flush(VirtualRegister operand)
 582     {
 583         flushDirect(m_inlineStackTop-&gt;remapOperand(operand));
 584     }
 585 
 586     void flushDirect(VirtualRegister operand)
 587     {
 588         flushDirect(operand, findArgumentPosition(operand));
 589     }
 590 
 591     void flushDirect(VirtualRegister operand, ArgumentPosition* argumentPosition)
 592     {
 593         addFlushOrPhantomLocal&lt;Flush&gt;(operand, argumentPosition);
 594     }
 595 
 596     template&lt;NodeType nodeType&gt;
 597     void addFlushOrPhantomLocal(VirtualRegister operand, ArgumentPosition* argumentPosition)
 598     {
 599         ASSERT(!operand.isConstant());
 600 
 601         Node* node = m_currentBlock-&gt;variablesAtTail.operand(operand);
 602 
 603         VariableAccessData* variable;
 604 
 605         if (node)
 606             variable = node-&gt;variableAccessData();
 607         else
 608             variable = newVariableAccessData(operand);
 609 
 610         node = addToGraph(nodeType, OpInfo(variable));
 611         m_currentBlock-&gt;variablesAtTail.operand(operand) = node;
 612         if (argumentPosition)
 613             argumentPosition-&gt;addVariable(variable);
 614     }
 615 
 616     void phantomLocalDirect(VirtualRegister operand)
 617     {
 618         addFlushOrPhantomLocal&lt;PhantomLocal&gt;(operand, findArgumentPosition(operand));
 619     }
 620 
 621     void flush(InlineStackEntry* inlineStackEntry)
 622     {
 623         auto addFlushDirect = [&amp;] (InlineCallFrame*, VirtualRegister reg) { flushDirect(reg); };
 624         flushImpl(inlineStackEntry-&gt;m_inlineCallFrame, addFlushDirect);
 625     }
 626 
 627     void flushForTerminal()
 628     {
 629         auto addFlushDirect = [&amp;] (InlineCallFrame*, VirtualRegister reg) { flushDirect(reg); };
 630         auto addPhantomLocalDirect = [&amp;] (InlineCallFrame*, VirtualRegister reg) { phantomLocalDirect(reg); };
 631         flushForTerminalImpl(currentCodeOrigin(), addFlushDirect, addPhantomLocalDirect);
 632     }
 633 
 634     void flushForReturn()
 635     {
 636         flush(m_inlineStackTop);
 637     }
 638 
 639     void flushIfTerminal(SwitchData&amp; data)
 640     {
 641         if (data.fallThrough.bytecodeIndex() &gt; m_currentIndex)
 642             return;
 643 
 644         for (unsigned i = data.cases.size(); i--;) {
 645             if (data.cases[i].target.bytecodeIndex() &gt; m_currentIndex)
 646                 return;
 647         }
 648 
 649         flushForTerminal();
 650     }
 651 
 652     // Assumes that the constant should be strongly marked.
 653     Node* jsConstant(JSValue constantValue)
 654     {
 655         return addToGraph(JSConstant, OpInfo(m_graph.freezeStrong(constantValue)));
 656     }
 657 
 658     Node* weakJSConstant(JSValue constantValue)
 659     {
 660         return addToGraph(JSConstant, OpInfo(m_graph.freeze(constantValue)));
 661     }
 662 
 663     // Helper functions to get/set the this value.
 664     Node* getThis()
 665     {
 666         return get(m_inlineStackTop-&gt;m_codeBlock-&gt;thisRegister());
 667     }
 668 
 669     void setThis(Node* value)
 670     {
 671         set(m_inlineStackTop-&gt;m_codeBlock-&gt;thisRegister(), value);
 672     }
 673 
 674     InlineCallFrame* inlineCallFrame()
 675     {
 676         return m_inlineStackTop-&gt;m_inlineCallFrame;
 677     }
 678 
 679     bool allInlineFramesAreTailCalls()
 680     {
 681         return !inlineCallFrame() || !inlineCallFrame()-&gt;getCallerSkippingTailCalls();
 682     }
 683 
 684     CodeOrigin currentCodeOrigin()
 685     {
 686         return CodeOrigin(m_currentIndex, inlineCallFrame());
 687     }
 688 
 689     NodeOrigin currentNodeOrigin()
 690     {
 691         CodeOrigin semantic;
 692         CodeOrigin forExit;
 693 
 694         if (m_currentSemanticOrigin.isSet())
 695             semantic = m_currentSemanticOrigin;
 696         else
 697             semantic = currentCodeOrigin();
 698 
 699         forExit = currentCodeOrigin();
 700 
 701         return NodeOrigin(semantic, forExit, m_exitOK);
 702     }
 703 
 704     BranchData* branchData(unsigned taken, unsigned notTaken)
 705     {
 706         // We assume that branches originating from bytecode always have a fall-through. We
 707         // use this assumption to avoid checking for the creation of terminal blocks.
 708         ASSERT((taken &gt; m_currentIndex) || (notTaken &gt; m_currentIndex));
 709         BranchData* data = m_graph.m_branchData.add();
 710         *data = BranchData::withBytecodeIndices(taken, notTaken);
 711         return data;
 712     }
 713 
 714     Node* addToGraph(Node* node)
 715     {
 716         VERBOSE_LOG(&quot;        appended &quot;, node, &quot; &quot;, Graph::opName(node-&gt;op()), &quot;\n&quot;);
 717 
 718         m_hasAnyForceOSRExits |= (node-&gt;op() == ForceOSRExit);
 719 
 720         m_currentBlock-&gt;append(node);
 721         if (clobbersExitState(m_graph, node))
 722             m_exitOK = false;
 723         return node;
 724     }
 725 
 726     Node* addToGraph(NodeType op, Node* child1 = 0, Node* child2 = 0, Node* child3 = 0)
 727     {
 728         Node* result = m_graph.addNode(
 729             op, currentNodeOrigin(), Edge(child1), Edge(child2),
 730             Edge(child3));
 731         return addToGraph(result);
 732     }
 733     Node* addToGraph(NodeType op, Edge child1, Edge child2 = Edge(), Edge child3 = Edge())
 734     {
 735         Node* result = m_graph.addNode(
 736             op, currentNodeOrigin(), child1, child2, child3);
 737         return addToGraph(result);
 738     }
 739     Node* addToGraph(NodeType op, OpInfo info, Node* child1 = 0, Node* child2 = 0, Node* child3 = 0)
 740     {
 741         Node* result = m_graph.addNode(
 742             op, currentNodeOrigin(), info, Edge(child1), Edge(child2),
 743             Edge(child3));
 744         return addToGraph(result);
 745     }
 746     Node* addToGraph(NodeType op, OpInfo info, Edge child1, Edge child2 = Edge(), Edge child3 = Edge())
 747     {
 748         Node* result = m_graph.addNode(op, currentNodeOrigin(), info, child1, child2, child3);
 749         return addToGraph(result);
 750     }
 751     Node* addToGraph(NodeType op, OpInfo info1, OpInfo info2, Node* child1 = 0, Node* child2 = 0, Node* child3 = 0)
 752     {
 753         Node* result = m_graph.addNode(
 754             op, currentNodeOrigin(), info1, info2,
 755             Edge(child1), Edge(child2), Edge(child3));
 756         return addToGraph(result);
 757     }
 758     Node* addToGraph(NodeType op, OpInfo info1, OpInfo info2, Edge child1, Edge child2 = Edge(), Edge child3 = Edge())
 759     {
 760         Node* result = m_graph.addNode(
 761             op, currentNodeOrigin(), info1, info2, child1, child2, child3);
 762         return addToGraph(result);
 763     }
 764 
 765     Node* addToGraph(Node::VarArgTag, NodeType op, OpInfo info1, OpInfo info2 = OpInfo())
 766     {
 767         Node* result = m_graph.addNode(
 768             Node::VarArg, op, currentNodeOrigin(), info1, info2,
 769             m_graph.m_varArgChildren.size() - m_numPassedVarArgs, m_numPassedVarArgs);
 770         addToGraph(result);
 771 
 772         m_numPassedVarArgs = 0;
 773 
 774         return result;
 775     }
 776 
 777     void addVarArgChild(Node* child)
 778     {
 779         m_graph.m_varArgChildren.append(Edge(child));
 780         m_numPassedVarArgs++;
 781     }
 782 
 783     void addVarArgChild(Edge child)
 784     {
 785         m_graph.m_varArgChildren.append(child);
 786         m_numPassedVarArgs++;
 787     }
 788 
 789     Node* addCallWithoutSettingResult(
 790         NodeType op, OpInfo opInfo, Node* callee, int argCount, int registerOffset,
 791         OpInfo prediction)
 792     {
 793         addVarArgChild(callee);
 794         size_t parameterSlots = Graph::parameterSlotsForArgCount(argCount);
 795 
 796         if (parameterSlots &gt; m_parameterSlots)
 797             m_parameterSlots = parameterSlots;
 798 
 799         for (int i = 0; i &lt; argCount; ++i)
 800             addVarArgChild(get(virtualRegisterForArgument(i, registerOffset)));
 801 
 802         return addToGraph(Node::VarArg, op, opInfo, prediction);
 803     }
 804 
 805     Node* addCall(
 806         VirtualRegister result, NodeType op, const DOMJIT::Signature* signature, Node* callee, int argCount, int registerOffset,
 807         SpeculatedType prediction)
 808     {
 809         if (op == TailCall) {
 810             if (allInlineFramesAreTailCalls())
 811                 return addCallWithoutSettingResult(op, OpInfo(signature), callee, argCount, registerOffset, OpInfo());
 812             op = TailCallInlinedCaller;
 813         }
 814 
 815 
 816         Node* call = addCallWithoutSettingResult(
 817             op, OpInfo(signature), callee, argCount, registerOffset, OpInfo(prediction));
 818         if (result.isValid())
 819             set(result, call);
 820         return call;
 821     }
 822 
 823     Node* cellConstantWithStructureCheck(JSCell* object, Structure* structure)
 824     {
 825         // FIXME: This should route to emitPropertyCheck, not the other way around. But currently,
 826         // this gets no profit from using emitPropertyCheck() since we&#39;ll non-adaptively watch the
 827         // object&#39;s structure as soon as we make it a weakJSCosntant.
 828         Node* objectNode = weakJSConstant(object);
 829         addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(structure)), objectNode);
 830         return objectNode;
 831     }
 832 
 833     SpeculatedType getPredictionWithoutOSRExit(unsigned bytecodeIndex)
 834     {
 835         SpeculatedType prediction;
 836         {
 837             ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
 838             prediction = m_inlineStackTop-&gt;m_profiledBlock-&gt;valueProfilePredictionForBytecodeOffset(locker, bytecodeIndex);
 839         }
 840 
 841         if (prediction != SpecNone)
 842             return prediction;
 843 
 844         // If we have no information about the values this
 845         // node generates, we check if by any chance it is
 846         // a tail call opcode. In that case, we walk up the
 847         // inline frames to find a call higher in the call
 848         // chain and use its prediction. If we only have
 849         // inlined tail call frames, we use SpecFullTop
 850         // to avoid a spurious OSR exit.
 851         auto instruction = m_inlineStackTop-&gt;m_profiledBlock-&gt;instructions().at(bytecodeIndex);
 852         OpcodeID opcodeID = instruction-&gt;opcodeID();
 853 
 854         switch (opcodeID) {
 855         case op_tail_call:
 856         case op_tail_call_varargs:
 857         case op_tail_call_forward_arguments: {
 858             // Things should be more permissive to us returning BOTTOM instead of TOP here.
 859             // Currently, this will cause us to Force OSR exit. This is bad because returning
 860             // TOP will cause anything that transitively touches this speculated type to
 861             // also become TOP during prediction propagation.
 862             // https://bugs.webkit.org/show_bug.cgi?id=164337
 863             if (!inlineCallFrame())
 864                 return SpecFullTop;
 865 
 866             CodeOrigin* codeOrigin = inlineCallFrame()-&gt;getCallerSkippingTailCalls();
 867             if (!codeOrigin)
 868                 return SpecFullTop;
 869 
 870             InlineStackEntry* stack = m_inlineStackTop;
 871             while (stack-&gt;m_inlineCallFrame != codeOrigin-&gt;inlineCallFrame)
 872                 stack = stack-&gt;m_caller;
 873 
 874             bytecodeIndex = codeOrigin-&gt;bytecodeIndex;
 875             CodeBlock* profiledBlock = stack-&gt;m_profiledBlock;
 876             ConcurrentJSLocker locker(profiledBlock-&gt;m_lock);
 877             return profiledBlock-&gt;valueProfilePredictionForBytecodeOffset(locker, bytecodeIndex);
 878         }
 879 
 880         default:
 881             return SpecNone;
 882         }
 883 
 884         RELEASE_ASSERT_NOT_REACHED();
 885         return SpecNone;
 886     }
 887 
 888     SpeculatedType getPrediction(unsigned bytecodeIndex)
 889     {
 890         SpeculatedType prediction = getPredictionWithoutOSRExit(bytecodeIndex);
 891 
 892         if (prediction == SpecNone) {
 893             // We have no information about what values this node generates. Give up
 894             // on executing this code, since we&#39;re likely to do more damage than good.
 895             addToGraph(ForceOSRExit);
 896         }
 897 
 898         return prediction;
 899     }
 900 
 901     SpeculatedType getPredictionWithoutOSRExit()
 902     {
 903         return getPredictionWithoutOSRExit(m_currentIndex);
 904     }
 905 
 906     SpeculatedType getPrediction()
 907     {
 908         return getPrediction(m_currentIndex);
 909     }
 910 
 911     ArrayMode getArrayMode(Array::Action action)
 912     {
 913         CodeBlock* codeBlock = m_inlineStackTop-&gt;m_profiledBlock;
 914         ArrayProfile* profile = codeBlock-&gt;getArrayProfile(codeBlock-&gt;bytecodeOffset(m_currentInstruction));
 915         return getArrayMode(*profile, action);
 916     }
 917 
 918     ArrayMode getArrayMode(ArrayProfile&amp; profile, Array::Action action)
 919     {
 920         ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
 921         profile.computeUpdatedPrediction(locker, m_inlineStackTop-&gt;m_profiledBlock);
 922         bool makeSafe = profile.outOfBounds(locker);
 923         return ArrayMode::fromObserved(locker, &amp;profile, action, makeSafe);
 924     }
 925 
 926     Node* makeSafe(Node* node)
 927     {
 928         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
 929             node-&gt;mergeFlags(NodeMayOverflowInt32InDFG);
 930         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
 931             node-&gt;mergeFlags(NodeMayNegZeroInDFG);
 932 
 933         if (!isX86() &amp;&amp; node-&gt;op() == ArithMod)
 934             return node;
 935 
 936         {
 937             ArithProfile* arithProfile = m_inlineStackTop-&gt;m_profiledBlock-&gt;arithProfileForBytecodeOffset(m_currentIndex);
 938             if (arithProfile) {
 939                 switch (node-&gt;op()) {
 940                 case ArithAdd:
 941                 case ArithSub:
 942                 case ValueAdd:
 943                     if (arithProfile-&gt;didObserveDouble())
 944                         node-&gt;mergeFlags(NodeMayHaveDoubleResult);
 945                     if (arithProfile-&gt;didObserveNonNumeric())
 946                         node-&gt;mergeFlags(NodeMayHaveNonNumericResult);
 947                     if (arithProfile-&gt;didObserveBigInt())
 948                         node-&gt;mergeFlags(NodeMayHaveBigIntResult);
 949                     break;
 950 
 951                 case ValueMul:
 952                 case ArithMul: {
 953                     if (arithProfile-&gt;didObserveInt52Overflow())
 954                         node-&gt;mergeFlags(NodeMayOverflowInt52);
 955                     if (arithProfile-&gt;didObserveInt32Overflow() || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
 956                         node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline);
 957                     if (arithProfile-&gt;didObserveNegZeroDouble() || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
 958                         node-&gt;mergeFlags(NodeMayNegZeroInBaseline);
 959                     if (arithProfile-&gt;didObserveDouble())
 960                         node-&gt;mergeFlags(NodeMayHaveDoubleResult);
 961                     if (arithProfile-&gt;didObserveNonNumeric())
 962                         node-&gt;mergeFlags(NodeMayHaveNonNumericResult);
 963                     if (arithProfile-&gt;didObserveBigInt())
 964                         node-&gt;mergeFlags(NodeMayHaveBigIntResult);
 965                     break;
 966                 }
 967                 case ValueNegate:
 968                 case ArithNegate: {
 969                     if (arithProfile-&gt;lhsObservedType().sawNumber() || arithProfile-&gt;didObserveDouble())
 970                         node-&gt;mergeFlags(NodeMayHaveDoubleResult);
 971                     if (arithProfile-&gt;didObserveNegZeroDouble() || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
 972                         node-&gt;mergeFlags(NodeMayNegZeroInBaseline);
 973                     if (arithProfile-&gt;didObserveInt32Overflow() || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
 974                         node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline);
 975                     if (arithProfile-&gt;didObserveNonNumeric())
 976                         node-&gt;mergeFlags(NodeMayHaveNonNumericResult);
 977                     if (arithProfile-&gt;didObserveBigInt())
 978                         node-&gt;mergeFlags(NodeMayHaveBigIntResult);
 979                     break;
 980                 }
 981 
 982                 default:
 983                     break;
 984                 }
 985             }
 986         }
 987 
 988         if (m_inlineStackTop-&gt;m_profiledBlock-&gt;likelyToTakeSlowCase(m_currentIndex)) {
 989             switch (node-&gt;op()) {
 990             case UInt32ToNumber:
 991             case ArithAdd:
 992             case ArithSub:
 993             case ValueAdd:
 994             case ArithMod: // for ArithMod &quot;MayOverflow&quot; means we tried to divide by zero, or we saw double.
 995                 node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline);
 996                 break;
 997 
 998             default:
 999                 break;
1000             }
1001         }
1002 
1003         return node;
1004     }
1005 
1006     Node* makeDivSafe(Node* node)
1007     {
1008         ASSERT(node-&gt;op() == ArithDiv || node-&gt;op() == ValueDiv);
1009 
1010         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
1011             node-&gt;mergeFlags(NodeMayOverflowInt32InDFG);
1012         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
1013             node-&gt;mergeFlags(NodeMayNegZeroInDFG);
1014 
1015         // The main slow case counter for op_div in the old JIT counts only when
1016         // the operands are not numbers. We don&#39;t care about that since we already
1017         // have speculations in place that take care of that separately. We only
1018         // care about when the outcome of the division is not an integer, which
1019         // is what the special fast case counter tells us.
1020 
1021         if (!m_inlineStackTop-&gt;m_profiledBlock-&gt;couldTakeSpecialFastCase(m_currentIndex))
1022             return node;
1023 
1024         // FIXME: It might be possible to make this more granular.
1025         node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline | NodeMayNegZeroInBaseline);
1026 
1027         ArithProfile* arithProfile = m_inlineStackTop-&gt;m_profiledBlock-&gt;arithProfileForBytecodeOffset(m_currentIndex);
1028         if (arithProfile-&gt;didObserveBigInt())
1029             node-&gt;mergeFlags(NodeMayHaveBigIntResult);
1030 
1031         return node;
1032     }
1033 
1034     void noticeArgumentsUse()
1035     {
1036         // All of the arguments in this function need to be formatted as JSValues because we will
1037         // load from them in a random-access fashion and we don&#39;t want to have to switch on
1038         // format.
1039 
1040         for (ArgumentPosition* argument : m_inlineStackTop-&gt;m_argumentPositions)
1041             argument-&gt;mergeShouldNeverUnbox(true);
1042     }
1043 
1044     bool needsDynamicLookup(ResolveType, OpcodeID);
1045 
1046     VM* m_vm;
1047     CodeBlock* m_codeBlock;
1048     CodeBlock* m_profiledBlock;
1049     Graph&amp; m_graph;
1050 
1051     // The current block being generated.
1052     BasicBlock* m_currentBlock;
1053     // The bytecode index of the current instruction being generated.
1054     unsigned m_currentIndex;
1055     // The semantic origin of the current node if different from the current Index.
1056     CodeOrigin m_currentSemanticOrigin;
1057     // True if it&#39;s OK to OSR exit right now.
1058     bool m_exitOK { false };
1059 
1060     FrozenValue* m_constantUndefined;
1061     FrozenValue* m_constantNull;
1062     FrozenValue* m_constantNaN;
1063     FrozenValue* m_constantOne;
1064     Vector&lt;Node*, 16&gt; m_constants;
1065 
1066     HashMap&lt;InlineCallFrame*, Vector&lt;ArgumentPosition*&gt;, WTF::DefaultHash&lt;InlineCallFrame*&gt;::Hash, WTF::NullableHashTraits&lt;InlineCallFrame*&gt;&gt; m_inlineCallFrameToArgumentPositions;
1067 
1068     // The number of arguments passed to the function.
1069     unsigned m_numArguments;
1070     // The number of locals (vars + temporaries) used in the function.
1071     unsigned m_numLocals;
1072     // The number of slots (in units of sizeof(Register)) that we need to
1073     // preallocate for arguments to outgoing calls from this frame. This
1074     // number includes the CallFrame slots that we initialize for the callee
1075     // (but not the callee-initialized CallerFrame and ReturnPC slots).
1076     // This number is 0 if and only if this function is a leaf.
1077     unsigned m_parameterSlots;
1078     // The number of var args passed to the next var arg node.
1079     unsigned m_numPassedVarArgs;
1080 
1081     struct InlineStackEntry {
1082         ByteCodeParser* m_byteCodeParser;
1083 
1084         CodeBlock* m_codeBlock;
1085         CodeBlock* m_profiledBlock;
1086         InlineCallFrame* m_inlineCallFrame;
1087 
1088         ScriptExecutable* executable() { return m_codeBlock-&gt;ownerExecutable(); }
1089 
1090         QueryableExitProfile m_exitProfile;
1091 
1092         // Remapping of identifier and constant numbers from the code block being
1093         // inlined (inline callee) to the code block that we&#39;re inlining into
1094         // (the machine code block, which is the transitive, though not necessarily
1095         // direct, caller).
1096         Vector&lt;unsigned&gt; m_identifierRemap;
1097         Vector&lt;unsigned&gt; m_switchRemap;
1098 
1099         // These are blocks whose terminal is a Jump, Branch or Switch, and whose target has not yet been linked.
1100         // Their terminal instead refers to a bytecode index, and the right BB can be found in m_blockLinkingTargets.
1101         Vector&lt;BasicBlock*&gt; m_unlinkedBlocks;
1102 
1103         // Potential block linking targets. Must be sorted by bytecodeBegin, and
1104         // cannot have two blocks that have the same bytecodeBegin.
1105         Vector&lt;BasicBlock*&gt; m_blockLinkingTargets;
1106 
1107         // Optional: a continuation block for returns to jump to. It is set by early returns if it does not exist.
1108         BasicBlock* m_continuationBlock;
1109 
1110         VirtualRegister m_returnValue;
1111 
1112         // Speculations about variable types collected from the profiled code block,
1113         // which are based on OSR exit profiles that past DFG compilations of this
1114         // code block had gathered.
1115         LazyOperandValueProfileParser m_lazyOperands;
1116 
1117         ICStatusMap m_baselineMap;
1118         ICStatusContext m_optimizedContext;
1119 
1120         // Pointers to the argument position trackers for this slice of code.
1121         Vector&lt;ArgumentPosition*&gt; m_argumentPositions;
1122 
1123         InlineStackEntry* m_caller;
1124 
1125         InlineStackEntry(
1126             ByteCodeParser*,
1127             CodeBlock*,
1128             CodeBlock* profiledBlock,
1129             JSFunction* callee, // Null if this is a closure call.
1130             VirtualRegister returnValueVR,
1131             VirtualRegister inlineCallFrameStart,
1132             int argumentCountIncludingThis,
1133             InlineCallFrame::Kind,
1134             BasicBlock* continuationBlock);
1135 
1136         ~InlineStackEntry();
1137 
1138         VirtualRegister remapOperand(VirtualRegister operand) const
1139         {
1140             if (!m_inlineCallFrame)
1141                 return operand;
1142 
1143             ASSERT(!operand.isConstant());
1144 
1145             return VirtualRegister(operand.offset() + m_inlineCallFrame-&gt;stackOffset);
1146         }
1147     };
1148 
1149     InlineStackEntry* m_inlineStackTop;
1150 
1151     ICStatusContextStack m_icContextStack;
1152 
1153     struct DelayedSetLocal {
1154         CodeOrigin m_origin;
1155         VirtualRegister m_operand;
1156         Node* m_value;
1157         SetMode m_setMode;
1158 
1159         DelayedSetLocal() { }
1160         DelayedSetLocal(const CodeOrigin&amp; origin, VirtualRegister operand, Node* value, SetMode setMode)
1161             : m_origin(origin)
1162             , m_operand(operand)
1163             , m_value(value)
1164             , m_setMode(setMode)
1165         {
1166             RELEASE_ASSERT(operand.isValid());
1167         }
1168 
1169         Node* execute(ByteCodeParser* parser)
1170         {
1171             if (m_operand.isArgument())
1172                 return parser-&gt;setArgument(m_origin, m_operand, m_value, m_setMode);
1173             return parser-&gt;setLocal(m_origin, m_operand, m_value, m_setMode);
1174         }
1175     };
1176 
1177     Vector&lt;DelayedSetLocal, 2&gt; m_setLocalQueue;
1178 
1179     const Instruction* m_currentInstruction;
1180     bool m_hasDebuggerEnabled;
1181     bool m_hasAnyForceOSRExits { false };
1182 };
1183 
1184 BasicBlock* ByteCodeParser::allocateTargetableBlock(unsigned bytecodeIndex)
1185 {
1186     ASSERT(bytecodeIndex != UINT_MAX);
1187     Ref&lt;BasicBlock&gt; block = adoptRef(*new BasicBlock(bytecodeIndex, m_numArguments, m_numLocals, 1));
1188     BasicBlock* blockPtr = block.ptr();
1189     // m_blockLinkingTargets must always be sorted in increasing order of bytecodeBegin
1190     if (m_inlineStackTop-&gt;m_blockLinkingTargets.size())
1191         ASSERT(m_inlineStackTop-&gt;m_blockLinkingTargets.last()-&gt;bytecodeBegin &lt; bytecodeIndex);
1192     m_inlineStackTop-&gt;m_blockLinkingTargets.append(blockPtr);
1193     m_graph.appendBlock(WTFMove(block));
1194     return blockPtr;
1195 }
1196 
1197 BasicBlock* ByteCodeParser::allocateUntargetableBlock()
1198 {
1199     Ref&lt;BasicBlock&gt; block = adoptRef(*new BasicBlock(UINT_MAX, m_numArguments, m_numLocals, 1));
1200     BasicBlock* blockPtr = block.ptr();
1201     m_graph.appendBlock(WTFMove(block));
1202     return blockPtr;
1203 }
1204 
1205 void ByteCodeParser::makeBlockTargetable(BasicBlock* block, unsigned bytecodeIndex)
1206 {
1207     RELEASE_ASSERT(block-&gt;bytecodeBegin == UINT_MAX);
1208     block-&gt;bytecodeBegin = bytecodeIndex;
1209     // m_blockLinkingTargets must always be sorted in increasing order of bytecodeBegin
1210     if (m_inlineStackTop-&gt;m_blockLinkingTargets.size())
1211         ASSERT(m_inlineStackTop-&gt;m_blockLinkingTargets.last()-&gt;bytecodeBegin &lt; bytecodeIndex);
1212     m_inlineStackTop-&gt;m_blockLinkingTargets.append(block);
1213 }
1214 
1215 void ByteCodeParser::addJumpTo(BasicBlock* block)
1216 {
1217     ASSERT(!m_currentBlock-&gt;terminal());
1218     Node* jumpNode = addToGraph(Jump);
1219     jumpNode-&gt;targetBlock() = block;
1220     m_currentBlock-&gt;didLink();
1221 }
1222 
1223 void ByteCodeParser::addJumpTo(unsigned bytecodeIndex)
1224 {
1225     ASSERT(!m_currentBlock-&gt;terminal());
1226     addToGraph(Jump, OpInfo(bytecodeIndex));
1227     m_inlineStackTop-&gt;m_unlinkedBlocks.append(m_currentBlock);
1228 }
1229 
1230 template&lt;typename CallOp&gt;
1231 ByteCodeParser::Terminality ByteCodeParser::handleCall(const Instruction* pc, NodeType op, CallMode callMode)
1232 {
1233     auto bytecode = pc-&gt;as&lt;CallOp&gt;();
1234     Node* callTarget = get(bytecode.m_callee);
1235     int registerOffset = -static_cast&lt;int&gt;(bytecode.m_argv);
1236 
1237     CallLinkStatus callLinkStatus = CallLinkStatus::computeFor(
1238         m_inlineStackTop-&gt;m_profiledBlock, currentCodeOrigin(),
1239         m_inlineStackTop-&gt;m_baselineMap, m_icContextStack);
1240 
1241     InlineCallFrame::Kind kind = InlineCallFrame::kindFor(callMode);
1242 
1243     return handleCall(bytecode.m_dst, op, kind, pc-&gt;size(), callTarget,
1244         bytecode.m_argc, registerOffset, callLinkStatus, getPrediction());
1245 }
1246 
1247 void ByteCodeParser::refineStatically(CallLinkStatus&amp; callLinkStatus, Node* callTarget)
1248 {
1249     if (callTarget-&gt;isCellConstant())
1250         callLinkStatus.setProvenConstantCallee(CallVariant(callTarget-&gt;asCell()));
1251 }
1252 
1253 ByteCodeParser::Terminality ByteCodeParser::handleCall(
1254     VirtualRegister result, NodeType op, InlineCallFrame::Kind kind, unsigned instructionSize,
1255     Node* callTarget, int argumentCountIncludingThis, int registerOffset,
1256     CallLinkStatus callLinkStatus, SpeculatedType prediction)
1257 {
1258     ASSERT(registerOffset &lt;= 0);
1259 
1260     refineStatically(callLinkStatus, callTarget);
1261 
1262     VERBOSE_LOG(&quot;    Handling call at &quot;, currentCodeOrigin(), &quot;: &quot;, callLinkStatus, &quot;\n&quot;);
1263 
1264     // If we have profiling information about this call, and it did not behave too polymorphically,
1265     // we may be able to inline it, or in the case of recursive tail calls turn it into a jump.
1266     if (callLinkStatus.canOptimize()) {
1267         addToGraph(FilterCallLinkStatus, OpInfo(m_graph.m_plan.recordedStatuses().addCallLinkStatus(currentCodeOrigin(), callLinkStatus)), callTarget);
1268 
1269         VirtualRegister thisArgument = virtualRegisterForArgument(0, registerOffset);
1270         auto optimizationResult = handleInlining(callTarget, result, callLinkStatus, registerOffset, thisArgument,
1271             argumentCountIncludingThis, m_currentIndex + instructionSize, op, kind, prediction);
1272         if (optimizationResult == CallOptimizationResult::OptimizedToJump)
1273             return Terminal;
1274         if (optimizationResult == CallOptimizationResult::Inlined) {
1275             if (UNLIKELY(m_graph.compilation()))
1276                 m_graph.compilation()-&gt;noticeInlinedCall();
1277             return NonTerminal;
1278         }
1279     }
1280 
1281     Node* callNode = addCall(result, op, nullptr, callTarget, argumentCountIncludingThis, registerOffset, prediction);
1282     ASSERT(callNode-&gt;op() != TailCallVarargs &amp;&amp; callNode-&gt;op() != TailCallForwardVarargs);
1283     return callNode-&gt;op() == TailCall ? Terminal : NonTerminal;
1284 }
1285 
1286 template&lt;typename CallOp&gt;
1287 ByteCodeParser::Terminality ByteCodeParser::handleVarargsCall(const Instruction* pc, NodeType op, CallMode callMode)
1288 {
1289     auto bytecode = pc-&gt;as&lt;CallOp&gt;();
1290     int firstFreeReg = bytecode.m_firstFree.offset();
1291     int firstVarArgOffset = bytecode.m_firstVarArg;
1292 
1293     SpeculatedType prediction = getPrediction();
1294 
1295     Node* callTarget = get(bytecode.m_callee);
1296 
1297     CallLinkStatus callLinkStatus = CallLinkStatus::computeFor(
1298         m_inlineStackTop-&gt;m_profiledBlock, currentCodeOrigin(),
1299         m_inlineStackTop-&gt;m_baselineMap, m_icContextStack);
1300     refineStatically(callLinkStatus, callTarget);
1301 
1302     VERBOSE_LOG(&quot;    Varargs call link status at &quot;, currentCodeOrigin(), &quot;: &quot;, callLinkStatus, &quot;\n&quot;);
1303 
1304     if (callLinkStatus.canOptimize()) {
1305         addToGraph(FilterCallLinkStatus, OpInfo(m_graph.m_plan.recordedStatuses().addCallLinkStatus(currentCodeOrigin(), callLinkStatus)), callTarget);
1306 
1307         if (handleVarargsInlining(callTarget, bytecode.m_dst,
1308             callLinkStatus, firstFreeReg, bytecode.m_thisValue, bytecode.m_arguments,
1309             firstVarArgOffset, op,
1310             InlineCallFrame::varargsKindFor(callMode))) {
1311             if (UNLIKELY(m_graph.compilation()))
1312                 m_graph.compilation()-&gt;noticeInlinedCall();
1313             return NonTerminal;
1314         }
1315     }
1316 
1317     CallVarargsData* data = m_graph.m_callVarargsData.add();
1318     data-&gt;firstVarArgOffset = firstVarArgOffset;
1319 
1320     Node* thisChild = get(bytecode.m_thisValue);
1321     Node* argumentsChild = nullptr;
1322     if (op != TailCallForwardVarargs)
1323         argumentsChild = get(bytecode.m_arguments);
1324 
1325     if (op == TailCallVarargs || op == TailCallForwardVarargs) {
1326         if (allInlineFramesAreTailCalls()) {
1327             addToGraph(op, OpInfo(data), OpInfo(), callTarget, thisChild, argumentsChild);
1328             return Terminal;
1329         }
1330         op = op == TailCallVarargs ? TailCallVarargsInlinedCaller : TailCallForwardVarargsInlinedCaller;
1331     }
1332 
1333     Node* call = addToGraph(op, OpInfo(data), OpInfo(prediction), callTarget, thisChild, argumentsChild);
1334     if (bytecode.m_dst.isValid())
1335         set(bytecode.m_dst, call);
1336     return NonTerminal;
1337 }
1338 
1339 void ByteCodeParser::emitFunctionChecks(CallVariant callee, Node* callTarget, VirtualRegister thisArgumentReg)
1340 {
1341     Node* thisArgument;
1342     if (thisArgumentReg.isValid())
1343         thisArgument = get(thisArgumentReg);
1344     else
1345         thisArgument = nullptr;
1346 
1347     JSCell* calleeCell;
1348     Node* callTargetForCheck;
1349     if (callee.isClosureCall()) {
1350         calleeCell = callee.executable();
1351         callTargetForCheck = addToGraph(GetExecutable, callTarget);
1352     } else {
1353         calleeCell = callee.nonExecutableCallee();
1354         callTargetForCheck = callTarget;
1355     }
1356 
1357     ASSERT(calleeCell);
1358     addToGraph(CheckCell, OpInfo(m_graph.freeze(calleeCell)), callTargetForCheck);
1359     if (thisArgument)
1360         addToGraph(Phantom, thisArgument);
1361 }
1362 
1363 Node* ByteCodeParser::getArgumentCount()
1364 {
1365     Node* argumentCount;
1366     if (m_inlineStackTop-&gt;m_inlineCallFrame &amp;&amp; !m_inlineStackTop-&gt;m_inlineCallFrame-&gt;isVarargs())
1367         argumentCount = jsConstant(m_graph.freeze(jsNumber(m_inlineStackTop-&gt;m_inlineCallFrame-&gt;argumentCountIncludingThis))-&gt;value());
1368     else
1369         argumentCount = addToGraph(GetArgumentCountIncludingThis, OpInfo(m_inlineStackTop-&gt;m_inlineCallFrame), OpInfo(SpecInt32Only));
1370     return argumentCount;
1371 }
1372 
1373 void ByteCodeParser::emitArgumentPhantoms(int registerOffset, int argumentCountIncludingThis)
1374 {
1375     for (int i = 0; i &lt; argumentCountIncludingThis; ++i)
1376         addToGraph(Phantom, get(virtualRegisterForArgument(i, registerOffset)));
1377 }
1378 
1379 template&lt;typename ChecksFunctor&gt;
1380 bool ByteCodeParser::handleRecursiveTailCall(Node* callTargetNode, CallVariant callVariant, int registerOffset, int argumentCountIncludingThis, const ChecksFunctor&amp; emitFunctionCheckIfNeeded)
1381 {
1382     if (UNLIKELY(!Options::optimizeRecursiveTailCalls()))
1383         return false;
1384 
1385     auto targetExecutable = callVariant.executable();
1386     InlineStackEntry* stackEntry = m_inlineStackTop;
1387     do {
1388         if (targetExecutable != stackEntry-&gt;executable())
1389             continue;
1390         VERBOSE_LOG(&quot;   We found a recursive tail call, trying to optimize it into a jump.\n&quot;);
1391 
1392         if (auto* callFrame = stackEntry-&gt;m_inlineCallFrame) {
1393             // Some code may statically use the argument count from the InlineCallFrame, so it would be invalid to loop back if it does not match.
1394             // We &quot;continue&quot; instead of returning false in case another stack entry further on the stack has the right number of arguments.
1395             if (argumentCountIncludingThis != static_cast&lt;int&gt;(callFrame-&gt;argumentCountIncludingThis))
1396                 continue;
1397         } else {
1398             // We are in the machine code entry (i.e. the original caller).
1399             // If we have more arguments than the number of parameters to the function, it is not clear where we could put them on the stack.
1400             if (argumentCountIncludingThis &gt; m_codeBlock-&gt;numParameters())
1401                 return false;
1402         }
1403 
1404         // If an InlineCallFrame is not a closure, it was optimized using a constant callee.
1405         // Check if this is the same callee that we try to inline here.
1406         if (stackEntry-&gt;m_inlineCallFrame &amp;&amp; !stackEntry-&gt;m_inlineCallFrame-&gt;isClosureCall) {
1407             if (stackEntry-&gt;m_inlineCallFrame-&gt;calleeConstant() != callVariant.function())
1408                 continue;
1409         }
1410 
1411         // We must add some check that the profiling information was correct and the target of this call is what we thought.
1412         emitFunctionCheckIfNeeded();
1413         // We flush everything, as if we were in the backedge of a loop (see treatment of op_jmp in parseBlock).
1414         flushForTerminal();
1415 
1416         // We must set the callee to the right value
1417         if (stackEntry-&gt;m_inlineCallFrame) {
1418             if (stackEntry-&gt;m_inlineCallFrame-&gt;isClosureCall)
1419                 setDirect(stackEntry-&gt;remapOperand(VirtualRegister(CallFrameSlot::callee)), callTargetNode, NormalSet);
1420         } else
1421             addToGraph(SetCallee, callTargetNode);
1422 
1423         // We must set the arguments to the right values
1424         if (!stackEntry-&gt;m_inlineCallFrame)
1425             addToGraph(SetArgumentCountIncludingThis, OpInfo(argumentCountIncludingThis));
1426         int argIndex = 0;
1427         for (; argIndex &lt; argumentCountIncludingThis; ++argIndex) {
1428             Node* value = get(virtualRegisterForArgument(argIndex, registerOffset));
1429             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForArgument(argIndex)), value, NormalSet);
1430         }
1431         Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
1432         for (; argIndex &lt; stackEntry-&gt;m_codeBlock-&gt;numParameters(); ++argIndex)
1433             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForArgument(argIndex)), undefined, NormalSet);
1434 
1435         // We must repeat the work of op_enter here as we will jump right after it.
1436         // We jump right after it and not before it, because of some invariant saying that a CFG root cannot have predecessors in the IR.
1437         for (int i = 0; i &lt; stackEntry-&gt;m_codeBlock-&gt;numVars(); ++i)
1438             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForLocal(i)), undefined, NormalSet);
1439 
1440         // We want to emit the SetLocals with an exit origin that points to the place we are jumping to.
1441         unsigned oldIndex = m_currentIndex;
1442         auto oldStackTop = m_inlineStackTop;
1443         m_inlineStackTop = stackEntry;
1444         m_currentIndex = opcodeLengths[op_enter];
1445         m_exitOK = true;
1446         processSetLocalQueue();
1447         m_currentIndex = oldIndex;
1448         m_inlineStackTop = oldStackTop;
1449         m_exitOK = false;
1450 
1451         BasicBlock** entryBlockPtr = tryBinarySearch&lt;BasicBlock*, unsigned&gt;(stackEntry-&gt;m_blockLinkingTargets, stackEntry-&gt;m_blockLinkingTargets.size(), opcodeLengths[op_enter], getBytecodeBeginForBlock);
1452         RELEASE_ASSERT(entryBlockPtr);
1453         addJumpTo(*entryBlockPtr);
1454         return true;
1455         // It would be unsound to jump over a non-tail call: the &quot;tail&quot; call is not really a tail call in that case.
1456     } while (stackEntry-&gt;m_inlineCallFrame &amp;&amp; stackEntry-&gt;m_inlineCallFrame-&gt;kind == InlineCallFrame::TailCall &amp;&amp; (stackEntry = stackEntry-&gt;m_caller));
1457 
1458     // The tail call was not recursive
1459     return false;
1460 }
1461 
1462 unsigned ByteCodeParser::inliningCost(CallVariant callee, int argumentCountIncludingThis, InlineCallFrame::Kind kind)
1463 {
1464     CallMode callMode = InlineCallFrame::callModeFor(kind);
1465     CodeSpecializationKind specializationKind = specializationKindFor(callMode);
1466     VERBOSE_LOG(&quot;Considering inlining &quot;, callee, &quot; into &quot;, currentCodeOrigin(), &quot;\n&quot;);
1467 
1468     if (m_hasDebuggerEnabled) {
1469         VERBOSE_LOG(&quot;    Failing because the debugger is in use.\n&quot;);
1470         return UINT_MAX;
1471     }
1472 
1473     FunctionExecutable* executable = callee.functionExecutable();
1474     if (!executable) {
1475         VERBOSE_LOG(&quot;    Failing because there is no function executable.\n&quot;);
1476         return UINT_MAX;
1477     }
1478 
1479     // Do we have a code block, and does the code block&#39;s size match the heuristics/requirements for
1480     // being an inline candidate? We might not have a code block (1) if code was thrown away,
1481     // (2) if we simply hadn&#39;t actually made this call yet or (3) code is a builtin function and
1482     // specialization kind is construct. In the former 2 cases, we could still theoretically attempt
1483     // to inline it if we had a static proof of what was being called; this might happen for example
1484     // if you call a global function, where watchpointing gives us static information. Overall,
1485     // it&#39;s a rare case because we expect that any hot callees would have already been compiled.
1486     CodeBlock* codeBlock = executable-&gt;baselineCodeBlockFor(specializationKind);
1487     if (!codeBlock) {
1488         VERBOSE_LOG(&quot;    Failing because no code block available.\n&quot;);
1489         return UINT_MAX;
1490     }
1491 
1492     if (!Options::useArityFixupInlining()) {
1493         if (codeBlock-&gt;numParameters() &gt; argumentCountIncludingThis) {
1494             VERBOSE_LOG(&quot;    Failing because of arity mismatch.\n&quot;);
1495             return UINT_MAX;
1496         }
1497     }
1498 
1499     CapabilityLevel capabilityLevel = inlineFunctionForCapabilityLevel(
1500         codeBlock, specializationKind, callee.isClosureCall());
1501     VERBOSE_LOG(&quot;    Call mode: &quot;, callMode, &quot;\n&quot;);
1502     VERBOSE_LOG(&quot;    Is closure call: &quot;, callee.isClosureCall(), &quot;\n&quot;);
1503     VERBOSE_LOG(&quot;    Capability level: &quot;, capabilityLevel, &quot;\n&quot;);
1504     VERBOSE_LOG(&quot;    Might inline function: &quot;, mightInlineFunctionFor(codeBlock, specializationKind), &quot;\n&quot;);
1505     VERBOSE_LOG(&quot;    Might compile function: &quot;, mightCompileFunctionFor(codeBlock, specializationKind), &quot;\n&quot;);
1506     VERBOSE_LOG(&quot;    Is supported for inlining: &quot;, isSupportedForInlining(codeBlock), &quot;\n&quot;);
1507     VERBOSE_LOG(&quot;    Is inlining candidate: &quot;, codeBlock-&gt;ownerExecutable()-&gt;isInliningCandidate(), &quot;\n&quot;);
1508     if (!canInline(capabilityLevel)) {
1509         VERBOSE_LOG(&quot;    Failing because the function is not inlineable.\n&quot;);
1510         return UINT_MAX;
1511     }
1512 
1513     // Check if the caller is already too large. We do this check here because that&#39;s just
1514     // where we happen to also have the callee&#39;s code block, and we want that for the
1515     // purpose of unsetting SABI.
1516     if (!isSmallEnoughToInlineCodeInto(m_codeBlock)) {
1517         codeBlock-&gt;m_shouldAlwaysBeInlined = false;
1518         VERBOSE_LOG(&quot;    Failing because the caller is too large.\n&quot;);
1519         return UINT_MAX;
1520     }
1521 
1522     // FIXME: this should be better at predicting how much bloat we will introduce by inlining
1523     // this function.
1524     // https://bugs.webkit.org/show_bug.cgi?id=127627
1525 
1526     // FIXME: We currently inline functions that have run in LLInt but not in Baseline. These
1527     // functions have very low fidelity profiling, and presumably they weren&#39;t very hot if they
1528     // haven&#39;t gotten to Baseline yet. Consider not inlining these functions.
1529     // https://bugs.webkit.org/show_bug.cgi?id=145503
1530 
1531     // Have we exceeded inline stack depth, or are we trying to inline a recursive call to
1532     // too many levels? If either of these are detected, then don&#39;t inline. We adjust our
1533     // heuristics if we are dealing with a function that cannot otherwise be compiled.
1534 
1535     unsigned depth = 0;
1536     unsigned recursion = 0;
1537 
1538     for (InlineStackEntry* entry = m_inlineStackTop; entry; entry = entry-&gt;m_caller) {
1539         ++depth;
1540         if (depth &gt;= Options::maximumInliningDepth()) {
1541             VERBOSE_LOG(&quot;    Failing because depth exceeded.\n&quot;);
1542             return UINT_MAX;
1543         }
1544 
1545         if (entry-&gt;executable() == executable) {
1546             ++recursion;
1547             if (recursion &gt;= Options::maximumInliningRecursion()) {
1548                 VERBOSE_LOG(&quot;    Failing because recursion detected.\n&quot;);
1549                 return UINT_MAX;
1550             }
1551         }
1552     }
1553 
1554     VERBOSE_LOG(&quot;    Inlining should be possible.\n&quot;);
1555 
1556     // It might be possible to inline.
1557     return codeBlock-&gt;instructionCount();
1558 }
1559 
1560 template&lt;typename ChecksFunctor&gt;
1561 void ByteCodeParser::inlineCall(Node* callTargetNode, VirtualRegister result, CallVariant callee, int registerOffset, int argumentCountIncludingThis, InlineCallFrame::Kind kind, BasicBlock* continuationBlock, const ChecksFunctor&amp; insertChecks)
1562 {
1563     const Instruction* savedCurrentInstruction = m_currentInstruction;
1564     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1565 
1566     ASSERT(inliningCost(callee, argumentCountIncludingThis, kind) != UINT_MAX);
1567 
1568     CodeBlock* codeBlock = callee.functionExecutable()-&gt;baselineCodeBlockFor(specializationKind);
1569     insertChecks(codeBlock);
1570 
1571     // FIXME: Don&#39;t flush constants!
1572 
1573     // arityFixupCount and numberOfStackPaddingSlots are different. While arityFixupCount does not consider about stack alignment,
1574     // numberOfStackPaddingSlots consider alignment. Consider the following case,
1575     //
1576     // before: [ ... ][arg0][header]
1577     // after:  [ ... ][ext ][arg1][arg0][header]
1578     //
1579     // In the above case, arityFixupCount is 1. But numberOfStackPaddingSlots is 2 because the stack needs to be aligned.
1580     // We insert extra slots to align stack.
1581     int arityFixupCount = std::max&lt;int&gt;(codeBlock-&gt;numParameters() - argumentCountIncludingThis, 0);
1582     int numberOfStackPaddingSlots = CommonSlowPaths::numberOfStackPaddingSlots(codeBlock, argumentCountIncludingThis);
1583     ASSERT(!(numberOfStackPaddingSlots % stackAlignmentRegisters()));
1584     int registerOffsetAfterFixup = registerOffset - numberOfStackPaddingSlots;
1585 
1586     int inlineCallFrameStart = m_inlineStackTop-&gt;remapOperand(VirtualRegister(registerOffsetAfterFixup)).offset() + CallFrame::headerSizeInRegisters;
1587 
1588     ensureLocals(
1589         VirtualRegister(inlineCallFrameStart).toLocal() + 1 +
1590         CallFrame::headerSizeInRegisters + codeBlock-&gt;numCalleeLocals());
1591 
1592     size_t argumentPositionStart = m_graph.m_argumentPositions.size();
1593 
1594     if (result.isValid())
1595         result = m_inlineStackTop-&gt;remapOperand(result);
1596 
1597     VariableAccessData* calleeVariable = nullptr;
1598     if (callee.isClosureCall()) {
1599         Node* calleeSet = set(
1600             VirtualRegister(registerOffsetAfterFixup + CallFrameSlot::callee), callTargetNode, ImmediateNakedSet);
1601 
1602         calleeVariable = calleeSet-&gt;variableAccessData();
1603         calleeVariable-&gt;mergeShouldNeverUnbox(true);
1604     }
1605 
1606     if (arityFixupCount) {
1607         // Note: we do arity fixup in two phases:
1608         // 1. We get all the values we need and MovHint them to the expected locals.
1609         // 2. We SetLocal them inside the callee&#39;s CodeOrigin. This way, if we exit, the callee&#39;s
1610         //    frame is already set up. If any SetLocal exits, we have a valid exit state.
1611         //    This is required because if we didn&#39;t do this in two phases, we may exit in
1612         //    the middle of arity fixup from the caller&#39;s CodeOrigin. This is unsound because if
1613         //    we did the SetLocals in the caller&#39;s frame, the memcpy may clobber needed parts
1614         //    of the frame right before exiting. For example, consider if we need to pad two args:
1615         //    [arg3][arg2][arg1][arg0]
1616         //    [fix ][fix ][arg3][arg2][arg1][arg0]
1617         //    We memcpy starting from arg0 in the direction of arg3. If we were to exit at a type check
1618         //    for arg3&#39;s SetLocal in the caller&#39;s CodeOrigin, we&#39;d exit with a frame like so:
1619         //    [arg3][arg2][arg1][arg2][arg1][arg0]
1620         //    And the caller would then just end up thinking its argument are:
1621         //    [arg3][arg2][arg1][arg2]
1622         //    which is incorrect.
1623 
1624         Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
1625         // The stack needs to be aligned due to the JS calling convention. Thus, we have a hole if the count of arguments is not aligned.
1626         // We call this hole &quot;extra slot&quot;. Consider the following case, the number of arguments is 2. If this argument
1627         // count does not fulfill the stack alignment requirement, we already inserted extra slots.
1628         //
1629         // before: [ ... ][ext ][arg1][arg0][header]
1630         //
1631         // In the above case, one extra slot is inserted. If the code&#39;s parameter count is 3, we will fixup arguments.
1632         // At that time, we can simply use this extra slots. So the fixuped stack is the following.
1633         //
1634         // before: [ ... ][ext ][arg1][arg0][header]
1635         // after:  [ ... ][arg2][arg1][arg0][header]
1636         //
1637         // In such cases, we do not need to move frames.
1638         if (registerOffsetAfterFixup != registerOffset) {
1639             for (int index = 0; index &lt; argumentCountIncludingThis; ++index) {
1640                 Node* value = get(virtualRegisterForArgument(index, registerOffset));
1641                 VirtualRegister argumentToSet = m_inlineStackTop-&gt;remapOperand(virtualRegisterForArgument(index, registerOffsetAfterFixup));
1642                 addToGraph(MovHint, OpInfo(argumentToSet.offset()), value);
1643                 m_setLocalQueue.append(DelayedSetLocal { currentCodeOrigin(), argumentToSet, value, ImmediateNakedSet });
1644             }
1645         }
1646         for (int index = 0; index &lt; arityFixupCount; ++index) {
1647             VirtualRegister argumentToSet = m_inlineStackTop-&gt;remapOperand(virtualRegisterForArgument(argumentCountIncludingThis + index, registerOffsetAfterFixup));
1648             addToGraph(MovHint, OpInfo(argumentToSet.offset()), undefined);
1649             m_setLocalQueue.append(DelayedSetLocal { currentCodeOrigin(), argumentToSet, undefined, ImmediateNakedSet });
1650         }
1651 
1652         // At this point, it&#39;s OK to OSR exit because we finished setting up
1653         // our callee&#39;s frame. We emit an ExitOK below from the callee&#39;s CodeOrigin.
1654     }
1655 
1656     InlineStackEntry inlineStackEntry(this, codeBlock, codeBlock, callee.function(), result,
1657         (VirtualRegister)inlineCallFrameStart, argumentCountIncludingThis, kind, continuationBlock);
1658 
1659     // This is where the actual inlining really happens.
1660     unsigned oldIndex = m_currentIndex;
1661     m_currentIndex = 0;
1662 
1663     // At this point, it&#39;s again OK to OSR exit.
1664     m_exitOK = true;
1665     addToGraph(ExitOK);
1666 
1667     processSetLocalQueue();
1668 
1669     InlineVariableData inlineVariableData;
1670     inlineVariableData.inlineCallFrame = m_inlineStackTop-&gt;m_inlineCallFrame;
1671     inlineVariableData.argumentPositionStart = argumentPositionStart;
1672     inlineVariableData.calleeVariable = 0;
1673 
1674     RELEASE_ASSERT(
1675         m_inlineStackTop-&gt;m_inlineCallFrame-&gt;isClosureCall
1676         == callee.isClosureCall());
1677     if (callee.isClosureCall()) {
1678         RELEASE_ASSERT(calleeVariable);
1679         inlineVariableData.calleeVariable = calleeVariable;
1680     }
1681 
1682     m_graph.m_inlineVariableData.append(inlineVariableData);
1683 
1684     parseCodeBlock();
1685     clearCaches(); // Reset our state now that we&#39;re back to the outer code.
1686 
1687     m_currentIndex = oldIndex;
1688     m_exitOK = false;
1689 
1690     linkBlocks(inlineStackEntry.m_unlinkedBlocks, inlineStackEntry.m_blockLinkingTargets);
1691 
1692     // Most functions have at least one op_ret and thus set up the continuation block.
1693     // In some rare cases, a function ends in op_unreachable, forcing us to allocate a new continuationBlock here.
1694     if (inlineStackEntry.m_continuationBlock)
1695         m_currentBlock = inlineStackEntry.m_continuationBlock;
1696     else
1697         m_currentBlock = allocateUntargetableBlock();
1698     ASSERT(!m_currentBlock-&gt;terminal());
1699 
1700     prepareToParseBlock();
1701     m_currentInstruction = savedCurrentInstruction;
1702 }
1703 
1704 ByteCodeParser::CallOptimizationResult ByteCodeParser::handleCallVariant(Node* callTargetNode, VirtualRegister result, CallVariant callee, int registerOffset, VirtualRegister thisArgument, int argumentCountIncludingThis, unsigned nextOffset, InlineCallFrame::Kind kind, SpeculatedType prediction, unsigned&amp; inliningBalance, BasicBlock* continuationBlock, bool needsToCheckCallee)
1705 {
1706     VERBOSE_LOG(&quot;    Considering callee &quot;, callee, &quot;\n&quot;);
1707 
1708     bool didInsertChecks = false;
1709     auto insertChecksWithAccounting = [&amp;] () {
1710         if (needsToCheckCallee)
1711             emitFunctionChecks(callee, callTargetNode, thisArgument);
1712         didInsertChecks = true;
1713     };
1714 
1715     if (kind == InlineCallFrame::TailCall &amp;&amp; ByteCodeParser::handleRecursiveTailCall(callTargetNode, callee, registerOffset, argumentCountIncludingThis, insertChecksWithAccounting)) {
1716         RELEASE_ASSERT(didInsertChecks);
1717         return CallOptimizationResult::OptimizedToJump;
1718     }
1719     RELEASE_ASSERT(!didInsertChecks);
1720 
1721     if (!inliningBalance)
1722         return CallOptimizationResult::DidNothing;
1723 
1724     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1725 
1726     auto endSpecialCase = [&amp;] () {
1727         RELEASE_ASSERT(didInsertChecks);
1728         addToGraph(Phantom, callTargetNode);
1729         emitArgumentPhantoms(registerOffset, argumentCountIncludingThis);
1730         inliningBalance--;
1731         if (continuationBlock) {
1732             m_currentIndex = nextOffset;
1733             m_exitOK = true;
1734             processSetLocalQueue();
1735             addJumpTo(continuationBlock);
1736         }
1737     };
1738 
1739     if (InternalFunction* function = callee.internalFunction()) {
1740         if (handleConstantInternalFunction(callTargetNode, result, function, registerOffset, argumentCountIncludingThis, specializationKind, prediction, insertChecksWithAccounting)) {
1741             endSpecialCase();
1742             return CallOptimizationResult::Inlined;
1743         }
1744         RELEASE_ASSERT(!didInsertChecks);
1745         return CallOptimizationResult::DidNothing;
1746     }
1747 
1748     Intrinsic intrinsic = callee.intrinsicFor(specializationKind);
1749     if (intrinsic != NoIntrinsic) {
1750         if (handleIntrinsicCall(callTargetNode, result, intrinsic, registerOffset, argumentCountIncludingThis, prediction, insertChecksWithAccounting)) {
1751             endSpecialCase();
1752             return CallOptimizationResult::Inlined;
1753         }
1754         RELEASE_ASSERT(!didInsertChecks);
1755         // We might still try to inline the Intrinsic because it might be a builtin JS function.
1756     }
1757 
1758     if (Options::useDOMJIT()) {
1759         if (const DOMJIT::Signature* signature = callee.signatureFor(specializationKind)) {
1760             if (handleDOMJITCall(callTargetNode, result, signature, registerOffset, argumentCountIncludingThis, prediction, insertChecksWithAccounting)) {
1761                 endSpecialCase();
1762                 return CallOptimizationResult::Inlined;
1763             }
1764             RELEASE_ASSERT(!didInsertChecks);
1765         }
1766     }
1767 
1768     unsigned myInliningCost = inliningCost(callee, argumentCountIncludingThis, kind);
1769     if (myInliningCost &gt; inliningBalance)
1770         return CallOptimizationResult::DidNothing;
1771 
1772     auto insertCheck = [&amp;] (CodeBlock*) {
1773         if (needsToCheckCallee)
1774             emitFunctionChecks(callee, callTargetNode, thisArgument);
1775     };
1776     inlineCall(callTargetNode, result, callee, registerOffset, argumentCountIncludingThis, kind, continuationBlock, insertCheck);
1777     inliningBalance -= myInliningCost;
1778     return CallOptimizationResult::Inlined;
1779 }
1780 
1781 bool ByteCodeParser::handleVarargsInlining(Node* callTargetNode, VirtualRegister result,
1782     const CallLinkStatus&amp; callLinkStatus, int firstFreeReg, VirtualRegister thisArgument,
1783     VirtualRegister argumentsArgument, unsigned argumentsOffset,
1784     NodeType callOp, InlineCallFrame::Kind kind)
1785 {
1786     VERBOSE_LOG(&quot;Handling inlining (Varargs)...\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1787     if (callLinkStatus.maxNumArguments() &gt; Options::maximumVarargsForInlining()) {
1788         VERBOSE_LOG(&quot;Bailing inlining: too many arguments for varargs inlining.\n&quot;);
1789         return false;
1790     }
1791     if (callLinkStatus.couldTakeSlowPath() || callLinkStatus.size() != 1) {
1792         VERBOSE_LOG(&quot;Bailing inlining: polymorphic inlining is not yet supported for varargs.\n&quot;);
1793         return false;
1794     }
1795 
1796     CallVariant callVariant = callLinkStatus[0];
1797 
1798     unsigned mandatoryMinimum;
1799     if (FunctionExecutable* functionExecutable = callVariant.functionExecutable())
1800         mandatoryMinimum = functionExecutable-&gt;parameterCount();
1801     else
1802         mandatoryMinimum = 0;
1803 
1804     // includes &quot;this&quot;
1805     unsigned maxNumArguments = std::max(callLinkStatus.maxNumArguments(), mandatoryMinimum + 1);
1806 
1807     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1808     if (inliningCost(callVariant, maxNumArguments, kind) &gt; getInliningBalance(callLinkStatus, specializationKind)) {
1809         VERBOSE_LOG(&quot;Bailing inlining: inlining cost too high.\n&quot;);
1810         return false;
1811     }
1812 
1813     int registerOffset = firstFreeReg + 1;
1814     registerOffset -= maxNumArguments; // includes &quot;this&quot;
1815     registerOffset -= CallFrame::headerSizeInRegisters;
1816     registerOffset = -WTF::roundUpToMultipleOf(stackAlignmentRegisters(), -registerOffset);
1817 
1818     auto insertChecks = [&amp;] (CodeBlock* codeBlock) {
1819         emitFunctionChecks(callVariant, callTargetNode, thisArgument);
1820 
1821         int remappedRegisterOffset =
1822         m_inlineStackTop-&gt;remapOperand(VirtualRegister(registerOffset)).offset();
1823 
1824         ensureLocals(VirtualRegister(remappedRegisterOffset).toLocal());
1825 
1826         int argumentStart = registerOffset + CallFrame::headerSizeInRegisters;
1827         int remappedArgumentStart =
1828         m_inlineStackTop-&gt;remapOperand(VirtualRegister(argumentStart)).offset();
1829 
1830         LoadVarargsData* data = m_graph.m_loadVarargsData.add();
1831         data-&gt;start = VirtualRegister(remappedArgumentStart + 1);
1832         data-&gt;count = VirtualRegister(remappedRegisterOffset + CallFrameSlot::argumentCount);
1833         data-&gt;offset = argumentsOffset;
1834         data-&gt;limit = maxNumArguments;
1835         data-&gt;mandatoryMinimum = mandatoryMinimum;
1836 
1837         if (callOp == TailCallForwardVarargs)
1838             addToGraph(ForwardVarargs, OpInfo(data));
1839         else
1840             addToGraph(LoadVarargs, OpInfo(data), get(argumentsArgument));
1841 
1842         // LoadVarargs may OSR exit. Hence, we need to keep alive callTargetNode, thisArgument
1843         // and argumentsArgument for the baseline JIT. However, we only need a Phantom for
1844         // callTargetNode because the other 2 are still in use and alive at this point.
1845         addToGraph(Phantom, callTargetNode);
1846 
1847         // In DFG IR before SSA, we cannot insert control flow between after the
1848         // LoadVarargs and the last SetArgument. This isn&#39;t a problem once we get to DFG
1849         // SSA. Fortunately, we also have other reasons for not inserting control flow
1850         // before SSA.
1851 
1852         VariableAccessData* countVariable = newVariableAccessData(VirtualRegister(remappedRegisterOffset + CallFrameSlot::argumentCount));
1853         // This is pretty lame, but it will force the count to be flushed as an int. This doesn&#39;t
1854         // matter very much, since our use of a SetArgument and Flushes for this local slot is
1855         // mostly just a formality.
1856         countVariable-&gt;predict(SpecInt32Only);
1857         countVariable-&gt;mergeIsProfitableToUnbox(true);
1858         Node* setArgumentCount = addToGraph(SetArgument, OpInfo(countVariable));
1859         m_currentBlock-&gt;variablesAtTail.setOperand(countVariable-&gt;local(), setArgumentCount);
1860 
1861         set(VirtualRegister(argumentStart), get(thisArgument), ImmediateNakedSet);
1862         for (unsigned argument = 1; argument &lt; maxNumArguments; ++argument) {
1863             VariableAccessData* variable = newVariableAccessData(VirtualRegister(remappedArgumentStart + argument));
1864             variable-&gt;mergeShouldNeverUnbox(true); // We currently have nowhere to put the type check on the LoadVarargs. LoadVarargs is effectful, so after it finishes, we cannot exit.
1865 
1866             // For a while it had been my intention to do things like this inside the
1867             // prediction injection phase. But in this case it&#39;s really best to do it here,
1868             // because it&#39;s here that we have access to the variable access datas for the
1869             // inlining we&#39;re about to do.
1870             //
1871             // Something else that&#39;s interesting here is that we&#39;d really love to get
1872             // predictions from the arguments loaded at the callsite, rather than the
1873             // arguments received inside the callee. But that probably won&#39;t matter for most
1874             // calls.
1875             if (codeBlock &amp;&amp; argument &lt; static_cast&lt;unsigned&gt;(codeBlock-&gt;numParameters())) {
1876                 ConcurrentJSLocker locker(codeBlock-&gt;m_lock);
1877                 ValueProfile&amp; profile = codeBlock-&gt;valueProfileForArgument(argument);
1878                 variable-&gt;predict(profile.computeUpdatedPrediction(locker));
1879             }
1880 
1881             Node* setArgument = addToGraph(SetArgument, OpInfo(variable));
1882             m_currentBlock-&gt;variablesAtTail.setOperand(variable-&gt;local(), setArgument);
1883         }
1884     };
1885 
1886     // Intrinsics and internal functions can only be inlined if we&#39;re not doing varargs. This is because
1887     // we currently don&#39;t have any way of getting profiling information for arguments to non-JS varargs
1888     // calls. The prediction propagator won&#39;t be of any help because LoadVarargs obscures the data flow,
1889     // and there are no callsite value profiles and native function won&#39;t have callee value profiles for
1890     // those arguments. Even worse, if the intrinsic decides to exit, it won&#39;t really have anywhere to
1891     // exit to: LoadVarargs is effectful and it&#39;s part of the op_call_varargs, so we can&#39;t exit without
1892     // calling LoadVarargs twice.
1893     inlineCall(callTargetNode, result, callVariant, registerOffset, maxNumArguments, kind, nullptr, insertChecks);
1894 
1895     VERBOSE_LOG(&quot;Successful inlining (varargs, monomorphic).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1896     return true;
1897 }
1898 
1899 unsigned ByteCodeParser::getInliningBalance(const CallLinkStatus&amp; callLinkStatus, CodeSpecializationKind specializationKind)
1900 {
1901     unsigned inliningBalance = Options::maximumFunctionForCallInlineCandidateInstructionCount();
1902     if (specializationKind == CodeForConstruct)
1903         inliningBalance = std::min(inliningBalance, Options::maximumFunctionForConstructInlineCandidateInstructionCount());
1904     if (callLinkStatus.isClosureCall())
1905         inliningBalance = std::min(inliningBalance, Options::maximumFunctionForClosureCallInlineCandidateInstructionCount());
1906     return inliningBalance;
1907 }
1908 
1909 ByteCodeParser::CallOptimizationResult ByteCodeParser::handleInlining(
1910     Node* callTargetNode, VirtualRegister result, const CallLinkStatus&amp; callLinkStatus,
1911     int registerOffset, VirtualRegister thisArgument,
1912     int argumentCountIncludingThis,
1913     unsigned nextOffset, NodeType callOp, InlineCallFrame::Kind kind, SpeculatedType prediction)
1914 {
1915     VERBOSE_LOG(&quot;Handling inlining...\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1916 
1917     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1918     unsigned inliningBalance = getInliningBalance(callLinkStatus, specializationKind);
1919 
1920     // First check if we can avoid creating control flow. Our inliner does some CFG
1921     // simplification on the fly and this helps reduce compile times, but we can only leverage
1922     // this in cases where we don&#39;t need control flow diamonds to check the callee.
1923     if (!callLinkStatus.couldTakeSlowPath() &amp;&amp; callLinkStatus.size() == 1) {
1924         return handleCallVariant(
1925             callTargetNode, result, callLinkStatus[0], registerOffset, thisArgument,
1926             argumentCountIncludingThis, nextOffset, kind, prediction, inliningBalance, nullptr, true);
1927     }
1928 
1929     // We need to create some kind of switch over callee. For now we only do this if we believe that
1930     // we&#39;re in the top tier. We have two reasons for this: first, it provides us an opportunity to
1931     // do more detailed polyvariant/polymorphic profiling; and second, it reduces compile times in
1932     // the DFG. And by polyvariant profiling we mean polyvariant profiling of *this* call. Note that
1933     // we could improve that aspect of this by doing polymorphic inlining but having the profiling
1934     // also.
1935     if (!m_graph.m_plan.isFTL() || !Options::usePolymorphicCallInlining()) {
1936         VERBOSE_LOG(&quot;Bailing inlining (hard).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1937         return CallOptimizationResult::DidNothing;
1938     }
1939 
1940     // If the claim is that this did not originate from a stub, then we don&#39;t want to emit a switch
1941     // statement. Whenever the non-stub profiling says that it could take slow path, it really means that
1942     // it has no idea.
1943     if (!Options::usePolymorphicCallInliningForNonStubStatus()
1944         &amp;&amp; !callLinkStatus.isBasedOnStub()) {
1945         VERBOSE_LOG(&quot;Bailing inlining (non-stub polymorphism).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1946         return CallOptimizationResult::DidNothing;
1947     }
1948 
1949     bool allAreClosureCalls = true;
1950     bool allAreDirectCalls = true;
1951     for (unsigned i = callLinkStatus.size(); i--;) {
1952         if (callLinkStatus[i].isClosureCall())
1953             allAreDirectCalls = false;
1954         else
1955             allAreClosureCalls = false;
1956     }
1957 
1958     Node* thingToSwitchOn;
1959     if (allAreDirectCalls)
1960         thingToSwitchOn = callTargetNode;
1961     else if (allAreClosureCalls)
1962         thingToSwitchOn = addToGraph(GetExecutable, callTargetNode);
1963     else {
1964         // FIXME: We should be able to handle this case, but it&#39;s tricky and we don&#39;t know of cases
1965         // where it would be beneficial. It might be best to handle these cases as if all calls were
1966         // closure calls.
1967         // https://bugs.webkit.org/show_bug.cgi?id=136020
1968         VERBOSE_LOG(&quot;Bailing inlining (mix).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1969         return CallOptimizationResult::DidNothing;
1970     }
1971 
1972     VERBOSE_LOG(&quot;Doing hard inlining...\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1973 
1974     // This makes me wish that we were in SSA all the time. We need to pick a variable into which to
1975     // store the callee so that it will be accessible to all of the blocks we&#39;re about to create. We
1976     // get away with doing an immediate-set here because we wouldn&#39;t have performed any side effects
1977     // yet.
1978     VERBOSE_LOG(&quot;Register offset: &quot;, registerOffset);
1979     VirtualRegister calleeReg(registerOffset + CallFrameSlot::callee);
1980     calleeReg = m_inlineStackTop-&gt;remapOperand(calleeReg);
1981     VERBOSE_LOG(&quot;Callee is going to be &quot;, calleeReg, &quot;\n&quot;);
1982     setDirect(calleeReg, callTargetNode, ImmediateSetWithFlush);
1983 
1984     // It&#39;s OK to exit right now, even though we set some locals. That&#39;s because those locals are not
1985     // user-visible.
1986     m_exitOK = true;
1987     addToGraph(ExitOK);
1988 
1989     SwitchData&amp; data = *m_graph.m_switchData.add();
1990     data.kind = SwitchCell;
1991     addToGraph(Switch, OpInfo(&amp;data), thingToSwitchOn);
1992     m_currentBlock-&gt;didLink();
1993 
1994     BasicBlock* continuationBlock = allocateUntargetableBlock();
1995     VERBOSE_LOG(&quot;Adding untargetable block &quot;, RawPointer(continuationBlock), &quot; (continuation)\n&quot;);
1996 
1997     // We may force this true if we give up on inlining any of the edges.
1998     bool couldTakeSlowPath = callLinkStatus.couldTakeSlowPath();
1999 
2000     VERBOSE_LOG(&quot;About to loop over functions at &quot;, currentCodeOrigin(), &quot;.\n&quot;);
2001 
2002     unsigned oldOffset = m_currentIndex;
2003     for (unsigned i = 0; i &lt; callLinkStatus.size(); ++i) {
2004         m_currentIndex = oldOffset;
2005         BasicBlock* calleeEntryBlock = allocateUntargetableBlock();
2006         m_currentBlock = calleeEntryBlock;
2007         prepareToParseBlock();
2008 
2009         // At the top of each switch case, we can exit.
2010         m_exitOK = true;
2011 
2012         Node* myCallTargetNode = getDirect(calleeReg);
2013 
2014         auto inliningResult = handleCallVariant(
2015             myCallTargetNode, result, callLinkStatus[i], registerOffset,
2016             thisArgument, argumentCountIncludingThis, nextOffset, kind, prediction,
2017             inliningBalance, continuationBlock, false);
2018 
2019         if (inliningResult == CallOptimizationResult::DidNothing) {
2020             // That failed so we let the block die. Nothing interesting should have been added to
2021             // the block. We also give up on inlining any of the (less frequent) callees.
2022             ASSERT(m_graph.m_blocks.last() == m_currentBlock);
2023             m_graph.killBlockAndItsContents(m_currentBlock);
2024             m_graph.m_blocks.removeLast();
2025             VERBOSE_LOG(&quot;Inlining of a poly call failed, we will have to go through a slow path\n&quot;);
2026 
2027             // The fact that inlining failed means we need a slow path.
2028             couldTakeSlowPath = true;
2029             break;
2030         }
2031 
2032         JSCell* thingToCaseOn;
2033         if (allAreDirectCalls)
2034             thingToCaseOn = callLinkStatus[i].nonExecutableCallee();
2035         else {
2036             ASSERT(allAreClosureCalls);
2037             thingToCaseOn = callLinkStatus[i].executable();
2038         }
2039         data.cases.append(SwitchCase(m_graph.freeze(thingToCaseOn), calleeEntryBlock));
2040         VERBOSE_LOG(&quot;Finished optimizing &quot;, callLinkStatus[i], &quot; at &quot;, currentCodeOrigin(), &quot;.\n&quot;);
2041     }
2042 
2043     // Slow path block
2044     m_currentBlock = allocateUntargetableBlock();
2045     m_currentIndex = oldOffset;
2046     m_exitOK = true;
2047     data.fallThrough = BranchTarget(m_currentBlock);
2048     prepareToParseBlock();
2049     Node* myCallTargetNode = getDirect(calleeReg);
2050     if (couldTakeSlowPath) {
2051         addCall(
2052             result, callOp, nullptr, myCallTargetNode, argumentCountIncludingThis,
2053             registerOffset, prediction);
2054         VERBOSE_LOG(&quot;We added a call in the slow path\n&quot;);
2055     } else {
2056         addToGraph(CheckBadCell);
2057         addToGraph(Phantom, myCallTargetNode);
2058         emitArgumentPhantoms(registerOffset, argumentCountIncludingThis);
2059 
2060         set(result, addToGraph(BottomValue));
2061         VERBOSE_LOG(&quot;couldTakeSlowPath was false\n&quot;);
2062     }
2063 
2064     m_currentIndex = nextOffset;
2065     m_exitOK = true; // Origin changed, so it&#39;s fine to exit again.
2066     processSetLocalQueue();
2067 
2068     if (Node* terminal = m_currentBlock-&gt;terminal())
2069         ASSERT_UNUSED(terminal, terminal-&gt;op() == TailCall || terminal-&gt;op() == TailCallVarargs || terminal-&gt;op() == TailCallForwardVarargs);
2070     else {
2071         addJumpTo(continuationBlock);
2072     }
2073 
2074     prepareToParseBlock();
2075 
2076     m_currentIndex = oldOffset;
2077     m_currentBlock = continuationBlock;
2078     m_exitOK = true;
2079 
2080     VERBOSE_LOG(&quot;Done inlining (hard).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
2081     return CallOptimizationResult::Inlined;
2082 }
2083 
2084 template&lt;typename ChecksFunctor&gt;
2085 bool ByteCodeParser::handleMinMax(VirtualRegister result, NodeType op, int registerOffset, int argumentCountIncludingThis, const ChecksFunctor&amp; insertChecks)
2086 {
2087     ASSERT(op == ArithMin || op == ArithMax);
2088 
2089     if (argumentCountIncludingThis == 1) {
2090         insertChecks();
2091         double limit = op == ArithMax ? -std::numeric_limits&lt;double&gt;::infinity() : +std::numeric_limits&lt;double&gt;::infinity();
2092         set(result, addToGraph(JSConstant, OpInfo(m_graph.freeze(jsDoubleNumber(limit)))));
2093         return true;
2094     }
2095 
2096     if (argumentCountIncludingThis == 2) {
2097         insertChecks();
2098         Node* resultNode = get(VirtualRegister(virtualRegisterForArgument(1, registerOffset)));
2099         addToGraph(Phantom, Edge(resultNode, NumberUse));
2100         set(result, resultNode);
2101         return true;
2102     }
2103 
2104     if (argumentCountIncludingThis == 3) {
2105         insertChecks();
2106         set(result, addToGraph(op, get(virtualRegisterForArgument(1, registerOffset)), get(virtualRegisterForArgument(2, registerOffset))));
2107         return true;
2108     }
2109 
2110     // Don&#39;t handle &gt;=3 arguments for now.
2111     return false;
2112 }
2113 
2114 template&lt;typename ChecksFunctor&gt;
2115 bool ByteCodeParser::handleIntrinsicCall(Node* callee, VirtualRegister result, Intrinsic intrinsic, int registerOffset, int argumentCountIncludingThis, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks)
2116 {
2117     VERBOSE_LOG(&quot;       The intrinsic is &quot;, intrinsic, &quot;\n&quot;);
2118 
2119     if (!isOpcodeShape&lt;OpCallShape&gt;(m_currentInstruction))
2120         return false;
2121 
2122     // It so happens that the code below doesn&#39;t handle the invalid result case. We could fix that, but
2123     // it would only benefit intrinsics called as setters, like if you do:
2124     //
2125     //     o.__defineSetter__(&quot;foo&quot;, Math.pow)
2126     //
2127     // Which is extremely amusing, but probably not worth optimizing.
2128     if (!result.isValid())
2129         return false;
2130 
2131     bool didSetResult = false;
2132     auto setResult = [&amp;] (Node* node) {
2133         RELEASE_ASSERT(!didSetResult);
2134         set(result, node);
2135         didSetResult = true;
2136     };
2137 
2138     auto inlineIntrinsic = [&amp;] {
2139         switch (intrinsic) {
2140 
2141         // Intrinsic Functions:
2142 
2143         case AbsIntrinsic: {
2144             if (argumentCountIncludingThis == 1) { // Math.abs()
2145                 insertChecks();
2146                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
2147                 return true;
2148             }
2149 
2150             if (!MacroAssembler::supportsFloatingPointAbs())
2151                 return false;
2152 
2153             insertChecks();
2154             Node* node = addToGraph(ArithAbs, get(virtualRegisterForArgument(1, registerOffset)));
2155             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
2156                 node-&gt;mergeFlags(NodeMayOverflowInt32InDFG);
2157             setResult(node);
2158             return true;
2159         }
2160 
2161         case MinIntrinsic:
2162         case MaxIntrinsic:
2163             if (handleMinMax(result, intrinsic == MinIntrinsic ? ArithMin : ArithMax, registerOffset, argumentCountIncludingThis, insertChecks)) {
2164                 didSetResult = true;
2165                 return true;
2166             }
2167             return false;
2168 
2169 #define DFG_ARITH_UNARY(capitalizedName, lowerName) \
2170         case capitalizedName##Intrinsic:
2171         FOR_EACH_DFG_ARITH_UNARY_OP(DFG_ARITH_UNARY)
2172 #undef DFG_ARITH_UNARY
2173         {
2174             if (argumentCountIncludingThis == 1) {
2175                 insertChecks();
2176                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
2177                 return true;
2178             }
2179             Arith::UnaryType type = Arith::UnaryType::Sin;
2180             switch (intrinsic) {
2181 #define DFG_ARITH_UNARY(capitalizedName, lowerName) \
2182             case capitalizedName##Intrinsic: \
2183                 type = Arith::UnaryType::capitalizedName; \
2184                 break;
2185         FOR_EACH_DFG_ARITH_UNARY_OP(DFG_ARITH_UNARY)
2186 #undef DFG_ARITH_UNARY
2187             default:
2188                 RELEASE_ASSERT_NOT_REACHED();
2189             }
2190             insertChecks();
2191             setResult(addToGraph(ArithUnary, OpInfo(static_cast&lt;std::underlying_type&lt;Arith::UnaryType&gt;::type&gt;(type)), get(virtualRegisterForArgument(1, registerOffset))));
2192             return true;
2193         }
2194 
2195         case FRoundIntrinsic:
2196         case SqrtIntrinsic: {
2197             if (argumentCountIncludingThis == 1) {
2198                 insertChecks();
2199                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
2200                 return true;
2201             }
2202 
2203             NodeType nodeType = Unreachable;
2204             switch (intrinsic) {
2205             case FRoundIntrinsic:
2206                 nodeType = ArithFRound;
2207                 break;
2208             case SqrtIntrinsic:
2209                 nodeType = ArithSqrt;
2210                 break;
2211             default:
2212                 RELEASE_ASSERT_NOT_REACHED();
2213             }
2214             insertChecks();
2215             setResult(addToGraph(nodeType, get(virtualRegisterForArgument(1, registerOffset))));
2216             return true;
2217         }
2218 
2219         case PowIntrinsic: {
2220             if (argumentCountIncludingThis &lt; 3) {
2221                 // Math.pow() and Math.pow(x) return NaN.
2222                 insertChecks();
2223                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
2224                 return true;
2225             }
2226             insertChecks();
2227             VirtualRegister xOperand = virtualRegisterForArgument(1, registerOffset);
2228             VirtualRegister yOperand = virtualRegisterForArgument(2, registerOffset);
2229             setResult(addToGraph(ArithPow, get(xOperand), get(yOperand)));
2230             return true;
2231         }
2232 
2233         case ArrayPushIntrinsic: {
2234 #if USE(JSVALUE32_64)
2235             if (isX86()) {
2236                 if (argumentCountIncludingThis &gt; 2)
2237                     return false;
2238             }
2239 #endif
2240 
2241             if (static_cast&lt;unsigned&gt;(argumentCountIncludingThis) &gt;= MIN_SPARSE_ARRAY_INDEX)
2242                 return false;
2243 
2244             ArrayMode arrayMode = getArrayMode(Array::Write);
2245             if (!arrayMode.isJSArray())
2246                 return false;
2247             switch (arrayMode.type()) {
2248             case Array::Int32:
2249             case Array::Double:
2250             case Array::Contiguous:
2251             case Array::ArrayStorage: {
2252                 insertChecks();
2253 
2254                 addVarArgChild(nullptr); // For storage.
2255                 for (int i = 0; i &lt; argumentCountIncludingThis; ++i)
2256                     addVarArgChild(get(virtualRegisterForArgument(i, registerOffset)));
2257                 Node* arrayPush = addToGraph(Node::VarArg, ArrayPush, OpInfo(arrayMode.asWord()), OpInfo(prediction));
2258                 setResult(arrayPush);
2259                 return true;
2260             }
2261 
2262             default:
2263                 return false;
2264             }
2265         }
2266 
2267         case ArraySliceIntrinsic: {
2268 #if USE(JSVALUE32_64)
2269             if (isX86()) {
2270                 // There aren&#39;t enough registers for this to be done easily.
2271                 return false;
2272             }
2273 #endif
2274             if (argumentCountIncludingThis &lt; 1)
2275                 return false;
2276 
2277             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadConstantCache)
2278                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache))
2279                 return false;
2280 
2281             ArrayMode arrayMode = getArrayMode(Array::Read);
2282             if (!arrayMode.isJSArray())
2283                 return false;
2284 
2285             if (!arrayMode.isJSArrayWithOriginalStructure())
2286                 return false;
2287 
2288             switch (arrayMode.type()) {
2289             case Array::Double:
2290             case Array::Int32:
2291             case Array::Contiguous: {
2292                 JSGlobalObject* globalObject = m_graph.globalObjectFor(currentNodeOrigin().semantic);
2293 
2294                 Structure* arrayPrototypeStructure = globalObject-&gt;arrayPrototype()-&gt;structure(*m_vm);
2295                 Structure* objectPrototypeStructure = globalObject-&gt;objectPrototype()-&gt;structure(*m_vm);
2296 
2297                 // FIXME: We could easily relax the Array/Object.prototype transition as long as we OSR exitted if we saw a hole.
2298                 // https://bugs.webkit.org/show_bug.cgi?id=173171
2299                 if (globalObject-&gt;arraySpeciesWatchpoint().state() == IsWatched
2300                     &amp;&amp; globalObject-&gt;havingABadTimeWatchpoint()-&gt;isStillValid()
2301                     &amp;&amp; arrayPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2302                     &amp;&amp; objectPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2303                     &amp;&amp; globalObject-&gt;arrayPrototypeChainIsSane()) {
2304 
2305                     m_graph.watchpoints().addLazily(globalObject-&gt;arraySpeciesWatchpoint());
2306                     m_graph.watchpoints().addLazily(globalObject-&gt;havingABadTimeWatchpoint());
2307                     m_graph.registerAndWatchStructureTransition(arrayPrototypeStructure);
2308                     m_graph.registerAndWatchStructureTransition(objectPrototypeStructure);
2309 
2310                     insertChecks();
2311 
2312                     Node* array = get(virtualRegisterForArgument(0, registerOffset));
2313                     // We do a few things here to prove that we aren&#39;t skipping doing side-effects in an observable way:
2314                     // 1. We ensure that the &quot;constructor&quot; property hasn&#39;t been changed (because the observable
2315                     // effects of slice require that we perform a Get(array, &quot;constructor&quot;) and we can skip
2316                     // that if we&#39;re an original array structure. (We can relax this in the future by using
2317                     // TryGetById and CheckCell).
2318                     //
2319                     // 2. We check that the array we&#39;re calling slice on has the same global object as the lexical
2320                     // global object that this code is running in. This requirement is necessary because we setup the
2321                     // watchpoints above on the lexical global object. This means that code that calls slice on
2322                     // arrays produced by other global objects won&#39;t get this optimization. We could relax this
2323                     // requirement in the future by checking that the watchpoint hasn&#39;t fired at runtime in the code
2324                     // we generate instead of registering it as a watchpoint that would invalidate the compilation.
2325                     //
2326                     // 3. By proving we&#39;re an original array structure, we guarantee that the incoming array
2327                     // isn&#39;t a subclass of Array.
2328 
2329                     StructureSet structureSet;
2330                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(ArrayWithInt32));
2331                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(ArrayWithContiguous));
2332                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(ArrayWithDouble));
2333                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(CopyOnWriteArrayWithInt32));
2334                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(CopyOnWriteArrayWithContiguous));
2335                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(CopyOnWriteArrayWithDouble));
2336                     addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(structureSet)), array);
2337 
2338                     addVarArgChild(array);
2339                     if (argumentCountIncludingThis &gt;= 2)
2340                         addVarArgChild(get(virtualRegisterForArgument(1, registerOffset))); // Start index.
2341                     if (argumentCountIncludingThis &gt;= 3)
2342                         addVarArgChild(get(virtualRegisterForArgument(2, registerOffset))); // End index.
2343                     addVarArgChild(addToGraph(GetButterfly, array));
2344 
2345                     Node* arraySlice = addToGraph(Node::VarArg, ArraySlice, OpInfo(), OpInfo());
2346                     setResult(arraySlice);
2347                     return true;
2348                 }
2349 
2350                 return false;
2351             }
2352             default:
2353                 return false;
2354             }
2355 
2356             RELEASE_ASSERT_NOT_REACHED();
2357             return false;
2358         }
2359 
2360         case ArrayIndexOfIntrinsic: {
2361             if (argumentCountIncludingThis &lt; 2)
2362                 return false;
2363 
2364             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIndexingType)
2365                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadConstantCache)
2366                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache)
2367                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
2368                 return false;
2369 
2370             ArrayMode arrayMode = getArrayMode(Array::Read);
2371             if (!arrayMode.isJSArray())
2372                 return false;
2373 
2374             if (!arrayMode.isJSArrayWithOriginalStructure())
2375                 return false;
2376 
2377             // We do not want to convert arrays into one type just to perform indexOf.
2378             if (arrayMode.doesConversion())
2379                 return false;
2380 
2381             switch (arrayMode.type()) {
2382             case Array::Double:
2383             case Array::Int32:
2384             case Array::Contiguous: {
2385                 JSGlobalObject* globalObject = m_graph.globalObjectFor(currentNodeOrigin().semantic);
2386 
2387                 Structure* arrayPrototypeStructure = globalObject-&gt;arrayPrototype()-&gt;structure(*m_vm);
2388                 Structure* objectPrototypeStructure = globalObject-&gt;objectPrototype()-&gt;structure(*m_vm);
2389 
2390                 // FIXME: We could easily relax the Array/Object.prototype transition as long as we OSR exitted if we saw a hole.
2391                 // https://bugs.webkit.org/show_bug.cgi?id=173171
2392                 if (arrayPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2393                     &amp;&amp; objectPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2394                     &amp;&amp; globalObject-&gt;arrayPrototypeChainIsSane()) {
2395 
2396                     m_graph.registerAndWatchStructureTransition(arrayPrototypeStructure);
2397                     m_graph.registerAndWatchStructureTransition(objectPrototypeStructure);
2398 
2399                     insertChecks();
2400 
2401                     Node* array = get(virtualRegisterForArgument(0, registerOffset));
2402                     addVarArgChild(array);
2403                     addVarArgChild(get(virtualRegisterForArgument(1, registerOffset))); // Search element.
2404                     if (argumentCountIncludingThis &gt;= 3)
2405                         addVarArgChild(get(virtualRegisterForArgument(2, registerOffset))); // Start index.
2406                     addVarArgChild(nullptr);
2407 
2408                     Node* node = addToGraph(Node::VarArg, ArrayIndexOf, OpInfo(arrayMode.asWord()), OpInfo());
2409                     setResult(node);
2410                     return true;
2411                 }
2412 
2413                 return false;
2414             }
2415             default:
2416                 return false;
2417             }
2418 
2419             RELEASE_ASSERT_NOT_REACHED();
2420             return false;
2421 
2422         }
2423 
2424         case ArrayPopIntrinsic: {
2425             if (argumentCountIncludingThis != 1)
2426                 return false;
2427 
2428             ArrayMode arrayMode = getArrayMode(Array::Write);
2429             if (!arrayMode.isJSArray())
2430                 return false;
2431             switch (arrayMode.type()) {
2432             case Array::Int32:
2433             case Array::Double:
2434             case Array::Contiguous:
2435             case Array::ArrayStorage: {
2436                 insertChecks();
2437                 Node* arrayPop = addToGraph(ArrayPop, OpInfo(arrayMode.asWord()), OpInfo(prediction), get(virtualRegisterForArgument(0, registerOffset)));
2438                 setResult(arrayPop);
2439                 return true;
2440             }
2441 
2442             default:
2443                 return false;
2444             }
2445         }
2446 
2447         case AtomicsAddIntrinsic:
2448         case AtomicsAndIntrinsic:
2449         case AtomicsCompareExchangeIntrinsic:
2450         case AtomicsExchangeIntrinsic:
2451         case AtomicsIsLockFreeIntrinsic:
2452         case AtomicsLoadIntrinsic:
2453         case AtomicsOrIntrinsic:
2454         case AtomicsStoreIntrinsic:
2455         case AtomicsSubIntrinsic:
2456         case AtomicsXorIntrinsic: {
2457             if (!is64Bit())
2458                 return false;
2459 
2460             NodeType op = LastNodeType;
2461             Array::Action action = Array::Write;
2462             unsigned numArgs = 0; // Number of actual args; we add one for the backing store pointer.
2463             switch (intrinsic) {
2464             case AtomicsAddIntrinsic:
2465                 op = AtomicsAdd;
2466                 numArgs = 3;
2467                 break;
2468             case AtomicsAndIntrinsic:
2469                 op = AtomicsAnd;
2470                 numArgs = 3;
2471                 break;
2472             case AtomicsCompareExchangeIntrinsic:
2473                 op = AtomicsCompareExchange;
2474                 numArgs = 4;
2475                 break;
2476             case AtomicsExchangeIntrinsic:
2477                 op = AtomicsExchange;
2478                 numArgs = 3;
2479                 break;
2480             case AtomicsIsLockFreeIntrinsic:
2481                 // This gets no backing store, but we need no special logic for this since this also does
2482                 // not need varargs.
2483                 op = AtomicsIsLockFree;
2484                 numArgs = 1;
2485                 break;
2486             case AtomicsLoadIntrinsic:
2487                 op = AtomicsLoad;
2488                 numArgs = 2;
2489                 action = Array::Read;
2490                 break;
2491             case AtomicsOrIntrinsic:
2492                 op = AtomicsOr;
2493                 numArgs = 3;
2494                 break;
2495             case AtomicsStoreIntrinsic:
2496                 op = AtomicsStore;
2497                 numArgs = 3;
2498                 break;
2499             case AtomicsSubIntrinsic:
2500                 op = AtomicsSub;
2501                 numArgs = 3;
2502                 break;
2503             case AtomicsXorIntrinsic:
2504                 op = AtomicsXor;
2505                 numArgs = 3;
2506                 break;
2507             default:
2508                 RELEASE_ASSERT_NOT_REACHED();
2509                 break;
2510             }
2511 
2512             if (static_cast&lt;unsigned&gt;(argumentCountIncludingThis) &lt; 1 + numArgs)
2513                 return false;
2514 
2515             insertChecks();
2516 
2517             Vector&lt;Node*, 3&gt; args;
2518             for (unsigned i = 0; i &lt; numArgs; ++i)
2519                 args.append(get(virtualRegisterForArgument(1 + i, registerOffset)));
2520 
2521             Node* resultNode;
2522             if (numArgs + 1 &lt;= 3) {
2523                 while (args.size() &lt; 3)
2524                     args.append(nullptr);
2525                 resultNode = addToGraph(op, OpInfo(ArrayMode(Array::SelectUsingPredictions, action).asWord()), OpInfo(prediction), args[0], args[1], args[2]);
2526             } else {
2527                 for (Node* node : args)
2528                     addVarArgChild(node);
2529                 addVarArgChild(nullptr);
2530                 resultNode = addToGraph(Node::VarArg, op, OpInfo(ArrayMode(Array::SelectUsingPredictions, action).asWord()), OpInfo(prediction));
2531             }
2532 
2533             setResult(resultNode);
2534             return true;
2535         }
2536 
2537         case ParseIntIntrinsic: {
2538             if (argumentCountIncludingThis &lt; 2)
2539                 return false;
2540 
2541             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell) || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
2542                 return false;
2543 
2544             insertChecks();
2545             VirtualRegister valueOperand = virtualRegisterForArgument(1, registerOffset);
2546             Node* parseInt;
2547             if (argumentCountIncludingThis == 2)
2548                 parseInt = addToGraph(ParseInt, OpInfo(), OpInfo(prediction), get(valueOperand));
2549             else {
2550                 ASSERT(argumentCountIncludingThis &gt; 2);
2551                 VirtualRegister radixOperand = virtualRegisterForArgument(2, registerOffset);
2552                 parseInt = addToGraph(ParseInt, OpInfo(), OpInfo(prediction), get(valueOperand), get(radixOperand));
2553             }
2554             setResult(parseInt);
2555             return true;
2556         }
2557 
2558         case CharCodeAtIntrinsic: {
2559             if (argumentCountIncludingThis != 2)
2560                 return false;
2561 
2562             insertChecks();
2563             VirtualRegister thisOperand = virtualRegisterForArgument(0, registerOffset);
2564             VirtualRegister indexOperand = virtualRegisterForArgument(1, registerOffset);
2565             Node* charCode = addToGraph(StringCharCodeAt, OpInfo(ArrayMode(Array::String, Array::Read).asWord()), get(thisOperand), get(indexOperand));
2566 
2567             setResult(charCode);
2568             return true;
2569         }
2570 
2571         case CharAtIntrinsic: {
2572             if (argumentCountIncludingThis != 2)
2573                 return false;
2574 
2575             insertChecks();
2576             VirtualRegister thisOperand = virtualRegisterForArgument(0, registerOffset);
2577             VirtualRegister indexOperand = virtualRegisterForArgument(1, registerOffset);
2578             Node* charCode = addToGraph(StringCharAt, OpInfo(ArrayMode(Array::String, Array::Read).asWord()), get(thisOperand), get(indexOperand));
2579 
2580             setResult(charCode);
2581             return true;
2582         }
2583         case Clz32Intrinsic: {
2584             insertChecks();
2585             if (argumentCountIncludingThis == 1)
2586                 setResult(addToGraph(JSConstant, OpInfo(m_graph.freeze(jsNumber(32)))));
2587             else {
2588                 Node* operand = get(virtualRegisterForArgument(1, registerOffset));
2589                 setResult(addToGraph(ArithClz32, operand));
2590             }
2591             return true;
2592         }
2593         case FromCharCodeIntrinsic: {
2594             if (argumentCountIncludingThis != 2)
2595                 return false;
2596 
2597             insertChecks();
2598             VirtualRegister indexOperand = virtualRegisterForArgument(1, registerOffset);
2599             Node* charCode = addToGraph(StringFromCharCode, get(indexOperand));
2600 
2601             setResult(charCode);
2602 
2603             return true;
2604         }
2605 
2606         case RegExpExecIntrinsic: {
2607             if (argumentCountIncludingThis != 2)
2608                 return false;
2609 
2610             insertChecks();
2611             Node* regExpExec = addToGraph(RegExpExec, OpInfo(0), OpInfo(prediction), addToGraph(GetGlobalObject, callee), get(virtualRegisterForArgument(0, registerOffset)), get(virtualRegisterForArgument(1, registerOffset)));
2612             setResult(regExpExec);
2613 
2614             return true;
2615         }
2616 
2617         case RegExpTestIntrinsic:
2618         case RegExpTestFastIntrinsic: {
2619             if (argumentCountIncludingThis != 2)
2620                 return false;
2621 
2622             if (intrinsic == RegExpTestIntrinsic) {
2623                 // Don&#39;t inline intrinsic if we exited due to one of the primordial RegExp checks failing.
2624                 if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell))
2625                     return false;
2626 
2627                 JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
2628                 Structure* regExpStructure = globalObject-&gt;regExpStructure();
2629                 m_graph.registerStructure(regExpStructure);
2630                 ASSERT(regExpStructure-&gt;storedPrototype().isObject());
2631                 ASSERT(regExpStructure-&gt;storedPrototype().asCell()-&gt;classInfo(*m_vm) == RegExpPrototype::info());
2632 
2633                 FrozenValue* regExpPrototypeObjectValue = m_graph.freeze(regExpStructure-&gt;storedPrototype());
2634                 Structure* regExpPrototypeStructure = regExpPrototypeObjectValue-&gt;structure();
2635 
2636                 auto isRegExpPropertySame = [&amp;] (JSValue primordialProperty, UniquedStringImpl* propertyUID) {
2637                     JSValue currentProperty;
2638                     if (!m_graph.getRegExpPrototypeProperty(regExpStructure-&gt;storedPrototypeObject(), regExpPrototypeStructure, propertyUID, currentProperty))
2639                         return false;
2640 
2641                     return currentProperty == primordialProperty;
2642                 };
2643 
2644                 // Check that RegExp.exec is still the primordial RegExp.prototype.exec
2645                 if (!isRegExpPropertySame(globalObject-&gt;regExpProtoExecFunction(), m_vm-&gt;propertyNames-&gt;exec.impl()))
2646                     return false;
2647 
2648                 // Check that regExpObject is actually a RegExp object.
2649                 Node* regExpObject = get(virtualRegisterForArgument(0, registerOffset));
2650                 addToGraph(Check, Edge(regExpObject, RegExpObjectUse));
2651 
2652                 // Check that regExpObject&#39;s exec is actually the primodial RegExp.prototype.exec.
2653                 UniquedStringImpl* execPropertyID = m_vm-&gt;propertyNames-&gt;exec.impl();
2654                 unsigned execIndex = m_graph.identifiers().ensure(execPropertyID);
2655                 Node* actualProperty = addToGraph(TryGetById, OpInfo(execIndex), OpInfo(SpecFunction), Edge(regExpObject, CellUse));
2656                 FrozenValue* regExpPrototypeExec = m_graph.freeze(globalObject-&gt;regExpProtoExecFunction());
2657                 addToGraph(CheckCell, OpInfo(regExpPrototypeExec), Edge(actualProperty, CellUse));
2658             }
2659 
2660             insertChecks();
2661             Node* regExpObject = get(virtualRegisterForArgument(0, registerOffset));
2662             Node* regExpExec = addToGraph(RegExpTest, OpInfo(0), OpInfo(prediction), addToGraph(GetGlobalObject, callee), regExpObject, get(virtualRegisterForArgument(1, registerOffset)));
2663             setResult(regExpExec);
2664 
2665             return true;
2666         }
2667 
2668         case RegExpMatchFastIntrinsic: {
2669             RELEASE_ASSERT(argumentCountIncludingThis == 2);
2670 
2671             insertChecks();
2672             Node* regExpMatch = addToGraph(RegExpMatchFast, OpInfo(0), OpInfo(prediction), addToGraph(GetGlobalObject, callee), get(virtualRegisterForArgument(0, registerOffset)), get(virtualRegisterForArgument(1, registerOffset)));
2673             setResult(regExpMatch);
2674             return true;
2675         }
2676 
2677         case ObjectCreateIntrinsic: {
2678             if (argumentCountIncludingThis != 2)
2679                 return false;
2680 
2681             insertChecks();
2682             setResult(addToGraph(ObjectCreate, get(virtualRegisterForArgument(1, registerOffset))));
2683             return true;
2684         }
2685 
2686         case ObjectGetPrototypeOfIntrinsic: {
2687             if (argumentCountIncludingThis != 2)
2688                 return false;
2689 
2690             insertChecks();
2691             setResult(addToGraph(GetPrototypeOf, OpInfo(0), OpInfo(prediction), get(virtualRegisterForArgument(1, registerOffset))));
2692             return true;
2693         }
2694 
2695         case ObjectIsIntrinsic: {
2696             if (argumentCountIncludingThis &lt; 3)
2697                 return false;
2698 
2699             insertChecks();
2700             setResult(addToGraph(SameValue, get(virtualRegisterForArgument(1, registerOffset)), get(virtualRegisterForArgument(2, registerOffset))));
2701             return true;
2702         }
2703 
2704         case ObjectKeysIntrinsic: {
2705             if (argumentCountIncludingThis &lt; 2)
2706                 return false;
2707 
2708             insertChecks();
2709             setResult(addToGraph(ObjectKeys, get(virtualRegisterForArgument(1, registerOffset))));
2710             return true;
2711         }
2712 
2713         case ReflectGetPrototypeOfIntrinsic: {
2714             if (argumentCountIncludingThis != 2)
2715                 return false;
2716 
2717             insertChecks();
2718             setResult(addToGraph(GetPrototypeOf, OpInfo(0), OpInfo(prediction), Edge(get(virtualRegisterForArgument(1, registerOffset)), ObjectUse)));
2719             return true;
2720         }
2721 
2722         case IsTypedArrayViewIntrinsic: {
2723             ASSERT(argumentCountIncludingThis == 2);
2724 
2725             insertChecks();
2726             setResult(addToGraph(IsTypedArrayView, OpInfo(prediction), get(virtualRegisterForArgument(1, registerOffset))));
2727             return true;
2728         }
2729 
2730         case StringPrototypeValueOfIntrinsic: {
2731             insertChecks();
2732             Node* value = get(virtualRegisterForArgument(0, registerOffset));
2733             setResult(addToGraph(StringValueOf, value));
2734             return true;
2735         }
2736 
2737         case StringPrototypeReplaceIntrinsic: {
2738             if (argumentCountIncludingThis != 3)
2739                 return false;
2740 
2741             // Don&#39;t inline intrinsic if we exited due to &quot;search&quot; not being a RegExp or String object.
2742             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
2743                 return false;
2744 
2745             // Don&#39;t inline intrinsic if we exited due to one of the primordial RegExp checks failing.
2746             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell))
2747                 return false;
2748 
2749             JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
2750             Structure* regExpStructure = globalObject-&gt;regExpStructure();
2751             m_graph.registerStructure(regExpStructure);
2752             ASSERT(regExpStructure-&gt;storedPrototype().isObject());
2753             ASSERT(regExpStructure-&gt;storedPrototype().asCell()-&gt;classInfo(*m_vm) == RegExpPrototype::info());
2754 
2755             FrozenValue* regExpPrototypeObjectValue = m_graph.freeze(regExpStructure-&gt;storedPrototype());
2756             Structure* regExpPrototypeStructure = regExpPrototypeObjectValue-&gt;structure();
2757 
2758             auto isRegExpPropertySame = [&amp;] (JSValue primordialProperty, UniquedStringImpl* propertyUID) {
2759                 JSValue currentProperty;
2760                 if (!m_graph.getRegExpPrototypeProperty(regExpStructure-&gt;storedPrototypeObject(), regExpPrototypeStructure, propertyUID, currentProperty))
2761                     return false;
2762 
2763                 return currentProperty == primordialProperty;
2764             };
2765 
2766             // Check that searchRegExp.exec is still the primordial RegExp.prototype.exec
2767             if (!isRegExpPropertySame(globalObject-&gt;regExpProtoExecFunction(), m_vm-&gt;propertyNames-&gt;exec.impl()))
2768                 return false;
2769 
2770             // Check that searchRegExp.global is still the primordial RegExp.prototype.global
2771             if (!isRegExpPropertySame(globalObject-&gt;regExpProtoGlobalGetter(), m_vm-&gt;propertyNames-&gt;global.impl()))
2772                 return false;
2773 
2774             // Check that searchRegExp.unicode is still the primordial RegExp.prototype.unicode
2775             if (!isRegExpPropertySame(globalObject-&gt;regExpProtoUnicodeGetter(), m_vm-&gt;propertyNames-&gt;unicode.impl()))
2776                 return false;
2777 
2778             // Check that searchRegExp[Symbol.match] is still the primordial RegExp.prototype[Symbol.replace]
2779             if (!isRegExpPropertySame(globalObject-&gt;regExpProtoSymbolReplaceFunction(), m_vm-&gt;propertyNames-&gt;replaceSymbol.impl()))
2780                 return false;
2781 
2782             insertChecks();
2783 
2784             Node* resultNode = addToGraph(StringReplace, OpInfo(0), OpInfo(prediction), get(virtualRegisterForArgument(0, registerOffset)), get(virtualRegisterForArgument(1, registerOffset)), get(virtualRegisterForArgument(2, registerOffset)));
2785             setResult(resultNode);
2786             return true;
2787         }
2788 
2789         case StringPrototypeReplaceRegExpIntrinsic: {
2790             if (argumentCountIncludingThis != 3)
2791                 return false;
2792 
2793             insertChecks();
2794             Node* resultNode = addToGraph(StringReplaceRegExp, OpInfo(0), OpInfo(prediction), get(virtualRegisterForArgument(0, registerOffset)), get(virtualRegisterForArgument(1, registerOffset)), get(virtualRegisterForArgument(2, registerOffset)));
2795             setResult(resultNode);
2796             return true;
2797         }
2798 
2799         case RoundIntrinsic:
2800         case FloorIntrinsic:
2801         case CeilIntrinsic:
2802         case TruncIntrinsic: {
2803             if (argumentCountIncludingThis == 1) {
2804                 insertChecks();
2805                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
2806                 return true;
2807             }
2808             insertChecks();
2809             Node* operand = get(virtualRegisterForArgument(1, registerOffset));
2810             NodeType op;
2811             if (intrinsic == RoundIntrinsic)
2812                 op = ArithRound;
2813             else if (intrinsic == FloorIntrinsic)
2814                 op = ArithFloor;
2815             else if (intrinsic == CeilIntrinsic)
2816                 op = ArithCeil;
2817             else {
2818                 ASSERT(intrinsic == TruncIntrinsic);
2819                 op = ArithTrunc;
2820             }
2821             Node* roundNode = addToGraph(op, OpInfo(0), OpInfo(prediction), operand);
2822             setResult(roundNode);
2823             return true;
2824         }
2825         case IMulIntrinsic: {
2826             if (argumentCountIncludingThis != 3)
2827                 return false;
2828             insertChecks();
2829             VirtualRegister leftOperand = virtualRegisterForArgument(1, registerOffset);
2830             VirtualRegister rightOperand = virtualRegisterForArgument(2, registerOffset);
2831             Node* left = get(leftOperand);
2832             Node* right = get(rightOperand);
2833             setResult(addToGraph(ArithIMul, left, right));
2834             return true;
2835         }
2836 
2837         case RandomIntrinsic: {
2838             if (argumentCountIncludingThis != 1)
2839                 return false;
2840             insertChecks();
2841             setResult(addToGraph(ArithRandom));
2842             return true;
2843         }
2844 
2845         case DFGTrueIntrinsic: {
2846             insertChecks();
2847             setResult(jsConstant(jsBoolean(true)));
2848             return true;
2849         }
2850 
2851         case FTLTrueIntrinsic: {
2852             insertChecks();
2853             setResult(jsConstant(jsBoolean(m_graph.m_plan.isFTL())));
2854             return true;
2855         }
2856 
2857         case OSRExitIntrinsic: {
2858             insertChecks();
2859             addToGraph(ForceOSRExit);
2860             setResult(addToGraph(JSConstant, OpInfo(m_constantUndefined)));
2861             return true;
2862         }
2863 
2864         case IsFinalTierIntrinsic: {
2865             insertChecks();
2866             setResult(jsConstant(jsBoolean(Options::useFTLJIT() ? m_graph.m_plan.isFTL() : true)));
2867             return true;
2868         }
2869 
2870         case SetInt32HeapPredictionIntrinsic: {
2871             insertChecks();
2872             for (int i = 1; i &lt; argumentCountIncludingThis; ++i) {
2873                 Node* node = get(virtualRegisterForArgument(i, registerOffset));
2874                 if (node-&gt;hasHeapPrediction())
2875                     node-&gt;setHeapPrediction(SpecInt32Only);
2876             }
2877             setResult(addToGraph(JSConstant, OpInfo(m_constantUndefined)));
2878             return true;
2879         }
2880 
2881         case CheckInt32Intrinsic: {
2882             insertChecks();
2883             for (int i = 1; i &lt; argumentCountIncludingThis; ++i) {
2884                 Node* node = get(virtualRegisterForArgument(i, registerOffset));
2885                 addToGraph(Phantom, Edge(node, Int32Use));
2886             }
2887             setResult(jsConstant(jsBoolean(true)));
2888             return true;
2889         }
2890 
2891         case FiatInt52Intrinsic: {
2892             if (argumentCountIncludingThis != 2)
2893                 return false;
2894             insertChecks();
2895             VirtualRegister operand = virtualRegisterForArgument(1, registerOffset);
2896             if (enableInt52())
2897                 setResult(addToGraph(FiatInt52, get(operand)));
2898             else
2899                 setResult(get(operand));
2900             return true;
2901         }
2902 
2903         case JSMapGetIntrinsic: {
2904             if (argumentCountIncludingThis != 2)
2905                 return false;
2906 
2907             insertChecks();
2908             Node* map = get(virtualRegisterForArgument(0, registerOffset));
2909             Node* key = get(virtualRegisterForArgument(1, registerOffset));
2910             Node* normalizedKey = addToGraph(NormalizeMapKey, key);
2911             Node* hash = addToGraph(MapHash, normalizedKey);
2912             Node* bucket = addToGraph(GetMapBucket, Edge(map, MapObjectUse), Edge(normalizedKey), Edge(hash));
2913             Node* resultNode = addToGraph(LoadValueFromMapBucket, OpInfo(BucketOwnerType::Map), OpInfo(prediction), bucket);
2914             setResult(resultNode);
2915             return true;
2916         }
2917 
2918         case JSSetHasIntrinsic:
2919         case JSMapHasIntrinsic: {
2920             if (argumentCountIncludingThis != 2)
2921                 return false;
2922 
2923             insertChecks();
2924             Node* mapOrSet = get(virtualRegisterForArgument(0, registerOffset));
2925             Node* key = get(virtualRegisterForArgument(1, registerOffset));
2926             Node* normalizedKey = addToGraph(NormalizeMapKey, key);
2927             Node* hash = addToGraph(MapHash, normalizedKey);
2928             UseKind useKind = intrinsic == JSSetHasIntrinsic ? SetObjectUse : MapObjectUse;
2929             Node* bucket = addToGraph(GetMapBucket, OpInfo(0), Edge(mapOrSet, useKind), Edge(normalizedKey), Edge(hash));
2930             JSCell* sentinel = nullptr;
2931             if (intrinsic == JSMapHasIntrinsic)
2932                 sentinel = m_vm-&gt;sentinelMapBucket();
2933             else
2934                 sentinel = m_vm-&gt;sentinelSetBucket();
2935 
2936             FrozenValue* frozenPointer = m_graph.freeze(sentinel);
2937             Node* invertedResult = addToGraph(CompareEqPtr, OpInfo(frozenPointer), bucket);
2938             Node* resultNode = addToGraph(LogicalNot, invertedResult);
2939             setResult(resultNode);
2940             return true;
2941         }
2942 
2943         case JSSetAddIntrinsic: {
2944             if (argumentCountIncludingThis != 2)
2945                 return false;
2946 
2947             insertChecks();
2948             Node* base = get(virtualRegisterForArgument(0, registerOffset));
2949             Node* key = get(virtualRegisterForArgument(1, registerOffset));
2950             Node* normalizedKey = addToGraph(NormalizeMapKey, key);
2951             Node* hash = addToGraph(MapHash, normalizedKey);
2952             addToGraph(SetAdd, base, normalizedKey, hash);
2953             setResult(base);
2954             return true;
2955         }
2956 
2957         case JSMapSetIntrinsic: {
2958             if (argumentCountIncludingThis != 3)
2959                 return false;
2960 
2961             insertChecks();
2962             Node* base = get(virtualRegisterForArgument(0, registerOffset));
2963             Node* key = get(virtualRegisterForArgument(1, registerOffset));
2964             Node* value = get(virtualRegisterForArgument(2, registerOffset));
2965 
2966             Node* normalizedKey = addToGraph(NormalizeMapKey, key);
2967             Node* hash = addToGraph(MapHash, normalizedKey);
2968 
2969             addVarArgChild(base);
2970             addVarArgChild(normalizedKey);
2971             addVarArgChild(value);
2972             addVarArgChild(hash);
2973             addToGraph(Node::VarArg, MapSet, OpInfo(0), OpInfo(0));
2974             setResult(base);
2975             return true;
2976         }
2977 
2978         case JSSetBucketHeadIntrinsic:
2979         case JSMapBucketHeadIntrinsic: {
2980             ASSERT(argumentCountIncludingThis == 2);
2981 
2982             insertChecks();
2983             Node* map = get(virtualRegisterForArgument(1, registerOffset));
2984             UseKind useKind = intrinsic == JSSetBucketHeadIntrinsic ? SetObjectUse : MapObjectUse;
2985             Node* resultNode = addToGraph(GetMapBucketHead, Edge(map, useKind));
2986             setResult(resultNode);
2987             return true;
2988         }
2989 
2990         case JSSetBucketNextIntrinsic:
2991         case JSMapBucketNextIntrinsic: {
2992             ASSERT(argumentCountIncludingThis == 2);
2993 
2994             insertChecks();
2995             Node* bucket = get(virtualRegisterForArgument(1, registerOffset));
2996             BucketOwnerType type = intrinsic == JSSetBucketNextIntrinsic ? BucketOwnerType::Set : BucketOwnerType::Map;
2997             Node* resultNode = addToGraph(GetMapBucketNext, OpInfo(type), bucket);
2998             setResult(resultNode);
2999             return true;
3000         }
3001 
3002         case JSSetBucketKeyIntrinsic:
3003         case JSMapBucketKeyIntrinsic: {
3004             ASSERT(argumentCountIncludingThis == 2);
3005 
3006             insertChecks();
3007             Node* bucket = get(virtualRegisterForArgument(1, registerOffset));
3008             BucketOwnerType type = intrinsic == JSSetBucketKeyIntrinsic ? BucketOwnerType::Set : BucketOwnerType::Map;
3009             Node* resultNode = addToGraph(LoadKeyFromMapBucket, OpInfo(type), OpInfo(prediction), bucket);
3010             setResult(resultNode);
3011             return true;
3012         }
3013 
3014         case JSMapBucketValueIntrinsic: {
3015             ASSERT(argumentCountIncludingThis == 2);
3016 
3017             insertChecks();
3018             Node* bucket = get(virtualRegisterForArgument(1, registerOffset));
3019             Node* resultNode = addToGraph(LoadValueFromMapBucket, OpInfo(BucketOwnerType::Map), OpInfo(prediction), bucket);
3020             setResult(resultNode);
3021             return true;
3022         }
3023 
3024         case JSWeakMapGetIntrinsic: {
3025             if (argumentCountIncludingThis != 2)
3026                 return false;
3027 
3028             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3029                 return false;
3030 
3031             insertChecks();
3032             Node* map = get(virtualRegisterForArgument(0, registerOffset));
3033             Node* key = get(virtualRegisterForArgument(1, registerOffset));
3034             addToGraph(Check, Edge(key, ObjectUse));
3035             Node* hash = addToGraph(MapHash, key);
3036             Node* holder = addToGraph(WeakMapGet, Edge(map, WeakMapObjectUse), Edge(key, ObjectUse), Edge(hash, Int32Use));
3037             Node* resultNode = addToGraph(ExtractValueFromWeakMapGet, OpInfo(), OpInfo(prediction), holder);
3038 
3039             setResult(resultNode);
3040             return true;
3041         }
3042 
3043         case JSWeakMapHasIntrinsic: {
3044             if (argumentCountIncludingThis != 2)
3045                 return false;
3046 
3047             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3048                 return false;
3049 
3050             insertChecks();
3051             Node* map = get(virtualRegisterForArgument(0, registerOffset));
3052             Node* key = get(virtualRegisterForArgument(1, registerOffset));
3053             addToGraph(Check, Edge(key, ObjectUse));
3054             Node* hash = addToGraph(MapHash, key);
3055             Node* holder = addToGraph(WeakMapGet, Edge(map, WeakMapObjectUse), Edge(key, ObjectUse), Edge(hash, Int32Use));
3056             Node* invertedResult = addToGraph(IsEmpty, holder);
3057             Node* resultNode = addToGraph(LogicalNot, invertedResult);
3058 
3059             setResult(resultNode);
3060             return true;
3061         }
3062 
3063         case JSWeakSetHasIntrinsic: {
3064             if (argumentCountIncludingThis != 2)
3065                 return false;
3066 
3067             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3068                 return false;
3069 
3070             insertChecks();
3071             Node* map = get(virtualRegisterForArgument(0, registerOffset));
3072             Node* key = get(virtualRegisterForArgument(1, registerOffset));
3073             addToGraph(Check, Edge(key, ObjectUse));
3074             Node* hash = addToGraph(MapHash, key);
3075             Node* holder = addToGraph(WeakMapGet, Edge(map, WeakSetObjectUse), Edge(key, ObjectUse), Edge(hash, Int32Use));
3076             Node* invertedResult = addToGraph(IsEmpty, holder);
3077             Node* resultNode = addToGraph(LogicalNot, invertedResult);
3078 
3079             setResult(resultNode);
3080             return true;
3081         }
3082 
3083         case JSWeakSetAddIntrinsic: {
3084             if (argumentCountIncludingThis != 2)
3085                 return false;
3086 
3087             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3088                 return false;
3089 
3090             insertChecks();
3091             Node* base = get(virtualRegisterForArgument(0, registerOffset));
3092             Node* key = get(virtualRegisterForArgument(1, registerOffset));
3093             addToGraph(Check, Edge(key, ObjectUse));
3094             Node* hash = addToGraph(MapHash, key);
3095             addToGraph(WeakSetAdd, Edge(base, WeakSetObjectUse), Edge(key, ObjectUse), Edge(hash, Int32Use));
3096             setResult(base);
3097             return true;
3098         }
3099 
3100         case JSWeakMapSetIntrinsic: {
3101             if (argumentCountIncludingThis != 3)
3102                 return false;
3103 
3104             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3105                 return false;
3106 
3107             insertChecks();
3108             Node* base = get(virtualRegisterForArgument(0, registerOffset));
3109             Node* key = get(virtualRegisterForArgument(1, registerOffset));
3110             Node* value = get(virtualRegisterForArgument(2, registerOffset));
3111 
3112             addToGraph(Check, Edge(key, ObjectUse));
3113             Node* hash = addToGraph(MapHash, key);
3114 
3115             addVarArgChild(Edge(base, WeakMapObjectUse));
3116             addVarArgChild(Edge(key, ObjectUse));
3117             addVarArgChild(Edge(value));
3118             addVarArgChild(Edge(hash, Int32Use));
3119             addToGraph(Node::VarArg, WeakMapSet, OpInfo(0), OpInfo(0));
3120             setResult(base);
3121             return true;
3122         }
3123 
3124         case DataViewGetInt8:
3125         case DataViewGetUint8:
3126         case DataViewGetInt16:
3127         case DataViewGetUint16:
3128         case DataViewGetInt32:
3129         case DataViewGetUint32:
3130         case DataViewGetFloat32:
3131         case DataViewGetFloat64: {
3132             if (!is64Bit())
3133                 return false;
3134 
3135             // To inline data view accesses, we assume the architecture we&#39;re running on:
3136             // - Is little endian.
3137             // - Allows unaligned loads/stores without crashing.
3138 
3139             if (argumentCountIncludingThis &lt; 2)
3140                 return false;
3141             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3142                 return false;
3143 
3144             insertChecks();
3145 
3146             uint8_t byteSize;
3147             NodeType op = DataViewGetInt;
3148             bool isSigned = false;
3149             switch (intrinsic) {
3150             case DataViewGetInt8:
3151                 isSigned = true;
3152                 FALLTHROUGH;
3153             case DataViewGetUint8:
3154                 byteSize = 1;
3155                 break;
3156 
3157             case DataViewGetInt16:
3158                 isSigned = true;
3159                 FALLTHROUGH;
3160             case DataViewGetUint16:
3161                 byteSize = 2;
3162                 break;
3163 
3164             case DataViewGetInt32:
3165                 isSigned = true;
3166                 FALLTHROUGH;
3167             case DataViewGetUint32:
3168                 byteSize = 4;
3169                 break;
3170 
3171             case DataViewGetFloat32:
3172                 byteSize = 4;
3173                 op = DataViewGetFloat;
3174                 break;
3175             case DataViewGetFloat64:
3176                 byteSize = 8;
3177                 op = DataViewGetFloat;
3178                 break;
3179             default:
3180                 RELEASE_ASSERT_NOT_REACHED();
3181             }
3182 
3183             TriState isLittleEndian = MixedTriState;
3184             Node* littleEndianChild = nullptr;
3185             if (byteSize &gt; 1) {
3186                 if (argumentCountIncludingThis &lt; 3)
3187                     isLittleEndian = FalseTriState;
3188                 else {
3189                     littleEndianChild = get(virtualRegisterForArgument(2, registerOffset));
3190                     if (littleEndianChild-&gt;hasConstant()) {
3191                         JSValue constant = littleEndianChild-&gt;constant()-&gt;value();
3192                         isLittleEndian = constant.pureToBoolean();
3193                         if (isLittleEndian != MixedTriState)
3194                             littleEndianChild = nullptr;
3195                     } else
3196                         isLittleEndian = MixedTriState;
3197                 }
3198             }
3199 
3200             DataViewData data { };
3201             data.isLittleEndian = isLittleEndian;
3202             data.isSigned = isSigned;
3203             data.byteSize = byteSize;
3204 
3205             setResult(
3206                 addToGraph(op, OpInfo(data.asQuadWord), OpInfo(prediction), get(virtualRegisterForArgument(0, registerOffset)), get(virtualRegisterForArgument(1, registerOffset)), littleEndianChild));
3207             return true;
3208         }
3209 
3210         case DataViewSetInt8:
3211         case DataViewSetUint8:
3212         case DataViewSetInt16:
3213         case DataViewSetUint16:
3214         case DataViewSetInt32:
3215         case DataViewSetUint32:
3216         case DataViewSetFloat32:
3217         case DataViewSetFloat64: {
3218             if (!is64Bit())
3219                 return false;
3220 
3221             if (argumentCountIncludingThis &lt; 3)
3222                 return false;
3223 
3224             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3225                 return false;
3226 
3227             insertChecks();
3228 
3229             uint8_t byteSize;
3230             bool isFloatingPoint = false;
3231             bool isSigned = false;
3232             switch (intrinsic) {
3233             case DataViewSetInt8:
3234                 isSigned = true;
3235                 FALLTHROUGH;
3236             case DataViewSetUint8:
3237                 byteSize = 1;
3238                 break;
3239 
3240             case DataViewSetInt16:
3241                 isSigned = true;
3242                 FALLTHROUGH;
3243             case DataViewSetUint16:
3244                 byteSize = 2;
3245                 break;
3246 
3247             case DataViewSetInt32:
3248                 isSigned = true;
3249                 FALLTHROUGH;
3250             case DataViewSetUint32:
3251                 byteSize = 4;
3252                 break;
3253 
3254             case DataViewSetFloat32:
3255                 isFloatingPoint = true;
3256                 byteSize = 4;
3257                 break;
3258             case DataViewSetFloat64:
3259                 isFloatingPoint = true;
3260                 byteSize = 8;
3261                 break;
3262             default:
3263                 RELEASE_ASSERT_NOT_REACHED();
3264             }
3265 
3266             TriState isLittleEndian = MixedTriState;
3267             Node* littleEndianChild = nullptr;
3268             if (byteSize &gt; 1) {
3269                 if (argumentCountIncludingThis &lt; 4)
3270                     isLittleEndian = FalseTriState;
3271                 else {
3272                     littleEndianChild = get(virtualRegisterForArgument(3, registerOffset));
3273                     if (littleEndianChild-&gt;hasConstant()) {
3274                         JSValue constant = littleEndianChild-&gt;constant()-&gt;value();
3275                         isLittleEndian = constant.pureToBoolean();
3276                         if (isLittleEndian != MixedTriState)
3277                             littleEndianChild = nullptr;
3278                     } else
3279                         isLittleEndian = MixedTriState;
3280                 }
3281             }
3282 
3283             DataViewData data { };
3284             data.isLittleEndian = isLittleEndian;
3285             data.isSigned = isSigned;
3286             data.byteSize = byteSize;
3287             data.isFloatingPoint = isFloatingPoint;
3288 
3289             addVarArgChild(get(virtualRegisterForArgument(0, registerOffset)));
3290             addVarArgChild(get(virtualRegisterForArgument(1, registerOffset)));
3291             addVarArgChild(get(virtualRegisterForArgument(2, registerOffset)));
3292             addVarArgChild(littleEndianChild);
3293 
3294             addToGraph(Node::VarArg, DataViewSet, OpInfo(data.asQuadWord), OpInfo());
3295             setResult(addToGraph(JSConstant, OpInfo(m_constantUndefined)));
3296             return true;
3297         }
3298 
3299         case HasOwnPropertyIntrinsic: {
3300             if (argumentCountIncludingThis != 2)
3301                 return false;
3302 
3303             // This can be racy, that&#39;s fine. We know that once we observe that this is created,
3304             // that it will never be destroyed until the VM is destroyed. It&#39;s unlikely that
3305             // we&#39;d ever get to the point where we inline this as an intrinsic without the
3306             // cache being created, however, it&#39;s possible if we always throw exceptions inside
3307             // hasOwnProperty.
3308             if (!m_vm-&gt;hasOwnPropertyCache())
3309                 return false;
3310 
3311             insertChecks();
3312             Node* object = get(virtualRegisterForArgument(0, registerOffset));
3313             Node* key = get(virtualRegisterForArgument(1, registerOffset));
3314             Node* resultNode = addToGraph(HasOwnProperty, object, key);
3315             setResult(resultNode);
3316             return true;
3317         }
3318 
3319         case StringPrototypeSliceIntrinsic: {
3320             if (argumentCountIncludingThis &lt; 2)
3321                 return false;
3322 
3323             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3324                 return false;
3325 
3326             insertChecks();
3327             Node* thisString = get(virtualRegisterForArgument(0, registerOffset));
3328             Node* start = get(virtualRegisterForArgument(1, registerOffset));
3329             Node* end = nullptr;
3330             if (argumentCountIncludingThis &gt; 2)
3331                 end = get(virtualRegisterForArgument(2, registerOffset));
3332             Node* resultNode = addToGraph(StringSlice, thisString, start, end);
3333             setResult(resultNode);
3334             return true;
3335         }
3336 
3337         case StringPrototypeToLowerCaseIntrinsic: {
3338             if (argumentCountIncludingThis != 1)
3339                 return false;
3340 
3341             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3342                 return false;
3343 
3344             insertChecks();
3345             Node* thisString = get(virtualRegisterForArgument(0, registerOffset));
3346             Node* resultNode = addToGraph(ToLowerCase, thisString);
3347             setResult(resultNode);
3348             return true;
3349         }
3350 
3351         case NumberPrototypeToStringIntrinsic: {
3352             if (argumentCountIncludingThis != 1 &amp;&amp; argumentCountIncludingThis != 2)
3353                 return false;
3354 
3355             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3356                 return false;
3357 
3358             insertChecks();
3359             Node* thisNumber = get(virtualRegisterForArgument(0, registerOffset));
3360             if (argumentCountIncludingThis == 1) {
3361                 Node* resultNode = addToGraph(ToString, thisNumber);
3362                 setResult(resultNode);
3363             } else {
3364                 Node* radix = get(virtualRegisterForArgument(1, registerOffset));
3365                 Node* resultNode = addToGraph(NumberToStringWithRadix, thisNumber, radix);
3366                 setResult(resultNode);
3367             }
3368             return true;
3369         }
3370 
3371         case NumberIsIntegerIntrinsic: {
3372             if (argumentCountIncludingThis &lt; 2)
3373                 return false;
3374 
3375             insertChecks();
3376             Node* input = get(virtualRegisterForArgument(1, registerOffset));
3377             Node* resultNode = addToGraph(NumberIsInteger, input);
3378             setResult(resultNode);
3379             return true;
3380         }
3381 
3382         case CPUMfenceIntrinsic:
3383         case CPURdtscIntrinsic:
3384         case CPUCpuidIntrinsic:
3385         case CPUPauseIntrinsic: {
3386 #if CPU(X86_64)
3387             if (!m_graph.m_plan.isFTL())
3388                 return false;
3389             insertChecks();
3390             setResult(addToGraph(CPUIntrinsic, OpInfo(intrinsic), OpInfo()));
3391             return true;
3392 #else
3393             return false;
3394 #endif
3395         }
3396 
3397         default:
3398             return false;
3399         }
3400     };
3401 
3402     if (inlineIntrinsic()) {
3403         RELEASE_ASSERT(didSetResult);
3404         return true;
3405     }
3406 
3407     return false;
3408 }
3409 
3410 template&lt;typename ChecksFunctor&gt;
3411 bool ByteCodeParser::handleDOMJITCall(Node* callTarget, VirtualRegister result, const DOMJIT::Signature* signature, int registerOffset, int argumentCountIncludingThis, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks)
3412 {
3413     if (argumentCountIncludingThis != static_cast&lt;int&gt;(1 + signature-&gt;argumentCount))
3414         return false;
3415     if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3416         return false;
3417 
3418     // FIXME: Currently, we only support functions which arguments are up to 2.
3419     // Eventually, we should extend this. But possibly, 2 or 3 can cover typical use cases.
3420     // https://bugs.webkit.org/show_bug.cgi?id=164346
3421     ASSERT_WITH_MESSAGE(argumentCountIncludingThis &lt;= JSC_DOMJIT_SIGNATURE_MAX_ARGUMENTS_INCLUDING_THIS, &quot;Currently CallDOM does not support an arbitrary length arguments.&quot;);
3422 
3423     insertChecks();
3424     addCall(result, Call, signature, callTarget, argumentCountIncludingThis, registerOffset, prediction);
3425     return true;
3426 }
3427 
3428 
3429 template&lt;typename ChecksFunctor&gt;
3430 bool ByteCodeParser::handleIntrinsicGetter(VirtualRegister result, SpeculatedType prediction, const GetByIdVariant&amp; variant, Node* thisNode, const ChecksFunctor&amp; insertChecks)
3431 {
3432     switch (variant.intrinsic()) {
3433     case TypedArrayByteLengthIntrinsic: {
3434         insertChecks();
3435 
3436         TypedArrayType type = (*variant.structureSet().begin())-&gt;classInfo()-&gt;typedArrayStorageType;
3437         Array::Type arrayType = toArrayType(type);
3438         size_t logSize = logElementSize(type);
3439 
3440         variant.structureSet().forEach([&amp;] (Structure* structure) {
3441             TypedArrayType curType = structure-&gt;classInfo()-&gt;typedArrayStorageType;
3442             ASSERT(logSize == logElementSize(curType));
3443             arrayType = refineTypedArrayType(arrayType, curType);
3444             ASSERT(arrayType != Array::Generic);
3445         });
3446 
3447         Node* lengthNode = addToGraph(GetArrayLength, OpInfo(ArrayMode(arrayType, Array::Read).asWord()), thisNode);
3448 
3449         if (!logSize) {
3450             set(result, lengthNode);
3451             return true;
3452         }
3453 
3454         // We can use a BitLShift here because typed arrays will never have a byteLength
3455         // that overflows int32.
3456         Node* shiftNode = jsConstant(jsNumber(logSize));
3457         set(result, addToGraph(BitLShift, lengthNode, shiftNode));
3458 
3459         return true;
3460     }
3461 
3462     case TypedArrayLengthIntrinsic: {
3463         insertChecks();
3464 
3465         TypedArrayType type = (*variant.structureSet().begin())-&gt;classInfo()-&gt;typedArrayStorageType;
3466         Array::Type arrayType = toArrayType(type);
3467 
3468         variant.structureSet().forEach([&amp;] (Structure* structure) {
3469             TypedArrayType curType = structure-&gt;classInfo()-&gt;typedArrayStorageType;
3470             arrayType = refineTypedArrayType(arrayType, curType);
3471             ASSERT(arrayType != Array::Generic);
3472         });
3473 
3474         set(result, addToGraph(GetArrayLength, OpInfo(ArrayMode(arrayType, Array::Read).asWord()), thisNode));
3475 
3476         return true;
3477 
3478     }
3479 
3480     case TypedArrayByteOffsetIntrinsic: {
3481         insertChecks();
3482 
3483         TypedArrayType type = (*variant.structureSet().begin())-&gt;classInfo()-&gt;typedArrayStorageType;
3484         Array::Type arrayType = toArrayType(type);
3485 
3486         variant.structureSet().forEach([&amp;] (Structure* structure) {
3487             TypedArrayType curType = structure-&gt;classInfo()-&gt;typedArrayStorageType;
3488             arrayType = refineTypedArrayType(arrayType, curType);
3489             ASSERT(arrayType != Array::Generic);
3490         });
3491 
3492         set(result, addToGraph(GetTypedArrayByteOffset, OpInfo(ArrayMode(arrayType, Array::Read).asWord()), thisNode));
3493 
3494         return true;
3495     }
3496 
3497     case UnderscoreProtoIntrinsic: {
3498         insertChecks();
3499 
3500         bool canFold = !variant.structureSet().isEmpty();
3501         JSValue prototype;
3502         variant.structureSet().forEach([&amp;] (Structure* structure) {
3503             auto getPrototypeMethod = structure-&gt;classInfo()-&gt;methodTable.getPrototype;
3504             MethodTable::GetPrototypeFunctionPtr defaultGetPrototype = JSObject::getPrototype;
3505             if (getPrototypeMethod != defaultGetPrototype) {
3506                 canFold = false;
3507                 return;
3508             }
3509 
3510             if (structure-&gt;hasPolyProto()) {
3511                 canFold = false;
3512                 return;
3513             }
3514             if (!prototype)
3515                 prototype = structure-&gt;storedPrototype();
3516             else if (prototype != structure-&gt;storedPrototype())
3517                 canFold = false;
3518         });
3519 
3520         // OK, only one prototype is found. We perform constant folding here.
3521         // This information is important for super&#39;s constructor call to get new.target constant.
3522         if (prototype &amp;&amp; canFold) {
3523             set(result, weakJSConstant(prototype));
3524             return true;
3525         }
3526 
3527         set(result, addToGraph(GetPrototypeOf, OpInfo(0), OpInfo(prediction), thisNode));
3528         return true;
3529     }
3530 
3531     default:
3532         return false;
3533     }
3534     RELEASE_ASSERT_NOT_REACHED();
3535 }
3536 
3537 static void blessCallDOMGetter(Node* node)
3538 {
3539     DOMJIT::CallDOMGetterSnippet* snippet = node-&gt;callDOMGetterData()-&gt;snippet;
3540     if (snippet &amp;&amp; !snippet-&gt;effect.mustGenerate())
3541         node-&gt;clearFlags(NodeMustGenerate);
3542 }
3543 
3544 bool ByteCodeParser::handleDOMJITGetter(VirtualRegister result, const GetByIdVariant&amp; variant, Node* thisNode, unsigned identifierNumber, SpeculatedType prediction)
3545 {
3546     if (!variant.domAttribute())
3547         return false;
3548 
3549     auto domAttribute = variant.domAttribute().value();
3550 
3551     // We do not need to actually look up CustomGetterSetter here. Checking Structures or registering watchpoints are enough,
3552     // since replacement of CustomGetterSetter always incurs Structure transition.
3553     if (!check(variant.conditionSet()))
3554         return false;
3555     addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(variant.structureSet())), thisNode);
3556 
3557     // We do not need to emit CheckCell thingy here. When the custom accessor is replaced to different one, Structure transition occurs.
3558     addToGraph(CheckSubClass, OpInfo(domAttribute.classInfo), thisNode);
3559 
3560     bool wasSeenInJIT = true;
3561     addToGraph(FilterGetByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addGetByIdStatus(currentCodeOrigin(), GetByIdStatus(GetByIdStatus::Custom, wasSeenInJIT, variant))), thisNode);
3562 
3563     CallDOMGetterData* callDOMGetterData = m_graph.m_callDOMGetterData.add();
3564     callDOMGetterData-&gt;customAccessorGetter = variant.customAccessorGetter();
3565     ASSERT(callDOMGetterData-&gt;customAccessorGetter);
3566 
3567     if (const auto* domJIT = domAttribute.domJIT) {
3568         callDOMGetterData-&gt;domJIT = domJIT;
3569         Ref&lt;DOMJIT::CallDOMGetterSnippet&gt; snippet = domJIT-&gt;compiler()();
3570         callDOMGetterData-&gt;snippet = snippet.ptr();
3571         m_graph.m_domJITSnippets.append(WTFMove(snippet));
3572     }
3573     DOMJIT::CallDOMGetterSnippet* callDOMGetterSnippet = callDOMGetterData-&gt;snippet;
3574     callDOMGetterData-&gt;identifierNumber = identifierNumber;
3575 
3576     Node* callDOMGetterNode = nullptr;
3577     // GlobalObject of thisNode is always used to create a DOMWrapper.
3578     if (callDOMGetterSnippet &amp;&amp; callDOMGetterSnippet-&gt;requireGlobalObject) {
3579         Node* globalObject = addToGraph(GetGlobalObject, thisNode);
3580         callDOMGetterNode = addToGraph(CallDOMGetter, OpInfo(callDOMGetterData), OpInfo(prediction), thisNode, globalObject);
3581     } else
3582         callDOMGetterNode = addToGraph(CallDOMGetter, OpInfo(callDOMGetterData), OpInfo(prediction), thisNode);
3583     blessCallDOMGetter(callDOMGetterNode);
3584     set(result, callDOMGetterNode);
3585     return true;
3586 }
3587 
3588 bool ByteCodeParser::handleModuleNamespaceLoad(VirtualRegister result, SpeculatedType prediction, Node* base, GetByIdStatus getById)
3589 {
3590     if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell))
3591         return false;
3592     addToGraph(CheckCell, OpInfo(m_graph.freeze(getById.moduleNamespaceObject())), Edge(base, CellUse));
3593 
3594     addToGraph(FilterGetByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addGetByIdStatus(currentCodeOrigin(), getById)), base);
3595 
3596     // Ideally we wouldn&#39;t have to do this Phantom. But:
3597     //
3598     // For the constant case: we must do it because otherwise we would have no way of knowing
3599     // that the scope is live at OSR here.
3600     //
3601     // For the non-constant case: GetClosureVar could be DCE&#39;d, but baseline&#39;s implementation
3602     // won&#39;t be able to handle an Undefined scope.
3603     addToGraph(Phantom, base);
3604 
3605     // Constant folding in the bytecode parser is important for performance. This may not
3606     // have executed yet. If it hasn&#39;t, then we won&#39;t have a prediction. Lacking a
3607     // prediction, we&#39;d otherwise think that it has to exit. Then when it did execute, we
3608     // would recompile. But if we can fold it here, we avoid the exit.
3609     m_graph.freeze(getById.moduleEnvironment());
3610     if (JSValue value = m_graph.tryGetConstantClosureVar(getById.moduleEnvironment(), getById.scopeOffset())) {
3611         set(result, weakJSConstant(value));
3612         return true;
3613     }
3614     set(result, addToGraph(GetClosureVar, OpInfo(getById.scopeOffset().offset()), OpInfo(prediction), weakJSConstant(getById.moduleEnvironment())));
3615     return true;
3616 }
3617 
3618 template&lt;typename ChecksFunctor&gt;
3619 bool ByteCodeParser::handleTypedArrayConstructor(
3620     VirtualRegister result, InternalFunction* function, int registerOffset,
3621     int argumentCountIncludingThis, TypedArrayType type, const ChecksFunctor&amp; insertChecks)
3622 {
3623     if (!isTypedView(type))
3624         return false;
3625 
3626     if (function-&gt;classInfo() != constructorClassInfoForType(type))
3627         return false;
3628 
3629     if (function-&gt;globalObject(*m_vm) != m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject())
3630         return false;
3631 
3632     // We only have an intrinsic for the case where you say:
3633     //
3634     // new FooArray(blah);
3635     //
3636     // Of course, &#39;blah&#39; could be any of the following:
3637     //
3638     // - Integer, indicating that you want to allocate an array of that length.
3639     //   This is the thing we&#39;re hoping for, and what we can actually do meaningful
3640     //   optimizations for.
3641     //
3642     // - Array buffer, indicating that you want to create a view onto that _entire_
3643     //   buffer.
3644     //
3645     // - Non-buffer object, indicating that you want to create a copy of that
3646     //   object by pretending that it quacks like an array.
3647     //
3648     // - Anything else, indicating that you want to have an exception thrown at
3649     //   you.
3650     //
3651     // The intrinsic, NewTypedArray, will behave as if it could do any of these
3652     // things up until we do Fixup. Thereafter, if child1 (i.e. &#39;blah&#39;) is
3653     // predicted Int32, then we lock it in as a normal typed array allocation.
3654     // Otherwise, NewTypedArray turns into a totally opaque function call that
3655     // may clobber the world - by virtue of it accessing properties on what could
3656     // be an object.
3657     //
3658     // Note that although the generic form of NewTypedArray sounds sort of awful,
3659     // it is actually quite likely to be more efficient than a fully generic
3660     // Construct. So, we might want to think about making NewTypedArray variadic,
3661     // or else making Construct not super slow.
3662 
3663     if (argumentCountIncludingThis != 2)
3664         return false;
3665 
3666     if (!function-&gt;globalObject(*m_vm)-&gt;typedArrayStructureConcurrently(type))
3667         return false;
3668 
3669     insertChecks();
3670     set(result,
3671         addToGraph(NewTypedArray, OpInfo(type), get(virtualRegisterForArgument(1, registerOffset))));
3672     return true;
3673 }
3674 
3675 template&lt;typename ChecksFunctor&gt;
3676 bool ByteCodeParser::handleConstantInternalFunction(
3677     Node* callTargetNode, VirtualRegister result, InternalFunction* function, int registerOffset,
3678     int argumentCountIncludingThis, CodeSpecializationKind kind, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks)
3679 {
3680     VERBOSE_LOG(&quot;    Handling constant internal function &quot;, JSValue(function), &quot;\n&quot;);
3681 
3682     // It so happens that the code below assumes that the result operand is valid. It&#39;s extremely
3683     // unlikely that the result operand would be invalid - you&#39;d have to call this via a setter call.
3684     if (!result.isValid())
3685         return false;
3686 
3687     if (kind == CodeForConstruct) {
3688         Node* newTargetNode = get(virtualRegisterForArgument(0, registerOffset));
3689         // We cannot handle the case where new.target != callee (i.e. a construct from a super call) because we
3690         // don&#39;t know what the prototype of the constructed object will be.
3691         // FIXME: If we have inlined super calls up to the call site, however, we should be able to figure out the structure. https://bugs.webkit.org/show_bug.cgi?id=152700
3692         if (newTargetNode != callTargetNode)
3693             return false;
3694     }
3695 
3696     if (function-&gt;classInfo() == ArrayConstructor::info()) {
3697         if (function-&gt;globalObject(*m_vm) != m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject())
3698             return false;
3699 
3700         insertChecks();
3701         if (argumentCountIncludingThis == 2) {
3702             set(result,
3703                 addToGraph(NewArrayWithSize, OpInfo(ArrayWithUndecided), get(virtualRegisterForArgument(1, registerOffset))));
3704             return true;
3705         }
3706 
3707         for (int i = 1; i &lt; argumentCountIncludingThis; ++i)
3708             addVarArgChild(get(virtualRegisterForArgument(i, registerOffset)));
3709         set(result,
3710             addToGraph(Node::VarArg, NewArray, OpInfo(ArrayWithUndecided), OpInfo(argumentCountIncludingThis - 1)));
3711         return true;
3712     }
3713 
3714     if (function-&gt;classInfo() == NumberConstructor::info()) {
3715         if (kind == CodeForConstruct)
3716             return false;
3717 
3718         insertChecks();
3719         if (argumentCountIncludingThis &lt;= 1)
3720             set(result, jsConstant(jsNumber(0)));
3721         else
3722             set(result, addToGraph(ToNumber, OpInfo(0), OpInfo(prediction), get(virtualRegisterForArgument(1, registerOffset))));
3723 
3724         return true;
3725     }
3726 
3727     if (function-&gt;classInfo() == StringConstructor::info()) {
3728         insertChecks();
3729 
3730         Node* resultNode;
3731 
3732         if (argumentCountIncludingThis &lt;= 1)
3733             resultNode = jsConstant(m_vm-&gt;smallStrings.emptyString());
3734         else
3735             resultNode = addToGraph(CallStringConstructor, get(virtualRegisterForArgument(1, registerOffset)));
3736 
3737         if (kind == CodeForConstruct)
3738             resultNode = addToGraph(NewStringObject, OpInfo(m_graph.registerStructure(function-&gt;globalObject(*m_vm)-&gt;stringObjectStructure())), resultNode);
3739 
3740         set(result, resultNode);
3741         return true;
3742     }
3743 
3744     if (function-&gt;classInfo() == SymbolConstructor::info() &amp;&amp; kind == CodeForCall) {
3745         insertChecks();
3746 
3747         Node* resultNode;
3748 
3749         if (argumentCountIncludingThis &lt;= 1)
3750             resultNode = addToGraph(NewSymbol);
3751         else
3752             resultNode = addToGraph(NewSymbol, addToGraph(ToString, get(virtualRegisterForArgument(1, registerOffset))));
3753 
3754         set(result, resultNode);
3755         return true;
3756     }
3757 
3758     // FIXME: This should handle construction as well. https://bugs.webkit.org/show_bug.cgi?id=155591
3759     if (function-&gt;classInfo() == ObjectConstructor::info() &amp;&amp; kind == CodeForCall) {
3760         insertChecks();
3761 
3762         Node* resultNode;
3763         if (argumentCountIncludingThis &lt;= 1)
3764             resultNode = addToGraph(NewObject, OpInfo(m_graph.registerStructure(function-&gt;globalObject(*m_vm)-&gt;objectStructureForObjectConstructor())));
3765         else
3766             resultNode = addToGraph(CallObjectConstructor, OpInfo(m_graph.freeze(function-&gt;globalObject(*m_vm))), OpInfo(prediction), get(virtualRegisterForArgument(1, registerOffset)));
3767         set(result, resultNode);
3768         return true;
3769     }
3770 
3771     for (unsigned typeIndex = 0; typeIndex &lt; NumberOfTypedArrayTypes; ++typeIndex) {
3772         bool handled = handleTypedArrayConstructor(
3773             result, function, registerOffset, argumentCountIncludingThis,
3774             indexToTypedArrayType(typeIndex), insertChecks);
3775         if (handled)
3776             return true;
3777     }
3778 
3779     return false;
3780 }
3781 
3782 Node* ByteCodeParser::handleGetByOffset(
3783     SpeculatedType prediction, Node* base, unsigned identifierNumber, PropertyOffset offset, NodeType op)
3784 {
3785     Node* propertyStorage;
3786     if (isInlineOffset(offset))
3787         propertyStorage = base;
3788     else
3789         propertyStorage = addToGraph(GetButterfly, base);
3790 
3791     StorageAccessData* data = m_graph.m_storageAccessData.add();
3792     data-&gt;offset = offset;
3793     data-&gt;identifierNumber = identifierNumber;
3794 
3795     Node* getByOffset = addToGraph(op, OpInfo(data), OpInfo(prediction), propertyStorage, base);
3796 
3797     return getByOffset;
3798 }
3799 
3800 Node* ByteCodeParser::handlePutByOffset(
3801     Node* base, unsigned identifier, PropertyOffset offset,
3802     Node* value)
3803 {
3804     Node* propertyStorage;
3805     if (isInlineOffset(offset))
3806         propertyStorage = base;
3807     else
3808         propertyStorage = addToGraph(GetButterfly, base);
3809 
3810     StorageAccessData* data = m_graph.m_storageAccessData.add();
3811     data-&gt;offset = offset;
3812     data-&gt;identifierNumber = identifier;
3813 
3814     Node* result = addToGraph(PutByOffset, OpInfo(data), propertyStorage, base, value);
3815 
3816     return result;
3817 }
3818 
3819 bool ByteCodeParser::check(const ObjectPropertyCondition&amp; condition)
3820 {
3821     if (!condition)
3822         return false;
3823 
3824     if (m_graph.watchCondition(condition))
3825         return true;
3826 
3827     Structure* structure = condition.object()-&gt;structure(*m_vm);
3828     if (!condition.structureEnsuresValidity(structure))
3829         return false;
3830 
3831     addToGraph(
3832         CheckStructure,
3833         OpInfo(m_graph.addStructureSet(structure)),
3834         weakJSConstant(condition.object()));
3835     return true;
3836 }
3837 
3838 GetByOffsetMethod ByteCodeParser::promoteToConstant(GetByOffsetMethod method)
3839 {
3840     if (method.kind() == GetByOffsetMethod::LoadFromPrototype
3841         &amp;&amp; method.prototype()-&gt;structure()-&gt;dfgShouldWatch()) {
3842         if (JSValue constant = m_graph.tryGetConstantProperty(method.prototype()-&gt;value(), method.prototype()-&gt;structure(), method.offset()))
3843             return GetByOffsetMethod::constant(m_graph.freeze(constant));
3844     }
3845 
3846     return method;
3847 }
3848 
3849 bool ByteCodeParser::needsDynamicLookup(ResolveType type, OpcodeID opcode)
3850 {
3851     ASSERT(opcode == op_resolve_scope || opcode == op_get_from_scope || opcode == op_put_to_scope);
3852 
3853     JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
3854     if (needsVarInjectionChecks(type) &amp;&amp; globalObject-&gt;varInjectionWatchpoint()-&gt;hasBeenInvalidated())
3855         return true;
3856 
3857     switch (type) {
3858     case GlobalProperty:
3859     case GlobalVar:
3860     case GlobalLexicalVar:
3861     case ClosureVar:
3862     case LocalClosureVar:
3863     case ModuleVar:
3864         return false;
3865 
3866     case UnresolvedProperty:
3867     case UnresolvedPropertyWithVarInjectionChecks: {
3868         // The heuristic for UnresolvedProperty scope accesses is we will ForceOSRExit if we
3869         // haven&#39;t exited from from this access before to let the baseline JIT try to better
3870         // cache the access. If we&#39;ve already exited from this operation, it&#39;s unlikely that
3871         // the baseline will come up with a better ResolveType and instead we will compile
3872         // this as a dynamic scope access.
3873 
3874         // We only track our heuristic through resolve_scope since resolve_scope will
3875         // dominate unresolved gets/puts on that scope.
3876         if (opcode != op_resolve_scope)
3877             return true;
3878 
3879         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, InadequateCoverage)) {
3880             // We&#39;ve already exited so give up on getting better ResolveType information.
3881             return true;
3882         }
3883 
3884         // We have not exited yet, so let&#39;s have the baseline get better ResolveType information for us.
3885         // This type of code is often seen when we tier up in a loop but haven&#39;t executed the part
3886         // of a function that comes after the loop.
3887         return false;
3888     }
3889 
3890     case Dynamic:
3891         return true;
3892 
3893     case GlobalPropertyWithVarInjectionChecks:
3894     case GlobalVarWithVarInjectionChecks:
3895     case GlobalLexicalVarWithVarInjectionChecks:
3896     case ClosureVarWithVarInjectionChecks:
3897         return false;
3898     }
3899 
3900     ASSERT_NOT_REACHED();
3901     return false;
3902 }
3903 
3904 GetByOffsetMethod ByteCodeParser::planLoad(const ObjectPropertyCondition&amp; condition)
3905 {
3906     VERBOSE_LOG(&quot;Planning a load: &quot;, condition, &quot;\n&quot;);
3907 
3908     // We might promote this to Equivalence, and a later DFG pass might also do such promotion
3909     // even if we fail, but for simplicity this cannot be asked to load an equivalence condition.
3910     // None of the clients of this method will request a load of an Equivalence condition anyway,
3911     // and supporting it would complicate the heuristics below.
3912     RELEASE_ASSERT(condition.kind() == PropertyCondition::Presence);
3913 
3914     // Here&#39;s the ranking of how to handle this, from most preferred to least preferred:
3915     //
3916     // 1) Watchpoint on an equivalence condition and return a constant node for the loaded value.
3917     //    No other code is emitted, and the structure of the base object is never registered.
3918     //    Hence this results in zero code and we won&#39;t jettison this compilation if the object
3919     //    transitions, even if the structure is watchable right now.
3920     //
3921     // 2) Need to emit a load, and the current structure of the base is going to be watched by the
3922     //    DFG anyway (i.e. dfgShouldWatch). Watch the structure and emit the load. Don&#39;t watch the
3923     //    condition, since the act of turning the base into a constant in IR will cause the DFG to
3924     //    watch the structure anyway and doing so would subsume watching the condition.
3925     //
3926     // 3) Need to emit a load, and the current structure of the base is watchable but not by the
3927     //    DFG (i.e. transitionWatchpointSetIsStillValid() and !dfgShouldWatchIfPossible()). Watch
3928     //    the condition, and emit a load.
3929     //
3930     // 4) Need to emit a load, and the current structure of the base is not watchable. Emit a
3931     //    structure check, and emit a load.
3932     //
3933     // 5) The condition does not hold. Give up and return null.
3934 
3935     // First, try to promote Presence to Equivalence. We do this before doing anything else
3936     // because it&#39;s the most profitable. Also, there are cases where the presence is watchable but
3937     // we don&#39;t want to watch it unless it became an equivalence (see the relationship between
3938     // (1), (2), and (3) above).
3939     ObjectPropertyCondition equivalenceCondition = condition.attemptToMakeEquivalenceWithoutBarrier(*m_vm);
3940     if (m_graph.watchCondition(equivalenceCondition))
3941         return GetByOffsetMethod::constant(m_graph.freeze(equivalenceCondition.requiredValue()));
3942 
3943     // At this point, we&#39;ll have to materialize the condition&#39;s base as a constant in DFG IR. Once
3944     // we do this, the frozen value will have its own idea of what the structure is. Use that from
3945     // now on just because it&#39;s less confusing.
3946     FrozenValue* base = m_graph.freeze(condition.object());
3947     Structure* structure = base-&gt;structure();
3948 
3949     // Check if the structure that we&#39;ve registered makes the condition hold. If not, just give
3950     // up. This is case (5) above.
3951     if (!condition.structureEnsuresValidity(structure))
3952         return GetByOffsetMethod();
3953 
3954     // If the structure is watched by the DFG already, then just use this fact to emit the load.
3955     // This is case (2) above.
3956     if (structure-&gt;dfgShouldWatch())
3957         return promoteToConstant(GetByOffsetMethod::loadFromPrototype(base, condition.offset()));
3958 
3959     // If we can watch the condition right now, then we can emit the load after watching it. This
3960     // is case (3) above.
3961     if (m_graph.watchCondition(condition))
3962         return promoteToConstant(GetByOffsetMethod::loadFromPrototype(base, condition.offset()));
3963 
3964     // We can&#39;t watch anything but we know that the current structure satisfies the condition. So,
3965     // check for that structure and then emit the load.
3966     addToGraph(
3967         CheckStructure,
3968         OpInfo(m_graph.addStructureSet(structure)),
3969         addToGraph(JSConstant, OpInfo(base)));
3970     return promoteToConstant(GetByOffsetMethod::loadFromPrototype(base, condition.offset()));
3971 }
3972 
3973 Node* ByteCodeParser::load(
3974     SpeculatedType prediction, unsigned identifierNumber, const GetByOffsetMethod&amp; method,
3975     NodeType op)
3976 {
3977     switch (method.kind()) {
3978     case GetByOffsetMethod::Invalid:
3979         return nullptr;
3980     case GetByOffsetMethod::Constant:
3981         return addToGraph(JSConstant, OpInfo(method.constant()));
3982     case GetByOffsetMethod::LoadFromPrototype: {
3983         Node* baseNode = addToGraph(JSConstant, OpInfo(method.prototype()));
3984         return handleGetByOffset(
3985             prediction, baseNode, identifierNumber, method.offset(), op);
3986     }
3987     case GetByOffsetMethod::Load:
3988         // Will never see this from planLoad().
3989         RELEASE_ASSERT_NOT_REACHED();
3990         return nullptr;
3991     }
3992 
3993     RELEASE_ASSERT_NOT_REACHED();
3994     return nullptr;
3995 }
3996 
3997 Node* ByteCodeParser::load(
3998     SpeculatedType prediction, const ObjectPropertyCondition&amp; condition, NodeType op)
3999 {
4000     GetByOffsetMethod method = planLoad(condition);
4001     return load(prediction, m_graph.identifiers().ensure(condition.uid()), method, op);
4002 }
4003 
4004 bool ByteCodeParser::check(const ObjectPropertyConditionSet&amp; conditionSet)
4005 {
4006     for (const ObjectPropertyCondition&amp; condition : conditionSet) {
4007         if (!check(condition))
4008             return false;
4009     }
4010     return true;
4011 }
4012 
4013 GetByOffsetMethod ByteCodeParser::planLoad(const ObjectPropertyConditionSet&amp; conditionSet)
4014 {
4015     VERBOSE_LOG(&quot;conditionSet = &quot;, conditionSet, &quot;\n&quot;);
4016 
4017     GetByOffsetMethod result;
4018     for (const ObjectPropertyCondition&amp; condition : conditionSet) {
4019         switch (condition.kind()) {
4020         case PropertyCondition::Presence:
4021             RELEASE_ASSERT(!result); // Should only see exactly one of these.
4022             result = planLoad(condition);
4023             if (!result)
4024                 return GetByOffsetMethod();
4025             break;
4026         default:
4027             if (!check(condition))
4028                 return GetByOffsetMethod();
4029             break;
4030         }
4031     }
4032     if (!result) {
4033         // We have a unset property.
4034         ASSERT(!conditionSet.numberOfConditionsWithKind(PropertyCondition::Presence));
4035         return GetByOffsetMethod::constant(m_constantUndefined);
4036     }
4037     return result;
4038 }
4039 
4040 Node* ByteCodeParser::load(
4041     SpeculatedType prediction, const ObjectPropertyConditionSet&amp; conditionSet, NodeType op)
4042 {
4043     GetByOffsetMethod method = planLoad(conditionSet);
4044     return load(
4045         prediction,
4046         m_graph.identifiers().ensure(conditionSet.slotBaseCondition().uid()),
4047         method, op);
4048 }
4049 
4050 ObjectPropertyCondition ByteCodeParser::presenceLike(
4051     JSObject* knownBase, UniquedStringImpl* uid, PropertyOffset offset, const StructureSet&amp; set)
4052 {
4053     if (set.isEmpty())
4054         return ObjectPropertyCondition();
4055     unsigned attributes;
4056     PropertyOffset firstOffset = set[0]-&gt;getConcurrently(uid, attributes);
4057     if (firstOffset != offset)
4058         return ObjectPropertyCondition();
4059     for (unsigned i = 1; i &lt; set.size(); ++i) {
4060         unsigned otherAttributes;
4061         PropertyOffset otherOffset = set[i]-&gt;getConcurrently(uid, otherAttributes);
4062         if (otherOffset != offset || otherAttributes != attributes)
4063             return ObjectPropertyCondition();
4064     }
4065     return ObjectPropertyCondition::presenceWithoutBarrier(knownBase, uid, offset, attributes);
4066 }
4067 
4068 bool ByteCodeParser::checkPresenceLike(
4069     JSObject* knownBase, UniquedStringImpl* uid, PropertyOffset offset, const StructureSet&amp; set)
4070 {
4071     return check(presenceLike(knownBase, uid, offset, set));
4072 }
4073 
4074 void ByteCodeParser::checkPresenceLike(
4075     Node* base, UniquedStringImpl* uid, PropertyOffset offset, const StructureSet&amp; set)
4076 {
4077     if (JSObject* knownBase = base-&gt;dynamicCastConstant&lt;JSObject*&gt;(*m_vm)) {
4078         if (checkPresenceLike(knownBase, uid, offset, set))
4079             return;
4080     }
4081 
4082     addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(set)), base);
4083 }
4084 
4085 template&lt;typename VariantType&gt;
4086 Node* ByteCodeParser::load(
4087     SpeculatedType prediction, Node* base, unsigned identifierNumber, const VariantType&amp; variant)
4088 {
4089     // Make sure backwards propagation knows that we&#39;ve used base.
4090     addToGraph(Phantom, base);
4091 
4092     bool needStructureCheck = true;
4093 
4094     UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
4095 
4096     if (JSObject* knownBase = base-&gt;dynamicCastConstant&lt;JSObject*&gt;(*m_vm)) {
4097         // Try to optimize away the structure check. Note that it&#39;s not worth doing anything about this
4098         // if the base&#39;s structure is watched.
4099         Structure* structure = base-&gt;constant()-&gt;structure();
4100         if (!structure-&gt;dfgShouldWatch()) {
4101             if (!variant.conditionSet().isEmpty()) {
4102                 // This means that we&#39;re loading from a prototype or we have a property miss. We expect
4103                 // the base not to have the property. We can only use ObjectPropertyCondition if all of
4104                 // the structures in the variant.structureSet() agree on the prototype (it would be
4105                 // hilariously rare if they didn&#39;t). Note that we are relying on structureSet() having
4106                 // at least one element. That will always be true here because of how GetByIdStatus/PutByIdStatus work.
4107 
4108                 // FIXME: right now, if we have an OPCS, we have mono proto. However, this will
4109                 // need to be changed in the future once we have a hybrid data structure for
4110                 // poly proto:
4111                 // https://bugs.webkit.org/show_bug.cgi?id=177339
4112                 JSObject* prototype = variant.structureSet()[0]-&gt;storedPrototypeObject();
4113                 bool allAgree = true;
4114                 for (unsigned i = 1; i &lt; variant.structureSet().size(); ++i) {
4115                     if (variant.structureSet()[i]-&gt;storedPrototypeObject() != prototype) {
4116                         allAgree = false;
4117                         break;
4118                     }
4119                 }
4120                 if (allAgree) {
4121                     ObjectPropertyCondition condition = ObjectPropertyCondition::absenceWithoutBarrier(
4122                         knownBase, uid, prototype);
4123                     if (check(condition))
4124                         needStructureCheck = false;
4125                 }
4126             } else {
4127                 // This means we&#39;re loading directly from base. We can avoid all of the code that follows
4128                 // if we can prove that the property is a constant. Otherwise, we try to prove that the
4129                 // property is watchably present, in which case we get rid of the structure check.
4130 
4131                 ObjectPropertyCondition presenceCondition =
4132                     presenceLike(knownBase, uid, variant.offset(), variant.structureSet());
4133                 if (presenceCondition) {
4134                     ObjectPropertyCondition equivalenceCondition =
4135                         presenceCondition.attemptToMakeEquivalenceWithoutBarrier(*m_vm);
4136                     if (m_graph.watchCondition(equivalenceCondition))
4137                         return weakJSConstant(equivalenceCondition.requiredValue());
4138 
4139                     if (check(presenceCondition))
4140                         needStructureCheck = false;
4141                 }
4142             }
4143         }
4144     }
4145 
4146     if (needStructureCheck)
4147         addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(variant.structureSet())), base);
4148 
4149     if (variant.isPropertyUnset()) {
4150         if (m_graph.watchConditions(variant.conditionSet()))
4151             return jsConstant(jsUndefined());
4152         return nullptr;
4153     }
4154 
4155     SpeculatedType loadPrediction;
4156     NodeType loadOp;
4157     if (variant.callLinkStatus() || variant.intrinsic() != NoIntrinsic) {
4158         loadPrediction = SpecCellOther;
4159         loadOp = GetGetterSetterByOffset;
4160     } else {
4161         loadPrediction = prediction;
4162         loadOp = GetByOffset;
4163     }
4164 
4165     Node* loadedValue;
4166     if (!variant.conditionSet().isEmpty())
4167         loadedValue = load(loadPrediction, variant.conditionSet(), loadOp);
4168     else {
4169         if (needStructureCheck &amp;&amp; base-&gt;hasConstant()) {
4170             // We did emit a structure check. That means that we have an opportunity to do constant folding
4171             // here, since we didn&#39;t do it above.
4172             JSValue constant = m_graph.tryGetConstantProperty(
4173                 base-&gt;asJSValue(), *m_graph.addStructureSet(variant.structureSet()), variant.offset());
4174             if (constant)
4175                 return weakJSConstant(constant);
4176         }
4177 
4178         loadedValue = handleGetByOffset(
4179             loadPrediction, base, identifierNumber, variant.offset(), loadOp);
4180     }
4181 
4182     return loadedValue;
4183 }
4184 
4185 Node* ByteCodeParser::store(Node* base, unsigned identifier, const PutByIdVariant&amp; variant, Node* value)
4186 {
4187     RELEASE_ASSERT(variant.kind() == PutByIdVariant::Replace);
4188 
4189     checkPresenceLike(base, m_graph.identifiers()[identifier], variant.offset(), variant.structure());
4190     return handlePutByOffset(base, identifier, variant.offset(), value);
4191 }
4192 
4193 void ByteCodeParser::handleGetById(
4194     VirtualRegister destination, SpeculatedType prediction, Node* base, unsigned identifierNumber,
4195     GetByIdStatus getByIdStatus, AccessType type, unsigned instructionSize)
4196 {
4197     // Attempt to reduce the set of things in the GetByIdStatus.
4198     if (base-&gt;op() == NewObject) {
4199         bool ok = true;
4200         for (unsigned i = m_currentBlock-&gt;size(); i--;) {
4201             Node* node = m_currentBlock-&gt;at(i);
4202             if (node == base)
4203                 break;
4204             if (writesOverlap(m_graph, node, JSCell_structureID)) {
4205                 ok = false;
4206                 break;
4207             }
4208         }
4209         if (ok)
4210             getByIdStatus.filter(base-&gt;structure().get());
4211     }
4212 
4213     NodeType getById;
4214     if (type == AccessType::Get)
4215         getById = getByIdStatus.makesCalls() ? GetByIdFlush : GetById;
4216     else if (type == AccessType::TryGet)
4217         getById = TryGetById;
4218     else
4219         getById = getByIdStatus.makesCalls() ? GetByIdDirectFlush : GetByIdDirect;
4220 
4221     if (getById != TryGetById &amp;&amp; getByIdStatus.isModuleNamespace()) {
4222         if (handleModuleNamespaceLoad(destination, prediction, base, getByIdStatus)) {
4223             if (UNLIKELY(m_graph.compilation()))
4224                 m_graph.compilation()-&gt;noticeInlinedGetById();
4225             return;
4226         }
4227     }
4228 
4229     // Special path for custom accessors since custom&#39;s offset does not have any meanings.
4230     // So, this is completely different from Simple one. But we have a chance to optimize it when we use DOMJIT.
4231     if (Options::useDOMJIT() &amp;&amp; getByIdStatus.isCustom()) {
4232         ASSERT(getByIdStatus.numVariants() == 1);
4233         ASSERT(!getByIdStatus.makesCalls());
4234         GetByIdVariant variant = getByIdStatus[0];
4235         ASSERT(variant.domAttribute());
4236         if (handleDOMJITGetter(destination, variant, base, identifierNumber, prediction)) {
4237             if (UNLIKELY(m_graph.compilation()))
4238                 m_graph.compilation()-&gt;noticeInlinedGetById();
4239             return;
4240         }
4241     }
4242 
4243     ASSERT(type == AccessType::Get || type == AccessType::GetDirect ||  !getByIdStatus.makesCalls());
4244     if (!getByIdStatus.isSimple() || !getByIdStatus.numVariants() || !Options::useAccessInlining()) {
4245         set(destination,
4246             addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4247         return;
4248     }
4249 
4250     // FIXME: If we use the GetByIdStatus for anything then we should record it and insert a node
4251     // after everything else (like the GetByOffset or whatever) that will filter the recorded
4252     // GetByIdStatus. That means that the constant folder also needs to do the same!
4253 
4254     if (getByIdStatus.numVariants() &gt; 1) {
4255         if (getByIdStatus.makesCalls() || !m_graph.m_plan.isFTL()
4256             || !Options::usePolymorphicAccessInlining()
4257             || getByIdStatus.numVariants() &gt; Options::maxPolymorphicAccessInliningListSize()) {
4258             set(destination,
4259                 addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4260             return;
4261         }
4262 
4263         addToGraph(FilterGetByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addGetByIdStatus(currentCodeOrigin(), getByIdStatus)), base);
4264 
4265         Vector&lt;MultiGetByOffsetCase, 2&gt; cases;
4266 
4267         // 1) Emit prototype structure checks for all chains. This could sort of maybe not be
4268         //    optimal, if there is some rarely executed case in the chain that requires a lot
4269         //    of checks and those checks are not watchpointable.
4270         for (const GetByIdVariant&amp; variant : getByIdStatus.variants()) {
4271             if (variant.intrinsic() != NoIntrinsic) {
4272                 set(destination,
4273                     addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4274                 return;
4275             }
4276 
4277             if (variant.conditionSet().isEmpty()) {
4278                 cases.append(
4279                     MultiGetByOffsetCase(
4280                         *m_graph.addStructureSet(variant.structureSet()),
4281                         GetByOffsetMethod::load(variant.offset())));
4282                 continue;
4283             }
4284 
4285             GetByOffsetMethod method = planLoad(variant.conditionSet());
4286             if (!method) {
4287                 set(destination,
4288                     addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4289                 return;
4290             }
4291 
4292             cases.append(MultiGetByOffsetCase(*m_graph.addStructureSet(variant.structureSet()), method));
4293         }
4294 
4295         if (UNLIKELY(m_graph.compilation()))
4296             m_graph.compilation()-&gt;noticeInlinedGetById();
4297 
4298         // 2) Emit a MultiGetByOffset
4299         MultiGetByOffsetData* data = m_graph.m_multiGetByOffsetData.add();
4300         data-&gt;cases = cases;
4301         data-&gt;identifierNumber = identifierNumber;
4302         set(destination,
4303             addToGraph(MultiGetByOffset, OpInfo(data), OpInfo(prediction), base));
4304         return;
4305     }
4306 
4307     addToGraph(FilterGetByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addGetByIdStatus(currentCodeOrigin(), getByIdStatus)), base);
4308 
4309     ASSERT(getByIdStatus.numVariants() == 1);
4310     GetByIdVariant variant = getByIdStatus[0];
4311 
4312     Node* loadedValue = load(prediction, base, identifierNumber, variant);
4313     if (!loadedValue) {
4314         set(destination,
4315             addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4316         return;
4317     }
4318 
4319     if (UNLIKELY(m_graph.compilation()))
4320         m_graph.compilation()-&gt;noticeInlinedGetById();
4321 
4322     ASSERT(type == AccessType::Get || type == AccessType::GetDirect || !variant.callLinkStatus());
4323     if (!variant.callLinkStatus() &amp;&amp; variant.intrinsic() == NoIntrinsic) {
4324         set(destination, loadedValue);
4325         return;
4326     }
4327 
4328     Node* getter = addToGraph(GetGetter, loadedValue);
4329 
4330     if (handleIntrinsicGetter(destination, prediction, variant, base,
4331             [&amp;] () {
4332                 addToGraph(CheckCell, OpInfo(m_graph.freeze(variant.intrinsicFunction())), getter);
4333             })) {
4334         addToGraph(Phantom, base);
4335         return;
4336     }
4337 
4338     ASSERT(variant.intrinsic() == NoIntrinsic);
4339 
4340     // Make a call. We don&#39;t try to get fancy with using the smallest operand number because
4341     // the stack layout phase should compress the stack anyway.
4342 
4343     unsigned numberOfParameters = 0;
4344     numberOfParameters++; // The &#39;this&#39; argument.
4345     numberOfParameters++; // True return PC.
4346 
4347     // Start with a register offset that corresponds to the last in-use register.
4348     int registerOffset = virtualRegisterForLocal(
4349         m_inlineStackTop-&gt;m_profiledBlock-&gt;numCalleeLocals() - 1).offset();
4350     registerOffset -= numberOfParameters;
4351     registerOffset -= CallFrame::headerSizeInRegisters;
4352 
4353     // Get the alignment right.
4354     registerOffset = -WTF::roundUpToMultipleOf(
4355         stackAlignmentRegisters(),
4356         -registerOffset);
4357 
4358     ensureLocals(
4359         m_inlineStackTop-&gt;remapOperand(
4360             VirtualRegister(registerOffset)).toLocal());
4361 
4362     // Issue SetLocals. This has two effects:
4363     // 1) That&#39;s how handleCall() sees the arguments.
4364     // 2) If we inline then this ensures that the arguments are flushed so that if you use
4365     //    the dreaded arguments object on the getter, the right things happen. Well, sort of -
4366     //    since we only really care about &#39;this&#39; in this case. But we&#39;re not going to take that
4367     //    shortcut.
4368     int nextRegister = registerOffset + CallFrame::headerSizeInRegisters;
4369     set(VirtualRegister(nextRegister++), base, ImmediateNakedSet);
4370 
4371     // We&#39;ve set some locals, but they are not user-visible. It&#39;s still OK to exit from here.
4372     m_exitOK = true;
4373     addToGraph(ExitOK);
4374 
4375     handleCall(
4376         destination, Call, InlineCallFrame::GetterCall, instructionSize,
4377         getter, numberOfParameters - 1, registerOffset, *variant.callLinkStatus(), prediction);
4378 }
4379 
4380 void ByteCodeParser::emitPutById(
4381     Node* base, unsigned identifierNumber, Node* value, const PutByIdStatus&amp; putByIdStatus, bool isDirect)
4382 {
4383     if (isDirect)
4384         addToGraph(PutByIdDirect, OpInfo(identifierNumber), base, value);
4385     else
4386         addToGraph(putByIdStatus.makesCalls() ? PutByIdFlush : PutById, OpInfo(identifierNumber), base, value);
4387 }
4388 
4389 void ByteCodeParser::handlePutById(
4390     Node* base, unsigned identifierNumber, Node* value,
4391     const PutByIdStatus&amp; putByIdStatus, bool isDirect, unsigned instructionSize)
4392 {
4393     if (!putByIdStatus.isSimple() || !putByIdStatus.numVariants() || !Options::useAccessInlining()) {
4394         if (!putByIdStatus.isSet())
4395             addToGraph(ForceOSRExit);
4396         emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4397         return;
4398     }
4399 
4400     if (putByIdStatus.numVariants() &gt; 1) {
4401         if (!m_graph.m_plan.isFTL() || putByIdStatus.makesCalls()
4402             || !Options::usePolymorphicAccessInlining()
4403             || putByIdStatus.numVariants() &gt; Options::maxPolymorphicAccessInliningListSize()) {
4404             emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4405             return;
4406         }
4407 
4408         if (!isDirect) {
4409             for (unsigned variantIndex = putByIdStatus.numVariants(); variantIndex--;) {
4410                 if (putByIdStatus[variantIndex].kind() != PutByIdVariant::Transition)
4411                     continue;
4412                 if (!check(putByIdStatus[variantIndex].conditionSet())) {
4413                     emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4414                     return;
4415                 }
4416             }
4417         }
4418 
4419         if (UNLIKELY(m_graph.compilation()))
4420             m_graph.compilation()-&gt;noticeInlinedPutById();
4421 
4422         addToGraph(FilterPutByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addPutByIdStatus(currentCodeOrigin(), putByIdStatus)), base);
4423 
4424         for (const PutByIdVariant&amp; variant : putByIdStatus.variants()) {
4425             for (Structure* structure : variant.oldStructure())
4426                 m_graph.registerStructure(structure);
4427             if (variant.kind() == PutByIdVariant::Transition)
4428                 m_graph.registerStructure(variant.newStructure());
4429         }
4430 
4431         MultiPutByOffsetData* data = m_graph.m_multiPutByOffsetData.add();
4432         data-&gt;variants = putByIdStatus.variants();
4433         data-&gt;identifierNumber = identifierNumber;
4434         addToGraph(MultiPutByOffset, OpInfo(data), base, value);
4435         return;
4436     }
4437 
4438     ASSERT(putByIdStatus.numVariants() == 1);
4439     const PutByIdVariant&amp; variant = putByIdStatus[0];
4440 
4441     switch (variant.kind()) {
4442     case PutByIdVariant::Replace: {
4443         addToGraph(FilterPutByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addPutByIdStatus(currentCodeOrigin(), putByIdStatus)), base);
4444 
4445         store(base, identifierNumber, variant, value);
4446         if (UNLIKELY(m_graph.compilation()))
4447             m_graph.compilation()-&gt;noticeInlinedPutById();
4448         return;
4449     }
4450 
4451     case PutByIdVariant::Transition: {
4452         addToGraph(FilterPutByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addPutByIdStatus(currentCodeOrigin(), putByIdStatus)), base);
4453 
4454         addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(variant.oldStructure())), base);
4455         if (!check(variant.conditionSet())) {
4456             emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4457             return;
4458         }
4459 
4460         ASSERT(variant.oldStructureForTransition()-&gt;transitionWatchpointSetHasBeenInvalidated());
4461 
4462         Node* propertyStorage;
4463         Transition* transition = m_graph.m_transitions.add(
4464             m_graph.registerStructure(variant.oldStructureForTransition()), m_graph.registerStructure(variant.newStructure()));
4465 
4466         if (variant.reallocatesStorage()) {
4467 
4468             // If we&#39;re growing the property storage then it must be because we&#39;re
4469             // storing into the out-of-line storage.
4470             ASSERT(!isInlineOffset(variant.offset()));
4471 
4472             if (!variant.oldStructureForTransition()-&gt;outOfLineCapacity()) {
4473                 propertyStorage = addToGraph(
4474                     AllocatePropertyStorage, OpInfo(transition), base);
4475             } else {
4476                 propertyStorage = addToGraph(
4477                     ReallocatePropertyStorage, OpInfo(transition),
4478                     base, addToGraph(GetButterfly, base));
4479             }
4480         } else {
4481             if (isInlineOffset(variant.offset()))
4482                 propertyStorage = base;
4483             else
4484                 propertyStorage = addToGraph(GetButterfly, base);
4485         }
4486 
4487         StorageAccessData* data = m_graph.m_storageAccessData.add();
4488         data-&gt;offset = variant.offset();
4489         data-&gt;identifierNumber = identifierNumber;
4490 
4491         // NOTE: We could GC at this point because someone could insert an operation that GCs.
4492         // That&#39;s fine because:
4493         // - Things already in the structure will get scanned because we haven&#39;t messed with
4494         //   the object yet.
4495         // - The value we are fixing to put is going to be kept live by OSR exit handling. So
4496         //   if the GC does a conservative scan here it will see the new value.
4497 
4498         addToGraph(
4499             PutByOffset,
4500             OpInfo(data),
4501             propertyStorage,
4502             base,
4503             value);
4504 
4505         if (variant.reallocatesStorage())
4506             addToGraph(NukeStructureAndSetButterfly, base, propertyStorage);
4507 
4508         // FIXME: PutStructure goes last until we fix either
4509         // https://bugs.webkit.org/show_bug.cgi?id=142921 or
4510         // https://bugs.webkit.org/show_bug.cgi?id=142924.
4511         addToGraph(PutStructure, OpInfo(transition), base);
4512 
4513         if (UNLIKELY(m_graph.compilation()))
4514             m_graph.compilation()-&gt;noticeInlinedPutById();
4515         return;
4516     }
4517 
4518     case PutByIdVariant::Setter: {
4519         addToGraph(FilterPutByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addPutByIdStatus(currentCodeOrigin(), putByIdStatus)), base);
4520 
4521         Node* loadedValue = load(SpecCellOther, base, identifierNumber, variant);
4522         if (!loadedValue) {
4523             emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4524             return;
4525         }
4526 
4527         Node* setter = addToGraph(GetSetter, loadedValue);
4528 
4529         // Make a call. We don&#39;t try to get fancy with using the smallest operand number because
4530         // the stack layout phase should compress the stack anyway.
4531 
4532         unsigned numberOfParameters = 0;
4533         numberOfParameters++; // The &#39;this&#39; argument.
4534         numberOfParameters++; // The new value.
4535         numberOfParameters++; // True return PC.
4536 
4537         // Start with a register offset that corresponds to the last in-use register.
4538         int registerOffset = virtualRegisterForLocal(
4539             m_inlineStackTop-&gt;m_profiledBlock-&gt;numCalleeLocals() - 1).offset();
4540         registerOffset -= numberOfParameters;
4541         registerOffset -= CallFrame::headerSizeInRegisters;
4542 
4543         // Get the alignment right.
4544         registerOffset = -WTF::roundUpToMultipleOf(
4545             stackAlignmentRegisters(),
4546             -registerOffset);
4547 
4548         ensureLocals(
4549             m_inlineStackTop-&gt;remapOperand(
4550                 VirtualRegister(registerOffset)).toLocal());
4551 
4552         int nextRegister = registerOffset + CallFrame::headerSizeInRegisters;
4553         set(VirtualRegister(nextRegister++), base, ImmediateNakedSet);
4554         set(VirtualRegister(nextRegister++), value, ImmediateNakedSet);
4555 
4556         // We&#39;ve set some locals, but they are not user-visible. It&#39;s still OK to exit from here.
4557         m_exitOK = true;
4558         addToGraph(ExitOK);
4559 
4560         handleCall(
4561             VirtualRegister(), Call, InlineCallFrame::SetterCall,
4562             instructionSize, setter, numberOfParameters - 1, registerOffset,
4563             *variant.callLinkStatus(), SpecOther);
4564         return;
4565     }
4566 
4567     default: {
4568         emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4569         return;
4570     } }
4571 }
4572 
4573 void ByteCodeParser::prepareToParseBlock()
4574 {
4575     clearCaches();
4576     ASSERT(m_setLocalQueue.isEmpty());
4577 }
4578 
4579 void ByteCodeParser::clearCaches()
4580 {
4581     m_constants.shrink(0);
4582 }
4583 
4584 template&lt;typename Op&gt;
4585 void ByteCodeParser::parseGetById(const Instruction* currentInstruction)
4586 {
4587     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
4588     SpeculatedType prediction = getPrediction();
4589 
4590     Node* base = get(bytecode.m_base);
4591     unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
4592 
4593     UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
4594     GetByIdStatus getByIdStatus = GetByIdStatus::computeFor(
4595         m_inlineStackTop-&gt;m_profiledBlock,
4596         m_inlineStackTop-&gt;m_baselineMap, m_icContextStack,
4597         currentCodeOrigin(), uid);
4598 
4599     AccessType type = AccessType::Get;
4600     unsigned opcodeLength = currentInstruction-&gt;size();
4601     if (Op::opcodeID == op_try_get_by_id)
4602         type = AccessType::TryGet;
4603     else if (Op::opcodeID == op_get_by_id_direct)
4604         type = AccessType::GetDirect;
4605 
4606     handleGetById(
4607         bytecode.m_dst, prediction, base, identifierNumber, getByIdStatus, type, opcodeLength);
4608 
4609 }
4610 
4611 static uint64_t makeDynamicVarOpInfo(unsigned identifierNumber, unsigned getPutInfo)
4612 {
4613     static_assert(sizeof(identifierNumber) == 4,
4614         &quot;We cannot fit identifierNumber into the high bits of m_opInfo&quot;);
4615     return static_cast&lt;uint64_t&gt;(identifierNumber) | (static_cast&lt;uint64_t&gt;(getPutInfo) &lt;&lt; 32);
4616 }
4617 
4618 // The idiom:
4619 //     if (true) { ...; goto label; } else label: continue
4620 // Allows using NEXT_OPCODE as a statement, even in unbraced if+else, while containing a `continue`.
4621 // The more common idiom:
4622 //     do { ...; } while (false)
4623 // Doesn&#39;t allow using `continue`.
4624 #define NEXT_OPCODE(name) \
4625     if (true) { \
4626         m_currentIndex += currentInstruction-&gt;size(); \
4627         goto WTF_CONCAT(NEXT_OPCODE_, __LINE__); /* Need a unique label: usable more than once per function. */ \
4628     } else \
4629         WTF_CONCAT(NEXT_OPCODE_, __LINE__): \
4630     continue
4631 
4632 #define LAST_OPCODE_LINKED(name) do { \
4633         m_currentIndex += currentInstruction-&gt;size(); \
4634         m_exitOK = false; \
4635         return; \
4636     } while (false)
4637 
4638 #define LAST_OPCODE(name) \
4639     do { \
4640         if (m_currentBlock-&gt;terminal()) { \
4641             switch (m_currentBlock-&gt;terminal()-&gt;op()) { \
4642             case Jump: \
4643             case Branch: \
4644             case Switch: \
4645                 ASSERT(!m_currentBlock-&gt;isLinked); \
4646                 m_inlineStackTop-&gt;m_unlinkedBlocks.append(m_currentBlock); \
4647                 break;\
4648             default: break; \
4649             } \
4650         } \
4651         LAST_OPCODE_LINKED(name); \
4652     } while (false)
4653 
4654 void ByteCodeParser::parseBlock(unsigned limit)
4655 {
4656     auto&amp; instructions = m_inlineStackTop-&gt;m_codeBlock-&gt;instructions();
4657     unsigned blockBegin = m_currentIndex;
4658 
4659     // If we are the first basic block, introduce markers for arguments. This allows
4660     // us to track if a use of an argument may use the actual argument passed, as
4661     // opposed to using a value we set explicitly.
4662     if (m_currentBlock == m_graph.block(0) &amp;&amp; !inlineCallFrame()) {
4663         auto addResult = m_graph.m_rootToArguments.add(m_currentBlock, ArgumentsVector());
4664         RELEASE_ASSERT(addResult.isNewEntry);
4665         ArgumentsVector&amp; entrypointArguments = addResult.iterator-&gt;value;
4666         entrypointArguments.resize(m_numArguments);
4667 
4668         // We will emit SetArgument nodes. They don&#39;t exit, but we&#39;re at the top of an op_enter so
4669         // exitOK = true.
4670         m_exitOK = true;
4671         for (unsigned argument = 0; argument &lt; m_numArguments; ++argument) {
4672             VariableAccessData* variable = newVariableAccessData(
4673                 virtualRegisterForArgument(argument));
4674             variable-&gt;mergeStructureCheckHoistingFailed(
4675                 m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache));
4676             variable-&gt;mergeCheckArrayHoistingFailed(
4677                 m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIndexingType));
4678 
4679             Node* setArgument = addToGraph(SetArgument, OpInfo(variable));
4680             entrypointArguments[argument] = setArgument;
4681             m_currentBlock-&gt;variablesAtTail.setArgumentFirstTime(argument, setArgument);
4682         }
4683     }
4684 
4685     CodeBlock* codeBlock = m_inlineStackTop-&gt;m_codeBlock;
4686 
4687     auto jumpTarget = [&amp;](int target) {
4688         if (target)
4689             return target;
4690         return codeBlock-&gt;outOfLineJumpOffset(m_currentInstruction);
4691     };
4692 
4693     while (true) {
4694         // We&#39;re staring at a new bytecode instruction. So we once again have a place that we can exit
4695         // to.
4696         m_exitOK = true;
4697 
4698         processSetLocalQueue();
4699 
4700         // Don&#39;t extend over jump destinations.
4701         if (m_currentIndex == limit) {
4702             // Ordinarily we want to plant a jump. But refuse to do this if the block is
4703             // empty. This is a special case for inlining, which might otherwise create
4704             // some empty blocks in some cases. When parseBlock() returns with an empty
4705             // block, it will get repurposed instead of creating a new one. Note that this
4706             // logic relies on every bytecode resulting in one or more nodes, which would
4707             // be true anyway except for op_loop_hint, which emits a Phantom to force this
4708             // to be true.
4709 
4710             if (!m_currentBlock-&gt;isEmpty())
4711                 addJumpTo(m_currentIndex);
4712             return;
4713         }
4714 
4715         // Switch on the current bytecode opcode.
4716         const Instruction* currentInstruction = instructions.at(m_currentIndex).ptr();
4717         m_currentInstruction = currentInstruction; // Some methods want to use this, and we&#39;d rather not thread it through calls.
4718         OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
4719 
4720         VERBOSE_LOG(&quot;    parsing &quot;, currentCodeOrigin(), &quot;: &quot;, opcodeID, &quot;\n&quot;);
4721 
4722         if (UNLIKELY(m_graph.compilation())) {
4723             addToGraph(CountExecution, OpInfo(m_graph.compilation()-&gt;executionCounterFor(
4724                 Profiler::OriginStack(*m_vm-&gt;m_perBytecodeProfiler, m_codeBlock, currentCodeOrigin()))));
4725         }
4726 
4727         switch (opcodeID) {
4728 
4729         // === Function entry opcodes ===
4730 
4731         case op_enter: {
4732             Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
4733             // Initialize all locals to undefined.
4734             for (int i = 0; i &lt; m_inlineStackTop-&gt;m_codeBlock-&gt;numVars(); ++i)
4735                 set(virtualRegisterForLocal(i), undefined, ImmediateNakedSet);
4736 
4737             NEXT_OPCODE(op_enter);
4738         }
4739 
4740         case op_to_this: {
4741             Node* op1 = getThis();
4742             auto&amp; metadata = currentInstruction-&gt;as&lt;OpToThis&gt;().metadata(codeBlock);
4743             Structure* cachedStructure = metadata.m_cachedStructure.get();
4744             if (metadata.m_toThisStatus != ToThisOK
4745                 || !cachedStructure
4746                 || cachedStructure-&gt;classInfo()-&gt;methodTable.toThis != JSObject::info()-&gt;methodTable.toThis
4747                 || m_inlineStackTop-&gt;m_profiledBlock-&gt;couldTakeSlowCase(m_currentIndex)
4748                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache)
4749                 || (op1-&gt;op() == GetLocal &amp;&amp; op1-&gt;variableAccessData()-&gt;structureCheckHoistingFailed())) {
4750                 setThis(addToGraph(ToThis, OpInfo(), OpInfo(getPrediction()), op1));
4751             } else {
4752                 addToGraph(
4753                     CheckStructure,
4754                     OpInfo(m_graph.addStructureSet(cachedStructure)),
4755                     op1);
4756             }
4757             NEXT_OPCODE(op_to_this);
4758         }
4759 
4760         case op_create_this: {
4761             auto bytecode = currentInstruction-&gt;as&lt;OpCreateThis&gt;();
4762             Node* callee = get(VirtualRegister(bytecode.m_callee));
4763 
4764             JSFunction* function = callee-&gt;dynamicCastConstant&lt;JSFunction*&gt;(*m_vm);
4765             if (!function) {
4766                 JSCell* cachedFunction = bytecode.metadata(codeBlock).m_cachedCallee.unvalidatedGet();
4767                 if (cachedFunction
4768                     &amp;&amp; cachedFunction != JSCell::seenMultipleCalleeObjects()
4769                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
4770                     ASSERT(cachedFunction-&gt;inherits&lt;JSFunction&gt;(*m_vm));
4771 
4772                     FrozenValue* frozen = m_graph.freeze(cachedFunction);
4773                     addToGraph(CheckCell, OpInfo(frozen), callee);
4774 
4775                     function = static_cast&lt;JSFunction*&gt;(cachedFunction);
4776                 }
4777             }
4778 
4779             bool alreadyEmitted = false;
4780             if (function) {
4781                 if (FunctionRareData* rareData = function-&gt;rareData()) {
4782                     if (rareData-&gt;allocationProfileWatchpointSet().isStillValid()) {
4783                         Structure* structure = rareData-&gt;objectAllocationStructure();
4784                         JSObject* prototype = rareData-&gt;objectAllocationPrototype();
4785                         if (structure
4786                             &amp;&amp; (structure-&gt;hasMonoProto() || prototype)
4787                             &amp;&amp; rareData-&gt;allocationProfileWatchpointSet().isStillValid()) {
4788 
4789                             m_graph.freeze(rareData);
4790                             m_graph.watchpoints().addLazily(rareData-&gt;allocationProfileWatchpointSet());
4791 
4792                             // The callee is still live up to this point.
4793                             addToGraph(Phantom, callee);
4794                             Node* object = addToGraph(NewObject, OpInfo(m_graph.registerStructure(structure)));
4795                             if (structure-&gt;hasPolyProto()) {
4796                                 StorageAccessData* data = m_graph.m_storageAccessData.add();
4797                                 data-&gt;offset = knownPolyProtoOffset;
4798                                 data-&gt;identifierNumber = m_graph.identifiers().ensure(m_graph.m_vm.propertyNames-&gt;builtinNames().polyProtoName().impl());
4799                                 ASSERT(isInlineOffset(knownPolyProtoOffset));
4800                                 addToGraph(PutByOffset, OpInfo(data), object, object, weakJSConstant(prototype));
4801                             }
4802                             set(VirtualRegister(bytecode.m_dst), object);
4803                             alreadyEmitted = true;
4804                         }
4805                     }
4806                 }
4807             }
4808             if (!alreadyEmitted) {
4809                 set(VirtualRegister(bytecode.m_dst),
4810                     addToGraph(CreateThis, OpInfo(bytecode.m_inlineCapacity), callee));
4811             }
4812             NEXT_OPCODE(op_create_this);
4813         }
4814 
4815         case op_new_object: {
4816             auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
4817             set(bytecode.m_dst,
4818                 addToGraph(NewObject,
4819                     OpInfo(m_graph.registerStructure(bytecode.metadata(codeBlock).m_objectAllocationProfile.structure()))));
4820             NEXT_OPCODE(op_new_object);
4821         }
4822 
4823         case op_new_array: {
4824             auto bytecode = currentInstruction-&gt;as&lt;OpNewArray&gt;();
4825             int startOperand = bytecode.m_argv.offset();
4826             int numOperands = bytecode.m_argc;
4827             ArrayAllocationProfile&amp; profile = bytecode.metadata(codeBlock).m_arrayAllocationProfile;
4828             for (int operandIdx = startOperand; operandIdx &gt; startOperand - numOperands; --operandIdx)
4829                 addVarArgChild(get(VirtualRegister(operandIdx)));
4830             unsigned vectorLengthHint = std::max&lt;unsigned&gt;(profile.vectorLengthHint(), numOperands);
4831             set(bytecode.m_dst, addToGraph(Node::VarArg, NewArray, OpInfo(profile.selectIndexingType()), OpInfo(vectorLengthHint)));
4832             NEXT_OPCODE(op_new_array);
4833         }
4834 
4835         case op_new_array_with_spread: {
4836             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSpread&gt;();
4837             int startOperand = bytecode.m_argv.offset();
4838             int numOperands = bytecode.m_argc;
4839             const BitVector&amp; bitVector = m_inlineStackTop-&gt;m_profiledBlock-&gt;unlinkedCodeBlock()-&gt;bitVector(bytecode.m_bitVector);
4840             for (int operandIdx = startOperand; operandIdx &gt; startOperand - numOperands; --operandIdx)
4841                 addVarArgChild(get(VirtualRegister(operandIdx)));
4842 
4843             BitVector* copy = m_graph.m_bitVectors.add(bitVector);
4844             ASSERT(*copy == bitVector);
4845 
4846             set(bytecode.m_dst,
4847                 addToGraph(Node::VarArg, NewArrayWithSpread, OpInfo(copy)));
4848             NEXT_OPCODE(op_new_array_with_spread);
4849         }
4850 
4851         case op_spread: {
4852             auto bytecode = currentInstruction-&gt;as&lt;OpSpread&gt;();
4853             set(bytecode.m_dst,
4854                 addToGraph(Spread, get(bytecode.m_argument)));
4855             NEXT_OPCODE(op_spread);
4856         }
4857 
4858         case op_new_array_with_size: {
4859             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSize&gt;();
4860             ArrayAllocationProfile&amp; profile = bytecode.metadata(codeBlock).m_arrayAllocationProfile;
4861             set(bytecode.m_dst, addToGraph(NewArrayWithSize, OpInfo(profile.selectIndexingType()), get(bytecode.m_length)));
4862             NEXT_OPCODE(op_new_array_with_size);
4863         }
4864 
4865         case op_new_array_buffer: {
4866             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayBuffer&gt;();
4867             // Unfortunately, we can&#39;t allocate a new JSImmutableButterfly if the profile tells us new information because we
4868             // cannot allocate from compilation threads.
4869             WTF::loadLoadFence();
4870             FrozenValue* frozen = get(VirtualRegister(bytecode.m_immutableButterfly))-&gt;constant();
4871             WTF::loadLoadFence();
4872             JSImmutableButterfly* immutableButterfly = frozen-&gt;cast&lt;JSImmutableButterfly*&gt;();
4873             NewArrayBufferData data { };
4874             data.indexingMode = immutableButterfly-&gt;indexingMode();
4875             data.vectorLengthHint = immutableButterfly-&gt;toButterfly()-&gt;vectorLength();
4876 
4877             set(VirtualRegister(bytecode.m_dst), addToGraph(NewArrayBuffer, OpInfo(frozen), OpInfo(data.asQuadWord)));
4878             NEXT_OPCODE(op_new_array_buffer);
4879         }
4880 
4881         case op_new_regexp: {
4882             auto bytecode = currentInstruction-&gt;as&lt;OpNewRegexp&gt;();
4883             ASSERT(bytecode.m_regexp.isConstant());
4884             FrozenValue* frozenRegExp = m_graph.freezeStrong(m_inlineStackTop-&gt;m_codeBlock-&gt;getConstant(bytecode.m_regexp.offset()));
4885             set(bytecode.m_dst, addToGraph(NewRegexp, OpInfo(frozenRegExp), jsConstant(jsNumber(0))));
4886             NEXT_OPCODE(op_new_regexp);
4887         }
4888 
4889         case op_get_rest_length: {
4890             auto bytecode = currentInstruction-&gt;as&lt;OpGetRestLength&gt;();
4891             InlineCallFrame* inlineCallFrame = this-&gt;inlineCallFrame();
4892             Node* length;
4893             if (inlineCallFrame &amp;&amp; !inlineCallFrame-&gt;isVarargs()) {
4894                 unsigned argumentsLength = inlineCallFrame-&gt;argumentCountIncludingThis - 1;
4895                 JSValue restLength;
4896                 if (argumentsLength &lt;= bytecode.m_numParametersToSkip)
4897                     restLength = jsNumber(0);
4898                 else
4899                     restLength = jsNumber(argumentsLength - bytecode.m_numParametersToSkip);
4900 
4901                 length = jsConstant(restLength);
4902             } else
4903                 length = addToGraph(GetRestLength, OpInfo(bytecode.m_numParametersToSkip));
4904             set(bytecode.m_dst, length);
4905             NEXT_OPCODE(op_get_rest_length);
4906         }
4907 
4908         case op_create_rest: {
4909             auto bytecode = currentInstruction-&gt;as&lt;OpCreateRest&gt;();
4910             noticeArgumentsUse();
4911             Node* arrayLength = get(bytecode.m_arraySize);
4912             set(bytecode.m_dst,
4913                 addToGraph(CreateRest, OpInfo(bytecode.m_numParametersToSkip), arrayLength));
4914             NEXT_OPCODE(op_create_rest);
4915         }
4916 
4917         // === Bitwise operations ===
4918 
4919         case op_bitnot: {
4920             auto bytecode = currentInstruction-&gt;as&lt;OpBitnot&gt;();
4921             Node* op1 = get(bytecode.m_operand);
4922             set(bytecode.m_dst, addToGraph(ArithBitNot, op1));
4923             NEXT_OPCODE(op_bitnot);
4924         }
4925 
4926         case op_bitand: {
4927             auto bytecode = currentInstruction-&gt;as&lt;OpBitand&gt;();
4928             SpeculatedType prediction = getPrediction();
4929             Node* op1 = get(bytecode.m_lhs);
4930             Node* op2 = get(bytecode.m_rhs);
4931             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
4932                 set(bytecode.m_dst, addToGraph(ArithBitAnd, op1, op2));
4933             else
4934                 set(bytecode.m_dst, addToGraph(ValueBitAnd, OpInfo(), OpInfo(prediction), op1, op2));
4935             NEXT_OPCODE(op_bitand);
4936         }
4937 
4938         case op_bitor: {
4939             auto bytecode = currentInstruction-&gt;as&lt;OpBitor&gt;();
4940             SpeculatedType prediction = getPrediction();
4941             Node* op1 = get(bytecode.m_lhs);
4942             Node* op2 = get(bytecode.m_rhs);
4943             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
4944                 set(bytecode.m_dst, addToGraph(ArithBitOr, op1, op2));
4945             else
4946                 set(bytecode.m_dst, addToGraph(ValueBitOr, OpInfo(), OpInfo(prediction), op1, op2));
4947             NEXT_OPCODE(op_bitor);
4948         }
4949 
4950         case op_bitxor: {
4951             auto bytecode = currentInstruction-&gt;as&lt;OpBitxor&gt;();
4952             SpeculatedType prediction = getPrediction();
4953             Node* op1 = get(bytecode.m_lhs);
4954             Node* op2 = get(bytecode.m_rhs);
4955             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
4956                 set(bytecode.m_dst, addToGraph(ArithBitXor, op1, op2));
4957             else
4958                 set(bytecode.m_dst, addToGraph(ValueBitXor, OpInfo(), OpInfo(prediction), op1, op2));
4959             NEXT_OPCODE(op_bitxor);
4960         }
4961 
4962         case op_rshift: {
4963             auto bytecode = currentInstruction-&gt;as&lt;OpRshift&gt;();
4964             Node* op1 = get(bytecode.m_lhs);
4965             Node* op2 = get(bytecode.m_rhs);
4966             set(bytecode.m_dst, addToGraph(BitRShift, op1, op2));
4967             NEXT_OPCODE(op_rshift);
4968         }
4969 
4970         case op_lshift: {
4971             auto bytecode = currentInstruction-&gt;as&lt;OpLshift&gt;();
4972             Node* op1 = get(bytecode.m_lhs);
4973             Node* op2 = get(bytecode.m_rhs);
4974             set(bytecode.m_dst, addToGraph(BitLShift, op1, op2));
4975             NEXT_OPCODE(op_lshift);
4976         }
4977 
4978         case op_urshift: {
4979             auto bytecode = currentInstruction-&gt;as&lt;OpUrshift&gt;();
4980             Node* op1 = get(bytecode.m_lhs);
4981             Node* op2 = get(bytecode.m_rhs);
4982             set(bytecode.m_dst, addToGraph(BitURShift, op1, op2));
4983             NEXT_OPCODE(op_urshift);
4984         }
4985 
4986         case op_unsigned: {
4987             auto bytecode = currentInstruction-&gt;as&lt;OpUnsigned&gt;();
4988             set(bytecode.m_dst, makeSafe(addToGraph(UInt32ToNumber, get(bytecode.m_operand))));
4989             NEXT_OPCODE(op_unsigned);
4990         }
4991 
4992         // === Increment/Decrement opcodes ===
4993 
4994         case op_inc: {
4995             auto bytecode = currentInstruction-&gt;as&lt;OpInc&gt;();
4996             Node* op = get(bytecode.m_srcDst);
4997             set(bytecode.m_srcDst, makeSafe(addToGraph(ArithAdd, op, addToGraph(JSConstant, OpInfo(m_constantOne)))));
4998             NEXT_OPCODE(op_inc);
4999         }
5000 
5001         case op_dec: {
5002             auto bytecode = currentInstruction-&gt;as&lt;OpDec&gt;();
5003             Node* op = get(bytecode.m_srcDst);
5004             set(bytecode.m_srcDst, makeSafe(addToGraph(ArithSub, op, addToGraph(JSConstant, OpInfo(m_constantOne)))));
5005             NEXT_OPCODE(op_dec);
5006         }
5007 
5008         // === Arithmetic operations ===
5009 
5010         case op_add: {
5011             auto bytecode = currentInstruction-&gt;as&lt;OpAdd&gt;();
5012             Node* op1 = get(bytecode.m_lhs);
5013             Node* op2 = get(bytecode.m_rhs);
5014             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5015                 set(bytecode.m_dst, makeSafe(addToGraph(ArithAdd, op1, op2)));
5016             else
5017                 set(bytecode.m_dst, makeSafe(addToGraph(ValueAdd, op1, op2)));
5018             NEXT_OPCODE(op_add);
5019         }
5020 
5021         case op_sub: {
5022             auto bytecode = currentInstruction-&gt;as&lt;OpSub&gt;();
5023             Node* op1 = get(bytecode.m_lhs);
5024             Node* op2 = get(bytecode.m_rhs);
5025             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5026                 set(bytecode.m_dst, makeSafe(addToGraph(ArithSub, op1, op2)));
5027             else
5028                 set(bytecode.m_dst, makeSafe(addToGraph(ValueSub, op1, op2)));
5029             NEXT_OPCODE(op_sub);
5030         }
5031 
5032         case op_negate: {
5033             auto bytecode = currentInstruction-&gt;as&lt;OpNegate&gt;();
5034             Node* op1 = get(bytecode.m_operand);
5035             if (op1-&gt;hasNumberResult())
5036                 set(bytecode.m_dst, makeSafe(addToGraph(ArithNegate, op1)));
5037             else
5038                 set(bytecode.m_dst, makeSafe(addToGraph(ValueNegate, op1)));
5039             NEXT_OPCODE(op_negate);
5040         }
5041 
5042         case op_mul: {
5043             // Multiply requires that the inputs are not truncated, unfortunately.
5044             auto bytecode = currentInstruction-&gt;as&lt;OpMul&gt;();
5045             Node* op1 = get(bytecode.m_lhs);
5046             Node* op2 = get(bytecode.m_rhs);
5047             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5048                 set(bytecode.m_dst, makeSafe(addToGraph(ArithMul, op1, op2)));
5049             else
5050                 set(bytecode.m_dst, makeSafe(addToGraph(ValueMul, op1, op2)));
5051             NEXT_OPCODE(op_mul);
5052         }
5053 
5054         case op_mod: {
5055             auto bytecode = currentInstruction-&gt;as&lt;OpMod&gt;();
5056             Node* op1 = get(bytecode.m_lhs);
5057             Node* op2 = get(bytecode.m_rhs);
5058             set(bytecode.m_dst, makeSafe(addToGraph(ArithMod, op1, op2)));
5059             NEXT_OPCODE(op_mod);
5060         }
5061 
5062         case op_pow: {
5063             // FIXME: ArithPow(Untyped, Untyped) should be supported as the same to ArithMul, ArithSub etc.
5064             // https://bugs.webkit.org/show_bug.cgi?id=160012
5065             auto bytecode = currentInstruction-&gt;as&lt;OpPow&gt;();
5066             Node* op1 = get(bytecode.m_lhs);
5067             Node* op2 = get(bytecode.m_rhs);
5068             set(bytecode.m_dst, addToGraph(ArithPow, op1, op2));
5069             NEXT_OPCODE(op_pow);
5070         }
5071 
5072         case op_div: {
5073             auto bytecode = currentInstruction-&gt;as&lt;OpDiv&gt;();
5074             Node* op1 = get(bytecode.m_lhs);
5075             Node* op2 = get(bytecode.m_rhs);
5076             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5077                 set(bytecode.m_dst, makeDivSafe(addToGraph(ArithDiv, op1, op2)));
5078             else
5079                 set(bytecode.m_dst, makeDivSafe(addToGraph(ValueDiv, op1, op2)));
5080             NEXT_OPCODE(op_div);
5081         }
5082 
5083         // === Misc operations ===
5084 
5085         case op_debug: {
5086             // This is a nop in the DFG/FTL because when we set a breakpoint in the debugger,
5087             // we will jettison all optimized CodeBlocks that contains the breakpoint.
5088             addToGraph(Check); // We add a nop here so that basic block linking doesn&#39;t break.
5089             NEXT_OPCODE(op_debug);
5090         }
5091 
5092         case op_mov: {
5093             auto bytecode = currentInstruction-&gt;as&lt;OpMov&gt;();
5094             Node* op = get(bytecode.m_src);
5095             set(bytecode.m_dst, op);
5096             NEXT_OPCODE(op_mov);
5097         }
5098 
5099         case op_check_tdz: {
5100             auto bytecode = currentInstruction-&gt;as&lt;OpCheckTdz&gt;();
5101             addToGraph(CheckNotEmpty, get(bytecode.m_targetVirtualRegister));
5102             NEXT_OPCODE(op_check_tdz);
5103         }
5104 
5105         case op_overrides_has_instance: {
5106             auto bytecode = currentInstruction-&gt;as&lt;OpOverridesHasInstance&gt;();
5107             JSFunction* defaultHasInstanceSymbolFunction = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObjectFor(currentCodeOrigin())-&gt;functionProtoHasInstanceSymbolFunction();
5108 
5109             Node* constructor = get(VirtualRegister(bytecode.m_constructor));
5110             Node* hasInstanceValue = get(VirtualRegister(bytecode.m_hasInstanceValue));
5111 
5112             set(VirtualRegister(bytecode.m_dst), addToGraph(OverridesHasInstance, OpInfo(m_graph.freeze(defaultHasInstanceSymbolFunction)), constructor, hasInstanceValue));
5113             NEXT_OPCODE(op_overrides_has_instance);
5114         }
5115 
5116         case op_identity_with_profile: {
5117             auto bytecode = currentInstruction-&gt;as&lt;OpIdentityWithProfile&gt;();
5118             Node* srcDst = get(bytecode.m_srcDst);
5119             SpeculatedType speculation = static_cast&lt;SpeculatedType&gt;(bytecode.m_topProfile) &lt;&lt; 32 | static_cast&lt;SpeculatedType&gt;(bytecode.m_bottomProfile);
5120             set(bytecode.m_srcDst, addToGraph(IdentityWithProfile, OpInfo(speculation), srcDst));
5121             NEXT_OPCODE(op_identity_with_profile);
5122         }
5123 
5124         case op_instanceof: {
5125             auto bytecode = currentInstruction-&gt;as&lt;OpInstanceof&gt;();
5126 
5127             InstanceOfStatus status = InstanceOfStatus::computeFor(
5128                 m_inlineStackTop-&gt;m_profiledBlock, m_inlineStackTop-&gt;m_baselineMap,
5129                 m_currentIndex);
5130 
5131             Node* value = get(bytecode.m_value);
5132             Node* prototype = get(bytecode.m_prototype);
5133 
5134             // Only inline it if it&#39;s Simple with a commonPrototype; bottom/top or variable
5135             // prototypes both get handled by the IC. This makes sense for bottom (unprofiled)
5136             // instanceof ICs because the profit of this optimization is fairly low. So, in the
5137             // absence of any information, it&#39;s better to avoid making this be the cause of a
5138             // recompilation.
5139             if (JSObject* commonPrototype = status.commonPrototype()) {
5140                 addToGraph(CheckCell, OpInfo(m_graph.freeze(commonPrototype)), prototype);
5141 
5142                 bool allOK = true;
5143                 MatchStructureData* data = m_graph.m_matchStructureData.add();
5144                 for (const InstanceOfVariant&amp; variant : status.variants()) {
5145                     if (!check(variant.conditionSet())) {
5146                         allOK = false;
5147                         break;
5148                     }
5149                     for (Structure* structure : variant.structureSet()) {
5150                         MatchStructureVariant matchVariant;
5151                         matchVariant.structure = m_graph.registerStructure(structure);
5152                         matchVariant.result = variant.isHit();
5153 
5154                         data-&gt;variants.append(WTFMove(matchVariant));
5155                     }
5156                 }
5157 
5158                 if (allOK) {
5159                     Node* match = addToGraph(MatchStructure, OpInfo(data), value);
5160                     set(bytecode.m_dst, match);
5161                     NEXT_OPCODE(op_instanceof);
5162                 }
5163             }
5164 
5165             set(bytecode.m_dst, addToGraph(InstanceOf, value, prototype));
5166             NEXT_OPCODE(op_instanceof);
5167         }
5168 
5169         case op_instanceof_custom: {
5170             auto bytecode = currentInstruction-&gt;as&lt;OpInstanceofCustom&gt;();
5171             Node* value = get(bytecode.m_value);
5172             Node* constructor = get(bytecode.m_constructor);
5173             Node* hasInstanceValue = get(bytecode.m_hasInstanceValue);
5174             set(bytecode.m_dst, addToGraph(InstanceOfCustom, value, constructor, hasInstanceValue));
5175             NEXT_OPCODE(op_instanceof_custom);
5176         }
5177         case op_is_empty: {
5178             auto bytecode = currentInstruction-&gt;as&lt;OpIsEmpty&gt;();
5179             Node* value = get(bytecode.m_operand);
5180             set(bytecode.m_dst, addToGraph(IsEmpty, value));
5181             NEXT_OPCODE(op_is_empty);
5182         }
5183         case op_is_undefined: {
5184             auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefined&gt;();
5185             Node* value = get(bytecode.m_operand);
5186             set(bytecode.m_dst, addToGraph(IsUndefined, value));
5187             NEXT_OPCODE(op_is_undefined);
5188         }
5189         case op_is_undefined_or_null: {
5190             auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefinedOrNull&gt;();
5191             Node* value = get(bytecode.m_operand);
5192             set(bytecode.m_dst, addToGraph(IsUndefinedOrNull, value));
5193             NEXT_OPCODE(op_is_undefined_or_null);
5194         }
5195 
5196         case op_is_boolean: {
5197             auto bytecode = currentInstruction-&gt;as&lt;OpIsBoolean&gt;();
5198             Node* value = get(bytecode.m_operand);
5199             set(bytecode.m_dst, addToGraph(IsBoolean, value));
5200             NEXT_OPCODE(op_is_boolean);
5201         }
5202 
5203         case op_is_number: {
5204             auto bytecode = currentInstruction-&gt;as&lt;OpIsNumber&gt;();
5205             Node* value = get(bytecode.m_operand);
5206             set(bytecode.m_dst, addToGraph(IsNumber, value));
5207             NEXT_OPCODE(op_is_number);
5208         }
5209 
5210         case op_is_cell_with_type: {
5211             auto bytecode = currentInstruction-&gt;as&lt;OpIsCellWithType&gt;();
5212             Node* value = get(bytecode.m_operand);
5213             set(bytecode.m_dst, addToGraph(IsCellWithType, OpInfo(bytecode.m_type), value));
5214             NEXT_OPCODE(op_is_cell_with_type);
5215         }
5216 
5217         case op_is_object: {
5218             auto bytecode = currentInstruction-&gt;as&lt;OpIsObject&gt;();
5219             Node* value = get(bytecode.m_operand);
5220             set(bytecode.m_dst, addToGraph(IsObject, value));
5221             NEXT_OPCODE(op_is_object);
5222         }
5223 
5224         case op_is_object_or_null: {
5225             auto bytecode = currentInstruction-&gt;as&lt;OpIsObjectOrNull&gt;();
5226             Node* value = get(bytecode.m_operand);
5227             set(bytecode.m_dst, addToGraph(IsObjectOrNull, value));
5228             NEXT_OPCODE(op_is_object_or_null);
5229         }
5230 
5231         case op_is_function: {
5232             auto bytecode = currentInstruction-&gt;as&lt;OpIsFunction&gt;();
5233             Node* value = get(bytecode.m_operand);
5234             set(bytecode.m_dst, addToGraph(IsFunction, value));
5235             NEXT_OPCODE(op_is_function);
5236         }
5237 
5238         case op_not: {
5239             auto bytecode = currentInstruction-&gt;as&lt;OpNot&gt;();
5240             Node* value = get(bytecode.m_operand);
5241             set(bytecode.m_dst, addToGraph(LogicalNot, value));
5242             NEXT_OPCODE(op_not);
5243         }
5244 
5245         case op_to_primitive: {
5246             auto bytecode = currentInstruction-&gt;as&lt;OpToPrimitive&gt;();
5247             Node* value = get(bytecode.m_src);
5248             set(bytecode.m_dst, addToGraph(ToPrimitive, value));
5249             NEXT_OPCODE(op_to_primitive);
5250         }
5251 
5252         case op_strcat: {
5253             auto bytecode = currentInstruction-&gt;as&lt;OpStrcat&gt;();
5254             int startOperand = bytecode.m_src.offset();
5255             int numOperands = bytecode.m_count;
5256 #if CPU(X86)
5257             // X86 doesn&#39;t have enough registers to compile MakeRope with three arguments. The
5258             // StrCat we emit here may be turned into a MakeRope. Rather than try to be clever,
5259             // we just make StrCat dumber on this processor.
5260             const unsigned maxArguments = 2;
5261 #else
5262             const unsigned maxArguments = 3;
5263 #endif
5264             Node* operands[AdjacencyList::Size];
5265             unsigned indexInOperands = 0;
5266             for (unsigned i = 0; i &lt; AdjacencyList::Size; ++i)
5267                 operands[i] = 0;
5268             for (int operandIdx = 0; operandIdx &lt; numOperands; ++operandIdx) {
5269                 if (indexInOperands == maxArguments) {
5270                     operands[0] = addToGraph(StrCat, operands[0], operands[1], operands[2]);
5271                     for (unsigned i = 1; i &lt; AdjacencyList::Size; ++i)
5272                         operands[i] = 0;
5273                     indexInOperands = 1;
5274                 }
5275 
5276                 ASSERT(indexInOperands &lt; AdjacencyList::Size);
5277                 ASSERT(indexInOperands &lt; maxArguments);
5278                 operands[indexInOperands++] = get(VirtualRegister(startOperand - operandIdx));
5279             }
5280             set(bytecode.m_dst, addToGraph(StrCat, operands[0], operands[1], operands[2]));
5281             NEXT_OPCODE(op_strcat);
5282         }
5283 
5284         case op_less: {
5285             auto bytecode = currentInstruction-&gt;as&lt;OpLess&gt;();
5286             Node* op1 = get(bytecode.m_lhs);
5287             Node* op2 = get(bytecode.m_rhs);
5288             set(bytecode.m_dst, addToGraph(CompareLess, op1, op2));
5289             NEXT_OPCODE(op_less);
5290         }
5291 
5292         case op_lesseq: {
5293             auto bytecode = currentInstruction-&gt;as&lt;OpLesseq&gt;();
5294             Node* op1 = get(bytecode.m_lhs);
5295             Node* op2 = get(bytecode.m_rhs);
5296             set(bytecode.m_dst, addToGraph(CompareLessEq, op1, op2));
5297             NEXT_OPCODE(op_lesseq);
5298         }
5299 
5300         case op_greater: {
5301             auto bytecode = currentInstruction-&gt;as&lt;OpGreater&gt;();
5302             Node* op1 = get(bytecode.m_lhs);
5303             Node* op2 = get(bytecode.m_rhs);
5304             set(bytecode.m_dst, addToGraph(CompareGreater, op1, op2));
5305             NEXT_OPCODE(op_greater);
5306         }
5307 
5308         case op_greatereq: {
5309             auto bytecode = currentInstruction-&gt;as&lt;OpGreatereq&gt;();
5310             Node* op1 = get(bytecode.m_lhs);
5311             Node* op2 = get(bytecode.m_rhs);
5312             set(bytecode.m_dst, addToGraph(CompareGreaterEq, op1, op2));
5313             NEXT_OPCODE(op_greatereq);
5314         }
5315 
5316         case op_below: {
5317             auto bytecode = currentInstruction-&gt;as&lt;OpBelow&gt;();
5318             Node* op1 = get(bytecode.m_lhs);
5319             Node* op2 = get(bytecode.m_rhs);
5320             set(bytecode.m_dst, addToGraph(CompareBelow, op1, op2));
5321             NEXT_OPCODE(op_below);
5322         }
5323 
5324         case op_beloweq: {
5325             auto bytecode = currentInstruction-&gt;as&lt;OpBeloweq&gt;();
5326             Node* op1 = get(bytecode.m_lhs);
5327             Node* op2 = get(bytecode.m_rhs);
5328             set(bytecode.m_dst, addToGraph(CompareBelowEq, op1, op2));
5329             NEXT_OPCODE(op_beloweq);
5330         }
5331 
5332         case op_eq: {
5333             auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
5334             Node* op1 = get(bytecode.m_lhs);
5335             Node* op2 = get(bytecode.m_rhs);
5336             set(bytecode.m_dst, addToGraph(CompareEq, op1, op2));
5337             NEXT_OPCODE(op_eq);
5338         }
5339 
5340         case op_eq_null: {
5341             auto bytecode = currentInstruction-&gt;as&lt;OpEqNull&gt;();
5342             Node* value = get(bytecode.m_operand);
5343             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
5344             set(bytecode.m_dst, addToGraph(CompareEq, value, nullConstant));
5345             NEXT_OPCODE(op_eq_null);
5346         }
5347 
5348         case op_stricteq: {
5349             auto bytecode = currentInstruction-&gt;as&lt;OpStricteq&gt;();
5350             Node* op1 = get(bytecode.m_lhs);
5351             Node* op2 = get(bytecode.m_rhs);
5352             set(bytecode.m_dst, addToGraph(CompareStrictEq, op1, op2));
5353             NEXT_OPCODE(op_stricteq);
5354         }
5355 
5356         case op_neq: {
5357             auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
5358             Node* op1 = get(bytecode.m_lhs);
5359             Node* op2 = get(bytecode.m_rhs);
5360             set(bytecode.m_dst, addToGraph(LogicalNot, addToGraph(CompareEq, op1, op2)));
5361             NEXT_OPCODE(op_neq);
5362         }
5363 
5364         case op_neq_null: {
5365             auto bytecode = currentInstruction-&gt;as&lt;OpNeqNull&gt;();
5366             Node* value = get(bytecode.m_operand);
5367             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
5368             set(bytecode.m_dst, addToGraph(LogicalNot, addToGraph(CompareEq, value, nullConstant)));
5369             NEXT_OPCODE(op_neq_null);
5370         }
5371 
5372         case op_nstricteq: {
5373             auto bytecode = currentInstruction-&gt;as&lt;OpNstricteq&gt;();
5374             Node* op1 = get(bytecode.m_lhs);
5375             Node* op2 = get(bytecode.m_rhs);
5376             Node* invertedResult;
5377             invertedResult = addToGraph(CompareStrictEq, op1, op2);
5378             set(bytecode.m_dst, addToGraph(LogicalNot, invertedResult));
5379             NEXT_OPCODE(op_nstricteq);
5380         }
5381 
5382         // === Property access operations ===
5383 
5384         case op_get_by_val: {
5385             auto bytecode = currentInstruction-&gt;as&lt;OpGetByVal&gt;();
5386             SpeculatedType prediction = getPredictionWithoutOSRExit();
5387 
5388             Node* base = get(bytecode.m_base);
5389             Node* property = get(bytecode.m_property);
5390             bool compiledAsGetById = false;
5391             GetByIdStatus getByIdStatus;
5392             unsigned identifierNumber = 0;
5393             {
5394                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
5395                 ByValInfo* byValInfo = m_inlineStackTop-&gt;m_baselineMap.get(CodeOrigin(currentCodeOrigin().bytecodeIndex)).byValInfo;
5396                 // FIXME: When the bytecode is not compiled in the baseline JIT, byValInfo becomes null.
5397                 // At that time, there is no information.
5398                 if (byValInfo
5399                     &amp;&amp; byValInfo-&gt;stubInfo
5400                     &amp;&amp; !byValInfo-&gt;tookSlowPath
5401                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIdent)
5402                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType)
5403                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
5404                     compiledAsGetById = true;
5405                     identifierNumber = m_graph.identifiers().ensure(byValInfo-&gt;cachedId.impl());
5406                     UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
5407 
5408                     if (Symbol* symbol = byValInfo-&gt;cachedSymbol.get()) {
5409                         FrozenValue* frozen = m_graph.freezeStrong(symbol);
5410                         addToGraph(CheckCell, OpInfo(frozen), property);
5411                     } else {
5412                         ASSERT(!uid-&gt;isSymbol());
5413                         addToGraph(CheckStringIdent, OpInfo(uid), property);
5414                     }
5415 
5416                     getByIdStatus = GetByIdStatus::computeForStubInfo(
5417                         locker, m_inlineStackTop-&gt;m_profiledBlock,
5418                         byValInfo-&gt;stubInfo, currentCodeOrigin(), uid);
5419                 }
5420             }
5421 
5422             if (compiledAsGetById)
5423                 handleGetById(bytecode.m_dst, prediction, base, identifierNumber, getByIdStatus, AccessType::Get, currentInstruction-&gt;size());
5424             else {
5425                 ArrayMode arrayMode = getArrayMode(bytecode.metadata(codeBlock).m_arrayProfile, Array::Read);
5426                 // FIXME: We could consider making this not vararg, since it only uses three child
5427                 // slots.
5428                 // https://bugs.webkit.org/show_bug.cgi?id=184192
5429                 addVarArgChild(base);
5430                 addVarArgChild(property);
5431                 addVarArgChild(0); // Leave room for property storage.
5432                 Node* getByVal = addToGraph(Node::VarArg, GetByVal, OpInfo(arrayMode.asWord()), OpInfo(prediction));
5433                 m_exitOK = false; // GetByVal must be treated as if it clobbers exit state, since FixupPhase may make it generic.
5434                 set(bytecode.m_dst, getByVal);
5435             }
5436 
5437             NEXT_OPCODE(op_get_by_val);
5438         }
5439 
5440         case op_get_by_val_with_this: {
5441             auto bytecode = currentInstruction-&gt;as&lt;OpGetByValWithThis&gt;();
5442             SpeculatedType prediction = getPrediction();
5443 
5444             Node* base = get(bytecode.m_base);
5445             Node* thisValue = get(bytecode.m_thisValue);
5446             Node* property = get(bytecode.m_property);
5447             Node* getByValWithThis = addToGraph(GetByValWithThis, OpInfo(), OpInfo(prediction), base, thisValue, property);
5448             set(bytecode.m_dst, getByValWithThis);
5449 
5450             NEXT_OPCODE(op_get_by_val_with_this);
5451         }
5452 
5453         case op_put_by_val_direct:
5454             handlePutByVal(currentInstruction-&gt;as&lt;OpPutByValDirect&gt;(), currentInstruction-&gt;size());
5455             NEXT_OPCODE(op_put_by_val_direct);
5456 
5457         case op_put_by_val: {
5458             handlePutByVal(currentInstruction-&gt;as&lt;OpPutByVal&gt;(), currentInstruction-&gt;size());
5459             NEXT_OPCODE(op_put_by_val);
5460         }
5461 
5462         case op_put_by_val_with_this: {
5463             auto bytecode = currentInstruction-&gt;as&lt;OpPutByValWithThis&gt;();
5464             Node* base = get(bytecode.m_base);
5465             Node* thisValue = get(bytecode.m_thisValue);
5466             Node* property = get(bytecode.m_property);
5467             Node* value = get(bytecode.m_value);
5468 
5469             addVarArgChild(base);
5470             addVarArgChild(thisValue);
5471             addVarArgChild(property);
5472             addVarArgChild(value);
5473             addToGraph(Node::VarArg, PutByValWithThis, OpInfo(0), OpInfo(0));
5474 
5475             NEXT_OPCODE(op_put_by_val_with_this);
5476         }
5477 
5478         case op_define_data_property: {
5479             auto bytecode = currentInstruction-&gt;as&lt;OpDefineDataProperty&gt;();
5480             Node* base = get(bytecode.m_base);
5481             Node* property = get(bytecode.m_property);
5482             Node* value = get(bytecode.m_value);
5483             Node* attributes = get(bytecode.m_attributes);
5484 
5485             addVarArgChild(base);
5486             addVarArgChild(property);
5487             addVarArgChild(value);
5488             addVarArgChild(attributes);
5489             addToGraph(Node::VarArg, DefineDataProperty, OpInfo(0), OpInfo(0));
5490 
5491             NEXT_OPCODE(op_define_data_property);
5492         }
5493 
5494         case op_define_accessor_property: {
5495             auto bytecode = currentInstruction-&gt;as&lt;OpDefineAccessorProperty&gt;();
5496             Node* base = get(bytecode.m_base);
5497             Node* property = get(bytecode.m_property);
5498             Node* getter = get(bytecode.m_getter);
5499             Node* setter = get(bytecode.m_setter);
5500             Node* attributes = get(bytecode.m_attributes);
5501 
5502             addVarArgChild(base);
5503             addVarArgChild(property);
5504             addVarArgChild(getter);
5505             addVarArgChild(setter);
5506             addVarArgChild(attributes);
5507             addToGraph(Node::VarArg, DefineAccessorProperty, OpInfo(0), OpInfo(0));
5508 
5509             NEXT_OPCODE(op_define_accessor_property);
5510         }
5511 
5512         case op_get_by_id_direct: {
5513             parseGetById&lt;OpGetByIdDirect&gt;(currentInstruction);
5514             NEXT_OPCODE(op_get_by_id_direct);
5515         }
5516         case op_try_get_by_id: {
5517             parseGetById&lt;OpTryGetById&gt;(currentInstruction);
5518             NEXT_OPCODE(op_try_get_by_id);
5519         }
5520         case op_get_by_id: {
5521             parseGetById&lt;OpGetById&gt;(currentInstruction);
5522             NEXT_OPCODE(op_get_by_id);
5523         }
5524         case op_get_by_id_with_this: {
5525             SpeculatedType prediction = getPrediction();
5526 
5527             auto bytecode = currentInstruction-&gt;as&lt;OpGetByIdWithThis&gt;();
5528             Node* base = get(bytecode.m_base);
5529             Node* thisValue = get(bytecode.m_thisValue);
5530             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5531 
5532             set(bytecode.m_dst,
5533                 addToGraph(GetByIdWithThis, OpInfo(identifierNumber), OpInfo(prediction), base, thisValue));
5534 
5535             NEXT_OPCODE(op_get_by_id_with_this);
5536         }
5537         case op_put_by_id: {
5538             auto bytecode = currentInstruction-&gt;as&lt;OpPutById&gt;();
5539             Node* value = get(bytecode.m_value);
5540             Node* base = get(bytecode.m_base);
5541             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5542             bool direct = !!(bytecode.m_flags &amp; PutByIdIsDirect);
5543 
5544             PutByIdStatus putByIdStatus = PutByIdStatus::computeFor(
5545                 m_inlineStackTop-&gt;m_profiledBlock,
5546                 m_inlineStackTop-&gt;m_baselineMap, m_icContextStack,
5547                 currentCodeOrigin(), m_graph.identifiers()[identifierNumber]);
5548 
5549             handlePutById(base, identifierNumber, value, putByIdStatus, direct, currentInstruction-&gt;size());
5550             NEXT_OPCODE(op_put_by_id);
5551         }
5552 
5553         case op_put_by_id_with_this: {
5554             auto bytecode = currentInstruction-&gt;as&lt;OpPutByIdWithThis&gt;();
5555             Node* base = get(bytecode.m_base);
5556             Node* thisValue = get(bytecode.m_thisValue);
5557             Node* value = get(bytecode.m_value);
5558             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5559 
5560             addToGraph(PutByIdWithThis, OpInfo(identifierNumber), base, thisValue, value);
5561             NEXT_OPCODE(op_put_by_id_with_this);
5562         }
5563 
5564         case op_put_getter_by_id:
5565             handlePutAccessorById(PutGetterById, currentInstruction-&gt;as&lt;OpPutGetterById&gt;());
5566             NEXT_OPCODE(op_put_getter_by_id);
5567         case op_put_setter_by_id: {
5568             handlePutAccessorById(PutSetterById, currentInstruction-&gt;as&lt;OpPutSetterById&gt;());
5569             NEXT_OPCODE(op_put_setter_by_id);
5570         }
5571 
5572         case op_put_getter_setter_by_id: {
5573             auto bytecode = currentInstruction-&gt;as&lt;OpPutGetterSetterById&gt;();
5574             Node* base = get(bytecode.m_base);
5575             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5576             Node* getter = get(bytecode.m_getter);
5577             Node* setter = get(bytecode.m_setter);
5578             addToGraph(PutGetterSetterById, OpInfo(identifierNumber), OpInfo(bytecode.m_attributes), base, getter, setter);
5579             NEXT_OPCODE(op_put_getter_setter_by_id);
5580         }
5581 
5582         case op_put_getter_by_val:
5583             handlePutAccessorByVal(PutGetterByVal, currentInstruction-&gt;as&lt;OpPutGetterByVal&gt;());
5584             NEXT_OPCODE(op_put_getter_by_val);
5585         case op_put_setter_by_val: {
5586             handlePutAccessorByVal(PutSetterByVal, currentInstruction-&gt;as&lt;OpPutSetterByVal&gt;());
5587             NEXT_OPCODE(op_put_setter_by_val);
5588         }
5589 
5590         case op_del_by_id: {
5591             auto bytecode = currentInstruction-&gt;as&lt;OpDelById&gt;();
5592             Node* base = get(bytecode.m_base);
5593             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5594             set(bytecode.m_dst, addToGraph(DeleteById, OpInfo(identifierNumber), base));
5595             NEXT_OPCODE(op_del_by_id);
5596         }
5597 
5598         case op_del_by_val: {
5599             auto bytecode = currentInstruction-&gt;as&lt;OpDelByVal&gt;();
5600             Node* base = get(bytecode.m_base);
5601             Node* key = get(bytecode.m_property);
5602             set(bytecode.m_dst, addToGraph(DeleteByVal, base, key));
5603             NEXT_OPCODE(op_del_by_val);
5604         }
5605 
5606         case op_profile_type: {
5607             auto bytecode = currentInstruction-&gt;as&lt;OpProfileType&gt;();
5608             auto&amp; metadata = bytecode.metadata(codeBlock);
5609             Node* valueToProfile = get(bytecode.m_targetVirtualRegister);
5610             addToGraph(ProfileType, OpInfo(metadata.m_typeLocation), valueToProfile);
5611             NEXT_OPCODE(op_profile_type);
5612         }
5613 
5614         case op_profile_control_flow: {
5615             auto bytecode = currentInstruction-&gt;as&lt;OpProfileControlFlow&gt;();
5616             BasicBlockLocation* basicBlockLocation = bytecode.metadata(codeBlock).m_basicBlockLocation;
5617             addToGraph(ProfileControlFlow, OpInfo(basicBlockLocation));
5618             NEXT_OPCODE(op_profile_control_flow);
5619         }
5620 
5621         // === Block terminators. ===
5622 
5623         case op_jmp: {
5624             ASSERT(!m_currentBlock-&gt;terminal());
5625             auto bytecode = currentInstruction-&gt;as&lt;OpJmp&gt;();
5626             int relativeOffset = jumpTarget(bytecode.m_targetLabel);
5627             addToGraph(Jump, OpInfo(m_currentIndex + relativeOffset));
5628             if (relativeOffset &lt;= 0)
5629                 flushForTerminal();
5630             LAST_OPCODE(op_jmp);
5631         }
5632 
5633         case op_jtrue: {
5634             auto bytecode = currentInstruction-&gt;as&lt;OpJtrue&gt;();
5635             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5636             Node* condition = get(bytecode.m_condition);
5637             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5638             LAST_OPCODE(op_jtrue);
5639         }
5640 
5641         case op_jfalse: {
5642             auto bytecode = currentInstruction-&gt;as&lt;OpJfalse&gt;();
5643             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5644             Node* condition = get(bytecode.m_condition);
5645             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5646             LAST_OPCODE(op_jfalse);
5647         }
5648 
5649         case op_jeq_null: {
5650             auto bytecode = currentInstruction-&gt;as&lt;OpJeqNull&gt;();
5651             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5652             Node* value = get(bytecode.m_value);
5653             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
5654             Node* condition = addToGraph(CompareEq, value, nullConstant);
5655             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5656             LAST_OPCODE(op_jeq_null);
5657         }
5658 
5659         case op_jneq_null: {
5660             auto bytecode = currentInstruction-&gt;as&lt;OpJneqNull&gt;();
5661             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5662             Node* value = get(bytecode.m_value);
5663             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
5664             Node* condition = addToGraph(CompareEq, value, nullConstant);
5665             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5666             LAST_OPCODE(op_jneq_null);
5667         }
5668 
5669         case op_jless: {
5670             auto bytecode = currentInstruction-&gt;as&lt;OpJless&gt;();
5671             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5672             Node* op1 = get(bytecode.m_lhs);
5673             Node* op2 = get(bytecode.m_rhs);
5674             Node* condition = addToGraph(CompareLess, op1, op2);
5675             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5676             LAST_OPCODE(op_jless);
5677         }
5678 
5679         case op_jlesseq: {
5680             auto bytecode = currentInstruction-&gt;as&lt;OpJlesseq&gt;();
5681             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5682             Node* op1 = get(bytecode.m_lhs);
5683             Node* op2 = get(bytecode.m_rhs);
5684             Node* condition = addToGraph(CompareLessEq, op1, op2);
5685             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5686             LAST_OPCODE(op_jlesseq);
5687         }
5688 
5689         case op_jgreater: {
5690             auto bytecode = currentInstruction-&gt;as&lt;OpJgreater&gt;();
5691             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5692             Node* op1 = get(bytecode.m_lhs);
5693             Node* op2 = get(bytecode.m_rhs);
5694             Node* condition = addToGraph(CompareGreater, op1, op2);
5695             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5696             LAST_OPCODE(op_jgreater);
5697         }
5698 
5699         case op_jgreatereq: {
5700             auto bytecode = currentInstruction-&gt;as&lt;OpJgreatereq&gt;();
5701             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5702             Node* op1 = get(bytecode.m_lhs);
5703             Node* op2 = get(bytecode.m_rhs);
5704             Node* condition = addToGraph(CompareGreaterEq, op1, op2);
5705             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5706             LAST_OPCODE(op_jgreatereq);
5707         }
5708 
5709         case op_jeq: {
5710             auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
5711             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5712             Node* op1 = get(bytecode.m_lhs);
5713             Node* op2 = get(bytecode.m_rhs);
5714             Node* condition = addToGraph(CompareEq, op1, op2);
5715             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5716             LAST_OPCODE(op_jeq);
5717         }
5718 
5719         case op_jstricteq: {
5720             auto bytecode = currentInstruction-&gt;as&lt;OpJstricteq&gt;();
5721             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5722             Node* op1 = get(bytecode.m_lhs);
5723             Node* op2 = get(bytecode.m_rhs);
5724             Node* condition = addToGraph(CompareStrictEq, op1, op2);
5725             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5726             LAST_OPCODE(op_jstricteq);
5727         }
5728 
5729         case op_jnless: {
5730             auto bytecode = currentInstruction-&gt;as&lt;OpJnless&gt;();
5731             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5732             Node* op1 = get(bytecode.m_lhs);
5733             Node* op2 = get(bytecode.m_rhs);
5734             Node* condition = addToGraph(CompareLess, op1, op2);
5735             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5736             LAST_OPCODE(op_jnless);
5737         }
5738 
5739         case op_jnlesseq: {
5740             auto bytecode = currentInstruction-&gt;as&lt;OpJnlesseq&gt;();
5741             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5742             Node* op1 = get(bytecode.m_lhs);
5743             Node* op2 = get(bytecode.m_rhs);
5744             Node* condition = addToGraph(CompareLessEq, op1, op2);
5745             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5746             LAST_OPCODE(op_jnlesseq);
5747         }
5748 
5749         case op_jngreater: {
5750             auto bytecode = currentInstruction-&gt;as&lt;OpJngreater&gt;();
5751             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5752             Node* op1 = get(bytecode.m_lhs);
5753             Node* op2 = get(bytecode.m_rhs);
5754             Node* condition = addToGraph(CompareGreater, op1, op2);
5755             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5756             LAST_OPCODE(op_jngreater);
5757         }
5758 
5759         case op_jngreatereq: {
5760             auto bytecode = currentInstruction-&gt;as&lt;OpJngreatereq&gt;();
5761             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5762             Node* op1 = get(bytecode.m_lhs);
5763             Node* op2 = get(bytecode.m_rhs);
5764             Node* condition = addToGraph(CompareGreaterEq, op1, op2);
5765             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5766             LAST_OPCODE(op_jngreatereq);
5767         }
5768 
5769         case op_jneq: {
5770             auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
5771             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5772             Node* op1 = get(bytecode.m_lhs);
5773             Node* op2 = get(bytecode.m_rhs);
5774             Node* condition = addToGraph(CompareEq, op1, op2);
5775             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5776             LAST_OPCODE(op_jneq);
5777         }
5778 
5779         case op_jnstricteq: {
5780             auto bytecode = currentInstruction-&gt;as&lt;OpJnstricteq&gt;();
5781             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5782             Node* op1 = get(bytecode.m_lhs);
5783             Node* op2 = get(bytecode.m_rhs);
5784             Node* condition = addToGraph(CompareStrictEq, op1, op2);
5785             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5786             LAST_OPCODE(op_jnstricteq);
5787         }
5788 
5789         case op_jbelow: {
5790             auto bytecode = currentInstruction-&gt;as&lt;OpJbelow&gt;();
5791             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5792             Node* op1 = get(bytecode.m_lhs);
5793             Node* op2 = get(bytecode.m_rhs);
5794             Node* condition = addToGraph(CompareBelow, op1, op2);
5795             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5796             LAST_OPCODE(op_jbelow);
5797         }
5798 
5799         case op_jbeloweq: {
5800             auto bytecode = currentInstruction-&gt;as&lt;OpJbeloweq&gt;();
5801             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5802             Node* op1 = get(bytecode.m_lhs);
5803             Node* op2 = get(bytecode.m_rhs);
5804             Node* condition = addToGraph(CompareBelowEq, op1, op2);
5805             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5806             LAST_OPCODE(op_jbeloweq);
5807         }
5808 
5809         case op_switch_imm: {
5810             auto bytecode = currentInstruction-&gt;as&lt;OpSwitchImm&gt;();
5811             SwitchData&amp; data = *m_graph.m_switchData.add();
5812             data.kind = SwitchImm;
5813             data.switchTableIndex = m_inlineStackTop-&gt;m_switchRemap[bytecode.m_tableIndex];
5814             data.fallThrough.setBytecodeIndex(m_currentIndex + jumpTarget(bytecode.m_defaultOffset));
5815             SimpleJumpTable&amp; table = m_codeBlock-&gt;switchJumpTable(data.switchTableIndex);
5816             for (unsigned i = 0; i &lt; table.branchOffsets.size(); ++i) {
5817                 if (!table.branchOffsets[i])
5818                     continue;
5819                 unsigned target = m_currentIndex + table.branchOffsets[i];
5820                 if (target == data.fallThrough.bytecodeIndex())
5821                     continue;
5822                 data.cases.append(SwitchCase::withBytecodeIndex(m_graph.freeze(jsNumber(static_cast&lt;int32_t&gt;(table.min + i))), target));
5823             }
5824             addToGraph(Switch, OpInfo(&amp;data), get(bytecode.m_scrutinee));
5825             flushIfTerminal(data);
5826             LAST_OPCODE(op_switch_imm);
5827         }
5828 
5829         case op_switch_char: {
5830             auto bytecode = currentInstruction-&gt;as&lt;OpSwitchChar&gt;();
5831             SwitchData&amp; data = *m_graph.m_switchData.add();
5832             data.kind = SwitchChar;
5833             data.switchTableIndex = m_inlineStackTop-&gt;m_switchRemap[bytecode.m_tableIndex];
5834             data.fallThrough.setBytecodeIndex(m_currentIndex + jumpTarget(bytecode.m_defaultOffset));
5835             SimpleJumpTable&amp; table = m_codeBlock-&gt;switchJumpTable(data.switchTableIndex);
5836             for (unsigned i = 0; i &lt; table.branchOffsets.size(); ++i) {
5837                 if (!table.branchOffsets[i])
5838                     continue;
5839                 unsigned target = m_currentIndex + table.branchOffsets[i];
5840                 if (target == data.fallThrough.bytecodeIndex())
5841                     continue;
5842                 data.cases.append(
5843                     SwitchCase::withBytecodeIndex(LazyJSValue::singleCharacterString(table.min + i), target));
5844             }
5845             addToGraph(Switch, OpInfo(&amp;data), get(bytecode.m_scrutinee));
5846             flushIfTerminal(data);
5847             LAST_OPCODE(op_switch_char);
5848         }
5849 
5850         case op_switch_string: {
5851             auto bytecode = currentInstruction-&gt;as&lt;OpSwitchString&gt;();
5852             SwitchData&amp; data = *m_graph.m_switchData.add();
5853             data.kind = SwitchString;
5854             data.switchTableIndex = bytecode.m_tableIndex;
5855             data.fallThrough.setBytecodeIndex(m_currentIndex + jumpTarget(bytecode.m_defaultOffset));
5856             StringJumpTable&amp; table = m_codeBlock-&gt;stringSwitchJumpTable(data.switchTableIndex);
5857             StringJumpTable::StringOffsetTable::iterator iter;
5858             StringJumpTable::StringOffsetTable::iterator end = table.offsetTable.end();
5859             for (iter = table.offsetTable.begin(); iter != end; ++iter) {
5860                 unsigned target = m_currentIndex + iter-&gt;value.branchOffset;
5861                 if (target == data.fallThrough.bytecodeIndex())
5862                     continue;
5863                 data.cases.append(
5864                     SwitchCase::withBytecodeIndex(LazyJSValue::knownStringImpl(iter-&gt;key.get()), target));
5865             }
5866             addToGraph(Switch, OpInfo(&amp;data), get(bytecode.m_scrutinee));
5867             flushIfTerminal(data);
5868             LAST_OPCODE(op_switch_string);
5869         }
5870 
5871         case op_ret: {
5872             auto bytecode = currentInstruction-&gt;as&lt;OpRet&gt;();
5873             ASSERT(!m_currentBlock-&gt;terminal());
5874             if (!inlineCallFrame()) {
5875                 // Simple case: we are just producing a return
5876                 addToGraph(Return, get(bytecode.m_value));
5877                 flushForReturn();
5878                 LAST_OPCODE(op_ret);
5879             }
5880 
5881             flushForReturn();
5882             if (m_inlineStackTop-&gt;m_returnValue.isValid())
5883                 setDirect(m_inlineStackTop-&gt;m_returnValue, get(bytecode.m_value), ImmediateSetWithFlush);
5884 
5885             if (!m_inlineStackTop-&gt;m_continuationBlock &amp;&amp; m_currentIndex + currentInstruction-&gt;size() != m_inlineStackTop-&gt;m_codeBlock-&gt;instructions().size()) {
5886                 // This is an early return from an inlined function and we do not have a continuation block, so we must allocate one.
5887                 // It is untargetable, because we do not know the appropriate index.
5888                 // If this block turns out to be a jump target, parseCodeBlock will fix its bytecodeIndex before putting it in m_blockLinkingTargets
5889                 m_inlineStackTop-&gt;m_continuationBlock = allocateUntargetableBlock();
5890             }
5891 
5892             if (m_inlineStackTop-&gt;m_continuationBlock)
5893                 addJumpTo(m_inlineStackTop-&gt;m_continuationBlock);
5894             else {
5895                 // We are returning from an inlined function, and do not need to jump anywhere, so we just keep the current block
5896                 m_inlineStackTop-&gt;m_continuationBlock = m_currentBlock;
5897             }
5898             LAST_OPCODE_LINKED(op_ret);
5899         }
5900         case op_end:
5901             ASSERT(!inlineCallFrame());
5902             addToGraph(Return, get(currentInstruction-&gt;as&lt;OpEnd&gt;().m_value));
5903             flushForReturn();
5904             LAST_OPCODE(op_end);
5905 
5906         case op_throw:
5907             addToGraph(Throw, get(currentInstruction-&gt;as&lt;OpThrow&gt;().m_value));
5908             flushForTerminal();
5909             LAST_OPCODE(op_throw);
5910 
5911         case op_throw_static_error: {
5912             auto bytecode = currentInstruction-&gt;as&lt;OpThrowStaticError&gt;();
5913             addToGraph(ThrowStaticError, OpInfo(bytecode.m_errorType), get(bytecode.m_message));
5914             flushForTerminal();
5915             LAST_OPCODE(op_throw_static_error);
5916         }
5917 
5918         case op_catch: {
5919             auto bytecode = currentInstruction-&gt;as&lt;OpCatch&gt;();
5920             m_graph.m_hasExceptionHandlers = true;
5921 
5922             if (inlineCallFrame()) {
5923                 // We can&#39;t do OSR entry into an inlined frame.
5924                 NEXT_OPCODE(op_catch);
5925             }
5926 
5927             if (m_graph.m_plan.mode() == FTLForOSREntryMode) {
5928                 NEXT_OPCODE(op_catch);
5929             }
5930 
5931             RELEASE_ASSERT(!m_currentBlock-&gt;size() || (m_graph.compilation() &amp;&amp; m_currentBlock-&gt;size() == 1 &amp;&amp; m_currentBlock-&gt;at(0)-&gt;op() == CountExecution));
5932 
5933             ValueProfileAndOperandBuffer* buffer = bytecode.metadata(codeBlock).m_buffer;
5934 
5935             if (!buffer) {
5936                 NEXT_OPCODE(op_catch); // This catch has yet to execute. Note: this load can be racy with the main thread.
5937             }
5938 
5939             // We&#39;re now committed to compiling this as an entrypoint.
5940             m_currentBlock-&gt;isCatchEntrypoint = true;
5941             m_graph.m_roots.append(m_currentBlock);
5942 
5943             Vector&lt;SpeculatedType&gt; argumentPredictions(m_numArguments);
5944             Vector&lt;SpeculatedType&gt; localPredictions;
5945             HashSet&lt;unsigned, WTF::IntHash&lt;unsigned&gt;, WTF::UnsignedWithZeroKeyHashTraits&lt;unsigned&gt;&gt; seenArguments;
5946 
5947             {
5948                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
5949 
5950                 buffer-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {
5951                     VirtualRegister operand(profile.m_operand);
5952                     SpeculatedType prediction = profile.m_profile.computeUpdatedPrediction(locker);
5953                     if (operand.isLocal())
5954                         localPredictions.append(prediction);
5955                     else {
5956                         RELEASE_ASSERT(operand.isArgument());
5957                         RELEASE_ASSERT(static_cast&lt;uint32_t&gt;(operand.toArgument()) &lt; argumentPredictions.size());
5958                         if (validationEnabled())
5959                             seenArguments.add(operand.toArgument());
5960                         argumentPredictions[operand.toArgument()] = prediction;
5961                     }
5962                 });
5963 
5964                 if (validationEnabled()) {
5965                     for (unsigned argument = 0; argument &lt; m_numArguments; ++argument)
5966                         RELEASE_ASSERT(seenArguments.contains(argument));
5967                 }
5968             }
5969 
5970             Vector&lt;std::pair&lt;VirtualRegister, Node*&gt;&gt; localsToSet;
5971             localsToSet.reserveInitialCapacity(buffer-&gt;m_size); // Note: This will reserve more than the number of locals we see below because the buffer includes arguments.
5972 
5973             // We&#39;re not allowed to exit here since we would not properly recover values.
5974             // We first need to bootstrap the catch entrypoint state.
5975             m_exitOK = false;
5976 
5977             unsigned numberOfLocals = 0;
5978             buffer-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {
5979                 VirtualRegister operand(profile.m_operand);
5980                 if (operand.isArgument())
5981                     return;
5982                 ASSERT(operand.isLocal());
5983                 Node* value = addToGraph(ExtractCatchLocal, OpInfo(numberOfLocals), OpInfo(localPredictions[numberOfLocals]));
5984                 ++numberOfLocals;
5985                 addToGraph(MovHint, OpInfo(profile.m_operand), value);
5986                 localsToSet.uncheckedAppend(std::make_pair(operand, value));
5987             });
5988             if (numberOfLocals)
5989                 addToGraph(ClearCatchLocals);
5990 
5991             if (!m_graph.m_maxLocalsForCatchOSREntry)
5992                 m_graph.m_maxLocalsForCatchOSREntry = 0;
5993             m_graph.m_maxLocalsForCatchOSREntry = std::max(numberOfLocals, *m_graph.m_maxLocalsForCatchOSREntry);
5994 
5995             // We could not exit before this point in the program because we would not know how to do value
5996             // recovery for live locals. The above IR sets up the necessary state so we can recover values
5997             // during OSR exit.
5998             //
5999             // The nodes that follow here all exit to the following bytecode instruction, not
6000             // the op_catch. Exiting to op_catch is reserved for when an exception is thrown.
6001             // The SetArgument nodes that follow below may exit because we may hoist type checks
6002             // to them. The SetLocal nodes that follow below may exit because we may choose
6003             // a flush format that speculates on the type of the local.
6004             m_exitOK = true;
6005             addToGraph(ExitOK);
6006 
6007             {
6008                 auto addResult = m_graph.m_rootToArguments.add(m_currentBlock, ArgumentsVector());
6009                 RELEASE_ASSERT(addResult.isNewEntry);
6010                 ArgumentsVector&amp; entrypointArguments = addResult.iterator-&gt;value;
6011                 entrypointArguments.resize(m_numArguments);
6012 
6013                 unsigned exitBytecodeIndex = m_currentIndex + currentInstruction-&gt;size();
6014 
6015                 for (unsigned argument = 0; argument &lt; argumentPredictions.size(); ++argument) {
6016                     VariableAccessData* variable = newVariableAccessData(virtualRegisterForArgument(argument));
6017                     variable-&gt;predict(argumentPredictions[argument]);
6018 
6019                     variable-&gt;mergeStructureCheckHoistingFailed(
6020                         m_inlineStackTop-&gt;m_exitProfile.hasExitSite(exitBytecodeIndex, BadCache));
6021                     variable-&gt;mergeCheckArrayHoistingFailed(
6022                         m_inlineStackTop-&gt;m_exitProfile.hasExitSite(exitBytecodeIndex, BadIndexingType));
6023 
6024                     Node* setArgument = addToGraph(SetArgument, OpInfo(variable));
6025                     setArgument-&gt;origin.forExit.bytecodeIndex = exitBytecodeIndex;
6026                     m_currentBlock-&gt;variablesAtTail.setArgumentFirstTime(argument, setArgument);
6027                     entrypointArguments[argument] = setArgument;
6028                 }
6029             }
6030 
6031             for (const std::pair&lt;VirtualRegister, Node*&gt;&amp; pair : localsToSet) {
6032                 DelayedSetLocal delayed { currentCodeOrigin(), pair.first, pair.second, ImmediateNakedSet };
6033                 m_setLocalQueue.append(delayed);
6034             }
6035 
6036             NEXT_OPCODE(op_catch);
6037         }
6038 
6039         case op_call:
6040             handleCall&lt;OpCall&gt;(currentInstruction, Call, CallMode::Regular);
6041             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6042             NEXT_OPCODE(op_call);
6043 
6044         case op_tail_call: {
6045             flushForReturn();
6046             Terminality terminality = handleCall&lt;OpTailCall&gt;(currentInstruction, TailCall, CallMode::Tail);
6047             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6048             // If the call is terminal then we should not parse any further bytecodes as the TailCall will exit the function.
6049             // If the call is not terminal, however, then we want the subsequent op_ret/op_jmp to update metadata and clean
6050             // things up.
6051             if (terminality == NonTerminal)
6052                 NEXT_OPCODE(op_tail_call);
6053             else
6054                 LAST_OPCODE_LINKED(op_tail_call);
6055             // We use LAST_OPCODE_LINKED instead of LAST_OPCODE because if the tail call was optimized, it may now be a jump to a bytecode index in a different InlineStackEntry.
6056         }
6057 
6058         case op_construct:
6059             handleCall&lt;OpConstruct&gt;(currentInstruction, Construct, CallMode::Construct);
6060             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6061             NEXT_OPCODE(op_construct);
6062 
6063         case op_call_varargs: {
6064             handleVarargsCall&lt;OpCallVarargs&gt;(currentInstruction, CallVarargs, CallMode::Regular);
6065             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleVarargsCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6066             NEXT_OPCODE(op_call_varargs);
6067         }
6068 
6069         case op_tail_call_varargs: {
6070             flushForReturn();
6071             Terminality terminality = handleVarargsCall&lt;OpTailCallVarargs&gt;(currentInstruction, TailCallVarargs, CallMode::Tail);
6072             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleVarargsCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6073             // If the call is terminal then we should not parse any further bytecodes as the TailCall will exit the function.
6074             // If the call is not terminal, however, then we want the subsequent op_ret/op_jmp to update metadata and clean
6075             // things up.
6076             if (terminality == NonTerminal)
6077                 NEXT_OPCODE(op_tail_call_varargs);
6078             else
6079                 LAST_OPCODE(op_tail_call_varargs);
6080         }
6081 
6082         case op_tail_call_forward_arguments: {
6083             // We need to make sure that we don&#39;t unbox our arguments here since that won&#39;t be
6084             // done by the arguments object creation node as that node may not exist.
6085             noticeArgumentsUse();
6086             flushForReturn();
6087             Terminality terminality = handleVarargsCall&lt;OpTailCallForwardArguments&gt;(currentInstruction, TailCallForwardVarargs, CallMode::Tail);
6088             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleVarargsCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6089             // If the call is terminal then we should not parse any further bytecodes as the TailCall will exit the function.
6090             // If the call is not terminal, however, then we want the subsequent op_ret/op_jmp to update metadata and clean
6091             // things up.
6092             if (terminality == NonTerminal)
6093                 NEXT_OPCODE(op_tail_call_forward_arguments);
6094             else
6095                 LAST_OPCODE(op_tail_call_forward_arguments);
6096         }
6097 
6098         case op_construct_varargs: {
6099             handleVarargsCall&lt;OpConstructVarargs&gt;(currentInstruction, ConstructVarargs, CallMode::Construct);
6100             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleVarargsCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6101             NEXT_OPCODE(op_construct_varargs);
6102         }
6103 
6104         case op_call_eval: {
6105             auto bytecode = currentInstruction-&gt;as&lt;OpCallEval&gt;();
6106             int registerOffset = -bytecode.m_argv;
6107             addCall(bytecode.m_dst, CallEval, nullptr, get(bytecode.m_callee), bytecode.m_argc, registerOffset, getPrediction());
6108             NEXT_OPCODE(op_call_eval);
6109         }
6110 
6111         case op_jneq_ptr: {
6112             auto bytecode = currentInstruction-&gt;as&lt;OpJneqPtr&gt;();
6113             Special::Pointer specialPointer = bytecode.m_specialPointer;
6114             ASSERT(pointerIsCell(specialPointer));
6115             JSCell* actualPointer = static_cast&lt;JSCell*&gt;(
6116                 actualPointerFor(m_inlineStackTop-&gt;m_codeBlock, specialPointer));
6117             FrozenValue* frozenPointer = m_graph.freeze(actualPointer);
6118             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6119             Node* child = get(bytecode.m_value);
6120             if (bytecode.metadata(codeBlock).m_hasJumped) {
6121                 Node* condition = addToGraph(CompareEqPtr, OpInfo(frozenPointer), child);
6122                 addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
6123                 LAST_OPCODE(op_jneq_ptr);
6124             }
6125             addToGraph(CheckCell, OpInfo(frozenPointer), child);
6126             NEXT_OPCODE(op_jneq_ptr);
6127         }
6128 
6129         case op_resolve_scope: {
6130             auto bytecode = currentInstruction-&gt;as&lt;OpResolveScope&gt;();
6131             auto&amp; metadata = bytecode.metadata(codeBlock);
6132 
6133             ResolveType resolveType;
6134             unsigned depth;
6135             JSScope* constantScope = nullptr;
6136             JSCell* lexicalEnvironment = nullptr;
6137             SymbolTable* symbolTable = nullptr;
6138             {
6139                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
6140                 resolveType = metadata.m_resolveType;
6141                 depth = metadata.m_localScopeDepth;
6142                 switch (resolveType) {
6143                 case GlobalProperty:
6144                 case GlobalVar:
6145                 case GlobalPropertyWithVarInjectionChecks:
6146                 case GlobalVarWithVarInjectionChecks:
6147                 case GlobalLexicalVar:
6148                 case GlobalLexicalVarWithVarInjectionChecks:
6149                     constantScope = metadata.m_constantScope.get();
6150                     break;
6151                 case ModuleVar:
6152                     lexicalEnvironment = metadata.m_lexicalEnvironment.get();
6153                     break;
6154                 case LocalClosureVar:
6155                 case ClosureVar:
6156                 case ClosureVarWithVarInjectionChecks:
6157                     symbolTable = metadata.m_symbolTable.get();
6158                     break;
6159                 default:
6160                     break;
6161                 }
6162             }
6163 
6164             if (needsDynamicLookup(resolveType, op_resolve_scope)) {
6165                 unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_var];
6166                 set(bytecode.m_dst, addToGraph(ResolveScope, OpInfo(identifierNumber), get(bytecode.m_scope)));
6167                 NEXT_OPCODE(op_resolve_scope);
6168             }
6169 
6170             // get_from_scope and put_to_scope depend on this watchpoint forcing OSR exit, so they don&#39;t add their own watchpoints.
6171             if (needsVarInjectionChecks(resolveType))
6172                 m_graph.watchpoints().addLazily(m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject()-&gt;varInjectionWatchpoint());
6173 
6174             // FIXME: Currently, module code do not query to JSGlobalLexicalEnvironment. So this case should be removed once it is fixed.
6175             // https://bugs.webkit.org/show_bug.cgi?id=193347
6176             if (m_inlineStackTop-&gt;m_codeBlock-&gt;scriptMode() != JSParserScriptMode::Module) {
6177                 if (resolveType == GlobalProperty || resolveType == GlobalPropertyWithVarInjectionChecks) {
6178                     JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
6179                     unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_var];
6180                     if (!m_graph.watchGlobalProperty(globalObject, identifierNumber))
6181                         addToGraph(ForceOSRExit);
6182                 }
6183             }
6184 
6185             switch (resolveType) {
6186             case GlobalProperty:
6187             case GlobalVar:
6188             case GlobalPropertyWithVarInjectionChecks:
6189             case GlobalVarWithVarInjectionChecks:
6190             case GlobalLexicalVar:
6191             case GlobalLexicalVarWithVarInjectionChecks: {
6192                 RELEASE_ASSERT(constantScope);
6193                 RELEASE_ASSERT(constantScope == JSScope::constantScopeForCodeBlock(resolveType, m_inlineStackTop-&gt;m_codeBlock));
6194                 set(bytecode.m_dst, weakJSConstant(constantScope));
6195                 addToGraph(Phantom, get(bytecode.m_scope));
6196                 break;
6197             }
6198             case ModuleVar: {
6199                 // Since the value of the &quot;scope&quot; virtual register is not used in LLInt / baseline op_resolve_scope with ModuleVar,
6200                 // we need not to keep it alive by the Phantom node.
6201                 // Module environment is already strongly referenced by the CodeBlock.
6202                 set(bytecode.m_dst, weakJSConstant(lexicalEnvironment));
6203                 break;
6204             }
6205             case LocalClosureVar:
6206             case ClosureVar:
6207             case ClosureVarWithVarInjectionChecks: {
6208                 Node* localBase = get(bytecode.m_scope);
6209                 addToGraph(Phantom, localBase); // OSR exit cannot handle resolve_scope on a DCE&#39;d scope.
6210 
6211                 // We have various forms of constant folding here. This is necessary to avoid
6212                 // spurious recompiles in dead-but-foldable code.
6213                 if (symbolTable) {
6214                     InferredValue* singleton = symbolTable-&gt;singletonScope();
6215                     if (JSValue value = singleton-&gt;inferredValue()) {
6216                         m_graph.watchpoints().addLazily(singleton);
6217                         set(bytecode.m_dst, weakJSConstant(value));
6218                         break;
6219                     }
6220                 }
6221                 if (JSScope* scope = localBase-&gt;dynamicCastConstant&lt;JSScope*&gt;(*m_vm)) {
6222                     for (unsigned n = depth; n--;)
6223                         scope = scope-&gt;next();
6224                     set(bytecode.m_dst, weakJSConstant(scope));
6225                     break;
6226                 }
6227                 for (unsigned n = depth; n--;)
6228                     localBase = addToGraph(SkipScope, localBase);
6229                 set(bytecode.m_dst, localBase);
6230                 break;
6231             }
6232             case UnresolvedProperty:
6233             case UnresolvedPropertyWithVarInjectionChecks: {
6234                 addToGraph(Phantom, get(bytecode.m_scope));
6235                 addToGraph(ForceOSRExit);
6236                 set(bytecode.m_dst, addToGraph(JSConstant, OpInfo(m_constantNull)));
6237                 break;
6238             }
6239             case Dynamic:
6240                 RELEASE_ASSERT_NOT_REACHED();
6241                 break;
6242             }
6243             NEXT_OPCODE(op_resolve_scope);
6244         }
6245         case op_resolve_scope_for_hoisting_func_decl_in_eval: {
6246             auto bytecode = currentInstruction-&gt;as&lt;OpResolveScopeForHoistingFuncDeclInEval&gt;();
6247             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
6248 
6249             set(bytecode.m_dst, addToGraph(ResolveScopeForHoistingFuncDeclInEval, OpInfo(identifierNumber), get(bytecode.m_scope)));
6250 
6251             NEXT_OPCODE(op_resolve_scope_for_hoisting_func_decl_in_eval);
6252         }
6253 
6254         case op_get_from_scope: {
6255             auto bytecode = currentInstruction-&gt;as&lt;OpGetFromScope&gt;();
6256             auto&amp; metadata = bytecode.metadata(codeBlock);
6257             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_var];
6258             UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
6259 
6260             ResolveType resolveType;
6261             GetPutInfo getPutInfo(0);
6262             Structure* structure = 0;
6263             WatchpointSet* watchpoints = 0;
6264             uintptr_t operand;
6265             {
6266                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
6267                 getPutInfo = metadata.m_getPutInfo;
6268                 resolveType = getPutInfo.resolveType();
6269                 if (resolveType == GlobalVar || resolveType == GlobalVarWithVarInjectionChecks || resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks)
6270                     watchpoints = metadata.m_watchpointSet;
6271                 else if (resolveType != UnresolvedProperty &amp;&amp; resolveType != UnresolvedPropertyWithVarInjectionChecks)
6272                     structure = metadata.m_structure.get();
6273                 operand = metadata.m_operand;
6274             }
6275 
6276             if (needsDynamicLookup(resolveType, op_get_from_scope)) {
6277                 uint64_t opInfo1 = makeDynamicVarOpInfo(identifierNumber, getPutInfo.operand());
6278                 SpeculatedType prediction = getPrediction();
6279                 set(bytecode.m_dst,
6280                     addToGraph(GetDynamicVar, OpInfo(opInfo1), OpInfo(prediction), get(bytecode.m_scope)));
6281                 NEXT_OPCODE(op_get_from_scope);
6282             }
6283 
6284             UNUSED_PARAM(watchpoints); // We will use this in the future. For now we set it as a way of documenting the fact that that&#39;s what index 5 is in GlobalVar mode.
6285 
6286             JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
6287 
6288             switch (resolveType) {
6289             case GlobalProperty:
6290             case GlobalPropertyWithVarInjectionChecks: {
6291                 // FIXME: Currently, module code do not query to JSGlobalLexicalEnvironment. So this case should be removed once it is fixed.
6292                 // https://bugs.webkit.org/show_bug.cgi?id=193347
6293                 if (m_inlineStackTop-&gt;m_codeBlock-&gt;scriptMode() != JSParserScriptMode::Module) {
6294                     if (!m_graph.watchGlobalProperty(globalObject, identifierNumber))
6295                         addToGraph(ForceOSRExit);
6296                 }
6297 
6298                 SpeculatedType prediction = getPrediction();
6299 
6300                 GetByIdStatus status = GetByIdStatus::computeFor(structure, uid);
6301                 if (status.state() != GetByIdStatus::Simple
6302                     || status.numVariants() != 1
6303                     || status[0].structureSet().size() != 1) {
6304                     set(bytecode.m_dst, addToGraph(GetByIdFlush, OpInfo(identifierNumber), OpInfo(prediction), get(bytecode.m_scope)));
6305                     break;
6306                 }
6307 
6308                 Node* base = weakJSConstant(globalObject);
6309                 Node* result = load(prediction, base, identifierNumber, status[0]);
6310                 addToGraph(Phantom, get(bytecode.m_scope));
6311                 set(bytecode.m_dst, result);
6312                 break;
6313             }
6314             case GlobalVar:
6315             case GlobalVarWithVarInjectionChecks:
6316             case GlobalLexicalVar:
6317             case GlobalLexicalVarWithVarInjectionChecks: {
6318                 addToGraph(Phantom, get(bytecode.m_scope));
6319                 WatchpointSet* watchpointSet;
6320                 ScopeOffset offset;
6321                 JSSegmentedVariableObject* scopeObject = jsCast&lt;JSSegmentedVariableObject*&gt;(JSScope::constantScopeForCodeBlock(resolveType, m_inlineStackTop-&gt;m_codeBlock));
6322                 {
6323                     ConcurrentJSLocker locker(scopeObject-&gt;symbolTable()-&gt;m_lock);
6324                     SymbolTableEntry entry = scopeObject-&gt;symbolTable()-&gt;get(locker, uid);
6325                     watchpointSet = entry.watchpointSet();
6326                     offset = entry.scopeOffset();
6327                 }
6328                 if (watchpointSet &amp;&amp; watchpointSet-&gt;state() == IsWatched) {
6329                     // This has a fun concurrency story. There is the possibility of a race in two
6330                     // directions:
6331                     //
6332                     // We see that the set IsWatched, but in the meantime it gets invalidated: this is
6333                     // fine because if we saw that it IsWatched then we add a watchpoint. If it gets
6334                     // invalidated, then this compilation is invalidated. Note that in the meantime we
6335                     // may load an absurd value from the global object. It&#39;s fine to load an absurd
6336                     // value if the compilation is invalidated anyway.
6337                     //
6338                     // We see that the set IsWatched, but the value isn&#39;t yet initialized: this isn&#39;t
6339                     // possible because of the ordering of operations.
6340                     //
6341                     // Here&#39;s how we order operations:
6342                     //
6343                     // Main thread stores to the global object: always store a value first, and only
6344                     // after that do we touch the watchpoint set. There is a fence in the touch, that
6345                     // ensures that the store to the global object always happens before the touch on the
6346                     // set.
6347                     //
6348                     // Compilation thread: always first load the state of the watchpoint set, and then
6349                     // load the value. The WatchpointSet::state() method does fences for us to ensure
6350                     // that the load of the state happens before our load of the value.
6351                     //
6352                     // Finalizing compilation: this happens on the main thread and synchronously checks
6353                     // validity of all watchpoint sets.
6354                     //
6355                     // We will only perform optimizations if the load of the state yields IsWatched. That
6356                     // means that at least one store would have happened to initialize the original value
6357                     // of the variable (that is, the value we&#39;d like to constant fold to). There may be
6358                     // other stores that happen after that, but those stores will invalidate the
6359                     // watchpoint set and also the compilation.
6360 
6361                     // Note that we need to use the operand, which is a direct pointer at the global,
6362                     // rather than looking up the global by doing variableAt(offset). That&#39;s because the
6363                     // internal data structures of JSSegmentedVariableObject are not thread-safe even
6364                     // though accessing the global itself is. The segmentation involves a vector spine
6365                     // that resizes with malloc/free, so if new globals unrelated to the one we are
6366                     // reading are added, we might access freed memory if we do variableAt().
6367                     WriteBarrier&lt;Unknown&gt;* pointer = bitwise_cast&lt;WriteBarrier&lt;Unknown&gt;*&gt;(operand);
6368 
6369                     ASSERT(scopeObject-&gt;findVariableIndex(pointer) == offset);
6370 
6371                     JSValue value = pointer-&gt;get();
6372                     if (value) {
6373                         m_graph.watchpoints().addLazily(watchpointSet);
6374                         set(bytecode.m_dst, weakJSConstant(value));
6375                         break;
6376                     }
6377                 }
6378 
6379                 SpeculatedType prediction = getPrediction();
6380                 NodeType nodeType;
6381                 if (resolveType == GlobalVar || resolveType == GlobalVarWithVarInjectionChecks)
6382                     nodeType = GetGlobalVar;
6383                 else
6384                     nodeType = GetGlobalLexicalVariable;
6385                 Node* value = addToGraph(nodeType, OpInfo(operand), OpInfo(prediction));
6386                 if (resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks)
6387                     addToGraph(CheckNotEmpty, value);
6388                 set(bytecode.m_dst, value);
6389                 break;
6390             }
6391             case LocalClosureVar:
6392             case ClosureVar:
6393             case ClosureVarWithVarInjectionChecks: {
6394                 Node* scopeNode = get(bytecode.m_scope);
6395 
6396                 // Ideally we wouldn&#39;t have to do this Phantom. But:
6397                 //
6398                 // For the constant case: we must do it because otherwise we would have no way of knowing
6399                 // that the scope is live at OSR here.
6400                 //
6401                 // For the non-constant case: GetClosureVar could be DCE&#39;d, but baseline&#39;s implementation
6402                 // won&#39;t be able to handle an Undefined scope.
6403                 addToGraph(Phantom, scopeNode);
6404 
6405                 // Constant folding in the bytecode parser is important for performance. This may not
6406                 // have executed yet. If it hasn&#39;t, then we won&#39;t have a prediction. Lacking a
6407                 // prediction, we&#39;d otherwise think that it has to exit. Then when it did execute, we
6408                 // would recompile. But if we can fold it here, we avoid the exit.
6409                 if (JSValue value = m_graph.tryGetConstantClosureVar(scopeNode, ScopeOffset(operand))) {
6410                     set(bytecode.m_dst, weakJSConstant(value));
6411                     break;
6412                 }
6413                 SpeculatedType prediction = getPrediction();
6414                 set(bytecode.m_dst,
6415                     addToGraph(GetClosureVar, OpInfo(operand), OpInfo(prediction), scopeNode));
6416                 break;
6417             }
6418             case UnresolvedProperty:
6419             case UnresolvedPropertyWithVarInjectionChecks:
6420             case ModuleVar:
6421             case Dynamic:
6422                 RELEASE_ASSERT_NOT_REACHED();
6423                 break;
6424             }
6425             NEXT_OPCODE(op_get_from_scope);
6426         }
6427 
6428         case op_put_to_scope: {
6429             auto bytecode = currentInstruction-&gt;as&lt;OpPutToScope&gt;();
6430             auto&amp; metadata = bytecode.metadata(codeBlock);
6431             unsigned identifierNumber = bytecode.m_var;
6432             if (identifierNumber != UINT_MAX)
6433                 identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[identifierNumber];
6434             UniquedStringImpl* uid;
6435             if (identifierNumber != UINT_MAX)
6436                 uid = m_graph.identifiers()[identifierNumber];
6437             else
6438                 uid = nullptr;
6439 
6440             ResolveType resolveType;
6441             GetPutInfo getPutInfo(0);
6442             Structure* structure = nullptr;
6443             WatchpointSet* watchpoints = nullptr;
6444             uintptr_t operand;
6445             {
6446                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
6447                 getPutInfo = metadata.m_getPutInfo;
6448                 resolveType = getPutInfo.resolveType();
6449                 if (resolveType == GlobalVar || resolveType == GlobalVarWithVarInjectionChecks || resolveType == LocalClosureVar || resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks)
6450                     watchpoints = metadata.m_watchpointSet;
6451                 else if (resolveType != UnresolvedProperty &amp;&amp; resolveType != UnresolvedPropertyWithVarInjectionChecks)
6452                     structure = metadata.m_structure.get();
6453                 operand = metadata.m_operand;
6454             }
6455 
6456             JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
6457 
6458             if (needsDynamicLookup(resolveType, op_put_to_scope)) {
6459                 ASSERT(identifierNumber != UINT_MAX);
6460                 uint64_t opInfo1 = makeDynamicVarOpInfo(identifierNumber, getPutInfo.operand());
6461                 addToGraph(PutDynamicVar, OpInfo(opInfo1), OpInfo(), get(bytecode.m_scope), get(bytecode.m_value));
6462                 NEXT_OPCODE(op_put_to_scope);
6463             }
6464 
6465             switch (resolveType) {
6466             case GlobalProperty:
6467             case GlobalPropertyWithVarInjectionChecks: {
6468                 // FIXME: Currently, module code do not query to JSGlobalLexicalEnvironment. So this case should be removed once it is fixed.
6469                 // https://bugs.webkit.org/show_bug.cgi?id=193347
6470                 if (m_inlineStackTop-&gt;m_codeBlock-&gt;scriptMode() != JSParserScriptMode::Module) {
6471                     if (!m_graph.watchGlobalProperty(globalObject, identifierNumber))
6472                         addToGraph(ForceOSRExit);
6473                 }
6474 
6475                 PutByIdStatus status;
6476                 if (uid)
6477                     status = PutByIdStatus::computeFor(globalObject, structure, uid, false);
6478                 else
6479                     status = PutByIdStatus(PutByIdStatus::TakesSlowPath);
6480                 if (status.numVariants() != 1
6481                     || status[0].kind() != PutByIdVariant::Replace
6482                     || status[0].structure().size() != 1) {
6483                     addToGraph(PutById, OpInfo(identifierNumber), get(bytecode.m_scope), get(bytecode.m_value));
6484                     break;
6485                 }
6486                 Node* base = weakJSConstant(globalObject);
6487                 store(base, identifierNumber, status[0], get(bytecode.m_value));
6488                 // Keep scope alive until after put.
6489                 addToGraph(Phantom, get(bytecode.m_scope));
6490                 break;
6491             }
6492             case GlobalLexicalVar:
6493             case GlobalLexicalVarWithVarInjectionChecks:
6494             case GlobalVar:
6495             case GlobalVarWithVarInjectionChecks: {
6496                 if (!isInitialization(getPutInfo.initializationMode()) &amp;&amp; (resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks)) {
6497                     SpeculatedType prediction = SpecEmpty;
6498                     Node* value = addToGraph(GetGlobalLexicalVariable, OpInfo(operand), OpInfo(prediction));
6499                     addToGraph(CheckNotEmpty, value);
6500                 }
6501 
6502                 JSSegmentedVariableObject* scopeObject = jsCast&lt;JSSegmentedVariableObject*&gt;(JSScope::constantScopeForCodeBlock(resolveType, m_inlineStackTop-&gt;m_codeBlock));
6503                 if (watchpoints) {
6504                     SymbolTableEntry entry = scopeObject-&gt;symbolTable()-&gt;get(uid);
6505                     ASSERT_UNUSED(entry, watchpoints == entry.watchpointSet());
6506                 }
6507                 Node* valueNode = get(bytecode.m_value);
6508                 addToGraph(PutGlobalVariable, OpInfo(operand), weakJSConstant(scopeObject), valueNode);
6509                 if (watchpoints &amp;&amp; watchpoints-&gt;state() != IsInvalidated) {
6510                     // Must happen after the store. See comment for GetGlobalVar.
6511                     addToGraph(NotifyWrite, OpInfo(watchpoints));
6512                 }
6513                 // Keep scope alive until after put.
6514                 addToGraph(Phantom, get(bytecode.m_scope));
6515                 break;
6516             }
6517             case LocalClosureVar:
6518             case ClosureVar:
6519             case ClosureVarWithVarInjectionChecks: {
6520                 Node* scopeNode = get(bytecode.m_scope);
6521                 Node* valueNode = get(bytecode.m_value);
6522 
6523                 addToGraph(PutClosureVar, OpInfo(operand), scopeNode, valueNode);
6524 
6525                 if (watchpoints &amp;&amp; watchpoints-&gt;state() != IsInvalidated) {
6526                     // Must happen after the store. See comment for GetGlobalVar.
6527                     addToGraph(NotifyWrite, OpInfo(watchpoints));
6528                 }
6529                 break;
6530             }
6531 
6532             case ModuleVar:
6533                 // Need not to keep &quot;scope&quot; and &quot;value&quot; register values here by Phantom because
6534                 // they are not used in LLInt / baseline op_put_to_scope with ModuleVar.
6535                 addToGraph(ForceOSRExit);
6536                 break;
6537 
6538             case Dynamic:
6539             case UnresolvedProperty:
6540             case UnresolvedPropertyWithVarInjectionChecks:
6541                 RELEASE_ASSERT_NOT_REACHED();
6542                 break;
6543             }
6544             NEXT_OPCODE(op_put_to_scope);
6545         }
6546 
6547         case op_loop_hint: {
6548             // Baseline-&gt;DFG OSR jumps between loop hints. The DFG assumes that Baseline-&gt;DFG
6549             // OSR can only happen at basic block boundaries. Assert that these two statements
6550             // are compatible.
6551             RELEASE_ASSERT(m_currentIndex == blockBegin);
6552 
6553             // We never do OSR into an inlined code block. That could not happen, since OSR
6554             // looks up the code block that is the replacement for the baseline JIT code
6555             // block. Hence, machine code block = true code block = not inline code block.
6556             if (!m_inlineStackTop-&gt;m_caller)
6557                 m_currentBlock-&gt;isOSRTarget = true;
6558 
6559             addToGraph(LoopHint);
6560             NEXT_OPCODE(op_loop_hint);
6561         }
6562 
6563         case op_check_traps: {
6564             addToGraph(Options::usePollingTraps() ? CheckTraps : InvalidationPoint);
6565             NEXT_OPCODE(op_check_traps);
6566         }
6567 
6568         case op_nop: {
6569             addToGraph(Check); // We add a nop here so that basic block linking doesn&#39;t break.
6570             NEXT_OPCODE(op_nop);
6571         }
6572 
6573         case op_super_sampler_begin: {
6574             addToGraph(SuperSamplerBegin);
6575             NEXT_OPCODE(op_super_sampler_begin);
6576         }
6577 
6578         case op_super_sampler_end: {
6579             addToGraph(SuperSamplerEnd);
6580             NEXT_OPCODE(op_super_sampler_end);
6581         }
6582 
6583         case op_create_lexical_environment: {
6584             auto bytecode = currentInstruction-&gt;as&lt;OpCreateLexicalEnvironment&gt;();
6585             ASSERT(bytecode.m_symbolTable.isConstant() &amp;&amp; bytecode.m_initialValue.isConstant());
6586             FrozenValue* symbolTable = m_graph.freezeStrong(m_inlineStackTop-&gt;m_codeBlock-&gt;getConstant(bytecode.m_symbolTable.offset()));
6587             FrozenValue* initialValue = m_graph.freezeStrong(m_inlineStackTop-&gt;m_codeBlock-&gt;getConstant(bytecode.m_initialValue.offset()));
6588             Node* scope = get(bytecode.m_scope);
6589             Node* lexicalEnvironment = addToGraph(CreateActivation, OpInfo(symbolTable), OpInfo(initialValue), scope);
6590             set(bytecode.m_dst, lexicalEnvironment);
6591             NEXT_OPCODE(op_create_lexical_environment);
6592         }
6593 
6594         case op_push_with_scope: {
6595             auto bytecode = currentInstruction-&gt;as&lt;OpPushWithScope&gt;();
6596             Node* currentScope = get(bytecode.m_currentScope);
6597             Node* object = get(bytecode.m_newScope);
6598             set(bytecode.m_dst, addToGraph(PushWithScope, currentScope, object));
6599             NEXT_OPCODE(op_push_with_scope);
6600         }
6601 
6602         case op_get_parent_scope: {
6603             auto bytecode = currentInstruction-&gt;as&lt;OpGetParentScope&gt;();
6604             Node* currentScope = get(bytecode.m_scope);
6605             Node* newScope = addToGraph(SkipScope, currentScope);
6606             set(bytecode.m_dst, newScope);
6607             addToGraph(Phantom, currentScope);
6608             NEXT_OPCODE(op_get_parent_scope);
6609         }
6610 
6611         case op_get_scope: {
6612             // Help the later stages a bit by doing some small constant folding here. Note that this
6613             // only helps for the first basic block. It&#39;s extremely important not to constant fold
6614             // loads from the scope register later, as that would prevent the DFG from tracking the
6615             // bytecode-level liveness of the scope register.
6616             auto bytecode = currentInstruction-&gt;as&lt;OpGetScope&gt;();
6617             Node* callee = get(VirtualRegister(CallFrameSlot::callee));
6618             Node* result;
6619             if (JSFunction* function = callee-&gt;dynamicCastConstant&lt;JSFunction*&gt;(*m_vm))
6620                 result = weakJSConstant(function-&gt;scope());
6621             else
6622                 result = addToGraph(GetScope, callee);
6623             set(bytecode.m_dst, result);
6624             NEXT_OPCODE(op_get_scope);
6625         }
6626 
6627         case op_argument_count: {
6628             auto bytecode = currentInstruction-&gt;as&lt;OpArgumentCount&gt;();
6629             Node* sub = addToGraph(ArithSub, OpInfo(Arith::Unchecked), OpInfo(SpecInt32Only), getArgumentCount(), addToGraph(JSConstant, OpInfo(m_constantOne)));
6630             set(bytecode.m_dst, sub);
6631             NEXT_OPCODE(op_argument_count);
6632         }
6633 
6634         case op_create_direct_arguments: {
6635             auto bytecode = currentInstruction-&gt;as&lt;OpCreateDirectArguments&gt;();
6636             noticeArgumentsUse();
6637             Node* createArguments = addToGraph(CreateDirectArguments);
6638             set(bytecode.m_dst, createArguments);
6639             NEXT_OPCODE(op_create_direct_arguments);
6640         }
6641 
6642         case op_create_scoped_arguments: {
6643             auto bytecode = currentInstruction-&gt;as&lt;OpCreateScopedArguments&gt;();
6644             noticeArgumentsUse();
6645             Node* createArguments = addToGraph(CreateScopedArguments, get(bytecode.m_scope));
6646             set(bytecode.m_dst, createArguments);
6647             NEXT_OPCODE(op_create_scoped_arguments);
6648         }
6649 
6650         case op_create_cloned_arguments: {
6651             auto bytecode = currentInstruction-&gt;as&lt;OpCreateClonedArguments&gt;();
6652             noticeArgumentsUse();
6653             Node* createArguments = addToGraph(CreateClonedArguments);
6654             set(bytecode.m_dst, createArguments);
6655             NEXT_OPCODE(op_create_cloned_arguments);
6656         }
6657 
6658         case op_get_from_arguments: {
6659             auto bytecode = currentInstruction-&gt;as&lt;OpGetFromArguments&gt;();
6660             set(bytecode.m_dst,
6661                 addToGraph(
6662                     GetFromArguments,
6663                     OpInfo(bytecode.m_index),
6664                     OpInfo(getPrediction()),
6665                     get(bytecode.m_arguments)));
6666             NEXT_OPCODE(op_get_from_arguments);
6667         }
6668 
6669         case op_put_to_arguments: {
6670             auto bytecode = currentInstruction-&gt;as&lt;OpPutToArguments&gt;();
6671             addToGraph(
6672                 PutToArguments,
6673                 OpInfo(bytecode.m_index),
6674                 get(bytecode.m_arguments),
6675                 get(bytecode.m_value));
6676             NEXT_OPCODE(op_put_to_arguments);
6677         }
6678 
6679         case op_get_argument: {
6680             auto bytecode = currentInstruction-&gt;as&lt;OpGetArgument&gt;();
6681             InlineCallFrame* inlineCallFrame = this-&gt;inlineCallFrame();
6682             Node* argument;
6683             int32_t argumentIndexIncludingThis = bytecode.m_index;
6684             if (inlineCallFrame &amp;&amp; !inlineCallFrame-&gt;isVarargs()) {
6685                 int32_t argumentCountIncludingThisWithFixup = inlineCallFrame-&gt;argumentsWithFixup.size();
6686                 if (argumentIndexIncludingThis &lt; argumentCountIncludingThisWithFixup)
6687                     argument = get(virtualRegisterForArgument(argumentIndexIncludingThis));
6688                 else
6689                     argument = addToGraph(JSConstant, OpInfo(m_constantUndefined));
6690             } else
6691                 argument = addToGraph(GetArgument, OpInfo(argumentIndexIncludingThis), OpInfo(getPrediction()));
6692             set(bytecode.m_dst, argument);
6693             NEXT_OPCODE(op_get_argument);
6694         }
6695         case op_new_async_generator_func:
6696             handleNewFunc(NewAsyncGeneratorFunction, currentInstruction-&gt;as&lt;OpNewAsyncGeneratorFunc&gt;());
6697             NEXT_OPCODE(op_new_async_generator_func);
6698         case op_new_func:
6699             handleNewFunc(NewFunction, currentInstruction-&gt;as&lt;OpNewFunc&gt;());
6700             NEXT_OPCODE(op_new_func);
6701         case op_new_generator_func:
6702             handleNewFunc(NewGeneratorFunction, currentInstruction-&gt;as&lt;OpNewGeneratorFunc&gt;());
6703             NEXT_OPCODE(op_new_generator_func);
6704         case op_new_async_func:
6705             handleNewFunc(NewAsyncFunction, currentInstruction-&gt;as&lt;OpNewAsyncFunc&gt;());
6706             NEXT_OPCODE(op_new_async_func);
6707 
6708         case op_new_func_exp:
6709             handleNewFuncExp(NewFunction, currentInstruction-&gt;as&lt;OpNewFuncExp&gt;());
6710             NEXT_OPCODE(op_new_func_exp);
6711         case op_new_generator_func_exp:
6712             handleNewFuncExp(NewGeneratorFunction, currentInstruction-&gt;as&lt;OpNewGeneratorFuncExp&gt;());
6713             NEXT_OPCODE(op_new_generator_func_exp);
6714         case op_new_async_generator_func_exp:
6715             handleNewFuncExp(NewAsyncGeneratorFunction, currentInstruction-&gt;as&lt;OpNewAsyncGeneratorFuncExp&gt;());
6716             NEXT_OPCODE(op_new_async_generator_func_exp);
6717         case op_new_async_func_exp:
6718             handleNewFuncExp(NewAsyncFunction, currentInstruction-&gt;as&lt;OpNewAsyncFuncExp&gt;());
6719             NEXT_OPCODE(op_new_async_func_exp);
6720 
6721         case op_set_function_name: {
6722             auto bytecode = currentInstruction-&gt;as&lt;OpSetFunctionName&gt;();
6723             Node* func = get(bytecode.m_function);
6724             Node* name = get(bytecode.m_name);
6725             addToGraph(SetFunctionName, func, name);
6726             NEXT_OPCODE(op_set_function_name);
6727         }
6728 
6729         case op_typeof: {
6730             auto bytecode = currentInstruction-&gt;as&lt;OpTypeof&gt;();
6731             set(bytecode.m_dst, addToGraph(TypeOf, get(bytecode.m_value)));
6732             NEXT_OPCODE(op_typeof);
6733         }
6734 
6735         case op_to_number: {
6736             auto bytecode = currentInstruction-&gt;as&lt;OpToNumber&gt;();
6737             SpeculatedType prediction = getPrediction();
6738             Node* value = get(bytecode.m_operand);
6739             set(bytecode.m_dst, addToGraph(ToNumber, OpInfo(0), OpInfo(prediction), value));
6740             NEXT_OPCODE(op_to_number);
6741         }
6742 
6743         case op_to_string: {
6744             auto bytecode = currentInstruction-&gt;as&lt;OpToString&gt;();
6745             Node* value = get(bytecode.m_operand);
6746             set(bytecode.m_dst, addToGraph(ToString, value));
6747             NEXT_OPCODE(op_to_string);
6748         }
6749 
6750         case op_to_object: {
6751             auto bytecode = currentInstruction-&gt;as&lt;OpToObject&gt;();
6752             SpeculatedType prediction = getPrediction();
6753             Node* value = get(bytecode.m_operand);
6754             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_message];
6755             set(bytecode.m_dst, addToGraph(ToObject, OpInfo(identifierNumber), OpInfo(prediction), value));
6756             NEXT_OPCODE(op_to_object);
6757         }
6758 
6759         case op_in_by_val: {
6760             auto bytecode = currentInstruction-&gt;as&lt;OpInByVal&gt;();
6761             ArrayMode arrayMode = getArrayMode(bytecode.metadata(codeBlock).m_arrayProfile, Array::Read);
6762             set(bytecode.m_dst, addToGraph(InByVal, OpInfo(arrayMode.asWord()), get(bytecode.m_base), get(bytecode.m_property)));
6763             NEXT_OPCODE(op_in_by_val);
6764         }
6765 
6766         case op_in_by_id: {
6767             auto bytecode = currentInstruction-&gt;as&lt;OpInById&gt;();
6768             Node* base = get(bytecode.m_base);
6769             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
6770             UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
6771 
6772             InByIdStatus status = InByIdStatus::computeFor(
6773                 m_inlineStackTop-&gt;m_profiledBlock,
6774                 m_inlineStackTop-&gt;m_baselineMap, m_icContextStack,
6775                 currentCodeOrigin(), uid);
6776 
6777             if (status.isSimple()) {
6778                 bool allOK = true;
6779                 MatchStructureData* data = m_graph.m_matchStructureData.add();
6780                 for (const InByIdVariant&amp; variant : status.variants()) {
6781                     if (!check(variant.conditionSet())) {
6782                         allOK = false;
6783                         break;
6784                     }
6785                     for (Structure* structure : variant.structureSet()) {
6786                         MatchStructureVariant matchVariant;
6787                         matchVariant.structure = m_graph.registerStructure(structure);
6788                         matchVariant.result = variant.isHit();
6789 
6790                         data-&gt;variants.append(WTFMove(matchVariant));
6791                     }
6792                 }
6793 
6794                 if (allOK) {
6795                     addToGraph(FilterInByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addInByIdStatus(currentCodeOrigin(), status)), base);
6796 
6797                     Node* match = addToGraph(MatchStructure, OpInfo(data), base);
6798                     set(bytecode.m_dst, match);
6799                     NEXT_OPCODE(op_in_by_id);
6800                 }
6801             }
6802 
6803             set(bytecode.m_dst, addToGraph(InById, OpInfo(identifierNumber), base));
6804             NEXT_OPCODE(op_in_by_id);
6805         }
6806 
6807         case op_get_enumerable_length: {
6808             auto bytecode = currentInstruction-&gt;as&lt;OpGetEnumerableLength&gt;();
6809             set(bytecode.m_dst, addToGraph(GetEnumerableLength, get(bytecode.m_base)));
6810             NEXT_OPCODE(op_get_enumerable_length);
6811         }
6812 
6813         case op_has_generic_property: {
6814             auto bytecode = currentInstruction-&gt;as&lt;OpHasGenericProperty&gt;();
6815             set(bytecode.m_dst, addToGraph(HasGenericProperty, get(bytecode.m_base), get(bytecode.m_property)));
6816             NEXT_OPCODE(op_has_generic_property);
6817         }
6818 
6819         case op_has_structure_property: {
6820             auto bytecode = currentInstruction-&gt;as&lt;OpHasStructureProperty&gt;();
6821             set(bytecode.m_dst, addToGraph(HasStructureProperty,
6822                 get(bytecode.m_base),
6823                 get(bytecode.m_property),
6824                 get(bytecode.m_enumerator)));
6825             NEXT_OPCODE(op_has_structure_property);
6826         }
6827 
6828         case op_has_indexed_property: {
6829             auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
6830             Node* base = get(bytecode.m_base);
6831             ArrayMode arrayMode = getArrayMode(bytecode.metadata(codeBlock).m_arrayProfile, Array::Read);
6832             Node* property = get(bytecode.m_property);
6833             addVarArgChild(base);
6834             addVarArgChild(property);
6835             addVarArgChild(nullptr);
6836             Node* hasIterableProperty = addToGraph(Node::VarArg, HasIndexedProperty, OpInfo(arrayMode.asWord()), OpInfo(static_cast&lt;uint32_t&gt;(PropertySlot::InternalMethodType::GetOwnProperty)));
6837             m_exitOK = false; // HasIndexedProperty must be treated as if it clobbers exit state, since FixupPhase may make it generic.
6838             set(bytecode.m_dst, hasIterableProperty);
6839             NEXT_OPCODE(op_has_indexed_property);
6840         }
6841 
6842         case op_get_direct_pname: {
6843             auto bytecode = currentInstruction-&gt;as&lt;OpGetDirectPname&gt;();
6844             SpeculatedType prediction = getPredictionWithoutOSRExit();
6845 
6846             Node* base = get(bytecode.m_base);
6847             Node* property = get(bytecode.m_property);
6848             Node* index = get(bytecode.m_index);
6849             Node* enumerator = get(bytecode.m_enumerator);
6850 
6851             addVarArgChild(base);
6852             addVarArgChild(property);
6853             addVarArgChild(index);
6854             addVarArgChild(enumerator);
6855             set(bytecode.m_dst, addToGraph(Node::VarArg, GetDirectPname, OpInfo(0), OpInfo(prediction)));
6856 
6857             NEXT_OPCODE(op_get_direct_pname);
6858         }
6859 
6860         case op_get_property_enumerator: {
6861             auto bytecode = currentInstruction-&gt;as&lt;OpGetPropertyEnumerator&gt;();
6862             set(bytecode.m_dst, addToGraph(GetPropertyEnumerator, get(bytecode.m_base)));
6863             NEXT_OPCODE(op_get_property_enumerator);
6864         }
6865 
6866         case op_enumerator_structure_pname: {
6867             auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorStructurePname&gt;();
6868             set(bytecode.m_dst, addToGraph(GetEnumeratorStructurePname,
6869                 get(bytecode.m_enumerator),
6870                 get(bytecode.m_index)));
6871             NEXT_OPCODE(op_enumerator_structure_pname);
6872         }
6873 
6874         case op_enumerator_generic_pname: {
6875             auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorGenericPname&gt;();
6876             set(bytecode.m_dst, addToGraph(GetEnumeratorGenericPname,
6877                 get(bytecode.m_enumerator),
6878                 get(bytecode.m_index)));
6879             NEXT_OPCODE(op_enumerator_generic_pname);
6880         }
6881 
6882         case op_to_index_string: {
6883             auto bytecode = currentInstruction-&gt;as&lt;OpToIndexString&gt;();
6884             set(bytecode.m_dst, addToGraph(ToIndexString, get(bytecode.m_index)));
6885             NEXT_OPCODE(op_to_index_string);
6886         }
6887 
6888         case op_log_shadow_chicken_prologue: {
6889             auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenPrologue&gt;();
6890             if (!m_inlineStackTop-&gt;m_inlineCallFrame)
6891                 addToGraph(LogShadowChickenPrologue, get(bytecode.m_scope));
6892             NEXT_OPCODE(op_log_shadow_chicken_prologue);
6893         }
6894 
6895         case op_log_shadow_chicken_tail: {
6896             auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenTail&gt;();
6897             if (!m_inlineStackTop-&gt;m_inlineCallFrame) {
6898                 // FIXME: The right solution for inlining is to elide these whenever the tail call
6899                 // ends up being inlined.
6900                 // https://bugs.webkit.org/show_bug.cgi?id=155686
6901                 addToGraph(LogShadowChickenTail, get(bytecode.m_thisValue), get(bytecode.m_scope));
6902             }
6903             NEXT_OPCODE(op_log_shadow_chicken_tail);
6904         }
6905 
6906         case op_unreachable: {
6907             flushForTerminal();
6908             addToGraph(Unreachable);
6909             LAST_OPCODE(op_unreachable);
6910         }
6911 
6912         default:
6913             // Parse failed! This should not happen because the capabilities checker
6914             // should have caught it.
6915             RELEASE_ASSERT_NOT_REACHED();
6916             return;
6917         }
6918     }
6919 }
6920 
6921 void ByteCodeParser::linkBlock(BasicBlock* block, Vector&lt;BasicBlock*&gt;&amp; possibleTargets)
6922 {
6923     ASSERT(!block-&gt;isLinked);
6924     ASSERT(!block-&gt;isEmpty());
6925     Node* node = block-&gt;terminal();
6926     ASSERT(node-&gt;isTerminal());
6927 
6928     switch (node-&gt;op()) {
6929     case Jump:
6930         node-&gt;targetBlock() = blockForBytecodeOffset(possibleTargets, node-&gt;targetBytecodeOffsetDuringParsing());
6931         break;
6932 
6933     case Branch: {
6934         BranchData* data = node-&gt;branchData();
6935         data-&gt;taken.block = blockForBytecodeOffset(possibleTargets, data-&gt;takenBytecodeIndex());
6936         data-&gt;notTaken.block = blockForBytecodeOffset(possibleTargets, data-&gt;notTakenBytecodeIndex());
6937         break;
6938     }
6939 
6940     case Switch: {
6941         SwitchData* data = node-&gt;switchData();
6942         for (unsigned i = node-&gt;switchData()-&gt;cases.size(); i--;)
6943             data-&gt;cases[i].target.block = blockForBytecodeOffset(possibleTargets, data-&gt;cases[i].target.bytecodeIndex());
6944         data-&gt;fallThrough.block = blockForBytecodeOffset(possibleTargets, data-&gt;fallThrough.bytecodeIndex());
6945         break;
6946     }
6947 
6948     default:
6949         RELEASE_ASSERT_NOT_REACHED();
6950     }
6951 
6952     VERBOSE_LOG(&quot;Marking &quot;, RawPointer(block), &quot; as linked (actually did linking)\n&quot;);
6953     block-&gt;didLink();
6954 }
6955 
6956 void ByteCodeParser::linkBlocks(Vector&lt;BasicBlock*&gt;&amp; unlinkedBlocks, Vector&lt;BasicBlock*&gt;&amp; possibleTargets)
6957 {
6958     for (size_t i = 0; i &lt; unlinkedBlocks.size(); ++i) {
6959         VERBOSE_LOG(&quot;Attempting to link &quot;, RawPointer(unlinkedBlocks[i]), &quot;\n&quot;);
6960         linkBlock(unlinkedBlocks[i], possibleTargets);
6961     }
6962 }
6963 
6964 ByteCodeParser::InlineStackEntry::InlineStackEntry(
6965     ByteCodeParser* byteCodeParser,
6966     CodeBlock* codeBlock,
6967     CodeBlock* profiledBlock,
6968     JSFunction* callee, // Null if this is a closure call.
6969     VirtualRegister returnValueVR,
6970     VirtualRegister inlineCallFrameStart,
6971     int argumentCountIncludingThis,
6972     InlineCallFrame::Kind kind,
6973     BasicBlock* continuationBlock)
6974     : m_byteCodeParser(byteCodeParser)
6975     , m_codeBlock(codeBlock)
6976     , m_profiledBlock(profiledBlock)
6977     , m_continuationBlock(continuationBlock)
6978     , m_returnValue(returnValueVR)
6979     , m_caller(byteCodeParser-&gt;m_inlineStackTop)
6980 {
6981     {
6982         m_exitProfile.initialize(m_profiledBlock-&gt;unlinkedCodeBlock());
6983 
6984         ConcurrentJSLocker locker(m_profiledBlock-&gt;m_lock);
6985         m_lazyOperands.initialize(locker, m_profiledBlock-&gt;lazyOperandValueProfiles(locker));
6986 
6987         // We do this while holding the lock because we want to encourage StructureStubInfo&#39;s
6988         // to be potentially added to operations and because the profiled block could be in the
6989         // middle of LLInt-&gt;JIT tier-up in which case we would be adding the info&#39;s right now.
6990         if (m_profiledBlock-&gt;hasBaselineJITProfiling())
6991             m_profiledBlock-&gt;getICStatusMap(locker, m_baselineMap);
6992     }
6993 
6994     CodeBlock* optimizedBlock = m_profiledBlock-&gt;replacement();
6995     m_optimizedContext.optimizedCodeBlock = optimizedBlock;
6996     if (Options::usePolyvariantDevirtualization() &amp;&amp; optimizedBlock) {
6997         ConcurrentJSLocker locker(optimizedBlock-&gt;m_lock);
6998         optimizedBlock-&gt;getICStatusMap(locker, m_optimizedContext.map);
6999     }
7000     byteCodeParser-&gt;m_icContextStack.append(&amp;m_optimizedContext);
7001 
7002     int argumentCountIncludingThisWithFixup = std::max&lt;int&gt;(argumentCountIncludingThis, codeBlock-&gt;numParameters());
7003 
7004     if (m_caller) {
7005         // Inline case.
7006         ASSERT(codeBlock != byteCodeParser-&gt;m_codeBlock);
7007         ASSERT(inlineCallFrameStart.isValid());
7008 
7009         m_inlineCallFrame = byteCodeParser-&gt;m_graph.m_plan.inlineCallFrames()-&gt;add();
7010         m_optimizedContext.inlineCallFrame = m_inlineCallFrame;
7011 
7012         // The owner is the machine code block, and we already have a barrier on that when the
7013         // plan finishes.
7014         m_inlineCallFrame-&gt;baselineCodeBlock.setWithoutWriteBarrier(codeBlock-&gt;baselineVersion());
7015         m_inlineCallFrame-&gt;setStackOffset(inlineCallFrameStart.offset() - CallFrame::headerSizeInRegisters);
7016         m_inlineCallFrame-&gt;argumentCountIncludingThis = argumentCountIncludingThis;
7017         if (callee) {
7018             m_inlineCallFrame-&gt;calleeRecovery = ValueRecovery::constant(callee);
7019             m_inlineCallFrame-&gt;isClosureCall = false;
7020         } else
7021             m_inlineCallFrame-&gt;isClosureCall = true;
7022         m_inlineCallFrame-&gt;directCaller = byteCodeParser-&gt;currentCodeOrigin();
7023         m_inlineCallFrame-&gt;argumentsWithFixup.resizeToFit(argumentCountIncludingThisWithFixup); // Set the number of arguments including this, but don&#39;t configure the value recoveries, yet.
7024         m_inlineCallFrame-&gt;kind = kind;
7025 
7026         m_identifierRemap.resize(codeBlock-&gt;numberOfIdentifiers());
7027         m_switchRemap.resize(codeBlock-&gt;numberOfSwitchJumpTables());
7028 
7029         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfIdentifiers(); ++i) {
7030             UniquedStringImpl* rep = codeBlock-&gt;identifier(i).impl();
7031             unsigned index = byteCodeParser-&gt;m_graph.identifiers().ensure(rep);
7032             m_identifierRemap[i] = index;
7033         }
7034         for (unsigned i = 0; i &lt; codeBlock-&gt;numberOfSwitchJumpTables(); ++i) {
7035             m_switchRemap[i] = byteCodeParser-&gt;m_codeBlock-&gt;numberOfSwitchJumpTables();
7036             byteCodeParser-&gt;m_codeBlock-&gt;addSwitchJumpTable() = codeBlock-&gt;switchJumpTable(i);
7037         }
7038     } else {
7039         // Machine code block case.
7040         ASSERT(codeBlock == byteCodeParser-&gt;m_codeBlock);
7041         ASSERT(!callee);
7042         ASSERT(!returnValueVR.isValid());
7043         ASSERT(!inlineCallFrameStart.isValid());
7044 
7045         m_inlineCallFrame = 0;
7046 
7047         m_identifierRemap.resize(codeBlock-&gt;numberOfIdentifiers());
7048         m_switchRemap.resize(codeBlock-&gt;numberOfSwitchJumpTables());
7049         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfIdentifiers(); ++i)
7050             m_identifierRemap[i] = i;
7051         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfSwitchJumpTables(); ++i)
7052             m_switchRemap[i] = i;
7053     }
7054 
7055     m_argumentPositions.resize(argumentCountIncludingThisWithFixup);
7056     for (int i = 0; i &lt; argumentCountIncludingThisWithFixup; ++i) {
7057         byteCodeParser-&gt;m_graph.m_argumentPositions.append(ArgumentPosition());
7058         ArgumentPosition* argumentPosition = &amp;byteCodeParser-&gt;m_graph.m_argumentPositions.last();
7059         m_argumentPositions[i] = argumentPosition;
7060     }
7061     byteCodeParser-&gt;m_inlineCallFrameToArgumentPositions.add(m_inlineCallFrame, m_argumentPositions);
7062 
7063     byteCodeParser-&gt;m_inlineStackTop = this;
7064 }
7065 
7066 ByteCodeParser::InlineStackEntry::~InlineStackEntry()
7067 {
7068     m_byteCodeParser-&gt;m_inlineStackTop = m_caller;
7069     RELEASE_ASSERT(m_byteCodeParser-&gt;m_icContextStack.last() == &amp;m_optimizedContext);
7070     m_byteCodeParser-&gt;m_icContextStack.removeLast();
7071 }
7072 
7073 void ByteCodeParser::parseCodeBlock()
7074 {
7075     clearCaches();
7076 
7077     CodeBlock* codeBlock = m_inlineStackTop-&gt;m_codeBlock;
7078 
7079     if (UNLIKELY(m_graph.compilation())) {
7080         m_graph.compilation()-&gt;addProfiledBytecodes(
7081             *m_vm-&gt;m_perBytecodeProfiler, m_inlineStackTop-&gt;m_profiledBlock);
7082     }
7083 
7084     if (UNLIKELY(Options::dumpSourceAtDFGTime())) {
7085         Vector&lt;DeferredSourceDump&gt;&amp; deferredSourceDump = m_graph.m_plan.callback()-&gt;ensureDeferredSourceDump();
7086         if (inlineCallFrame()) {
7087             DeferredSourceDump dump(codeBlock-&gt;baselineVersion(), m_codeBlock, JITCode::DFGJIT, inlineCallFrame()-&gt;directCaller.bytecodeIndex);
7088             deferredSourceDump.append(dump);
7089         } else
7090             deferredSourceDump.append(DeferredSourceDump(codeBlock-&gt;baselineVersion()));
7091     }
7092 
7093     if (Options::dumpBytecodeAtDFGTime()) {
7094         dataLog(&quot;Parsing &quot;, *codeBlock);
7095         if (inlineCallFrame()) {
7096             dataLog(
7097                 &quot; for inlining at &quot;, CodeBlockWithJITType(m_codeBlock, JITCode::DFGJIT),
7098                 &quot; &quot;, inlineCallFrame()-&gt;directCaller);
7099         }
7100         dataLog(
7101             &quot;, isStrictMode = &quot;, codeBlock-&gt;ownerExecutable()-&gt;isStrictMode(), &quot;\n&quot;);
7102         codeBlock-&gt;baselineVersion()-&gt;dumpBytecode();
7103     }
7104 
7105     Vector&lt;InstructionStream::Offset, 32&gt; jumpTargets;
7106     computePreciseJumpTargets(codeBlock, jumpTargets);
7107     if (Options::dumpBytecodeAtDFGTime()) {
7108         dataLog(&quot;Jump targets: &quot;);
7109         CommaPrinter comma;
7110         for (unsigned i = 0; i &lt; jumpTargets.size(); ++i)
7111             dataLog(comma, jumpTargets[i]);
7112         dataLog(&quot;\n&quot;);
7113     }
7114 
7115     for (unsigned jumpTargetIndex = 0; jumpTargetIndex &lt;= jumpTargets.size(); ++jumpTargetIndex) {
7116         // The maximum bytecode offset to go into the current basicblock is either the next jump target, or the end of the instructions.
7117         unsigned limit = jumpTargetIndex &lt; jumpTargets.size() ? jumpTargets[jumpTargetIndex] : codeBlock-&gt;instructions().size();
7118         ASSERT(m_currentIndex &lt; limit);
7119 
7120         // Loop until we reach the current limit (i.e. next jump target).
7121         do {
7122             // There may already be a currentBlock in two cases:
7123             // - we may have just entered the loop for the first time
7124             // - we may have just returned from an inlined callee that had some early returns and
7125             //   so allocated a continuation block, and the instruction after the call is a jump target.
7126             // In both cases, we want to keep using it.
7127             if (!m_currentBlock) {
7128                 m_currentBlock = allocateTargetableBlock(m_currentIndex);
7129 
7130                 // The first block is definitely an OSR target.
7131                 if (m_graph.numBlocks() == 1) {
7132                     m_currentBlock-&gt;isOSRTarget = true;
7133                     m_graph.m_roots.append(m_currentBlock);
7134                 }
7135                 prepareToParseBlock();
7136             }
7137 
7138             parseBlock(limit);
7139 
7140             // We should not have gone beyond the limit.
7141             ASSERT(m_currentIndex &lt;= limit);
7142 
7143             if (m_currentBlock-&gt;isEmpty()) {
7144                 // This case only happens if the last instruction was an inlined call with early returns
7145                 // or polymorphic (creating an empty continuation block),
7146                 // and then we hit the limit before putting anything in the continuation block.
7147                 ASSERT(m_currentIndex == limit);
7148                 makeBlockTargetable(m_currentBlock, m_currentIndex);
7149             } else {
7150                 ASSERT(m_currentBlock-&gt;terminal() || (m_currentIndex == codeBlock-&gt;instructions().size() &amp;&amp; inlineCallFrame()));
7151                 m_currentBlock = nullptr;
7152             }
7153         } while (m_currentIndex &lt; limit);
7154     }
7155 
7156     // Should have reached the end of the instructions.
7157     ASSERT(m_currentIndex == codeBlock-&gt;instructions().size());
7158 
7159     VERBOSE_LOG(&quot;Done parsing &quot;, *codeBlock, &quot; (fell off end)\n&quot;);
7160 }
7161 
7162 template &lt;typename Bytecode&gt;
7163 void ByteCodeParser::handlePutByVal(Bytecode bytecode, unsigned instructionSize)
7164 {
7165     Node* base = get(bytecode.m_base);
7166     Node* property = get(bytecode.m_property);
7167     Node* value = get(bytecode.m_value);
7168     bool isDirect = Bytecode::opcodeID == op_put_by_val_direct;
7169     bool compiledAsPutById = false;
7170     {
7171         unsigned identifierNumber = std::numeric_limits&lt;unsigned&gt;::max();
7172         PutByIdStatus putByIdStatus;
7173         {
7174             ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
7175             ByValInfo* byValInfo = m_inlineStackTop-&gt;m_baselineMap.get(CodeOrigin(currentCodeOrigin().bytecodeIndex)).byValInfo;
7176             // FIXME: When the bytecode is not compiled in the baseline JIT, byValInfo becomes null.
7177             // At that time, there is no information.
7178             if (byValInfo
7179                 &amp;&amp; byValInfo-&gt;stubInfo
7180                 &amp;&amp; !byValInfo-&gt;tookSlowPath
7181                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIdent)
7182                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType)
7183                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
7184                 compiledAsPutById = true;
7185                 identifierNumber = m_graph.identifiers().ensure(byValInfo-&gt;cachedId.impl());
7186                 UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
7187 
7188                 if (Symbol* symbol = byValInfo-&gt;cachedSymbol.get()) {
7189                     FrozenValue* frozen = m_graph.freezeStrong(symbol);
7190                     addToGraph(CheckCell, OpInfo(frozen), property);
7191                 } else {
7192                     ASSERT(!uid-&gt;isSymbol());
7193                     addToGraph(CheckStringIdent, OpInfo(uid), property);
7194                 }
7195 
7196                 putByIdStatus = PutByIdStatus::computeForStubInfo(
7197                     locker, m_inlineStackTop-&gt;m_profiledBlock,
7198                     byValInfo-&gt;stubInfo, currentCodeOrigin(), uid);
7199 
7200             }
7201         }
7202 
7203         if (compiledAsPutById)
7204             handlePutById(base, identifierNumber, value, putByIdStatus, isDirect, instructionSize);
7205     }
7206 
7207     if (!compiledAsPutById) {
7208         ArrayMode arrayMode = getArrayMode(bytecode.metadata(m_inlineStackTop-&gt;m_codeBlock).m_arrayProfile, Array::Write);
7209 
7210         addVarArgChild(base);
7211         addVarArgChild(property);
7212         addVarArgChild(value);
7213         addVarArgChild(0); // Leave room for property storage.
7214         addVarArgChild(0); // Leave room for length.
7215         addToGraph(Node::VarArg, isDirect ? PutByValDirect : PutByVal, OpInfo(arrayMode.asWord()), OpInfo(0));
7216         m_exitOK = false; // PutByVal and PutByValDirect must be treated as if they clobber exit state, since FixupPhase may make them generic.
7217     }
7218 }
7219 
7220 template &lt;typename Bytecode&gt;
7221 void ByteCodeParser::handlePutAccessorById(NodeType op, Bytecode bytecode)
7222 {
7223     Node* base = get(bytecode.m_base);
7224     unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
7225     Node* accessor = get(bytecode.m_accessor);
7226     addToGraph(op, OpInfo(identifierNumber), OpInfo(bytecode.m_attributes), base, accessor);
7227 }
7228 
7229 template &lt;typename Bytecode&gt;
7230 void ByteCodeParser::handlePutAccessorByVal(NodeType op, Bytecode bytecode)
7231 {
7232     Node* base = get(bytecode.m_base);
7233     Node* subscript = get(bytecode.m_property);
7234     Node* accessor = get(bytecode.m_accessor);
7235     addToGraph(op, OpInfo(bytecode.m_attributes), base, subscript, accessor);
7236 }
7237 
7238 template &lt;typename Bytecode&gt;
7239 void ByteCodeParser::handleNewFunc(NodeType op, Bytecode bytecode)
7240 {
7241     FunctionExecutable* decl = m_inlineStackTop-&gt;m_profiledBlock-&gt;functionDecl(bytecode.m_functionDecl);
7242     FrozenValue* frozen = m_graph.freezeStrong(decl);
7243     Node* scope = get(bytecode.m_scope);
7244     set(bytecode.m_dst, addToGraph(op, OpInfo(frozen), scope));
7245     // Ideally we wouldn&#39;t have to do this Phantom. But:
7246     //
7247     // For the constant case: we must do it because otherwise we would have no way of knowing
7248     // that the scope is live at OSR here.
7249     //
7250     // For the non-constant case: NewFunction could be DCE&#39;d, but baseline&#39;s implementation
7251     // won&#39;t be able to handle an Undefined scope.
7252     addToGraph(Phantom, scope);
7253 }
7254 
7255 template &lt;typename Bytecode&gt;
7256 void ByteCodeParser::handleNewFuncExp(NodeType op, Bytecode bytecode)
7257 {
7258     FunctionExecutable* expr = m_inlineStackTop-&gt;m_profiledBlock-&gt;functionExpr(bytecode.m_functionDecl);
7259     FrozenValue* frozen = m_graph.freezeStrong(expr);
7260     Node* scope = get(bytecode.m_scope);
7261     set(bytecode.m_dst, addToGraph(op, OpInfo(frozen), scope));
7262     // Ideally we wouldn&#39;t have to do this Phantom. But:
7263     //
7264     // For the constant case: we must do it because otherwise we would have no way of knowing
7265     // that the scope is live at OSR here.
7266     //
7267     // For the non-constant case: NewFunction could be DCE&#39;d, but baseline&#39;s implementation
7268     // won&#39;t be able to handle an Undefined scope.
7269     addToGraph(Phantom, scope);
7270 }
7271 
7272 void ByteCodeParser::parse()
7273 {
7274     // Set during construction.
7275     ASSERT(!m_currentIndex);
7276 
7277     VERBOSE_LOG(&quot;Parsing &quot;, *m_codeBlock, &quot;\n&quot;);
7278 
7279     InlineStackEntry inlineStackEntry(
7280         this, m_codeBlock, m_profiledBlock, 0, VirtualRegister(), VirtualRegister(),
7281         m_codeBlock-&gt;numParameters(), InlineCallFrame::Call, nullptr);
7282 
7283     parseCodeBlock();
7284     linkBlocks(inlineStackEntry.m_unlinkedBlocks, inlineStackEntry.m_blockLinkingTargets);
7285 
7286     if (m_hasAnyForceOSRExits) {
7287         BlockSet blocksToIgnore;
7288         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
7289             if (block-&gt;isOSRTarget &amp;&amp; block-&gt;bytecodeBegin == m_graph.m_plan.osrEntryBytecodeIndex()) {
7290                 blocksToIgnore.add(block);
7291                 break;
7292             }
7293         }
7294 
7295         {
7296             bool isSafeToValidate = false;
7297             auto postOrder = m_graph.blocksInPostOrder(isSafeToValidate); // This algorithm doesn&#39;t rely on the predecessors list, which is not yet built.
7298             bool changed;
7299             do {
7300                 changed = false;
7301                 for (BasicBlock* block : postOrder) {
7302                     for (BasicBlock* successor : block-&gt;successors()) {
7303                         if (blocksToIgnore.contains(successor)) {
7304                             changed |= blocksToIgnore.add(block);
7305                             break;
7306                         }
7307                     }
7308                 }
7309             } while (changed);
7310         }
7311 
7312         InsertionSet insertionSet(m_graph);
7313         Operands&lt;VariableAccessData*&gt; mapping(OperandsLike, m_graph.block(0)-&gt;variablesAtHead);
7314 
7315         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
7316             if (blocksToIgnore.contains(block))
7317                 continue;
7318 
7319             mapping.fill(nullptr);
7320             if (validationEnabled()) {
7321                 // Verify that it&#39;s correct to fill mapping with nullptr.
7322                 for (unsigned i = 0; i &lt; block-&gt;variablesAtHead.size(); ++i) {
7323                     Node* node = block-&gt;variablesAtHead.at(i);
7324                     RELEASE_ASSERT(!node);
7325                 }
7326             }
7327 
7328             for (unsigned nodeIndex = 0; nodeIndex &lt; block-&gt;size(); ++nodeIndex) {
7329                 Node* node = block-&gt;at(nodeIndex);
7330 
7331                 if (node-&gt;hasVariableAccessData(m_graph))
7332                     mapping.operand(node-&gt;local()) = node-&gt;variableAccessData();
7333 
7334                 if (node-&gt;op() == ForceOSRExit) {
7335                     NodeOrigin endOrigin = node-&gt;origin.withExitOK(true);
7336 
7337                     if (validationEnabled()) {
7338                         // This verifies that we don&#39;t need to change any of the successors&#39;s predecessor
7339                         // list after planting the Unreachable below. At this point in the bytecode
7340                         // parser, we haven&#39;t linked up the predecessor lists yet.
7341                         for (BasicBlock* successor : block-&gt;successors())
7342                             RELEASE_ASSERT(successor-&gt;predecessors.isEmpty());
7343                     }
7344 
7345                     block-&gt;resize(nodeIndex + 1);
7346 
7347                     insertionSet.insertNode(block-&gt;size(), SpecNone, ExitOK, endOrigin);
7348 
7349                     auto insertLivenessPreservingOp = [&amp;] (InlineCallFrame* inlineCallFrame, NodeType op, VirtualRegister operand) {
7350                         VariableAccessData* variable = mapping.operand(operand);
7351                         if (!variable) {
7352                             variable = newVariableAccessData(operand);
7353                             mapping.operand(operand) = variable;
7354                         }
7355 
7356                         VirtualRegister argument = operand - (inlineCallFrame ? inlineCallFrame-&gt;stackOffset : 0);
7357                         if (argument.isArgument() &amp;&amp; !argument.isHeader()) {
7358                             const Vector&lt;ArgumentPosition*&gt;&amp; arguments = m_inlineCallFrameToArgumentPositions.get(inlineCallFrame);
7359                             arguments[argument.toArgument()]-&gt;addVariable(variable);
7360                         } insertionSet.insertNode(block-&gt;size(), SpecNone, op, endOrigin, OpInfo(variable));
7361                     };
7362                     auto addFlushDirect = [&amp;] (InlineCallFrame* inlineCallFrame, VirtualRegister operand) {
7363                         insertLivenessPreservingOp(inlineCallFrame, Flush, operand);
7364                     };
7365                     auto addPhantomLocalDirect = [&amp;] (InlineCallFrame* inlineCallFrame, VirtualRegister operand) {
7366                         insertLivenessPreservingOp(inlineCallFrame, PhantomLocal, operand);
7367                     };
7368                     flushForTerminalImpl(endOrigin.semantic, addFlushDirect, addPhantomLocalDirect);
7369 
7370                     insertionSet.insertNode(block-&gt;size(), SpecNone, Unreachable, endOrigin);
7371                     insertionSet.execute(block);
7372                     break;
7373                 }
7374             }
7375         }
7376     } else if (validationEnabled()) {
7377         // Ensure our bookkeeping for ForceOSRExit nodes is working.
7378         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
7379             for (Node* node : *block)
7380                 RELEASE_ASSERT(node-&gt;op() != ForceOSRExit);
7381         }
7382     }
7383 
7384     m_graph.determineReachability();
7385     m_graph.killUnreachableBlocks();
7386 
7387     for (BlockIndex blockIndex = m_graph.numBlocks(); blockIndex--;) {
7388         BasicBlock* block = m_graph.block(blockIndex);
7389         if (!block)
7390             continue;
7391         ASSERT(block-&gt;variablesAtHead.numberOfLocals() == m_graph.block(0)-&gt;variablesAtHead.numberOfLocals());
7392         ASSERT(block-&gt;variablesAtHead.numberOfArguments() == m_graph.block(0)-&gt;variablesAtHead.numberOfArguments());
7393         ASSERT(block-&gt;variablesAtTail.numberOfLocals() == m_graph.block(0)-&gt;variablesAtHead.numberOfLocals());
7394         ASSERT(block-&gt;variablesAtTail.numberOfArguments() == m_graph.block(0)-&gt;variablesAtHead.numberOfArguments());
7395     }
7396 
7397     m_graph.m_localVars = m_numLocals;
7398     m_graph.m_parameterSlots = m_parameterSlots;
7399 }
7400 
7401 void parse(Graph&amp; graph)
7402 {
7403     ByteCodeParser(graph).parse();
7404 }
7405 
7406 } } // namespace JSC::DFG
7407 
7408 #endif
    </pre>
  </body>
</html>