<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/Repatch.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2011-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;Repatch.h&quot;
  28 
  29 #if ENABLE(JIT)
  30 
  31 #include &quot;BinarySwitch.h&quot;
  32 #include &quot;CCallHelpers.h&quot;
  33 #include &quot;CallFrameShuffler.h&quot;
  34 #include &quot;DFGOperations.h&quot;
  35 #include &quot;DFGSpeculativeJIT.h&quot;
  36 #include &quot;DOMJITGetterSetter.h&quot;
  37 #include &quot;DirectArguments.h&quot;
  38 #include &quot;ExecutableBaseInlines.h&quot;
  39 #include &quot;FTLThunks.h&quot;
  40 #include &quot;FullCodeOrigin.h&quot;
  41 #include &quot;FunctionCodeBlock.h&quot;
  42 #include &quot;GCAwareJITStubRoutine.h&quot;
  43 #include &quot;GetterSetter.h&quot;
  44 #include &quot;GetterSetterAccessCase.h&quot;
  45 #include &quot;ICStats.h&quot;
  46 #include &quot;InlineAccess.h&quot;
  47 #include &quot;InstanceOfAccessCase.h&quot;
  48 #include &quot;IntrinsicGetterAccessCase.h&quot;
  49 #include &quot;JIT.h&quot;
  50 #include &quot;JITInlines.h&quot;
  51 #include &quot;JSCInlines.h&quot;
  52 #include &quot;JSModuleNamespaceObject.h&quot;
  53 #include &quot;JSWebAssembly.h&quot;
  54 #include &quot;LinkBuffer.h&quot;
  55 #include &quot;ModuleNamespaceAccessCase.h&quot;
  56 #include &quot;PolymorphicAccess.h&quot;
  57 #include &quot;ScopedArguments.h&quot;
  58 #include &quot;ScratchRegisterAllocator.h&quot;
  59 #include &quot;StackAlignment.h&quot;
  60 #include &quot;StructureRareDataInlines.h&quot;
  61 #include &quot;StructureStubClearingWatchpoint.h&quot;
  62 #include &quot;StructureStubInfo.h&quot;
  63 #include &quot;SuperSampler.h&quot;
  64 #include &quot;ThunkGenerators.h&quot;
  65 #include &lt;wtf/CommaPrinter.h&gt;
  66 #include &lt;wtf/ListDump.h&gt;
  67 #include &lt;wtf/StringPrintStream.h&gt;
  68 
  69 namespace JSC {
  70 
  71 static FunctionPtr&lt;CFunctionPtrTag&gt; readPutICCallTarget(CodeBlock* codeBlock, CodeLocationCall&lt;JSInternalPtrTag&gt; call)
  72 {
  73     FunctionPtr&lt;OperationPtrTag&gt; target = MacroAssembler::readCallTarget&lt;OperationPtrTag&gt;(call);
  74 #if ENABLE(FTL_JIT)
  75     if (codeBlock-&gt;jitType() == JITCode::FTLJIT) {
  76         MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt; thunk = MacroAssemblerCodePtr&lt;OperationPtrTag&gt;::createFromExecutableAddress(target.executableAddress()).retagged&lt;JITThunkPtrTag&gt;();
  77         return codeBlock-&gt;vm()-&gt;ftlThunks-&gt;keyForSlowPathCallThunk(thunk).callTarget().retagged&lt;CFunctionPtrTag&gt;();
  78     }
  79 #else
  80     UNUSED_PARAM(codeBlock);
  81 #endif // ENABLE(FTL_JIT)
  82     return target.retagged&lt;CFunctionPtrTag&gt;();
  83 }
  84 
  85 void ftlThunkAwareRepatchCall(CodeBlock* codeBlock, CodeLocationCall&lt;JSInternalPtrTag&gt; call, FunctionPtr&lt;CFunctionPtrTag&gt; newCalleeFunction)
  86 {
  87 #if ENABLE(FTL_JIT)
  88     if (codeBlock-&gt;jitType() == JITCode::FTLJIT) {
  89         VM&amp; vm = *codeBlock-&gt;vm();
  90         FTL::Thunks&amp; thunks = *vm.ftlThunks;
  91         FunctionPtr&lt;OperationPtrTag&gt; target = MacroAssembler::readCallTarget&lt;OperationPtrTag&gt;(call);
  92         auto slowPathThunk = MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt;::createFromExecutableAddress(target.retaggedExecutableAddress&lt;JITThunkPtrTag&gt;());
  93         FTL::SlowPathCallKey key = thunks.keyForSlowPathCallThunk(slowPathThunk);
  94         key = key.withCallTarget(newCalleeFunction);
  95         MacroAssembler::repatchCall(call, FunctionPtr&lt;OperationPtrTag&gt;(thunks.getSlowPathCallThunk(key).retaggedCode&lt;OperationPtrTag&gt;()));
  96         return;
  97     }
  98 #else // ENABLE(FTL_JIT)
  99     UNUSED_PARAM(codeBlock);
 100 #endif // ENABLE(FTL_JIT)
 101     MacroAssembler::repatchCall(call, newCalleeFunction.retagged&lt;OperationPtrTag&gt;());
 102 }
 103 
 104 enum InlineCacheAction {
 105     GiveUpOnCache,
 106     RetryCacheLater,
 107     AttemptToCache
 108 };
 109 
 110 static InlineCacheAction actionForCell(VM&amp; vm, JSCell* cell)
 111 {
 112     Structure* structure = cell-&gt;structure(vm);
 113 
 114     TypeInfo typeInfo = structure-&gt;typeInfo();
 115     if (typeInfo.prohibitsPropertyCaching())
 116         return GiveUpOnCache;
 117 
 118     if (structure-&gt;isUncacheableDictionary()) {
 119         if (structure-&gt;hasBeenFlattenedBefore())
 120             return GiveUpOnCache;
 121         // Flattening could have changed the offset, so return early for another try.
 122         asObject(cell)-&gt;flattenDictionaryObject(vm);
 123         return RetryCacheLater;
 124     }
 125 
 126     if (!structure-&gt;propertyAccessesAreCacheable())
 127         return GiveUpOnCache;
 128 
 129     return AttemptToCache;
 130 }
 131 
 132 static bool forceICFailure(ExecState*)
 133 {
 134     return Options::forceICFailure();
 135 }
 136 
 137 ALWAYS_INLINE static void fireWatchpointsAndClearStubIfNeeded(VM&amp; vm, StructureStubInfo&amp; stubInfo, CodeBlock* codeBlock, AccessGenerationResult&amp; result)
 138 {
 139     if (result.shouldResetStubAndFireWatchpoints()) {
 140         result.fireWatchpoints(vm);
 141         stubInfo.reset(codeBlock);
 142     }
 143 }
 144 
 145 inline FunctionPtr&lt;CFunctionPtrTag&gt; appropriateOptimizingGetByIdFunction(GetByIDKind kind)
 146 {
 147     switch (kind) {
 148     case GetByIDKind::Normal:
 149         return operationGetByIdOptimize;
 150     case GetByIDKind::WithThis:
 151         return operationGetByIdWithThisOptimize;
 152     case GetByIDKind::Try:
 153         return operationTryGetByIdOptimize;
 154     case GetByIDKind::Direct:
 155         return operationGetByIdDirectOptimize;
 156     }
 157     ASSERT_NOT_REACHED();
 158     return operationGetById;
 159 }
 160 
 161 inline FunctionPtr&lt;CFunctionPtrTag&gt; appropriateGetByIdFunction(GetByIDKind kind)
 162 {
 163     switch (kind) {
 164     case GetByIDKind::Normal:
 165         return operationGetById;
 166     case GetByIDKind::WithThis:
 167         return operationGetByIdWithThis;
 168     case GetByIDKind::Try:
 169         return operationTryGetById;
 170     case GetByIDKind::Direct:
 171         return operationGetByIdDirect;
 172     }
 173     ASSERT_NOT_REACHED();
 174     return operationGetById;
 175 }
 176 
 177 static InlineCacheAction tryCacheGetByID(ExecState* exec, JSValue baseValue, const Identifier&amp; propertyName, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, GetByIDKind kind)
 178 {
 179     VM&amp; vm = exec-&gt;vm();
 180     AccessGenerationResult result;
 181 
 182     {
 183         GCSafeConcurrentJSLocker locker(exec-&gt;codeBlock()-&gt;m_lock, exec-&gt;vm().heap);
 184 
 185         if (forceICFailure(exec))
 186             return GiveUpOnCache;
 187 
 188         // FIXME: Cache property access for immediates.
 189         if (!baseValue.isCell())
 190             return GiveUpOnCache;
 191         JSCell* baseCell = baseValue.asCell();
 192 
 193         CodeBlock* codeBlock = exec-&gt;codeBlock();
 194 
 195         std::unique_ptr&lt;AccessCase&gt; newCase;
 196 
 197         if (propertyName == vm.propertyNames-&gt;length) {
 198             if (isJSArray(baseCell)) {
 199                 if (stubInfo.cacheType == CacheType::Unset
 200                     &amp;&amp; slot.slotBase() == baseCell
 201                     &amp;&amp; InlineAccess::isCacheableArrayLength(stubInfo, jsCast&lt;JSArray*&gt;(baseCell))) {
 202 
 203                     bool generatedCodeInline = InlineAccess::generateArrayLength(stubInfo, jsCast&lt;JSArray*&gt;(baseCell));
 204                     if (generatedCodeInline) {
 205                         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
 206                         stubInfo.initArrayLength();
 207                         return RetryCacheLater;
 208                     }
 209                 }
 210 
 211                 newCase = AccessCase::create(vm, codeBlock, AccessCase::ArrayLength);
 212             } else if (isJSString(baseCell)) {
 213                 if (stubInfo.cacheType == CacheType::Unset &amp;&amp; InlineAccess::isCacheableStringLength(stubInfo)) {
 214                     bool generatedCodeInline = InlineAccess::generateStringLength(stubInfo);
 215                     if (generatedCodeInline) {
 216                         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
 217                         stubInfo.initStringLength();
 218                         return RetryCacheLater;
 219                     }
 220                 }
 221 
 222                 newCase = AccessCase::create(vm, codeBlock, AccessCase::StringLength);
 223             }
 224             else if (DirectArguments* arguments = jsDynamicCast&lt;DirectArguments*&gt;(vm, baseCell)) {
 225                 // If there were overrides, then we can handle this as a normal property load! Guarding
 226                 // this with such a check enables us to add an IC case for that load if needed.
 227                 if (!arguments-&gt;overrodeThings())
 228                     newCase = AccessCase::create(vm, codeBlock, AccessCase::DirectArgumentsLength);
 229             } else if (ScopedArguments* arguments = jsDynamicCast&lt;ScopedArguments*&gt;(vm, baseCell)) {
 230                 // Ditto.
 231                 if (!arguments-&gt;overrodeThings())
 232                     newCase = AccessCase::create(vm, codeBlock, AccessCase::ScopedArgumentsLength);
 233             }
 234         }
 235 
 236         if (!propertyName.isSymbol() &amp;&amp; baseCell-&gt;inherits&lt;JSModuleNamespaceObject&gt;(vm) &amp;&amp; !slot.isUnset()) {
 237             if (auto moduleNamespaceSlot = slot.moduleNamespaceSlot())
 238                 newCase = ModuleNamespaceAccessCase::create(vm, codeBlock, jsCast&lt;JSModuleNamespaceObject*&gt;(baseCell), moduleNamespaceSlot-&gt;environment, ScopeOffset(moduleNamespaceSlot-&gt;scopeOffset));
 239         }
 240 
 241         if (!newCase) {
 242             if (!slot.isCacheable() &amp;&amp; !slot.isUnset())
 243                 return GiveUpOnCache;
 244 
 245             ObjectPropertyConditionSet conditionSet;
 246             Structure* structure = baseCell-&gt;structure(vm);
 247 
 248             bool loadTargetFromProxy = false;
 249             if (baseCell-&gt;type() == PureForwardingProxyType) {
 250                 baseValue = jsCast&lt;JSProxy*&gt;(baseCell)-&gt;target();
 251                 baseCell = baseValue.asCell();
 252                 structure = baseCell-&gt;structure(vm);
 253                 loadTargetFromProxy = true;
 254             }
 255 
 256             InlineCacheAction action = actionForCell(vm, baseCell);
 257             if (action != AttemptToCache)
 258                 return action;
 259 
 260             // Optimize self access.
 261             if (stubInfo.cacheType == CacheType::Unset
 262                 &amp;&amp; slot.isCacheableValue()
 263                 &amp;&amp; slot.slotBase() == baseValue
 264                 &amp;&amp; !slot.watchpointSet()
 265                 &amp;&amp; !structure-&gt;needImpurePropertyWatchpoint()
 266                 &amp;&amp; !loadTargetFromProxy) {
 267 
 268                 bool generatedCodeInline = InlineAccess::generateSelfPropertyAccess(stubInfo, structure, slot.cachedOffset());
 269                 if (generatedCodeInline) {
 270                     LOG_IC((ICEvent::GetByIdSelfPatch, structure-&gt;classInfo(), propertyName));
 271                     structure-&gt;startWatchingPropertyForReplacements(vm, slot.cachedOffset());
 272                     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
 273                     stubInfo.initGetByIdSelf(codeBlock, structure, slot.cachedOffset());
 274                     return RetryCacheLater;
 275                 }
 276             }
 277 
 278             std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 279 
 280             PropertyOffset offset = slot.isUnset() ? invalidOffset : slot.cachedOffset();
 281 
 282             if (slot.isUnset() || slot.slotBase() != baseValue) {
 283                 if (structure-&gt;typeInfo().prohibitsPropertyCaching())
 284                     return GiveUpOnCache;
 285 
 286                 if (structure-&gt;isDictionary()) {
 287                     if (structure-&gt;hasBeenFlattenedBefore())
 288                         return GiveUpOnCache;
 289                     structure-&gt;flattenDictionaryStructure(vm, jsCast&lt;JSObject*&gt;(baseCell));
 290                 }
 291 
 292                 if (slot.isUnset() &amp;&amp; structure-&gt;typeInfo().getOwnPropertySlotIsImpureForPropertyAbsence())
 293                     return GiveUpOnCache;
 294 
 295                 // If a kind is GetByIDKind::Direct, we do not need to investigate prototype chains further.
 296                 // Cacheability just depends on the head structure.
 297                 if (kind != GetByIDKind::Direct) {
 298                     bool usesPolyProto;
 299                     prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), baseCell, slot, usesPolyProto);
 300                     if (!prototypeAccessChain) {
 301                         // It&#39;s invalid to access this prototype property.
 302                         return GiveUpOnCache;
 303                     }
 304 
 305                     if (!usesPolyProto) {
 306                         // We use ObjectPropertyConditionSet instead for faster accesses.
 307                         prototypeAccessChain = nullptr;
 308 
 309                         // FIXME: Maybe this `if` should be inside generateConditionsForPropertyBlah.
 310                         // https://bugs.webkit.org/show_bug.cgi?id=185215
 311                         if (slot.isUnset()) {
 312                             conditionSet = generateConditionsForPropertyMiss(
 313                                 vm, codeBlock, exec, structure, propertyName.impl());
 314                         } else if (!slot.isCacheableCustom()) {
 315                             conditionSet = generateConditionsForPrototypePropertyHit(
 316                                 vm, codeBlock, exec, structure, slot.slotBase(),
 317                                 propertyName.impl());
 318                         } else {
 319                             conditionSet = generateConditionsForPrototypePropertyHitCustom(
 320                                 vm, codeBlock, exec, structure, slot.slotBase(),
 321                                 propertyName.impl());
 322                         }
 323 
 324                         if (!conditionSet.isValid())
 325                             return GiveUpOnCache;
 326                     }
 327                 }
 328 
 329                 offset = slot.isUnset() ? invalidOffset : slot.cachedOffset();
 330             }
 331 
 332             JSFunction* getter = nullptr;
 333             if (slot.isCacheableGetter())
 334                 getter = jsDynamicCast&lt;JSFunction*&gt;(vm, slot.getterSetter()-&gt;getter());
 335 
 336             Optional&lt;DOMAttributeAnnotation&gt; domAttribute;
 337             if (slot.isCacheableCustom() &amp;&amp; slot.domAttribute())
 338                 domAttribute = slot.domAttribute();
 339 
 340             if (kind == GetByIDKind::Try) {
 341                 AccessCase::AccessType type;
 342                 if (slot.isCacheableValue())
 343                     type = AccessCase::Load;
 344                 else if (slot.isUnset())
 345                     type = AccessCase::Miss;
 346                 else if (slot.isCacheableGetter())
 347                     type = AccessCase::GetGetter;
 348                 else
 349                     RELEASE_ASSERT_NOT_REACHED();
 350 
 351                 newCase = ProxyableAccessCase::create(vm, codeBlock, type, offset, structure, conditionSet, loadTargetFromProxy, slot.watchpointSet(), WTFMove(prototypeAccessChain));
 352             } else if (!loadTargetFromProxy &amp;&amp; getter &amp;&amp; IntrinsicGetterAccessCase::canEmitIntrinsicGetter(getter, structure))
 353                 newCase = IntrinsicGetterAccessCase::create(vm, codeBlock, slot.cachedOffset(), structure, conditionSet, getter, WTFMove(prototypeAccessChain));
 354             else {
 355                 if (slot.isCacheableValue() || slot.isUnset()) {
 356                     newCase = ProxyableAccessCase::create(vm, codeBlock, slot.isUnset() ? AccessCase::Miss : AccessCase::Load,
 357                         offset, structure, conditionSet, loadTargetFromProxy, slot.watchpointSet(), WTFMove(prototypeAccessChain));
 358                 } else {
 359                     AccessCase::AccessType type;
 360                     if (slot.isCacheableGetter())
 361                         type = AccessCase::Getter;
 362                     else if (slot.attributes() &amp; PropertyAttribute::CustomAccessor)
 363                         type = AccessCase::CustomAccessorGetter;
 364                     else
 365                         type = AccessCase::CustomValueGetter;
 366 
 367                     if (kind == GetByIDKind::WithThis &amp;&amp; type == AccessCase::CustomAccessorGetter &amp;&amp; domAttribute)
 368                         return GiveUpOnCache;
 369 
 370                     newCase = GetterSetterAccessCase::create(
 371                         vm, codeBlock, type, offset, structure, conditionSet, loadTargetFromProxy,
 372                         slot.watchpointSet(), slot.isCacheableCustom() ? slot.customGetter() : nullptr,
 373                         slot.isCacheableCustom() &amp;&amp; slot.slotBase() != baseValue ? slot.slotBase() : nullptr,
 374                         domAttribute, WTFMove(prototypeAccessChain));
 375                 }
 376             }
 377         }
 378 
 379         LOG_IC((ICEvent::GetByIdAddAccessCase, baseValue.classInfoOrNull(vm), propertyName));
 380 
 381         result = stubInfo.addAccessCase(locker, codeBlock, propertyName, WTFMove(newCase));
 382 
 383         if (result.generatedSomeCode()) {
 384             LOG_IC((ICEvent::GetByIdReplaceWithJump, baseValue.classInfoOrNull(vm), propertyName));
 385 
 386             RELEASE_ASSERT(result.code());
 387             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 388         }
 389     }
 390 
 391     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec-&gt;codeBlock(), result);
 392 
 393     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 394 }
 395 
 396 void repatchGetByID(ExecState* exec, JSValue baseValue, const Identifier&amp; propertyName, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, GetByIDKind kind)
 397 {
 398     SuperSamplerScope superSamplerScope(false);
 399 
 400     if (tryCacheGetByID(exec, baseValue, propertyName, slot, stubInfo, kind) == GiveUpOnCache) {
 401         CodeBlock* codeBlock = exec-&gt;codeBlock();
 402         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateGetByIdFunction(kind));
 403     }
 404 }
 405 
 406 static V_JITOperation_ESsiJJI appropriateGenericPutByIdFunction(const PutPropertySlot &amp;slot, PutKind putKind)
 407 {
 408     if (slot.isStrictMode()) {
 409         if (putKind == Direct)
 410             return operationPutByIdDirectStrict;
 411         return operationPutByIdStrict;
 412     }
 413     if (putKind == Direct)
 414         return operationPutByIdDirectNonStrict;
 415     return operationPutByIdNonStrict;
 416 }
 417 
 418 static V_JITOperation_ESsiJJI appropriateOptimizingPutByIdFunction(const PutPropertySlot &amp;slot, PutKind putKind)
 419 {
 420     if (slot.isStrictMode()) {
 421         if (putKind == Direct)
 422             return operationPutByIdDirectStrictOptimize;
 423         return operationPutByIdStrictOptimize;
 424     }
 425     if (putKind == Direct)
 426         return operationPutByIdDirectNonStrictOptimize;
 427     return operationPutByIdNonStrictOptimize;
 428 }
 429 
 430 static InlineCacheAction tryCachePutByID(ExecState* exec, JSValue baseValue, Structure* structure, const Identifier&amp; ident, const PutPropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, PutKind putKind)
 431 {
 432     VM&amp; vm = exec-&gt;vm();
 433     AccessGenerationResult result;
 434     {
 435         GCSafeConcurrentJSLocker locker(exec-&gt;codeBlock()-&gt;m_lock, exec-&gt;vm().heap);
 436 
 437         if (forceICFailure(exec))
 438             return GiveUpOnCache;
 439 
 440         CodeBlock* codeBlock = exec-&gt;codeBlock();
 441 
 442         if (!baseValue.isCell())
 443             return GiveUpOnCache;
 444 
 445         if (!slot.isCacheablePut() &amp;&amp; !slot.isCacheableCustom() &amp;&amp; !slot.isCacheableSetter())
 446             return GiveUpOnCache;
 447 
 448         // FIXME: We should try to do something smarter here...
 449         if (isCopyOnWrite(structure-&gt;indexingMode()))
 450             return GiveUpOnCache;
 451         // We can&#39;t end up storing to a CoW on the prototype since it shouldn&#39;t own properties.
 452         ASSERT(!isCopyOnWrite(slot.base()-&gt;indexingMode()));
 453 
 454         if (!structure-&gt;propertyAccessesAreCacheable())
 455             return GiveUpOnCache;
 456 
 457         std::unique_ptr&lt;AccessCase&gt; newCase;
 458         JSCell* baseCell = baseValue.asCell();
 459 
 460         if (slot.base() == baseValue &amp;&amp; slot.isCacheablePut()) {
 461             if (slot.type() == PutPropertySlot::ExistingProperty) {
 462                 // This assert helps catch bugs if we accidentally forget to disable caching
 463                 // when we transition then store to an existing property. This is common among
 464                 // paths that reify lazy properties. If we reify a lazy property and forget
 465                 // to disable caching, we may come down this path. The Replace IC does not
 466                 // know how to model these types of structure transitions (or any structure
 467                 // transition for that matter).
 468                 RELEASE_ASSERT(baseValue.asCell()-&gt;structure(vm) == structure);
 469 
 470                 structure-&gt;didCachePropertyReplacement(vm, slot.cachedOffset());
 471 
 472                 if (stubInfo.cacheType == CacheType::Unset
 473                     &amp;&amp; InlineAccess::canGenerateSelfPropertyReplace(stubInfo, slot.cachedOffset())
 474                     &amp;&amp; !structure-&gt;needImpurePropertyWatchpoint()) {
 475 
 476                     bool generatedCodeInline = InlineAccess::generateSelfPropertyReplace(stubInfo, structure, slot.cachedOffset());
 477                     if (generatedCodeInline) {
 478                         LOG_IC((ICEvent::PutByIdSelfPatch, structure-&gt;classInfo(), ident));
 479                         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingPutByIdFunction(slot, putKind));
 480                         stubInfo.initPutByIdReplace(codeBlock, structure, slot.cachedOffset());
 481                         return RetryCacheLater;
 482                     }
 483                 }
 484 
 485                 newCase = AccessCase::create(vm, codeBlock, AccessCase::Replace, slot.cachedOffset(), structure);
 486             } else {
 487                 ASSERT(slot.type() == PutPropertySlot::NewProperty);
 488 
 489                 if (!structure-&gt;isObject())
 490                     return GiveUpOnCache;
 491 
 492                 if (structure-&gt;isDictionary()) {
 493                     if (structure-&gt;hasBeenFlattenedBefore())
 494                         return GiveUpOnCache;
 495                     structure-&gt;flattenDictionaryStructure(vm, jsCast&lt;JSObject*&gt;(baseValue));
 496                 }
 497 
 498                 PropertyOffset offset;
 499                 Structure* newStructure =
 500                     Structure::addPropertyTransitionToExistingStructureConcurrently(
 501                         structure, ident.impl(), 0, offset);
 502                 if (!newStructure || !newStructure-&gt;propertyAccessesAreCacheable())
 503                     return GiveUpOnCache;
 504 
 505                 ASSERT(newStructure-&gt;previousID() == structure);
 506                 ASSERT(!newStructure-&gt;isDictionary());
 507                 ASSERT(newStructure-&gt;isObject());
 508 
 509                 std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 510                 ObjectPropertyConditionSet conditionSet;
 511                 if (putKind == NotDirect) {
 512                     bool usesPolyProto;
 513                     prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), baseCell, nullptr, usesPolyProto);
 514                     if (!prototypeAccessChain) {
 515                         // It&#39;s invalid to access this prototype property.
 516                         return GiveUpOnCache;
 517                     }
 518 
 519                     if (!usesPolyProto) {
 520                         prototypeAccessChain = nullptr;
 521                         conditionSet =
 522                             generateConditionsForPropertySetterMiss(
 523                                 vm, codeBlock, exec, newStructure, ident.impl());
 524                         if (!conditionSet.isValid())
 525                             return GiveUpOnCache;
 526                     }
 527 
 528                 }
 529 
 530                 newCase = AccessCase::create(vm, codeBlock, offset, structure, newStructure, conditionSet, WTFMove(prototypeAccessChain));
 531             }
 532         } else if (slot.isCacheableCustom() || slot.isCacheableSetter()) {
 533             if (slot.isCacheableCustom()) {
 534                 ObjectPropertyConditionSet conditionSet;
 535                 std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 536 
 537                 if (slot.base() != baseValue) {
 538                     bool usesPolyProto;
 539                     prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), baseCell, slot.base(), usesPolyProto);
 540                     if (!prototypeAccessChain) {
 541                         // It&#39;s invalid to access this prototype property.
 542                         return GiveUpOnCache;
 543                     }
 544 
 545                     if (!usesPolyProto) {
 546                         prototypeAccessChain = nullptr;
 547                         conditionSet =
 548                             generateConditionsForPrototypePropertyHitCustom(
 549                                 vm, codeBlock, exec, structure, slot.base(), ident.impl());
 550                         if (!conditionSet.isValid())
 551                             return GiveUpOnCache;
 552                     }
 553                 }
 554 
 555                 newCase = GetterSetterAccessCase::create(
 556                     vm, codeBlock, slot.isCustomAccessor() ? AccessCase::CustomAccessorSetter : AccessCase::CustomValueSetter, structure, invalidOffset,
 557                     conditionSet, WTFMove(prototypeAccessChain), slot.customSetter(), slot.base() != baseValue ? slot.base() : nullptr);
 558             } else {
 559                 ObjectPropertyConditionSet conditionSet;
 560                 std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 561                 PropertyOffset offset = slot.cachedOffset();
 562 
 563                 if (slot.base() != baseValue) {
 564                     bool usesPolyProto;
 565                     prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), baseCell, slot.base(), usesPolyProto);
 566                     if (!prototypeAccessChain) {
 567                         // It&#39;s invalid to access this prototype property.
 568                         return GiveUpOnCache;
 569                     }
 570 
 571                     if (!usesPolyProto) {
 572                         prototypeAccessChain = nullptr;
 573                         conditionSet =
 574                             generateConditionsForPrototypePropertyHit(
 575                                 vm, codeBlock, exec, structure, slot.base(), ident.impl());
 576                         if (!conditionSet.isValid())
 577                             return GiveUpOnCache;
 578 
 579                         PropertyOffset conditionSetOffset = conditionSet.slotBaseCondition().offset();
 580                         if (UNLIKELY(offset != conditionSetOffset))
 581                             CRASH_WITH_INFO(offset, conditionSetOffset, slot.base()-&gt;type(), baseCell-&gt;type(), conditionSet.size());
 582                     }
 583 
 584                 }
 585 
 586                 newCase = GetterSetterAccessCase::create(
 587                     vm, codeBlock, AccessCase::Setter, structure, offset, conditionSet, WTFMove(prototypeAccessChain));
 588             }
 589         }
 590 
 591         LOG_IC((ICEvent::PutByIdAddAccessCase, structure-&gt;classInfo(), ident));
 592 
 593         result = stubInfo.addAccessCase(locker, codeBlock, ident, WTFMove(newCase));
 594 
 595         if (result.generatedSomeCode()) {
 596             LOG_IC((ICEvent::PutByIdReplaceWithJump, structure-&gt;classInfo(), ident));
 597 
 598             RELEASE_ASSERT(result.code());
 599 
 600             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 601         }
 602     }
 603 
 604     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec-&gt;codeBlock(), result);
 605 
 606     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 607 }
 608 
 609 void repatchPutByID(ExecState* exec, JSValue baseValue, Structure* structure, const Identifier&amp; propertyName, const PutPropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, PutKind putKind)
 610 {
 611     SuperSamplerScope superSamplerScope(false);
 612 
 613     if (tryCachePutByID(exec, baseValue, structure, propertyName, slot, stubInfo, putKind) == GiveUpOnCache) {
 614         CodeBlock* codeBlock = exec-&gt;codeBlock();
 615         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateGenericPutByIdFunction(slot, putKind));
 616     }
 617 }
 618 
 619 static InlineCacheAction tryCacheInByID(
 620     ExecState* exec, JSObject* base, const Identifier&amp; ident,
 621     bool wasFound, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo)
 622 {
 623     VM&amp; vm = exec-&gt;vm();
 624     AccessGenerationResult result;
 625 
 626     {
 627         GCSafeConcurrentJSLocker locker(exec-&gt;codeBlock()-&gt;m_lock, vm.heap);
 628         if (forceICFailure(exec))
 629             return GiveUpOnCache;
 630 
 631         if (!base-&gt;structure(vm)-&gt;propertyAccessesAreCacheable() || (!wasFound &amp;&amp; !base-&gt;structure(vm)-&gt;propertyAccessesAreCacheableForAbsence()))
 632             return GiveUpOnCache;
 633 
 634         if (wasFound) {
 635             if (!slot.isCacheable())
 636                 return GiveUpOnCache;
 637         }
 638 
 639         CodeBlock* codeBlock = exec-&gt;codeBlock();
 640         Structure* structure = base-&gt;structure(vm);
 641 
 642         std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 643         ObjectPropertyConditionSet conditionSet;
 644         if (wasFound) {
 645             InlineCacheAction action = actionForCell(vm, base);
 646             if (action != AttemptToCache)
 647                 return action;
 648 
 649             // Optimize self access.
 650             if (stubInfo.cacheType == CacheType::Unset
 651                 &amp;&amp; slot.isCacheableValue()
 652                 &amp;&amp; slot.slotBase() == base
 653                 &amp;&amp; !slot.watchpointSet()
 654                 &amp;&amp; !structure-&gt;needImpurePropertyWatchpoint()) {
 655                 bool generatedCodeInline = InlineAccess::generateSelfInAccess(stubInfo, structure);
 656                 if (generatedCodeInline) {
 657                     LOG_IC((ICEvent::InByIdSelfPatch, structure-&gt;classInfo(), ident));
 658                     structure-&gt;startWatchingPropertyForReplacements(vm, slot.cachedOffset());
 659                     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInByIdOptimize);
 660                     stubInfo.initInByIdSelf(codeBlock, structure, slot.cachedOffset());
 661                     return RetryCacheLater;
 662                 }
 663             }
 664 
 665             if (slot.slotBase() != base) {
 666                 bool usesPolyProto;
 667                 prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), base, slot, usesPolyProto);
 668                 if (!prototypeAccessChain) {
 669                     // It&#39;s invalid to access this prototype property.
 670                     return GiveUpOnCache;
 671                 }
 672                 if (!usesPolyProto) {
 673                     prototypeAccessChain = nullptr;
 674                     conditionSet = generateConditionsForPrototypePropertyHit(
 675                         vm, codeBlock, exec, structure, slot.slotBase(), ident.impl());
 676                 }
 677             }
 678         } else {
 679             bool usesPolyProto;
 680             prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), base, slot, usesPolyProto);
 681             if (!prototypeAccessChain) {
 682                 // It&#39;s invalid to access this prototype property.
 683                 return GiveUpOnCache;
 684             }
 685 
 686             if (!usesPolyProto) {
 687                 prototypeAccessChain = nullptr;
 688                 conditionSet = generateConditionsForPropertyMiss(
 689                     vm, codeBlock, exec, structure, ident.impl());
 690             }
 691         }
 692         if (!conditionSet.isValid())
 693             return GiveUpOnCache;
 694 
 695         LOG_IC((ICEvent::InAddAccessCase, structure-&gt;classInfo(), ident));
 696 
 697         std::unique_ptr&lt;AccessCase&gt; newCase = AccessCase::create(
 698             vm, codeBlock, wasFound ? AccessCase::InHit : AccessCase::InMiss, wasFound ? slot.cachedOffset() : invalidOffset, structure, conditionSet, WTFMove(prototypeAccessChain));
 699 
 700         result = stubInfo.addAccessCase(locker, codeBlock, ident, WTFMove(newCase));
 701 
 702         if (result.generatedSomeCode()) {
 703             LOG_IC((ICEvent::InReplaceWithJump, structure-&gt;classInfo(), ident));
 704 
 705             RELEASE_ASSERT(result.code());
 706             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 707         }
 708     }
 709 
 710     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec-&gt;codeBlock(), result);
 711 
 712     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 713 }
 714 
 715 void repatchInByID(ExecState* exec, JSObject* baseObject, const Identifier&amp; propertyName, bool wasFound, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo)
 716 {
 717     SuperSamplerScope superSamplerScope(false);
 718 
 719     if (tryCacheInByID(exec, baseObject, propertyName, wasFound, slot, stubInfo) == GiveUpOnCache) {
 720         CodeBlock* codeBlock = exec-&gt;codeBlock();
 721         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInById);
 722     }
 723 }
 724 
 725 static InlineCacheAction tryCacheInstanceOf(
 726     ExecState* exec, JSValue valueValue, JSValue prototypeValue, StructureStubInfo&amp; stubInfo,
 727     bool wasFound)
 728 {
 729     VM&amp; vm = exec-&gt;vm();
 730     CodeBlock* codeBlock = exec-&gt;codeBlock();
 731     AccessGenerationResult result;
 732 
 733     RELEASE_ASSERT(valueValue.isCell()); // shouldConsiderCaching rejects non-cells.
 734 
 735     if (forceICFailure(exec))
 736         return GiveUpOnCache;
 737 
 738     {
 739         GCSafeConcurrentJSLocker locker(codeBlock-&gt;m_lock, vm.heap);
 740 
 741         JSCell* value = valueValue.asCell();
 742         Structure* structure = value-&gt;structure(vm);
 743         std::unique_ptr&lt;AccessCase&gt; newCase;
 744         JSObject* prototype = jsDynamicCast&lt;JSObject*&gt;(vm, prototypeValue);
 745         if (prototype) {
 746             if (!jsDynamicCast&lt;JSObject*&gt;(vm, value)) {
 747                 newCase = InstanceOfAccessCase::create(
 748                     vm, codeBlock, AccessCase::InstanceOfMiss, structure, ObjectPropertyConditionSet(),
 749                     prototype);
 750             } else if (structure-&gt;prototypeQueriesAreCacheable()) {
 751                 // FIXME: Teach this to do poly proto.
 752                 // https://bugs.webkit.org/show_bug.cgi?id=185663
 753 
 754                 ObjectPropertyConditionSet conditionSet = generateConditionsForInstanceOf(
 755                     vm, codeBlock, exec, structure, prototype, wasFound);
 756 
 757                 if (conditionSet.isValid()) {
 758                     newCase = InstanceOfAccessCase::create(
 759                         vm, codeBlock,
 760                         wasFound ? AccessCase::InstanceOfHit : AccessCase::InstanceOfMiss,
 761                         structure, conditionSet, prototype);
 762                 }
 763             }
 764         }
 765 
 766         if (!newCase)
 767             newCase = AccessCase::create(vm, codeBlock, AccessCase::InstanceOfGeneric);
 768 
 769         LOG_IC((ICEvent::InstanceOfAddAccessCase, structure-&gt;classInfo(), Identifier()));
 770 
 771         result = stubInfo.addAccessCase(locker, codeBlock, Identifier(), WTFMove(newCase));
 772 
 773         if (result.generatedSomeCode()) {
 774             LOG_IC((ICEvent::InstanceOfReplaceWithJump, structure-&gt;classInfo(), Identifier()));
 775 
 776             RELEASE_ASSERT(result.code());
 777 
 778             MacroAssembler::repatchJump(
 779                 stubInfo.patchableJump(),
 780                 CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 781         }
 782     }
 783 
 784     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, codeBlock, result);
 785 
 786     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 787 }
 788 
 789 void repatchInstanceOf(
 790     ExecState* exec, JSValue valueValue, JSValue prototypeValue, StructureStubInfo&amp; stubInfo,
 791     bool wasFound)
 792 {
 793     SuperSamplerScope superSamplerScope(false);
 794     if (tryCacheInstanceOf(exec, valueValue, prototypeValue, stubInfo, wasFound) == GiveUpOnCache)
 795         ftlThunkAwareRepatchCall(exec-&gt;codeBlock(), stubInfo.slowPathCallLocation(), operationInstanceOfGeneric);
 796 }
 797 
 798 static void linkSlowFor(VM*, CallLinkInfo&amp; callLinkInfo, MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; codeRef)
 799 {
 800     MacroAssembler::repatchNearCall(callLinkInfo.callReturnLocation(), CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(codeRef.code()));
 801 }
 802 
 803 static void linkSlowFor(VM* vm, CallLinkInfo&amp; callLinkInfo, ThunkGenerator generator)
 804 {
 805     linkSlowFor(vm, callLinkInfo, vm-&gt;getCTIStub(generator).retagged&lt;JITStubRoutinePtrTag&gt;());
 806 }
 807 
 808 static void linkSlowFor(VM* vm, CallLinkInfo&amp; callLinkInfo)
 809 {
 810     MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; virtualThunk = virtualThunkFor(vm, callLinkInfo);
 811     linkSlowFor(vm, callLinkInfo, virtualThunk);
 812     callLinkInfo.setSlowStub(createJITStubRoutine(virtualThunk, *vm, nullptr, true));
 813 }
 814 
 815 static JSCell* webAssemblyOwner(JSCell* callee)
 816 {
 817 #if ENABLE(WEBASSEMBLY)
 818     // Each WebAssembly.Instance shares the stubs from their WebAssembly.Module, which are therefore the appropriate owner.
 819     return jsCast&lt;WebAssemblyToJSCallee*&gt;(callee)-&gt;module();
 820 #else
 821     UNUSED_PARAM(callee);
 822     RELEASE_ASSERT_NOT_REACHED();
 823     return nullptr;
 824 #endif // ENABLE(WEBASSEMBLY)
 825 }
 826 
 827 void linkFor(
 828     ExecState* exec, CallLinkInfo&amp; callLinkInfo, CodeBlock* calleeCodeBlock,
 829     JSObject* callee, MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr)
 830 {
 831     ASSERT(!callLinkInfo.stub());
 832 
 833     CallFrame* callerFrame = exec-&gt;callerFrame();
 834     // Our caller must have a cell for a callee. When calling
 835     // this from Wasm, we ensure the callee is a cell.
 836     ASSERT(callerFrame-&gt;callee().isCell());
 837 
 838     VM&amp; vm = callerFrame-&gt;vm();
 839     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
 840 
 841     // WebAssembly -&gt; JS stubs don&#39;t have a valid CodeBlock.
 842     JSCell* owner = isWebAssemblyToJSCallee(callerFrame-&gt;callee().asCell()) ? webAssemblyOwner(callerFrame-&gt;callee().asCell()) : callerCodeBlock;
 843     ASSERT(owner);
 844 
 845     ASSERT(!callLinkInfo.isLinked());
 846     callLinkInfo.setCallee(vm, owner, callee);
 847     callLinkInfo.setLastSeenCallee(vm, owner, callee);
 848     if (shouldDumpDisassemblyFor(callerCodeBlock))
 849         dataLog(&quot;Linking call in &quot;, FullCodeOrigin(callerCodeBlock, callLinkInfo.codeOrigin()), &quot; to &quot;, pointerDump(calleeCodeBlock), &quot;, entrypoint at &quot;, codePtr, &quot;\n&quot;);
 850 
 851     MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), CodeLocationLabel&lt;JSEntryPtrTag&gt;(codePtr));
 852 
 853     if (calleeCodeBlock)
 854         calleeCodeBlock-&gt;linkIncomingCall(callerFrame, &amp;callLinkInfo);
 855 
 856     if (callLinkInfo.specializationKind() == CodeForCall &amp;&amp; callLinkInfo.allowStubs()) {
 857         linkSlowFor(&amp;vm, callLinkInfo, linkPolymorphicCallThunkGenerator);
 858         return;
 859     }
 860 
 861     linkSlowFor(&amp;vm, callLinkInfo);
 862 }
 863 
 864 void linkDirectFor(
 865     ExecState* exec, CallLinkInfo&amp; callLinkInfo, CodeBlock* calleeCodeBlock,
 866     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr)
 867 {
 868     ASSERT(!callLinkInfo.stub());
 869 
 870     CodeBlock* callerCodeBlock = exec-&gt;codeBlock();
 871 
 872     VM* vm = callerCodeBlock-&gt;vm();
 873 
 874     ASSERT(!callLinkInfo.isLinked());
 875     callLinkInfo.setCodeBlock(*vm, callerCodeBlock, jsCast&lt;FunctionCodeBlock*&gt;(calleeCodeBlock));
 876     if (shouldDumpDisassemblyFor(callerCodeBlock))
 877         dataLog(&quot;Linking call in &quot;, FullCodeOrigin(callerCodeBlock, callLinkInfo.codeOrigin()), &quot; to &quot;, pointerDump(calleeCodeBlock), &quot;, entrypoint at &quot;, codePtr, &quot;\n&quot;);
 878 
 879     if (callLinkInfo.callType() == CallLinkInfo::DirectTailCall)
 880         MacroAssembler::repatchJumpToNop(callLinkInfo.patchableJump());
 881     MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), CodeLocationLabel&lt;JSEntryPtrTag&gt;(codePtr));
 882 
 883     if (calleeCodeBlock)
 884         calleeCodeBlock-&gt;linkIncomingCall(exec, &amp;callLinkInfo);
 885 }
 886 
 887 void linkSlowFor(
 888     ExecState* exec, CallLinkInfo&amp; callLinkInfo)
 889 {
 890     CodeBlock* callerCodeBlock = exec-&gt;callerFrame()-&gt;codeBlock();
 891     VM* vm = callerCodeBlock-&gt;vm();
 892 
 893     linkSlowFor(vm, callLinkInfo);
 894 }
 895 
 896 static void revertCall(VM* vm, CallLinkInfo&amp; callLinkInfo, MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; codeRef)
 897 {
 898     if (callLinkInfo.isDirect()) {
 899         callLinkInfo.clearCodeBlock();
 900         if (callLinkInfo.callType() == CallLinkInfo::DirectTailCall)
 901             MacroAssembler::repatchJump(callLinkInfo.patchableJump(), callLinkInfo.slowPathStart());
 902         else
 903             MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), callLinkInfo.slowPathStart());
 904     } else {
 905         MacroAssembler::revertJumpReplacementToBranchPtrWithPatch(
 906             MacroAssembler::startOfBranchPtrWithPatchOnRegister(callLinkInfo.hotPathBegin()),
 907             static_cast&lt;MacroAssembler::RegisterID&gt;(callLinkInfo.calleeGPR()), 0);
 908         linkSlowFor(vm, callLinkInfo, codeRef);
 909         callLinkInfo.clearCallee();
 910     }
 911     callLinkInfo.clearSeen();
 912     callLinkInfo.clearStub();
 913     callLinkInfo.clearSlowStub();
 914     if (callLinkInfo.isOnList())
 915         callLinkInfo.remove();
 916 }
 917 
 918 void unlinkFor(VM&amp; vm, CallLinkInfo&amp; callLinkInfo)
 919 {
 920     if (Options::dumpDisassembly())
 921         dataLog(&quot;Unlinking call at &quot;, callLinkInfo.hotPathOther(), &quot;\n&quot;);
 922 
 923     revertCall(&amp;vm, callLinkInfo, vm.getCTIStub(linkCallThunkGenerator).retagged&lt;JITStubRoutinePtrTag&gt;());
 924 }
 925 
 926 void linkVirtualFor(ExecState* exec, CallLinkInfo&amp; callLinkInfo)
 927 {
 928     CallFrame* callerFrame = exec-&gt;callerFrame();
 929     VM&amp; vm = callerFrame-&gt;vm();
 930     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
 931 
 932     if (shouldDumpDisassemblyFor(callerCodeBlock))
 933         dataLog(&quot;Linking virtual call at &quot;, FullCodeOrigin(callerCodeBlock, callerFrame-&gt;codeOrigin()), &quot;\n&quot;);
 934 
 935     MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; virtualThunk = virtualThunkFor(&amp;vm, callLinkInfo);
 936     revertCall(&amp;vm, callLinkInfo, virtualThunk);
 937     callLinkInfo.setSlowStub(createJITStubRoutine(virtualThunk, vm, nullptr, true));
 938     callLinkInfo.setClearedByVirtual();
 939 }
 940 
 941 namespace {
 942 struct CallToCodePtr {
 943     CCallHelpers::Call call;
 944     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr;
 945 };
 946 } // annonymous namespace
 947 
 948 void linkPolymorphicCall(
 949     ExecState* exec, CallLinkInfo&amp; callLinkInfo, CallVariant newVariant)
 950 {
 951     RELEASE_ASSERT(callLinkInfo.allowStubs());
 952 
 953     if (!newVariant) {
 954         linkVirtualFor(exec, callLinkInfo);
 955         return;
 956     }
 957 
 958     CallFrame* callerFrame = exec-&gt;callerFrame();
 959 
 960     // Our caller must be have a cell for a callee. When calling
 961     // this from Wasm, we ensure the callee is a cell.
 962     ASSERT(callerFrame-&gt;callee().isCell());
 963 
 964     VM&amp; vm = callerFrame-&gt;vm();
 965     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
 966     bool isWebAssembly = isWebAssemblyToJSCallee(callerFrame-&gt;callee().asCell());
 967 
 968     // WebAssembly -&gt; JS stubs don&#39;t have a valid CodeBlock.
 969     JSCell* owner = isWebAssembly ? webAssemblyOwner(callerFrame-&gt;callee().asCell()) : callerCodeBlock;
 970     ASSERT(owner);
 971 
 972     CallVariantList list;
 973     if (PolymorphicCallStubRoutine* stub = callLinkInfo.stub())
 974         list = stub-&gt;variants();
 975     else if (JSObject* oldCallee = callLinkInfo.callee())
 976         list = CallVariantList{ CallVariant(oldCallee) };
 977 
 978     list = variantListWithVariant(list, newVariant);
 979 
 980     // If there are any closure calls then it makes sense to treat all of them as closure calls.
 981     // This makes switching on callee cheaper. It also produces profiling that&#39;s easier on the DFG;
 982     // the DFG doesn&#39;t really want to deal with a combination of closure and non-closure callees.
 983     bool isClosureCall = false;
 984     for (CallVariant variant : list)  {
 985         if (variant.isClosureCall()) {
 986             list = despecifiedVariantList(list);
 987             isClosureCall = true;
 988             break;
 989         }
 990     }
 991 
 992     if (isClosureCall)
 993         callLinkInfo.setHasSeenClosure();
 994 
 995     Vector&lt;PolymorphicCallCase&gt; callCases;
 996 
 997     // Figure out what our cases are.
 998     for (CallVariant variant : list) {
 999         CodeBlock* codeBlock = nullptr;
1000         if (variant.executable() &amp;&amp; !variant.executable()-&gt;isHostFunction()) {
1001             ExecutableBase* executable = variant.executable();
1002             codeBlock = jsCast&lt;FunctionExecutable*&gt;(executable)-&gt;codeBlockForCall();
1003             // If we cannot handle a callee, either because we don&#39;t have a CodeBlock or because arity mismatch,
1004             // assume that it&#39;s better for this whole thing to be a virtual call.
1005             if (!codeBlock || exec-&gt;argumentCountIncludingThis() &lt; static_cast&lt;size_t&gt;(codeBlock-&gt;numParameters()) || callLinkInfo.isVarargs()) {
1006                 linkVirtualFor(exec, callLinkInfo);
1007                 return;
1008             }
1009         }
1010 
1011         callCases.append(PolymorphicCallCase(variant, codeBlock));
1012     }
1013 
1014     // If we are over the limit, just use a normal virtual call.
1015     unsigned maxPolymorphicCallVariantListSize;
1016     if (isWebAssembly)
1017         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSizeForWebAssemblyToJS();
1018     else if (callerCodeBlock-&gt;jitType() == JITCode::topTierJIT())
1019         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSizeForTopTier();
1020     else
1021         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSize();
1022 
1023     if (list.size() &gt; maxPolymorphicCallVariantListSize) {
1024         linkVirtualFor(exec, callLinkInfo);
1025         return;
1026     }
1027 
1028     GPRReg calleeGPR = static_cast&lt;GPRReg&gt;(callLinkInfo.calleeGPR());
1029 
1030     CCallHelpers stubJit(callerCodeBlock);
1031 
1032     CCallHelpers::JumpList slowPath;
1033 
1034     std::unique_ptr&lt;CallFrameShuffler&gt; frameShuffler;
1035     if (callLinkInfo.frameShuffleData()) {
1036         ASSERT(callLinkInfo.isTailCall());
1037         frameShuffler = std::make_unique&lt;CallFrameShuffler&gt;(stubJit, *callLinkInfo.frameShuffleData());
1038 #if USE(JSVALUE32_64)
1039         // We would have already checked that the callee is a cell, and we can
1040         // use the additional register this buys us.
1041         frameShuffler-&gt;assumeCalleeIsCell();
1042 #endif
1043         frameShuffler-&gt;lockGPR(calleeGPR);
1044     }
1045     GPRReg comparisonValueGPR;
1046 
1047     if (isClosureCall) {
1048         GPRReg scratchGPR;
1049         if (frameShuffler)
1050             scratchGPR = frameShuffler-&gt;acquireGPR();
1051         else
1052             scratchGPR = AssemblyHelpers::selectScratchGPR(calleeGPR);
1053         // Verify that we have a function and stash the executable in scratchGPR.
1054 
1055 #if USE(JSVALUE64)
1056         slowPath.append(stubJit.branchIfNotCell(calleeGPR));
1057 #else
1058         // We would have already checked that the callee is a cell.
1059 #endif
1060 
1061         // FIXME: We could add a fast path for InternalFunction with closure call.
1062         slowPath.append(stubJit.branchIfNotFunction(calleeGPR));
1063 
1064         stubJit.loadPtr(
1065             CCallHelpers::Address(calleeGPR, JSFunction::offsetOfExecutable()),
1066             scratchGPR);
1067 
1068         comparisonValueGPR = scratchGPR;
1069     } else
1070         comparisonValueGPR = calleeGPR;
1071 
1072     Vector&lt;int64_t&gt; caseValues(callCases.size());
1073     Vector&lt;CallToCodePtr&gt; calls(callCases.size());
1074     UniqueArray&lt;uint32_t&gt; fastCounts;
1075 
1076     if (!isWebAssembly &amp;&amp; callerCodeBlock-&gt;jitType() != JITCode::topTierJIT())
1077         fastCounts = makeUniqueArray&lt;uint32_t&gt;(callCases.size());
1078 
1079     for (size_t i = 0; i &lt; callCases.size(); ++i) {
1080         if (fastCounts)
1081             fastCounts[i] = 0;
1082 
1083         CallVariant variant = callCases[i].variant();
1084         int64_t newCaseValue = 0;
1085         if (isClosureCall) {
1086             newCaseValue = bitwise_cast&lt;intptr_t&gt;(variant.executable());
1087             // FIXME: We could add a fast path for InternalFunction with closure call.
1088             // https://bugs.webkit.org/show_bug.cgi?id=179311
1089             if (!newCaseValue)
1090                 continue;
1091         } else {
1092             if (auto* function = variant.function())
1093                 newCaseValue = bitwise_cast&lt;intptr_t&gt;(function);
1094             else
1095                 newCaseValue = bitwise_cast&lt;intptr_t&gt;(variant.internalFunction());
1096         }
1097 
1098         if (!ASSERT_DISABLED) {
1099             for (size_t j = 0; j &lt; i; ++j) {
1100                 if (caseValues[j] != newCaseValue)
1101                     continue;
1102 
1103                 dataLog(&quot;ERROR: Attempt to add duplicate case value.\n&quot;);
1104                 dataLog(&quot;Existing case values: &quot;);
1105                 CommaPrinter comma;
1106                 for (size_t k = 0; k &lt; i; ++k)
1107                     dataLog(comma, caseValues[k]);
1108                 dataLog(&quot;\n&quot;);
1109                 dataLog(&quot;Attempting to add: &quot;, newCaseValue, &quot;\n&quot;);
1110                 dataLog(&quot;Variant list: &quot;, listDump(callCases), &quot;\n&quot;);
1111                 RELEASE_ASSERT_NOT_REACHED();
1112             }
1113         }
1114 
1115         caseValues[i] = newCaseValue;
1116     }
1117 
1118     GPRReg fastCountsBaseGPR;
1119     if (frameShuffler)
1120         fastCountsBaseGPR = frameShuffler-&gt;acquireGPR();
1121     else {
1122         fastCountsBaseGPR =
1123             AssemblyHelpers::selectScratchGPR(calleeGPR, comparisonValueGPR, GPRInfo::regT3);
1124     }
1125     stubJit.move(CCallHelpers::TrustedImmPtr(fastCounts.get()), fastCountsBaseGPR);
1126     if (!frameShuffler &amp;&amp; callLinkInfo.isTailCall())
1127         stubJit.emitRestoreCalleeSaves();
1128     BinarySwitch binarySwitch(comparisonValueGPR, caseValues, BinarySwitch::IntPtr);
1129     CCallHelpers::JumpList done;
1130     while (binarySwitch.advance(stubJit)) {
1131         size_t caseIndex = binarySwitch.caseIndex();
1132 
1133         CallVariant variant = callCases[caseIndex].variant();
1134 
1135         MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr;
1136         if (variant.executable()) {
1137             ASSERT(variant.executable()-&gt;hasJITCodeForCall());
1138             codePtr = variant.executable()-&gt;generatedJITCodeForCall()-&gt;addressForCall(ArityCheckNotRequired);
1139         } else {
1140             ASSERT(variant.internalFunction());
1141             codePtr = vm.getCTIInternalFunctionTrampolineFor(CodeForCall);
1142         }
1143 
1144         if (fastCounts) {
1145             stubJit.add32(
1146                 CCallHelpers::TrustedImm32(1),
1147                 CCallHelpers::Address(fastCountsBaseGPR, caseIndex * sizeof(uint32_t)));
1148         }
1149         if (frameShuffler) {
1150             CallFrameShuffler(stubJit, frameShuffler-&gt;snapshot()).prepareForTailCall();
1151             calls[caseIndex].call = stubJit.nearTailCall();
1152         } else if (callLinkInfo.isTailCall()) {
1153             stubJit.prepareForTailCallSlow();
1154             calls[caseIndex].call = stubJit.nearTailCall();
1155         } else
1156             calls[caseIndex].call = stubJit.nearCall();
1157         calls[caseIndex].codePtr = codePtr;
1158         done.append(stubJit.jump());
1159     }
1160 
1161     slowPath.link(&amp;stubJit);
1162     binarySwitch.fallThrough().link(&amp;stubJit);
1163 
1164     if (frameShuffler) {
1165         frameShuffler-&gt;releaseGPR(calleeGPR);
1166         frameShuffler-&gt;releaseGPR(comparisonValueGPR);
1167         frameShuffler-&gt;releaseGPR(fastCountsBaseGPR);
1168 #if USE(JSVALUE32_64)
1169         frameShuffler-&gt;setCalleeJSValueRegs(JSValueRegs(GPRInfo::regT1, GPRInfo::regT0));
1170 #else
1171         frameShuffler-&gt;setCalleeJSValueRegs(JSValueRegs(GPRInfo::regT0));
1172 #endif
1173         frameShuffler-&gt;prepareForSlowPath();
1174     } else {
1175         stubJit.move(calleeGPR, GPRInfo::regT0);
1176 #if USE(JSVALUE32_64)
1177         stubJit.move(CCallHelpers::TrustedImm32(JSValue::CellTag), GPRInfo::regT1);
1178 #endif
1179     }
1180     stubJit.move(CCallHelpers::TrustedImmPtr(&amp;callLinkInfo), GPRInfo::regT2);
1181     stubJit.move(CCallHelpers::TrustedImmPtr(callLinkInfo.callReturnLocation().untaggedExecutableAddress()), GPRInfo::regT4);
1182 
1183     stubJit.restoreReturnAddressBeforeReturn(GPRInfo::regT4);
1184     AssemblyHelpers::Jump slow = stubJit.jump();
1185 
1186     LinkBuffer patchBuffer(stubJit, owner, JITCompilationCanFail);
1187     if (patchBuffer.didFailToAllocate()) {
1188         linkVirtualFor(exec, callLinkInfo);
1189         return;
1190     }
1191 
1192     RELEASE_ASSERT(callCases.size() == calls.size());
1193     for (CallToCodePtr callToCodePtr : calls) {
1194 #if CPU(ARM_THUMB2)
1195         // Tail call special-casing ensures proper linking on ARM Thumb2, where a tail call jumps to an address
1196         // with a non-decorated bottom bit but a normal call calls an address with a decorated bottom bit.
1197         bool isTailCall = callToCodePtr.call.isFlagSet(CCallHelpers::Call::Tail);
1198         void* target = isTailCall ? callToCodePtr.codePtr.dataLocation() : callToCodePtr.codePtr.executableAddress();
1199         patchBuffer.link(callToCodePtr.call, FunctionPtr&lt;JSEntryPtrTag&gt;(MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt;::createFromExecutableAddress(target)));
1200 #else
1201         patchBuffer.link(callToCodePtr.call, FunctionPtr&lt;JSEntryPtrTag&gt;(callToCodePtr.codePtr));
1202 #endif
1203     }
1204     if (isWebAssembly || JITCode::isOptimizingJIT(callerCodeBlock-&gt;jitType()))
1205         patchBuffer.link(done, callLinkInfo.callReturnLocation().labelAtOffset(0));
1206     else
1207         patchBuffer.link(done, callLinkInfo.hotPathOther().labelAtOffset(0));
1208     patchBuffer.link(slow, CodeLocationLabel&lt;JITThunkPtrTag&gt;(vm.getCTIStub(linkPolymorphicCallThunkGenerator).code()));
1209 
1210     auto stubRoutine = adoptRef(*new PolymorphicCallStubRoutine(
1211         FINALIZE_CODE_FOR(
1212             callerCodeBlock, patchBuffer, JITStubRoutinePtrTag,
1213             &quot;Polymorphic call stub for %s, return point %p, targets %s&quot;,
1214                 isWebAssembly ? &quot;WebAssembly&quot; : toCString(*callerCodeBlock).data(), callLinkInfo.callReturnLocation().labelAtOffset(0).executableAddress(),
1215                 toCString(listDump(callCases)).data()),
1216         vm, owner, exec-&gt;callerFrame(), callLinkInfo, callCases,
1217         WTFMove(fastCounts)));
1218 
1219     MacroAssembler::replaceWithJump(
1220         MacroAssembler::startOfBranchPtrWithPatchOnRegister(callLinkInfo.hotPathBegin()),
1221         CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(stubRoutine-&gt;code().code()));
1222     // The original slow path is unreachable on 64-bits, but still
1223     // reachable on 32-bits since a non-cell callee will always
1224     // trigger the slow path
1225     linkSlowFor(&amp;vm, callLinkInfo);
1226 
1227     // If there had been a previous stub routine, that one will die as soon as the GC runs and sees
1228     // that it&#39;s no longer on stack.
1229     callLinkInfo.setStub(WTFMove(stubRoutine));
1230 
1231     // The call link info no longer has a call cache apart from the jump to the polymorphic call
1232     // stub.
1233     if (callLinkInfo.isOnList())
1234         callLinkInfo.remove();
1235 }
1236 
1237 void resetGetByID(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo, GetByIDKind kind)
1238 {
1239     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
1240     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
1241 }
1242 
1243 void resetPutByID(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo)
1244 {
1245     V_JITOperation_ESsiJJI unoptimizedFunction = reinterpret_cast&lt;V_JITOperation_ESsiJJI&gt;(readPutICCallTarget(codeBlock, stubInfo.slowPathCallLocation()).executableAddress());
1246     V_JITOperation_ESsiJJI optimizedFunction;
1247     if (unoptimizedFunction == operationPutByIdStrict || unoptimizedFunction == operationPutByIdStrictOptimize)
1248         optimizedFunction = operationPutByIdStrictOptimize;
1249     else if (unoptimizedFunction == operationPutByIdNonStrict || unoptimizedFunction == operationPutByIdNonStrictOptimize)
1250         optimizedFunction = operationPutByIdNonStrictOptimize;
1251     else if (unoptimizedFunction == operationPutByIdDirectStrict || unoptimizedFunction == operationPutByIdDirectStrictOptimize)
1252         optimizedFunction = operationPutByIdDirectStrictOptimize;
1253     else {
1254         ASSERT(unoptimizedFunction == operationPutByIdDirectNonStrict || unoptimizedFunction == operationPutByIdDirectNonStrictOptimize);
1255         optimizedFunction = operationPutByIdDirectNonStrictOptimize;
1256     }
1257 
1258     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), optimizedFunction);
1259     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
1260 }
1261 
1262 static void resetPatchableJump(StructureStubInfo&amp; stubInfo)
1263 {
1264     MacroAssembler::repatchJump(stubInfo.patchableJump(), stubInfo.slowPathStartLocation());
1265 }
1266 
1267 void resetInByID(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo)
1268 {
1269     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInByIdOptimize);
1270     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
1271 }
1272 
1273 void resetInstanceOf(StructureStubInfo&amp; stubInfo)
1274 {
1275     resetPatchableJump(stubInfo);
1276 }
1277 
1278 } // namespace JSC
1279 
1280 #endif
    </pre>
  </body>
</html>