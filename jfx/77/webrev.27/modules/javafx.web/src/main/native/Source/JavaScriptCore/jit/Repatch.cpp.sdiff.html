<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/Repatch.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="RegisterSet.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="Repatch.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/Repatch.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  34 #include &quot;DFGOperations.h&quot;
  35 #include &quot;DFGSpeculativeJIT.h&quot;
  36 #include &quot;DOMJITGetterSetter.h&quot;
  37 #include &quot;DirectArguments.h&quot;
  38 #include &quot;ExecutableBaseInlines.h&quot;
  39 #include &quot;FTLThunks.h&quot;
  40 #include &quot;FullCodeOrigin.h&quot;
  41 #include &quot;FunctionCodeBlock.h&quot;
  42 #include &quot;GCAwareJITStubRoutine.h&quot;
  43 #include &quot;GetterSetter.h&quot;
  44 #include &quot;GetterSetterAccessCase.h&quot;
  45 #include &quot;ICStats.h&quot;
  46 #include &quot;InlineAccess.h&quot;
  47 #include &quot;InstanceOfAccessCase.h&quot;
  48 #include &quot;IntrinsicGetterAccessCase.h&quot;
  49 #include &quot;JIT.h&quot;
  50 #include &quot;JITInlines.h&quot;
  51 #include &quot;JSCInlines.h&quot;
  52 #include &quot;JSModuleNamespaceObject.h&quot;
  53 #include &quot;JSWebAssembly.h&quot;

  54 #include &quot;LinkBuffer.h&quot;
  55 #include &quot;ModuleNamespaceAccessCase.h&quot;
  56 #include &quot;PolymorphicAccess.h&quot;
  57 #include &quot;ScopedArguments.h&quot;
  58 #include &quot;ScratchRegisterAllocator.h&quot;
  59 #include &quot;StackAlignment.h&quot;
  60 #include &quot;StructureRareDataInlines.h&quot;
  61 #include &quot;StructureStubClearingWatchpoint.h&quot;
  62 #include &quot;StructureStubInfo.h&quot;
  63 #include &quot;SuperSampler.h&quot;
  64 #include &quot;ThunkGenerators.h&quot;


  65 #include &lt;wtf/CommaPrinter.h&gt;
  66 #include &lt;wtf/ListDump.h&gt;
  67 #include &lt;wtf/StringPrintStream.h&gt;
  68 
  69 namespace JSC {
  70 
  71 static FunctionPtr&lt;CFunctionPtrTag&gt; readPutICCallTarget(CodeBlock* codeBlock, CodeLocationCall&lt;JSInternalPtrTag&gt; call)
  72 {
  73     FunctionPtr&lt;OperationPtrTag&gt; target = MacroAssembler::readCallTarget&lt;OperationPtrTag&gt;(call);
  74 #if ENABLE(FTL_JIT)
<span class="line-modified">  75     if (codeBlock-&gt;jitType() == JITCode::FTLJIT) {</span>
  76         MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt; thunk = MacroAssemblerCodePtr&lt;OperationPtrTag&gt;::createFromExecutableAddress(target.executableAddress()).retagged&lt;JITThunkPtrTag&gt;();
<span class="line-modified">  77         return codeBlock-&gt;vm()-&gt;ftlThunks-&gt;keyForSlowPathCallThunk(thunk).callTarget().retagged&lt;CFunctionPtrTag&gt;();</span>
  78     }
  79 #else
  80     UNUSED_PARAM(codeBlock);
  81 #endif // ENABLE(FTL_JIT)
  82     return target.retagged&lt;CFunctionPtrTag&gt;();
  83 }
  84 
  85 void ftlThunkAwareRepatchCall(CodeBlock* codeBlock, CodeLocationCall&lt;JSInternalPtrTag&gt; call, FunctionPtr&lt;CFunctionPtrTag&gt; newCalleeFunction)
  86 {
  87 #if ENABLE(FTL_JIT)
<span class="line-modified">  88     if (codeBlock-&gt;jitType() == JITCode::FTLJIT) {</span>
<span class="line-modified">  89         VM&amp; vm = *codeBlock-&gt;vm();</span>
  90         FTL::Thunks&amp; thunks = *vm.ftlThunks;
  91         FunctionPtr&lt;OperationPtrTag&gt; target = MacroAssembler::readCallTarget&lt;OperationPtrTag&gt;(call);
  92         auto slowPathThunk = MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt;::createFromExecutableAddress(target.retaggedExecutableAddress&lt;JITThunkPtrTag&gt;());
  93         FTL::SlowPathCallKey key = thunks.keyForSlowPathCallThunk(slowPathThunk);
  94         key = key.withCallTarget(newCalleeFunction);
  95         MacroAssembler::repatchCall(call, FunctionPtr&lt;OperationPtrTag&gt;(thunks.getSlowPathCallThunk(key).retaggedCode&lt;OperationPtrTag&gt;()));
  96         return;
  97     }
  98 #else // ENABLE(FTL_JIT)
  99     UNUSED_PARAM(codeBlock);
 100 #endif // ENABLE(FTL_JIT)
 101     MacroAssembler::repatchCall(call, newCalleeFunction.retagged&lt;OperationPtrTag&gt;());
 102 }
 103 
 104 enum InlineCacheAction {
 105     GiveUpOnCache,
 106     RetryCacheLater,
 107     AttemptToCache
 108 };
 109 
</pre>
<hr />
<pre>
 250                 baseValue = jsCast&lt;JSProxy*&gt;(baseCell)-&gt;target();
 251                 baseCell = baseValue.asCell();
 252                 structure = baseCell-&gt;structure(vm);
 253                 loadTargetFromProxy = true;
 254             }
 255 
 256             InlineCacheAction action = actionForCell(vm, baseCell);
 257             if (action != AttemptToCache)
 258                 return action;
 259 
 260             // Optimize self access.
 261             if (stubInfo.cacheType == CacheType::Unset
 262                 &amp;&amp; slot.isCacheableValue()
 263                 &amp;&amp; slot.slotBase() == baseValue
 264                 &amp;&amp; !slot.watchpointSet()
 265                 &amp;&amp; !structure-&gt;needImpurePropertyWatchpoint()
 266                 &amp;&amp; !loadTargetFromProxy) {
 267 
 268                 bool generatedCodeInline = InlineAccess::generateSelfPropertyAccess(stubInfo, structure, slot.cachedOffset());
 269                 if (generatedCodeInline) {
<span class="line-modified"> 270                     LOG_IC((ICEvent::GetByIdSelfPatch, structure-&gt;classInfo(), propertyName));</span>
 271                     structure-&gt;startWatchingPropertyForReplacements(vm, slot.cachedOffset());
 272                     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
 273                     stubInfo.initGetByIdSelf(codeBlock, structure, slot.cachedOffset());
 274                     return RetryCacheLater;
 275                 }
 276             }
 277 
 278             std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 279 
 280             PropertyOffset offset = slot.isUnset() ? invalidOffset : slot.cachedOffset();
 281 
 282             if (slot.isUnset() || slot.slotBase() != baseValue) {
 283                 if (structure-&gt;typeInfo().prohibitsPropertyCaching())
 284                     return GiveUpOnCache;
 285 
 286                 if (structure-&gt;isDictionary()) {
 287                     if (structure-&gt;hasBeenFlattenedBefore())
 288                         return GiveUpOnCache;
 289                     structure-&gt;flattenDictionaryStructure(vm, jsCast&lt;JSObject*&gt;(baseCell));
 290                 }
</pre>
<hr />
<pre>
 359                     AccessCase::AccessType type;
 360                     if (slot.isCacheableGetter())
 361                         type = AccessCase::Getter;
 362                     else if (slot.attributes() &amp; PropertyAttribute::CustomAccessor)
 363                         type = AccessCase::CustomAccessorGetter;
 364                     else
 365                         type = AccessCase::CustomValueGetter;
 366 
 367                     if (kind == GetByIDKind::WithThis &amp;&amp; type == AccessCase::CustomAccessorGetter &amp;&amp; domAttribute)
 368                         return GiveUpOnCache;
 369 
 370                     newCase = GetterSetterAccessCase::create(
 371                         vm, codeBlock, type, offset, structure, conditionSet, loadTargetFromProxy,
 372                         slot.watchpointSet(), slot.isCacheableCustom() ? slot.customGetter() : nullptr,
 373                         slot.isCacheableCustom() &amp;&amp; slot.slotBase() != baseValue ? slot.slotBase() : nullptr,
 374                         domAttribute, WTFMove(prototypeAccessChain));
 375                 }
 376             }
 377         }
 378 
<span class="line-modified"> 379         LOG_IC((ICEvent::GetByIdAddAccessCase, baseValue.classInfoOrNull(vm), propertyName));</span>
 380 
 381         result = stubInfo.addAccessCase(locker, codeBlock, propertyName, WTFMove(newCase));
 382 
 383         if (result.generatedSomeCode()) {
<span class="line-modified"> 384             LOG_IC((ICEvent::GetByIdReplaceWithJump, baseValue.classInfoOrNull(vm), propertyName));</span>
 385 
 386             RELEASE_ASSERT(result.code());
 387             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 388         }
 389     }
 390 
 391     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec-&gt;codeBlock(), result);
 392 
 393     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 394 }
 395 
 396 void repatchGetByID(ExecState* exec, JSValue baseValue, const Identifier&amp; propertyName, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, GetByIDKind kind)
 397 {
 398     SuperSamplerScope superSamplerScope(false);
 399 
 400     if (tryCacheGetByID(exec, baseValue, propertyName, slot, stubInfo, kind) == GiveUpOnCache) {
 401         CodeBlock* codeBlock = exec-&gt;codeBlock();
 402         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateGetByIdFunction(kind));
 403     }
 404 }
</pre>
<hr />
<pre>
 458         JSCell* baseCell = baseValue.asCell();
 459 
 460         if (slot.base() == baseValue &amp;&amp; slot.isCacheablePut()) {
 461             if (slot.type() == PutPropertySlot::ExistingProperty) {
 462                 // This assert helps catch bugs if we accidentally forget to disable caching
 463                 // when we transition then store to an existing property. This is common among
 464                 // paths that reify lazy properties. If we reify a lazy property and forget
 465                 // to disable caching, we may come down this path. The Replace IC does not
 466                 // know how to model these types of structure transitions (or any structure
 467                 // transition for that matter).
 468                 RELEASE_ASSERT(baseValue.asCell()-&gt;structure(vm) == structure);
 469 
 470                 structure-&gt;didCachePropertyReplacement(vm, slot.cachedOffset());
 471 
 472                 if (stubInfo.cacheType == CacheType::Unset
 473                     &amp;&amp; InlineAccess::canGenerateSelfPropertyReplace(stubInfo, slot.cachedOffset())
 474                     &amp;&amp; !structure-&gt;needImpurePropertyWatchpoint()) {
 475 
 476                     bool generatedCodeInline = InlineAccess::generateSelfPropertyReplace(stubInfo, structure, slot.cachedOffset());
 477                     if (generatedCodeInline) {
<span class="line-modified"> 478                         LOG_IC((ICEvent::PutByIdSelfPatch, structure-&gt;classInfo(), ident));</span>
 479                         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingPutByIdFunction(slot, putKind));
 480                         stubInfo.initPutByIdReplace(codeBlock, structure, slot.cachedOffset());
 481                         return RetryCacheLater;
 482                     }
 483                 }
 484 
 485                 newCase = AccessCase::create(vm, codeBlock, AccessCase::Replace, slot.cachedOffset(), structure);
 486             } else {
 487                 ASSERT(slot.type() == PutPropertySlot::NewProperty);
 488 
 489                 if (!structure-&gt;isObject())
 490                     return GiveUpOnCache;
 491 
 492                 if (structure-&gt;isDictionary()) {
 493                     if (structure-&gt;hasBeenFlattenedBefore())
 494                         return GiveUpOnCache;
 495                     structure-&gt;flattenDictionaryStructure(vm, jsCast&lt;JSObject*&gt;(baseValue));
 496                 }
 497 
 498                 PropertyOffset offset;
</pre>
<hr />
<pre>
 559                 ObjectPropertyConditionSet conditionSet;
 560                 std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 561                 PropertyOffset offset = slot.cachedOffset();
 562 
 563                 if (slot.base() != baseValue) {
 564                     bool usesPolyProto;
 565                     prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), baseCell, slot.base(), usesPolyProto);
 566                     if (!prototypeAccessChain) {
 567                         // It&#39;s invalid to access this prototype property.
 568                         return GiveUpOnCache;
 569                     }
 570 
 571                     if (!usesPolyProto) {
 572                         prototypeAccessChain = nullptr;
 573                         conditionSet =
 574                             generateConditionsForPrototypePropertyHit(
 575                                 vm, codeBlock, exec, structure, slot.base(), ident.impl());
 576                         if (!conditionSet.isValid())
 577                             return GiveUpOnCache;
 578 
<span class="line-modified"> 579                         PropertyOffset conditionSetOffset = conditionSet.slotBaseCondition().offset();</span>
<span class="line-modified"> 580                         if (UNLIKELY(offset != conditionSetOffset))</span>
<span class="line-modified"> 581                             CRASH_WITH_INFO(offset, conditionSetOffset, slot.base()-&gt;type(), baseCell-&gt;type(), conditionSet.size());</span>

 582                     }
 583 
 584                 }
 585 
 586                 newCase = GetterSetterAccessCase::create(
 587                     vm, codeBlock, AccessCase::Setter, structure, offset, conditionSet, WTFMove(prototypeAccessChain));
 588             }
 589         }
 590 
<span class="line-modified"> 591         LOG_IC((ICEvent::PutByIdAddAccessCase, structure-&gt;classInfo(), ident));</span>
 592 
 593         result = stubInfo.addAccessCase(locker, codeBlock, ident, WTFMove(newCase));
 594 
 595         if (result.generatedSomeCode()) {
<span class="line-modified"> 596             LOG_IC((ICEvent::PutByIdReplaceWithJump, structure-&gt;classInfo(), ident));</span>
 597 
 598             RELEASE_ASSERT(result.code());
 599 
 600             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 601         }
 602     }
 603 
 604     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec-&gt;codeBlock(), result);
 605 
 606     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 607 }
 608 
 609 void repatchPutByID(ExecState* exec, JSValue baseValue, Structure* structure, const Identifier&amp; propertyName, const PutPropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, PutKind putKind)
 610 {
 611     SuperSamplerScope superSamplerScope(false);
 612 
 613     if (tryCachePutByID(exec, baseValue, structure, propertyName, slot, stubInfo, putKind) == GiveUpOnCache) {
 614         CodeBlock* codeBlock = exec-&gt;codeBlock();
 615         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateGenericPutByIdFunction(slot, putKind));
 616     }
</pre>
<hr />
<pre>
 637         }
 638 
 639         CodeBlock* codeBlock = exec-&gt;codeBlock();
 640         Structure* structure = base-&gt;structure(vm);
 641 
 642         std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 643         ObjectPropertyConditionSet conditionSet;
 644         if (wasFound) {
 645             InlineCacheAction action = actionForCell(vm, base);
 646             if (action != AttemptToCache)
 647                 return action;
 648 
 649             // Optimize self access.
 650             if (stubInfo.cacheType == CacheType::Unset
 651                 &amp;&amp; slot.isCacheableValue()
 652                 &amp;&amp; slot.slotBase() == base
 653                 &amp;&amp; !slot.watchpointSet()
 654                 &amp;&amp; !structure-&gt;needImpurePropertyWatchpoint()) {
 655                 bool generatedCodeInline = InlineAccess::generateSelfInAccess(stubInfo, structure);
 656                 if (generatedCodeInline) {
<span class="line-modified"> 657                     LOG_IC((ICEvent::InByIdSelfPatch, structure-&gt;classInfo(), ident));</span>
 658                     structure-&gt;startWatchingPropertyForReplacements(vm, slot.cachedOffset());
 659                     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInByIdOptimize);
 660                     stubInfo.initInByIdSelf(codeBlock, structure, slot.cachedOffset());
 661                     return RetryCacheLater;
 662                 }
 663             }
 664 
 665             if (slot.slotBase() != base) {
 666                 bool usesPolyProto;
 667                 prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), base, slot, usesPolyProto);
 668                 if (!prototypeAccessChain) {
 669                     // It&#39;s invalid to access this prototype property.
 670                     return GiveUpOnCache;
 671                 }
 672                 if (!usesPolyProto) {
 673                     prototypeAccessChain = nullptr;
 674                     conditionSet = generateConditionsForPrototypePropertyHit(
 675                         vm, codeBlock, exec, structure, slot.slotBase(), ident.impl());
 676                 }
 677             }
 678         } else {
 679             bool usesPolyProto;
 680             prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), base, slot, usesPolyProto);
 681             if (!prototypeAccessChain) {
 682                 // It&#39;s invalid to access this prototype property.
 683                 return GiveUpOnCache;
 684             }
 685 
 686             if (!usesPolyProto) {
 687                 prototypeAccessChain = nullptr;
 688                 conditionSet = generateConditionsForPropertyMiss(
 689                     vm, codeBlock, exec, structure, ident.impl());
 690             }
 691         }
 692         if (!conditionSet.isValid())
 693             return GiveUpOnCache;
 694 
<span class="line-modified"> 695         LOG_IC((ICEvent::InAddAccessCase, structure-&gt;classInfo(), ident));</span>
 696 
 697         std::unique_ptr&lt;AccessCase&gt; newCase = AccessCase::create(
 698             vm, codeBlock, wasFound ? AccessCase::InHit : AccessCase::InMiss, wasFound ? slot.cachedOffset() : invalidOffset, structure, conditionSet, WTFMove(prototypeAccessChain));
 699 
 700         result = stubInfo.addAccessCase(locker, codeBlock, ident, WTFMove(newCase));
 701 
 702         if (result.generatedSomeCode()) {
<span class="line-modified"> 703             LOG_IC((ICEvent::InReplaceWithJump, structure-&gt;classInfo(), ident));</span>
 704 
 705             RELEASE_ASSERT(result.code());
 706             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 707         }
 708     }
 709 
 710     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec-&gt;codeBlock(), result);
 711 
 712     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 713 }
 714 
 715 void repatchInByID(ExecState* exec, JSObject* baseObject, const Identifier&amp; propertyName, bool wasFound, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo)
 716 {
 717     SuperSamplerScope superSamplerScope(false);
 718 
 719     if (tryCacheInByID(exec, baseObject, propertyName, wasFound, slot, stubInfo) == GiveUpOnCache) {
 720         CodeBlock* codeBlock = exec-&gt;codeBlock();
 721         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInById);
 722     }
 723 }
</pre>
<hr />
<pre>
 778             MacroAssembler::repatchJump(
 779                 stubInfo.patchableJump(),
 780                 CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 781         }
 782     }
 783 
 784     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, codeBlock, result);
 785 
 786     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 787 }
 788 
 789 void repatchInstanceOf(
 790     ExecState* exec, JSValue valueValue, JSValue prototypeValue, StructureStubInfo&amp; stubInfo,
 791     bool wasFound)
 792 {
 793     SuperSamplerScope superSamplerScope(false);
 794     if (tryCacheInstanceOf(exec, valueValue, prototypeValue, stubInfo, wasFound) == GiveUpOnCache)
 795         ftlThunkAwareRepatchCall(exec-&gt;codeBlock(), stubInfo.slowPathCallLocation(), operationInstanceOfGeneric);
 796 }
 797 
<span class="line-modified"> 798 static void linkSlowFor(VM*, CallLinkInfo&amp; callLinkInfo, MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; codeRef)</span>
 799 {
 800     MacroAssembler::repatchNearCall(callLinkInfo.callReturnLocation(), CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(codeRef.code()));
 801 }
 802 
<span class="line-modified"> 803 static void linkSlowFor(VM* vm, CallLinkInfo&amp; callLinkInfo, ThunkGenerator generator)</span>
 804 {
<span class="line-modified"> 805     linkSlowFor(vm, callLinkInfo, vm-&gt;getCTIStub(generator).retagged&lt;JITStubRoutinePtrTag&gt;());</span>
 806 }
 807 
<span class="line-modified"> 808 static void linkSlowFor(VM* vm, CallLinkInfo&amp; callLinkInfo)</span>
 809 {
 810     MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; virtualThunk = virtualThunkFor(vm, callLinkInfo);
 811     linkSlowFor(vm, callLinkInfo, virtualThunk);
<span class="line-modified"> 812     callLinkInfo.setSlowStub(createJITStubRoutine(virtualThunk, *vm, nullptr, true));</span>
 813 }
 814 
 815 static JSCell* webAssemblyOwner(JSCell* callee)
 816 {
 817 #if ENABLE(WEBASSEMBLY)
 818     // Each WebAssembly.Instance shares the stubs from their WebAssembly.Module, which are therefore the appropriate owner.
 819     return jsCast&lt;WebAssemblyToJSCallee*&gt;(callee)-&gt;module();
 820 #else
 821     UNUSED_PARAM(callee);
 822     RELEASE_ASSERT_NOT_REACHED();
 823     return nullptr;
 824 #endif // ENABLE(WEBASSEMBLY)
 825 }
 826 
 827 void linkFor(
 828     ExecState* exec, CallLinkInfo&amp; callLinkInfo, CodeBlock* calleeCodeBlock,
 829     JSObject* callee, MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr)
 830 {
 831     ASSERT(!callLinkInfo.stub());
 832 
 833     CallFrame* callerFrame = exec-&gt;callerFrame();
 834     // Our caller must have a cell for a callee. When calling
 835     // this from Wasm, we ensure the callee is a cell.
 836     ASSERT(callerFrame-&gt;callee().isCell());
 837 
 838     VM&amp; vm = callerFrame-&gt;vm();
 839     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
 840 
 841     // WebAssembly -&gt; JS stubs don&#39;t have a valid CodeBlock.
 842     JSCell* owner = isWebAssemblyToJSCallee(callerFrame-&gt;callee().asCell()) ? webAssemblyOwner(callerFrame-&gt;callee().asCell()) : callerCodeBlock;
 843     ASSERT(owner);
 844 
 845     ASSERT(!callLinkInfo.isLinked());
 846     callLinkInfo.setCallee(vm, owner, callee);

 847     callLinkInfo.setLastSeenCallee(vm, owner, callee);
 848     if (shouldDumpDisassemblyFor(callerCodeBlock))
 849         dataLog(&quot;Linking call in &quot;, FullCodeOrigin(callerCodeBlock, callLinkInfo.codeOrigin()), &quot; to &quot;, pointerDump(calleeCodeBlock), &quot;, entrypoint at &quot;, codePtr, &quot;\n&quot;);
 850 
 851     MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), CodeLocationLabel&lt;JSEntryPtrTag&gt;(codePtr));
 852 
 853     if (calleeCodeBlock)
 854         calleeCodeBlock-&gt;linkIncomingCall(callerFrame, &amp;callLinkInfo);
 855 
 856     if (callLinkInfo.specializationKind() == CodeForCall &amp;&amp; callLinkInfo.allowStubs()) {
<span class="line-modified"> 857         linkSlowFor(&amp;vm, callLinkInfo, linkPolymorphicCallThunkGenerator);</span>
 858         return;
 859     }
 860 
<span class="line-modified"> 861     linkSlowFor(&amp;vm, callLinkInfo);</span>
 862 }
 863 
 864 void linkDirectFor(
 865     ExecState* exec, CallLinkInfo&amp; callLinkInfo, CodeBlock* calleeCodeBlock,
 866     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr)
 867 {
 868     ASSERT(!callLinkInfo.stub());
 869 
 870     CodeBlock* callerCodeBlock = exec-&gt;codeBlock();
 871 
<span class="line-modified"> 872     VM* vm = callerCodeBlock-&gt;vm();</span>
 873 
 874     ASSERT(!callLinkInfo.isLinked());
<span class="line-modified"> 875     callLinkInfo.setCodeBlock(*vm, callerCodeBlock, jsCast&lt;FunctionCodeBlock*&gt;(calleeCodeBlock));</span>
 876     if (shouldDumpDisassemblyFor(callerCodeBlock))
 877         dataLog(&quot;Linking call in &quot;, FullCodeOrigin(callerCodeBlock, callLinkInfo.codeOrigin()), &quot; to &quot;, pointerDump(calleeCodeBlock), &quot;, entrypoint at &quot;, codePtr, &quot;\n&quot;);
 878 
 879     if (callLinkInfo.callType() == CallLinkInfo::DirectTailCall)
 880         MacroAssembler::repatchJumpToNop(callLinkInfo.patchableJump());
 881     MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), CodeLocationLabel&lt;JSEntryPtrTag&gt;(codePtr));
 882 
 883     if (calleeCodeBlock)
 884         calleeCodeBlock-&gt;linkIncomingCall(exec, &amp;callLinkInfo);
 885 }
 886 
 887 void linkSlowFor(
 888     ExecState* exec, CallLinkInfo&amp; callLinkInfo)
 889 {
 890     CodeBlock* callerCodeBlock = exec-&gt;callerFrame()-&gt;codeBlock();
<span class="line-modified"> 891     VM* vm = callerCodeBlock-&gt;vm();</span>
 892 
 893     linkSlowFor(vm, callLinkInfo);
 894 }
 895 
<span class="line-modified"> 896 static void revertCall(VM* vm, CallLinkInfo&amp; callLinkInfo, MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; codeRef)</span>
 897 {
 898     if (callLinkInfo.isDirect()) {
 899         callLinkInfo.clearCodeBlock();
<span class="line-modified"> 900         if (callLinkInfo.callType() == CallLinkInfo::DirectTailCall)</span>
<span class="line-modified"> 901             MacroAssembler::repatchJump(callLinkInfo.patchableJump(), callLinkInfo.slowPathStart());</span>
<span class="line-modified"> 902         else</span>
<span class="line-modified"> 903             MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), callLinkInfo.slowPathStart());</span>


 904     } else {
<span class="line-modified"> 905         MacroAssembler::revertJumpReplacementToBranchPtrWithPatch(</span>
<span class="line-modified"> 906             MacroAssembler::startOfBranchPtrWithPatchOnRegister(callLinkInfo.hotPathBegin()),</span>
<span class="line-modified"> 907             static_cast&lt;MacroAssembler::RegisterID&gt;(callLinkInfo.calleeGPR()), 0);</span>
<span class="line-modified"> 908         linkSlowFor(vm, callLinkInfo, codeRef);</span>



 909         callLinkInfo.clearCallee();
 910     }
 911     callLinkInfo.clearSeen();
 912     callLinkInfo.clearStub();
 913     callLinkInfo.clearSlowStub();
 914     if (callLinkInfo.isOnList())
 915         callLinkInfo.remove();
 916 }
 917 
 918 void unlinkFor(VM&amp; vm, CallLinkInfo&amp; callLinkInfo)
 919 {
 920     if (Options::dumpDisassembly())
 921         dataLog(&quot;Unlinking call at &quot;, callLinkInfo.hotPathOther(), &quot;\n&quot;);
 922 
<span class="line-modified"> 923     revertCall(&amp;vm, callLinkInfo, vm.getCTIStub(linkCallThunkGenerator).retagged&lt;JITStubRoutinePtrTag&gt;());</span>
 924 }
 925 
<span class="line-modified"> 926 void linkVirtualFor(ExecState* exec, CallLinkInfo&amp; callLinkInfo)</span>
 927 {
 928     CallFrame* callerFrame = exec-&gt;callerFrame();
 929     VM&amp; vm = callerFrame-&gt;vm();
 930     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
 931 
 932     if (shouldDumpDisassemblyFor(callerCodeBlock))
 933         dataLog(&quot;Linking virtual call at &quot;, FullCodeOrigin(callerCodeBlock, callerFrame-&gt;codeOrigin()), &quot;\n&quot;);
 934 
<span class="line-modified"> 935     MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; virtualThunk = virtualThunkFor(&amp;vm, callLinkInfo);</span>
<span class="line-modified"> 936     revertCall(&amp;vm, callLinkInfo, virtualThunk);</span>
 937     callLinkInfo.setSlowStub(createJITStubRoutine(virtualThunk, vm, nullptr, true));
 938     callLinkInfo.setClearedByVirtual();
 939 }
 940 
 941 namespace {
 942 struct CallToCodePtr {
 943     CCallHelpers::Call call;
 944     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr;
 945 };
 946 } // annonymous namespace
 947 
 948 void linkPolymorphicCall(
 949     ExecState* exec, CallLinkInfo&amp; callLinkInfo, CallVariant newVariant)
 950 {
 951     RELEASE_ASSERT(callLinkInfo.allowStubs());
 952 







 953     if (!newVariant) {
 954         linkVirtualFor(exec, callLinkInfo);
 955         return;
 956     }
 957 
<span class="line-removed"> 958     CallFrame* callerFrame = exec-&gt;callerFrame();</span>
<span class="line-removed"> 959 </span>
 960     // Our caller must be have a cell for a callee. When calling
 961     // this from Wasm, we ensure the callee is a cell.
 962     ASSERT(callerFrame-&gt;callee().isCell());
 963 
<span class="line-removed"> 964     VM&amp; vm = callerFrame-&gt;vm();</span>
 965     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
 966     bool isWebAssembly = isWebAssemblyToJSCallee(callerFrame-&gt;callee().asCell());
 967 
 968     // WebAssembly -&gt; JS stubs don&#39;t have a valid CodeBlock.
 969     JSCell* owner = isWebAssembly ? webAssemblyOwner(callerFrame-&gt;callee().asCell()) : callerCodeBlock;
 970     ASSERT(owner);
 971 
 972     CallVariantList list;
 973     if (PolymorphicCallStubRoutine* stub = callLinkInfo.stub())
 974         list = stub-&gt;variants();
 975     else if (JSObject* oldCallee = callLinkInfo.callee())
<span class="line-modified"> 976         list = CallVariantList{ CallVariant(oldCallee) };</span>
 977 
 978     list = variantListWithVariant(list, newVariant);
 979 
 980     // If there are any closure calls then it makes sense to treat all of them as closure calls.
 981     // This makes switching on callee cheaper. It also produces profiling that&#39;s easier on the DFG;
 982     // the DFG doesn&#39;t really want to deal with a combination of closure and non-closure callees.
 983     bool isClosureCall = false;
 984     for (CallVariant variant : list)  {
 985         if (variant.isClosureCall()) {
 986             list = despecifiedVariantList(list);
 987             isClosureCall = true;
 988             break;
 989         }
 990     }
 991 
 992     if (isClosureCall)
 993         callLinkInfo.setHasSeenClosure();
 994 
 995     Vector&lt;PolymorphicCallCase&gt; callCases;

 996 
 997     // Figure out what our cases are.
 998     for (CallVariant variant : list) {
 999         CodeBlock* codeBlock = nullptr;
1000         if (variant.executable() &amp;&amp; !variant.executable()-&gt;isHostFunction()) {
1001             ExecutableBase* executable = variant.executable();
1002             codeBlock = jsCast&lt;FunctionExecutable*&gt;(executable)-&gt;codeBlockForCall();
1003             // If we cannot handle a callee, either because we don&#39;t have a CodeBlock or because arity mismatch,
1004             // assume that it&#39;s better for this whole thing to be a virtual call.
1005             if (!codeBlock || exec-&gt;argumentCountIncludingThis() &lt; static_cast&lt;size_t&gt;(codeBlock-&gt;numParameters()) || callLinkInfo.isVarargs()) {
1006                 linkVirtualFor(exec, callLinkInfo);
1007                 return;
1008             }
1009         }
1010 




























1011         callCases.append(PolymorphicCallCase(variant, codeBlock));

1012     }

1013 
1014     // If we are over the limit, just use a normal virtual call.
1015     unsigned maxPolymorphicCallVariantListSize;
1016     if (isWebAssembly)
1017         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSizeForWebAssemblyToJS();
1018     else if (callerCodeBlock-&gt;jitType() == JITCode::topTierJIT())
1019         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSizeForTopTier();
1020     else
1021         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSize();
1022 

1023     if (list.size() &gt; maxPolymorphicCallVariantListSize) {
1024         linkVirtualFor(exec, callLinkInfo);
1025         return;
1026     }
1027 
<span class="line-modified">1028     GPRReg calleeGPR = static_cast&lt;GPRReg&gt;(callLinkInfo.calleeGPR());</span>

1029 
<span class="line-modified">1030     CCallHelpers stubJit(callerCodeBlock);</span>



1031 
<span class="line-modified">1032     CCallHelpers::JumpList slowPath;</span>


1033 
1034     std::unique_ptr&lt;CallFrameShuffler&gt; frameShuffler;
1035     if (callLinkInfo.frameShuffleData()) {
1036         ASSERT(callLinkInfo.isTailCall());
<span class="line-modified">1037         frameShuffler = std::make_unique&lt;CallFrameShuffler&gt;(stubJit, *callLinkInfo.frameShuffleData());</span>
1038 #if USE(JSVALUE32_64)
1039         // We would have already checked that the callee is a cell, and we can
1040         // use the additional register this buys us.
1041         frameShuffler-&gt;assumeCalleeIsCell();
1042 #endif
1043         frameShuffler-&gt;lockGPR(calleeGPR);
1044     }
<span class="line-removed">1045     GPRReg comparisonValueGPR;</span>
1046 

1047     if (isClosureCall) {
<span class="line-removed">1048         GPRReg scratchGPR;</span>
1049         if (frameShuffler)
<span class="line-modified">1050             scratchGPR = frameShuffler-&gt;acquireGPR();</span>
1051         else
<span class="line-modified">1052             scratchGPR = AssemblyHelpers::selectScratchGPR(calleeGPR);</span>
<span class="line-removed">1053         // Verify that we have a function and stash the executable in scratchGPR.</span>
<span class="line-removed">1054 </span>
<span class="line-removed">1055 #if USE(JSVALUE64)</span>
<span class="line-removed">1056         slowPath.append(stubJit.branchIfNotCell(calleeGPR));</span>
<span class="line-removed">1057 #else</span>
<span class="line-removed">1058         // We would have already checked that the callee is a cell.</span>
<span class="line-removed">1059 #endif</span>
<span class="line-removed">1060 </span>
<span class="line-removed">1061         // FIXME: We could add a fast path for InternalFunction with closure call.</span>
<span class="line-removed">1062         slowPath.append(stubJit.branchIfNotFunction(calleeGPR));</span>
<span class="line-removed">1063 </span>
<span class="line-removed">1064         stubJit.loadPtr(</span>
<span class="line-removed">1065             CCallHelpers::Address(calleeGPR, JSFunction::offsetOfExecutable()),</span>
<span class="line-removed">1066             scratchGPR);</span>
<span class="line-removed">1067 </span>
<span class="line-removed">1068         comparisonValueGPR = scratchGPR;</span>
1069     } else
1070         comparisonValueGPR = calleeGPR;
1071 
<span class="line-removed">1072     Vector&lt;int64_t&gt; caseValues(callCases.size());</span>
<span class="line-removed">1073     Vector&lt;CallToCodePtr&gt; calls(callCases.size());</span>
<span class="line-removed">1074     UniqueArray&lt;uint32_t&gt; fastCounts;</span>
<span class="line-removed">1075 </span>
<span class="line-removed">1076     if (!isWebAssembly &amp;&amp; callerCodeBlock-&gt;jitType() != JITCode::topTierJIT())</span>
<span class="line-removed">1077         fastCounts = makeUniqueArray&lt;uint32_t&gt;(callCases.size());</span>
<span class="line-removed">1078 </span>
<span class="line-removed">1079     for (size_t i = 0; i &lt; callCases.size(); ++i) {</span>
<span class="line-removed">1080         if (fastCounts)</span>
<span class="line-removed">1081             fastCounts[i] = 0;</span>
<span class="line-removed">1082 </span>
<span class="line-removed">1083         CallVariant variant = callCases[i].variant();</span>
<span class="line-removed">1084         int64_t newCaseValue = 0;</span>
<span class="line-removed">1085         if (isClosureCall) {</span>
<span class="line-removed">1086             newCaseValue = bitwise_cast&lt;intptr_t&gt;(variant.executable());</span>
<span class="line-removed">1087             // FIXME: We could add a fast path for InternalFunction with closure call.</span>
<span class="line-removed">1088             // https://bugs.webkit.org/show_bug.cgi?id=179311</span>
<span class="line-removed">1089             if (!newCaseValue)</span>
<span class="line-removed">1090                 continue;</span>
<span class="line-removed">1091         } else {</span>
<span class="line-removed">1092             if (auto* function = variant.function())</span>
<span class="line-removed">1093                 newCaseValue = bitwise_cast&lt;intptr_t&gt;(function);</span>
<span class="line-removed">1094             else</span>
<span class="line-removed">1095                 newCaseValue = bitwise_cast&lt;intptr_t&gt;(variant.internalFunction());</span>
<span class="line-removed">1096         }</span>
<span class="line-removed">1097 </span>
<span class="line-removed">1098         if (!ASSERT_DISABLED) {</span>
<span class="line-removed">1099             for (size_t j = 0; j &lt; i; ++j) {</span>
<span class="line-removed">1100                 if (caseValues[j] != newCaseValue)</span>
<span class="line-removed">1101                     continue;</span>
<span class="line-removed">1102 </span>
<span class="line-removed">1103                 dataLog(&quot;ERROR: Attempt to add duplicate case value.\n&quot;);</span>
<span class="line-removed">1104                 dataLog(&quot;Existing case values: &quot;);</span>
<span class="line-removed">1105                 CommaPrinter comma;</span>
<span class="line-removed">1106                 for (size_t k = 0; k &lt; i; ++k)</span>
<span class="line-removed">1107                     dataLog(comma, caseValues[k]);</span>
<span class="line-removed">1108                 dataLog(&quot;\n&quot;);</span>
<span class="line-removed">1109                 dataLog(&quot;Attempting to add: &quot;, newCaseValue, &quot;\n&quot;);</span>
<span class="line-removed">1110                 dataLog(&quot;Variant list: &quot;, listDump(callCases), &quot;\n&quot;);</span>
<span class="line-removed">1111                 RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-removed">1112             }</span>
<span class="line-removed">1113         }</span>
<span class="line-removed">1114 </span>
<span class="line-removed">1115         caseValues[i] = newCaseValue;</span>
<span class="line-removed">1116     }</span>
<span class="line-removed">1117 </span>
1118     GPRReg fastCountsBaseGPR;
1119     if (frameShuffler)
1120         fastCountsBaseGPR = frameShuffler-&gt;acquireGPR();
1121     else {
1122         fastCountsBaseGPR =
1123             AssemblyHelpers::selectScratchGPR(calleeGPR, comparisonValueGPR, GPRInfo::regT3);
1124     }
1125     stubJit.move(CCallHelpers::TrustedImmPtr(fastCounts.get()), fastCountsBaseGPR);
<span class="line-modified">1126     if (!frameShuffler &amp;&amp; callLinkInfo.isTailCall())</span>



1127         stubJit.emitRestoreCalleeSaves();





















1128     BinarySwitch binarySwitch(comparisonValueGPR, caseValues, BinarySwitch::IntPtr);
1129     CCallHelpers::JumpList done;
1130     while (binarySwitch.advance(stubJit)) {
1131         size_t caseIndex = binarySwitch.caseIndex();
1132 
1133         CallVariant variant = callCases[caseIndex].variant();
1134 
1135         MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr;
1136         if (variant.executable()) {
1137             ASSERT(variant.executable()-&gt;hasJITCodeForCall());
<span class="line-modified">1138             codePtr = variant.executable()-&gt;generatedJITCodeForCall()-&gt;addressForCall(ArityCheckNotRequired);</span>



1139         } else {
1140             ASSERT(variant.internalFunction());
1141             codePtr = vm.getCTIInternalFunctionTrampolineFor(CodeForCall);
1142         }
1143 
1144         if (fastCounts) {
1145             stubJit.add32(
1146                 CCallHelpers::TrustedImm32(1),
1147                 CCallHelpers::Address(fastCountsBaseGPR, caseIndex * sizeof(uint32_t)));
1148         }
1149         if (frameShuffler) {
1150             CallFrameShuffler(stubJit, frameShuffler-&gt;snapshot()).prepareForTailCall();
1151             calls[caseIndex].call = stubJit.nearTailCall();
1152         } else if (callLinkInfo.isTailCall()) {
1153             stubJit.prepareForTailCallSlow();
1154             calls[caseIndex].call = stubJit.nearTailCall();
1155         } else
1156             calls[caseIndex].call = stubJit.nearCall();
1157         calls[caseIndex].codePtr = codePtr;
1158         done.append(stubJit.jump());
</pre>
<hr />
<pre>
1205         patchBuffer.link(done, callLinkInfo.callReturnLocation().labelAtOffset(0));
1206     else
1207         patchBuffer.link(done, callLinkInfo.hotPathOther().labelAtOffset(0));
1208     patchBuffer.link(slow, CodeLocationLabel&lt;JITThunkPtrTag&gt;(vm.getCTIStub(linkPolymorphicCallThunkGenerator).code()));
1209 
1210     auto stubRoutine = adoptRef(*new PolymorphicCallStubRoutine(
1211         FINALIZE_CODE_FOR(
1212             callerCodeBlock, patchBuffer, JITStubRoutinePtrTag,
1213             &quot;Polymorphic call stub for %s, return point %p, targets %s&quot;,
1214                 isWebAssembly ? &quot;WebAssembly&quot; : toCString(*callerCodeBlock).data(), callLinkInfo.callReturnLocation().labelAtOffset(0).executableAddress(),
1215                 toCString(listDump(callCases)).data()),
1216         vm, owner, exec-&gt;callerFrame(), callLinkInfo, callCases,
1217         WTFMove(fastCounts)));
1218 
1219     MacroAssembler::replaceWithJump(
1220         MacroAssembler::startOfBranchPtrWithPatchOnRegister(callLinkInfo.hotPathBegin()),
1221         CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(stubRoutine-&gt;code().code()));
1222     // The original slow path is unreachable on 64-bits, but still
1223     // reachable on 32-bits since a non-cell callee will always
1224     // trigger the slow path
<span class="line-modified">1225     linkSlowFor(&amp;vm, callLinkInfo);</span>
1226 
1227     // If there had been a previous stub routine, that one will die as soon as the GC runs and sees
1228     // that it&#39;s no longer on stack.
1229     callLinkInfo.setStub(WTFMove(stubRoutine));
1230 
1231     // The call link info no longer has a call cache apart from the jump to the polymorphic call
1232     // stub.
1233     if (callLinkInfo.isOnList())
1234         callLinkInfo.remove();
1235 }
1236 
1237 void resetGetByID(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo, GetByIDKind kind)
1238 {
1239     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
1240     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
1241 }
1242 
1243 void resetPutByID(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo)
1244 {
1245     V_JITOperation_ESsiJJI unoptimizedFunction = reinterpret_cast&lt;V_JITOperation_ESsiJJI&gt;(readPutICCallTarget(codeBlock, stubInfo.slowPathCallLocation()).executableAddress());
</pre>
<hr />
<pre>
1258     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), optimizedFunction);
1259     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
1260 }
1261 
1262 static void resetPatchableJump(StructureStubInfo&amp; stubInfo)
1263 {
1264     MacroAssembler::repatchJump(stubInfo.patchableJump(), stubInfo.slowPathStartLocation());
1265 }
1266 
1267 void resetInByID(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo)
1268 {
1269     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInByIdOptimize);
1270     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
1271 }
1272 
1273 void resetInstanceOf(StructureStubInfo&amp; stubInfo)
1274 {
1275     resetPatchableJump(stubInfo);
1276 }
1277 

















1278 } // namespace JSC
1279 
1280 #endif
</pre>
</td>
<td>
<hr />
<pre>
  34 #include &quot;DFGOperations.h&quot;
  35 #include &quot;DFGSpeculativeJIT.h&quot;
  36 #include &quot;DOMJITGetterSetter.h&quot;
  37 #include &quot;DirectArguments.h&quot;
  38 #include &quot;ExecutableBaseInlines.h&quot;
  39 #include &quot;FTLThunks.h&quot;
  40 #include &quot;FullCodeOrigin.h&quot;
  41 #include &quot;FunctionCodeBlock.h&quot;
  42 #include &quot;GCAwareJITStubRoutine.h&quot;
  43 #include &quot;GetterSetter.h&quot;
  44 #include &quot;GetterSetterAccessCase.h&quot;
  45 #include &quot;ICStats.h&quot;
  46 #include &quot;InlineAccess.h&quot;
  47 #include &quot;InstanceOfAccessCase.h&quot;
  48 #include &quot;IntrinsicGetterAccessCase.h&quot;
  49 #include &quot;JIT.h&quot;
  50 #include &quot;JITInlines.h&quot;
  51 #include &quot;JSCInlines.h&quot;
  52 #include &quot;JSModuleNamespaceObject.h&quot;
  53 #include &quot;JSWebAssembly.h&quot;
<span class="line-added">  54 #include &quot;JSWebAssemblyModule.h&quot;</span>
  55 #include &quot;LinkBuffer.h&quot;
  56 #include &quot;ModuleNamespaceAccessCase.h&quot;
  57 #include &quot;PolymorphicAccess.h&quot;
  58 #include &quot;ScopedArguments.h&quot;
  59 #include &quot;ScratchRegisterAllocator.h&quot;
  60 #include &quot;StackAlignment.h&quot;
  61 #include &quot;StructureRareDataInlines.h&quot;
  62 #include &quot;StructureStubClearingWatchpoint.h&quot;
  63 #include &quot;StructureStubInfo.h&quot;
  64 #include &quot;SuperSampler.h&quot;
  65 #include &quot;ThunkGenerators.h&quot;
<span class="line-added">  66 #include &quot;WebAssemblyFunction.h&quot;</span>
<span class="line-added">  67 #include &quot;WebAssemblyToJSCallee.h&quot;</span>
  68 #include &lt;wtf/CommaPrinter.h&gt;
  69 #include &lt;wtf/ListDump.h&gt;
  70 #include &lt;wtf/StringPrintStream.h&gt;
  71 
  72 namespace JSC {
  73 
  74 static FunctionPtr&lt;CFunctionPtrTag&gt; readPutICCallTarget(CodeBlock* codeBlock, CodeLocationCall&lt;JSInternalPtrTag&gt; call)
  75 {
  76     FunctionPtr&lt;OperationPtrTag&gt; target = MacroAssembler::readCallTarget&lt;OperationPtrTag&gt;(call);
  77 #if ENABLE(FTL_JIT)
<span class="line-modified">  78     if (codeBlock-&gt;jitType() == JITType::FTLJIT) {</span>
  79         MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt; thunk = MacroAssemblerCodePtr&lt;OperationPtrTag&gt;::createFromExecutableAddress(target.executableAddress()).retagged&lt;JITThunkPtrTag&gt;();
<span class="line-modified">  80         return codeBlock-&gt;vm().ftlThunks-&gt;keyForSlowPathCallThunk(thunk).callTarget().retagged&lt;CFunctionPtrTag&gt;();</span>
  81     }
  82 #else
  83     UNUSED_PARAM(codeBlock);
  84 #endif // ENABLE(FTL_JIT)
  85     return target.retagged&lt;CFunctionPtrTag&gt;();
  86 }
  87 
  88 void ftlThunkAwareRepatchCall(CodeBlock* codeBlock, CodeLocationCall&lt;JSInternalPtrTag&gt; call, FunctionPtr&lt;CFunctionPtrTag&gt; newCalleeFunction)
  89 {
  90 #if ENABLE(FTL_JIT)
<span class="line-modified">  91     if (codeBlock-&gt;jitType() == JITType::FTLJIT) {</span>
<span class="line-modified">  92         VM&amp; vm = codeBlock-&gt;vm();</span>
  93         FTL::Thunks&amp; thunks = *vm.ftlThunks;
  94         FunctionPtr&lt;OperationPtrTag&gt; target = MacroAssembler::readCallTarget&lt;OperationPtrTag&gt;(call);
  95         auto slowPathThunk = MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt;::createFromExecutableAddress(target.retaggedExecutableAddress&lt;JITThunkPtrTag&gt;());
  96         FTL::SlowPathCallKey key = thunks.keyForSlowPathCallThunk(slowPathThunk);
  97         key = key.withCallTarget(newCalleeFunction);
  98         MacroAssembler::repatchCall(call, FunctionPtr&lt;OperationPtrTag&gt;(thunks.getSlowPathCallThunk(key).retaggedCode&lt;OperationPtrTag&gt;()));
  99         return;
 100     }
 101 #else // ENABLE(FTL_JIT)
 102     UNUSED_PARAM(codeBlock);
 103 #endif // ENABLE(FTL_JIT)
 104     MacroAssembler::repatchCall(call, newCalleeFunction.retagged&lt;OperationPtrTag&gt;());
 105 }
 106 
 107 enum InlineCacheAction {
 108     GiveUpOnCache,
 109     RetryCacheLater,
 110     AttemptToCache
 111 };
 112 
</pre>
<hr />
<pre>
 253                 baseValue = jsCast&lt;JSProxy*&gt;(baseCell)-&gt;target();
 254                 baseCell = baseValue.asCell();
 255                 structure = baseCell-&gt;structure(vm);
 256                 loadTargetFromProxy = true;
 257             }
 258 
 259             InlineCacheAction action = actionForCell(vm, baseCell);
 260             if (action != AttemptToCache)
 261                 return action;
 262 
 263             // Optimize self access.
 264             if (stubInfo.cacheType == CacheType::Unset
 265                 &amp;&amp; slot.isCacheableValue()
 266                 &amp;&amp; slot.slotBase() == baseValue
 267                 &amp;&amp; !slot.watchpointSet()
 268                 &amp;&amp; !structure-&gt;needImpurePropertyWatchpoint()
 269                 &amp;&amp; !loadTargetFromProxy) {
 270 
 271                 bool generatedCodeInline = InlineAccess::generateSelfPropertyAccess(stubInfo, structure, slot.cachedOffset());
 272                 if (generatedCodeInline) {
<span class="line-modified"> 273                     LOG_IC((ICEvent::GetByIdSelfPatch, structure-&gt;classInfo(), propertyName, slot.slotBase() == baseValue));</span>
 274                     structure-&gt;startWatchingPropertyForReplacements(vm, slot.cachedOffset());
 275                     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
 276                     stubInfo.initGetByIdSelf(codeBlock, structure, slot.cachedOffset());
 277                     return RetryCacheLater;
 278                 }
 279             }
 280 
 281             std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 282 
 283             PropertyOffset offset = slot.isUnset() ? invalidOffset : slot.cachedOffset();
 284 
 285             if (slot.isUnset() || slot.slotBase() != baseValue) {
 286                 if (structure-&gt;typeInfo().prohibitsPropertyCaching())
 287                     return GiveUpOnCache;
 288 
 289                 if (structure-&gt;isDictionary()) {
 290                     if (structure-&gt;hasBeenFlattenedBefore())
 291                         return GiveUpOnCache;
 292                     structure-&gt;flattenDictionaryStructure(vm, jsCast&lt;JSObject*&gt;(baseCell));
 293                 }
</pre>
<hr />
<pre>
 362                     AccessCase::AccessType type;
 363                     if (slot.isCacheableGetter())
 364                         type = AccessCase::Getter;
 365                     else if (slot.attributes() &amp; PropertyAttribute::CustomAccessor)
 366                         type = AccessCase::CustomAccessorGetter;
 367                     else
 368                         type = AccessCase::CustomValueGetter;
 369 
 370                     if (kind == GetByIDKind::WithThis &amp;&amp; type == AccessCase::CustomAccessorGetter &amp;&amp; domAttribute)
 371                         return GiveUpOnCache;
 372 
 373                     newCase = GetterSetterAccessCase::create(
 374                         vm, codeBlock, type, offset, structure, conditionSet, loadTargetFromProxy,
 375                         slot.watchpointSet(), slot.isCacheableCustom() ? slot.customGetter() : nullptr,
 376                         slot.isCacheableCustom() &amp;&amp; slot.slotBase() != baseValue ? slot.slotBase() : nullptr,
 377                         domAttribute, WTFMove(prototypeAccessChain));
 378                 }
 379             }
 380         }
 381 
<span class="line-modified"> 382         LOG_IC((ICEvent::GetByIdAddAccessCase, baseValue.classInfoOrNull(vm), propertyName, slot.slotBase() == baseValue));</span>
 383 
 384         result = stubInfo.addAccessCase(locker, codeBlock, propertyName, WTFMove(newCase));
 385 
 386         if (result.generatedSomeCode()) {
<span class="line-modified"> 387             LOG_IC((ICEvent::GetByIdReplaceWithJump, baseValue.classInfoOrNull(vm), propertyName, slot.slotBase() == baseValue));</span>
 388 
 389             RELEASE_ASSERT(result.code());
 390             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 391         }
 392     }
 393 
 394     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec-&gt;codeBlock(), result);
 395 
 396     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 397 }
 398 
 399 void repatchGetByID(ExecState* exec, JSValue baseValue, const Identifier&amp; propertyName, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, GetByIDKind kind)
 400 {
 401     SuperSamplerScope superSamplerScope(false);
 402 
 403     if (tryCacheGetByID(exec, baseValue, propertyName, slot, stubInfo, kind) == GiveUpOnCache) {
 404         CodeBlock* codeBlock = exec-&gt;codeBlock();
 405         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateGetByIdFunction(kind));
 406     }
 407 }
</pre>
<hr />
<pre>
 461         JSCell* baseCell = baseValue.asCell();
 462 
 463         if (slot.base() == baseValue &amp;&amp; slot.isCacheablePut()) {
 464             if (slot.type() == PutPropertySlot::ExistingProperty) {
 465                 // This assert helps catch bugs if we accidentally forget to disable caching
 466                 // when we transition then store to an existing property. This is common among
 467                 // paths that reify lazy properties. If we reify a lazy property and forget
 468                 // to disable caching, we may come down this path. The Replace IC does not
 469                 // know how to model these types of structure transitions (or any structure
 470                 // transition for that matter).
 471                 RELEASE_ASSERT(baseValue.asCell()-&gt;structure(vm) == structure);
 472 
 473                 structure-&gt;didCachePropertyReplacement(vm, slot.cachedOffset());
 474 
 475                 if (stubInfo.cacheType == CacheType::Unset
 476                     &amp;&amp; InlineAccess::canGenerateSelfPropertyReplace(stubInfo, slot.cachedOffset())
 477                     &amp;&amp; !structure-&gt;needImpurePropertyWatchpoint()) {
 478 
 479                     bool generatedCodeInline = InlineAccess::generateSelfPropertyReplace(stubInfo, structure, slot.cachedOffset());
 480                     if (generatedCodeInline) {
<span class="line-modified"> 481                         LOG_IC((ICEvent::PutByIdSelfPatch, structure-&gt;classInfo(), ident, slot.base() == baseValue));</span>
 482                         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingPutByIdFunction(slot, putKind));
 483                         stubInfo.initPutByIdReplace(codeBlock, structure, slot.cachedOffset());
 484                         return RetryCacheLater;
 485                     }
 486                 }
 487 
 488                 newCase = AccessCase::create(vm, codeBlock, AccessCase::Replace, slot.cachedOffset(), structure);
 489             } else {
 490                 ASSERT(slot.type() == PutPropertySlot::NewProperty);
 491 
 492                 if (!structure-&gt;isObject())
 493                     return GiveUpOnCache;
 494 
 495                 if (structure-&gt;isDictionary()) {
 496                     if (structure-&gt;hasBeenFlattenedBefore())
 497                         return GiveUpOnCache;
 498                     structure-&gt;flattenDictionaryStructure(vm, jsCast&lt;JSObject*&gt;(baseValue));
 499                 }
 500 
 501                 PropertyOffset offset;
</pre>
<hr />
<pre>
 562                 ObjectPropertyConditionSet conditionSet;
 563                 std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 564                 PropertyOffset offset = slot.cachedOffset();
 565 
 566                 if (slot.base() != baseValue) {
 567                     bool usesPolyProto;
 568                     prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), baseCell, slot.base(), usesPolyProto);
 569                     if (!prototypeAccessChain) {
 570                         // It&#39;s invalid to access this prototype property.
 571                         return GiveUpOnCache;
 572                     }
 573 
 574                     if (!usesPolyProto) {
 575                         prototypeAccessChain = nullptr;
 576                         conditionSet =
 577                             generateConditionsForPrototypePropertyHit(
 578                                 vm, codeBlock, exec, structure, slot.base(), ident.impl());
 579                         if (!conditionSet.isValid())
 580                             return GiveUpOnCache;
 581 
<span class="line-modified"> 582                         if (!(conditionSet.slotBaseCondition().attributes() &amp; PropertyAttribute::Accessor))</span>
<span class="line-modified"> 583                             return GiveUpOnCache;</span>
<span class="line-modified"> 584 </span>
<span class="line-added"> 585                         offset = conditionSet.slotBaseCondition().offset();</span>
 586                     }
 587 
 588                 }
 589 
 590                 newCase = GetterSetterAccessCase::create(
 591                     vm, codeBlock, AccessCase::Setter, structure, offset, conditionSet, WTFMove(prototypeAccessChain));
 592             }
 593         }
 594 
<span class="line-modified"> 595         LOG_IC((ICEvent::PutByIdAddAccessCase, structure-&gt;classInfo(), ident, slot.base() == baseValue));</span>
 596 
 597         result = stubInfo.addAccessCase(locker, codeBlock, ident, WTFMove(newCase));
 598 
 599         if (result.generatedSomeCode()) {
<span class="line-modified"> 600             LOG_IC((ICEvent::PutByIdReplaceWithJump, structure-&gt;classInfo(), ident, slot.base() == baseValue));</span>
 601 
 602             RELEASE_ASSERT(result.code());
 603 
 604             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 605         }
 606     }
 607 
 608     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec-&gt;codeBlock(), result);
 609 
 610     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 611 }
 612 
 613 void repatchPutByID(ExecState* exec, JSValue baseValue, Structure* structure, const Identifier&amp; propertyName, const PutPropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, PutKind putKind)
 614 {
 615     SuperSamplerScope superSamplerScope(false);
 616 
 617     if (tryCachePutByID(exec, baseValue, structure, propertyName, slot, stubInfo, putKind) == GiveUpOnCache) {
 618         CodeBlock* codeBlock = exec-&gt;codeBlock();
 619         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateGenericPutByIdFunction(slot, putKind));
 620     }
</pre>
<hr />
<pre>
 641         }
 642 
 643         CodeBlock* codeBlock = exec-&gt;codeBlock();
 644         Structure* structure = base-&gt;structure(vm);
 645 
 646         std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 647         ObjectPropertyConditionSet conditionSet;
 648         if (wasFound) {
 649             InlineCacheAction action = actionForCell(vm, base);
 650             if (action != AttemptToCache)
 651                 return action;
 652 
 653             // Optimize self access.
 654             if (stubInfo.cacheType == CacheType::Unset
 655                 &amp;&amp; slot.isCacheableValue()
 656                 &amp;&amp; slot.slotBase() == base
 657                 &amp;&amp; !slot.watchpointSet()
 658                 &amp;&amp; !structure-&gt;needImpurePropertyWatchpoint()) {
 659                 bool generatedCodeInline = InlineAccess::generateSelfInAccess(stubInfo, structure);
 660                 if (generatedCodeInline) {
<span class="line-modified"> 661                     LOG_IC((ICEvent::InByIdSelfPatch, structure-&gt;classInfo(), ident, slot.slotBase() == base));</span>
 662                     structure-&gt;startWatchingPropertyForReplacements(vm, slot.cachedOffset());
 663                     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInByIdOptimize);
 664                     stubInfo.initInByIdSelf(codeBlock, structure, slot.cachedOffset());
 665                     return RetryCacheLater;
 666                 }
 667             }
 668 
 669             if (slot.slotBase() != base) {
 670                 bool usesPolyProto;
 671                 prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), base, slot, usesPolyProto);
 672                 if (!prototypeAccessChain) {
 673                     // It&#39;s invalid to access this prototype property.
 674                     return GiveUpOnCache;
 675                 }
 676                 if (!usesPolyProto) {
 677                     prototypeAccessChain = nullptr;
 678                     conditionSet = generateConditionsForPrototypePropertyHit(
 679                         vm, codeBlock, exec, structure, slot.slotBase(), ident.impl());
 680                 }
 681             }
 682         } else {
 683             bool usesPolyProto;
 684             prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), base, slot, usesPolyProto);
 685             if (!prototypeAccessChain) {
 686                 // It&#39;s invalid to access this prototype property.
 687                 return GiveUpOnCache;
 688             }
 689 
 690             if (!usesPolyProto) {
 691                 prototypeAccessChain = nullptr;
 692                 conditionSet = generateConditionsForPropertyMiss(
 693                     vm, codeBlock, exec, structure, ident.impl());
 694             }
 695         }
 696         if (!conditionSet.isValid())
 697             return GiveUpOnCache;
 698 
<span class="line-modified"> 699         LOG_IC((ICEvent::InAddAccessCase, structure-&gt;classInfo(), ident, slot.slotBase() == base));</span>
 700 
 701         std::unique_ptr&lt;AccessCase&gt; newCase = AccessCase::create(
 702             vm, codeBlock, wasFound ? AccessCase::InHit : AccessCase::InMiss, wasFound ? slot.cachedOffset() : invalidOffset, structure, conditionSet, WTFMove(prototypeAccessChain));
 703 
 704         result = stubInfo.addAccessCase(locker, codeBlock, ident, WTFMove(newCase));
 705 
 706         if (result.generatedSomeCode()) {
<span class="line-modified"> 707             LOG_IC((ICEvent::InReplaceWithJump, structure-&gt;classInfo(), ident, slot.slotBase() == base));</span>
 708 
 709             RELEASE_ASSERT(result.code());
 710             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 711         }
 712     }
 713 
 714     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec-&gt;codeBlock(), result);
 715 
 716     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 717 }
 718 
 719 void repatchInByID(ExecState* exec, JSObject* baseObject, const Identifier&amp; propertyName, bool wasFound, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo)
 720 {
 721     SuperSamplerScope superSamplerScope(false);
 722 
 723     if (tryCacheInByID(exec, baseObject, propertyName, wasFound, slot, stubInfo) == GiveUpOnCache) {
 724         CodeBlock* codeBlock = exec-&gt;codeBlock();
 725         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInById);
 726     }
 727 }
</pre>
<hr />
<pre>
 782             MacroAssembler::repatchJump(
 783                 stubInfo.patchableJump(),
 784                 CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 785         }
 786     }
 787 
 788     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, codeBlock, result);
 789 
 790     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 791 }
 792 
 793 void repatchInstanceOf(
 794     ExecState* exec, JSValue valueValue, JSValue prototypeValue, StructureStubInfo&amp; stubInfo,
 795     bool wasFound)
 796 {
 797     SuperSamplerScope superSamplerScope(false);
 798     if (tryCacheInstanceOf(exec, valueValue, prototypeValue, stubInfo, wasFound) == GiveUpOnCache)
 799         ftlThunkAwareRepatchCall(exec-&gt;codeBlock(), stubInfo.slowPathCallLocation(), operationInstanceOfGeneric);
 800 }
 801 
<span class="line-modified"> 802 static void linkSlowFor(VM&amp;, CallLinkInfo&amp; callLinkInfo, MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; codeRef)</span>
 803 {
 804     MacroAssembler::repatchNearCall(callLinkInfo.callReturnLocation(), CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(codeRef.code()));
 805 }
 806 
<span class="line-modified"> 807 static void linkSlowFor(VM&amp; vm, CallLinkInfo&amp; callLinkInfo, ThunkGenerator generator)</span>
 808 {
<span class="line-modified"> 809     linkSlowFor(vm, callLinkInfo, vm.getCTIStub(generator).retagged&lt;JITStubRoutinePtrTag&gt;());</span>
 810 }
 811 
<span class="line-modified"> 812 static void linkSlowFor(VM&amp; vm, CallLinkInfo&amp; callLinkInfo)</span>
 813 {
 814     MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; virtualThunk = virtualThunkFor(vm, callLinkInfo);
 815     linkSlowFor(vm, callLinkInfo, virtualThunk);
<span class="line-modified"> 816     callLinkInfo.setSlowStub(createJITStubRoutine(virtualThunk, vm, nullptr, true));</span>
 817 }
 818 
 819 static JSCell* webAssemblyOwner(JSCell* callee)
 820 {
 821 #if ENABLE(WEBASSEMBLY)
 822     // Each WebAssembly.Instance shares the stubs from their WebAssembly.Module, which are therefore the appropriate owner.
 823     return jsCast&lt;WebAssemblyToJSCallee*&gt;(callee)-&gt;module();
 824 #else
 825     UNUSED_PARAM(callee);
 826     RELEASE_ASSERT_NOT_REACHED();
 827     return nullptr;
 828 #endif // ENABLE(WEBASSEMBLY)
 829 }
 830 
 831 void linkFor(
 832     ExecState* exec, CallLinkInfo&amp; callLinkInfo, CodeBlock* calleeCodeBlock,
 833     JSObject* callee, MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr)
 834 {
 835     ASSERT(!callLinkInfo.stub());
 836 
 837     CallFrame* callerFrame = exec-&gt;callerFrame();
 838     // Our caller must have a cell for a callee. When calling
 839     // this from Wasm, we ensure the callee is a cell.
 840     ASSERT(callerFrame-&gt;callee().isCell());
 841 
 842     VM&amp; vm = callerFrame-&gt;vm();
 843     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
 844 
 845     // WebAssembly -&gt; JS stubs don&#39;t have a valid CodeBlock.
 846     JSCell* owner = isWebAssemblyToJSCallee(callerFrame-&gt;callee().asCell()) ? webAssemblyOwner(callerFrame-&gt;callee().asCell()) : callerCodeBlock;
 847     ASSERT(owner);
 848 
 849     ASSERT(!callLinkInfo.isLinked());
 850     callLinkInfo.setCallee(vm, owner, callee);
<span class="line-added"> 851     MacroAssembler::repatchPointer(callLinkInfo.hotPathBegin(), callee);</span>
 852     callLinkInfo.setLastSeenCallee(vm, owner, callee);
 853     if (shouldDumpDisassemblyFor(callerCodeBlock))
 854         dataLog(&quot;Linking call in &quot;, FullCodeOrigin(callerCodeBlock, callLinkInfo.codeOrigin()), &quot; to &quot;, pointerDump(calleeCodeBlock), &quot;, entrypoint at &quot;, codePtr, &quot;\n&quot;);
 855 
 856     MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), CodeLocationLabel&lt;JSEntryPtrTag&gt;(codePtr));
 857 
 858     if (calleeCodeBlock)
 859         calleeCodeBlock-&gt;linkIncomingCall(callerFrame, &amp;callLinkInfo);
 860 
 861     if (callLinkInfo.specializationKind() == CodeForCall &amp;&amp; callLinkInfo.allowStubs()) {
<span class="line-modified"> 862         linkSlowFor(vm, callLinkInfo, linkPolymorphicCallThunkGenerator);</span>
 863         return;
 864     }
 865 
<span class="line-modified"> 866     linkSlowFor(vm, callLinkInfo);</span>
 867 }
 868 
 869 void linkDirectFor(
 870     ExecState* exec, CallLinkInfo&amp; callLinkInfo, CodeBlock* calleeCodeBlock,
 871     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr)
 872 {
 873     ASSERT(!callLinkInfo.stub());
 874 
 875     CodeBlock* callerCodeBlock = exec-&gt;codeBlock();
 876 
<span class="line-modified"> 877     VM&amp; vm = callerCodeBlock-&gt;vm();</span>
 878 
 879     ASSERT(!callLinkInfo.isLinked());
<span class="line-modified"> 880     callLinkInfo.setCodeBlock(vm, callerCodeBlock, jsCast&lt;FunctionCodeBlock*&gt;(calleeCodeBlock));</span>
 881     if (shouldDumpDisassemblyFor(callerCodeBlock))
 882         dataLog(&quot;Linking call in &quot;, FullCodeOrigin(callerCodeBlock, callLinkInfo.codeOrigin()), &quot; to &quot;, pointerDump(calleeCodeBlock), &quot;, entrypoint at &quot;, codePtr, &quot;\n&quot;);
 883 
 884     if (callLinkInfo.callType() == CallLinkInfo::DirectTailCall)
 885         MacroAssembler::repatchJumpToNop(callLinkInfo.patchableJump());
 886     MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), CodeLocationLabel&lt;JSEntryPtrTag&gt;(codePtr));
 887 
 888     if (calleeCodeBlock)
 889         calleeCodeBlock-&gt;linkIncomingCall(exec, &amp;callLinkInfo);
 890 }
 891 
 892 void linkSlowFor(
 893     ExecState* exec, CallLinkInfo&amp; callLinkInfo)
 894 {
 895     CodeBlock* callerCodeBlock = exec-&gt;callerFrame()-&gt;codeBlock();
<span class="line-modified"> 896     VM&amp; vm = callerCodeBlock-&gt;vm();</span>
 897 
 898     linkSlowFor(vm, callLinkInfo);
 899 }
 900 
<span class="line-modified"> 901 static void revertCall(VM&amp; vm, CallLinkInfo&amp; callLinkInfo, MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; codeRef)</span>
 902 {
 903     if (callLinkInfo.isDirect()) {
 904         callLinkInfo.clearCodeBlock();
<span class="line-modified"> 905         if (!callLinkInfo.clearedByJettison()) {</span>
<span class="line-modified"> 906             if (callLinkInfo.callType() == CallLinkInfo::DirectTailCall)</span>
<span class="line-modified"> 907                 MacroAssembler::repatchJump(callLinkInfo.patchableJump(), callLinkInfo.slowPathStart());</span>
<span class="line-modified"> 908             else</span>
<span class="line-added"> 909                 MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), callLinkInfo.slowPathStart());</span>
<span class="line-added"> 910         }</span>
 911     } else {
<span class="line-modified"> 912         if (!callLinkInfo.clearedByJettison()) {</span>
<span class="line-modified"> 913             MacroAssembler::revertJumpReplacementToBranchPtrWithPatch(</span>
<span class="line-modified"> 914                 MacroAssembler::startOfBranchPtrWithPatchOnRegister(callLinkInfo.hotPathBegin()),</span>
<span class="line-modified"> 915                 callLinkInfo.calleeGPR(), 0);</span>
<span class="line-added"> 916             linkSlowFor(vm, callLinkInfo, codeRef);</span>
<span class="line-added"> 917             MacroAssembler::repatchPointer(callLinkInfo.hotPathBegin(), nullptr);</span>
<span class="line-added"> 918         }</span>
 919         callLinkInfo.clearCallee();
 920     }
 921     callLinkInfo.clearSeen();
 922     callLinkInfo.clearStub();
 923     callLinkInfo.clearSlowStub();
 924     if (callLinkInfo.isOnList())
 925         callLinkInfo.remove();
 926 }
 927 
 928 void unlinkFor(VM&amp; vm, CallLinkInfo&amp; callLinkInfo)
 929 {
 930     if (Options::dumpDisassembly())
 931         dataLog(&quot;Unlinking call at &quot;, callLinkInfo.hotPathOther(), &quot;\n&quot;);
 932 
<span class="line-modified"> 933     revertCall(vm, callLinkInfo, vm.getCTIStub(linkCallThunkGenerator).retagged&lt;JITStubRoutinePtrTag&gt;());</span>
 934 }
 935 
<span class="line-modified"> 936 static void linkVirtualFor(ExecState* exec, CallLinkInfo&amp; callLinkInfo)</span>
 937 {
 938     CallFrame* callerFrame = exec-&gt;callerFrame();
 939     VM&amp; vm = callerFrame-&gt;vm();
 940     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
 941 
 942     if (shouldDumpDisassemblyFor(callerCodeBlock))
 943         dataLog(&quot;Linking virtual call at &quot;, FullCodeOrigin(callerCodeBlock, callerFrame-&gt;codeOrigin()), &quot;\n&quot;);
 944 
<span class="line-modified"> 945     MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; virtualThunk = virtualThunkFor(vm, callLinkInfo);</span>
<span class="line-modified"> 946     revertCall(vm, callLinkInfo, virtualThunk);</span>
 947     callLinkInfo.setSlowStub(createJITStubRoutine(virtualThunk, vm, nullptr, true));
 948     callLinkInfo.setClearedByVirtual();
 949 }
 950 
 951 namespace {
 952 struct CallToCodePtr {
 953     CCallHelpers::Call call;
 954     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr;
 955 };
 956 } // annonymous namespace
 957 
 958 void linkPolymorphicCall(
 959     ExecState* exec, CallLinkInfo&amp; callLinkInfo, CallVariant newVariant)
 960 {
 961     RELEASE_ASSERT(callLinkInfo.allowStubs());
 962 
<span class="line-added"> 963     CallFrame* callerFrame = exec-&gt;callerFrame();</span>
<span class="line-added"> 964     VM&amp; vm = callerFrame-&gt;vm();</span>
<span class="line-added"> 965 </span>
<span class="line-added"> 966     // During execution of linkPolymorphicCall, we strongly assume that we never do GC.</span>
<span class="line-added"> 967     // GC jettisons CodeBlocks, changes CallLinkInfo etc. and breaks assumption done before and after this call.</span>
<span class="line-added"> 968     DeferGCForAWhile deferGCForAWhile(vm.heap);</span>
<span class="line-added"> 969 </span>
 970     if (!newVariant) {
 971         linkVirtualFor(exec, callLinkInfo);
 972         return;
 973     }
 974 


 975     // Our caller must be have a cell for a callee. When calling
 976     // this from Wasm, we ensure the callee is a cell.
 977     ASSERT(callerFrame-&gt;callee().isCell());
 978 

 979     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
 980     bool isWebAssembly = isWebAssemblyToJSCallee(callerFrame-&gt;callee().asCell());
 981 
 982     // WebAssembly -&gt; JS stubs don&#39;t have a valid CodeBlock.
 983     JSCell* owner = isWebAssembly ? webAssemblyOwner(callerFrame-&gt;callee().asCell()) : callerCodeBlock;
 984     ASSERT(owner);
 985 
 986     CallVariantList list;
 987     if (PolymorphicCallStubRoutine* stub = callLinkInfo.stub())
 988         list = stub-&gt;variants();
 989     else if (JSObject* oldCallee = callLinkInfo.callee())
<span class="line-modified"> 990         list = CallVariantList { CallVariant(oldCallee) };</span>
 991 
 992     list = variantListWithVariant(list, newVariant);
 993 
 994     // If there are any closure calls then it makes sense to treat all of them as closure calls.
 995     // This makes switching on callee cheaper. It also produces profiling that&#39;s easier on the DFG;
 996     // the DFG doesn&#39;t really want to deal with a combination of closure and non-closure callees.
 997     bool isClosureCall = false;
 998     for (CallVariant variant : list)  {
 999         if (variant.isClosureCall()) {
1000             list = despecifiedVariantList(list);
1001             isClosureCall = true;
1002             break;
1003         }
1004     }
1005 
1006     if (isClosureCall)
1007         callLinkInfo.setHasSeenClosure();
1008 
1009     Vector&lt;PolymorphicCallCase&gt; callCases;
<span class="line-added">1010     Vector&lt;int64_t&gt; caseValues;</span>
1011 
1012     // Figure out what our cases are.
1013     for (CallVariant variant : list) {
1014         CodeBlock* codeBlock = nullptr;
1015         if (variant.executable() &amp;&amp; !variant.executable()-&gt;isHostFunction()) {
1016             ExecutableBase* executable = variant.executable();
1017             codeBlock = jsCast&lt;FunctionExecutable*&gt;(executable)-&gt;codeBlockForCall();
1018             // If we cannot handle a callee, either because we don&#39;t have a CodeBlock or because arity mismatch,
1019             // assume that it&#39;s better for this whole thing to be a virtual call.
1020             if (!codeBlock || exec-&gt;argumentCountIncludingThis() &lt; static_cast&lt;size_t&gt;(codeBlock-&gt;numParameters()) || callLinkInfo.isVarargs()) {
1021                 linkVirtualFor(exec, callLinkInfo);
1022                 return;
1023             }
1024         }
1025 
<span class="line-added">1026         int64_t newCaseValue = 0;</span>
<span class="line-added">1027         if (isClosureCall) {</span>
<span class="line-added">1028             newCaseValue = bitwise_cast&lt;intptr_t&gt;(variant.executable());</span>
<span class="line-added">1029             // FIXME: We could add a fast path for InternalFunction with closure call.</span>
<span class="line-added">1030             // https://bugs.webkit.org/show_bug.cgi?id=179311</span>
<span class="line-added">1031             if (!newCaseValue)</span>
<span class="line-added">1032                 continue;</span>
<span class="line-added">1033         } else {</span>
<span class="line-added">1034             if (auto* function = variant.function())</span>
<span class="line-added">1035                 newCaseValue = bitwise_cast&lt;intptr_t&gt;(function);</span>
<span class="line-added">1036             else</span>
<span class="line-added">1037                 newCaseValue = bitwise_cast&lt;intptr_t&gt;(variant.internalFunction());</span>
<span class="line-added">1038         }</span>
<span class="line-added">1039 </span>
<span class="line-added">1040         if (!ASSERT_DISABLED) {</span>
<span class="line-added">1041             if (caseValues.contains(newCaseValue)) {</span>
<span class="line-added">1042                 dataLog(&quot;ERROR: Attempt to add duplicate case value.\n&quot;);</span>
<span class="line-added">1043                 dataLog(&quot;Existing case values: &quot;);</span>
<span class="line-added">1044                 CommaPrinter comma;</span>
<span class="line-added">1045                 for (auto&amp; value : caseValues)</span>
<span class="line-added">1046                     dataLog(comma, value);</span>
<span class="line-added">1047                 dataLog(&quot;\n&quot;);</span>
<span class="line-added">1048                 dataLog(&quot;Attempting to add: &quot;, newCaseValue, &quot;\n&quot;);</span>
<span class="line-added">1049                 dataLog(&quot;Variant list: &quot;, listDump(callCases), &quot;\n&quot;);</span>
<span class="line-added">1050                 RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added">1051             }</span>
<span class="line-added">1052         }</span>
<span class="line-added">1053 </span>
1054         callCases.append(PolymorphicCallCase(variant, codeBlock));
<span class="line-added">1055         caseValues.append(newCaseValue);</span>
1056     }
<span class="line-added">1057     ASSERT(callCases.size() == caseValues.size());</span>
1058 
1059     // If we are over the limit, just use a normal virtual call.
1060     unsigned maxPolymorphicCallVariantListSize;
1061     if (isWebAssembly)
1062         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSizeForWebAssemblyToJS();
1063     else if (callerCodeBlock-&gt;jitType() == JITCode::topTierJIT())
1064         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSizeForTopTier();
1065     else
1066         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSize();
1067 
<span class="line-added">1068     // We use list.size() instead of callCases.size() because we respect CallVariant size for now.</span>
1069     if (list.size() &gt; maxPolymorphicCallVariantListSize) {
1070         linkVirtualFor(exec, callLinkInfo);
1071         return;
1072     }
1073 
<span class="line-modified">1074     Vector&lt;CallToCodePtr&gt; calls(callCases.size());</span>
<span class="line-added">1075     UniqueArray&lt;uint32_t&gt; fastCounts;</span>
1076 
<span class="line-modified">1077     if (!isWebAssembly &amp;&amp; callerCodeBlock-&gt;jitType() != JITCode::topTierJIT()) {</span>
<span class="line-added">1078         fastCounts = makeUniqueArray&lt;uint32_t&gt;(callCases.size());</span>
<span class="line-added">1079         memset(fastCounts.get(), 0, callCases.size() * sizeof(uint32_t));</span>
<span class="line-added">1080     }</span>
1081 
<span class="line-modified">1082     GPRReg calleeGPR = callLinkInfo.calleeGPR();</span>
<span class="line-added">1083 </span>
<span class="line-added">1084     CCallHelpers stubJit(callerCodeBlock);</span>
1085 
1086     std::unique_ptr&lt;CallFrameShuffler&gt; frameShuffler;
1087     if (callLinkInfo.frameShuffleData()) {
1088         ASSERT(callLinkInfo.isTailCall());
<span class="line-modified">1089         frameShuffler = makeUnique&lt;CallFrameShuffler&gt;(stubJit, *callLinkInfo.frameShuffleData());</span>
1090 #if USE(JSVALUE32_64)
1091         // We would have already checked that the callee is a cell, and we can
1092         // use the additional register this buys us.
1093         frameShuffler-&gt;assumeCalleeIsCell();
1094 #endif
1095         frameShuffler-&gt;lockGPR(calleeGPR);
1096     }

1097 
<span class="line-added">1098     GPRReg comparisonValueGPR;</span>
1099     if (isClosureCall) {

1100         if (frameShuffler)
<span class="line-modified">1101             comparisonValueGPR = frameShuffler-&gt;acquireGPR();</span>
1102         else
<span class="line-modified">1103             comparisonValueGPR = AssemblyHelpers::selectScratchGPR(calleeGPR);</span>
















1104     } else
1105         comparisonValueGPR = calleeGPR;
1106 














































1107     GPRReg fastCountsBaseGPR;
1108     if (frameShuffler)
1109         fastCountsBaseGPR = frameShuffler-&gt;acquireGPR();
1110     else {
1111         fastCountsBaseGPR =
1112             AssemblyHelpers::selectScratchGPR(calleeGPR, comparisonValueGPR, GPRInfo::regT3);
1113     }
1114     stubJit.move(CCallHelpers::TrustedImmPtr(fastCounts.get()), fastCountsBaseGPR);
<span class="line-modified">1115 </span>
<span class="line-added">1116     if (!frameShuffler &amp;&amp; callLinkInfo.isTailCall()) {</span>
<span class="line-added">1117         // We strongly assume that calleeGPR is not a callee save register in the slow path.</span>
<span class="line-added">1118         ASSERT(!callerCodeBlock-&gt;calleeSaveRegisters()-&gt;find(calleeGPR));</span>
1119         stubJit.emitRestoreCalleeSaves();
<span class="line-added">1120     }</span>
<span class="line-added">1121 </span>
<span class="line-added">1122     CCallHelpers::JumpList slowPath;</span>
<span class="line-added">1123     if (isClosureCall) {</span>
<span class="line-added">1124         // Verify that we have a function and stash the executable in scratchGPR.</span>
<span class="line-added">1125 #if USE(JSVALUE64)</span>
<span class="line-added">1126         if (callLinkInfo.isTailCall())</span>
<span class="line-added">1127             slowPath.append(stubJit.branchIfNotCell(calleeGPR, DoNotHaveTagRegisters));</span>
<span class="line-added">1128         else</span>
<span class="line-added">1129             slowPath.append(stubJit.branchIfNotCell(calleeGPR));</span>
<span class="line-added">1130 #else</span>
<span class="line-added">1131         // We would have already checked that the callee is a cell.</span>
<span class="line-added">1132 #endif</span>
<span class="line-added">1133         // FIXME: We could add a fast path for InternalFunction with closure call.</span>
<span class="line-added">1134         slowPath.append(stubJit.branchIfNotFunction(calleeGPR));</span>
<span class="line-added">1135 </span>
<span class="line-added">1136         stubJit.loadPtr(</span>
<span class="line-added">1137             CCallHelpers::Address(calleeGPR, JSFunction::offsetOfExecutable()),</span>
<span class="line-added">1138             comparisonValueGPR);</span>
<span class="line-added">1139     }</span>
<span class="line-added">1140 </span>
1141     BinarySwitch binarySwitch(comparisonValueGPR, caseValues, BinarySwitch::IntPtr);
1142     CCallHelpers::JumpList done;
1143     while (binarySwitch.advance(stubJit)) {
1144         size_t caseIndex = binarySwitch.caseIndex();
1145 
1146         CallVariant variant = callCases[caseIndex].variant();
1147 
1148         MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr;
1149         if (variant.executable()) {
1150             ASSERT(variant.executable()-&gt;hasJITCodeForCall());
<span class="line-modified">1151 </span>
<span class="line-added">1152             codePtr = jsToWasmICCodePtr(vm, callLinkInfo.specializationKind(), variant.function());</span>
<span class="line-added">1153             if (!codePtr)</span>
<span class="line-added">1154                 codePtr = variant.executable()-&gt;generatedJITCodeForCall()-&gt;addressForCall(ArityCheckNotRequired);</span>
1155         } else {
1156             ASSERT(variant.internalFunction());
1157             codePtr = vm.getCTIInternalFunctionTrampolineFor(CodeForCall);
1158         }
1159 
1160         if (fastCounts) {
1161             stubJit.add32(
1162                 CCallHelpers::TrustedImm32(1),
1163                 CCallHelpers::Address(fastCountsBaseGPR, caseIndex * sizeof(uint32_t)));
1164         }
1165         if (frameShuffler) {
1166             CallFrameShuffler(stubJit, frameShuffler-&gt;snapshot()).prepareForTailCall();
1167             calls[caseIndex].call = stubJit.nearTailCall();
1168         } else if (callLinkInfo.isTailCall()) {
1169             stubJit.prepareForTailCallSlow();
1170             calls[caseIndex].call = stubJit.nearTailCall();
1171         } else
1172             calls[caseIndex].call = stubJit.nearCall();
1173         calls[caseIndex].codePtr = codePtr;
1174         done.append(stubJit.jump());
</pre>
<hr />
<pre>
1221         patchBuffer.link(done, callLinkInfo.callReturnLocation().labelAtOffset(0));
1222     else
1223         patchBuffer.link(done, callLinkInfo.hotPathOther().labelAtOffset(0));
1224     patchBuffer.link(slow, CodeLocationLabel&lt;JITThunkPtrTag&gt;(vm.getCTIStub(linkPolymorphicCallThunkGenerator).code()));
1225 
1226     auto stubRoutine = adoptRef(*new PolymorphicCallStubRoutine(
1227         FINALIZE_CODE_FOR(
1228             callerCodeBlock, patchBuffer, JITStubRoutinePtrTag,
1229             &quot;Polymorphic call stub for %s, return point %p, targets %s&quot;,
1230                 isWebAssembly ? &quot;WebAssembly&quot; : toCString(*callerCodeBlock).data(), callLinkInfo.callReturnLocation().labelAtOffset(0).executableAddress(),
1231                 toCString(listDump(callCases)).data()),
1232         vm, owner, exec-&gt;callerFrame(), callLinkInfo, callCases,
1233         WTFMove(fastCounts)));
1234 
1235     MacroAssembler::replaceWithJump(
1236         MacroAssembler::startOfBranchPtrWithPatchOnRegister(callLinkInfo.hotPathBegin()),
1237         CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(stubRoutine-&gt;code().code()));
1238     // The original slow path is unreachable on 64-bits, but still
1239     // reachable on 32-bits since a non-cell callee will always
1240     // trigger the slow path
<span class="line-modified">1241     linkSlowFor(vm, callLinkInfo);</span>
1242 
1243     // If there had been a previous stub routine, that one will die as soon as the GC runs and sees
1244     // that it&#39;s no longer on stack.
1245     callLinkInfo.setStub(WTFMove(stubRoutine));
1246 
1247     // The call link info no longer has a call cache apart from the jump to the polymorphic call
1248     // stub.
1249     if (callLinkInfo.isOnList())
1250         callLinkInfo.remove();
1251 }
1252 
1253 void resetGetByID(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo, GetByIDKind kind)
1254 {
1255     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
1256     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
1257 }
1258 
1259 void resetPutByID(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo)
1260 {
1261     V_JITOperation_ESsiJJI unoptimizedFunction = reinterpret_cast&lt;V_JITOperation_ESsiJJI&gt;(readPutICCallTarget(codeBlock, stubInfo.slowPathCallLocation()).executableAddress());
</pre>
<hr />
<pre>
1274     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), optimizedFunction);
1275     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
1276 }
1277 
1278 static void resetPatchableJump(StructureStubInfo&amp; stubInfo)
1279 {
1280     MacroAssembler::repatchJump(stubInfo.patchableJump(), stubInfo.slowPathStartLocation());
1281 }
1282 
1283 void resetInByID(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo)
1284 {
1285     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInByIdOptimize);
1286     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
1287 }
1288 
1289 void resetInstanceOf(StructureStubInfo&amp; stubInfo)
1290 {
1291     resetPatchableJump(stubInfo);
1292 }
1293 
<span class="line-added">1294 MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; jsToWasmICCodePtr(VM&amp; vm, CodeSpecializationKind kind, JSObject* callee)</span>
<span class="line-added">1295 {</span>
<span class="line-added">1296 #if ENABLE(WEBASSEMBLY)</span>
<span class="line-added">1297     if (!callee)</span>
<span class="line-added">1298         return nullptr;</span>
<span class="line-added">1299     if (kind != CodeForCall)</span>
<span class="line-added">1300         return nullptr;</span>
<span class="line-added">1301     if (auto* wasmFunction = jsDynamicCast&lt;WebAssemblyFunction*&gt;(vm, callee))</span>
<span class="line-added">1302         return wasmFunction-&gt;jsCallEntrypoint();</span>
<span class="line-added">1303 #else</span>
<span class="line-added">1304     UNUSED_PARAM(vm);</span>
<span class="line-added">1305     UNUSED_PARAM(kind);</span>
<span class="line-added">1306     UNUSED_PARAM(callee);</span>
<span class="line-added">1307 #endif</span>
<span class="line-added">1308     return nullptr;</span>
<span class="line-added">1309 }</span>
<span class="line-added">1310 </span>
1311 } // namespace JSC
1312 
1313 #endif
</pre>
</td>
</tr>
</table>
<center><a href="RegisterSet.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="Repatch.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>