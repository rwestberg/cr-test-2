diff a/modules/javafx.web/src/main/native/Source/WebCore/Modules/webaudio/AudioContext.cpp b/modules/javafx.web/src/main/native/Source/WebCore/Modules/webaudio/AudioContext.cpp
--- a/modules/javafx.web/src/main/native/Source/WebCore/Modules/webaudio/AudioContext.cpp
+++ b/modules/javafx.web/src/main/native/Source/WebCore/Modules/webaudio/AudioContext.cpp
@@ -93,20 +93,24 @@
 #include "Settings.h"
 #endif
 
 #include <JavaScriptCore/ArrayBuffer.h>
 #include <wtf/Atomics.h>
+#include <wtf/IsoMallocInlines.h>
 #include <wtf/MainThread.h>
 #include <wtf/Ref.h>
 #include <wtf/RefCounted.h>
+#include <wtf/Scope.h>
 #include <wtf/text/WTFString.h>
 
 const unsigned MaxPeriodicWaveLength = 4096;
 
 namespace WebCore {
 
-#define RELEASE_LOG_IF_ALLOWED(fmt, ...) RELEASE_LOG_IF(document()->page() && document()->page()->isAlwaysOnLoggingAllowed(), Media, "%p - AudioContext::" fmt, this, ##__VA_ARGS__)
+WTF_MAKE_ISO_ALLOCATED_IMPL(AudioContext);
+
+#define RELEASE_LOG_IF_ALLOWED(fmt, ...) RELEASE_LOG_IF(document() && document()->page() && document()->page()->isAlwaysOnLoggingAllowed(), Media, "%p - AudioContext::" fmt, this, ##__VA_ARGS__)
 
 bool AudioContext::isSampleRateRangeGood(float sampleRate)
 {
     // FIXME: It would be nice if the minimum sample-rate could be less than 44.1KHz,
     // but that will require some fixes in HRTFPanner::fftSizeForSampleRate(), and some testing there.
@@ -129,45 +133,57 @@
 }
 
 // Constructor for rendering to the audio hardware.
 AudioContext::AudioContext(Document& document)
     : ActiveDOMObject(document)
+#if !RELEASE_LOG_DISABLED
+    , m_logger(document.logger())
+    , m_logIdentifier(uniqueLogIdentifier())
+#endif
     , m_mediaSession(PlatformMediaSession::create(*this))
-    , m_eventQueue(std::make_unique<GenericEventQueue>(*this))
+    , m_eventQueue(makeUnique<GenericEventQueue>(*this))
 {
+    // According to spec AudioContext must die only after page navigate.
+    // Lets mark it as ActiveDOMObject with pending activity and unmark it in clear method.
+    makePendingActivity();
+
     constructCommon();
 
     m_destinationNode = DefaultAudioDestinationNode::create(*this);
 
     // Initialize the destination node's muted state to match the page's current muted state.
     pageMutedStateDidChange();
+
+    document.addAudioProducer(*this);
+    document.registerForVisibilityStateChangedCallbacks(*this);
 }
 
 // Constructor for offline (non-realtime) rendering.
 AudioContext::AudioContext(Document& document, unsigned numberOfChannels, size_t numberOfFrames, float sampleRate)
     : ActiveDOMObject(document)
+#if !RELEASE_LOG_DISABLED
+    , m_logger(document.logger())
+    , m_logIdentifier(uniqueLogIdentifier())
+#endif
     , m_isOfflineContext(true)
     , m_mediaSession(PlatformMediaSession::create(*this))
-    , m_eventQueue(std::make_unique<GenericEventQueue>(*this))
+    , m_eventQueue(makeUnique<GenericEventQueue>(*this))
 {
     constructCommon();
 
     // Create a new destination for offline rendering.
     m_renderTarget = AudioBuffer::create(numberOfChannels, numberOfFrames, sampleRate);
     m_destinationNode = OfflineAudioDestinationNode::create(*this, m_renderTarget.get());
 }
 
 void AudioContext::constructCommon()
 {
-    // According to spec AudioContext must die only after page navigate.
-    // Lets mark it as ActiveDOMObject with pending activity and unmark it in clear method.
-    setPendingActivity(*this);
-
     FFTFrame::initialize();
 
     m_listener = AudioListener::create();
 
+    ASSERT(document());
     if (document()->audioPlaybackRequiresUserGesture())
         addBehaviorRestriction(RequireUserGestureForAudioStartRestriction);
     else
         m_restrictions = NoRestrictions;
 
@@ -189,14 +205,21 @@
     ASSERT(m_automaticPullNodes.isEmpty());
     if (m_automaticPullNodesNeedUpdating)
         m_renderingAutomaticPullNodes.resize(m_automaticPullNodes.size());
     ASSERT(m_renderingAutomaticPullNodes.isEmpty());
     // FIXME: Can we assert that m_deferredFinishDerefList is empty?
+
+    if (!isOfflineContext() && scriptExecutionContext()) {
+        document()->removeAudioProducer(*this);
+        document()->unregisterForVisibilityStateChangedCallbacks(*this);
+    }
 }
 
 void AudioContext::lazyInitialize()
 {
+    ASSERT(!m_isStopScheduled);
+
     if (m_isInitialized)
         return;
 
     // Don't allow the context to initialize a second time after it's already been explicitly uninitialized.
     ASSERT(!m_isAudioThreadFinished);
@@ -205,13 +228,10 @@
 
     if (m_destinationNode) {
         m_destinationNode->initialize();
 
         if (!isOfflineContext()) {
-            document()->addAudioProducer(*this);
-            document()->registerForVisibilityStateChangedCallbacks(*this);
-
             // This starts the audio thread. The destination node's provideInput() method will now be called repeatedly to render audio.
             // Each time provideInput() is called, a portion of the audio stream is rendered. Let's call this time period a "render quantum".
             // NOTE: for now default AudioContext does not need an explicit startRendering() call from JavaScript.
             // We may want to consider requiring it for symmetry with OfflineAudioContext.
             startRendering();
@@ -221,10 +241,12 @@
     m_isInitialized = true;
 }
 
 void AudioContext::clear()
 {
+    Ref<AudioContext> protectedThis(*this);
+
     // We have to release our reference to the destination node before the context will ever be deleted since the destination node holds a reference to the context.
     if (m_destinationNode)
         m_destinationNode = nullptr;
 
     // Audio thread is dead. Nobody will schedule node deletion action. Let's do it ourselves.
@@ -232,31 +254,30 @@
         deleteMarkedNodes();
         m_nodesToDelete.appendVector(m_nodesMarkedForDeletion);
         m_nodesMarkedForDeletion.clear();
     } while (m_nodesToDelete.size());
 
-    // It was set in constructCommon.
-    unsetPendingActivity(*this);
+    clearPendingActivity();
 }
 
 void AudioContext::uninitialize()
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
 
     if (!m_isInitialized)
         return;
 
     // This stops the audio thread and all audio rendering.
-    m_destinationNode->uninitialize();
+    if (m_destinationNode)
+        m_destinationNode->uninitialize();
 
     // Don't allow the context to initialize a second time after it's already been explicitly uninitialized.
     m_isAudioThreadFinished = true;
 
     if (!isOfflineContext()) {
-        document()->removeAudioProducer(*this);
-        document()->unregisterForVisibilityStateChangedCallbacks(*this);
-
         ASSERT(s_hardwareContextCount);
         --s_hardwareContextCount;
 
         // Offline contexts move to 'Closed' state when dispatching the completion event.
         setState(State::Closed);
@@ -301,17 +322,20 @@
         promise.resolve();
 }
 
 void AudioContext::stop()
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
 
     // Usually ScriptExecutionContext calls stop twice.
     if (m_isStopScheduled)
         return;
     m_isStopScheduled = true;
 
+    ASSERT(document());
     document()->updateIsPlayingMedia();
 
     m_eventQueue->close();
 
     uninitialize();
@@ -329,11 +353,10 @@
     return "AudioContext";
 }
 
 Document* AudioContext::document() const
 {
-    ASSERT(m_scriptExecutionContext);
     return downcast<Document>(m_scriptExecutionContext);
 }
 
 Document* AudioContext::hostingDocument() const
 {
@@ -361,11 +384,11 @@
 }
 
 void AudioContext::visibilityStateChanged()
 {
     // Do not suspend if audio is audible.
-    if (mediaState() == MediaProducer::IsPlayingAudio)
+    if (!document() || mediaState() == MediaProducer::IsPlayingAudio || m_isStopScheduled)
         return;
 
     if (document()->hidden()) {
         if (state() == State::Running) {
             RELEASE_LOG_IF_ALLOWED("visibilityStateChanged() Suspending playback after going to the background");
@@ -406,18 +429,26 @@
     return audioBuffer.releaseNonNull();
 }
 
 void AudioContext::decodeAudioData(Ref<ArrayBuffer>&& audioData, RefPtr<AudioBufferCallback>&& successCallback, RefPtr<AudioBufferCallback>&& errorCallback)
 {
-    m_audioDecoder.decodeAsync(WTFMove(audioData), sampleRate(), WTFMove(successCallback), WTFMove(errorCallback));
+    if (!m_audioDecoder)
+        m_audioDecoder = makeUnique<AsyncAudioDecoder>();
+    m_audioDecoder->decodeAsync(WTFMove(audioData), sampleRate(), WTFMove(successCallback), WTFMove(errorCallback));
 }
 
-Ref<AudioBufferSourceNode> AudioContext::createBufferSource()
+ExceptionOr<Ref<AudioBufferSourceNode>> AudioContext::createBufferSource()
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
+
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     lazyInitialize();
-    Ref<AudioBufferSourceNode> node = AudioBufferSourceNode::create(*this, m_destinationNode->sampleRate());
+    Ref<AudioBufferSourceNode> node = AudioBufferSourceNode::create(*this, sampleRate());
 
     // Because this is an AudioScheduledSourceNode, the context keeps a reference until it has finished playing.
     // When this happens, AudioScheduledSourceNode::finish() calls AudioContext::notifyNodeFinishedProcessing().
     refNode(node);
 
@@ -426,32 +457,40 @@
 
 #if ENABLE(VIDEO)
 
 ExceptionOr<Ref<MediaElementAudioSourceNode>> AudioContext::createMediaElementSource(HTMLMediaElement& mediaElement)
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
-    lazyInitialize();
 
-    if (mediaElement.audioSourceNode())
+    if (m_isStopScheduled || mediaElement.audioSourceNode())
         return Exception { InvalidStateError };
 
+    lazyInitialize();
+
     auto node = MediaElementAudioSourceNode::create(*this, mediaElement);
 
     mediaElement.setAudioSourceNode(node.ptr());
 
     refNode(node.get()); // context keeps reference until node is disconnected
-    return WTFMove(node);
+    return node;
 }
 
 #endif
 
 #if ENABLE(MEDIA_STREAM)
 
 ExceptionOr<Ref<MediaStreamAudioSourceNode>> AudioContext::createMediaStreamSource(MediaStream& mediaStream)
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
 
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     auto audioTracks = mediaStream.getAudioTracks();
     if (audioTracks.isEmpty())
         return Exception { InvalidStateError };
 
     MediaStreamTrack* providerTrack = nullptr;
@@ -468,25 +507,34 @@
 
     auto node = MediaStreamAudioSourceNode::create(*this, mediaStream, *providerTrack);
     node->setFormat(2, sampleRate());
 
     refNode(node); // context keeps reference until node is disconnected
-    return WTFMove(node);
+    return node;
 }
 
-Ref<MediaStreamAudioDestinationNode> AudioContext::createMediaStreamDestination()
+ExceptionOr<Ref<MediaStreamAudioDestinationNode>> AudioContext::createMediaStreamDestination()
 {
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     // FIXME: Add support for an optional argument which specifies the number of channels.
     // FIXME: The default should probably be stereo instead of mono.
     return MediaStreamAudioDestinationNode::create(*this, 1);
 }
 
 #endif
 
 ExceptionOr<Ref<ScriptProcessorNode>> AudioContext::createScriptProcessor(size_t bufferSize, size_t numberOfInputChannels, size_t numberOfOutputChannels)
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
+
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     lazyInitialize();
 
     // W3C Editor's Draft 06 June 2017
     //  https://webaudio.github.io/web-audio-api/#widl-BaseAudioContext-createScriptProcessor-ScriptProcessorNode-unsigned-long-bufferSize-unsigned-long-numberOfInputChannels-unsigned-long-numberOfOutputChannels
 
@@ -532,109 +580,170 @@
     // up to 32 must be supported. A NotSupportedError must be thrown if the number of channels is not supported.
 
     if (numberOfOutputChannels > maxNumberOfChannels())
         return Exception { NotSupportedError };
 
-    auto node = ScriptProcessorNode::create(*this, m_destinationNode->sampleRate(), bufferSize, numberOfInputChannels, numberOfOutputChannels);
+    auto node = ScriptProcessorNode::create(*this, sampleRate(), bufferSize, numberOfInputChannels, numberOfOutputChannels);
 
     refNode(node); // context keeps reference until we stop making javascript rendering callbacks
-    return WTFMove(node);
+    return node;
 }
 
-Ref<BiquadFilterNode> AudioContext::createBiquadFilter()
+ExceptionOr<Ref<BiquadFilterNode>> AudioContext::createBiquadFilter()
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     lazyInitialize();
-    return BiquadFilterNode::create(*this, m_destinationNode->sampleRate());
+
+    return BiquadFilterNode::create(*this, sampleRate());
 }
 
-Ref<WaveShaperNode> AudioContext::createWaveShaper()
+ExceptionOr<Ref<WaveShaperNode>> AudioContext::createWaveShaper()
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     lazyInitialize();
     return WaveShaperNode::create(*this);
 }
 
-Ref<PannerNode> AudioContext::createPanner()
+ExceptionOr<Ref<PannerNode>> AudioContext::createPanner()
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     lazyInitialize();
-    return PannerNode::create(*this, m_destinationNode->sampleRate());
+    return PannerNode::create(*this, sampleRate());
 }
 
-Ref<ConvolverNode> AudioContext::createConvolver()
+ExceptionOr<Ref<ConvolverNode>> AudioContext::createConvolver()
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     lazyInitialize();
-    return ConvolverNode::create(*this, m_destinationNode->sampleRate());
+    return ConvolverNode::create(*this, sampleRate());
 }
 
-Ref<DynamicsCompressorNode> AudioContext::createDynamicsCompressor()
+ExceptionOr<Ref<DynamicsCompressorNode>> AudioContext::createDynamicsCompressor()
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     lazyInitialize();
-    return DynamicsCompressorNode::create(*this, m_destinationNode->sampleRate());
+    return DynamicsCompressorNode::create(*this, sampleRate());
 }
 
-Ref<AnalyserNode> AudioContext::createAnalyser()
+ExceptionOr<Ref<AnalyserNode>> AudioContext::createAnalyser()
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     lazyInitialize();
-    return AnalyserNode::create(*this, m_destinationNode->sampleRate());
+    return AnalyserNode::create(*this, sampleRate());
 }
 
-Ref<GainNode> AudioContext::createGain()
+ExceptionOr<Ref<GainNode>> AudioContext::createGain()
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     lazyInitialize();
-    return GainNode::create(*this, m_destinationNode->sampleRate());
+    return GainNode::create(*this, sampleRate());
 }
 
 ExceptionOr<Ref<DelayNode>> AudioContext::createDelay(double maxDelayTime)
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     lazyInitialize();
-    return DelayNode::create(*this, m_destinationNode->sampleRate(), maxDelayTime);
+    return DelayNode::create(*this, sampleRate(), maxDelayTime);
 }
 
 ExceptionOr<Ref<ChannelSplitterNode>> AudioContext::createChannelSplitter(size_t numberOfOutputs)
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     lazyInitialize();
-    auto node = ChannelSplitterNode::create(*this, m_destinationNode->sampleRate(), numberOfOutputs);
+    auto node = ChannelSplitterNode::create(*this, sampleRate(), numberOfOutputs);
     if (!node)
         return Exception { IndexSizeError };
     return node.releaseNonNull();
 }
 
 ExceptionOr<Ref<ChannelMergerNode>> AudioContext::createChannelMerger(size_t numberOfInputs)
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     lazyInitialize();
-    auto node = ChannelMergerNode::create(*this, m_destinationNode->sampleRate(), numberOfInputs);
+    auto node = ChannelMergerNode::create(*this, sampleRate(), numberOfInputs);
     if (!node)
         return Exception { IndexSizeError };
     return node.releaseNonNull();
 }
 
-Ref<OscillatorNode> AudioContext::createOscillator()
+ExceptionOr<Ref<OscillatorNode>> AudioContext::createOscillator()
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     lazyInitialize();
 
-    Ref<OscillatorNode> node = OscillatorNode::create(*this, m_destinationNode->sampleRate());
+    Ref<OscillatorNode> node = OscillatorNode::create(*this, sampleRate());
 
     // Because this is an AudioScheduledSourceNode, the context keeps a reference until it has finished playing.
     // When this happens, AudioScheduledSourceNode::finish() calls AudioContext::notifyNodeFinishedProcessing().
     refNode(node);
 
     return node;
 }
 
 ExceptionOr<Ref<PeriodicWave>> AudioContext::createPeriodicWave(Float32Array& real, Float32Array& imaginary)
 {
+    ALWAYS_LOG(LOGIDENTIFIER);
+
     ASSERT(isMainThread());
+    if (m_isStopScheduled)
+        return Exception { InvalidStateError };
+
     if (real.length() != imaginary.length() || (real.length() > MaxPeriodicWaveLength) || !real.length())
         return Exception { IndexSizeError };
     lazyInitialize();
     return PeriodicWave::create(sampleRate(), real, imaginary);
 }
@@ -960,11 +1069,11 @@
         node->processIfNecessary(framesToProcess);
 }
 
 ScriptExecutionContext* AudioContext::scriptExecutionContext() const
 {
-    return m_isStopScheduled ? 0 : ActiveDOMObject::scriptExecutionContext();
+    return ActiveDOMObject::scriptExecutionContext();
 }
 
 void AudioContext::nodeWillBeginPlayback()
 {
     // Called by scheduled AudioNodes when clients schedule their start times.
@@ -978,30 +1087,42 @@
         startRendering();
 }
 
 bool AudioContext::willBeginPlayback()
 {
+    if (!document())
+        return false;
+
     if (userGestureRequiredForAudioStart()) {
-        if (!processingUserGestureForMedia() && !document()->isCapturing())
+        if (!processingUserGestureForMedia() && !document()->isCapturing()) {
+            ALWAYS_LOG(LOGIDENTIFIER, "returning false, not processing user gesture or capturing");
             return false;
+        }
         removeBehaviorRestriction(AudioContext::RequireUserGestureForAudioStartRestriction);
     }
 
     if (pageConsentRequiredForAudioStart()) {
         Page* page = document()->page();
         if (page && !page->canStartMedia()) {
             document()->addMediaCanStartListener(*this);
+            ALWAYS_LOG(LOGIDENTIFIER, "returning false, page doesn't allow media to start");
             return false;
         }
         removeBehaviorRestriction(AudioContext::RequirePageConsentForAudioStartRestriction);
     }
 
-    return m_mediaSession->clientWillBeginPlayback();
+    auto willBegin = m_mediaSession->clientWillBeginPlayback();
+    ALWAYS_LOG(LOGIDENTIFIER, "returning ", willBegin);
+
+    return willBegin;
 }
 
 bool AudioContext::willPausePlayback()
 {
+    if (!document())
+        return false;
+
     if (userGestureRequiredForAudioStart()) {
         if (!processingUserGestureForMedia())
             return false;
         removeBehaviorRestriction(AudioContext::RequireUserGestureForAudioStartRestriction);
     }
@@ -1018,13 +1139,16 @@
     return m_mediaSession->clientWillPausePlayback();
 }
 
 void AudioContext::startRendering()
 {
-    if (!willBeginPlayback())
+    ALWAYS_LOG(LOGIDENTIFIER);
+    if (m_isStopScheduled || !willBeginPlayback())
         return;
 
+    makePendingActivity();
+
     destination()->startRendering();
     setState(State::Running);
 }
 
 void AudioContext::mediaCanStart(Document& document)
@@ -1042,11 +1166,11 @@
     return MediaProducer::IsNotPlaying;
 }
 
 void AudioContext::pageMutedStateDidChange()
 {
-    if (m_destinationNode && document()->page())
+    if (m_destinationNode && document() && document()->page())
         m_destinationNode->setMuted(document()->page()->isAudioMuted());
 }
 
 void AudioContext::isPlayingAudioDidChange()
 {
@@ -1056,28 +1180,47 @@
         if (protectedThis->document())
             protectedThis->document()->updateIsPlayingMedia();
     });
 }
 
-void AudioContext::fireCompletionEvent()
+void AudioContext::finishedRendering(bool didRendering)
 {
+    ASSERT(isOfflineContext());
     ASSERT(isMainThread());
     if (!isMainThread())
         return;
 
+    auto clearPendingActivityIfExitEarly = WTF::makeScopeExit([this] {
+        clearPendingActivity();
+    });
+
+
+    ALWAYS_LOG(LOGIDENTIFIER);
+
+    if (!didRendering)
+        return;
+
     AudioBuffer* renderedBuffer = m_renderTarget.get();
     setState(State::Closed);
 
     ASSERT(renderedBuffer);
     if (!renderedBuffer)
         return;
 
     // Avoid firing the event if the document has already gone away.
-    if (scriptExecutionContext()) {
-        // Call the offline rendering completion event listener.
-        m_eventQueue->enqueueEvent(OfflineAudioCompletionEvent::create(renderedBuffer));
-    }
+    if (m_isStopScheduled)
+        return;
+
+    clearPendingActivityIfExitEarly.release();
+    m_eventQueue->enqueueEvent(OfflineAudioCompletionEvent::create(renderedBuffer));
+}
+
+void AudioContext::dispatchEvent(Event& event)
+{
+    EventTarget::dispatchEvent(event);
+    if (event.eventInterface() == OfflineAudioCompletionEventInterfaceType)
+        clearPendingActivity();
 }
 
 void AudioContext::incrementActiveSourceCount()
 {
     ++m_activeSourceCount;
@@ -1088,11 +1231,11 @@
     --m_activeSourceCount;
 }
 
 void AudioContext::suspend(DOMPromiseDeferred<void>&& promise)
 {
-    if (isOfflineContext()) {
+    if (isOfflineContext() || m_isStopScheduled) {
         promise.reject(InvalidStateError);
         return;
     }
 
     if (m_state == State::Suspended) {
@@ -1117,11 +1260,11 @@
     });
 }
 
 void AudioContext::resume(DOMPromiseDeferred<void>&& promise)
 {
-    if (isOfflineContext()) {
+    if (isOfflineContext() || m_isStopScheduled) {
         promise.reject(InvalidStateError);
         return;
     }
 
     if (m_state == State::Running) {
@@ -1146,11 +1289,11 @@
     });
 }
 
 void AudioContext::close(DOMPromiseDeferred<void>&& promise)
 {
-    if (isOfflineContext()) {
+    if (isOfflineContext() || m_isStopScheduled) {
         promise.reject(InvalidStateError);
         return;
     }
 
     if (m_state == State::Closed || !m_destinationNode) {
@@ -1206,9 +1349,51 @@
     m_destinationNode->resume([this, protectedThis = makeRef(*this)] {
         setState(State::Running);
     });
 }
 
+void AudioContext::postTask(WTF::Function<void()>&& task)
+{
+    if (m_isStopScheduled)
+        return;
+
+    m_scriptExecutionContext->postTask(WTFMove(task));
+}
+
+const SecurityOrigin* AudioContext::origin() const
+{
+    return m_scriptExecutionContext ? m_scriptExecutionContext->securityOrigin() : nullptr;
+}
+
+void AudioContext::addConsoleMessage(MessageSource source, MessageLevel level, const String& message)
+{
+    if (m_scriptExecutionContext)
+        m_scriptExecutionContext->addConsoleMessage(source, level, message);
+}
+
+void AudioContext::clearPendingActivity()
+{
+    if (!m_pendingActivity)
+        return;
+    m_pendingActivity = nullptr;
+    // FIXME: Remove this specific deref() and ref() call in makePendingActivity().
+    deref();
+}
+
+void AudioContext::makePendingActivity()
+{
+    if (m_pendingActivity)
+        return;
+    m_pendingActivity = ActiveDOMObject::makePendingActivity(*this);
+    ref();
+}
+
+#if !RELEASE_LOG_DISABLED
+WTFLogChannel& AudioContext::logChannel() const
+{
+    return LogMedia;
+}
+#endif
 
 } // namespace WebCore
 
 #endif // ENABLE(WEB_AUDIO)
