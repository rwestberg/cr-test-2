<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITOpcodes.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2009-2019 Apple Inc. All rights reserved.
   3  * Copyright (C) 2010 Patrick Gansterer &lt;paroga@paroga.com&gt;
   4  *
   5  * Redistribution and use in source and binary forms, with or without
   6  * modification, are permitted provided that the following conditions
   7  * are met:
   8  * 1. Redistributions of source code must retain the above copyright
   9  *    notice, this list of conditions and the following disclaimer.
  10  * 2. Redistributions in binary form must reproduce the above copyright
  11  *    notice, this list of conditions and the following disclaimer in the
  12  *    documentation and/or other materials provided with the distribution.
  13  *
  14  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  15  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  16  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  17  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  18  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  19  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  20  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  21  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  22  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  23  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  24  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  25  */
  26 
  27 #include &quot;config.h&quot;
  28 #if ENABLE(JIT)
  29 #include &quot;JIT.h&quot;
  30 
  31 #include &quot;BasicBlockLocation.h&quot;
  32 #include &quot;BytecodeStructs.h&quot;
  33 #include &quot;Exception.h&quot;
  34 #include &quot;Heap.h&quot;
  35 #include &quot;InterpreterInlines.h&quot;
  36 #include &quot;JITInlines.h&quot;
  37 #include &quot;JSArray.h&quot;
  38 #include &quot;JSCast.h&quot;
  39 #include &quot;JSFunction.h&quot;
  40 #include &quot;JSPropertyNameEnumerator.h&quot;
  41 #include &quot;LinkBuffer.h&quot;
  42 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;
  43 #include &quot;OpcodeInlines.h&quot;
  44 #include &quot;SlowPathCall.h&quot;
  45 #include &quot;SuperSampler.h&quot;
  46 #include &quot;ThunkGenerators.h&quot;
  47 #include &quot;TypeLocation.h&quot;
  48 #include &quot;TypeProfilerLog.h&quot;
  49 #include &quot;VirtualRegister.h&quot;
  50 #include &quot;Watchdog.h&quot;
  51 
  52 namespace JSC {
  53 
  54 #if USE(JSVALUE64)
  55 
  56 void JIT::emit_op_mov(const Instruction* currentInstruction)
  57 {
  58     auto bytecode = currentInstruction-&gt;as&lt;OpMov&gt;();
  59     int dst = bytecode.m_dst.offset();
  60     int src = bytecode.m_src.offset();
  61 
  62     if (m_codeBlock-&gt;isConstantRegisterIndex(src)) {
  63         JSValue value = m_codeBlock-&gt;getConstant(src);
  64         if (!value.isNumber())
  65             store64(TrustedImm64(JSValue::encode(value)), addressFor(dst));
  66         else
  67             store64(Imm64(JSValue::encode(value)), addressFor(dst));
  68         return;
  69     }
  70 
  71     load64(addressFor(src), regT0);
  72     store64(regT0, addressFor(dst));
  73 }
  74 
  75 
  76 void JIT::emit_op_end(const Instruction* currentInstruction)
  77 {
  78     auto bytecode = currentInstruction-&gt;as&lt;OpEnd&gt;();
  79     RELEASE_ASSERT(returnValueGPR != callFrameRegister);
  80     emitGetVirtualRegister(bytecode.m_value.offset(), returnValueGPR);
  81     emitRestoreCalleeSaves();
  82     emitFunctionEpilogue();
  83     ret();
  84 }
  85 
  86 void JIT::emit_op_jmp(const Instruction* currentInstruction)
  87 {
  88     auto bytecode = currentInstruction-&gt;as&lt;OpJmp&gt;();
  89     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
  90     addJump(jump(), target);
  91 }
  92 
  93 void JIT::emit_op_new_object(const Instruction* currentInstruction)
  94 {
  95     auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
  96     auto&amp; metadata = bytecode.metadata(m_codeBlock);
  97     Structure* structure = metadata.m_objectAllocationProfile.structure();
  98     size_t allocationSize = JSFinalObject::allocationSize(structure-&gt;inlineCapacity());
  99     Allocator allocator = allocatorForNonVirtualConcurrently&lt;JSFinalObject&gt;(*m_vm, allocationSize, AllocatorForMode::AllocatorIfExists);
 100 
 101     RegisterID resultReg = regT0;
 102     RegisterID allocatorReg = regT1;
 103     RegisterID scratchReg = regT2;
 104 
 105     if (!allocator)
 106         addSlowCase(jump());
 107     else {
 108         JumpList slowCases;
 109         auto butterfly = TrustedImmPtr(nullptr);
 110         emitAllocateJSObject(resultReg, JITAllocator::constant(allocator), allocatorReg, TrustedImmPtr(structure), butterfly, scratchReg, slowCases);
 111         emitInitializeInlineStorage(resultReg, structure-&gt;inlineCapacity());
 112         addSlowCase(slowCases);
 113         emitPutVirtualRegister(bytecode.m_dst.offset());
 114     }
 115 }
 116 
 117 void JIT::emitSlow_op_new_object(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 118 {
 119     linkAllSlowCases(iter);
 120 
 121     auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
 122     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 123     int dst = bytecode.m_dst.offset();
 124     Structure* structure = metadata.m_objectAllocationProfile.structure();
 125     callOperation(operationNewObject, structure);
 126     emitStoreCell(dst, returnValueGPR);
 127 }
 128 
 129 void JIT::emit_op_overrides_has_instance(const Instruction* currentInstruction)
 130 {
 131     auto bytecode = currentInstruction-&gt;as&lt;OpOverridesHasInstance&gt;();
 132     int dst = bytecode.m_dst.offset();
 133     int constructor = bytecode.m_constructor.offset();
 134     int hasInstanceValue = bytecode.m_hasInstanceValue.offset();
 135 
 136     emitGetVirtualRegister(hasInstanceValue, regT0);
 137 
 138     // We don&#39;t jump if we know what Symbol.hasInstance would do.
 139     Jump customhasInstanceValue = branchPtr(NotEqual, regT0, TrustedImmPtr(m_codeBlock-&gt;globalObject()-&gt;functionProtoHasInstanceSymbolFunction()));
 140 
 141     emitGetVirtualRegister(constructor, regT0);
 142 
 143     // Check that constructor &#39;ImplementsDefaultHasInstance&#39; i.e. the object is not a C-API user nor a bound function.
 144     test8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(ImplementsDefaultHasInstance), regT0);
 145     boxBoolean(regT0, JSValueRegs { regT0 });
 146     Jump done = jump();
 147 
 148     customhasInstanceValue.link(this);
 149     move(TrustedImm32(ValueTrue), regT0);
 150 
 151     done.link(this);
 152     emitPutVirtualRegister(dst);
 153 }
 154 
 155 void JIT::emit_op_instanceof(const Instruction* currentInstruction)
 156 {
 157     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceof&gt;();
 158     int dst = bytecode.m_dst.offset();
 159     int value = bytecode.m_value.offset();
 160     int proto = bytecode.m_prototype.offset();
 161 
 162     // Load the operands (baseVal, proto, and value respectively) into registers.
 163     // We use regT0 for baseVal since we will be done with this first, and we can then use it for the result.
 164     emitGetVirtualRegister(value, regT2);
 165     emitGetVirtualRegister(proto, regT1);
 166 
 167     // Check that proto are cells. baseVal must be a cell - this is checked by the get_by_id for Symbol.hasInstance.
 168     emitJumpSlowCaseIfNotJSCell(regT2, value);
 169     emitJumpSlowCaseIfNotJSCell(regT1, proto);
 170 
 171     JITInstanceOfGenerator gen(
 172         m_codeBlock, CodeOrigin(m_bytecodeOffset), CallSiteIndex(m_bytecodeOffset),
 173         RegisterSet::stubUnavailableRegisters(),
 174         regT0, // result
 175         regT2, // value
 176         regT1, // proto
 177         regT3, regT4); // scratch
 178     gen.generateFastPath(*this);
 179     m_instanceOfs.append(gen);
 180 
 181     emitPutVirtualRegister(dst);
 182 }
 183 
 184 void JIT::emitSlow_op_instanceof(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 185 {
 186     linkAllSlowCases(iter);
 187 
 188     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceof&gt;();
 189     int resultVReg = bytecode.m_dst.offset();
 190 
 191     JITInstanceOfGenerator&amp; gen = m_instanceOfs[m_instanceOfIndex++];
 192 
 193     Label coldPathBegin = label();
 194     Call call = callOperation(operationInstanceOfOptimize, resultVReg, gen.stubInfo(), regT2, regT1);
 195     gen.reportSlowPathCall(coldPathBegin, call);
 196 }
 197 
 198 void JIT::emit_op_instanceof_custom(const Instruction*)
 199 {
 200     // This always goes to slow path since we expect it to be rare.
 201     addSlowCase(jump());
 202 }
 203 
 204 void JIT::emit_op_is_empty(const Instruction* currentInstruction)
 205 {
 206     auto bytecode = currentInstruction-&gt;as&lt;OpIsEmpty&gt;();
 207     int dst = bytecode.m_dst.offset();
 208     int value = bytecode.m_operand.offset();
 209 
 210     emitGetVirtualRegister(value, regT0);
 211     compare64(Equal, regT0, TrustedImm32(JSValue::encode(JSValue())), regT0);
 212 
 213     boxBoolean(regT0, JSValueRegs { regT0 });
 214     emitPutVirtualRegister(dst);
 215 }
 216 
 217 void JIT::emit_op_is_undefined(const Instruction* currentInstruction)
 218 {
 219     auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefined&gt;();
 220     int dst = bytecode.m_dst.offset();
 221     int value = bytecode.m_operand.offset();
 222 
 223     emitGetVirtualRegister(value, regT0);
 224     Jump isCell = branchIfCell(regT0);
 225 
 226     compare64(Equal, regT0, TrustedImm32(ValueUndefined), regT0);
 227     Jump done = jump();
 228 
 229     isCell.link(this);
 230     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 231     move(TrustedImm32(0), regT0);
 232     Jump notMasqueradesAsUndefined = jump();
 233 
 234     isMasqueradesAsUndefined.link(this);
 235     emitLoadStructure(*vm(), regT0, regT1, regT2);
 236     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 237     loadPtr(Address(regT1, Structure::globalObjectOffset()), regT1);
 238     comparePtr(Equal, regT0, regT1, regT0);
 239 
 240     notMasqueradesAsUndefined.link(this);
 241     done.link(this);
 242     boxBoolean(regT0, JSValueRegs { regT0 });
 243     emitPutVirtualRegister(dst);
 244 }
 245 
 246 void JIT::emit_op_is_undefined_or_null(const Instruction* currentInstruction)
 247 {
 248     auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefinedOrNull&gt;();
 249     int dst = bytecode.m_dst.offset();
 250     int value = bytecode.m_operand.offset();
 251 
 252     emitGetVirtualRegister(value, regT0);
 253 
 254     and64(TrustedImm32(~TagBitUndefined), regT0);
 255     compare64(Equal, regT0, TrustedImm32(ValueNull), regT0);
 256 
 257     boxBoolean(regT0, JSValueRegs { regT0 });
 258     emitPutVirtualRegister(dst);
 259 }
 260 
 261 void JIT::emit_op_is_boolean(const Instruction* currentInstruction)
 262 {
 263     auto bytecode = currentInstruction-&gt;as&lt;OpIsBoolean&gt;();
 264     int dst = bytecode.m_dst.offset();
 265     int value = bytecode.m_operand.offset();
 266 
 267     emitGetVirtualRegister(value, regT0);
 268     xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueFalse)), regT0);
 269     test64(Zero, regT0, TrustedImm32(static_cast&lt;int32_t&gt;(~1)), regT0);
 270     boxBoolean(regT0, JSValueRegs { regT0 });
 271     emitPutVirtualRegister(dst);
 272 }
 273 
 274 void JIT::emit_op_is_number(const Instruction* currentInstruction)
 275 {
 276     auto bytecode = currentInstruction-&gt;as&lt;OpIsNumber&gt;();
 277     int dst = bytecode.m_dst.offset();
 278     int value = bytecode.m_operand.offset();
 279 
 280     emitGetVirtualRegister(value, regT0);
 281     test64(NonZero, regT0, tagTypeNumberRegister, regT0);
 282     boxBoolean(regT0, JSValueRegs { regT0 });
 283     emitPutVirtualRegister(dst);
 284 }
 285 
 286 void JIT::emit_op_is_cell_with_type(const Instruction* currentInstruction)
 287 {
 288     auto bytecode = currentInstruction-&gt;as&lt;OpIsCellWithType&gt;();
 289     int dst = bytecode.m_dst.offset();
 290     int value = bytecode.m_operand.offset();
 291     int type = bytecode.m_type;
 292 
 293     emitGetVirtualRegister(value, regT0);
 294     Jump isNotCell = branchIfNotCell(regT0);
 295 
 296     compare8(Equal, Address(regT0, JSCell::typeInfoTypeOffset()), TrustedImm32(type), regT0);
 297     boxBoolean(regT0, JSValueRegs { regT0 });
 298     Jump done = jump();
 299 
 300     isNotCell.link(this);
 301     move(TrustedImm32(ValueFalse), regT0);
 302 
 303     done.link(this);
 304     emitPutVirtualRegister(dst);
 305 }
 306 
 307 void JIT::emit_op_is_object(const Instruction* currentInstruction)
 308 {
 309     auto bytecode = currentInstruction-&gt;as&lt;OpIsObject&gt;();
 310     int dst = bytecode.m_dst.offset();
 311     int value = bytecode.m_operand.offset();
 312 
 313     emitGetVirtualRegister(value, regT0);
 314     Jump isNotCell = branchIfNotCell(regT0);
 315 
 316     compare8(AboveOrEqual, Address(regT0, JSCell::typeInfoTypeOffset()), TrustedImm32(ObjectType), regT0);
 317     boxBoolean(regT0, JSValueRegs { regT0 });
 318     Jump done = jump();
 319 
 320     isNotCell.link(this);
 321     move(TrustedImm32(ValueFalse), regT0);
 322 
 323     done.link(this);
 324     emitPutVirtualRegister(dst);
 325 }
 326 
 327 void JIT::emit_op_ret(const Instruction* currentInstruction)
 328 {
 329     ASSERT(callFrameRegister != regT1);
 330     ASSERT(regT1 != returnValueGPR);
 331     ASSERT(returnValueGPR != callFrameRegister);
 332 
 333     // Return the result in %eax.
 334     auto bytecode = currentInstruction-&gt;as&lt;OpRet&gt;();
 335     emitGetVirtualRegister(bytecode.m_value.offset(), returnValueGPR);
 336 
 337     checkStackPointerAlignment();
 338     emitRestoreCalleeSaves();
 339     emitFunctionEpilogue();
 340     ret();
 341 }
 342 
 343 void JIT::emit_op_to_primitive(const Instruction* currentInstruction)
 344 {
 345     auto bytecode = currentInstruction-&gt;as&lt;OpToPrimitive&gt;();
 346     int dst = bytecode.m_dst.offset();
 347     int src = bytecode.m_src.offset();
 348 
 349     emitGetVirtualRegister(src, regT0);
 350 
 351     Jump isImm = branchIfNotCell(regT0);
 352     addSlowCase(branchIfObject(regT0));
 353     isImm.link(this);
 354 
 355     if (dst != src)
 356         emitPutVirtualRegister(dst);
 357 
 358 }
 359 
 360 void JIT::emit_op_set_function_name(const Instruction* currentInstruction)
 361 {
 362     auto bytecode = currentInstruction-&gt;as&lt;OpSetFunctionName&gt;();
 363     emitGetVirtualRegister(bytecode.m_function.offset(), regT0);
 364     emitGetVirtualRegister(bytecode.m_name.offset(), regT1);
 365     callOperation(operationSetFunctionName, regT0, regT1);
 366 }
 367 
 368 void JIT::emit_op_not(const Instruction* currentInstruction)
 369 {
 370     auto bytecode = currentInstruction-&gt;as&lt;OpNot&gt;();
 371     emitGetVirtualRegister(bytecode.m_operand.offset(), regT0);
 372 
 373     // Invert against JSValue(false); if the value was tagged as a boolean, then all bits will be
 374     // clear other than the low bit (which will be 0 or 1 for false or true inputs respectively).
 375     // Then invert against JSValue(true), which will add the tag back in, and flip the low bit.
 376     xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueFalse)), regT0);
 377     addSlowCase(branchTestPtr(NonZero, regT0, TrustedImm32(static_cast&lt;int32_t&gt;(~1))));
 378     xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueTrue)), regT0);
 379 
 380     emitPutVirtualRegister(bytecode.m_dst.offset());
 381 }
 382 
 383 void JIT::emit_op_jfalse(const Instruction* currentInstruction)
 384 {
 385     auto bytecode = currentInstruction-&gt;as&lt;OpJfalse&gt;();
 386     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 387 
 388     GPRReg value = regT0;
 389     GPRReg scratch1 = regT1;
 390     GPRReg scratch2 = regT2;
 391     bool shouldCheckMasqueradesAsUndefined = true;
 392 
 393     emitGetVirtualRegister(bytecode.m_condition.offset(), value);
 394     addJump(branchIfFalsey(*vm(), JSValueRegs(value), scratch1, scratch2, fpRegT0, fpRegT1, shouldCheckMasqueradesAsUndefined, m_codeBlock-&gt;globalObject()), target);
 395 }
 396 
 397 void JIT::emit_op_jeq_null(const Instruction* currentInstruction)
 398 {
 399     auto bytecode = currentInstruction-&gt;as&lt;OpJeqNull&gt;();
 400     int src = bytecode.m_value.offset();
 401     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 402 
 403     emitGetVirtualRegister(src, regT0);
 404     Jump isImmediate = branchIfNotCell(regT0);
 405 
 406     // First, handle JSCell cases - check MasqueradesAsUndefined bit on the structure.
 407     Jump isNotMasqueradesAsUndefined = branchTest8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 408     emitLoadStructure(*vm(), regT0, regT2, regT1);
 409     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 410     addJump(branchPtr(Equal, Address(regT2, Structure::globalObjectOffset()), regT0), target);
 411     Jump masqueradesGlobalObjectIsForeign = jump();
 412 
 413     // Now handle the immediate cases - undefined &amp; null
 414     isImmediate.link(this);
 415     and64(TrustedImm32(~TagBitUndefined), regT0);
 416     addJump(branch64(Equal, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 417 
 418     isNotMasqueradesAsUndefined.link(this);
 419     masqueradesGlobalObjectIsForeign.link(this);
 420 };
 421 void JIT::emit_op_jneq_null(const Instruction* currentInstruction)
 422 {
 423     auto bytecode = currentInstruction-&gt;as&lt;OpJneqNull&gt;();
 424     int src = bytecode.m_value.offset();
 425     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 426 
 427     emitGetVirtualRegister(src, regT0);
 428     Jump isImmediate = branchIfNotCell(regT0);
 429 
 430     // First, handle JSCell cases - check MasqueradesAsUndefined bit on the structure.
 431     addJump(branchTest8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined)), target);
 432     emitLoadStructure(*vm(), regT0, regT2, regT1);
 433     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 434     addJump(branchPtr(NotEqual, Address(regT2, Structure::globalObjectOffset()), regT0), target);
 435     Jump wasNotImmediate = jump();
 436 
 437     // Now handle the immediate cases - undefined &amp; null
 438     isImmediate.link(this);
 439     and64(TrustedImm32(~TagBitUndefined), regT0);
 440     addJump(branch64(NotEqual, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 441 
 442     wasNotImmediate.link(this);
 443 }
 444 
 445 void JIT::emit_op_jneq_ptr(const Instruction* currentInstruction)
 446 {
 447     auto bytecode = currentInstruction-&gt;as&lt;OpJneqPtr&gt;();
 448     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 449     int src = bytecode.m_value.offset();
 450     Special::Pointer ptr = bytecode.m_specialPointer;
 451     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 452 
 453     emitGetVirtualRegister(src, regT0);
 454     CCallHelpers::Jump equal = branchPtr(Equal, regT0, TrustedImmPtr(actualPointerFor(m_codeBlock, ptr)));
 455     store8(TrustedImm32(1), &amp;metadata.m_hasJumped);
 456     addJump(jump(), target);
 457     equal.link(this);
 458 }
 459 
 460 void JIT::emit_op_eq(const Instruction* currentInstruction)
 461 {
 462     auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
 463     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 464     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 465     compare32(Equal, regT1, regT0, regT0);
 466     boxBoolean(regT0, JSValueRegs { regT0 });
 467     emitPutVirtualRegister(bytecode.m_dst.offset());
 468 }
 469 
 470 void JIT::emit_op_jeq(const Instruction* currentInstruction)
 471 {
 472     auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
 473     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 474     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 475     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 476     addJump(branch32(Equal, regT0, regT1), target);
 477 }
 478 
 479 void JIT::emit_op_jtrue(const Instruction* currentInstruction)
 480 {
 481     auto bytecode = currentInstruction-&gt;as&lt;OpJtrue&gt;();
 482     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 483 
 484     GPRReg value = regT0;
 485     GPRReg scratch1 = regT1;
 486     GPRReg scratch2 = regT2;
 487     bool shouldCheckMasqueradesAsUndefined = true;
 488     emitGetVirtualRegister(bytecode.m_condition.offset(), value);
 489     addJump(branchIfTruthy(*vm(), JSValueRegs(value), scratch1, scratch2, fpRegT0, fpRegT1, shouldCheckMasqueradesAsUndefined, m_codeBlock-&gt;globalObject()), target);
 490 }
 491 
 492 void JIT::emit_op_neq(const Instruction* currentInstruction)
 493 {
 494     auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
 495     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 496     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 497     compare32(NotEqual, regT1, regT0, regT0);
 498     boxBoolean(regT0, JSValueRegs { regT0 });
 499 
 500     emitPutVirtualRegister(bytecode.m_dst.offset());
 501 }
 502 
 503 void JIT::emit_op_jneq(const Instruction* currentInstruction)
 504 {
 505     auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
 506     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 507     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 508     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 509     addJump(branch32(NotEqual, regT0, regT1), target);
 510 }
 511 
 512 void JIT::emit_op_throw(const Instruction* currentInstruction)
 513 {
 514     auto bytecode = currentInstruction-&gt;as&lt;OpThrow&gt;();
 515     ASSERT(regT0 == returnValueGPR);
 516     copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm()-&gt;topEntryFrame);
 517     emitGetVirtualRegister(bytecode.m_value.offset(), regT0);
 518     callOperationNoExceptionCheck(operationThrow, regT0);
 519     jumpToExceptionHandler(*vm());
 520 }
 521 
 522 template&lt;typename Op&gt;
 523 void JIT::compileOpStrictEq(const Instruction* currentInstruction, CompileOpStrictEqType type)
 524 {
 525     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 526     int dst = bytecode.m_dst.offset();
 527     int src1 = bytecode.m_lhs.offset();
 528     int src2 = bytecode.m_rhs.offset();
 529 
 530     emitGetVirtualRegisters(src1, regT0, src2, regT1);
 531 
 532     // Jump slow if both are cells (to cover strings).
 533     move(regT0, regT2);
 534     or64(regT1, regT2);
 535     addSlowCase(branchIfCell(regT2));
 536 
 537     // Jump slow if either is a double. First test if it&#39;s an integer, which is fine, and then test
 538     // if it&#39;s a double.
 539     Jump leftOK = branchIfInt32(regT0);
 540     addSlowCase(branchIfNumber(regT0));
 541     leftOK.link(this);
 542     Jump rightOK = branchIfInt32(regT1);
 543     addSlowCase(branchIfNumber(regT1));
 544     rightOK.link(this);
 545 
 546     if (type == CompileOpStrictEqType::StrictEq)
 547         compare64(Equal, regT1, regT0, regT0);
 548     else
 549         compare64(NotEqual, regT1, regT0, regT0);
 550     boxBoolean(regT0, JSValueRegs { regT0 });
 551 
 552     emitPutVirtualRegister(dst);
 553 }
 554 
 555 void JIT::emit_op_stricteq(const Instruction* currentInstruction)
 556 {
 557     compileOpStrictEq&lt;OpStricteq&gt;(currentInstruction, CompileOpStrictEqType::StrictEq);
 558 }
 559 
 560 void JIT::emit_op_nstricteq(const Instruction* currentInstruction)
 561 {
 562     compileOpStrictEq&lt;OpNstricteq&gt;(currentInstruction, CompileOpStrictEqType::NStrictEq);
 563 }
 564 
 565 template&lt;typename Op&gt;
 566 void JIT::compileOpStrictEqJump(const Instruction* currentInstruction, CompileOpStrictEqType type)
 567 {
 568     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 569     int target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 570     int src1 = bytecode.m_lhs.offset();
 571     int src2 = bytecode.m_rhs.offset();
 572 
 573     emitGetVirtualRegisters(src1, regT0, src2, regT1);
 574 
 575     // Jump slow if both are cells (to cover strings).
 576     move(regT0, regT2);
 577     or64(regT1, regT2);
 578     addSlowCase(branchIfCell(regT2));
 579 
 580     // Jump slow if either is a double. First test if it&#39;s an integer, which is fine, and then test
 581     // if it&#39;s a double.
 582     Jump leftOK = branchIfInt32(regT0);
 583     addSlowCase(branchIfNumber(regT0));
 584     leftOK.link(this);
 585     Jump rightOK = branchIfInt32(regT1);
 586     addSlowCase(branchIfNumber(regT1));
 587     rightOK.link(this);
 588 
 589     if (type == CompileOpStrictEqType::StrictEq)
 590         addJump(branch64(Equal, regT1, regT0), target);
 591     else
 592         addJump(branch64(NotEqual, regT1, regT0), target);
 593 }
 594 
 595 void JIT::emit_op_jstricteq(const Instruction* currentInstruction)
 596 {
 597     compileOpStrictEqJump&lt;OpJstricteq&gt;(currentInstruction, CompileOpStrictEqType::StrictEq);
 598 }
 599 
 600 void JIT::emit_op_jnstricteq(const Instruction* currentInstruction)
 601 {
 602     compileOpStrictEqJump&lt;OpJnstricteq&gt;(currentInstruction, CompileOpStrictEqType::NStrictEq);
 603 }
 604 
 605 void JIT::emitSlow_op_jstricteq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 606 {
 607     linkAllSlowCases(iter);
 608 
 609     auto bytecode = currentInstruction-&gt;as&lt;OpJstricteq&gt;();
 610     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 611     callOperation(operationCompareStrictEq, regT0, regT1);
 612     emitJumpSlowToHot(branchTest32(NonZero, returnValueGPR), target);
 613 }
 614 
 615 void JIT::emitSlow_op_jnstricteq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 616 {
 617     linkAllSlowCases(iter);
 618 
 619     auto bytecode = currentInstruction-&gt;as&lt;OpJnstricteq&gt;();
 620     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 621     callOperation(operationCompareStrictEq, regT0, regT1);
 622     emitJumpSlowToHot(branchTest32(Zero, returnValueGPR), target);
 623 }
 624 
 625 void JIT::emit_op_to_number(const Instruction* currentInstruction)
 626 {
 627     auto bytecode = currentInstruction-&gt;as&lt;OpToNumber&gt;();
 628     int dstVReg = bytecode.m_dst.offset();
 629     int srcVReg = bytecode.m_operand.offset();
 630     emitGetVirtualRegister(srcVReg, regT0);
 631 
 632     addSlowCase(branchIfNotNumber(regT0));
 633 
 634     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 635     if (srcVReg != dstVReg)
 636         emitPutVirtualRegister(dstVReg);
 637 }
 638 
 639 void JIT::emit_op_to_string(const Instruction* currentInstruction)
 640 {
 641     auto bytecode = currentInstruction-&gt;as&lt;OpToString&gt;();
 642     int srcVReg = bytecode.m_operand.offset();
 643     emitGetVirtualRegister(srcVReg, regT0);
 644 
 645     addSlowCase(branchIfNotCell(regT0));
 646     addSlowCase(branchIfNotString(regT0));
 647 
 648     emitPutVirtualRegister(bytecode.m_dst.offset());
 649 }
 650 
 651 void JIT::emit_op_to_object(const Instruction* currentInstruction)
 652 {
 653     auto bytecode = currentInstruction-&gt;as&lt;OpToObject&gt;();
 654     int dstVReg = bytecode.m_dst.offset();
 655     int srcVReg = bytecode.m_operand.offset();
 656     emitGetVirtualRegister(srcVReg, regT0);
 657 
 658     addSlowCase(branchIfNotCell(regT0));
 659     addSlowCase(branchIfNotObject(regT0));
 660 
 661     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 662     if (srcVReg != dstVReg)
 663         emitPutVirtualRegister(dstVReg);
 664 }
 665 
 666 void JIT::emit_op_catch(const Instruction* currentInstruction)
 667 {
 668     auto bytecode = currentInstruction-&gt;as&lt;OpCatch&gt;();
 669 
 670     restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm()-&gt;topEntryFrame);
 671 
 672     move(TrustedImmPtr(m_vm), regT3);
 673     load64(Address(regT3, VM::callFrameForCatchOffset()), callFrameRegister);
 674     storePtr(TrustedImmPtr(nullptr), Address(regT3, VM::callFrameForCatchOffset()));
 675 
 676     addPtr(TrustedImm32(stackPointerOffsetFor(codeBlock()) * sizeof(Register)), callFrameRegister, stackPointerRegister);
 677 
 678     callOperationNoExceptionCheck(operationCheckIfExceptionIsUncatchableAndNotifyProfiler);
 679     Jump isCatchableException = branchTest32(Zero, returnValueGPR);
 680     jumpToExceptionHandler(*vm());
 681     isCatchableException.link(this);
 682 
 683     move(TrustedImmPtr(m_vm), regT3);
 684     load64(Address(regT3, VM::exceptionOffset()), regT0);
 685     store64(TrustedImm64(JSValue::encode(JSValue())), Address(regT3, VM::exceptionOffset()));
 686     emitPutVirtualRegister(bytecode.m_exception.offset());
 687 
 688     load64(Address(regT0, Exception::valueOffset()), regT0);
 689     emitPutVirtualRegister(bytecode.m_thrownValue.offset());
 690 
 691 #if ENABLE(DFG_JIT)
 692     // FIXME: consider inline caching the process of doing OSR entry, including
 693     // argument type proofs, storing locals to the buffer, etc
 694     // https://bugs.webkit.org/show_bug.cgi?id=175598
 695 
 696     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 697     ValueProfileAndOperandBuffer* buffer = metadata.m_buffer;
 698     if (buffer || !shouldEmitProfiling())
 699         callOperation(operationTryOSREnterAtCatch, m_bytecodeOffset);
 700     else
 701         callOperation(operationTryOSREnterAtCatchAndValueProfile, m_bytecodeOffset);
 702     auto skipOSREntry = branchTestPtr(Zero, returnValueGPR);
 703     emitRestoreCalleeSaves();
 704     jump(returnValueGPR, ExceptionHandlerPtrTag);
 705     skipOSREntry.link(this);
 706     if (buffer &amp;&amp; shouldEmitProfiling()) {
 707         buffer-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {
 708             JSValueRegs regs(regT0);
 709             emitGetVirtualRegister(profile.m_operand, regs);
 710             emitValueProfilingSite(profile.m_profile);
 711         });
 712     }
 713 #endif // ENABLE(DFG_JIT)
 714 }
 715 
 716 void JIT::emit_op_identity_with_profile(const Instruction*)
 717 {
 718     // We don&#39;t need to do anything here...
 719 }
 720 
 721 void JIT::emit_op_get_parent_scope(const Instruction* currentInstruction)
 722 {
 723     auto bytecode = currentInstruction-&gt;as&lt;OpGetParentScope&gt;();
 724     int currentScope = bytecode.m_scope.offset();
 725     emitGetVirtualRegister(currentScope, regT0);
 726     loadPtr(Address(regT0, JSScope::offsetOfNext()), regT0);
 727     emitStoreCell(bytecode.m_dst.offset(), regT0);
 728 }
 729 
 730 void JIT::emit_op_switch_imm(const Instruction* currentInstruction)
 731 {
 732     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchImm&gt;();
 733     size_t tableIndex = bytecode.m_tableIndex;
 734     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 735     unsigned scrutinee = bytecode.m_scrutinee.offset();
 736 
 737     // create jump table for switch destinations, track this switch statement.
 738     SimpleJumpTable* jumpTable = &amp;m_codeBlock-&gt;switchJumpTable(tableIndex);
 739     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset, SwitchRecord::Immediate));
 740     jumpTable-&gt;ensureCTITable();
 741 
 742     emitGetVirtualRegister(scrutinee, regT0);
 743     callOperation(operationSwitchImmWithUnknownKeyType, regT0, tableIndex);
 744     jump(returnValueGPR, JSSwitchPtrTag);
 745 }
 746 
 747 void JIT::emit_op_switch_char(const Instruction* currentInstruction)
 748 {
 749     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchChar&gt;();
 750     size_t tableIndex = bytecode.m_tableIndex;
 751     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 752     unsigned scrutinee = bytecode.m_scrutinee.offset();
 753 
 754     // create jump table for switch destinations, track this switch statement.
 755     SimpleJumpTable* jumpTable = &amp;m_codeBlock-&gt;switchJumpTable(tableIndex);
 756     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset, SwitchRecord::Character));
 757     jumpTable-&gt;ensureCTITable();
 758 
 759     emitGetVirtualRegister(scrutinee, regT0);
 760     callOperation(operationSwitchCharWithUnknownKeyType, regT0, tableIndex);
 761     jump(returnValueGPR, JSSwitchPtrTag);
 762 }
 763 
 764 void JIT::emit_op_switch_string(const Instruction* currentInstruction)
 765 {
 766     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchString&gt;();
 767     size_t tableIndex = bytecode.m_tableIndex;
 768     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 769     unsigned scrutinee = bytecode.m_scrutinee.offset();
 770 
 771     // create jump table for switch destinations, track this switch statement.
 772     StringJumpTable* jumpTable = &amp;m_codeBlock-&gt;stringSwitchJumpTable(tableIndex);
 773     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset));
 774 
 775     emitGetVirtualRegister(scrutinee, regT0);
 776     callOperation(operationSwitchStringWithUnknownKeyType, regT0, tableIndex);
 777     jump(returnValueGPR, JSSwitchPtrTag);
 778 }
 779 
 780 void JIT::emit_op_debug(const Instruction* currentInstruction)
 781 {
 782     auto bytecode = currentInstruction-&gt;as&lt;OpDebug&gt;();
 783     load32(codeBlock()-&gt;debuggerRequestsAddress(), regT0);
 784     Jump noDebuggerRequests = branchTest32(Zero, regT0);
 785     callOperation(operationDebug, static_cast&lt;int&gt;(bytecode.m_debugHookType));
 786     noDebuggerRequests.link(this);
 787 }
 788 
 789 void JIT::emit_op_eq_null(const Instruction* currentInstruction)
 790 {
 791     auto bytecode = currentInstruction-&gt;as&lt;OpEqNull&gt;();
 792     int dst = bytecode.m_dst.offset();
 793     int src1 = bytecode.m_operand.offset();
 794 
 795     emitGetVirtualRegister(src1, regT0);
 796     Jump isImmediate = branchIfNotCell(regT0);
 797 
 798     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 799     move(TrustedImm32(0), regT0);
 800     Jump wasNotMasqueradesAsUndefined = jump();
 801 
 802     isMasqueradesAsUndefined.link(this);
 803     emitLoadStructure(*vm(), regT0, regT2, regT1);
 804     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 805     loadPtr(Address(regT2, Structure::globalObjectOffset()), regT2);
 806     comparePtr(Equal, regT0, regT2, regT0);
 807     Jump wasNotImmediate = jump();
 808 
 809     isImmediate.link(this);
 810 
 811     and64(TrustedImm32(~TagBitUndefined), regT0);
 812     compare64(Equal, regT0, TrustedImm32(ValueNull), regT0);
 813 
 814     wasNotImmediate.link(this);
 815     wasNotMasqueradesAsUndefined.link(this);
 816 
 817     boxBoolean(regT0, JSValueRegs { regT0 });
 818     emitPutVirtualRegister(dst);
 819 
 820 }
 821 
 822 void JIT::emit_op_neq_null(const Instruction* currentInstruction)
 823 {
 824     auto bytecode = currentInstruction-&gt;as&lt;OpNeqNull&gt;();
 825     int dst = bytecode.m_dst.offset();
 826     int src1 = bytecode.m_operand.offset();
 827 
 828     emitGetVirtualRegister(src1, regT0);
 829     Jump isImmediate = branchIfNotCell(regT0);
 830 
 831     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 832     move(TrustedImm32(1), regT0);
 833     Jump wasNotMasqueradesAsUndefined = jump();
 834 
 835     isMasqueradesAsUndefined.link(this);
 836     emitLoadStructure(*vm(), regT0, regT2, regT1);
 837     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 838     loadPtr(Address(regT2, Structure::globalObjectOffset()), regT2);
 839     comparePtr(NotEqual, regT0, regT2, regT0);
 840     Jump wasNotImmediate = jump();
 841 
 842     isImmediate.link(this);
 843 
 844     and64(TrustedImm32(~TagBitUndefined), regT0);
 845     compare64(NotEqual, regT0, TrustedImm32(ValueNull), regT0);
 846 
 847     wasNotImmediate.link(this);
 848     wasNotMasqueradesAsUndefined.link(this);
 849 
 850     boxBoolean(regT0, JSValueRegs { regT0 });
 851     emitPutVirtualRegister(dst);
 852 }
 853 
 854 void JIT::emit_op_enter(const Instruction*)
 855 {
 856     // Even though CTI doesn&#39;t use them, we initialize our constant
 857     // registers to zap stale pointers, to avoid unnecessarily prolonging
 858     // object lifetime and increasing GC pressure.
 859     size_t count = m_codeBlock-&gt;numVars();
 860     for (size_t j = CodeBlock::llintBaselineCalleeSaveSpaceAsVirtualRegisters(); j &lt; count; ++j)
 861         emitInitRegister(virtualRegisterForLocal(j).offset());
 862 
 863     emitWriteBarrier(m_codeBlock);
 864 
 865     emitEnterOptimizationCheck();
 866 }
 867 
 868 void JIT::emit_op_get_scope(const Instruction* currentInstruction)
 869 {
 870     auto bytecode = currentInstruction-&gt;as&lt;OpGetScope&gt;();
 871     int dst = bytecode.m_dst.offset();
 872     emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, regT0);
 873     loadPtr(Address(regT0, JSFunction::offsetOfScopeChain()), regT0);
 874     emitStoreCell(dst, regT0);
 875 }
 876 
 877 void JIT::emit_op_to_this(const Instruction* currentInstruction)
 878 {
 879     auto bytecode = currentInstruction-&gt;as&lt;OpToThis&gt;();
 880     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 881     WriteBarrierBase&lt;Structure&gt;* cachedStructure = &amp;metadata.m_cachedStructure;
 882     emitGetVirtualRegister(bytecode.m_srcDst.offset(), regT1);
 883 
 884     emitJumpSlowCaseIfNotJSCell(regT1);
 885 
 886     addSlowCase(branchIfNotType(regT1, FinalObjectType));
 887     loadPtr(cachedStructure, regT2);
 888     addSlowCase(branchTestPtr(Zero, regT2));
 889     load32(Address(regT2, Structure::structureIDOffset()), regT2);
 890     addSlowCase(branch32(NotEqual, Address(regT1, JSCell::structureIDOffset()), regT2));
 891 }
 892 
 893 void JIT::emit_op_create_this(const Instruction* currentInstruction)
 894 {
 895     auto bytecode = currentInstruction-&gt;as&lt;OpCreateThis&gt;();
 896     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 897     int callee = bytecode.m_callee.offset();
 898     WriteBarrierBase&lt;JSCell&gt;* cachedFunction = &amp;metadata.m_cachedCallee;
 899     RegisterID calleeReg = regT0;
 900     RegisterID rareDataReg = regT4;
 901     RegisterID resultReg = regT0;
 902     RegisterID allocatorReg = regT1;
 903     RegisterID structureReg = regT2;
 904     RegisterID cachedFunctionReg = regT4;
 905     RegisterID scratchReg = regT3;
 906 
 907     emitGetVirtualRegister(callee, calleeReg);
 908     addSlowCase(branchIfNotFunction(calleeReg));
 909     loadPtr(Address(calleeReg, JSFunction::offsetOfRareData()), rareDataReg);
 910     addSlowCase(branchTestPtr(Zero, rareDataReg));
 911     loadPtr(Address(rareDataReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfile::offsetOfAllocator()), allocatorReg);
 912     loadPtr(Address(rareDataReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfile::offsetOfStructure()), structureReg);
 913 
 914     loadPtr(cachedFunction, cachedFunctionReg);
 915     Jump hasSeenMultipleCallees = branchPtr(Equal, cachedFunctionReg, TrustedImmPtr(JSCell::seenMultipleCalleeObjects()));
 916     addSlowCase(branchPtr(NotEqual, calleeReg, cachedFunctionReg));
 917     hasSeenMultipleCallees.link(this);
 918 
 919     JumpList slowCases;
 920     auto butterfly = TrustedImmPtr(nullptr);
 921     emitAllocateJSObject(resultReg, JITAllocator::variable(), allocatorReg, structureReg, butterfly, scratchReg, slowCases);
 922     emitGetVirtualRegister(callee, scratchReg);
 923     loadPtr(Address(scratchReg, JSFunction::offsetOfRareData()), scratchReg);
 924     load32(Address(scratchReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfile::offsetOfInlineCapacity()), scratchReg);
 925     emitInitializeInlineStorage(resultReg, scratchReg);
 926     addSlowCase(slowCases);
 927     emitPutVirtualRegister(bytecode.m_dst.offset());
 928 }
 929 
 930 void JIT::emit_op_check_tdz(const Instruction* currentInstruction)
 931 {
 932     auto bytecode = currentInstruction-&gt;as&lt;OpCheckTdz&gt;();
 933     emitGetVirtualRegister(bytecode.m_targetVirtualRegister.offset(), regT0);
 934     addSlowCase(branchIfEmpty(regT0));
 935 }
 936 
 937 
 938 // Slow cases
 939 
 940 void JIT::emitSlow_op_eq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 941 {
 942     linkAllSlowCases(iter);
 943 
 944     auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
 945     callOperation(operationCompareEq, regT0, regT1);
 946     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
 947     emitPutVirtualRegister(bytecode.m_dst.offset(), returnValueGPR);
 948 }
 949 
 950 void JIT::emitSlow_op_neq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 951 {
 952     linkAllSlowCases(iter);
 953 
 954     auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
 955     callOperation(operationCompareEq, regT0, regT1);
 956     xor32(TrustedImm32(0x1), regT0);
 957     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
 958     emitPutVirtualRegister(bytecode.m_dst.offset(), returnValueGPR);
 959 }
 960 
 961 void JIT::emitSlow_op_jeq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 962 {
 963     linkAllSlowCases(iter);
 964 
 965     auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
 966     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 967     callOperation(operationCompareEq, regT0, regT1);
 968     emitJumpSlowToHot(branchTest32(NonZero, returnValueGPR), target);
 969 }
 970 
 971 void JIT::emitSlow_op_jneq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 972 {
 973     linkAllSlowCases(iter);
 974 
 975     auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
 976     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 977     callOperation(operationCompareEq, regT0, regT1);
 978     emitJumpSlowToHot(branchTest32(Zero, returnValueGPR), target);
 979 }
 980 
 981 void JIT::emitSlow_op_instanceof_custom(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 982 {
 983     linkAllSlowCases(iter);
 984 
 985     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceofCustom&gt;();
 986     int dst = bytecode.m_dst.offset();
 987     int value = bytecode.m_value.offset();
 988     int constructor = bytecode.m_constructor.offset();
 989     int hasInstanceValue = bytecode.m_hasInstanceValue.offset();
 990 
 991     emitGetVirtualRegister(value, regT0);
 992     emitGetVirtualRegister(constructor, regT1);
 993     emitGetVirtualRegister(hasInstanceValue, regT2);
 994     callOperation(operationInstanceOfCustom, regT0, regT1, regT2);
 995     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
 996     emitPutVirtualRegister(dst, returnValueGPR);
 997 }
 998 
 999 #endif // USE(JSVALUE64)
1000 
1001 void JIT::emit_op_loop_hint(const Instruction*)
1002 {
1003     // Emit the JIT optimization check:
1004     if (canBeOptimized()) {
1005         addSlowCase(branchAdd32(PositiveOrZero, TrustedImm32(Options::executionCounterIncrementForLoop()),
1006             AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())));
1007     }
1008 }
1009 
1010 void JIT::emitSlow_op_loop_hint(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1011 {
1012 #if ENABLE(DFG_JIT)
1013     // Emit the slow path for the JIT optimization check:
1014     if (canBeOptimized()) {
1015         linkAllSlowCases(iter);
1016 
1017         copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm()-&gt;topEntryFrame);
1018 
1019         callOperation(operationOptimize, m_bytecodeOffset);
1020         Jump noOptimizedEntry = branchTestPtr(Zero, returnValueGPR);
1021         if (!ASSERT_DISABLED) {
1022             Jump ok = branchPtr(MacroAssembler::Above, returnValueGPR, TrustedImmPtr(bitwise_cast&lt;void*&gt;(static_cast&lt;intptr_t&gt;(1000))));
1023             abortWithReason(JITUnreasonableLoopHintJumpTarget);
1024             ok.link(this);
1025         }
1026         jump(returnValueGPR, GPRInfo::callFrameRegister);
1027         noOptimizedEntry.link(this);
1028 
1029         emitJumpSlowToHot(jump(), currentInstruction-&gt;size());
1030     }
1031 #else
1032     UNUSED_PARAM(currentInstruction);
1033     UNUSED_PARAM(iter);
1034 #endif
1035 }
1036 
1037 void JIT::emit_op_check_traps(const Instruction*)
1038 {
1039     addSlowCase(branchTest8(NonZero, AbsoluteAddress(m_vm-&gt;needTrapHandlingAddress())));
1040 }
1041 
1042 void JIT::emit_op_nop(const Instruction*)
1043 {
1044 }
1045 
1046 void JIT::emit_op_super_sampler_begin(const Instruction*)
1047 {
1048     add32(TrustedImm32(1), AbsoluteAddress(bitwise_cast&lt;void*&gt;(&amp;g_superSamplerCount)));
1049 }
1050 
1051 void JIT::emit_op_super_sampler_end(const Instruction*)
1052 {
1053     sub32(TrustedImm32(1), AbsoluteAddress(bitwise_cast&lt;void*&gt;(&amp;g_superSamplerCount)));
1054 }
1055 
1056 void JIT::emitSlow_op_check_traps(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1057 {
1058     linkAllSlowCases(iter);
1059 
1060     callOperation(operationHandleTraps);
1061 }
1062 
1063 void JIT::emit_op_new_regexp(const Instruction* currentInstruction)
1064 {
1065     auto bytecode = currentInstruction-&gt;as&lt;OpNewRegexp&gt;();
1066     int dst = bytecode.m_dst.offset();
1067     int regexp = bytecode.m_regexp.offset();
1068     callOperation(operationNewRegexp, jsCast&lt;RegExp*&gt;(m_codeBlock-&gt;getConstant(regexp)));
1069     emitStoreCell(dst, returnValueGPR);
1070 }
1071 
1072 template&lt;typename Op&gt;
1073 void JIT::emitNewFuncCommon(const Instruction* currentInstruction)
1074 {
1075     Jump lazyJump;
1076     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
1077     int dst = bytecode.m_dst.offset();
1078 
1079 #if USE(JSVALUE64)
1080     emitGetVirtualRegister(bytecode.m_scope.offset(), regT0);
1081 #else
1082     emitLoadPayload(bytecode.m_scope.offset(), regT0);
1083 #endif
1084     FunctionExecutable* funcExec = m_codeBlock-&gt;functionDecl(bytecode.m_functionDecl);
1085 
1086     OpcodeID opcodeID = Op::opcodeID;
1087     if (opcodeID == op_new_func)
1088         callOperation(operationNewFunction, dst, regT0, funcExec);
1089     else if (opcodeID == op_new_generator_func)
1090         callOperation(operationNewGeneratorFunction, dst, regT0, funcExec);
1091     else if (opcodeID == op_new_async_func)
1092         callOperation(operationNewAsyncFunction, dst, regT0, funcExec);
1093     else {
1094         ASSERT(opcodeID == op_new_async_generator_func);
1095         callOperation(operationNewAsyncGeneratorFunction, dst, regT0, funcExec);
1096     }
1097 }
1098 
1099 void JIT::emit_op_new_func(const Instruction* currentInstruction)
1100 {
1101     emitNewFuncCommon&lt;OpNewFunc&gt;(currentInstruction);
1102 }
1103 
1104 void JIT::emit_op_new_generator_func(const Instruction* currentInstruction)
1105 {
1106     emitNewFuncCommon&lt;OpNewGeneratorFunc&gt;(currentInstruction);
1107 }
1108 
1109 void JIT::emit_op_new_async_generator_func(const Instruction* currentInstruction)
1110 {
1111     emitNewFuncCommon&lt;OpNewAsyncGeneratorFunc&gt;(currentInstruction);
1112 }
1113 
1114 void JIT::emit_op_new_async_func(const Instruction* currentInstruction)
1115 {
1116     emitNewFuncCommon&lt;OpNewAsyncFunc&gt;(currentInstruction);
1117 }
1118 
1119 template&lt;typename Op&gt;
1120 void JIT::emitNewFuncExprCommon(const Instruction* currentInstruction)
1121 {
1122     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
1123     int dst = bytecode.m_dst.offset();
1124 #if USE(JSVALUE64)
1125     emitGetVirtualRegister(bytecode.m_scope.offset(), regT0);
1126 #else
1127     emitLoadPayload(bytecode.m_scope.offset(), regT0);
1128 #endif
1129 
1130     FunctionExecutable* function = m_codeBlock-&gt;functionExpr(bytecode.m_functionDecl);
1131     OpcodeID opcodeID = Op::opcodeID;
1132 
1133     if (opcodeID == op_new_func_exp)
1134         callOperation(operationNewFunction, dst, regT0, function);
1135     else if (opcodeID == op_new_generator_func_exp)
1136         callOperation(operationNewGeneratorFunction, dst, regT0, function);
1137     else if (opcodeID == op_new_async_func_exp)
1138         callOperation(operationNewAsyncFunction, dst, regT0, function);
1139     else {
1140         ASSERT(opcodeID == op_new_async_generator_func_exp);
1141         callOperation(operationNewAsyncGeneratorFunction, dst, regT0, function);
1142     }
1143 }
1144 
1145 void JIT::emit_op_new_func_exp(const Instruction* currentInstruction)
1146 {
1147     emitNewFuncExprCommon&lt;OpNewFuncExp&gt;(currentInstruction);
1148 }
1149 
1150 void JIT::emit_op_new_generator_func_exp(const Instruction* currentInstruction)
1151 {
1152     emitNewFuncExprCommon&lt;OpNewGeneratorFuncExp&gt;(currentInstruction);
1153 }
1154 
1155 void JIT::emit_op_new_async_func_exp(const Instruction* currentInstruction)
1156 {
1157     emitNewFuncExprCommon&lt;OpNewAsyncFuncExp&gt;(currentInstruction);
1158 }
1159 
1160 void JIT::emit_op_new_async_generator_func_exp(const Instruction* currentInstruction)
1161 {
1162     emitNewFuncExprCommon&lt;OpNewAsyncGeneratorFuncExp&gt;(currentInstruction);
1163 }
1164 
1165 void JIT::emit_op_new_array(const Instruction* currentInstruction)
1166 {
1167     auto bytecode = currentInstruction-&gt;as&lt;OpNewArray&gt;();
1168     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1169     int dst = bytecode.m_dst.offset();
1170     int valuesIndex = bytecode.m_argv.offset();
1171     int size = bytecode.m_argc;
1172     addPtr(TrustedImm32(valuesIndex * sizeof(Register)), callFrameRegister, regT0);
1173     callOperation(operationNewArrayWithProfile, dst,
1174         &amp;metadata.m_arrayAllocationProfile, regT0, size);
1175 }
1176 
1177 void JIT::emit_op_new_array_with_size(const Instruction* currentInstruction)
1178 {
1179     auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSize&gt;();
1180     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1181     int dst = bytecode.m_dst.offset();
1182     int sizeIndex = bytecode.m_length.offset();
1183 #if USE(JSVALUE64)
1184     emitGetVirtualRegister(sizeIndex, regT0);
1185     callOperation(operationNewArrayWithSizeAndProfile, dst,
1186         &amp;metadata.m_arrayAllocationProfile, regT0);
1187 #else
1188     emitLoad(sizeIndex, regT1, regT0);
1189     callOperation(operationNewArrayWithSizeAndProfile, dst,
1190         &amp;metadata.m_arrayAllocationProfile, JSValueRegs(regT1, regT0));
1191 #endif
1192 }
1193 
1194 #if USE(JSVALUE64)
1195 void JIT::emit_op_has_structure_property(const Instruction* currentInstruction)
1196 {
1197     auto bytecode = currentInstruction-&gt;as&lt;OpHasStructureProperty&gt;();
1198     int dst = bytecode.m_dst.offset();
1199     int base = bytecode.m_base.offset();
1200     int enumerator = bytecode.m_enumerator.offset();
1201 
1202     emitGetVirtualRegister(base, regT0);
1203     emitGetVirtualRegister(enumerator, regT1);
1204     emitJumpSlowCaseIfNotJSCell(regT0, base);
1205 
1206     load32(Address(regT0, JSCell::structureIDOffset()), regT0);
1207     addSlowCase(branch32(NotEqual, regT0, Address(regT1, JSPropertyNameEnumerator::cachedStructureIDOffset())));
1208 
1209     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
1210     emitPutVirtualRegister(dst);
1211 }
1212 
1213 void JIT::privateCompileHasIndexedProperty(ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
1214 {
1215     const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(byValInfo-&gt;bytecodeIndex).ptr();
1216 
1217     PatchableJump badType;
1218 
1219     // FIXME: Add support for other types like TypedArrays and Arguments.
1220     // See https://bugs.webkit.org/show_bug.cgi?id=135033 and https://bugs.webkit.org/show_bug.cgi?id=135034.
1221     JumpList slowCases = emitLoadForArrayMode(currentInstruction, arrayMode, badType);
1222     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
1223     Jump done = jump();
1224 
1225     LinkBuffer patchBuffer(*this, m_codeBlock);
1226 
1227     patchBuffer.link(badType, byValInfo-&gt;slowPathTarget);
1228     patchBuffer.link(slowCases, byValInfo-&gt;slowPathTarget);
1229 
1230     patchBuffer.link(done, byValInfo-&gt;badTypeDoneTarget);
1231 
1232     byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1233         m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1234         &quot;Baseline has_indexed_property stub for %s, return point %p&quot;, toCString(*m_codeBlock).data(), returnAddress.value());
1235 
1236     MacroAssembler::repatchJump(byValInfo-&gt;badTypeJump, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(byValInfo-&gt;stubRoutine-&gt;code().code()));
1237     MacroAssembler::repatchCall(CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)), FunctionPtr&lt;OperationPtrTag&gt;(operationHasIndexedPropertyGeneric));
1238 }
1239 
1240 void JIT::emit_op_has_indexed_property(const Instruction* currentInstruction)
1241 {
1242     auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
1243     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1244     int dst = bytecode.m_dst.offset();
1245     int base = bytecode.m_base.offset();
1246     int property = bytecode.m_property.offset();
1247     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
1248     ByValInfo* byValInfo = m_codeBlock-&gt;addByValInfo();
1249 
1250     emitGetVirtualRegisters(base, regT0, property, regT1);
1251 
1252     // This is technically incorrect - we&#39;re zero-extending an int32. On the hot path this doesn&#39;t matter.
1253     // We check the value as if it was a uint32 against the m_vectorLength - which will always fail if
1254     // number was signed since m_vectorLength is always less than intmax (since the total allocation
1255     // size is always less than 4Gb). As such zero extending will have been correct (and extending the value
1256     // to 64-bits is necessary since it&#39;s used in the address calculation. We zero extend rather than sign
1257     // extending since it makes it easier to re-tag the value in the slow case.
1258     zeroExtend32ToPtr(regT1, regT1);
1259 
1260     emitJumpSlowCaseIfNotJSCell(regT0, base);
1261     emitArrayProfilingSiteWithCell(regT0, regT2, profile);
1262     and32(TrustedImm32(IndexingShapeMask), regT2);
1263 
1264     JITArrayMode mode = chooseArrayMode(profile);
1265     PatchableJump badType;
1266 
1267     // FIXME: Add support for other types like TypedArrays and Arguments.
1268     // See https://bugs.webkit.org/show_bug.cgi?id=135033 and https://bugs.webkit.org/show_bug.cgi?id=135034.
1269     JumpList slowCases = emitLoadForArrayMode(currentInstruction, mode, badType);
1270 
1271     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
1272 
1273     addSlowCase(badType);
1274     addSlowCase(slowCases);
1275 
1276     Label done = label();
1277 
1278     emitPutVirtualRegister(dst);
1279 
1280     Label nextHotPath = label();
1281 
1282     m_byValCompilationInfo.append(ByValCompilationInfo(byValInfo, m_bytecodeOffset, PatchableJump(), badType, mode, profile, done, nextHotPath));
1283 }
1284 
1285 void JIT::emitSlow_op_has_indexed_property(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1286 {
1287     linkAllSlowCases(iter);
1288 
1289     auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
1290     int dst = bytecode.m_dst.offset();
1291     int base = bytecode.m_base.offset();
1292     int property = bytecode.m_property.offset();
1293     ByValInfo* byValInfo = m_byValCompilationInfo[m_byValInstructionIndex].byValInfo;
1294 
1295     Label slowPath = label();
1296 
1297     emitGetVirtualRegister(base, regT0);
1298     emitGetVirtualRegister(property, regT1);
1299     Call call = callOperation(operationHasIndexedPropertyDefault, dst, regT0, regT1, byValInfo);
1300 
1301     m_byValCompilationInfo[m_byValInstructionIndex].slowPathTarget = slowPath;
1302     m_byValCompilationInfo[m_byValInstructionIndex].returnAddress = call;
1303     m_byValInstructionIndex++;
1304 }
1305 
1306 void JIT::emit_op_get_direct_pname(const Instruction* currentInstruction)
1307 {
1308     auto bytecode = currentInstruction-&gt;as&lt;OpGetDirectPname&gt;();
1309     int dst = bytecode.m_dst.offset();
1310     int base = bytecode.m_base.offset();
1311     int index = bytecode.m_index.offset();
1312     int enumerator = bytecode.m_enumerator.offset();
1313 
1314     // Check that base is a cell
1315     emitGetVirtualRegister(base, regT0);
1316     emitJumpSlowCaseIfNotJSCell(regT0, base);
1317 
1318     // Check the structure
1319     emitGetVirtualRegister(enumerator, regT2);
1320     load32(Address(regT0, JSCell::structureIDOffset()), regT1);
1321     addSlowCase(branch32(NotEqual, regT1, Address(regT2, JSPropertyNameEnumerator::cachedStructureIDOffset())));
1322 
1323     // Compute the offset
1324     emitGetVirtualRegister(index, regT1);
1325     // If index is less than the enumerator&#39;s cached inline storage, then it&#39;s an inline access
1326     Jump outOfLineAccess = branch32(AboveOrEqual, regT1, Address(regT2, JSPropertyNameEnumerator::cachedInlineCapacityOffset()));
1327     addPtr(TrustedImm32(JSObject::offsetOfInlineStorage()), regT0);
1328     signExtend32ToPtr(regT1, regT1);
1329     load64(BaseIndex(regT0, regT1, TimesEight), regT0);
1330 
1331     Jump done = jump();
1332 
1333     // Otherwise it&#39;s out of line
1334     outOfLineAccess.link(this);
1335     loadPtr(Address(regT0, JSObject::butterflyOffset()), regT0);
1336     sub32(Address(regT2, JSPropertyNameEnumerator::cachedInlineCapacityOffset()), regT1);
1337     neg32(regT1);
1338     signExtend32ToPtr(regT1, regT1);
1339     int32_t offsetOfFirstProperty = static_cast&lt;int32_t&gt;(offsetInButterfly(firstOutOfLineOffset)) * sizeof(EncodedJSValue);
1340     load64(BaseIndex(regT0, regT1, TimesEight, offsetOfFirstProperty), regT0);
1341 
1342     done.link(this);
1343     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
1344     emitPutVirtualRegister(dst, regT0);
1345 }
1346 
1347 void JIT::emit_op_enumerator_structure_pname(const Instruction* currentInstruction)
1348 {
1349     auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorStructurePname&gt;();
1350     int dst = bytecode.m_dst.offset();
1351     int enumerator = bytecode.m_enumerator.offset();
1352     int index = bytecode.m_index.offset();
1353 
1354     emitGetVirtualRegister(index, regT0);
1355     emitGetVirtualRegister(enumerator, regT1);
1356     Jump inBounds = branch32(Below, regT0, Address(regT1, JSPropertyNameEnumerator::endStructurePropertyIndexOffset()));
1357 
1358     move(TrustedImm64(JSValue::encode(jsNull())), regT0);
1359 
1360     Jump done = jump();
1361     inBounds.link(this);
1362 
1363     loadPtr(Address(regT1, JSPropertyNameEnumerator::cachedPropertyNamesVectorOffset()), regT1);
1364     signExtend32ToPtr(regT0, regT0);
1365     load64(BaseIndex(regT1, regT0, TimesEight), regT0);
1366 
1367     done.link(this);
1368     emitPutVirtualRegister(dst);
1369 }
1370 
1371 void JIT::emit_op_enumerator_generic_pname(const Instruction* currentInstruction)
1372 {
1373     auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorGenericPname&gt;();
1374     int dst = bytecode.m_dst.offset();
1375     int enumerator = bytecode.m_enumerator.offset();
1376     int index = bytecode.m_index.offset();
1377 
1378     emitGetVirtualRegister(index, regT0);
1379     emitGetVirtualRegister(enumerator, regT1);
1380     Jump inBounds = branch32(Below, regT0, Address(regT1, JSPropertyNameEnumerator::endGenericPropertyIndexOffset()));
1381 
1382     move(TrustedImm64(JSValue::encode(jsNull())), regT0);
1383 
1384     Jump done = jump();
1385     inBounds.link(this);
1386 
1387     loadPtr(Address(regT1, JSPropertyNameEnumerator::cachedPropertyNamesVectorOffset()), regT1);
1388     signExtend32ToPtr(regT0, regT0);
1389     load64(BaseIndex(regT1, regT0, TimesEight), regT0);
1390 
1391     done.link(this);
1392     emitPutVirtualRegister(dst);
1393 }
1394 
1395 void JIT::emit_op_profile_type(const Instruction* currentInstruction)
1396 {
1397     auto bytecode = currentInstruction-&gt;as&lt;OpProfileType&gt;();
1398     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1399     TypeLocation* cachedTypeLocation = metadata.m_typeLocation;
1400     int valueToProfile = bytecode.m_targetVirtualRegister.offset();
1401 
1402     emitGetVirtualRegister(valueToProfile, regT0);
1403 
1404     JumpList jumpToEnd;
1405 
1406     jumpToEnd.append(branchIfEmpty(regT0));
1407 
1408     // Compile in a predictive type check, if possible, to see if we can skip writing to the log.
1409     // These typechecks are inlined to match those of the 64-bit JSValue type checks.
1410     if (cachedTypeLocation-&gt;m_lastSeenType == TypeUndefined)
1411         jumpToEnd.append(branchIfUndefined(regT0));
1412     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeNull)
1413         jumpToEnd.append(branchIfNull(regT0));
1414     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeBoolean)
1415         jumpToEnd.append(branchIfBoolean(regT0, regT1));
1416     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeAnyInt)
1417         jumpToEnd.append(branchIfInt32(regT0));
1418     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeNumber)
1419         jumpToEnd.append(branchIfNumber(regT0));
1420     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeString) {
1421         Jump isNotCell = branchIfNotCell(regT0);
1422         jumpToEnd.append(branchIfString(regT0));
1423         isNotCell.link(this);
1424     }
1425 
1426     // Load the type profiling log into T2.
1427     TypeProfilerLog* cachedTypeProfilerLog = m_vm-&gt;typeProfilerLog();
1428     move(TrustedImmPtr(cachedTypeProfilerLog), regT2);
1429     // Load the next log entry into T1.
1430     loadPtr(Address(regT2, TypeProfilerLog::currentLogEntryOffset()), regT1);
1431 
1432     // Store the JSValue onto the log entry.
1433     store64(regT0, Address(regT1, TypeProfilerLog::LogEntry::valueOffset()));
1434 
1435     // Store the structureID of the cell if T0 is a cell, otherwise, store 0 on the log entry.
1436     Jump notCell = branchIfNotCell(regT0);
1437     load32(Address(regT0, JSCell::structureIDOffset()), regT0);
1438     store32(regT0, Address(regT1, TypeProfilerLog::LogEntry::structureIDOffset()));
1439     Jump skipIsCell = jump();
1440     notCell.link(this);
1441     store32(TrustedImm32(0), Address(regT1, TypeProfilerLog::LogEntry::structureIDOffset()));
1442     skipIsCell.link(this);
1443 
1444     // Store the typeLocation on the log entry.
1445     move(TrustedImmPtr(cachedTypeLocation), regT0);
1446     store64(regT0, Address(regT1, TypeProfilerLog::LogEntry::locationOffset()));
1447 
1448     // Increment the current log entry.
1449     addPtr(TrustedImm32(sizeof(TypeProfilerLog::LogEntry)), regT1);
1450     store64(regT1, Address(regT2, TypeProfilerLog::currentLogEntryOffset()));
1451     Jump skipClearLog = branchPtr(NotEqual, regT1, TrustedImmPtr(cachedTypeProfilerLog-&gt;logEndPtr()));
1452     // Clear the log if we&#39;re at the end of the log.
1453     callOperation(operationProcessTypeProfilerLog);
1454     skipClearLog.link(this);
1455 
1456     jumpToEnd.link(this);
1457 }
1458 
1459 void JIT::emit_op_log_shadow_chicken_prologue(const Instruction* currentInstruction)
1460 {
1461     RELEASE_ASSERT(vm()-&gt;shadowChicken());
1462     updateTopCallFrame();
1463     static_assert(nonArgGPR0 != regT0 &amp;&amp; nonArgGPR0 != regT2, &quot;we will have problems if this is true.&quot;);
1464     auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenPrologue&gt;();
1465     GPRReg shadowPacketReg = regT0;
1466     GPRReg scratch1Reg = nonArgGPR0; // This must be a non-argument register.
1467     GPRReg scratch2Reg = regT2;
1468     ensureShadowChickenPacket(*vm(), shadowPacketReg, scratch1Reg, scratch2Reg);
1469     emitGetVirtualRegister(bytecode.m_scope.offset(), regT3);
1470     logShadowChickenProloguePacket(shadowPacketReg, scratch1Reg, regT3);
1471 }
1472 
1473 void JIT::emit_op_log_shadow_chicken_tail(const Instruction* currentInstruction)
1474 {
1475     RELEASE_ASSERT(vm()-&gt;shadowChicken());
1476     updateTopCallFrame();
1477     static_assert(nonArgGPR0 != regT0 &amp;&amp; nonArgGPR0 != regT2, &quot;we will have problems if this is true.&quot;);
1478     auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenTail&gt;();
1479     GPRReg shadowPacketReg = regT0;
1480     GPRReg scratch1Reg = nonArgGPR0; // This must be a non-argument register.
1481     GPRReg scratch2Reg = regT2;
1482     ensureShadowChickenPacket(*vm(), shadowPacketReg, scratch1Reg, scratch2Reg);
1483     emitGetVirtualRegister(bytecode.m_thisValue.offset(), regT2);
1484     emitGetVirtualRegister(bytecode.m_scope.offset(), regT3);
1485     logShadowChickenTailPacket(shadowPacketReg, JSValueRegs(regT2), regT3, m_codeBlock, CallSiteIndex(m_bytecodeOffset));
1486 }
1487 
1488 #endif // USE(JSVALUE64)
1489 
1490 void JIT::emit_op_profile_control_flow(const Instruction* currentInstruction)
1491 {
1492     auto bytecode = currentInstruction-&gt;as&lt;OpProfileControlFlow&gt;();
1493     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1494     BasicBlockLocation* basicBlockLocation = metadata.m_basicBlockLocation;
1495 #if USE(JSVALUE64)
1496     basicBlockLocation-&gt;emitExecuteCode(*this);
1497 #else
1498     basicBlockLocation-&gt;emitExecuteCode(*this, regT0);
1499 #endif
1500 }
1501 
1502 void JIT::emit_op_argument_count(const Instruction* currentInstruction)
1503 {
1504     auto bytecode = currentInstruction-&gt;as&lt;OpArgumentCount&gt;();
1505     int dst = bytecode.m_dst.offset();
1506     load32(payloadFor(CallFrameSlot::argumentCount), regT0);
1507     sub32(TrustedImm32(1), regT0);
1508     JSValueRegs result = JSValueRegs::withTwoAvailableRegs(regT0, regT1);
1509     boxInt32(regT0, result);
1510     emitPutVirtualRegister(dst, result);
1511 }
1512 
1513 void JIT::emit_op_get_rest_length(const Instruction* currentInstruction)
1514 {
1515     auto bytecode = currentInstruction-&gt;as&lt;OpGetRestLength&gt;();
1516     int dst = bytecode.m_dst.offset();
1517     unsigned numParamsToSkip = bytecode.m_numParametersToSkip;
1518     load32(payloadFor(CallFrameSlot::argumentCount), regT0);
1519     sub32(TrustedImm32(1), regT0);
1520     Jump zeroLength = branch32(LessThanOrEqual, regT0, Imm32(numParamsToSkip));
1521     sub32(Imm32(numParamsToSkip), regT0);
1522 #if USE(JSVALUE64)
1523     boxInt32(regT0, JSValueRegs(regT0));
1524 #endif
1525     Jump done = jump();
1526 
1527     zeroLength.link(this);
1528 #if USE(JSVALUE64)
1529     move(TrustedImm64(JSValue::encode(jsNumber(0))), regT0);
1530 #else
1531     move(TrustedImm32(0), regT0);
1532 #endif
1533 
1534     done.link(this);
1535 #if USE(JSVALUE64)
1536     emitPutVirtualRegister(dst, regT0);
1537 #else
1538     move(TrustedImm32(JSValue::Int32Tag), regT1);
1539     emitPutVirtualRegister(dst, JSValueRegs(regT1, regT0));
1540 #endif
1541 }
1542 
1543 void JIT::emit_op_get_argument(const Instruction* currentInstruction)
1544 {
1545     auto bytecode = currentInstruction-&gt;as&lt;OpGetArgument&gt;();
1546     int dst = bytecode.m_dst.offset();
1547     int index = bytecode.m_index;
1548 #if USE(JSVALUE64)
1549     JSValueRegs resultRegs(regT0);
1550 #else
1551     JSValueRegs resultRegs(regT1, regT0);
1552 #endif
1553 
1554     load32(payloadFor(CallFrameSlot::argumentCount), regT2);
1555     Jump argumentOutOfBounds = branch32(LessThanOrEqual, regT2, TrustedImm32(index));
1556     loadValue(addressFor(CallFrameSlot::thisArgument + index), resultRegs);
1557     Jump done = jump();
1558 
1559     argumentOutOfBounds.link(this);
1560     moveValue(jsUndefined(), resultRegs);
1561 
1562     done.link(this);
1563     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
1564     emitPutVirtualRegister(dst, resultRegs);
1565 }
1566 
1567 } // namespace JSC
1568 
1569 #endif // ENABLE(JIT)
    </pre>
  </body>
</html>