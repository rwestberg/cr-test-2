diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter64.asm b/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter64.asm
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter64.asm
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter64.asm
@@ -28,29 +28,43 @@
     loadb [PB, PC, 1], t0
     leap _g_opcodeMap, t1
     jmp [t1, t0, PtrSize], BytecodePtrTag
 end
 
-macro nextInstructionWide()
+macro nextInstructionWide16()
+    loadh 1[PB, PC, 1], t0
+    leap _g_opcodeMapWide16, t1
+    jmp [t1, t0, PtrSize], BytecodePtrTag
+end
+
+macro nextInstructionWide32()
     loadi 1[PB, PC, 1], t0
-    leap _g_opcodeMapWide, t1
+    leap _g_opcodeMapWide32, t1
     jmp [t1, t0, PtrSize], BytecodePtrTag
 end
 
 macro getuOperandNarrow(opcodeStruct, fieldName, dst)
     loadb constexpr %opcodeStruct%_%fieldName%_index[PB, PC, 1], dst
 end
 
 macro getOperandNarrow(opcodeStruct, fieldName, dst)
-    loadbsp constexpr %opcodeStruct%_%fieldName%_index[PB, PC, 1], dst
+    loadbsq constexpr %opcodeStruct%_%fieldName%_index[PB, PC, 1], dst
+end
+
+macro getuOperandWide16(opcodeStruct, fieldName, dst)
+    loadh constexpr %opcodeStruct%_%fieldName%_index * 2 + 1[PB, PC, 1], dst
 end
 
-macro getuOperandWide(opcodeStruct, fieldName, dst)
+macro getOperandWide16(opcodeStruct, fieldName, dst)
+    loadhsq constexpr %opcodeStruct%_%fieldName%_index * 2 + 1[PB, PC, 1], dst
+end
+
+macro getuOperandWide32(opcodeStruct, fieldName, dst)
     loadi constexpr %opcodeStruct%_%fieldName%_index * 4 + 1[PB, PC, 1], dst
 end
 
-macro getOperandWide(opcodeStruct, fieldName, dst)
+macro getOperandWide32(opcodeStruct, fieldName, dst)
     loadis constexpr %opcodeStruct%_%fieldName%_index * 4 + 1[PB, PC, 1], dst
 end
 
 macro makeReturn(get, dispatch, fn)
     fn(macro (value)
@@ -107,19 +121,19 @@
         addp 32, a0
         call function
         addp 48, sp
         move 8[r0], r1
         move [r0], r0
-    elsif C_LOOP
+    elsif C_LOOP or C_LOOP_WIN
         cloopCallSlowPath function, a0, a1
     else
         error
     end
 end
 
 macro cCall2Void(function)
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         cloopCallSlowPathVoid function, a0, a1
     elsif X86_64_WIN
         # Note: we cannot use the cCall2 macro for Win64 in this case,
         # as the Win64 cCall2 implemenation is only correct when the return type size is > 8 bytes.
         # On Win64, rcx and rdx are used for passing the first two parameters.
@@ -177,11 +191,11 @@
     bqbeq sp, t3, .throwStackOverflow
 
     # Ensure that we have enough additional stack capacity for the incoming args,
     # and the frame for the JS code we're executing. We need to do this check
     # before we start copying the args from the protoCallFrame below.
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         bpaeq t3, VM::m_cloopStackLimit[vm], .stackHeightOK
         move entry, t4
         move vm, t5
         cloopCallSlowPath _llint_stack_check_at_vm_entry, vm, t3
         bpeq t0, 0, .stackCheckFailed
@@ -283,11 +297,11 @@
 end
 
 
 macro makeJavaScriptCall(entry, temp, unused)
     addp 16, sp
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         cloopCallJSFunction entry
     else
         call entry, JSEntryPtrTag
     end
     subp 16, sp
@@ -295,11 +309,11 @@
 
 macro makeHostFunctionCall(entry, temp, unused)
     move entry, temp
     storep cfr, [sp]
     move sp, a0
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         storep lr, 8[sp]
         cloopCallNative temp
     elsif X86_64_WIN
         # We need to allocate 32 bytes on the stack for the shadow space.
         subp 32, sp
@@ -406,23 +420,42 @@
         .recover:
             loadi ArgumentCount + TagOffset[cfr], PC
         end)
 end
 
-macro uncage(basePtr, mask, ptr, scratch)
-    if GIGACAGE_ENABLED and not C_LOOP
+macro cage(basePtr, mask, ptr, scratch)
+    if GIGACAGE_ENABLED and not (C_LOOP or C_LOOP_WIN)
         loadp basePtr, scratch
         btpz scratch, .done
         andp mask, ptr
         addp scratch, ptr
     .done:
     end
 end
 
-macro loadCaged(basePtr, mask, source, dest, scratch)
+macro cagedPrimitive(ptr, length, scratch, scratch2)
+    if ARM64E
+        const source = scratch2
+        move ptr, scratch2
+    else
+        const source = ptr
+    end
+    if GIGACAGE_ENABLED
+        cage(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, source, scratch)
+        if ARM64E
+            const numberOfPACBits = constexpr MacroAssembler::numberOfPACBits
+            bfiq scratch2, 0, 64 - numberOfPACBits, ptr
+        end
+    end
+    if ARM64E
+        untagArrayPtr length, ptr
+    end
+end
+
+macro loadCagedJSValue(source, dest, scratchOrLength)
     loadp source, dest
-    uncage(basePtr, mask, dest, scratch)
+    cage(_g_gigacageBasePtrs + Gigacage::BasePtrs::jsValue, constexpr Gigacage::jsValueGigacageMask, dest, scratchOrLength)
 end
 
 macro loadVariable(get, fieldName, valueReg)
     get(fieldName, valueReg)
     loadq [cfr, valueReg, 8], valueReg
@@ -439,23 +472,34 @@
         loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
         loadq -(FirstConstantRegisterIndexNarrow * 8)[value, index, 8], value
     .done:
     end
 
-    macro loadWide()
-        bpgteq index, FirstConstantRegisterIndexWide, .constant
+    macro loadWide16()
+        bpgteq index, FirstConstantRegisterIndexWide16, .constant
         loadq [cfr, index, 8], value
         jmp .done
     .constant:
         loadp CodeBlock[cfr], value
         loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
-        subp FirstConstantRegisterIndexWide, index
+        loadq -(FirstConstantRegisterIndexWide16 * 8)[value, index, 8], value
+    .done:
+    end
+
+    macro loadWide32()
+        bpgteq index, FirstConstantRegisterIndexWide32, .constant
+        loadq [cfr, index, 8], value
+        jmp .done
+    .constant:
+        loadp CodeBlock[cfr], value
+        loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
+        subp FirstConstantRegisterIndexWide32, index
         loadq [value, index, 8], value
     .done:
     end
 
-    size(loadNarrow, loadWide, macro (load) load() end)
+    size(loadNarrow, loadWide16, loadWide32, macro (load) load() end)
 end
 
 macro loadConstantOrVariableInt32(size, index, value, slow)
     loadConstantOrVariable(size, index, value)
     bqb value, tagTypeNumber, slow
@@ -464,23 +508,27 @@
 macro loadConstantOrVariableCell(size, index, value, slow)
     loadConstantOrVariable(size, index, value)
     btqnz value, tagMask, slow
 end
 
-macro writeBarrierOnOperandWithReload(size, get, cellFieldName, reloadAfterSlowPath)
-    get(cellFieldName, t1)
-    loadConstantOrVariableCell(size, t1, t2, .writeBarrierDone)
+macro writeBarrierOnCellWithReload(cell, reloadAfterSlowPath)
     skipIfIsRememberedOrInEden(
-        t2,
+        cell,
         macro()
             push PB, PC
-            move t2, a1 # t2 can be a0 (not on 64 bits, but better safe than sorry)
+            move cell, a1 # cell can be a0
             move cfr, a0
             cCall2Void(_llint_write_barrier_slow)
             pop PC, PB
             reloadAfterSlowPath()
         end)
+end
+
+macro writeBarrierOnOperandWithReload(size, get, cellFieldName, reloadAfterSlowPath)
+    get(cellFieldName, t1)
+    loadConstantOrVariableCell(size, t1, t2, .writeBarrierDone)
+    writeBarrierOnCellWithReload(t2, reloadAfterSlowPath)
 .writeBarrierDone:
 end
 
 macro writeBarrierOnOperand(size, get, cellFieldName)
     writeBarrierOnOperandWithReload(size, get, cellFieldName, macro () end)
@@ -499,19 +547,11 @@
     get(valueFieldName, t1)
     loadConstantOrVariableCell(size, t1, t0, .writeBarrierDone)
     btpz t0, .writeBarrierDone
 
     loadMacro(t3)
-    skipIfIsRememberedOrInEden(
-        t3,
-        macro()
-            push PB, PC
-            move cfr, a0
-            move t3, a1
-            cCall2Void(_llint_write_barrier_slow)
-            pop PC, PB
-        end)
+    writeBarrierOnCellWithReload(t3, macro() end)
 .writeBarrierDone:
 end
 
 macro writeBarrierOnGlobalObject(size, get, valueFieldName)
     writeBarrierOnGlobal(size, get, valueFieldName,
@@ -583,15 +623,12 @@
     bsubinz 1, t3, .fillExtraSlots
     andi ~(StackAlignmentSlots - 1), t1
     btiz t1, .continue
 
 .noExtraSlot:
-    if POINTER_PROFILING
-        if ARM64 or ARM64E
-            loadp 8[cfr], lr
-        end
-
+    if ARM64E
+        loadp 8[cfr], lr
         addp 16, cfr, t3
         untagReturnAddress t3
     end
 
     // Move frame up t1 slots
@@ -616,17 +653,14 @@
 .fillLoop:
     storeq t0, [t3, t1, 8]
     addp 8, t3
     baddinz 1, t2, .fillLoop
 
-    if POINTER_PROFILING
+    if ARM64E
         addp 16, cfr, t1
         tagReturnAddress t1
-
-        if ARM64 or ARM64E
-            storep lr, 8[cfr]
-        end
+        storep lr, 8[cfr]
     end
 
 .continue:
     # Reload CodeBlock and reset PC, since the slow_path clobbered them.
     loadp CodeBlock[cfr], t1
@@ -646,12 +680,12 @@
 
 # Instruction implementations
 _llint_op_enter:
     traceExecution()
     checkStackPointerAlignment(t2, 0xdead00e1)
-    loadp CodeBlock[cfr], t2                // t2<CodeBlock> = cfr.CodeBlock
-    loadi CodeBlock::m_numVars[t2], t2      // t2<size_t> = t2<CodeBlock>.m_numVars
+    loadp CodeBlock[cfr], t3                // t3<CodeBlock> = cfr.CodeBlock
+    loadi CodeBlock::m_numVars[t3], t2      // t2<size_t> = t3<CodeBlock>.m_numVars
     subq CalleeSaveSpaceAsVirtualRegisters, t2
     move cfr, t1
     subq CalleeSaveSpaceAsVirtualRegisters * 8, t1
     btiz t2, .opEnterDone
     move ValueUndefined, t0
@@ -660,13 +694,20 @@
 .opEnterLoop:
     storeq t0, [t1, t2, 8]
     addq 1, t2
     btqnz t2, .opEnterLoop
 .opEnterDone:
-    callSlowPath(_slow_path_enter)
+    writeBarrierOnCellWithReload(t3, macro ()
+        loadp CodeBlock[cfr], t3 # Reload CodeBlock
+    end)
+    loadp CodeBlock::m_vm[t3], t1
+    btbnz VM::m_traps + VMTraps::m_needTrapHandling[t1], .handleTraps
+.afterHandlingTraps:
     dispatchOp(narrow, op_enter)
-
+.handleTraps:
+    callTrapHandler(_llint_throw_from_slow_path_trampoline)
+    jmp .afterHandlingTraps
 
 llintOpWithProfile(op_get_argument, OpGetArgument, macro (size, get, dispatch, return)
     get(m_index, t2)
     loadi PayloadOffset + ArgumentCount[cfr], t0
     bilteq t0, t2, .opGetArgumentOutOfBounds
@@ -696,14 +737,14 @@
 llintOpWithMetadata(op_to_this, OpToThis, macro (size, get, dispatch, metadata, return)
     get(m_srcDst, t0)
     loadq [cfr, t0, 8], t0
     btqnz t0, tagMask, .opToThisSlow
     bbneq JSCell::m_type[t0], FinalObjectType, .opToThisSlow
-    loadStructureWithScratch(t0, t1, t2, t3)
+    loadi JSCell::m_structureID[t0], t1
     metadata(t2, t3)
-    loadp OpToThis::Metadata::m_cachedStructure[t2], t2
-    bpneq t1, t2, .opToThisSlow
+    loadi OpToThis::Metadata::m_cachedStructureID[t2], t2
+    bineq t1, t2, .opToThisSlow
     dispatch()
 
 .opToThisSlow:
     callSlowPath(_slow_path_to_this)
     dispatch()
@@ -789,12 +830,12 @@
 equalNullComparisonOp(op_neq_null, OpNeqNull,
     macro (value) xorq ValueTrue, value end)
 
 
 llintOpWithReturn(op_is_undefined_or_null, OpIsUndefinedOrNull, macro (size, get, dispatch, return)
-    get(m_operand, t0)
-    loadq [cfr, t0, 8], t0
+    get(m_operand, t1)
+    loadConstantOrVariable(size, t1, t0)
     andq ~TagBitUndefined, t0
     cqeq t0, ValueNull, t0
     orq ValueFalse, t0
     return(t0)
 end)
@@ -1108,11 +1149,11 @@
 
 macro bitOpProfiled(opcodeName, opcodeStruct, operation)
     commonBitOp(llintOpWithProfile, opcodeName, opcodeStruct, operation)
 end
 
-bitOp(lshift, OpLshift,
+bitOpProfiled(lshift, OpLshift,
     macro (left, right) lshifti left, right end)
 
 
 bitOp(rshift, OpRshift,
     macro (left, right) rshifti left, right end)
@@ -1283,11 +1324,11 @@
 end)
 
 
 llintOpWithMetadata(op_get_by_id, OpGetById, macro (size, get, dispatch, metadata, return)
     metadata(t2, t1)
-    loadb OpGetById::Metadata::m_mode[t2], t1
+    loadb OpGetById::Metadata::m_modeMetadata.mode[t2], t1
     get(m_base, t0)
     loadConstantOrVariableCell(size, t0, t3, .opGetByIdSlow)
 
 .opGetByIdDefault:
     bbneq t1, constexpr GetByIdMode::Default, .opGetByIdProtoLoad
@@ -1314,11 +1355,11 @@
     bbneq t1, constexpr GetByIdMode::ArrayLength, .opGetByIdUnset
     move t3, t0
     arrayProfile(OpGetById::Metadata::m_modeMetadata.arrayLengthMode.arrayProfile, t0, t2, t5)
     btiz t0, IsArray, .opGetByIdSlow
     btiz t0, IndexingShapeMask, .opGetByIdSlow
-    loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::jsValue, constexpr Gigacage::jsValueGigacageMask, JSObject::m_butterfly[t3], t0, t1)
+    loadCagedJSValue(JSObject::m_butterfly[t3], t0, t1)
     loadi -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0], t0
     bilt t0, 0, .opGetByIdSlow
     orq tagTypeNumber, t0
     valueProfile(OpGetById, t2, t0)
     return(t0)
@@ -1437,11 +1478,11 @@
 
     get(m_property, t3)
     loadConstantOrVariableInt32(size, t3, t1, .opGetByValSlow)
     sxi2q t1, t1
 
-    loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::jsValue, constexpr Gigacage::jsValueGigacageMask, JSObject::m_butterfly[t0], t3, tagTypeNumber)
+    loadCagedJSValue(JSObject::m_butterfly[t0], t3, tagTypeNumber)
     move TagTypeNumber, tagTypeNumber
 
     andi IndexingShapeMask, t2
     bieq t2, Int32Shape, .opGetByValIsContiguous
     bineq t2, ContiguousShape, .opGetByValNotContiguous
@@ -1481,11 +1522,23 @@
     loadb JSCell::m_type[t0], t2
     subi FirstTypedArrayType, t2
     biaeq t2, NumberOfTypedArrayTypesExcludingDataView, .opGetByValSlow
     
     # Sweet, now we know that we have a typed array. Do some basic things now.
-    biaeq t1, JSArrayBufferView::m_length[t0], .opGetByValSlow
+
+    if ARM64E
+        const length = t6
+        const scratch = t7
+        loadi JSArrayBufferView::m_length[t0], length
+        biaeq t1, length, .opGetByValSlow
+    else
+        # length and scratch are intentionally undefined on this branch because they are not used on other platforms.
+        biaeq t1, JSArrayBufferView::m_length[t0], .opGetByValSlow
+    end
+
+    loadp JSArrayBufferView::m_vector[t0], t3
+    cagedPrimitive(t3, length, t0, scratch)
 
     # Now bisect through the various types:
     #    Int8ArrayType,
     #    Uint8ArrayType,
     #    Uint8ClampedArrayType,
@@ -1503,40 +1556,35 @@
 
     # We have one of Int8ArrayType ... Uint8ClampedArrayType
     bia t2, Int8ArrayType - FirstTypedArrayType, .opGetByValUint8ArrayOrUint8ClampedArray
 
     # We have Int8ArrayType.
-    loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)
-    loadbs [t3, t1], t0
+    loadbsi [t3, t1], t0
     finishIntGetByVal(t0, t1)
 
 .opGetByValUint8ArrayOrUint8ClampedArray:
     bia t2, Uint8ArrayType - FirstTypedArrayType, .opGetByValUint8ClampedArray
 
     # We have Uint8ArrayType.
-    loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)
     loadb [t3, t1], t0
     finishIntGetByVal(t0, t1)
 
 .opGetByValUint8ClampedArray:
     # We have Uint8ClampedArrayType.
-    loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)
     loadb [t3, t1], t0
     finishIntGetByVal(t0, t1)
 
 .opGetByValInt16ArrayOrUint16Array:
     # We have either Int16ArrayType or Uint16ClampedArrayType.
     bia t2, Int16ArrayType - FirstTypedArrayType, .opGetByValUint16Array
 
     # We have Int16ArrayType.
-    loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)
-    loadhs [t3, t1, 2], t0
+    loadhsi [t3, t1, 2], t0
     finishIntGetByVal(t0, t1)
 
 .opGetByValUint16Array:
     # We have Uint16ArrayType.
-    loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)
     loadh [t3, t1, 2], t0
     finishIntGetByVal(t0, t1)
 
 .opGetByValAboveUint16Array:
     # We have one of Int32ArrayType .. Float64ArrayType.
@@ -1544,17 +1592,15 @@
 
     # We have either Int32ArrayType or Uint32ArrayType
     bia t2, Int32ArrayType - FirstTypedArrayType, .opGetByValUint32Array
 
     # We have Int32ArrayType.
-    loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)
     loadi [t3, t1, 4], t0
     finishIntGetByVal(t0, t1)
 
 .opGetByValUint32Array:
     # We have Uint32ArrayType.
-    loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)
     # This is the hardest part because of large unsigned values.
     loadi [t3, t1, 4], t0
     bilt t0, 0, .opGetByValSlow # This case is still awkward to implement in LLInt.
     finishIntGetByVal(t0, t1)
 
@@ -1562,11 +1608,10 @@
     # We have one of Float32ArrayType or Float64ArrayType. Sadly, we cannot handle Float32Array
     # inline yet. That would require some offlineasm changes.
     bieq t2, Float32ArrayType - FirstTypedArrayType, .opGetByValSlow
 
     # We have Float64ArrayType.
-    loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)
     loadd [t3, t1, 8], ft0
     bdnequn ft0, ft0, .opGetByValSlow
     finishDoubleGetByVal(ft0, t0, t1)
 
 .opGetByValSlow:
@@ -1598,11 +1643,11 @@
         metadata(t5, t0)
         arrayProfile(%opcodeStruct%::Metadata::m_arrayProfile, t2, t5, t0)
         get(m_property, t0)
         loadConstantOrVariableInt32(size, t0, t3, .opPutByValSlow)
         sxi2q t3, t3
-        loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::jsValue, constexpr Gigacage::jsValueGigacageMask, JSObject::m_butterfly[t1], t0, tagTypeNumber)
+        loadCagedJSValue(JSObject::m_butterfly[t1], t0, tagTypeNumber)
         move TagTypeNumber, tagTypeNumber
         btinz t2, CopyOnWrite, .opPutByValSlow
         andi IndexingShapeMask, t2
         bineq t2, Int32Shape, .opPutByValNotInt32
         contiguousPutByVal(
@@ -1727,10 +1772,28 @@
         loadp CodeBlock::m_globalObject[t0], t0
         bpneq Structure::m_globalObject[structure], t0, target
     end,
     macro (value, target) bqneq value, ValueNull, target end)
 
+macro undefinedOrNullJumpOp(opcodeName, opcodeStruct, fn)
+    llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
+        get(m_value, t1)
+        loadConstantOrVariable(size, t1, t0)
+        andq ~TagBitUndefined, t0
+        fn(t0, .target)
+        dispatch()
+
+    .target:
+        jump(m_targetLabel)
+    end)
+end
+
+undefinedOrNullJumpOp(jundefined_or_null, OpJundefinedOrNull,
+    macro (value, target) bqeq value, ValueNull, target end)
+
+undefinedOrNullJumpOp(jnundefined_or_null, OpJnundefinedOrNull,
+    macro (value, target) bqneq value, ValueNull, target end)
 
 llintOpWithMetadata(op_jneq_ptr, OpJneqPtr, macro (size, get, dispatch, metadata, return)
     get(m_value, t0)
     getu(size, OpJneqPtr, m_specialPointer, t1)
     loadp CodeBlock[cfr], t2
@@ -1896,10 +1959,13 @@
 
 .opSwitchCharFallThrough:
     jump(m_defaultOffset)
 
 .opSwitchOnRope:
+    bineq JSRopeString::m_compactFibers + JSRopeString::CompactFibers::m_length[t1], 1, .opSwitchCharFallThrough
+
+.opSwitchOnRopeChar:
     callSlowPath(_llint_slow_path_switch_char)
     nextInstruction()
 end)
 
 
@@ -1908,11 +1974,11 @@
     getu(m_argv, t3)
     negp t3
     loadq ThisArgumentOffset[cfr, t3, 8], t0
     btqnz t0, tagMask, .done
     loadi JSCell::m_structureID[t0], t3
-    storei t3, %opcodeStruct%::Metadata::m_arrayProfile.m_lastSeenStructureID[t5]
+    storei t3, %opcodeStruct%::Metadata::m_callLinkInfo.m_arrayProfile.m_lastSeenStructureID[t5]
 .done:
 end
 
 macro commonCallOp(opcodeName, slowPath, opcodeStruct, prepareCall, prologue)
     llintOpWithMetadata(opcodeName, opcodeStruct, macro (size, get, dispatch, metadata, return)
@@ -1921,11 +1987,11 @@
         prologue(macro (fieldName, dst)
             getu(size, opcodeStruct, fieldName, dst)
         end, metadata)
 
         get(m_callee, t0)
-        loadp %opcodeStruct%::Metadata::m_callLinkInfo.callee[t5], t2
+        loadp %opcodeStruct%::Metadata::m_callLinkInfo.m_calleeOrLastSeenCalleeWithLinkBit[t5], t2
         loadConstantOrVariable(size, t0, t3)
         bqneq t3, t2, .opCallSlow
         getu(size, opcodeStruct, m_argv, t3)
         lshifti 3, t3
         negp t3
@@ -1933,12 +1999,12 @@
         storeq t2, Callee[t3]
         getu(size, opcodeStruct, m_argc, t2)
         storei PC, ArgumentCount + TagOffset[cfr]
         storei t2, ArgumentCount + PayloadOffset[t3]
         move t3, sp
-        prepareCall(%opcodeStruct%::Metadata::m_callLinkInfo.machineCodeTarget[t5], t2, t3, t4, JSEntryPtrTag)
-        callTargetFunction(size, opcodeStruct, dispatch, %opcodeStruct%::Metadata::m_callLinkInfo.machineCodeTarget[t5], JSEntryPtrTag)
+        prepareCall(%opcodeStruct%::Metadata::m_callLinkInfo.m_machineCodeTarget[t5], t2, t3, t4, JSEntryPtrTag)
+        callTargetFunction(size, opcodeStruct, dispatch, %opcodeStruct%::Metadata::m_callLinkInfo.m_machineCodeTarget[t5], JSEntryPtrTag)
 
     .opCallSlow:
         slowPathForCall(size, opcodeStruct, dispatch, slowPath, prepareCall)
     end)
 end
@@ -2050,18 +2116,18 @@
     storep 0, CodeBlock[cfr]
     loadp Callee[cfr], t0
     andp MarkedBlockMask, t0, t1
     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
     storep cfr, VM::topCallFrame[t1]
-    if ARM64 or ARM64E or C_LOOP
+    if ARM64 or ARM64E or C_LOOP or C_LOOP_WIN
         storep lr, ReturnPC[cfr]
     end
     move cfr, a0
     loadp Callee[cfr], t1
     loadp JSFunction::m_executable[t1], t1
     checkStackPointerAlignment(t3, 0xdead0001)
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         cloopCallNative executableOffsetToFunction[t1]
     else
         if X86_64_WIN
             subp 32, sp
             call executableOffsetToFunction[t1], JSEntryPtrTag
@@ -2090,17 +2156,17 @@
     storep 0, CodeBlock[cfr]
     loadp Callee[cfr], t0
     andp MarkedBlockMask, t0, t1
     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
     storep cfr, VM::topCallFrame[t1]
-    if ARM64 or ARM64E or C_LOOP
+    if ARM64 or ARM64E or C_LOOP or C_LOOP_WIN
         storep lr, ReturnPC[cfr]
     end
     move cfr, a0
     loadp Callee[cfr], t1
     checkStackPointerAlignment(t3, 0xdead0001)
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         cloopCallNative offsetOfFunction[t1]
     else
         if X86_64_WIN
             subp 32, sp
             call offsetOfFunction[t1], JSEntryPtrTag
