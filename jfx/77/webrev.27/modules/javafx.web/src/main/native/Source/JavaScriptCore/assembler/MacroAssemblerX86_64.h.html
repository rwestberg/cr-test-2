<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/assembler/MacroAssemblerX86_64.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2008-2018 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #pragma once
  27 
  28 #if ENABLE(ASSEMBLER) &amp;&amp; CPU(X86_64)
  29 
  30 #include &quot;MacroAssemblerX86Common.h&quot;
  31 
  32 #define REPATCH_OFFSET_CALL_R11 3
  33 
  34 inline bool CAN_SIGN_EXTEND_32_64(int64_t value) { return value == (int64_t)(int32_t)value; }
  35 
  36 namespace JSC {
  37 
  38 class MacroAssemblerX86_64 : public MacroAssemblerX86Common {
  39 public:
  40     static const unsigned numGPRs = 16;
  41     static const unsigned numFPRs = 16;
  42 
  43     static const Scale ScalePtr = TimesEight;
  44 
  45     using MacroAssemblerX86Common::add32;
  46     using MacroAssemblerX86Common::and32;
  47     using MacroAssemblerX86Common::branch32;
  48     using MacroAssemblerX86Common::branchAdd32;
  49     using MacroAssemblerX86Common::or32;
  50     using MacroAssemblerX86Common::sub32;
  51     using MacroAssemblerX86Common::load8;
  52     using MacroAssemblerX86Common::load32;
  53     using MacroAssemblerX86Common::store32;
  54     using MacroAssemblerX86Common::store8;
  55     using MacroAssemblerX86Common::call;
  56     using MacroAssemblerX86Common::jump;
  57     using MacroAssemblerX86Common::farJump;
  58     using MacroAssemblerX86Common::addDouble;
  59     using MacroAssemblerX86Common::loadDouble;
  60     using MacroAssemblerX86Common::convertInt32ToDouble;
  61 
  62     void add32(TrustedImm32 imm, AbsoluteAddress address)
  63     {
  64         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  65         add32(imm, Address(scratchRegister()));
  66     }
  67 
  68     void and32(TrustedImm32 imm, AbsoluteAddress address)
  69     {
  70         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  71         and32(imm, Address(scratchRegister()));
  72     }
  73 
  74     void add32(AbsoluteAddress address, RegisterID dest)
  75     {
  76         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  77         add32(Address(scratchRegister()), dest);
  78     }
  79 
  80     void or32(TrustedImm32 imm, AbsoluteAddress address)
  81     {
  82         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  83         or32(imm, Address(scratchRegister()));
  84     }
  85 
  86     void or32(RegisterID reg, AbsoluteAddress address)
  87     {
  88         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  89         or32(reg, Address(scratchRegister()));
  90     }
  91 
  92     void sub32(TrustedImm32 imm, AbsoluteAddress address)
  93     {
  94         move(TrustedImmPtr(address.m_ptr), scratchRegister());
  95         sub32(imm, Address(scratchRegister()));
  96     }
  97 
  98     void load8(const void* address, RegisterID dest)
  99     {
 100         move(TrustedImmPtr(address), dest);
 101         load8(dest, dest);
 102     }
 103 
 104     void load16(ExtendedAddress address, RegisterID dest)
 105     {
 106         TrustedImmPtr addr(reinterpret_cast&lt;void*&gt;(address.offset));
 107         MacroAssemblerX86Common::move(addr, scratchRegister());
 108         MacroAssemblerX86Common::load16(BaseIndex(scratchRegister(), address.base, TimesTwo), dest);
 109     }
 110 
 111     void load16(BaseIndex address, RegisterID dest)
 112     {
 113         MacroAssemblerX86Common::load16(address, dest);
 114     }
 115 
 116     void load16(Address address, RegisterID dest)
 117     {
 118         MacroAssemblerX86Common::load16(address, dest);
 119     }
 120 
 121     void load32(const void* address, RegisterID dest)
 122     {
 123         if (dest == X86Registers::eax)
 124             m_assembler.movl_mEAX(address);
 125         else {
 126             move(TrustedImmPtr(address), dest);
 127             load32(dest, dest);
 128         }
 129     }
 130 
 131     void addDouble(AbsoluteAddress address, FPRegisterID dest)
 132     {
 133         move(TrustedImmPtr(address.m_ptr), scratchRegister());
 134         m_assembler.addsd_mr(0, scratchRegister(), dest);
 135     }
 136 
 137     void convertInt32ToDouble(TrustedImm32 imm, FPRegisterID dest)
 138     {
 139         move(imm, scratchRegister());
 140         m_assembler.cvtsi2sd_rr(scratchRegister(), dest);
 141     }
 142 
 143     void store32(TrustedImm32 imm, void* address)
 144     {
 145         move(TrustedImmPtr(address), scratchRegister());
 146         store32(imm, scratchRegister());
 147     }
 148 
 149     void store32(RegisterID source, void* address)
 150     {
 151         if (source == X86Registers::eax)
 152             m_assembler.movl_EAXm(address);
 153         else {
 154             move(TrustedImmPtr(address), scratchRegister());
 155             store32(source, scratchRegister());
 156         }
 157     }
 158 
 159     void store8(TrustedImm32 imm, void* address)
 160     {
 161         TrustedImm32 imm8(static_cast&lt;int8_t&gt;(imm.m_value));
 162         move(TrustedImmPtr(address), scratchRegister());
 163         store8(imm8, Address(scratchRegister()));
 164     }
 165 
 166     void store8(RegisterID reg, void* address)
 167     {
 168         move(TrustedImmPtr(address), scratchRegister());
 169         store8(reg, Address(scratchRegister()));
 170     }
 171 
 172 #if OS(WINDOWS)
 173     Call callWithSlowPathReturnType(PtrTag)
 174     {
 175         // On Win64, when the return type is larger than 8 bytes, we need to allocate space on the stack for the return value.
 176         // On entry, rcx should contain a pointer to this stack space. The other parameters are shifted to the right,
 177         // rdx should contain the first argument, r8 should contain the second argument, and r9 should contain the third argument.
 178         // On return, rax contains a pointer to this stack value. See http://msdn.microsoft.com/en-us/library/7572ztz4.aspx.
 179         // We then need to copy the 16 byte return value into rax and rdx, since JIT expects the return value to be split between the two.
 180         // It is assumed that the parameters are already shifted to the right, when entering this method.
 181         // Note: this implementation supports up to 3 parameters.
 182 
 183         // JIT relies on the CallerFrame (frame pointer) being put on the stack,
 184         // On Win64 we need to manually copy the frame pointer to the stack, since MSVC may not maintain a frame pointer on 64-bit.
 185         // See http://msdn.microsoft.com/en-us/library/9z1stfyw.aspx where it&#39;s stated that rbp MAY be used as a frame pointer.
 186         store64(X86Registers::ebp, Address(X86Registers::esp, -16));
 187 
 188         // We also need to allocate the shadow space on the stack for the 4 parameter registers.
 189         // In addition, we need to allocate 16 bytes for the return value.
 190         // Also, we should allocate 16 bytes for the frame pointer, and return address (not populated).
 191         sub64(TrustedImm32(8 * sizeof(int64_t)), X86Registers::esp);
 192 
 193         // The first parameter register should contain a pointer to the stack allocated space for the return value.
 194         move(X86Registers::esp, X86Registers::ecx);
 195         add64(TrustedImm32(4 * sizeof(int64_t)), X86Registers::ecx);
 196 
 197         DataLabelPtr label = moveWithPatch(TrustedImmPtr(nullptr), scratchRegister());
 198         Call result = Call(m_assembler.call(scratchRegister()), Call::Linkable);
 199 
 200         add64(TrustedImm32(8 * sizeof(int64_t)), X86Registers::esp);
 201 
 202         // Copy the return value into rax and rdx.
 203         load64(Address(X86Registers::eax, sizeof(int64_t)), X86Registers::edx);
 204         load64(Address(X86Registers::eax), X86Registers::eax);
 205 
 206         ASSERT_UNUSED(label, differenceBetween(label, result) == REPATCH_OFFSET_CALL_R11);
 207         return result;
 208     }
 209 #endif
 210 
 211     Call call(PtrTag)
 212     {
 213 #if OS(WINDOWS)
 214         // JIT relies on the CallerFrame (frame pointer) being put on the stack,
 215         // On Win64 we need to manually copy the frame pointer to the stack, since MSVC may not maintain a frame pointer on 64-bit.
 216         // See http://msdn.microsoft.com/en-us/library/9z1stfyw.aspx where it&#39;s stated that rbp MAY be used as a frame pointer.
 217         store64(X86Registers::ebp, Address(X86Registers::esp, -16));
 218 
 219         // On Windows we need to copy the arguments that don&#39;t fit in registers to the stack location where the callee expects to find them.
 220         // We don&#39;t know the number of arguments at this point, so the arguments (5, 6, ...) should always be copied.
 221 
 222         // Copy argument 5
 223         load64(Address(X86Registers::esp, 4 * sizeof(int64_t)), scratchRegister());
 224         store64(scratchRegister(), Address(X86Registers::esp, -4 * static_cast&lt;int32_t&gt;(sizeof(int64_t))));
 225 
 226         // Copy argument 6
 227         load64(Address(X86Registers::esp, 5 * sizeof(int64_t)), scratchRegister());
 228         store64(scratchRegister(), Address(X86Registers::esp, -3 * static_cast&lt;int32_t&gt;(sizeof(int64_t))));
 229 
 230         // We also need to allocate the shadow space on the stack for the 4 parameter registers.
 231         // Also, we should allocate 16 bytes for the frame pointer, and return address (not populated).
 232         // In addition, we need to allocate 16 bytes for two more parameters, since the call can have up to 6 parameters.
 233         sub64(TrustedImm32(8 * sizeof(int64_t)), X86Registers::esp);
 234 #endif
 235         DataLabelPtr label = moveWithPatch(TrustedImmPtr(nullptr), scratchRegister());
 236         Call result = Call(m_assembler.call(scratchRegister()), Call::Linkable);
 237 #if OS(WINDOWS)
 238         add64(TrustedImm32(8 * sizeof(int64_t)), X86Registers::esp);
 239 #endif
 240         ASSERT_UNUSED(label, differenceBetween(label, result) == REPATCH_OFFSET_CALL_R11);
 241         return result;
 242     }
 243 
 244     ALWAYS_INLINE Call call(RegisterID callTag) { return UNUSED_PARAM(callTag), call(NoPtrTag); }
 245 
 246     // Address is a memory location containing the address to jump to
 247     void farJump(AbsoluteAddress address, PtrTag tag)
 248     {
 249         move(TrustedImmPtr(address.m_ptr), scratchRegister());
 250         farJump(Address(scratchRegister()), tag);
 251     }
 252 
 253     ALWAYS_INLINE void farJump(AbsoluteAddress address, RegisterID jumpTag) { UNUSED_PARAM(jumpTag), farJump(address, NoPtrTag); }
 254 
 255     Call threadSafePatchableNearCall()
 256     {
 257         const size_t nearCallOpcodeSize = 1;
 258         const size_t nearCallRelativeLocationSize = sizeof(int32_t);
 259         // We want to make sure the 32-bit near call immediate is 32-bit aligned.
 260         size_t codeSize = m_assembler.codeSize();
 261         size_t alignedSize = WTF::roundUpToMultipleOf&lt;nearCallRelativeLocationSize&gt;(codeSize + nearCallOpcodeSize);
 262         emitNops(alignedSize - (codeSize + nearCallOpcodeSize));
 263         DataLabelPtr label = DataLabelPtr(this);
 264         Call result = nearCall();
 265         ASSERT_UNUSED(label, differenceBetween(label, result) == (nearCallOpcodeSize + nearCallRelativeLocationSize));
 266         return result;
 267     }
 268 
 269     Jump branchAdd32(ResultCondition cond, TrustedImm32 src, AbsoluteAddress dest)
 270     {
 271         move(TrustedImmPtr(dest.m_ptr), scratchRegister());
 272         add32(src, Address(scratchRegister()));
 273         return Jump(m_assembler.jCC(x86Condition(cond)));
 274     }
 275 
 276     void add64(RegisterID src, RegisterID dest)
 277     {
 278         m_assembler.addq_rr(src, dest);
 279     }
 280 
 281     void add64(Address src, RegisterID dest)
 282     {
 283         m_assembler.addq_mr(src.offset, src.base, dest);
 284     }
 285 
 286     void add64(BaseIndex src, RegisterID dest)
 287     {
 288         m_assembler.addq_mr(src.offset, src.base, src.index, src.scale, dest);
 289     }
 290 
 291     void add64(RegisterID src, Address dest)
 292     {
 293         m_assembler.addq_rm(src, dest.offset, dest.base);
 294     }
 295 
 296     void add64(RegisterID src, BaseIndex dest)
 297     {
 298         m_assembler.addq_rm(src, dest.offset, dest.base, dest.index, dest.scale);
 299     }
 300 
 301     void add64(AbsoluteAddress src, RegisterID dest)
 302     {
 303         move(TrustedImmPtr(src.m_ptr), scratchRegister());
 304         add64(Address(scratchRegister()), dest);
 305     }
 306 
 307     void add64(TrustedImm32 imm, RegisterID srcDest)
 308     {
 309         if (imm.m_value == 1)
 310             m_assembler.incq_r(srcDest);
 311         else
 312             m_assembler.addq_ir(imm.m_value, srcDest);
 313     }
 314 
 315     void add64(TrustedImm64 imm, RegisterID dest)
 316     {
 317         if (imm.m_value == 1)
 318             m_assembler.incq_r(dest);
 319         else {
 320             move(imm, scratchRegister());
 321             add64(scratchRegister(), dest);
 322         }
 323     }
 324 
 325     void add64(TrustedImm32 imm, RegisterID src, RegisterID dest)
 326     {
 327         m_assembler.leaq_mr(imm.m_value, src, dest);
 328     }
 329 
 330     void add64(TrustedImm32 imm, Address address)
 331     {
 332         if (imm.m_value == 1)
 333             m_assembler.incq_m(address.offset, address.base);
 334         else
 335             m_assembler.addq_im(imm.m_value, address.offset, address.base);
 336     }
 337 
 338     void add64(TrustedImm32 imm, BaseIndex address)
 339     {
 340         if (imm.m_value == 1)
 341             m_assembler.incq_m(address.offset, address.base, address.index, address.scale);
 342         else
 343             m_assembler.addq_im(imm.m_value, address.offset, address.base, address.index, address.scale);
 344     }
 345 
 346     void add64(TrustedImm32 imm, AbsoluteAddress address)
 347     {
 348         move(TrustedImmPtr(address.m_ptr), scratchRegister());
 349         add64(imm, Address(scratchRegister()));
 350     }
 351 
 352     void add64(RegisterID a, RegisterID b, RegisterID dest)
 353     {
 354         x86Lea64(BaseIndex(a, b, TimesOne), dest);
 355     }
 356 
 357     void x86Lea64(BaseIndex index, RegisterID dest)
 358     {
 359         if (!index.scale &amp;&amp; !index.offset) {
 360             if (index.base == dest) {
 361                 add64(index.index, dest);
 362                 return;
 363             }
 364             if (index.index == dest) {
 365                 add64(index.base, dest);
 366                 return;
 367             }
 368         }
 369         m_assembler.leaq_mr(index.offset, index.base, index.index, index.scale, dest);
 370     }
 371 
 372     void getEffectiveAddress(BaseIndex address, RegisterID dest)
 373     {
 374         return x86Lea64(address, dest);
 375     }
 376 
 377     void addPtrNoFlags(TrustedImm32 imm, RegisterID srcDest)
 378     {
 379         m_assembler.leaq_mr(imm.m_value, srcDest, srcDest);
 380     }
 381 
 382     void and64(RegisterID src, RegisterID dest)
 383     {
 384         m_assembler.andq_rr(src, dest);
 385     }
 386 
 387     void and64(RegisterID src, Address dest)
 388     {
 389         m_assembler.andq_rm(src, dest.offset, dest.base);
 390     }
 391 
 392     void and64(RegisterID src, BaseIndex dest)
 393     {
 394         m_assembler.andq_rm(src, dest.offset, dest.base, dest.index, dest.scale);
 395     }
 396 
 397     void and64(Address src, RegisterID dest)
 398     {
 399         m_assembler.andq_mr(src.offset, src.base, dest);
 400     }
 401 
 402     void and64(BaseIndex src, RegisterID dest)
 403     {
 404         m_assembler.andq_mr(src.offset, src.base, src.index, src.scale, dest);
 405     }
 406 
 407     void and64(TrustedImm32 imm, RegisterID srcDest)
 408     {
 409         m_assembler.andq_ir(imm.m_value, srcDest);
 410     }
 411 
 412     void and64(TrustedImm32 imm, Address dest)
 413     {
 414         m_assembler.andq_im(imm.m_value, dest.offset, dest.base);
 415     }
 416 
 417     void and64(TrustedImm32 imm, BaseIndex dest)
 418     {
 419         m_assembler.andq_im(imm.m_value, dest.offset, dest.base, dest.index, dest.scale);
 420     }
 421 
 422     void and64(TrustedImmPtr imm, RegisterID srcDest)
 423     {
 424         intptr_t intValue = imm.asIntptr();
 425         if (intValue &lt;= std::numeric_limits&lt;int32_t&gt;::max()
 426             &amp;&amp; intValue &gt;= std::numeric_limits&lt;int32_t&gt;::min()) {
 427             and64(TrustedImm32(static_cast&lt;int32_t&gt;(intValue)), srcDest);
 428             return;
 429         }
 430         move(imm, scratchRegister());
 431         and64(scratchRegister(), srcDest);
 432     }
 433 
 434     void and64(RegisterID op1, RegisterID op2, RegisterID dest)
 435     {
 436         if (op1 == op2 &amp;&amp; op1 != dest &amp;&amp; op2 != dest)
 437             move(op1, dest);
 438         else if (op1 == dest)
 439             and64(op2, dest);
 440         else {
 441             move(op2, dest);
 442             and64(op1, dest);
 443         }
 444     }
 445 
 446     void countLeadingZeros64(RegisterID src, RegisterID dst)
 447     {
 448         if (supportsLZCNT()) {
 449             m_assembler.lzcntq_rr(src, dst);
 450             return;
 451         }
 452         m_assembler.bsrq_rr(src, dst);
 453         clz64AfterBsr(dst);
 454     }
 455 
 456     void countLeadingZeros64(Address src, RegisterID dst)
 457     {
 458         if (supportsLZCNT()) {
 459             m_assembler.lzcntq_mr(src.offset, src.base, dst);
 460             return;
 461         }
 462         m_assembler.bsrq_mr(src.offset, src.base, dst);
 463         clz64AfterBsr(dst);
 464     }
 465 
 466     void countTrailingZeros64(RegisterID src, RegisterID dst)
 467     {
 468         if (supportsBMI1()) {
 469             m_assembler.tzcntq_rr(src, dst);
 470             return;
 471         }
 472         m_assembler.bsfq_rr(src, dst);
 473         ctzAfterBsf&lt;64&gt;(dst);
 474     }
 475 
 476     void countPopulation64(RegisterID src, RegisterID dst)
 477     {
 478         ASSERT(supportsCountPopulation());
 479         m_assembler.popcntq_rr(src, dst);
 480     }
 481 
 482     void countPopulation64(Address src, RegisterID dst)
 483     {
 484         ASSERT(supportsCountPopulation());
 485         m_assembler.popcntq_mr(src.offset, src.base, dst);
 486     }
 487 
 488     void lshift64(TrustedImm32 imm, RegisterID dest)
 489     {
 490         m_assembler.shlq_i8r(imm.m_value, dest);
 491     }
 492 
 493     void lshift64(RegisterID src, RegisterID dest)
 494     {
 495         if (src == X86Registers::ecx)
 496             m_assembler.shlq_CLr(dest);
 497         else {
 498             ASSERT(src != dest);
 499 
 500             // Can only shift by ecx, so we do some swapping if we see anything else.
 501             swap(src, X86Registers::ecx);
 502             m_assembler.shlq_CLr(dest == X86Registers::ecx ? src : dest);
 503             swap(src, X86Registers::ecx);
 504         }
 505     }
 506 
 507     void rshift64(TrustedImm32 imm, RegisterID dest)
 508     {
 509         m_assembler.sarq_i8r(imm.m_value, dest);
 510     }
 511 
 512     void rshift64(RegisterID src, RegisterID dest)
 513     {
 514         if (src == X86Registers::ecx)
 515             m_assembler.sarq_CLr(dest);
 516         else {
 517             ASSERT(src != dest);
 518 
 519             // Can only shift by ecx, so we do some swapping if we see anything else.
 520             swap(src, X86Registers::ecx);
 521             m_assembler.sarq_CLr(dest == X86Registers::ecx ? src : dest);
 522             swap(src, X86Registers::ecx);
 523         }
 524     }
 525 
 526     void urshift64(TrustedImm32 imm, RegisterID dest)
 527     {
 528         m_assembler.shrq_i8r(imm.m_value, dest);
 529     }
 530 
 531     void urshift64(RegisterID src, RegisterID dest)
 532     {
 533         if (src == X86Registers::ecx)
 534             m_assembler.shrq_CLr(dest);
 535         else {
 536             ASSERT(src != dest);
 537 
 538             // Can only shift by ecx, so we do some swapping if we see anything else.
 539             swap(src, X86Registers::ecx);
 540             m_assembler.shrq_CLr(dest == X86Registers::ecx ? src : dest);
 541             swap(src, X86Registers::ecx);
 542         }
 543     }
 544 
 545     void rotateRight64(TrustedImm32 imm, RegisterID dest)
 546     {
 547         m_assembler.rorq_i8r(imm.m_value, dest);
 548     }
 549 
 550     void rotateRight64(RegisterID src, RegisterID dest)
 551     {
 552         if (src == X86Registers::ecx)
 553             m_assembler.rorq_CLr(dest);
 554         else {
 555             ASSERT(src != dest);
 556 
 557             // Can only rotate by ecx, so we do some swapping if we see anything else.
 558             swap(src, X86Registers::ecx);
 559             m_assembler.rorq_CLr(dest == X86Registers::ecx ? src : dest);
 560             swap(src, X86Registers::ecx);
 561         }
 562     }
 563 
 564     void rotateLeft64(TrustedImm32 imm, RegisterID dest)
 565     {
 566         m_assembler.rolq_i8r(imm.m_value, dest);
 567     }
 568 
 569     void rotateLeft64(RegisterID src, RegisterID dest)
 570     {
 571         if (src == X86Registers::ecx)
 572             m_assembler.rolq_CLr(dest);
 573         else {
 574             ASSERT(src != dest);
 575 
 576             // Can only rotate by ecx, so we do some swapping if we see anything else.
 577             swap(src, X86Registers::ecx);
 578             m_assembler.rolq_CLr(dest == X86Registers::ecx ? src : dest);
 579             swap(src, X86Registers::ecx);
 580         }
 581     }
 582 
 583     void mul64(RegisterID src, RegisterID dest)
 584     {
 585         m_assembler.imulq_rr(src, dest);
 586     }
 587 
 588     void mul64(RegisterID src1, RegisterID src2, RegisterID dest)
 589     {
 590         if (src2 == dest) {
 591             m_assembler.imulq_rr(src1, dest);
 592             return;
 593         }
 594         move(src1, dest);
 595         m_assembler.imulq_rr(src2, dest);
 596     }
 597 
 598     void x86ConvertToQuadWord64()
 599     {
 600         m_assembler.cqo();
 601     }
 602 
 603     void x86ConvertToQuadWord64(RegisterID rax, RegisterID rdx)
 604     {
 605         ASSERT_UNUSED(rax, rax == X86Registers::eax);
 606         ASSERT_UNUSED(rdx, rdx == X86Registers::edx);
 607         x86ConvertToQuadWord64();
 608     }
 609 
 610     void x86Div64(RegisterID denominator)
 611     {
 612         m_assembler.idivq_r(denominator);
 613     }
 614 
 615     void x86Div64(RegisterID rax, RegisterID rdx, RegisterID denominator)
 616     {
 617         ASSERT_UNUSED(rax, rax == X86Registers::eax);
 618         ASSERT_UNUSED(rdx, rdx == X86Registers::edx);
 619         x86Div64(denominator);
 620     }
 621 
 622     void x86UDiv64(RegisterID denominator)
 623     {
 624         m_assembler.divq_r(denominator);
 625     }
 626 
 627     void x86UDiv64(RegisterID rax, RegisterID rdx, RegisterID denominator)
 628     {
 629         ASSERT_UNUSED(rax, rax == X86Registers::eax);
 630         ASSERT_UNUSED(rdx, rdx == X86Registers::edx);
 631         x86UDiv64(denominator);
 632     }
 633 
 634     void neg64(RegisterID dest)
 635     {
 636         m_assembler.negq_r(dest);
 637     }
 638 
 639     void neg64(RegisterID src, RegisterID dest)
 640     {
 641         move(src, dest);
 642         m_assembler.negq_r(dest);
 643     }
 644 
 645     void neg64(Address dest)
 646     {
 647         m_assembler.negq_m(dest.offset, dest.base);
 648     }
 649 
 650     void neg64(BaseIndex dest)
 651     {
 652         m_assembler.negq_m(dest.offset, dest.base, dest.index, dest.scale);
 653     }
 654 
 655     void or64(RegisterID src, RegisterID dest)
 656     {
 657         m_assembler.orq_rr(src, dest);
 658     }
 659 
 660     void or64(RegisterID src, Address dest)
 661     {
 662         m_assembler.orq_rm(src, dest.offset, dest.base);
 663     }
 664 
 665     void or64(RegisterID src, BaseIndex dest)
 666     {
 667         m_assembler.orq_rm(src, dest.offset, dest.base, dest.index, dest.scale);
 668     }
 669 
 670     void or64(Address src, RegisterID dest)
 671     {
 672         m_assembler.orq_mr(src.offset, src.base, dest);
 673     }
 674 
 675     void or64(BaseIndex src, RegisterID dest)
 676     {
 677         m_assembler.orq_mr(src.offset, src.base, src.index, src.scale, dest);
 678     }
 679 
 680     void or64(TrustedImm32 imm, Address dest)
 681     {
 682         m_assembler.orq_im(imm.m_value, dest.offset, dest.base);
 683     }
 684 
 685     void or64(TrustedImm32 imm, BaseIndex dest)
 686     {
 687         m_assembler.orq_im(imm.m_value, dest.offset, dest.base, dest.index, dest.scale);
 688     }
 689 
 690     void or64(TrustedImm64 imm, RegisterID srcDest)
 691     {
 692         if (imm.m_value &lt;= std::numeric_limits&lt;int32_t&gt;::max()
 693             &amp;&amp; imm.m_value &gt;= std::numeric_limits&lt;int32_t&gt;::min()) {
 694             or64(TrustedImm32(static_cast&lt;int32_t&gt;(imm.m_value)), srcDest);
 695             return;
 696         }
 697         move(imm, scratchRegister());
 698         or64(scratchRegister(), srcDest);
 699     }
 700 
 701     void or64(TrustedImm32 imm, RegisterID dest)
 702     {
 703         m_assembler.orq_ir(imm.m_value, dest);
 704     }
 705 
 706     void or64(RegisterID op1, RegisterID op2, RegisterID dest)
 707     {
 708         if (op1 == op2)
 709             move(op1, dest);
 710         else if (op1 == dest)
 711             or64(op2, dest);
 712         else {
 713             move(op2, dest);
 714             or64(op1, dest);
 715         }
 716     }
 717 
 718     void or64(TrustedImm32 imm, RegisterID src, RegisterID dest)
 719     {
 720         move(src, dest);
 721         or64(imm, dest);
 722     }
 723 
 724     void sub64(RegisterID src, RegisterID dest)
 725     {
 726         m_assembler.subq_rr(src, dest);
 727     }
 728 
 729     void sub64(TrustedImm32 imm, RegisterID dest)
 730     {
 731         if (imm.m_value == 1)
 732             m_assembler.decq_r(dest);
 733         else
 734             m_assembler.subq_ir(imm.m_value, dest);
 735     }
 736 
 737     void sub64(TrustedImm64 imm, RegisterID dest)
 738     {
 739         if (imm.m_value == 1)
 740             m_assembler.decq_r(dest);
 741         else {
 742             move(imm, scratchRegister());
 743             sub64(scratchRegister(), dest);
 744         }
 745     }
 746 
 747     void sub64(TrustedImm32 imm, Address address)
 748     {
 749         m_assembler.subq_im(imm.m_value, address.offset, address.base);
 750     }
 751 
 752     void sub64(TrustedImm32 imm, BaseIndex address)
 753     {
 754         m_assembler.subq_im(imm.m_value, address.offset, address.base, address.index, address.scale);
 755     }
 756 
 757     void sub64(Address src, RegisterID dest)
 758     {
 759         m_assembler.subq_mr(src.offset, src.base, dest);
 760     }
 761 
 762     void sub64(BaseIndex src, RegisterID dest)
 763     {
 764         m_assembler.subq_mr(src.offset, src.base, src.index, src.scale, dest);
 765     }
 766 
 767     void sub64(RegisterID src, Address dest)
 768     {
 769         m_assembler.subq_rm(src, dest.offset, dest.base);
 770     }
 771 
 772     void sub64(RegisterID src, BaseIndex dest)
 773     {
 774         m_assembler.subq_rm(src, dest.offset, dest.base, dest.index, dest.scale);
 775     }
 776 
 777     void xor64(RegisterID src, RegisterID dest)
 778     {
 779         m_assembler.xorq_rr(src, dest);
 780     }
 781 
 782     void xor64(RegisterID op1, RegisterID op2, RegisterID dest)
 783     {
 784         if (op1 == op2)
 785             move(TrustedImm32(0), dest);
 786         else if (op1 == dest)
 787             xor64(op2, dest);
 788         else {
 789             move(op2, dest);
 790             xor64(op1, dest);
 791         }
 792     }
 793 
 794     void xor64(RegisterID src, Address dest)
 795     {
 796         m_assembler.xorq_rm(src, dest.offset, dest.base);
 797     }
 798 
 799     void xor64(RegisterID src, BaseIndex dest)
 800     {
 801         m_assembler.xorq_rm(src, dest.offset, dest.base, dest.index, dest.scale);
 802     }
 803 
 804     void xor64(Address src, RegisterID dest)
 805     {
 806         m_assembler.xorq_mr(src.offset, src.base, dest);
 807     }
 808 
 809     void xor64(BaseIndex src, RegisterID dest)
 810     {
 811         m_assembler.xorq_mr(src.offset, src.base, src.index, src.scale, dest);
 812     }
 813 
 814     void xor64(TrustedImm32 imm, Address dest)
 815     {
 816         m_assembler.xorq_im(imm.m_value, dest.offset, dest.base);
 817     }
 818 
 819     void xor64(TrustedImm32 imm, BaseIndex dest)
 820     {
 821         m_assembler.xorq_im(imm.m_value, dest.offset, dest.base, dest.index, dest.scale);
 822     }
 823 
 824     void xor64(TrustedImm32 imm, RegisterID srcDest)
 825     {
 826         m_assembler.xorq_ir(imm.m_value, srcDest);
 827     }
 828 
 829     void xor64(TrustedImm64 imm, RegisterID srcDest)
 830     {
 831         move(imm, scratchRegister());
 832         xor64(scratchRegister(), srcDest);
 833     }
 834 
 835     void not64(RegisterID srcDest)
 836     {
 837         m_assembler.notq_r(srcDest);
 838     }
 839 
 840     void not64(Address dest)
 841     {
 842         m_assembler.notq_m(dest.offset, dest.base);
 843     }
 844 
 845     void not64(BaseIndex dest)
 846     {
 847         m_assembler.notq_m(dest.offset, dest.base, dest.index, dest.scale);
 848     }
 849 
 850     void load64(ImplicitAddress address, RegisterID dest)
 851     {
 852         m_assembler.movq_mr(address.offset, address.base, dest);
 853     }
 854 
 855     void load64(BaseIndex address, RegisterID dest)
 856     {
 857         m_assembler.movq_mr(address.offset, address.base, address.index, address.scale, dest);
 858     }
 859 
 860     void load64(const void* address, RegisterID dest)
 861     {
 862         if (dest == X86Registers::eax)
 863             m_assembler.movq_mEAX(address);
 864         else {
 865             move(TrustedImmPtr(address), dest);
 866             load64(dest, dest);
 867         }
 868     }
 869 
 870     DataLabel32 load64WithAddressOffsetPatch(Address address, RegisterID dest)
 871     {
 872         padBeforePatch();
 873         m_assembler.movq_mr_disp32(address.offset, address.base, dest);
 874         return DataLabel32(this);
 875     }
 876 
 877     DataLabelCompact load64WithCompactAddressOffsetPatch(Address address, RegisterID dest)
 878     {
 879         padBeforePatch();
 880         m_assembler.movq_mr_disp8(address.offset, address.base, dest);
 881         return DataLabelCompact(this);
 882     }
 883 
 884     void store64(RegisterID src, ImplicitAddress address)
 885     {
 886         m_assembler.movq_rm(src, address.offset, address.base);
 887     }
 888 
 889     void store64(RegisterID src, BaseIndex address)
 890     {
 891         m_assembler.movq_rm(src, address.offset, address.base, address.index, address.scale);
 892     }
 893 
 894     void store64(RegisterID src, void* address)
 895     {
 896         if (src == X86Registers::eax)
 897             m_assembler.movq_EAXm(address);
 898         else {
 899             move(TrustedImmPtr(address), scratchRegister());
 900             store64(src, scratchRegister());
 901         }
 902     }
 903 
 904     void store64(TrustedImm32 imm, ImplicitAddress address)
 905     {
 906         m_assembler.movq_i32m(imm.m_value, address.offset, address.base);
 907     }
 908 
 909     void store64(TrustedImm32 imm, BaseIndex address)
 910     {
 911         m_assembler.movq_i32m(imm.m_value, address.offset, address.base, address.index, address.scale);
 912     }
 913 
 914     void store64(TrustedImm64 imm, ImplicitAddress address)
 915     {
 916         if (CAN_SIGN_EXTEND_32_64(imm.m_value)) {
 917             store64(TrustedImm32(static_cast&lt;int32_t&gt;(imm.m_value)), address);
 918             return;
 919         }
 920 
 921         move(imm, scratchRegister());
 922         store64(scratchRegister(), address);
 923     }
 924 
 925     void store64(TrustedImm64 imm, BaseIndex address)
 926     {
 927         move(imm, scratchRegister());
 928         m_assembler.movq_rm(scratchRegister(), address.offset, address.base, address.index, address.scale);
 929     }
 930 
 931     void storeZero64(ImplicitAddress address)
 932     {
 933         store64(TrustedImm32(0), address);
 934     }
 935 
 936     void storeZero64(BaseIndex address)
 937     {
 938         store64(TrustedImm32(0), address);
 939     }
 940 
 941     DataLabel32 store64WithAddressOffsetPatch(RegisterID src, Address address)
 942     {
 943         padBeforePatch();
 944         m_assembler.movq_rm_disp32(src, address.offset, address.base);
 945         return DataLabel32(this);
 946     }
 947 
 948     void swap64(RegisterID src, RegisterID dest)
 949     {
 950         m_assembler.xchgq_rr(src, dest);
 951     }
 952 
 953     void swap64(RegisterID src, Address dest)
 954     {
 955         m_assembler.xchgq_rm(src, dest.offset, dest.base);
 956     }
 957 
 958     void move64ToDouble(RegisterID src, FPRegisterID dest)
 959     {
 960         m_assembler.movq_rr(src, dest);
 961     }
 962 
 963     void moveDoubleTo64(FPRegisterID src, RegisterID dest)
 964     {
 965         m_assembler.movq_rr(src, dest);
 966     }
 967 
 968     void compare64(RelationalCondition cond, RegisterID left, TrustedImm32 right, RegisterID dest)
 969     {
 970         if (!right.m_value) {
 971             if (auto resultCondition = commuteCompareToZeroIntoTest(cond)) {
 972                 test64(*resultCondition, left, left, dest);
 973                 return;
 974             }
 975         }
 976 
 977         m_assembler.cmpq_ir(right.m_value, left);
 978         set32(x86Condition(cond), dest);
 979     }
 980 
 981     void compare64(RelationalCondition cond, RegisterID left, RegisterID right, RegisterID dest)
 982     {
 983         m_assembler.cmpq_rr(right, left);
 984         set32(x86Condition(cond), dest);
 985     }
 986 
 987     Jump branch64(RelationalCondition cond, RegisterID left, RegisterID right)
 988     {
 989         m_assembler.cmpq_rr(right, left);
 990         return Jump(m_assembler.jCC(x86Condition(cond)));
 991     }
 992 
 993     Jump branch64(RelationalCondition cond, RegisterID left, TrustedImm32 right)
 994     {
 995         if (!right.m_value) {
 996             if (auto resultCondition = commuteCompareToZeroIntoTest(cond))
 997                 return branchTest64(*resultCondition, left, left);
 998         }
 999         m_assembler.cmpq_ir(right.m_value, left);
1000         return Jump(m_assembler.jCC(x86Condition(cond)));
1001     }
1002 
1003     Jump branch64(RelationalCondition cond, RegisterID left, TrustedImm64 right)
1004     {
1005         if (((cond == Equal) || (cond == NotEqual)) &amp;&amp; !right.m_value) {
1006             m_assembler.testq_rr(left, left);
1007             return Jump(m_assembler.jCC(x86Condition(cond)));
1008         }
1009         move(right, scratchRegister());
1010         return branch64(cond, left, scratchRegister());
1011     }
1012 
1013     Jump branch64(RelationalCondition cond, RegisterID left, Address right)
1014     {
1015         m_assembler.cmpq_mr(right.offset, right.base, left);
1016         return Jump(m_assembler.jCC(x86Condition(cond)));
1017     }
1018 
1019     Jump branch64(RelationalCondition cond, AbsoluteAddress left, RegisterID right)
1020     {
1021         move(TrustedImmPtr(left.m_ptr), scratchRegister());
1022         return branch64(cond, Address(scratchRegister()), right);
1023     }
1024 
1025     Jump branch64(RelationalCondition cond, Address left, RegisterID right)
1026     {
1027         m_assembler.cmpq_rm(right, left.offset, left.base);
1028         return Jump(m_assembler.jCC(x86Condition(cond)));
1029     }
1030 
1031     Jump branch64(RelationalCondition cond, Address left, TrustedImm32 right)
1032     {
1033         m_assembler.cmpq_im(right.m_value, left.offset, left.base);
1034         return Jump(m_assembler.jCC(x86Condition(cond)));
1035     }
1036 
1037     Jump branch64(RelationalCondition cond, Address left, TrustedImm64 right)
1038     {
1039         move(right, scratchRegister());
1040         return branch64(cond, left, scratchRegister());
1041     }
1042 
1043     Jump branch64(RelationalCondition cond, BaseIndex address, RegisterID right)
1044     {
1045         m_assembler.cmpq_rm(right, address.offset, address.base, address.index, address.scale);
1046         return Jump(m_assembler.jCC(x86Condition(cond)));
1047     }
1048 
1049     Jump branch32(RelationalCondition cond, AbsoluteAddress left, RegisterID right)
1050     {
1051         load32(left.m_ptr, scratchRegister());
1052         return branch32(cond, scratchRegister(), right);
1053     }
1054 
1055     Jump branchPtr(RelationalCondition cond, BaseIndex left, RegisterID right)
1056     {
1057         return branch64(cond, left, right);
1058     }
1059 
1060     Jump branchPtr(RelationalCondition cond, BaseIndex left, TrustedImmPtr right)
1061     {
1062         move(right, scratchRegister());
1063         return branchPtr(cond, left, scratchRegister());
1064     }
1065 
1066     Jump branchTest64(ResultCondition cond, RegisterID reg, RegisterID mask)
1067     {
1068         m_assembler.testq_rr(reg, mask);
1069         return Jump(m_assembler.jCC(x86Condition(cond)));
1070     }
1071 
1072     Jump branchTest64(ResultCondition cond, RegisterID reg, TrustedImm32 mask = TrustedImm32(-1))
1073     {
1074         // if we are only interested in the low seven bits, this can be tested with a testb
1075         if (mask.m_value == -1)
1076             m_assembler.testq_rr(reg, reg);
1077         else if ((mask.m_value &amp; ~0x7f) == 0)
1078             m_assembler.testb_i8r(mask.m_value, reg);
1079         else
1080             m_assembler.testq_i32r(mask.m_value, reg);
1081         return Jump(m_assembler.jCC(x86Condition(cond)));
1082     }
1083 
1084     Jump branchTest64(ResultCondition cond, RegisterID reg, TrustedImm64 mask)
1085     {
1086         move(mask, scratchRegister());
1087         return branchTest64(cond, reg, scratchRegister());
1088     }
1089 
1090     Jump branchTestBit64(ResultCondition cond, RegisterID testValue, TrustedImm32 bit)
1091     {
1092         m_assembler.btw_ir(static_cast&lt;unsigned&gt;(bit.m_value) % 64, testValue);
1093         if (cond == NonZero)
1094             return Jump(m_assembler.jb());
1095         if (cond == Zero)
1096             return Jump(m_assembler.jae());
1097         RELEASE_ASSERT_NOT_REACHED();
1098     }
1099 
1100     Jump branchTestBit64(ResultCondition cond, Address testValue, TrustedImm32 bit)
1101     {
1102         m_assembler.btw_im(static_cast&lt;unsigned&gt;(bit.m_value) % 64, testValue.offset, testValue.base);
1103         if (cond == NonZero)
1104             return Jump(m_assembler.jb());
1105         if (cond == Zero)
1106             return Jump(m_assembler.jae());
1107         RELEASE_ASSERT_NOT_REACHED();
1108     }
1109 
1110     Jump branchTestBit64(ResultCondition cond, RegisterID reg, RegisterID bit)
1111     {
1112         m_assembler.btw_ir(bit, reg);
1113         if (cond == NonZero)
1114             return Jump(m_assembler.jb());
1115         if (cond == Zero)
1116             return Jump(m_assembler.jae());
1117         RELEASE_ASSERT_NOT_REACHED();
1118     }
1119 
1120     void test64(ResultCondition cond, RegisterID reg, TrustedImm32 mask, RegisterID dest)
1121     {
1122         if (mask.m_value == -1)
1123             m_assembler.testq_rr(reg, reg);
1124         else if ((mask.m_value &amp; ~0x7f) == 0)
1125             m_assembler.testb_i8r(mask.m_value, reg);
1126         else
1127             m_assembler.testq_i32r(mask.m_value, reg);
1128         set32(x86Condition(cond), dest);
1129     }
1130 
1131     void test64(ResultCondition cond, RegisterID reg, RegisterID mask, RegisterID dest)
1132     {
1133         m_assembler.testq_rr(reg, mask);
1134         set32(x86Condition(cond), dest);
1135     }
1136 
1137     Jump branchTest64(ResultCondition cond, AbsoluteAddress address, TrustedImm32 mask = TrustedImm32(-1))
1138     {
1139         load64(address.m_ptr, scratchRegister());
1140         return branchTest64(cond, scratchRegister(), mask);
1141     }
1142 
1143     Jump branchTest64(ResultCondition cond, Address address, TrustedImm32 mask = TrustedImm32(-1))
1144     {
1145         if (mask.m_value == -1)
1146             m_assembler.cmpq_im(0, address.offset, address.base);
1147         else
1148             m_assembler.testq_i32m(mask.m_value, address.offset, address.base);
1149         return Jump(m_assembler.jCC(x86Condition(cond)));
1150     }
1151 
1152     Jump branchTest64(ResultCondition cond, Address address, RegisterID reg)
1153     {
1154         m_assembler.testq_rm(reg, address.offset, address.base);
1155         return Jump(m_assembler.jCC(x86Condition(cond)));
1156     }
1157 
1158     Jump branchTest64(ResultCondition cond, BaseIndex address, TrustedImm32 mask = TrustedImm32(-1))
1159     {
1160         if (mask.m_value == -1)
1161             m_assembler.cmpq_im(0, address.offset, address.base, address.index, address.scale);
1162         else
1163             m_assembler.testq_i32m(mask.m_value, address.offset, address.base, address.index, address.scale);
1164         return Jump(m_assembler.jCC(x86Condition(cond)));
1165     }
1166 
1167 
1168     Jump branchAdd64(ResultCondition cond, TrustedImm32 imm, RegisterID dest)
1169     {
1170         add64(imm, dest);
1171         return Jump(m_assembler.jCC(x86Condition(cond)));
1172     }
1173 
1174     Jump branchAdd64(ResultCondition cond, RegisterID src1, RegisterID src2, RegisterID dest)
1175     {
1176         if (src1 == dest)
1177             return branchAdd64(cond, src2, dest);
1178         move(src2, dest);
1179         return branchAdd64(cond, src1, dest);
1180     }
1181 
1182     Jump branchAdd64(ResultCondition cond, Address op1, RegisterID op2, RegisterID dest)
1183     {
1184         if (op2 == dest)
1185             return branchAdd64(cond, op1, dest);
1186         if (op1.base == dest) {
1187             load32(op1, dest);
1188             return branchAdd64(cond, op2, dest);
1189         }
1190         move(op2, dest);
1191         return branchAdd64(cond, op1, dest);
1192     }
1193 
1194     Jump branchAdd64(ResultCondition cond, RegisterID src1, Address src2, RegisterID dest)
1195     {
1196         return branchAdd64(cond, src2, src1, dest);
1197     }
1198 
1199     Jump branchAdd64(ResultCondition cond, RegisterID src, RegisterID dest)
1200     {
1201         add64(src, dest);
1202         return Jump(m_assembler.jCC(x86Condition(cond)));
1203     }
1204 
1205     Jump branchAdd64(ResultCondition cond, Address src, RegisterID dest)
1206     {
1207         add64(src, dest);
1208         return Jump(m_assembler.jCC(x86Condition(cond)));
1209     }
1210 
1211     Jump branchMul64(ResultCondition cond, RegisterID src, RegisterID dest)
1212     {
1213         mul64(src, dest);
1214         if (cond != Overflow)
1215             m_assembler.testq_rr(dest, dest);
1216         return Jump(m_assembler.jCC(x86Condition(cond)));
1217     }
1218 
1219     Jump branchMul64(ResultCondition cond, RegisterID src1, RegisterID src2, RegisterID dest)
1220     {
1221         if (src1 == dest)
1222             return branchMul64(cond, src2, dest);
1223         move(src2, dest);
1224         return branchMul64(cond, src1, dest);
1225     }
1226 
1227     Jump branchSub64(ResultCondition cond, TrustedImm32 imm, RegisterID dest)
1228     {
1229         sub64(imm, dest);
1230         return Jump(m_assembler.jCC(x86Condition(cond)));
1231     }
1232 
1233     Jump branchSub64(ResultCondition cond, RegisterID src, RegisterID dest)
1234     {
1235         sub64(src, dest);
1236         return Jump(m_assembler.jCC(x86Condition(cond)));
1237     }
1238 
1239     Jump branchSub64(ResultCondition cond, RegisterID src1, TrustedImm32 src2, RegisterID dest)
1240     {
1241         move(src1, dest);
1242         return branchSub64(cond, src2, dest);
1243     }
1244 
1245     Jump branchNeg64(ResultCondition cond, RegisterID srcDest)
1246     {
1247         neg64(srcDest);
1248         return Jump(m_assembler.jCC(x86Condition(cond)));
1249     }
1250 
1251     void moveConditionally64(RelationalCondition cond, RegisterID left, RegisterID right, RegisterID src, RegisterID dest)
1252     {
1253         m_assembler.cmpq_rr(right, left);
1254         cmov(x86Condition(cond), src, dest);
1255     }
1256 
1257     void moveConditionally64(RelationalCondition cond, RegisterID left, RegisterID right, RegisterID thenCase, RegisterID elseCase, RegisterID dest)
1258     {
1259         m_assembler.cmpq_rr(right, left);
1260 
1261         if (thenCase != dest &amp;&amp; elseCase != dest) {
1262             move(elseCase, dest);
1263             elseCase = dest;
1264         }
1265 
1266         if (elseCase == dest)
1267             cmov(x86Condition(cond), thenCase, dest);
1268         else
1269             cmov(x86Condition(invert(cond)), elseCase, dest);
1270     }
1271 
1272     void moveConditionally64(RelationalCondition cond, RegisterID left, TrustedImm32 right, RegisterID thenCase, RegisterID elseCase, RegisterID dest)
1273     {
1274         if (!right.m_value) {
1275             if (auto resultCondition = commuteCompareToZeroIntoTest(cond)) {
1276                 moveConditionallyTest64(*resultCondition, left, left, thenCase, elseCase, dest);
1277                 return;
1278             }
1279         }
1280 
1281         m_assembler.cmpq_ir(right.m_value, left);
1282 
1283         if (thenCase != dest &amp;&amp; elseCase != dest) {
1284             move(elseCase, dest);
1285             elseCase = dest;
1286         }
1287 
1288         if (elseCase == dest)
1289             cmov(x86Condition(cond), thenCase, dest);
1290         else
1291             cmov(x86Condition(invert(cond)), elseCase, dest);
1292     }
1293 
1294     void moveConditionallyTest64(ResultCondition cond, RegisterID testReg, RegisterID mask, RegisterID src, RegisterID dest)
1295     {
1296         m_assembler.testq_rr(testReg, mask);
1297         cmov(x86Condition(cond), src, dest);
1298     }
1299 
1300     void moveConditionallyTest64(ResultCondition cond, RegisterID left, RegisterID right, RegisterID thenCase, RegisterID elseCase, RegisterID dest)
1301     {
1302         ASSERT(isInvertible(cond));
1303         ASSERT_WITH_MESSAGE(cond != Overflow, &quot;TEST does not set the Overflow Flag.&quot;);
1304 
1305         m_assembler.testq_rr(right, left);
1306 
1307         if (thenCase != dest &amp;&amp; elseCase != dest) {
1308             move(elseCase, dest);
1309             elseCase = dest;
1310         }
1311 
1312         if (elseCase == dest)
1313             cmov(x86Condition(cond), thenCase, dest);
1314         else
1315             cmov(x86Condition(invert(cond)), elseCase, dest);
1316     }
1317 
1318     void moveConditionallyTest64(ResultCondition cond, RegisterID testReg, TrustedImm32 mask, RegisterID src, RegisterID dest)
1319     {
1320         // if we are only interested in the low seven bits, this can be tested with a testb
1321         if (mask.m_value == -1)
1322             m_assembler.testq_rr(testReg, testReg);
1323         else if ((mask.m_value &amp; ~0x7f) == 0)
1324             m_assembler.testb_i8r(mask.m_value, testReg);
1325         else
1326             m_assembler.testq_i32r(mask.m_value, testReg);
1327         cmov(x86Condition(cond), src, dest);
1328     }
1329 
1330     void moveConditionallyTest64(ResultCondition cond, RegisterID testReg, TrustedImm32 mask, RegisterID thenCase, RegisterID elseCase, RegisterID dest)
1331     {
1332         ASSERT(isInvertible(cond));
1333         ASSERT_WITH_MESSAGE(cond != Overflow, &quot;TEST does not set the Overflow Flag.&quot;);
1334 
1335         if (mask.m_value == -1)
1336             m_assembler.testq_rr(testReg, testReg);
1337         else if (!(mask.m_value &amp; ~0x7f))
1338             m_assembler.testb_i8r(mask.m_value, testReg);
1339         else
1340             m_assembler.testq_i32r(mask.m_value, testReg);
1341 
1342         if (thenCase != dest &amp;&amp; elseCase != dest) {
1343             move(elseCase, dest);
1344             elseCase = dest;
1345         }
1346 
1347         if (elseCase == dest)
1348             cmov(x86Condition(cond), thenCase, dest);
1349         else
1350             cmov(x86Condition(invert(cond)), elseCase, dest);
1351     }
1352 
1353     template&lt;typename LeftType, typename RightType&gt;
1354     void moveDoubleConditionally64(RelationalCondition cond, LeftType left, RightType right, FPRegisterID thenCase, FPRegisterID elseCase, FPRegisterID dest)
1355     {
1356         static_assert(!std::is_same&lt;LeftType, FPRegisterID&gt;::value &amp;&amp; !std::is_same&lt;RightType, FPRegisterID&gt;::value, &quot;One of the tested argument could be aliased on dest. Use moveDoubleConditionallyDouble().&quot;);
1357 
1358         if (thenCase != dest &amp;&amp; elseCase != dest) {
1359             moveDouble(elseCase, dest);
1360             elseCase = dest;
1361         }
1362 
1363         if (elseCase == dest) {
1364             Jump falseCase = branch64(invert(cond), left, right);
1365             moveDouble(thenCase, dest);
1366             falseCase.link(this);
1367         } else {
1368             Jump trueCase = branch64(cond, left, right);
1369             moveDouble(elseCase, dest);
1370             trueCase.link(this);
1371         }
1372     }
1373 
1374     template&lt;typename TestType, typename MaskType&gt;
1375     void moveDoubleConditionallyTest64(ResultCondition cond, TestType test, MaskType mask, FPRegisterID thenCase, FPRegisterID elseCase, FPRegisterID dest)
1376     {
1377         static_assert(!std::is_same&lt;TestType, FPRegisterID&gt;::value &amp;&amp; !std::is_same&lt;MaskType, FPRegisterID&gt;::value, &quot;One of the tested argument could be aliased on dest. Use moveDoubleConditionallyDouble().&quot;);
1378 
1379         if (elseCase == dest &amp;&amp; isInvertible(cond)) {
1380             Jump falseCase = branchTest64(invert(cond), test, mask);
1381             moveDouble(thenCase, dest);
1382             falseCase.link(this);
1383         } else if (thenCase == dest) {
1384             Jump trueCase = branchTest64(cond, test, mask);
1385             moveDouble(elseCase, dest);
1386             trueCase.link(this);
1387         }
1388 
1389         Jump trueCase = branchTest64(cond, test, mask);
1390         moveDouble(elseCase, dest);
1391         Jump falseCase = jump();
1392         trueCase.link(this);
1393         moveDouble(thenCase, dest);
1394         falseCase.link(this);
1395     }
1396 
1397     void abortWithReason(AbortReason reason)
1398     {
1399         move(TrustedImm32(reason), X86Registers::r11);
1400         breakpoint();
1401     }
1402 
1403     void abortWithReason(AbortReason reason, intptr_t misc)
1404     {
1405         move(TrustedImm64(misc), X86Registers::r10);
1406         abortWithReason(reason);
1407     }
1408 
1409     ConvertibleLoadLabel convertibleLoadPtr(Address address, RegisterID dest)
1410     {
1411         ConvertibleLoadLabel result = ConvertibleLoadLabel(this);
1412         m_assembler.movq_mr(address.offset, address.base, dest);
1413         return result;
1414     }
1415 
1416     DataLabelPtr moveWithPatch(TrustedImmPtr initialValue, RegisterID dest)
1417     {
1418         padBeforePatch();
1419         m_assembler.movq_i64r(initialValue.asIntptr(), dest);
1420         return DataLabelPtr(this);
1421     }
1422 
1423     DataLabelPtr moveWithPatch(TrustedImm32 initialValue, RegisterID dest)
1424     {
1425         padBeforePatch();
1426         m_assembler.movq_i64r(initialValue.m_value, dest);
1427         return DataLabelPtr(this);
1428     }
1429 
1430     Jump branchPtrWithPatch(RelationalCondition cond, RegisterID left, DataLabelPtr&amp; dataLabel, TrustedImmPtr initialRightValue = TrustedImmPtr(nullptr))
1431     {
1432         dataLabel = moveWithPatch(initialRightValue, scratchRegister());
1433         return branch64(cond, left, scratchRegister());
1434     }
1435 
1436     Jump branchPtrWithPatch(RelationalCondition cond, Address left, DataLabelPtr&amp; dataLabel, TrustedImmPtr initialRightValue = TrustedImmPtr(nullptr))
1437     {
1438         dataLabel = moveWithPatch(initialRightValue, scratchRegister());
1439         return branch64(cond, left, scratchRegister());
1440     }
1441 
1442     Jump branch32WithPatch(RelationalCondition cond, Address left, DataLabel32&amp; dataLabel, TrustedImm32 initialRightValue = TrustedImm32(0))
1443     {
1444         padBeforePatch();
1445         m_assembler.movl_i32r(initialRightValue.m_value, scratchRegister());
1446         dataLabel = DataLabel32(this);
1447         return branch32(cond, left, scratchRegister());
1448     }
1449 
1450     DataLabelPtr storePtrWithPatch(TrustedImmPtr initialValue, ImplicitAddress address)
1451     {
1452         DataLabelPtr label = moveWithPatch(initialValue, scratchRegister());
1453         store64(scratchRegister(), address);
1454         return label;
1455     }
1456 
1457     PatchableJump patchableBranch64(RelationalCondition cond, RegisterID reg, TrustedImm64 imm)
1458     {
1459         return PatchableJump(branch64(cond, reg, imm));
1460     }
1461 
1462     PatchableJump patchableBranch64(RelationalCondition cond, RegisterID left, RegisterID right)
1463     {
1464         return PatchableJump(branch64(cond, left, right));
1465     }
1466 
1467     using MacroAssemblerX86Common::branch8;
1468     Jump branch8(RelationalCondition cond, AbsoluteAddress left, TrustedImm32 right)
1469     {
1470         MacroAssemblerX86Common::move(TrustedImmPtr(left.m_ptr), scratchRegister());
1471         return MacroAssemblerX86Common::branch8(cond, Address(scratchRegister()), right);
1472     }
1473 
1474     using MacroAssemblerX86Common::branchTest8;
1475     Jump branchTest8(ResultCondition cond, ExtendedAddress address, TrustedImm32 mask = TrustedImm32(-1))
1476     {
1477         TrustedImm32 mask8(static_cast&lt;int8_t&gt;(mask.m_value));
1478         TrustedImmPtr addr(reinterpret_cast&lt;void*&gt;(address.offset));
1479         MacroAssemblerX86Common::move(addr, scratchRegister());
1480         return MacroAssemblerX86Common::branchTest8(cond, BaseIndex(scratchRegister(), address.base, TimesOne), mask8);
1481     }
1482 
1483     Jump branchTest8(ResultCondition cond, AbsoluteAddress address, TrustedImm32 mask = TrustedImm32(-1))
1484     {
1485         TrustedImm32 mask8(static_cast&lt;int8_t&gt;(mask.m_value));
1486         MacroAssemblerX86Common::move(TrustedImmPtr(address.m_ptr), scratchRegister());
1487         return MacroAssemblerX86Common::branchTest8(cond, Address(scratchRegister()), mask8);
1488     }
1489 
1490     void xchg64(RegisterID reg, Address address)
1491     {
1492         m_assembler.xchgq_rm(reg, address.offset, address.base);
1493     }
1494 
1495     void xchg64(RegisterID reg, BaseIndex address)
1496     {
1497         m_assembler.xchgq_rm(reg, address.offset, address.base, address.index, address.scale);
1498     }
1499 
1500     void atomicStrongCAS64(StatusCondition cond, RegisterID expectedAndResult, RegisterID newValue, Address address, RegisterID result)
1501     {
1502         atomicStrongCAS(cond, expectedAndResult, result, address, [&amp;] { m_assembler.cmpxchgq_rm(newValue, address.offset, address.base); });
1503     }
1504 
1505     void atomicStrongCAS64(StatusCondition cond, RegisterID expectedAndResult, RegisterID newValue, BaseIndex address, RegisterID result)
1506     {
1507         atomicStrongCAS(cond, expectedAndResult, result, address, [&amp;] { m_assembler.cmpxchgq_rm(newValue, address.offset, address.base, address.index, address.scale); });
1508     }
1509 
1510     void atomicStrongCAS64(RegisterID expectedAndResult, RegisterID newValue, Address address)
1511     {
1512         atomicStrongCAS(expectedAndResult, address, [&amp;] { m_assembler.cmpxchgq_rm(newValue, address.offset, address.base); });
1513     }
1514 
1515     void atomicStrongCAS64(RegisterID expectedAndResult, RegisterID newValue, BaseIndex address)
1516     {
1517         atomicStrongCAS(expectedAndResult, address, [&amp;] { m_assembler.cmpxchgq_rm(newValue, address.offset, address.base, address.index, address.scale); });
1518     }
1519 
1520     Jump branchAtomicStrongCAS64(StatusCondition cond, RegisterID expectedAndResult, RegisterID newValue, Address address)
1521     {
1522         return branchAtomicStrongCAS(cond, expectedAndResult, address, [&amp;] { m_assembler.cmpxchgq_rm(newValue, address.offset, address.base); });
1523     }
1524 
1525     Jump branchAtomicStrongCAS64(StatusCondition cond, RegisterID expectedAndResult, RegisterID newValue, BaseIndex address)
1526     {
1527         return branchAtomicStrongCAS(cond, expectedAndResult, address, [&amp;] { m_assembler.cmpxchgq_rm(newValue, address.offset, address.base, address.index, address.scale); });
1528     }
1529 
1530     void atomicWeakCAS64(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, Address address, RegisterID result)
1531     {
1532         atomicStrongCAS64(cond, expectedAndClobbered, newValue, address, result);
1533     }
1534 
1535     void atomicWeakCAS64(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, BaseIndex address, RegisterID result)
1536     {
1537         atomicStrongCAS64(cond, expectedAndClobbered, newValue, address, result);
1538     }
1539 
1540     Jump branchAtomicWeakCAS64(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, Address address)
1541     {
1542         return branchAtomicStrongCAS64(cond, expectedAndClobbered, newValue, address);
1543     }
1544 
1545     Jump branchAtomicWeakCAS64(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, BaseIndex address)
1546     {
1547         return branchAtomicStrongCAS64(cond, expectedAndClobbered, newValue, address);
1548     }
1549 
1550     void atomicRelaxedWeakCAS64(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, Address address, RegisterID result)
1551     {
1552         atomicStrongCAS64(cond, expectedAndClobbered, newValue, address, result);
1553     }
1554 
1555     void atomicRelaxedWeakCAS64(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, BaseIndex address, RegisterID result)
1556     {
1557         atomicStrongCAS64(cond, expectedAndClobbered, newValue, address, result);
1558     }
1559 
1560     Jump branchAtomicRelaxedWeakCAS64(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, Address address)
1561     {
1562         return branchAtomicStrongCAS64(cond, expectedAndClobbered, newValue, address);
1563     }
1564 
1565     Jump branchAtomicRelaxedWeakCAS64(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, BaseIndex address)
1566     {
1567         return branchAtomicStrongCAS64(cond, expectedAndClobbered, newValue, address);
1568     }
1569 
1570     void atomicAdd64(TrustedImm32 imm, Address address)
1571     {
1572         m_assembler.lock();
1573         add64(imm, address);
1574     }
1575 
1576     void atomicAdd64(TrustedImm32 imm, BaseIndex address)
1577     {
1578         m_assembler.lock();
1579         add64(imm, address);
1580     }
1581 
1582     void atomicAdd64(RegisterID reg, Address address)
1583     {
1584         m_assembler.lock();
1585         add64(reg, address);
1586     }
1587 
1588     void atomicAdd64(RegisterID reg, BaseIndex address)
1589     {
1590         m_assembler.lock();
1591         add64(reg, address);
1592     }
1593 
1594     void atomicSub64(TrustedImm32 imm, Address address)
1595     {
1596         m_assembler.lock();
1597         sub64(imm, address);
1598     }
1599 
1600     void atomicSub64(TrustedImm32 imm, BaseIndex address)
1601     {
1602         m_assembler.lock();
1603         sub64(imm, address);
1604     }
1605 
1606     void atomicSub64(RegisterID reg, Address address)
1607     {
1608         m_assembler.lock();
1609         sub64(reg, address);
1610     }
1611 
1612     void atomicSub64(RegisterID reg, BaseIndex address)
1613     {
1614         m_assembler.lock();
1615         sub64(reg, address);
1616     }
1617 
1618     void atomicAnd64(TrustedImm32 imm, Address address)
1619     {
1620         m_assembler.lock();
1621         and64(imm, address);
1622     }
1623 
1624     void atomicAnd64(TrustedImm32 imm, BaseIndex address)
1625     {
1626         m_assembler.lock();
1627         and64(imm, address);
1628     }
1629 
1630     void atomicAnd64(RegisterID reg, Address address)
1631     {
1632         m_assembler.lock();
1633         and64(reg, address);
1634     }
1635 
1636     void atomicAnd64(RegisterID reg, BaseIndex address)
1637     {
1638         m_assembler.lock();
1639         and64(reg, address);
1640     }
1641 
1642     void atomicOr64(TrustedImm32 imm, Address address)
1643     {
1644         m_assembler.lock();
1645         or64(imm, address);
1646     }
1647 
1648     void atomicOr64(TrustedImm32 imm, BaseIndex address)
1649     {
1650         m_assembler.lock();
1651         or64(imm, address);
1652     }
1653 
1654     void atomicOr64(RegisterID reg, Address address)
1655     {
1656         m_assembler.lock();
1657         or64(reg, address);
1658     }
1659 
1660     void atomicOr64(RegisterID reg, BaseIndex address)
1661     {
1662         m_assembler.lock();
1663         or64(reg, address);
1664     }
1665 
1666     void atomicXor64(TrustedImm32 imm, Address address)
1667     {
1668         m_assembler.lock();
1669         xor64(imm, address);
1670     }
1671 
1672     void atomicXor64(TrustedImm32 imm, BaseIndex address)
1673     {
1674         m_assembler.lock();
1675         xor64(imm, address);
1676     }
1677 
1678     void atomicXor64(RegisterID reg, Address address)
1679     {
1680         m_assembler.lock();
1681         xor64(reg, address);
1682     }
1683 
1684     void atomicXor64(RegisterID reg, BaseIndex address)
1685     {
1686         m_assembler.lock();
1687         xor64(reg, address);
1688     }
1689 
1690     void atomicNeg64(Address address)
1691     {
1692         m_assembler.lock();
1693         neg64(address);
1694     }
1695 
1696     void atomicNeg64(BaseIndex address)
1697     {
1698         m_assembler.lock();
1699         neg64(address);
1700     }
1701 
1702     void atomicNot64(Address address)
1703     {
1704         m_assembler.lock();
1705         not64(address);
1706     }
1707 
1708     void atomicNot64(BaseIndex address)
1709     {
1710         m_assembler.lock();
1711         not64(address);
1712     }
1713 
1714     void atomicXchgAdd64(RegisterID reg, Address address)
1715     {
1716         m_assembler.lock();
1717         m_assembler.xaddq_rm(reg, address.offset, address.base);
1718     }
1719 
1720     void atomicXchgAdd64(RegisterID reg, BaseIndex address)
1721     {
1722         m_assembler.lock();
1723         m_assembler.xaddq_rm(reg, address.offset, address.base, address.index, address.scale);
1724     }
1725 
1726     void atomicXchg64(RegisterID reg, Address address)
1727     {
1728         m_assembler.lock();
1729         m_assembler.xchgq_rm(reg, address.offset, address.base);
1730     }
1731 
1732     void atomicXchg64(RegisterID reg, BaseIndex address)
1733     {
1734         m_assembler.lock();
1735         m_assembler.xchgq_rm(reg, address.offset, address.base, address.index, address.scale);
1736     }
1737 
1738 #if ENABLE(FAST_TLS_JIT)
1739     void loadFromTLS64(uint32_t offset, RegisterID dst)
1740     {
1741         m_assembler.gs();
1742         m_assembler.movq_mr(offset, dst);
1743     }
1744 
1745     void storeToTLS64(RegisterID src, uint32_t offset)
1746     {
1747         m_assembler.gs();
1748         m_assembler.movq_rm(src, offset);
1749     }
1750 #endif
1751 
1752     void truncateDoubleToUint32(FPRegisterID src, RegisterID dest)
1753     {
1754         m_assembler.cvttsd2siq_rr(src, dest);
1755     }
1756 
1757     void truncateDoubleToInt64(FPRegisterID src, RegisterID dest)
1758     {
1759         m_assembler.cvttsd2siq_rr(src, dest);
1760     }
1761 
1762     // int64Min should contain exactly 0x43E0000000000000 == static_cast&lt;double&gt;(int64_t::min()). scratch may
1763     // be the same FPR as src.
1764     void truncateDoubleToUint64(FPRegisterID src, RegisterID dest, FPRegisterID scratch, FPRegisterID int64Min)
1765     {
1766         ASSERT(scratch != int64Min);
1767 
1768         // Since X86 does not have a floating point to unsigned integer instruction, we need to use the signed
1769         // integer conversion instruction. If the src is less than int64_t::min() then the results of the two
1770         // instructions are the same. Otherwise, we need to: subtract int64_t::min(); truncate double to
1771         // uint64_t; then add back int64_t::min() in the destination gpr.
1772 
1773         Jump large = branchDouble(DoubleGreaterThanOrEqual, src, int64Min);
1774         m_assembler.cvttsd2siq_rr(src, dest);
1775         Jump done = jump();
1776         large.link(this);
1777         moveDouble(src, scratch);
1778         m_assembler.subsd_rr(int64Min, scratch);
1779         m_assembler.movq_i64r(0x8000000000000000, scratchRegister());
1780         m_assembler.cvttsd2siq_rr(scratch, dest);
1781         m_assembler.orq_rr(scratchRegister(), dest);
1782         done.link(this);
1783     }
1784 
1785     void truncateFloatToUint32(FPRegisterID src, RegisterID dest)
1786     {
1787         m_assembler.cvttss2siq_rr(src, dest);
1788     }
1789 
1790     void truncateFloatToInt64(FPRegisterID src, RegisterID dest)
1791     {
1792         m_assembler.cvttss2siq_rr(src, dest);
1793     }
1794 
1795     // int64Min should contain exactly 0x5f000000 == static_cast&lt;float&gt;(int64_t::min()). scratch may be the
1796     // same FPR as src.
1797     void truncateFloatToUint64(FPRegisterID src, RegisterID dest, FPRegisterID scratch, FPRegisterID int64Min)
1798     {
1799         ASSERT(scratch != int64Min);
1800 
1801         // Since X86 does not have a floating point to unsigned integer instruction, we need to use the signed
1802         // integer conversion instruction. If the src is less than int64_t::min() then the results of the two
1803         // instructions are the same. Otherwise, we need to: subtract int64_t::min(); truncate double to
1804         // uint64_t; then add back int64_t::min() in the destination gpr.
1805 
1806         Jump large = branchFloat(DoubleGreaterThanOrEqual, src, int64Min);
1807         m_assembler.cvttss2siq_rr(src, dest);
1808         Jump done = jump();
1809         large.link(this);
1810         moveDouble(src, scratch);
1811         m_assembler.subss_rr(int64Min, scratch);
1812         m_assembler.movq_i64r(0x8000000000000000, scratchRegister());
1813         m_assembler.cvttss2siq_rr(scratch, dest);
1814         m_assembler.orq_rr(scratchRegister(), dest);
1815         done.link(this);
1816     }
1817 
1818     void convertInt64ToDouble(RegisterID src, FPRegisterID dest)
1819     {
1820         m_assembler.cvtsi2sdq_rr(src, dest);
1821     }
1822 
1823     void convertInt64ToDouble(Address src, FPRegisterID dest)
1824     {
1825         m_assembler.cvtsi2sdq_mr(src.offset, src.base, dest);
1826     }
1827 
1828     void convertInt64ToFloat(RegisterID src, FPRegisterID dest)
1829     {
1830         m_assembler.cvtsi2ssq_rr(src, dest);
1831     }
1832 
1833     void convertInt64ToFloat(Address src, FPRegisterID dest)
1834     {
1835         m_assembler.cvtsi2ssq_mr(src.offset, src.base, dest);
1836     }
1837 
1838     // One of scratch or scratch2 may be the same as src
1839     void convertUInt64ToDouble(RegisterID src, FPRegisterID dest, RegisterID scratch)
1840     {
1841         RegisterID scratch2 = scratchRegister();
1842 
1843         m_assembler.testq_rr(src, src);
1844         AssemblerLabel signBitSet = m_assembler.jCC(x86Condition(Signed));
1845         m_assembler.cvtsi2sdq_rr(src, dest);
1846         AssemblerLabel done = m_assembler.jmp();
1847         m_assembler.linkJump(signBitSet, m_assembler.label());
1848         if (scratch != src)
1849             m_assembler.movq_rr(src, scratch);
1850         m_assembler.movq_rr(src, scratch2);
1851         m_assembler.shrq_i8r(1, scratch);
1852         m_assembler.andq_ir(1, scratch2);
1853         m_assembler.orq_rr(scratch, scratch2);
1854         m_assembler.cvtsi2sdq_rr(scratch2, dest);
1855         m_assembler.addsd_rr(dest, dest);
1856         m_assembler.linkJump(done, m_assembler.label());
1857     }
1858 
1859     // One of scratch or scratch2 may be the same as src
1860     void convertUInt64ToFloat(RegisterID src, FPRegisterID dest, RegisterID scratch)
1861     {
1862         RegisterID scratch2 = scratchRegister();
1863         m_assembler.testq_rr(src, src);
1864         AssemblerLabel signBitSet = m_assembler.jCC(x86Condition(Signed));
1865         m_assembler.cvtsi2ssq_rr(src, dest);
1866         AssemblerLabel done = m_assembler.jmp();
1867         m_assembler.linkJump(signBitSet, m_assembler.label());
1868         if (scratch != src)
1869             m_assembler.movq_rr(src, scratch);
1870         m_assembler.movq_rr(src, scratch2);
1871         m_assembler.shrq_i8r(1, scratch);
1872         m_assembler.andq_ir(1, scratch2);
1873         m_assembler.orq_rr(scratch, scratch2);
1874         m_assembler.cvtsi2ssq_rr(scratch2, dest);
1875         m_assembler.addss_rr(dest, dest);
1876         m_assembler.linkJump(done, m_assembler.label());
1877     }
1878 
1879     static bool supportsFloatingPoint() { return true; }
1880     static bool supportsFloatingPointTruncate() { return true; }
1881     static bool supportsFloatingPointSqrt() { return true; }
1882     static bool supportsFloatingPointAbs() { return true; }
1883 
1884     template&lt;PtrTag resultTag, PtrTag locationTag&gt;
1885     static FunctionPtr&lt;resultTag&gt; readCallTarget(CodeLocationCall&lt;locationTag&gt; call)
1886     {
1887         return FunctionPtr&lt;resultTag&gt;(X86Assembler::readPointer(call.dataLabelPtrAtOffset(-REPATCH_OFFSET_CALL_R11).dataLocation()));
1888     }
1889 
1890     bool haveScratchRegisterForBlinding() { return m_allowScratchRegister; }
1891     RegisterID scratchRegisterForBlinding() { return scratchRegister(); }
1892 
1893     static bool canJumpReplacePatchableBranchPtrWithPatch() { return true; }
1894     static bool canJumpReplacePatchableBranch32WithPatch() { return true; }
1895 
1896     template&lt;PtrTag tag&gt;
1897     static CodeLocationLabel&lt;tag&gt; startOfBranchPtrWithPatchOnRegister(CodeLocationDataLabelPtr&lt;tag&gt; label)
1898     {
1899         const int rexBytes = 1;
1900         const int opcodeBytes = 1;
1901         const int immediateBytes = 8;
1902         const int totalBytes = rexBytes + opcodeBytes + immediateBytes;
1903         ASSERT(totalBytes &gt;= maxJumpReplacementSize());
1904         return label.labelAtOffset(-totalBytes);
1905     }
1906 
1907     template&lt;PtrTag tag&gt;
1908     static CodeLocationLabel&lt;tag&gt; startOfBranch32WithPatchOnRegister(CodeLocationDataLabel32&lt;tag&gt; label)
1909     {
1910         const int rexBytes = 1;
1911         const int opcodeBytes = 1;
1912         const int immediateBytes = 4;
1913         const int totalBytes = rexBytes + opcodeBytes + immediateBytes;
1914         ASSERT(totalBytes &gt;= maxJumpReplacementSize());
1915         return label.labelAtOffset(-totalBytes);
1916     }
1917 
1918     template&lt;PtrTag tag&gt;
1919     static CodeLocationLabel&lt;tag&gt; startOfPatchableBranchPtrWithPatchOnAddress(CodeLocationDataLabelPtr&lt;tag&gt; label)
1920     {
1921         return startOfBranchPtrWithPatchOnRegister(label);
1922     }
1923 
1924     template&lt;PtrTag tag&gt;
1925     static CodeLocationLabel&lt;tag&gt; startOfPatchableBranch32WithPatchOnAddress(CodeLocationDataLabel32&lt;tag&gt; label)
1926     {
1927         return startOfBranch32WithPatchOnRegister(label);
1928     }
1929 
1930     template&lt;PtrTag tag&gt;
1931     static void revertJumpReplacementToPatchableBranchPtrWithPatch(CodeLocationLabel&lt;tag&gt; instructionStart, Address, void* initialValue)
1932     {
1933         X86Assembler::revertJumpTo_movq_i64r(instructionStart.executableAddress(), reinterpret_cast&lt;intptr_t&gt;(initialValue), s_scratchRegister);
1934     }
1935 
1936     template&lt;PtrTag tag&gt;
1937     static void revertJumpReplacementToPatchableBranch32WithPatch(CodeLocationLabel&lt;tag&gt; instructionStart, Address, int32_t initialValue)
1938     {
1939         X86Assembler::revertJumpTo_movl_i32r(instructionStart.executableAddress(), initialValue, s_scratchRegister);
1940     }
1941 
1942     template&lt;PtrTag tag&gt;
1943     static void revertJumpReplacementToBranchPtrWithPatch(CodeLocationLabel&lt;tag&gt; instructionStart, RegisterID, void* initialValue)
1944     {
1945         X86Assembler::revertJumpTo_movq_i64r(instructionStart.executableAddress(), reinterpret_cast&lt;intptr_t&gt;(initialValue), s_scratchRegister);
1946     }
1947 
1948     template&lt;PtrTag callTag, PtrTag destTag&gt;
1949     static void repatchCall(CodeLocationCall&lt;callTag&gt; call, CodeLocationLabel&lt;destTag&gt; destination)
1950     {
1951         X86Assembler::repatchPointer(call.dataLabelPtrAtOffset(-REPATCH_OFFSET_CALL_R11).dataLocation(), destination.executableAddress());
1952     }
1953 
1954     template&lt;PtrTag callTag, PtrTag destTag&gt;
1955     static void repatchCall(CodeLocationCall&lt;callTag&gt; call, FunctionPtr&lt;destTag&gt; destination)
1956     {
1957         X86Assembler::repatchPointer(call.dataLabelPtrAtOffset(-REPATCH_OFFSET_CALL_R11).dataLocation(), destination.executableAddress());
1958     }
1959 
1960 private:
1961     // If lzcnt is not available, use this after BSR
1962     // to count the leading zeros.
1963     void clz64AfterBsr(RegisterID dst)
1964     {
1965         Jump srcIsNonZero = m_assembler.jCC(x86Condition(NonZero));
1966         move(TrustedImm32(64), dst);
1967 
1968         Jump skipNonZeroCase = jump();
1969         srcIsNonZero.link(this);
1970         xor64(TrustedImm32(0x3f), dst);
1971         skipNonZeroCase.link(this);
1972     }
1973 
1974     friend class LinkBuffer;
1975 
1976     template&lt;PtrTag tag&gt;
1977     static void linkCall(void* code, Call call, FunctionPtr&lt;tag&gt; function)
1978     {
1979         if (!call.isFlagSet(Call::Near))
1980             X86Assembler::linkPointer(code, call.m_label.labelAtOffset(-REPATCH_OFFSET_CALL_R11), function.executableAddress());
1981         else if (call.isFlagSet(Call::Tail))
1982             X86Assembler::linkJump(code, call.m_label, function.executableAddress());
1983         else
1984             X86Assembler::linkCall(code, call.m_label, function.executableAddress());
1985     }
1986 };
1987 
1988 } // namespace JSC
1989 
1990 #endif // ENABLE(ASSEMBLER)
    </pre>
  </body>
</html>