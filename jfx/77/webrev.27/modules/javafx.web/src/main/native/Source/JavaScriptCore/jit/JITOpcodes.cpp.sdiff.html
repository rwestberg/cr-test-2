<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITOpcodes.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="JITMathIC.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="JITOpcodes32_64.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITOpcodes.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 215 }
 216 
 217 void JIT::emit_op_is_undefined(const Instruction* currentInstruction)
 218 {
 219     auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefined&gt;();
 220     int dst = bytecode.m_dst.offset();
 221     int value = bytecode.m_operand.offset();
 222 
 223     emitGetVirtualRegister(value, regT0);
 224     Jump isCell = branchIfCell(regT0);
 225 
 226     compare64(Equal, regT0, TrustedImm32(ValueUndefined), regT0);
 227     Jump done = jump();
 228 
 229     isCell.link(this);
 230     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 231     move(TrustedImm32(0), regT0);
 232     Jump notMasqueradesAsUndefined = jump();
 233 
 234     isMasqueradesAsUndefined.link(this);
<span class="line-modified"> 235     emitLoadStructure(*vm(), regT0, regT1, regT2);</span>
 236     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 237     loadPtr(Address(regT1, Structure::globalObjectOffset()), regT1);
 238     comparePtr(Equal, regT0, regT1, regT0);
 239 
 240     notMasqueradesAsUndefined.link(this);
 241     done.link(this);
 242     boxBoolean(regT0, JSValueRegs { regT0 });
 243     emitPutVirtualRegister(dst);
 244 }
 245 
 246 void JIT::emit_op_is_undefined_or_null(const Instruction* currentInstruction)
 247 {
 248     auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefinedOrNull&gt;();
 249     int dst = bytecode.m_dst.offset();
 250     int value = bytecode.m_operand.offset();
 251 
 252     emitGetVirtualRegister(value, regT0);
 253 
 254     and64(TrustedImm32(~TagBitUndefined), regT0);
 255     compare64(Equal, regT0, TrustedImm32(ValueNull), regT0);
</pre>
<hr />
<pre>
 374     // clear other than the low bit (which will be 0 or 1 for false or true inputs respectively).
 375     // Then invert against JSValue(true), which will add the tag back in, and flip the low bit.
 376     xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueFalse)), regT0);
 377     addSlowCase(branchTestPtr(NonZero, regT0, TrustedImm32(static_cast&lt;int32_t&gt;(~1))));
 378     xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueTrue)), regT0);
 379 
 380     emitPutVirtualRegister(bytecode.m_dst.offset());
 381 }
 382 
 383 void JIT::emit_op_jfalse(const Instruction* currentInstruction)
 384 {
 385     auto bytecode = currentInstruction-&gt;as&lt;OpJfalse&gt;();
 386     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 387 
 388     GPRReg value = regT0;
 389     GPRReg scratch1 = regT1;
 390     GPRReg scratch2 = regT2;
 391     bool shouldCheckMasqueradesAsUndefined = true;
 392 
 393     emitGetVirtualRegister(bytecode.m_condition.offset(), value);
<span class="line-modified"> 394     addJump(branchIfFalsey(*vm(), JSValueRegs(value), scratch1, scratch2, fpRegT0, fpRegT1, shouldCheckMasqueradesAsUndefined, m_codeBlock-&gt;globalObject()), target);</span>
 395 }
 396 
 397 void JIT::emit_op_jeq_null(const Instruction* currentInstruction)
 398 {
 399     auto bytecode = currentInstruction-&gt;as&lt;OpJeqNull&gt;();
 400     int src = bytecode.m_value.offset();
 401     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 402 
 403     emitGetVirtualRegister(src, regT0);
 404     Jump isImmediate = branchIfNotCell(regT0);
 405 
 406     // First, handle JSCell cases - check MasqueradesAsUndefined bit on the structure.
 407     Jump isNotMasqueradesAsUndefined = branchTest8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
<span class="line-modified"> 408     emitLoadStructure(*vm(), regT0, regT2, regT1);</span>
 409     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 410     addJump(branchPtr(Equal, Address(regT2, Structure::globalObjectOffset()), regT0), target);
 411     Jump masqueradesGlobalObjectIsForeign = jump();
 412 
 413     // Now handle the immediate cases - undefined &amp; null
 414     isImmediate.link(this);
 415     and64(TrustedImm32(~TagBitUndefined), regT0);
 416     addJump(branch64(Equal, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 417 
 418     isNotMasqueradesAsUndefined.link(this);
 419     masqueradesGlobalObjectIsForeign.link(this);
 420 };
 421 void JIT::emit_op_jneq_null(const Instruction* currentInstruction)
 422 {
 423     auto bytecode = currentInstruction-&gt;as&lt;OpJneqNull&gt;();
 424     int src = bytecode.m_value.offset();
 425     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 426 
 427     emitGetVirtualRegister(src, regT0);
 428     Jump isImmediate = branchIfNotCell(regT0);
 429 
 430     // First, handle JSCell cases - check MasqueradesAsUndefined bit on the structure.
 431     addJump(branchTest8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined)), target);
<span class="line-modified"> 432     emitLoadStructure(*vm(), regT0, regT2, regT1);</span>
 433     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 434     addJump(branchPtr(NotEqual, Address(regT2, Structure::globalObjectOffset()), regT0), target);
 435     Jump wasNotImmediate = jump();
 436 
 437     // Now handle the immediate cases - undefined &amp; null
 438     isImmediate.link(this);
 439     and64(TrustedImm32(~TagBitUndefined), regT0);
 440     addJump(branch64(NotEqual, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 441 
 442     wasNotImmediate.link(this);
 443 }
 444 
























 445 void JIT::emit_op_jneq_ptr(const Instruction* currentInstruction)
 446 {
 447     auto bytecode = currentInstruction-&gt;as&lt;OpJneqPtr&gt;();
 448     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 449     int src = bytecode.m_value.offset();
 450     Special::Pointer ptr = bytecode.m_specialPointer;
 451     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 452 
 453     emitGetVirtualRegister(src, regT0);
 454     CCallHelpers::Jump equal = branchPtr(Equal, regT0, TrustedImmPtr(actualPointerFor(m_codeBlock, ptr)));
 455     store8(TrustedImm32(1), &amp;metadata.m_hasJumped);
 456     addJump(jump(), target);
 457     equal.link(this);
 458 }
 459 
 460 void JIT::emit_op_eq(const Instruction* currentInstruction)
 461 {
 462     auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
 463     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 464     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
</pre>
<hr />
<pre>
 469 
 470 void JIT::emit_op_jeq(const Instruction* currentInstruction)
 471 {
 472     auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
 473     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 474     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 475     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 476     addJump(branch32(Equal, regT0, regT1), target);
 477 }
 478 
 479 void JIT::emit_op_jtrue(const Instruction* currentInstruction)
 480 {
 481     auto bytecode = currentInstruction-&gt;as&lt;OpJtrue&gt;();
 482     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 483 
 484     GPRReg value = regT0;
 485     GPRReg scratch1 = regT1;
 486     GPRReg scratch2 = regT2;
 487     bool shouldCheckMasqueradesAsUndefined = true;
 488     emitGetVirtualRegister(bytecode.m_condition.offset(), value);
<span class="line-modified"> 489     addJump(branchIfTruthy(*vm(), JSValueRegs(value), scratch1, scratch2, fpRegT0, fpRegT1, shouldCheckMasqueradesAsUndefined, m_codeBlock-&gt;globalObject()), target);</span>
 490 }
 491 
 492 void JIT::emit_op_neq(const Instruction* currentInstruction)
 493 {
 494     auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
 495     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 496     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 497     compare32(NotEqual, regT1, regT0, regT0);
 498     boxBoolean(regT0, JSValueRegs { regT0 });
 499 
 500     emitPutVirtualRegister(bytecode.m_dst.offset());
 501 }
 502 
 503 void JIT::emit_op_jneq(const Instruction* currentInstruction)
 504 {
 505     auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
 506     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 507     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 508     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 509     addJump(branch32(NotEqual, regT0, regT1), target);
 510 }
 511 
 512 void JIT::emit_op_throw(const Instruction* currentInstruction)
 513 {
 514     auto bytecode = currentInstruction-&gt;as&lt;OpThrow&gt;();
 515     ASSERT(regT0 == returnValueGPR);
<span class="line-modified"> 516     copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm()-&gt;topEntryFrame);</span>
 517     emitGetVirtualRegister(bytecode.m_value.offset(), regT0);
 518     callOperationNoExceptionCheck(operationThrow, regT0);
<span class="line-modified"> 519     jumpToExceptionHandler(*vm());</span>
 520 }
 521 
 522 template&lt;typename Op&gt;
 523 void JIT::compileOpStrictEq(const Instruction* currentInstruction, CompileOpStrictEqType type)
 524 {
 525     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 526     int dst = bytecode.m_dst.offset();
 527     int src1 = bytecode.m_lhs.offset();
 528     int src2 = bytecode.m_rhs.offset();
 529 
 530     emitGetVirtualRegisters(src1, regT0, src2, regT1);
 531 
 532     // Jump slow if both are cells (to cover strings).
 533     move(regT0, regT2);
 534     or64(regT1, regT2);
 535     addSlowCase(branchIfCell(regT2));
 536 
 537     // Jump slow if either is a double. First test if it&#39;s an integer, which is fine, and then test
 538     // if it&#39;s a double.
 539     Jump leftOK = branchIfInt32(regT0);
</pre>
<hr />
<pre>
 650 
 651 void JIT::emit_op_to_object(const Instruction* currentInstruction)
 652 {
 653     auto bytecode = currentInstruction-&gt;as&lt;OpToObject&gt;();
 654     int dstVReg = bytecode.m_dst.offset();
 655     int srcVReg = bytecode.m_operand.offset();
 656     emitGetVirtualRegister(srcVReg, regT0);
 657 
 658     addSlowCase(branchIfNotCell(regT0));
 659     addSlowCase(branchIfNotObject(regT0));
 660 
 661     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 662     if (srcVReg != dstVReg)
 663         emitPutVirtualRegister(dstVReg);
 664 }
 665 
 666 void JIT::emit_op_catch(const Instruction* currentInstruction)
 667 {
 668     auto bytecode = currentInstruction-&gt;as&lt;OpCatch&gt;();
 669 
<span class="line-modified"> 670     restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm()-&gt;topEntryFrame);</span>
 671 
 672     move(TrustedImmPtr(m_vm), regT3);
 673     load64(Address(regT3, VM::callFrameForCatchOffset()), callFrameRegister);
 674     storePtr(TrustedImmPtr(nullptr), Address(regT3, VM::callFrameForCatchOffset()));
 675 
 676     addPtr(TrustedImm32(stackPointerOffsetFor(codeBlock()) * sizeof(Register)), callFrameRegister, stackPointerRegister);
 677 
 678     callOperationNoExceptionCheck(operationCheckIfExceptionIsUncatchableAndNotifyProfiler);
 679     Jump isCatchableException = branchTest32(Zero, returnValueGPR);
<span class="line-modified"> 680     jumpToExceptionHandler(*vm());</span>
 681     isCatchableException.link(this);
 682 
 683     move(TrustedImmPtr(m_vm), regT3);
 684     load64(Address(regT3, VM::exceptionOffset()), regT0);
 685     store64(TrustedImm64(JSValue::encode(JSValue())), Address(regT3, VM::exceptionOffset()));
 686     emitPutVirtualRegister(bytecode.m_exception.offset());
 687 
 688     load64(Address(regT0, Exception::valueOffset()), regT0);
 689     emitPutVirtualRegister(bytecode.m_thrownValue.offset());
 690 
 691 #if ENABLE(DFG_JIT)
 692     // FIXME: consider inline caching the process of doing OSR entry, including
 693     // argument type proofs, storing locals to the buffer, etc
 694     // https://bugs.webkit.org/show_bug.cgi?id=175598
 695 
 696     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 697     ValueProfileAndOperandBuffer* buffer = metadata.m_buffer;
 698     if (buffer || !shouldEmitProfiling())
 699         callOperation(operationTryOSREnterAtCatch, m_bytecodeOffset);
 700     else
 701         callOperation(operationTryOSREnterAtCatchAndValueProfile, m_bytecodeOffset);
 702     auto skipOSREntry = branchTestPtr(Zero, returnValueGPR);
 703     emitRestoreCalleeSaves();
<span class="line-modified"> 704     jump(returnValueGPR, ExceptionHandlerPtrTag);</span>
 705     skipOSREntry.link(this);
 706     if (buffer &amp;&amp; shouldEmitProfiling()) {
 707         buffer-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {
 708             JSValueRegs regs(regT0);
 709             emitGetVirtualRegister(profile.m_operand, regs);
<span class="line-modified"> 710             emitValueProfilingSite(profile.m_profile);</span>
 711         });
 712     }
 713 #endif // ENABLE(DFG_JIT)
 714 }
 715 
 716 void JIT::emit_op_identity_with_profile(const Instruction*)
 717 {
 718     // We don&#39;t need to do anything here...
 719 }
 720 
 721 void JIT::emit_op_get_parent_scope(const Instruction* currentInstruction)
 722 {
 723     auto bytecode = currentInstruction-&gt;as&lt;OpGetParentScope&gt;();
 724     int currentScope = bytecode.m_scope.offset();
 725     emitGetVirtualRegister(currentScope, regT0);
 726     loadPtr(Address(regT0, JSScope::offsetOfNext()), regT0);
 727     emitStoreCell(bytecode.m_dst.offset(), regT0);
 728 }
 729 
 730 void JIT::emit_op_switch_imm(const Instruction* currentInstruction)
 731 {
 732     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchImm&gt;();
 733     size_t tableIndex = bytecode.m_tableIndex;
 734     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 735     unsigned scrutinee = bytecode.m_scrutinee.offset();
 736 
 737     // create jump table for switch destinations, track this switch statement.
 738     SimpleJumpTable* jumpTable = &amp;m_codeBlock-&gt;switchJumpTable(tableIndex);
 739     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset, SwitchRecord::Immediate));
 740     jumpTable-&gt;ensureCTITable();
 741 
 742     emitGetVirtualRegister(scrutinee, regT0);
 743     callOperation(operationSwitchImmWithUnknownKeyType, regT0, tableIndex);
<span class="line-modified"> 744     jump(returnValueGPR, JSSwitchPtrTag);</span>
 745 }
 746 
 747 void JIT::emit_op_switch_char(const Instruction* currentInstruction)
 748 {
 749     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchChar&gt;();
 750     size_t tableIndex = bytecode.m_tableIndex;
 751     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 752     unsigned scrutinee = bytecode.m_scrutinee.offset();
 753 
 754     // create jump table for switch destinations, track this switch statement.
 755     SimpleJumpTable* jumpTable = &amp;m_codeBlock-&gt;switchJumpTable(tableIndex);
 756     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset, SwitchRecord::Character));
 757     jumpTable-&gt;ensureCTITable();
 758 
 759     emitGetVirtualRegister(scrutinee, regT0);
 760     callOperation(operationSwitchCharWithUnknownKeyType, regT0, tableIndex);
<span class="line-modified"> 761     jump(returnValueGPR, JSSwitchPtrTag);</span>
 762 }
 763 
 764 void JIT::emit_op_switch_string(const Instruction* currentInstruction)
 765 {
 766     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchString&gt;();
 767     size_t tableIndex = bytecode.m_tableIndex;
 768     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 769     unsigned scrutinee = bytecode.m_scrutinee.offset();
 770 
 771     // create jump table for switch destinations, track this switch statement.
 772     StringJumpTable* jumpTable = &amp;m_codeBlock-&gt;stringSwitchJumpTable(tableIndex);
 773     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset));
 774 
 775     emitGetVirtualRegister(scrutinee, regT0);
 776     callOperation(operationSwitchStringWithUnknownKeyType, regT0, tableIndex);
<span class="line-modified"> 777     jump(returnValueGPR, JSSwitchPtrTag);</span>
 778 }
 779 
 780 void JIT::emit_op_debug(const Instruction* currentInstruction)
 781 {
 782     auto bytecode = currentInstruction-&gt;as&lt;OpDebug&gt;();
 783     load32(codeBlock()-&gt;debuggerRequestsAddress(), regT0);
 784     Jump noDebuggerRequests = branchTest32(Zero, regT0);
 785     callOperation(operationDebug, static_cast&lt;int&gt;(bytecode.m_debugHookType));
 786     noDebuggerRequests.link(this);
 787 }
 788 
 789 void JIT::emit_op_eq_null(const Instruction* currentInstruction)
 790 {
 791     auto bytecode = currentInstruction-&gt;as&lt;OpEqNull&gt;();
 792     int dst = bytecode.m_dst.offset();
 793     int src1 = bytecode.m_operand.offset();
 794 
 795     emitGetVirtualRegister(src1, regT0);
 796     Jump isImmediate = branchIfNotCell(regT0);
 797 
 798     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 799     move(TrustedImm32(0), regT0);
 800     Jump wasNotMasqueradesAsUndefined = jump();
 801 
 802     isMasqueradesAsUndefined.link(this);
<span class="line-modified"> 803     emitLoadStructure(*vm(), regT0, regT2, regT1);</span>
 804     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 805     loadPtr(Address(regT2, Structure::globalObjectOffset()), regT2);
 806     comparePtr(Equal, regT0, regT2, regT0);
 807     Jump wasNotImmediate = jump();
 808 
 809     isImmediate.link(this);
 810 
 811     and64(TrustedImm32(~TagBitUndefined), regT0);
 812     compare64(Equal, regT0, TrustedImm32(ValueNull), regT0);
 813 
 814     wasNotImmediate.link(this);
 815     wasNotMasqueradesAsUndefined.link(this);
 816 
 817     boxBoolean(regT0, JSValueRegs { regT0 });
 818     emitPutVirtualRegister(dst);
 819 
 820 }
 821 
 822 void JIT::emit_op_neq_null(const Instruction* currentInstruction)
 823 {
 824     auto bytecode = currentInstruction-&gt;as&lt;OpNeqNull&gt;();
 825     int dst = bytecode.m_dst.offset();
 826     int src1 = bytecode.m_operand.offset();
 827 
 828     emitGetVirtualRegister(src1, regT0);
 829     Jump isImmediate = branchIfNotCell(regT0);
 830 
 831     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 832     move(TrustedImm32(1), regT0);
 833     Jump wasNotMasqueradesAsUndefined = jump();
 834 
 835     isMasqueradesAsUndefined.link(this);
<span class="line-modified"> 836     emitLoadStructure(*vm(), regT0, regT2, regT1);</span>
 837     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 838     loadPtr(Address(regT2, Structure::globalObjectOffset()), regT2);
 839     comparePtr(NotEqual, regT0, regT2, regT0);
 840     Jump wasNotImmediate = jump();
 841 
 842     isImmediate.link(this);
 843 
 844     and64(TrustedImm32(~TagBitUndefined), regT0);
 845     compare64(NotEqual, regT0, TrustedImm32(ValueNull), regT0);
 846 
 847     wasNotImmediate.link(this);
 848     wasNotMasqueradesAsUndefined.link(this);
 849 
 850     boxBoolean(regT0, JSValueRegs { regT0 });
 851     emitPutVirtualRegister(dst);
 852 }
 853 
<span class="line-removed"> 854 void JIT::emit_op_enter(const Instruction*)</span>
<span class="line-removed"> 855 {</span>
<span class="line-removed"> 856     // Even though CTI doesn&#39;t use them, we initialize our constant</span>
<span class="line-removed"> 857     // registers to zap stale pointers, to avoid unnecessarily prolonging</span>
<span class="line-removed"> 858     // object lifetime and increasing GC pressure.</span>
<span class="line-removed"> 859     size_t count = m_codeBlock-&gt;numVars();</span>
<span class="line-removed"> 860     for (size_t j = CodeBlock::llintBaselineCalleeSaveSpaceAsVirtualRegisters(); j &lt; count; ++j)</span>
<span class="line-removed"> 861         emitInitRegister(virtualRegisterForLocal(j).offset());</span>
<span class="line-removed"> 862 </span>
<span class="line-removed"> 863     emitWriteBarrier(m_codeBlock);</span>
<span class="line-removed"> 864 </span>
<span class="line-removed"> 865     emitEnterOptimizationCheck();</span>
<span class="line-removed"> 866 }</span>
<span class="line-removed"> 867 </span>
 868 void JIT::emit_op_get_scope(const Instruction* currentInstruction)
 869 {
 870     auto bytecode = currentInstruction-&gt;as&lt;OpGetScope&gt;();
 871     int dst = bytecode.m_dst.offset();
 872     emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, regT0);
 873     loadPtr(Address(regT0, JSFunction::offsetOfScopeChain()), regT0);
 874     emitStoreCell(dst, regT0);
 875 }
 876 
 877 void JIT::emit_op_to_this(const Instruction* currentInstruction)
 878 {
 879     auto bytecode = currentInstruction-&gt;as&lt;OpToThis&gt;();
 880     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified"> 881     WriteBarrierBase&lt;Structure&gt;* cachedStructure = &amp;metadata.m_cachedStructure;</span>
 882     emitGetVirtualRegister(bytecode.m_srcDst.offset(), regT1);
 883 
 884     emitJumpSlowCaseIfNotJSCell(regT1);
 885 
 886     addSlowCase(branchIfNotType(regT1, FinalObjectType));
<span class="line-modified"> 887     loadPtr(cachedStructure, regT2);</span>
<span class="line-removed"> 888     addSlowCase(branchTestPtr(Zero, regT2));</span>
<span class="line-removed"> 889     load32(Address(regT2, Structure::structureIDOffset()), regT2);</span>
 890     addSlowCase(branch32(NotEqual, Address(regT1, JSCell::structureIDOffset()), regT2));
 891 }
 892 
 893 void JIT::emit_op_create_this(const Instruction* currentInstruction)
 894 {
 895     auto bytecode = currentInstruction-&gt;as&lt;OpCreateThis&gt;();
 896     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 897     int callee = bytecode.m_callee.offset();
 898     WriteBarrierBase&lt;JSCell&gt;* cachedFunction = &amp;metadata.m_cachedCallee;
 899     RegisterID calleeReg = regT0;
 900     RegisterID rareDataReg = regT4;
 901     RegisterID resultReg = regT0;
 902     RegisterID allocatorReg = regT1;
 903     RegisterID structureReg = regT2;
 904     RegisterID cachedFunctionReg = regT4;
 905     RegisterID scratchReg = regT3;
 906 
 907     emitGetVirtualRegister(callee, calleeReg);
 908     addSlowCase(branchIfNotFunction(calleeReg));
 909     loadPtr(Address(calleeReg, JSFunction::offsetOfRareData()), rareDataReg);
 910     addSlowCase(branchTestPtr(Zero, rareDataReg));
<span class="line-modified"> 911     loadPtr(Address(rareDataReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfile::offsetOfAllocator()), allocatorReg);</span>
<span class="line-modified"> 912     loadPtr(Address(rareDataReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfile::offsetOfStructure()), structureReg);</span>
 913 
 914     loadPtr(cachedFunction, cachedFunctionReg);
 915     Jump hasSeenMultipleCallees = branchPtr(Equal, cachedFunctionReg, TrustedImmPtr(JSCell::seenMultipleCalleeObjects()));
 916     addSlowCase(branchPtr(NotEqual, calleeReg, cachedFunctionReg));
 917     hasSeenMultipleCallees.link(this);
 918 
 919     JumpList slowCases;
 920     auto butterfly = TrustedImmPtr(nullptr);
 921     emitAllocateJSObject(resultReg, JITAllocator::variable(), allocatorReg, structureReg, butterfly, scratchReg, slowCases);
<span class="line-modified"> 922     emitGetVirtualRegister(callee, scratchReg);</span>
<span class="line-removed"> 923     loadPtr(Address(scratchReg, JSFunction::offsetOfRareData()), scratchReg);</span>
<span class="line-removed"> 924     load32(Address(scratchReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfile::offsetOfInlineCapacity()), scratchReg);</span>
 925     emitInitializeInlineStorage(resultReg, scratchReg);
 926     addSlowCase(slowCases);
 927     emitPutVirtualRegister(bytecode.m_dst.offset());
 928 }
 929 
 930 void JIT::emit_op_check_tdz(const Instruction* currentInstruction)
 931 {
 932     auto bytecode = currentInstruction-&gt;as&lt;OpCheckTdz&gt;();
 933     emitGetVirtualRegister(bytecode.m_targetVirtualRegister.offset(), regT0);
 934     addSlowCase(branchIfEmpty(regT0));
 935 }
 936 
 937 
 938 // Slow cases
 939 
 940 void JIT::emitSlow_op_eq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 941 {
 942     linkAllSlowCases(iter);
 943 
 944     auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
</pre>
<hr />
<pre>
 983     linkAllSlowCases(iter);
 984 
 985     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceofCustom&gt;();
 986     int dst = bytecode.m_dst.offset();
 987     int value = bytecode.m_value.offset();
 988     int constructor = bytecode.m_constructor.offset();
 989     int hasInstanceValue = bytecode.m_hasInstanceValue.offset();
 990 
 991     emitGetVirtualRegister(value, regT0);
 992     emitGetVirtualRegister(constructor, regT1);
 993     emitGetVirtualRegister(hasInstanceValue, regT2);
 994     callOperation(operationInstanceOfCustom, regT0, regT1, regT2);
 995     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
 996     emitPutVirtualRegister(dst, returnValueGPR);
 997 }
 998 
 999 #endif // USE(JSVALUE64)
1000 
1001 void JIT::emit_op_loop_hint(const Instruction*)
1002 {



1003     // Emit the JIT optimization check:
1004     if (canBeOptimized()) {
1005         addSlowCase(branchAdd32(PositiveOrZero, TrustedImm32(Options::executionCounterIncrementForLoop()),
1006             AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())));
1007     }

1008 }
1009 
1010 void JIT::emitSlow_op_loop_hint(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1011 {


1012 #if ENABLE(DFG_JIT)
1013     // Emit the slow path for the JIT optimization check:
1014     if (canBeOptimized()) {
<span class="line-modified">1015         linkAllSlowCases(iter);</span>

1016 
<span class="line-modified">1017         copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm()-&gt;topEntryFrame);</span>
1018 
1019         callOperation(operationOptimize, m_bytecodeOffset);
<span class="line-modified">1020         Jump noOptimizedEntry = branchTestPtr(Zero, returnValueGPR);</span>
1021         if (!ASSERT_DISABLED) {
1022             Jump ok = branchPtr(MacroAssembler::Above, returnValueGPR, TrustedImmPtr(bitwise_cast&lt;void*&gt;(static_cast&lt;intptr_t&gt;(1000))));
1023             abortWithReason(JITUnreasonableLoopHintJumpTarget);
1024             ok.link(this);
1025         }
<span class="line-modified">1026         jump(returnValueGPR, GPRInfo::callFrameRegister);</span>
<span class="line-removed">1027         noOptimizedEntry.link(this);</span>
<span class="line-removed">1028 </span>
<span class="line-removed">1029         emitJumpSlowToHot(jump(), currentInstruction-&gt;size());</span>
1030     }
1031 #else
1032     UNUSED_PARAM(currentInstruction);
<span class="line-removed">1033     UNUSED_PARAM(iter);</span>
1034 #endif
1035 }
1036 
<span class="line-removed">1037 void JIT::emit_op_check_traps(const Instruction*)</span>
<span class="line-removed">1038 {</span>
<span class="line-removed">1039     addSlowCase(branchTest8(NonZero, AbsoluteAddress(m_vm-&gt;needTrapHandlingAddress())));</span>
<span class="line-removed">1040 }</span>
<span class="line-removed">1041 </span>
1042 void JIT::emit_op_nop(const Instruction*)
1043 {
1044 }
1045 
1046 void JIT::emit_op_super_sampler_begin(const Instruction*)
1047 {
1048     add32(TrustedImm32(1), AbsoluteAddress(bitwise_cast&lt;void*&gt;(&amp;g_superSamplerCount)));
1049 }
1050 
1051 void JIT::emit_op_super_sampler_end(const Instruction*)
1052 {
1053     sub32(TrustedImm32(1), AbsoluteAddress(bitwise_cast&lt;void*&gt;(&amp;g_superSamplerCount)));
1054 }
1055 
<span class="line-modified">1056 void JIT::emitSlow_op_check_traps(const Instruction*, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)</span>
1057 {
<span class="line-modified">1058     linkAllSlowCases(iter);</span>










1059 









1060     callOperation(operationHandleTraps);
















1061 }
1062 
1063 void JIT::emit_op_new_regexp(const Instruction* currentInstruction)
1064 {
1065     auto bytecode = currentInstruction-&gt;as&lt;OpNewRegexp&gt;();
1066     int dst = bytecode.m_dst.offset();
1067     int regexp = bytecode.m_regexp.offset();
1068     callOperation(operationNewRegexp, jsCast&lt;RegExp*&gt;(m_codeBlock-&gt;getConstant(regexp)));
1069     emitStoreCell(dst, returnValueGPR);
1070 }
1071 
1072 template&lt;typename Op&gt;
1073 void JIT::emitNewFuncCommon(const Instruction* currentInstruction)
1074 {
1075     Jump lazyJump;
1076     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
1077     int dst = bytecode.m_dst.offset();
1078 
1079 #if USE(JSVALUE64)
1080     emitGetVirtualRegister(bytecode.m_scope.offset(), regT0);
</pre>
<hr />
<pre>
1232     byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1233         m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1234         &quot;Baseline has_indexed_property stub for %s, return point %p&quot;, toCString(*m_codeBlock).data(), returnAddress.value());
1235 
1236     MacroAssembler::repatchJump(byValInfo-&gt;badTypeJump, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(byValInfo-&gt;stubRoutine-&gt;code().code()));
1237     MacroAssembler::repatchCall(CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)), FunctionPtr&lt;OperationPtrTag&gt;(operationHasIndexedPropertyGeneric));
1238 }
1239 
1240 void JIT::emit_op_has_indexed_property(const Instruction* currentInstruction)
1241 {
1242     auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
1243     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1244     int dst = bytecode.m_dst.offset();
1245     int base = bytecode.m_base.offset();
1246     int property = bytecode.m_property.offset();
1247     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
1248     ByValInfo* byValInfo = m_codeBlock-&gt;addByValInfo();
1249 
1250     emitGetVirtualRegisters(base, regT0, property, regT1);
1251 


1252     // This is technically incorrect - we&#39;re zero-extending an int32. On the hot path this doesn&#39;t matter.
1253     // We check the value as if it was a uint32 against the m_vectorLength - which will always fail if
1254     // number was signed since m_vectorLength is always less than intmax (since the total allocation
1255     // size is always less than 4Gb). As such zero extending will have been correct (and extending the value
1256     // to 64-bits is necessary since it&#39;s used in the address calculation. We zero extend rather than sign
1257     // extending since it makes it easier to re-tag the value in the slow case.
1258     zeroExtend32ToPtr(regT1, regT1);
1259 
1260     emitJumpSlowCaseIfNotJSCell(regT0, base);
1261     emitArrayProfilingSiteWithCell(regT0, regT2, profile);
1262     and32(TrustedImm32(IndexingShapeMask), regT2);
1263 
1264     JITArrayMode mode = chooseArrayMode(profile);
1265     PatchableJump badType;
1266 
1267     // FIXME: Add support for other types like TypedArrays and Arguments.
1268     // See https://bugs.webkit.org/show_bug.cgi?id=135033 and https://bugs.webkit.org/show_bug.cgi?id=135034.
1269     JumpList slowCases = emitLoadForArrayMode(currentInstruction, mode, badType);
1270 
1271     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
</pre>
<hr />
<pre>
1441     store32(TrustedImm32(0), Address(regT1, TypeProfilerLog::LogEntry::structureIDOffset()));
1442     skipIsCell.link(this);
1443 
1444     // Store the typeLocation on the log entry.
1445     move(TrustedImmPtr(cachedTypeLocation), regT0);
1446     store64(regT0, Address(regT1, TypeProfilerLog::LogEntry::locationOffset()));
1447 
1448     // Increment the current log entry.
1449     addPtr(TrustedImm32(sizeof(TypeProfilerLog::LogEntry)), regT1);
1450     store64(regT1, Address(regT2, TypeProfilerLog::currentLogEntryOffset()));
1451     Jump skipClearLog = branchPtr(NotEqual, regT1, TrustedImmPtr(cachedTypeProfilerLog-&gt;logEndPtr()));
1452     // Clear the log if we&#39;re at the end of the log.
1453     callOperation(operationProcessTypeProfilerLog);
1454     skipClearLog.link(this);
1455 
1456     jumpToEnd.link(this);
1457 }
1458 
1459 void JIT::emit_op_log_shadow_chicken_prologue(const Instruction* currentInstruction)
1460 {
<span class="line-modified">1461     RELEASE_ASSERT(vm()-&gt;shadowChicken());</span>
1462     updateTopCallFrame();
1463     static_assert(nonArgGPR0 != regT0 &amp;&amp; nonArgGPR0 != regT2, &quot;we will have problems if this is true.&quot;);
1464     auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenPrologue&gt;();
1465     GPRReg shadowPacketReg = regT0;
1466     GPRReg scratch1Reg = nonArgGPR0; // This must be a non-argument register.
1467     GPRReg scratch2Reg = regT2;
<span class="line-modified">1468     ensureShadowChickenPacket(*vm(), shadowPacketReg, scratch1Reg, scratch2Reg);</span>
1469     emitGetVirtualRegister(bytecode.m_scope.offset(), regT3);
1470     logShadowChickenProloguePacket(shadowPacketReg, scratch1Reg, regT3);
1471 }
1472 
1473 void JIT::emit_op_log_shadow_chicken_tail(const Instruction* currentInstruction)
1474 {
<span class="line-modified">1475     RELEASE_ASSERT(vm()-&gt;shadowChicken());</span>
1476     updateTopCallFrame();
1477     static_assert(nonArgGPR0 != regT0 &amp;&amp; nonArgGPR0 != regT2, &quot;we will have problems if this is true.&quot;);
1478     auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenTail&gt;();
1479     GPRReg shadowPacketReg = regT0;
1480     GPRReg scratch1Reg = nonArgGPR0; // This must be a non-argument register.
1481     GPRReg scratch2Reg = regT2;
<span class="line-modified">1482     ensureShadowChickenPacket(*vm(), shadowPacketReg, scratch1Reg, scratch2Reg);</span>
1483     emitGetVirtualRegister(bytecode.m_thisValue.offset(), regT2);
1484     emitGetVirtualRegister(bytecode.m_scope.offset(), regT3);
1485     logShadowChickenTailPacket(shadowPacketReg, JSValueRegs(regT2), regT3, m_codeBlock, CallSiteIndex(m_bytecodeOffset));
1486 }
1487 
1488 #endif // USE(JSVALUE64)
1489 
1490 void JIT::emit_op_profile_control_flow(const Instruction* currentInstruction)
1491 {
1492     auto bytecode = currentInstruction-&gt;as&lt;OpProfileControlFlow&gt;();
1493     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1494     BasicBlockLocation* basicBlockLocation = metadata.m_basicBlockLocation;
1495 #if USE(JSVALUE64)
1496     basicBlockLocation-&gt;emitExecuteCode(*this);
1497 #else
1498     basicBlockLocation-&gt;emitExecuteCode(*this, regT0);
1499 #endif
1500 }
1501 
1502 void JIT::emit_op_argument_count(const Instruction* currentInstruction)
</pre>
</td>
<td>
<hr />
<pre>
 215 }
 216 
 217 void JIT::emit_op_is_undefined(const Instruction* currentInstruction)
 218 {
 219     auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefined&gt;();
 220     int dst = bytecode.m_dst.offset();
 221     int value = bytecode.m_operand.offset();
 222 
 223     emitGetVirtualRegister(value, regT0);
 224     Jump isCell = branchIfCell(regT0);
 225 
 226     compare64(Equal, regT0, TrustedImm32(ValueUndefined), regT0);
 227     Jump done = jump();
 228 
 229     isCell.link(this);
 230     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 231     move(TrustedImm32(0), regT0);
 232     Jump notMasqueradesAsUndefined = jump();
 233 
 234     isMasqueradesAsUndefined.link(this);
<span class="line-modified"> 235     emitLoadStructure(vm(), regT0, regT1, regT2);</span>
 236     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 237     loadPtr(Address(regT1, Structure::globalObjectOffset()), regT1);
 238     comparePtr(Equal, regT0, regT1, regT0);
 239 
 240     notMasqueradesAsUndefined.link(this);
 241     done.link(this);
 242     boxBoolean(regT0, JSValueRegs { regT0 });
 243     emitPutVirtualRegister(dst);
 244 }
 245 
 246 void JIT::emit_op_is_undefined_or_null(const Instruction* currentInstruction)
 247 {
 248     auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefinedOrNull&gt;();
 249     int dst = bytecode.m_dst.offset();
 250     int value = bytecode.m_operand.offset();
 251 
 252     emitGetVirtualRegister(value, regT0);
 253 
 254     and64(TrustedImm32(~TagBitUndefined), regT0);
 255     compare64(Equal, regT0, TrustedImm32(ValueNull), regT0);
</pre>
<hr />
<pre>
 374     // clear other than the low bit (which will be 0 or 1 for false or true inputs respectively).
 375     // Then invert against JSValue(true), which will add the tag back in, and flip the low bit.
 376     xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueFalse)), regT0);
 377     addSlowCase(branchTestPtr(NonZero, regT0, TrustedImm32(static_cast&lt;int32_t&gt;(~1))));
 378     xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueTrue)), regT0);
 379 
 380     emitPutVirtualRegister(bytecode.m_dst.offset());
 381 }
 382 
 383 void JIT::emit_op_jfalse(const Instruction* currentInstruction)
 384 {
 385     auto bytecode = currentInstruction-&gt;as&lt;OpJfalse&gt;();
 386     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 387 
 388     GPRReg value = regT0;
 389     GPRReg scratch1 = regT1;
 390     GPRReg scratch2 = regT2;
 391     bool shouldCheckMasqueradesAsUndefined = true;
 392 
 393     emitGetVirtualRegister(bytecode.m_condition.offset(), value);
<span class="line-modified"> 394     addJump(branchIfFalsey(vm(), JSValueRegs(value), scratch1, scratch2, fpRegT0, fpRegT1, shouldCheckMasqueradesAsUndefined, m_codeBlock-&gt;globalObject()), target);</span>
 395 }
 396 
 397 void JIT::emit_op_jeq_null(const Instruction* currentInstruction)
 398 {
 399     auto bytecode = currentInstruction-&gt;as&lt;OpJeqNull&gt;();
 400     int src = bytecode.m_value.offset();
 401     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 402 
 403     emitGetVirtualRegister(src, regT0);
 404     Jump isImmediate = branchIfNotCell(regT0);
 405 
 406     // First, handle JSCell cases - check MasqueradesAsUndefined bit on the structure.
 407     Jump isNotMasqueradesAsUndefined = branchTest8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
<span class="line-modified"> 408     emitLoadStructure(vm(), regT0, regT2, regT1);</span>
 409     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 410     addJump(branchPtr(Equal, Address(regT2, Structure::globalObjectOffset()), regT0), target);
 411     Jump masqueradesGlobalObjectIsForeign = jump();
 412 
 413     // Now handle the immediate cases - undefined &amp; null
 414     isImmediate.link(this);
 415     and64(TrustedImm32(~TagBitUndefined), regT0);
 416     addJump(branch64(Equal, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 417 
 418     isNotMasqueradesAsUndefined.link(this);
 419     masqueradesGlobalObjectIsForeign.link(this);
 420 };
 421 void JIT::emit_op_jneq_null(const Instruction* currentInstruction)
 422 {
 423     auto bytecode = currentInstruction-&gt;as&lt;OpJneqNull&gt;();
 424     int src = bytecode.m_value.offset();
 425     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 426 
 427     emitGetVirtualRegister(src, regT0);
 428     Jump isImmediate = branchIfNotCell(regT0);
 429 
 430     // First, handle JSCell cases - check MasqueradesAsUndefined bit on the structure.
 431     addJump(branchTest8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined)), target);
<span class="line-modified"> 432     emitLoadStructure(vm(), regT0, regT2, regT1);</span>
 433     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 434     addJump(branchPtr(NotEqual, Address(regT2, Structure::globalObjectOffset()), regT0), target);
 435     Jump wasNotImmediate = jump();
 436 
 437     // Now handle the immediate cases - undefined &amp; null
 438     isImmediate.link(this);
 439     and64(TrustedImm32(~TagBitUndefined), regT0);
 440     addJump(branch64(NotEqual, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 441 
 442     wasNotImmediate.link(this);
 443 }
 444 
<span class="line-added"> 445 void JIT::emit_op_jundefined_or_null(const Instruction* currentInstruction)</span>
<span class="line-added"> 446 {</span>
<span class="line-added"> 447     auto bytecode = currentInstruction-&gt;as&lt;OpJundefinedOrNull&gt;();</span>
<span class="line-added"> 448     int value = bytecode.m_value.offset();</span>
<span class="line-added"> 449     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);</span>
<span class="line-added"> 450 </span>
<span class="line-added"> 451     emitGetVirtualRegister(value, regT0);</span>
<span class="line-added"> 452 </span>
<span class="line-added"> 453     and64(TrustedImm32(~TagBitUndefined), regT0);</span>
<span class="line-added"> 454     addJump(branch64(Equal, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);</span>
<span class="line-added"> 455 }</span>
<span class="line-added"> 456 </span>
<span class="line-added"> 457 void JIT::emit_op_jnundefined_or_null(const Instruction* currentInstruction)</span>
<span class="line-added"> 458 {</span>
<span class="line-added"> 459     auto bytecode = currentInstruction-&gt;as&lt;OpJnundefinedOrNull&gt;();</span>
<span class="line-added"> 460     int value = bytecode.m_value.offset();</span>
<span class="line-added"> 461     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);</span>
<span class="line-added"> 462 </span>
<span class="line-added"> 463     emitGetVirtualRegister(value, regT0);</span>
<span class="line-added"> 464 </span>
<span class="line-added"> 465     and64(TrustedImm32(~TagBitUndefined), regT0);</span>
<span class="line-added"> 466     addJump(branch64(NotEqual, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);</span>
<span class="line-added"> 467 }</span>
<span class="line-added"> 468 </span>
 469 void JIT::emit_op_jneq_ptr(const Instruction* currentInstruction)
 470 {
 471     auto bytecode = currentInstruction-&gt;as&lt;OpJneqPtr&gt;();
 472     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 473     int src = bytecode.m_value.offset();
 474     Special::Pointer ptr = bytecode.m_specialPointer;
 475     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 476 
 477     emitGetVirtualRegister(src, regT0);
 478     CCallHelpers::Jump equal = branchPtr(Equal, regT0, TrustedImmPtr(actualPointerFor(m_codeBlock, ptr)));
 479     store8(TrustedImm32(1), &amp;metadata.m_hasJumped);
 480     addJump(jump(), target);
 481     equal.link(this);
 482 }
 483 
 484 void JIT::emit_op_eq(const Instruction* currentInstruction)
 485 {
 486     auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
 487     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 488     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
</pre>
<hr />
<pre>
 493 
 494 void JIT::emit_op_jeq(const Instruction* currentInstruction)
 495 {
 496     auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
 497     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 498     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 499     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 500     addJump(branch32(Equal, regT0, regT1), target);
 501 }
 502 
 503 void JIT::emit_op_jtrue(const Instruction* currentInstruction)
 504 {
 505     auto bytecode = currentInstruction-&gt;as&lt;OpJtrue&gt;();
 506     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 507 
 508     GPRReg value = regT0;
 509     GPRReg scratch1 = regT1;
 510     GPRReg scratch2 = regT2;
 511     bool shouldCheckMasqueradesAsUndefined = true;
 512     emitGetVirtualRegister(bytecode.m_condition.offset(), value);
<span class="line-modified"> 513     addJump(branchIfTruthy(vm(), JSValueRegs(value), scratch1, scratch2, fpRegT0, fpRegT1, shouldCheckMasqueradesAsUndefined, m_codeBlock-&gt;globalObject()), target);</span>
 514 }
 515 
 516 void JIT::emit_op_neq(const Instruction* currentInstruction)
 517 {
 518     auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
 519     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 520     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 521     compare32(NotEqual, regT1, regT0, regT0);
 522     boxBoolean(regT0, JSValueRegs { regT0 });
 523 
 524     emitPutVirtualRegister(bytecode.m_dst.offset());
 525 }
 526 
 527 void JIT::emit_op_jneq(const Instruction* currentInstruction)
 528 {
 529     auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
 530     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 531     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 532     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 533     addJump(branch32(NotEqual, regT0, regT1), target);
 534 }
 535 
 536 void JIT::emit_op_throw(const Instruction* currentInstruction)
 537 {
 538     auto bytecode = currentInstruction-&gt;as&lt;OpThrow&gt;();
 539     ASSERT(regT0 == returnValueGPR);
<span class="line-modified"> 540     copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
 541     emitGetVirtualRegister(bytecode.m_value.offset(), regT0);
 542     callOperationNoExceptionCheck(operationThrow, regT0);
<span class="line-modified"> 543     jumpToExceptionHandler(vm());</span>
 544 }
 545 
 546 template&lt;typename Op&gt;
 547 void JIT::compileOpStrictEq(const Instruction* currentInstruction, CompileOpStrictEqType type)
 548 {
 549     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 550     int dst = bytecode.m_dst.offset();
 551     int src1 = bytecode.m_lhs.offset();
 552     int src2 = bytecode.m_rhs.offset();
 553 
 554     emitGetVirtualRegisters(src1, regT0, src2, regT1);
 555 
 556     // Jump slow if both are cells (to cover strings).
 557     move(regT0, regT2);
 558     or64(regT1, regT2);
 559     addSlowCase(branchIfCell(regT2));
 560 
 561     // Jump slow if either is a double. First test if it&#39;s an integer, which is fine, and then test
 562     // if it&#39;s a double.
 563     Jump leftOK = branchIfInt32(regT0);
</pre>
<hr />
<pre>
 674 
 675 void JIT::emit_op_to_object(const Instruction* currentInstruction)
 676 {
 677     auto bytecode = currentInstruction-&gt;as&lt;OpToObject&gt;();
 678     int dstVReg = bytecode.m_dst.offset();
 679     int srcVReg = bytecode.m_operand.offset();
 680     emitGetVirtualRegister(srcVReg, regT0);
 681 
 682     addSlowCase(branchIfNotCell(regT0));
 683     addSlowCase(branchIfNotObject(regT0));
 684 
 685     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 686     if (srcVReg != dstVReg)
 687         emitPutVirtualRegister(dstVReg);
 688 }
 689 
 690 void JIT::emit_op_catch(const Instruction* currentInstruction)
 691 {
 692     auto bytecode = currentInstruction-&gt;as&lt;OpCatch&gt;();
 693 
<span class="line-modified"> 694     restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
 695 
 696     move(TrustedImmPtr(m_vm), regT3);
 697     load64(Address(regT3, VM::callFrameForCatchOffset()), callFrameRegister);
 698     storePtr(TrustedImmPtr(nullptr), Address(regT3, VM::callFrameForCatchOffset()));
 699 
 700     addPtr(TrustedImm32(stackPointerOffsetFor(codeBlock()) * sizeof(Register)), callFrameRegister, stackPointerRegister);
 701 
 702     callOperationNoExceptionCheck(operationCheckIfExceptionIsUncatchableAndNotifyProfiler);
 703     Jump isCatchableException = branchTest32(Zero, returnValueGPR);
<span class="line-modified"> 704     jumpToExceptionHandler(vm());</span>
 705     isCatchableException.link(this);
 706 
 707     move(TrustedImmPtr(m_vm), regT3);
 708     load64(Address(regT3, VM::exceptionOffset()), regT0);
 709     store64(TrustedImm64(JSValue::encode(JSValue())), Address(regT3, VM::exceptionOffset()));
 710     emitPutVirtualRegister(bytecode.m_exception.offset());
 711 
 712     load64(Address(regT0, Exception::valueOffset()), regT0);
 713     emitPutVirtualRegister(bytecode.m_thrownValue.offset());
 714 
 715 #if ENABLE(DFG_JIT)
 716     // FIXME: consider inline caching the process of doing OSR entry, including
 717     // argument type proofs, storing locals to the buffer, etc
 718     // https://bugs.webkit.org/show_bug.cgi?id=175598
 719 
 720     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 721     ValueProfileAndOperandBuffer* buffer = metadata.m_buffer;
 722     if (buffer || !shouldEmitProfiling())
 723         callOperation(operationTryOSREnterAtCatch, m_bytecodeOffset);
 724     else
 725         callOperation(operationTryOSREnterAtCatchAndValueProfile, m_bytecodeOffset);
 726     auto skipOSREntry = branchTestPtr(Zero, returnValueGPR);
 727     emitRestoreCalleeSaves();
<span class="line-modified"> 728     farJump(returnValueGPR, ExceptionHandlerPtrTag);</span>
 729     skipOSREntry.link(this);
 730     if (buffer &amp;&amp; shouldEmitProfiling()) {
 731         buffer-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {
 732             JSValueRegs regs(regT0);
 733             emitGetVirtualRegister(profile.m_operand, regs);
<span class="line-modified"> 734             emitValueProfilingSite(static_cast&lt;ValueProfile&amp;&gt;(profile));</span>
 735         });
 736     }
 737 #endif // ENABLE(DFG_JIT)
 738 }
 739 
 740 void JIT::emit_op_identity_with_profile(const Instruction*)
 741 {
 742     // We don&#39;t need to do anything here...
 743 }
 744 
 745 void JIT::emit_op_get_parent_scope(const Instruction* currentInstruction)
 746 {
 747     auto bytecode = currentInstruction-&gt;as&lt;OpGetParentScope&gt;();
 748     int currentScope = bytecode.m_scope.offset();
 749     emitGetVirtualRegister(currentScope, regT0);
 750     loadPtr(Address(regT0, JSScope::offsetOfNext()), regT0);
 751     emitStoreCell(bytecode.m_dst.offset(), regT0);
 752 }
 753 
 754 void JIT::emit_op_switch_imm(const Instruction* currentInstruction)
 755 {
 756     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchImm&gt;();
 757     size_t tableIndex = bytecode.m_tableIndex;
 758     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 759     unsigned scrutinee = bytecode.m_scrutinee.offset();
 760 
 761     // create jump table for switch destinations, track this switch statement.
 762     SimpleJumpTable* jumpTable = &amp;m_codeBlock-&gt;switchJumpTable(tableIndex);
 763     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset, SwitchRecord::Immediate));
 764     jumpTable-&gt;ensureCTITable();
 765 
 766     emitGetVirtualRegister(scrutinee, regT0);
 767     callOperation(operationSwitchImmWithUnknownKeyType, regT0, tableIndex);
<span class="line-modified"> 768     farJump(returnValueGPR, JSSwitchPtrTag);</span>
 769 }
 770 
 771 void JIT::emit_op_switch_char(const Instruction* currentInstruction)
 772 {
 773     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchChar&gt;();
 774     size_t tableIndex = bytecode.m_tableIndex;
 775     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 776     unsigned scrutinee = bytecode.m_scrutinee.offset();
 777 
 778     // create jump table for switch destinations, track this switch statement.
 779     SimpleJumpTable* jumpTable = &amp;m_codeBlock-&gt;switchJumpTable(tableIndex);
 780     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset, SwitchRecord::Character));
 781     jumpTable-&gt;ensureCTITable();
 782 
 783     emitGetVirtualRegister(scrutinee, regT0);
 784     callOperation(operationSwitchCharWithUnknownKeyType, regT0, tableIndex);
<span class="line-modified"> 785     farJump(returnValueGPR, JSSwitchPtrTag);</span>
 786 }
 787 
 788 void JIT::emit_op_switch_string(const Instruction* currentInstruction)
 789 {
 790     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchString&gt;();
 791     size_t tableIndex = bytecode.m_tableIndex;
 792     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 793     unsigned scrutinee = bytecode.m_scrutinee.offset();
 794 
 795     // create jump table for switch destinations, track this switch statement.
 796     StringJumpTable* jumpTable = &amp;m_codeBlock-&gt;stringSwitchJumpTable(tableIndex);
 797     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset));
 798 
 799     emitGetVirtualRegister(scrutinee, regT0);
 800     callOperation(operationSwitchStringWithUnknownKeyType, regT0, tableIndex);
<span class="line-modified"> 801     farJump(returnValueGPR, JSSwitchPtrTag);</span>
 802 }
 803 
 804 void JIT::emit_op_debug(const Instruction* currentInstruction)
 805 {
 806     auto bytecode = currentInstruction-&gt;as&lt;OpDebug&gt;();
 807     load32(codeBlock()-&gt;debuggerRequestsAddress(), regT0);
 808     Jump noDebuggerRequests = branchTest32(Zero, regT0);
 809     callOperation(operationDebug, static_cast&lt;int&gt;(bytecode.m_debugHookType));
 810     noDebuggerRequests.link(this);
 811 }
 812 
 813 void JIT::emit_op_eq_null(const Instruction* currentInstruction)
 814 {
 815     auto bytecode = currentInstruction-&gt;as&lt;OpEqNull&gt;();
 816     int dst = bytecode.m_dst.offset();
 817     int src1 = bytecode.m_operand.offset();
 818 
 819     emitGetVirtualRegister(src1, regT0);
 820     Jump isImmediate = branchIfNotCell(regT0);
 821 
 822     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 823     move(TrustedImm32(0), regT0);
 824     Jump wasNotMasqueradesAsUndefined = jump();
 825 
 826     isMasqueradesAsUndefined.link(this);
<span class="line-modified"> 827     emitLoadStructure(vm(), regT0, regT2, regT1);</span>
 828     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 829     loadPtr(Address(regT2, Structure::globalObjectOffset()), regT2);
 830     comparePtr(Equal, regT0, regT2, regT0);
 831     Jump wasNotImmediate = jump();
 832 
 833     isImmediate.link(this);
 834 
 835     and64(TrustedImm32(~TagBitUndefined), regT0);
 836     compare64(Equal, regT0, TrustedImm32(ValueNull), regT0);
 837 
 838     wasNotImmediate.link(this);
 839     wasNotMasqueradesAsUndefined.link(this);
 840 
 841     boxBoolean(regT0, JSValueRegs { regT0 });
 842     emitPutVirtualRegister(dst);
 843 
 844 }
 845 
 846 void JIT::emit_op_neq_null(const Instruction* currentInstruction)
 847 {
 848     auto bytecode = currentInstruction-&gt;as&lt;OpNeqNull&gt;();
 849     int dst = bytecode.m_dst.offset();
 850     int src1 = bytecode.m_operand.offset();
 851 
 852     emitGetVirtualRegister(src1, regT0);
 853     Jump isImmediate = branchIfNotCell(regT0);
 854 
 855     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 856     move(TrustedImm32(1), regT0);
 857     Jump wasNotMasqueradesAsUndefined = jump();
 858 
 859     isMasqueradesAsUndefined.link(this);
<span class="line-modified"> 860     emitLoadStructure(vm(), regT0, regT2, regT1);</span>
 861     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 862     loadPtr(Address(regT2, Structure::globalObjectOffset()), regT2);
 863     comparePtr(NotEqual, regT0, regT2, regT0);
 864     Jump wasNotImmediate = jump();
 865 
 866     isImmediate.link(this);
 867 
 868     and64(TrustedImm32(~TagBitUndefined), regT0);
 869     compare64(NotEqual, regT0, TrustedImm32(ValueNull), regT0);
 870 
 871     wasNotImmediate.link(this);
 872     wasNotMasqueradesAsUndefined.link(this);
 873 
 874     boxBoolean(regT0, JSValueRegs { regT0 });
 875     emitPutVirtualRegister(dst);
 876 }
 877 














 878 void JIT::emit_op_get_scope(const Instruction* currentInstruction)
 879 {
 880     auto bytecode = currentInstruction-&gt;as&lt;OpGetScope&gt;();
 881     int dst = bytecode.m_dst.offset();
 882     emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, regT0);
 883     loadPtr(Address(regT0, JSFunction::offsetOfScopeChain()), regT0);
 884     emitStoreCell(dst, regT0);
 885 }
 886 
 887 void JIT::emit_op_to_this(const Instruction* currentInstruction)
 888 {
 889     auto bytecode = currentInstruction-&gt;as&lt;OpToThis&gt;();
 890     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<span class="line-modified"> 891     StructureID* cachedStructureID = &amp;metadata.m_cachedStructureID;</span>
 892     emitGetVirtualRegister(bytecode.m_srcDst.offset(), regT1);
 893 
 894     emitJumpSlowCaseIfNotJSCell(regT1);
 895 
 896     addSlowCase(branchIfNotType(regT1, FinalObjectType));
<span class="line-modified"> 897     load32(cachedStructureID, regT2);</span>


 898     addSlowCase(branch32(NotEqual, Address(regT1, JSCell::structureIDOffset()), regT2));
 899 }
 900 
 901 void JIT::emit_op_create_this(const Instruction* currentInstruction)
 902 {
 903     auto bytecode = currentInstruction-&gt;as&lt;OpCreateThis&gt;();
 904     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 905     int callee = bytecode.m_callee.offset();
 906     WriteBarrierBase&lt;JSCell&gt;* cachedFunction = &amp;metadata.m_cachedCallee;
 907     RegisterID calleeReg = regT0;
 908     RegisterID rareDataReg = regT4;
 909     RegisterID resultReg = regT0;
 910     RegisterID allocatorReg = regT1;
 911     RegisterID structureReg = regT2;
 912     RegisterID cachedFunctionReg = regT4;
 913     RegisterID scratchReg = regT3;
 914 
 915     emitGetVirtualRegister(callee, calleeReg);
 916     addSlowCase(branchIfNotFunction(calleeReg));
 917     loadPtr(Address(calleeReg, JSFunction::offsetOfRareData()), rareDataReg);
 918     addSlowCase(branchTestPtr(Zero, rareDataReg));
<span class="line-modified"> 919     loadPtr(Address(rareDataReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfileWithPrototype::offsetOfAllocator()), allocatorReg);</span>
<span class="line-modified"> 920     loadPtr(Address(rareDataReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfileWithPrototype::offsetOfStructure()), structureReg);</span>
 921 
 922     loadPtr(cachedFunction, cachedFunctionReg);
 923     Jump hasSeenMultipleCallees = branchPtr(Equal, cachedFunctionReg, TrustedImmPtr(JSCell::seenMultipleCalleeObjects()));
 924     addSlowCase(branchPtr(NotEqual, calleeReg, cachedFunctionReg));
 925     hasSeenMultipleCallees.link(this);
 926 
 927     JumpList slowCases;
 928     auto butterfly = TrustedImmPtr(nullptr);
 929     emitAllocateJSObject(resultReg, JITAllocator::variable(), allocatorReg, structureReg, butterfly, scratchReg, slowCases);
<span class="line-modified"> 930     load8(Address(structureReg, Structure::inlineCapacityOffset()), scratchReg);</span>


 931     emitInitializeInlineStorage(resultReg, scratchReg);
 932     addSlowCase(slowCases);
 933     emitPutVirtualRegister(bytecode.m_dst.offset());
 934 }
 935 
 936 void JIT::emit_op_check_tdz(const Instruction* currentInstruction)
 937 {
 938     auto bytecode = currentInstruction-&gt;as&lt;OpCheckTdz&gt;();
 939     emitGetVirtualRegister(bytecode.m_targetVirtualRegister.offset(), regT0);
 940     addSlowCase(branchIfEmpty(regT0));
 941 }
 942 
 943 
 944 // Slow cases
 945 
 946 void JIT::emitSlow_op_eq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 947 {
 948     linkAllSlowCases(iter);
 949 
 950     auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
</pre>
<hr />
<pre>
 989     linkAllSlowCases(iter);
 990 
 991     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceofCustom&gt;();
 992     int dst = bytecode.m_dst.offset();
 993     int value = bytecode.m_value.offset();
 994     int constructor = bytecode.m_constructor.offset();
 995     int hasInstanceValue = bytecode.m_hasInstanceValue.offset();
 996 
 997     emitGetVirtualRegister(value, regT0);
 998     emitGetVirtualRegister(constructor, regT1);
 999     emitGetVirtualRegister(hasInstanceValue, regT2);
1000     callOperation(operationInstanceOfCustom, regT0, regT1, regT2);
1001     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
1002     emitPutVirtualRegister(dst, returnValueGPR);
1003 }
1004 
1005 #endif // USE(JSVALUE64)
1006 
1007 void JIT::emit_op_loop_hint(const Instruction*)
1008 {
<span class="line-added">1009     // Check traps.</span>
<span class="line-added">1010     addSlowCase(branchTest8(NonZero, AbsoluteAddress(m_vm-&gt;needTrapHandlingAddress())));</span>
<span class="line-added">1011 #if ENABLE(DFG_JIT)</span>
1012     // Emit the JIT optimization check:
1013     if (canBeOptimized()) {
1014         addSlowCase(branchAdd32(PositiveOrZero, TrustedImm32(Options::executionCounterIncrementForLoop()),
1015             AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())));
1016     }
<span class="line-added">1017 #endif</span>
1018 }
1019 
1020 void JIT::emitSlow_op_loop_hint(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1021 {
<span class="line-added">1022     linkSlowCase(iter);</span>
<span class="line-added">1023     callOperation(operationHandleTraps);</span>
1024 #if ENABLE(DFG_JIT)
1025     // Emit the slow path for the JIT optimization check:
1026     if (canBeOptimized()) {
<span class="line-modified">1027         emitJumpSlowToHot(branchAdd32(Signed, TrustedImm32(Options::executionCounterIncrementForLoop()), AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())), currentInstruction-&gt;size());</span>
<span class="line-added">1028         linkSlowCase(iter);</span>
1029 
<span class="line-modified">1030         copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
1031 
1032         callOperation(operationOptimize, m_bytecodeOffset);
<span class="line-modified">1033         emitJumpSlowToHot(branchTestPtr(Zero, returnValueGPR), currentInstruction-&gt;size());</span>
1034         if (!ASSERT_DISABLED) {
1035             Jump ok = branchPtr(MacroAssembler::Above, returnValueGPR, TrustedImmPtr(bitwise_cast&lt;void*&gt;(static_cast&lt;intptr_t&gt;(1000))));
1036             abortWithReason(JITUnreasonableLoopHintJumpTarget);
1037             ok.link(this);
1038         }
<span class="line-modified">1039         farJump(returnValueGPR, GPRInfo::callFrameRegister);</span>



1040     }
1041 #else
1042     UNUSED_PARAM(currentInstruction);

1043 #endif
1044 }
1045 





1046 void JIT::emit_op_nop(const Instruction*)
1047 {
1048 }
1049 
1050 void JIT::emit_op_super_sampler_begin(const Instruction*)
1051 {
1052     add32(TrustedImm32(1), AbsoluteAddress(bitwise_cast&lt;void*&gt;(&amp;g_superSamplerCount)));
1053 }
1054 
1055 void JIT::emit_op_super_sampler_end(const Instruction*)
1056 {
1057     sub32(TrustedImm32(1), AbsoluteAddress(bitwise_cast&lt;void*&gt;(&amp;g_superSamplerCount)));
1058 }
1059 
<span class="line-modified">1060 void JIT::emit_op_enter(const Instruction*)</span>
1061 {
<span class="line-modified">1062     // Even though JIT doesn&#39;t use them, we initialize our constant</span>
<span class="line-added">1063     // registers to zap stale pointers, to avoid unnecessarily prolonging</span>
<span class="line-added">1064     // object lifetime and increasing GC pressure.</span>
<span class="line-added">1065     size_t count = m_codeBlock-&gt;numVars();</span>
<span class="line-added">1066     for (size_t i = CodeBlock::llintBaselineCalleeSaveSpaceAsVirtualRegisters(); i &lt; count; ++i)</span>
<span class="line-added">1067         emitInitRegister(virtualRegisterForLocal(i).offset());</span>
<span class="line-added">1068 </span>
<span class="line-added">1069     emitWriteBarrier(m_codeBlock);</span>
<span class="line-added">1070 </span>
<span class="line-added">1071     // Check traps.</span>
<span class="line-added">1072     addSlowCase(branchTest8(NonZero, AbsoluteAddress(m_vm-&gt;needTrapHandlingAddress())));</span>
1073 
<span class="line-added">1074 #if ENABLE(DFG_JIT)</span>
<span class="line-added">1075     if (canBeOptimized())</span>
<span class="line-added">1076         addSlowCase(branchAdd32(PositiveOrZero, TrustedImm32(Options::executionCounterIncrementForEntry()), AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())));</span>
<span class="line-added">1077 #endif</span>
<span class="line-added">1078 }</span>
<span class="line-added">1079 </span>
<span class="line-added">1080 void JIT::emitSlow_op_enter(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)</span>
<span class="line-added">1081 {</span>
<span class="line-added">1082     linkSlowCase(iter);</span>
1083     callOperation(operationHandleTraps);
<span class="line-added">1084 #if ENABLE(DFG_JIT)</span>
<span class="line-added">1085     if (canBeOptimized()) {</span>
<span class="line-added">1086         emitJumpSlowToHot(branchAdd32(Signed, TrustedImm32(Options::executionCounterIncrementForEntry()), AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())), currentInstruction-&gt;size());</span>
<span class="line-added">1087         linkSlowCase(iter);</span>
<span class="line-added">1088 </span>
<span class="line-added">1089         ASSERT(!m_bytecodeOffset);</span>
<span class="line-added">1090 </span>
<span class="line-added">1091         copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
<span class="line-added">1092 </span>
<span class="line-added">1093         callOperation(operationOptimize, m_bytecodeOffset);</span>
<span class="line-added">1094         emitJumpSlowToHot(branchTestPtr(Zero, returnValueGPR), currentInstruction-&gt;size());</span>
<span class="line-added">1095         farJump(returnValueGPR, GPRInfo::callFrameRegister);</span>
<span class="line-added">1096     }</span>
<span class="line-added">1097 #else</span>
<span class="line-added">1098     UNUSED_PARAM(currentInstruction);</span>
<span class="line-added">1099 #endif</span>
1100 }
1101 
1102 void JIT::emit_op_new_regexp(const Instruction* currentInstruction)
1103 {
1104     auto bytecode = currentInstruction-&gt;as&lt;OpNewRegexp&gt;();
1105     int dst = bytecode.m_dst.offset();
1106     int regexp = bytecode.m_regexp.offset();
1107     callOperation(operationNewRegexp, jsCast&lt;RegExp*&gt;(m_codeBlock-&gt;getConstant(regexp)));
1108     emitStoreCell(dst, returnValueGPR);
1109 }
1110 
1111 template&lt;typename Op&gt;
1112 void JIT::emitNewFuncCommon(const Instruction* currentInstruction)
1113 {
1114     Jump lazyJump;
1115     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
1116     int dst = bytecode.m_dst.offset();
1117 
1118 #if USE(JSVALUE64)
1119     emitGetVirtualRegister(bytecode.m_scope.offset(), regT0);
</pre>
<hr />
<pre>
1271     byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1272         m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1273         &quot;Baseline has_indexed_property stub for %s, return point %p&quot;, toCString(*m_codeBlock).data(), returnAddress.value());
1274 
1275     MacroAssembler::repatchJump(byValInfo-&gt;badTypeJump, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(byValInfo-&gt;stubRoutine-&gt;code().code()));
1276     MacroAssembler::repatchCall(CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)), FunctionPtr&lt;OperationPtrTag&gt;(operationHasIndexedPropertyGeneric));
1277 }
1278 
1279 void JIT::emit_op_has_indexed_property(const Instruction* currentInstruction)
1280 {
1281     auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
1282     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1283     int dst = bytecode.m_dst.offset();
1284     int base = bytecode.m_base.offset();
1285     int property = bytecode.m_property.offset();
1286     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
1287     ByValInfo* byValInfo = m_codeBlock-&gt;addByValInfo();
1288 
1289     emitGetVirtualRegisters(base, regT0, property, regT1);
1290 
<span class="line-added">1291     emitJumpSlowCaseIfNotInt(regT1);</span>
<span class="line-added">1292 </span>
1293     // This is technically incorrect - we&#39;re zero-extending an int32. On the hot path this doesn&#39;t matter.
1294     // We check the value as if it was a uint32 against the m_vectorLength - which will always fail if
1295     // number was signed since m_vectorLength is always less than intmax (since the total allocation
1296     // size is always less than 4Gb). As such zero extending will have been correct (and extending the value
1297     // to 64-bits is necessary since it&#39;s used in the address calculation. We zero extend rather than sign
1298     // extending since it makes it easier to re-tag the value in the slow case.
1299     zeroExtend32ToPtr(regT1, regT1);
1300 
1301     emitJumpSlowCaseIfNotJSCell(regT0, base);
1302     emitArrayProfilingSiteWithCell(regT0, regT2, profile);
1303     and32(TrustedImm32(IndexingShapeMask), regT2);
1304 
1305     JITArrayMode mode = chooseArrayMode(profile);
1306     PatchableJump badType;
1307 
1308     // FIXME: Add support for other types like TypedArrays and Arguments.
1309     // See https://bugs.webkit.org/show_bug.cgi?id=135033 and https://bugs.webkit.org/show_bug.cgi?id=135034.
1310     JumpList slowCases = emitLoadForArrayMode(currentInstruction, mode, badType);
1311 
1312     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
</pre>
<hr />
<pre>
1482     store32(TrustedImm32(0), Address(regT1, TypeProfilerLog::LogEntry::structureIDOffset()));
1483     skipIsCell.link(this);
1484 
1485     // Store the typeLocation on the log entry.
1486     move(TrustedImmPtr(cachedTypeLocation), regT0);
1487     store64(regT0, Address(regT1, TypeProfilerLog::LogEntry::locationOffset()));
1488 
1489     // Increment the current log entry.
1490     addPtr(TrustedImm32(sizeof(TypeProfilerLog::LogEntry)), regT1);
1491     store64(regT1, Address(regT2, TypeProfilerLog::currentLogEntryOffset()));
1492     Jump skipClearLog = branchPtr(NotEqual, regT1, TrustedImmPtr(cachedTypeProfilerLog-&gt;logEndPtr()));
1493     // Clear the log if we&#39;re at the end of the log.
1494     callOperation(operationProcessTypeProfilerLog);
1495     skipClearLog.link(this);
1496 
1497     jumpToEnd.link(this);
1498 }
1499 
1500 void JIT::emit_op_log_shadow_chicken_prologue(const Instruction* currentInstruction)
1501 {
<span class="line-modified">1502     RELEASE_ASSERT(vm().shadowChicken());</span>
1503     updateTopCallFrame();
1504     static_assert(nonArgGPR0 != regT0 &amp;&amp; nonArgGPR0 != regT2, &quot;we will have problems if this is true.&quot;);
1505     auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenPrologue&gt;();
1506     GPRReg shadowPacketReg = regT0;
1507     GPRReg scratch1Reg = nonArgGPR0; // This must be a non-argument register.
1508     GPRReg scratch2Reg = regT2;
<span class="line-modified">1509     ensureShadowChickenPacket(vm(), shadowPacketReg, scratch1Reg, scratch2Reg);</span>
1510     emitGetVirtualRegister(bytecode.m_scope.offset(), regT3);
1511     logShadowChickenProloguePacket(shadowPacketReg, scratch1Reg, regT3);
1512 }
1513 
1514 void JIT::emit_op_log_shadow_chicken_tail(const Instruction* currentInstruction)
1515 {
<span class="line-modified">1516     RELEASE_ASSERT(vm().shadowChicken());</span>
1517     updateTopCallFrame();
1518     static_assert(nonArgGPR0 != regT0 &amp;&amp; nonArgGPR0 != regT2, &quot;we will have problems if this is true.&quot;);
1519     auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenTail&gt;();
1520     GPRReg shadowPacketReg = regT0;
1521     GPRReg scratch1Reg = nonArgGPR0; // This must be a non-argument register.
1522     GPRReg scratch2Reg = regT2;
<span class="line-modified">1523     ensureShadowChickenPacket(vm(), shadowPacketReg, scratch1Reg, scratch2Reg);</span>
1524     emitGetVirtualRegister(bytecode.m_thisValue.offset(), regT2);
1525     emitGetVirtualRegister(bytecode.m_scope.offset(), regT3);
1526     logShadowChickenTailPacket(shadowPacketReg, JSValueRegs(regT2), regT3, m_codeBlock, CallSiteIndex(m_bytecodeOffset));
1527 }
1528 
1529 #endif // USE(JSVALUE64)
1530 
1531 void JIT::emit_op_profile_control_flow(const Instruction* currentInstruction)
1532 {
1533     auto bytecode = currentInstruction-&gt;as&lt;OpProfileControlFlow&gt;();
1534     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1535     BasicBlockLocation* basicBlockLocation = metadata.m_basicBlockLocation;
1536 #if USE(JSVALUE64)
1537     basicBlockLocation-&gt;emitExecuteCode(*this);
1538 #else
1539     basicBlockLocation-&gt;emitExecuteCode(*this, regT0);
1540 #endif
1541 }
1542 
1543 void JIT::emit_op_argument_count(const Instruction* currentInstruction)
</pre>
</td>
</tr>
</table>
<center><a href="JITMathIC.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="JITOpcodes32_64.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>