<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGJITCompiler.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="DFGJITCode.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGJITCompiler.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGJITCompiler.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (C) 2011-2018 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
</pre>
<hr />
<pre>
 40 #include &quot;DFGSpeculativeJIT.h&quot;
 41 #include &quot;DFGThunks.h&quot;
 42 #include &quot;JSCInlines.h&quot;
 43 #include &quot;JSCJSValueInlines.h&quot;
 44 #include &quot;LinkBuffer.h&quot;
 45 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;
 46 #include &quot;StructureStubInfo.h&quot;
 47 #include &quot;ThunkGenerators.h&quot;
 48 #include &quot;VM.h&quot;
 49 
 50 namespace JSC { namespace DFG {
 51 
 52 JITCompiler::JITCompiler(Graph&amp; dfg)
 53     : CCallHelpers(dfg.m_codeBlock)
 54     , m_graph(dfg)
 55     , m_jitCode(adoptRef(new JITCode()))
 56     , m_blockHeads(dfg.numBlocks())
 57     , m_pcToCodeOriginMapBuilder(dfg.m_vm)
 58 {
 59     if (UNLIKELY(shouldDumpDisassembly() || m_graph.m_vm.m_perBytecodeProfiler))
<span class="line-modified"> 60         m_disassembler = std::make_unique&lt;Disassembler&gt;(dfg);</span>
 61 #if ENABLE(FTL_JIT)
 62     m_jitCode-&gt;tierUpInLoopHierarchy = WTFMove(m_graph.m_plan.tierUpInLoopHierarchy());
 63     for (unsigned tierUpBytecode : m_graph.m_plan.tierUpAndOSREnterBytecodes())
 64         m_jitCode-&gt;tierUpEntryTriggers.add(tierUpBytecode, JITCode::TriggerReason::DontTrigger);
 65 #endif
 66 }
 67 
 68 JITCompiler::~JITCompiler()
 69 {
 70 }
 71 
 72 void JITCompiler::linkOSRExits()
 73 {
 74     ASSERT(m_jitCode-&gt;osrExit.size() == m_exitCompilationInfo.size());
 75     if (UNLIKELY(m_graph.compilation())) {
 76         for (unsigned i = 0; i &lt; m_jitCode-&gt;osrExit.size(); ++i) {
 77             OSRExitCompilationInfo&amp; info = m_exitCompilationInfo[i];
 78             Vector&lt;Label&gt; labels;
 79             if (!info.m_failureJumps.empty()) {
 80                 for (unsigned j = 0; j &lt; info.m_failureJumps.jumps().size(); ++j)
 81                     labels.append(info.m_failureJumps.jumps()[j].label());
 82             } else
 83                 labels.append(info.m_replacementSource);
 84             m_exitSiteLabels.append(labels);
 85         }
 86     }
 87 
<span class="line-modified"> 88     MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; osrExitThunk = vm()-&gt;getCTIStub(osrExitThunkGenerator);</span>
 89     auto osrExitThunkLabel = CodeLocationLabel&lt;JITThunkPtrTag&gt;(osrExitThunk.code());
 90     for (unsigned i = 0; i &lt; m_jitCode-&gt;osrExit.size(); ++i) {
 91         OSRExitCompilationInfo&amp; info = m_exitCompilationInfo[i];
 92         JumpList&amp; failureJumps = info.m_failureJumps;
 93         if (!failureJumps.empty())
 94             failureJumps.link(this);
 95         else
 96             info.m_replacementDestination = label();
 97 
 98         jitAssertHasValidCallFrame();
<span class="line-modified"> 99         store32(TrustedImm32(i), &amp;vm()-&gt;osrExitIndex);</span>
100         if (Options::useProbeOSRExit()) {
101             Jump target = jump();
102             addLinkTask([target, osrExitThunkLabel] (LinkBuffer&amp; linkBuffer) {
103                 linkBuffer.link(target, osrExitThunkLabel);
104             });
105         } else
106             info.m_patchableJump = patchableJump();
107     }
108 }
109 
110 void JITCompiler::compileEntry()
111 {
112     // This code currently matches the old JIT. In the function header we need to
113     // save return address and call frame via the prologue and perform a fast stack check.
114     // FIXME: https://bugs.webkit.org/show_bug.cgi?id=56292
115     // We&#39;ll need to convert the remaining cti_ style calls (specifically the stack
116     // check) which will be dependent on stack layout. (We&#39;d need to account for this in
117     // both normal return code and when jumping to an exception handler).
118     emitFunctionPrologue();
119     emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
</pre>
<hr />
<pre>
130 #if ENABLE(FTL_JIT)
131     if (m_graph.m_plan.canTierUpAndOSREnter())
132         store8(TrustedImm32(0), &amp;m_jitCode-&gt;neverExecutedEntry);
133 #endif // ENABLE(FTL_JIT)
134 }
135 
136 void JITCompiler::compileBody()
137 {
138     // We generate the speculative code path, followed by OSR exit code to return
139     // to the old JIT code if speculations fail.
140 
141     bool compiledSpeculative = m_speculative-&gt;compile();
142     ASSERT_UNUSED(compiledSpeculative, compiledSpeculative);
143 }
144 
145 void JITCompiler::compileExceptionHandlers()
146 {
147     if (!m_exceptionChecksWithCallFrameRollback.empty()) {
148         m_exceptionChecksWithCallFrameRollback.link(this);
149 
<span class="line-modified">150         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm()-&gt;topEntryFrame);</span>
151 
152         // lookupExceptionHandlerFromCallerFrame is passed two arguments, the VM and the exec (the CallFrame*).
<span class="line-modified">153         move(TrustedImmPtr(vm()), GPRInfo::argumentGPR0);</span>
154         move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
155         addPtr(TrustedImm32(m_graph.stackPointerOffset() * sizeof(Register)), GPRInfo::callFrameRegister, stackPointerRegister);
156 
157 #if CPU(X86)
158         // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!
159         poke(GPRInfo::argumentGPR0);
160         poke(GPRInfo::argumentGPR1, 1);
161 #endif
162         m_calls.append(CallLinkRecord(call(OperationPtrTag), FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandlerFromCallerFrame)));
163 
<span class="line-modified">164         jumpToExceptionHandler(*vm());</span>
165     }
166 
167     if (!m_exceptionChecks.empty()) {
168         m_exceptionChecks.link(this);
169 
<span class="line-modified">170         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm()-&gt;topEntryFrame);</span>
171 
172         // lookupExceptionHandler is passed two arguments, the VM and the exec (the CallFrame*).
<span class="line-modified">173         move(TrustedImmPtr(vm()), GPRInfo::argumentGPR0);</span>
174         move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
175 
176 #if CPU(X86)
177         // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!
178         poke(GPRInfo::argumentGPR0);
179         poke(GPRInfo::argumentGPR1, 1);
180 #endif
181         m_calls.append(CallLinkRecord(call(OperationPtrTag), FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandler)));
182 
<span class="line-modified">183         jumpToExceptionHandler(*vm());</span>
184     }
185 }
186 
187 void JITCompiler::link(LinkBuffer&amp; linkBuffer)
188 {
189     // Link the code, populate data in CodeBlock data structures.
190     m_jitCode-&gt;common.frameRegisterCount = m_graph.frameRegisterCount();
191     m_jitCode-&gt;common.requiredRegisterCountForExit = m_graph.requiredRegisterCountForExit();
192 
193     if (!m_graph.m_plan.inlineCallFrames()-&gt;isEmpty())
194         m_jitCode-&gt;common.inlineCallFrames = m_graph.m_plan.inlineCallFrames();
195 
196 #if USE(JSVALUE32_64)
197     m_jitCode-&gt;common.doubleConstants = WTFMove(m_graph.m_doubleConstants);
198 #endif
199 
200     m_graph.registerFrozenValues();
201 
202     BitVector usedJumpTables;
203     for (Bag&lt;SwitchData&gt;::iterator iter = m_graph.m_switchData.begin(); !!iter; ++iter) {
</pre>
<hr />
<pre>
249         for (iter = table.offsetTable.begin(); iter != end; ++iter)
250             iter-&gt;value.ctiOffset = table.ctiDefault;
251         for (unsigned j = data.cases.size(); j--;) {
252             SwitchCase&amp; myCase = data.cases[j];
253             iter = table.offsetTable.find(myCase.value.stringImpl());
254             RELEASE_ASSERT(iter != end);
255             iter-&gt;value.ctiOffset = linkBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_blockHeads[myCase.target.block-&gt;index]);
256         }
257     }
258 
259     // Link all calls out from the JIT code to their respective functions.
260     for (unsigned i = 0; i &lt; m_calls.size(); ++i)
261         linkBuffer.link(m_calls[i].m_call, m_calls[i].m_function);
262 
263     finalizeInlineCaches(m_getByIds, linkBuffer);
264     finalizeInlineCaches(m_getByIdsWithThis, linkBuffer);
265     finalizeInlineCaches(m_putByIds, linkBuffer);
266     finalizeInlineCaches(m_inByIds, linkBuffer);
267     finalizeInlineCaches(m_instanceOfs, linkBuffer);
268 
<span class="line-modified">269     auto linkCallThunk = FunctionPtr&lt;NoPtrTag&gt;(vm()-&gt;getCTIStub(linkCallThunkGenerator).retaggedCode&lt;NoPtrTag&gt;());</span>
270     for (auto&amp; record : m_jsCalls) {
271         CallLinkInfo&amp; info = *record.info;
272         linkBuffer.link(record.slowCall, linkCallThunk);
273         info.setCallLocations(
274             CodeLocationLabel&lt;JSInternalPtrTag&gt;(linkBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(record.slowCall)),
275             CodeLocationLabel&lt;JSInternalPtrTag&gt;(linkBuffer.locationOf&lt;JSInternalPtrTag&gt;(record.targetToCheck)),
276             linkBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(record.fastCall));
277     }
278 
279     for (JSDirectCallRecord&amp; record : m_jsDirectCalls) {
280         CallLinkInfo&amp; info = *record.info;
281         linkBuffer.link(record.call, linkBuffer.locationOf&lt;NoPtrTag&gt;(record.slowPath));
282         info.setCallLocations(
283             CodeLocationLabel&lt;JSInternalPtrTag&gt;(),
284             linkBuffer.locationOf&lt;JSInternalPtrTag&gt;(record.slowPath),
285             linkBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(record.call));
286     }
287 
288     for (JSDirectTailCallRecord&amp; record : m_jsDirectTailCalls) {
289         CallLinkInfo&amp; info = *record.info;
290         info.setCallLocations(
291             linkBuffer.locationOf&lt;JSInternalPtrTag&gt;(record.patchableJump),
292             linkBuffer.locationOf&lt;JSInternalPtrTag&gt;(record.slowPath),
293             linkBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(record.call));
294     }
295 
<span class="line-modified">296     MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; osrExitThunk = vm()-&gt;getCTIStub(osrExitGenerationThunkGenerator);</span>
297     auto target = CodeLocationLabel&lt;JITThunkPtrTag&gt;(osrExitThunk.code());
298     for (unsigned i = 0; i &lt; m_jitCode-&gt;osrExit.size(); ++i) {
299         OSRExitCompilationInfo&amp; info = m_exitCompilationInfo[i];
300         if (!Options::useProbeOSRExit()) {
301             linkBuffer.link(info.m_patchableJump.m_jump, target);
302             OSRExit&amp; exit = m_jitCode-&gt;osrExit[i];
303             exit.m_patchableJumpLocation = linkBuffer.locationOf&lt;JSInternalPtrTag&gt;(info.m_patchableJump);
304         }
305         if (info.m_replacementSource.isSet()) {
306             m_jitCode-&gt;common.jumpReplacements.append(JumpReplacement(
307                 linkBuffer.locationOf&lt;JSInternalPtrTag&gt;(info.m_replacementSource),
308                 linkBuffer.locationOf&lt;OSRExitPtrTag&gt;(info.m_replacementDestination)));
309         }
310     }
311 
312     if (UNLIKELY(m_graph.compilation())) {
313         ASSERT(m_exitSiteLabels.size() == m_jitCode-&gt;osrExit.size());
314         for (unsigned i = 0; i &lt; m_exitSiteLabels.size(); ++i) {
315             Vector&lt;Label&gt;&amp; labels = m_exitSiteLabels[i];
316             Vector&lt;MacroAssemblerCodePtr&lt;JSInternalPtrTag&gt;&gt; addresses;
</pre>
<hr />
<pre>
326     // Link new DFG exception handlers and remove baseline JIT handlers.
327     m_codeBlock-&gt;clearExceptionHandlers();
328     for (unsigned  i = 0; i &lt; m_exceptionHandlerOSRExitCallSites.size(); i++) {
329         OSRExitCompilationInfo&amp; info = m_exceptionHandlerOSRExitCallSites[i].exitInfo;
330         if (info.m_replacementDestination.isSet()) {
331             // If this is is *not* set, it means that we already jumped to the OSR exit in pure generated control flow.
332             // i.e, we explicitly emitted an exceptionCheck that we know will be caught in this machine frame.
333             // If this *is set*, it means we will be landing at this code location from genericUnwind from an
334             // exception thrown in a child call frame.
335             CodeLocationLabel&lt;ExceptionHandlerPtrTag&gt; catchLabel = linkBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(info.m_replacementDestination);
336             HandlerInfo newExceptionHandler = m_exceptionHandlerOSRExitCallSites[i].baselineExceptionHandler;
337             CallSiteIndex callSite = m_exceptionHandlerOSRExitCallSites[i].callSiteIndex;
338             newExceptionHandler.start = callSite.bits();
339             newExceptionHandler.end = callSite.bits() + 1;
340             newExceptionHandler.nativeCode = catchLabel;
341             m_codeBlock-&gt;appendExceptionHandler(newExceptionHandler);
342         }
343     }
344 
345     if (m_pcToCodeOriginMapBuilder.didBuildMapping())
<span class="line-modified">346         m_codeBlock-&gt;setPCToCodeOriginMap(std::make_unique&lt;PCToCodeOriginMap&gt;(WTFMove(m_pcToCodeOriginMapBuilder), linkBuffer));</span>
347 }
348 
349 static void emitStackOverflowCheck(JITCompiler&amp; jit, MacroAssembler::JumpList&amp; stackOverflow)
350 {
351     int frameTopOffset = virtualRegisterForLocal(jit.graph().requiredRegisterCountForExecutionAndExit() - 1).offset() * sizeof(Register);
352     unsigned maxFrameSize = -frameTopOffset;
353 
354     jit.addPtr(MacroAssembler::TrustedImm32(frameTopOffset), GPRInfo::callFrameRegister, GPRInfo::regT1);
355     if (UNLIKELY(maxFrameSize &gt; Options::reservedZoneSize()))
356         stackOverflow.append(jit.branchPtr(MacroAssembler::Above, GPRInfo::regT1, GPRInfo::callFrameRegister));
<span class="line-modified">357     stackOverflow.append(jit.branchPtr(MacroAssembler::Above, MacroAssembler::AbsoluteAddress(jit.vm()-&gt;addressOfSoftStackLimit()), GPRInfo::regT1));</span>
358 }
359 
360 void JITCompiler::compile()
361 {
362     makeCatchOSREntryBuffer();
363 
364     setStartOfCode();
365     compileEntry();
<span class="line-modified">366     m_speculative = std::make_unique&lt;SpeculativeJIT&gt;(*this);</span>
367 
368     // Plant a check that sufficient space is available in the JSStack.
369     JumpList stackOverflow;
370     emitStackOverflowCheck(*this, stackOverflow);
371 
372     addPtr(TrustedImm32(-(m_graph.frameRegisterCount() * sizeof(Register))), GPRInfo::callFrameRegister, stackPointerRegister);
373     if (Options::zeroStackFrame())
374         clearStackFrame(GPRInfo::callFrameRegister, stackPointerRegister, GPRInfo::regT0, m_graph.frameRegisterCount() * sizeof(Register));
375     checkStackPointerAlignment();
376     compileSetupRegistersForEntry();
377     compileEntryExecutionFlag();
378     compileBody();
379     setEndOfMainPath();
380 
381     // === Footer code generation ===
382     //
383     // Generate the stack overflow handling; if the stack check in the entry head fails,
384     // we need to call out to a helper function to throw the StackOverflowError.
385     stackOverflow.link(this);
386 
387     emitStoreCodeOrigin(CodeOrigin(0));
388 
389     if (maxFrameExtentForSlowPathCall)
390         addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
391 
392     m_speculative-&gt;callOperationWithCallFrameRollbackOnException(operationThrowStackOverflowError, m_codeBlock);
393 
394     // Generate slow path code.
395     m_speculative-&gt;runSlowPathGenerators(m_pcToCodeOriginMapBuilder);
396     m_pcToCodeOriginMapBuilder.appendItem(labelIgnoringWatchpoints(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
397 
398     compileExceptionHandlers();
399     linkOSRExits();
400 
401     // Create OSR entry trampolines if necessary.
402     m_speculative-&gt;createOSREntries();
403     setEndOfCode();
404 
<span class="line-modified">405     auto linkBuffer = std::make_unique&lt;LinkBuffer&gt;(*this, m_codeBlock, JITCompilationCanFail);</span>
406     if (linkBuffer-&gt;didFailToAllocate()) {
<span class="line-modified">407         m_graph.m_plan.setFinalizer(std::make_unique&lt;FailedFinalizer&gt;(m_graph.m_plan));</span>
408         return;
409     }
410 
411     link(*linkBuffer);
412     m_speculative-&gt;linkOSREntries(*linkBuffer);
413 
414     m_jitCode-&gt;shrinkToFit();
415     codeBlock()-&gt;shrinkToFit(CodeBlock::LateShrink);
416 
417     disassemble(*linkBuffer);
418 
<span class="line-modified">419     m_graph.m_plan.setFinalizer(std::make_unique&lt;JITFinalizer&gt;(</span>
420         m_graph.m_plan, m_jitCode.releaseNonNull(), WTFMove(linkBuffer)));
421 }
422 
423 void JITCompiler::compileFunction()
424 {
425     makeCatchOSREntryBuffer();
426 
427     setStartOfCode();
428     Label entryLabel(this);
429     compileEntry();
430 
431     // === Function header code generation ===
432     // This is the main entry point, without performing an arity check.
433     // If we needed to perform an arity check we will already have moved the return address,
434     // so enter after this.
435     Label fromArityCheck(this);
436     // Plant a check that sufficient space is available in the JSStack.
437     JumpList stackOverflow;
438     emitStackOverflowCheck(*this, stackOverflow);
439 
440     // Move the stack pointer down to accommodate locals
441     addPtr(TrustedImm32(-(m_graph.frameRegisterCount() * sizeof(Register))), GPRInfo::callFrameRegister, stackPointerRegister);
442     if (Options::zeroStackFrame())
443         clearStackFrame(GPRInfo::callFrameRegister, stackPointerRegister, GPRInfo::regT0, m_graph.frameRegisterCount() * sizeof(Register));
444     checkStackPointerAlignment();
445 
446     compileSetupRegistersForEntry();
447     compileEntryExecutionFlag();
448 
449     // === Function body code generation ===
<span class="line-modified">450     m_speculative = std::make_unique&lt;SpeculativeJIT&gt;(*this);</span>
451     compileBody();
452     setEndOfMainPath();
453 
454     // === Function footer code generation ===
455     //
456     // Generate code to perform the stack overflow handling (if the stack check in
457     // the function header fails), and generate the entry point with arity check.
458     //
459     // Generate the stack overflow handling; if the stack check in the function head fails,
460     // we need to call out to a helper function to throw the StackOverflowError.
461     stackOverflow.link(this);
462 
463     emitStoreCodeOrigin(CodeOrigin(0));
464 
465     if (maxFrameExtentForSlowPathCall)
466         addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
467 
468     m_speculative-&gt;callOperationWithCallFrameRollbackOnException(operationThrowStackOverflowError, m_codeBlock);
469 
470     // The fast entry point into a function does not check the correct number of arguments
</pre>
<hr />
<pre>
490         branchTest32(Zero, GPRInfo::returnValueGPR).linkTo(fromArityCheck, this);
491         emitStoreCodeOrigin(CodeOrigin(0));
492         move(GPRInfo::returnValueGPR, GPRInfo::argumentGPR0);
493         callArityFixup = nearCall();
494         jump(fromArityCheck);
495     } else
496         arityCheck = entryLabel;
497 
498     // Generate slow path code.
499     m_speculative-&gt;runSlowPathGenerators(m_pcToCodeOriginMapBuilder);
500     m_pcToCodeOriginMapBuilder.appendItem(labelIgnoringWatchpoints(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
501 
502     compileExceptionHandlers();
503     linkOSRExits();
504 
505     // Create OSR entry trampolines if necessary.
506     m_speculative-&gt;createOSREntries();
507     setEndOfCode();
508 
509     // === Link ===
<span class="line-modified">510     auto linkBuffer = std::make_unique&lt;LinkBuffer&gt;(*this, m_codeBlock, JITCompilationCanFail);</span>
511     if (linkBuffer-&gt;didFailToAllocate()) {
<span class="line-modified">512         m_graph.m_plan.setFinalizer(std::make_unique&lt;FailedFinalizer&gt;(m_graph.m_plan));</span>
513         return;
514     }
515     link(*linkBuffer);
516     m_speculative-&gt;linkOSREntries(*linkBuffer);
517 
518     m_jitCode-&gt;shrinkToFit();
519     codeBlock()-&gt;shrinkToFit(CodeBlock::LateShrink);
520 
521     if (requiresArityFixup)
<span class="line-modified">522         linkBuffer-&gt;link(callArityFixup, FunctionPtr&lt;JITThunkPtrTag&gt;(vm()-&gt;getCTIStub(arityFixupGenerator).code()));</span>
523 
524     disassemble(*linkBuffer);
525 
526     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; withArityCheck = linkBuffer-&gt;locationOf&lt;JSEntryPtrTag&gt;(arityCheck);
527 
<span class="line-modified">528     m_graph.m_plan.setFinalizer(std::make_unique&lt;JITFinalizer&gt;(</span>
529         m_graph.m_plan, m_jitCode.releaseNonNull(), WTFMove(linkBuffer), withArityCheck));
530 }
531 
532 void JITCompiler::disassemble(LinkBuffer&amp; linkBuffer)
533 {
534     if (shouldDumpDisassembly()) {
535         m_disassembler-&gt;dump(linkBuffer);
536         linkBuffer.didAlreadyDisassemble();
537     }
538 
539     if (UNLIKELY(m_graph.m_plan.compilation()))
540         m_disassembler-&gt;reportToProfiler(m_graph.m_plan.compilation(), linkBuffer);
541 }
542 
543 #if USE(JSVALUE32_64)
544 void* JITCompiler::addressOfDoubleConstant(Node* node)
545 {
546     double value = node-&gt;asNumber();
547     int64_t valueBits = bitwise_cast&lt;int64_t&gt;(value);
548     auto it = m_graph.m_doubleConstantsMap.find(valueBits);
549     if (it != m_graph.m_doubleConstantsMap.end())
550         return it-&gt;second;
551 
552     if (!m_graph.m_doubleConstants)
<span class="line-modified">553         m_graph.m_doubleConstants = std::make_unique&lt;Bag&lt;double&gt;&gt;();</span>
554 
555     double* addressInConstantPool = m_graph.m_doubleConstants-&gt;add();
556     *addressInConstantPool = value;
557     m_graph.m_doubleConstantsMap[valueBits] = addressInConstantPool;
558     return addressInConstantPool;
559 }
560 #endif
561 
562 void JITCompiler::noticeCatchEntrypoint(BasicBlock&amp; basicBlock, JITCompiler::Label blockHead, LinkBuffer&amp; linkBuffer, Vector&lt;FlushFormat&gt;&amp;&amp; argumentFormats)
563 {
564     RELEASE_ASSERT(basicBlock.isCatchEntrypoint);
565     RELEASE_ASSERT(basicBlock.intersectionOfCFAHasVisited); // An entrypoint is reachable by definition.
566     m_jitCode-&gt;common.appendCatchEntrypoint(basicBlock.bytecodeBegin, linkBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(blockHead), WTFMove(argumentFormats));
567 }
568 
569 void JITCompiler::noticeOSREntry(BasicBlock&amp; basicBlock, JITCompiler::Label blockHead, LinkBuffer&amp; linkBuffer)
570 {
571     RELEASE_ASSERT(!basicBlock.isCatchEntrypoint);
572 
573     // OSR entry is not allowed into blocks deemed unreachable by control flow analysis.
574     if (!basicBlock.intersectionOfCFAHasVisited)
575         return;
576 
577     OSREntryData* entry = m_jitCode-&gt;appendOSREntryData(basicBlock.bytecodeBegin, linkBuffer.locationOf&lt;OSREntryPtrTag&gt;(blockHead));
578 
579     entry-&gt;m_expectedValues = basicBlock.intersectionOfPastValuesAtHead;
580 
581     // Fix the expected values: in our protocol, a dead variable will have an expected
582     // value of (None, []). But the old JIT may stash some values there. So we really
583     // need (Top, TOP).
584     for (size_t argument = 0; argument &lt; basicBlock.variablesAtHead.numberOfArguments(); ++argument) {
585         Node* node = basicBlock.variablesAtHead.argument(argument);
586         if (!node || !node-&gt;shouldGenerate())
<span class="line-modified">587             entry-&gt;m_expectedValues.argument(argument).makeHeapTop();</span>
588     }
589     for (size_t local = 0; local &lt; basicBlock.variablesAtHead.numberOfLocals(); ++local) {
590         Node* node = basicBlock.variablesAtHead.local(local);
591         if (!node || !node-&gt;shouldGenerate())
<span class="line-modified">592             entry-&gt;m_expectedValues.local(local).makeHeapTop();</span>
593         else {
594             VariableAccessData* variable = node-&gt;variableAccessData();
595             entry-&gt;m_machineStackUsed.set(variable-&gt;machineLocal().toLocal());
596 
597             switch (variable-&gt;flushFormat()) {
598             case FlushedDouble:
599                 entry-&gt;m_localsForcedDouble.set(local);
600                 break;
601             case FlushedInt52:
602                 entry-&gt;m_localsForcedAnyInt.set(local);
603                 break;
604             default:
605                 break;
606             }
607 
608             if (variable-&gt;local() != variable-&gt;machineLocal()) {
609                 entry-&gt;m_reshufflings.append(
610                     OSREntryReshuffling(
611                         variable-&gt;local().offset(), variable-&gt;machineLocal().offset()));
612             }
</pre>
<hr />
<pre>
636     // in the OSR exit value recovery. It&#39;s more defensible for the string concatenation,
637     // then, to not be caught by the for loops&#39; try/catch.
638     // Here is the program I&#39;m speaking about:
639     //
640     // &gt;&gt;&gt;&gt; lets presume &quot;c = a + b&quot; gets hoisted here.
641     // for (var i = 0; i &lt; length; i++) {
642     //     try {
643     //         c = a + b
644     //     } catch(e) {
645     //         If we threw an out of memory error, and we cought the exception
646     //         right here, then &quot;i&quot; would almost certainly be undefined, which
647     //         would make no sense.
648     //         ...
649     //     }
650     // }
651     CodeOrigin opCatchOrigin;
652     HandlerInfo* exceptionHandler;
653     bool willCatchException = m_graph.willCatchExceptionInMachineFrame(m_speculative-&gt;m_currentNode-&gt;origin.forExit, opCatchOrigin, exceptionHandler);
654     if (willCatchException) {
655         unsigned streamIndex = m_speculative-&gt;m_outOfLineStreamIndex ? *m_speculative-&gt;m_outOfLineStreamIndex : m_speculative-&gt;m_stream-&gt;size();
<span class="line-modified">656         MacroAssembler::Jump hadException = emitNonPatchableExceptionCheck(*vm());</span>
657         // We assume here that this is called after callOpeartion()/appendCall() is called.
658         appendExceptionHandlingOSRExit(ExceptionCheck, streamIndex, opCatchOrigin, exceptionHandler, m_jitCode-&gt;common.lastCallSite(), hadException);
659     } else
<span class="line-modified">660         m_exceptionChecks.append(emitExceptionCheck(*vm()));</span>
661 }
662 
663 CallSiteIndex JITCompiler::recordCallSiteAndGenerateExceptionHandlingOSRExitIfNeeded(const CodeOrigin&amp; callSiteCodeOrigin, unsigned eventStreamIndex)
664 {
665     CodeOrigin opCatchOrigin;
666     HandlerInfo* exceptionHandler;
667     bool willCatchException = m_graph.willCatchExceptionInMachineFrame(callSiteCodeOrigin, opCatchOrigin, exceptionHandler);
668     CallSiteIndex callSite = addCallSite(callSiteCodeOrigin);
669     if (willCatchException)
670         appendExceptionHandlingOSRExit(GenericUnwind, eventStreamIndex, opCatchOrigin, exceptionHandler, callSite);
671     return callSite;
672 }
673 
674 void JITCompiler::setEndOfMainPath()
675 {
676     m_pcToCodeOriginMapBuilder.appendItem(labelIgnoringWatchpoints(), m_speculative-&gt;m_origin.semantic);
677     if (LIKELY(!m_disassembler))
678         return;
679     m_disassembler-&gt;setEndOfMainPath(labelIgnoringWatchpoints());
680 }
681 
682 void JITCompiler::setEndOfCode()
683 {
684     m_pcToCodeOriginMapBuilder.appendItem(labelIgnoringWatchpoints(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
685     if (LIKELY(!m_disassembler))
686         return;
687     m_disassembler-&gt;setEndOfCode(labelIgnoringWatchpoints());
688 }
689 
690 void JITCompiler::makeCatchOSREntryBuffer()
691 {
692     if (m_graph.m_maxLocalsForCatchOSREntry) {
693         uint32_t numberOfLiveLocals = std::max(*m_graph.m_maxLocalsForCatchOSREntry, 1u); // Make sure we always allocate a non-null catchOSREntryBuffer.
<span class="line-modified">694         m_jitCode-&gt;common.catchOSREntryBuffer = vm()-&gt;scratchBufferForSize(sizeof(JSValue) * numberOfLiveLocals);</span>
695     }
696 }
697 
698 } } // namespace JSC::DFG
699 
700 #endif // ENABLE(DFG_JIT)
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (C) 2011-2019 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
</pre>
<hr />
<pre>
 40 #include &quot;DFGSpeculativeJIT.h&quot;
 41 #include &quot;DFGThunks.h&quot;
 42 #include &quot;JSCInlines.h&quot;
 43 #include &quot;JSCJSValueInlines.h&quot;
 44 #include &quot;LinkBuffer.h&quot;
 45 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;
 46 #include &quot;StructureStubInfo.h&quot;
 47 #include &quot;ThunkGenerators.h&quot;
 48 #include &quot;VM.h&quot;
 49 
 50 namespace JSC { namespace DFG {
 51 
 52 JITCompiler::JITCompiler(Graph&amp; dfg)
 53     : CCallHelpers(dfg.m_codeBlock)
 54     , m_graph(dfg)
 55     , m_jitCode(adoptRef(new JITCode()))
 56     , m_blockHeads(dfg.numBlocks())
 57     , m_pcToCodeOriginMapBuilder(dfg.m_vm)
 58 {
 59     if (UNLIKELY(shouldDumpDisassembly() || m_graph.m_vm.m_perBytecodeProfiler))
<span class="line-modified"> 60         m_disassembler = makeUnique&lt;Disassembler&gt;(dfg);</span>
 61 #if ENABLE(FTL_JIT)
 62     m_jitCode-&gt;tierUpInLoopHierarchy = WTFMove(m_graph.m_plan.tierUpInLoopHierarchy());
 63     for (unsigned tierUpBytecode : m_graph.m_plan.tierUpAndOSREnterBytecodes())
 64         m_jitCode-&gt;tierUpEntryTriggers.add(tierUpBytecode, JITCode::TriggerReason::DontTrigger);
 65 #endif
 66 }
 67 
 68 JITCompiler::~JITCompiler()
 69 {
 70 }
 71 
 72 void JITCompiler::linkOSRExits()
 73 {
 74     ASSERT(m_jitCode-&gt;osrExit.size() == m_exitCompilationInfo.size());
 75     if (UNLIKELY(m_graph.compilation())) {
 76         for (unsigned i = 0; i &lt; m_jitCode-&gt;osrExit.size(); ++i) {
 77             OSRExitCompilationInfo&amp; info = m_exitCompilationInfo[i];
 78             Vector&lt;Label&gt; labels;
 79             if (!info.m_failureJumps.empty()) {
 80                 for (unsigned j = 0; j &lt; info.m_failureJumps.jumps().size(); ++j)
 81                     labels.append(info.m_failureJumps.jumps()[j].label());
 82             } else
 83                 labels.append(info.m_replacementSource);
 84             m_exitSiteLabels.append(labels);
 85         }
 86     }
 87 
<span class="line-modified"> 88     MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; osrExitThunk = vm().getCTIStub(osrExitThunkGenerator);</span>
 89     auto osrExitThunkLabel = CodeLocationLabel&lt;JITThunkPtrTag&gt;(osrExitThunk.code());
 90     for (unsigned i = 0; i &lt; m_jitCode-&gt;osrExit.size(); ++i) {
 91         OSRExitCompilationInfo&amp; info = m_exitCompilationInfo[i];
 92         JumpList&amp; failureJumps = info.m_failureJumps;
 93         if (!failureJumps.empty())
 94             failureJumps.link(this);
 95         else
 96             info.m_replacementDestination = label();
 97 
 98         jitAssertHasValidCallFrame();
<span class="line-modified"> 99         store32(TrustedImm32(i), &amp;vm().osrExitIndex);</span>
100         if (Options::useProbeOSRExit()) {
101             Jump target = jump();
102             addLinkTask([target, osrExitThunkLabel] (LinkBuffer&amp; linkBuffer) {
103                 linkBuffer.link(target, osrExitThunkLabel);
104             });
105         } else
106             info.m_patchableJump = patchableJump();
107     }
108 }
109 
110 void JITCompiler::compileEntry()
111 {
112     // This code currently matches the old JIT. In the function header we need to
113     // save return address and call frame via the prologue and perform a fast stack check.
114     // FIXME: https://bugs.webkit.org/show_bug.cgi?id=56292
115     // We&#39;ll need to convert the remaining cti_ style calls (specifically the stack
116     // check) which will be dependent on stack layout. (We&#39;d need to account for this in
117     // both normal return code and when jumping to an exception handler).
118     emitFunctionPrologue();
119     emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
</pre>
<hr />
<pre>
130 #if ENABLE(FTL_JIT)
131     if (m_graph.m_plan.canTierUpAndOSREnter())
132         store8(TrustedImm32(0), &amp;m_jitCode-&gt;neverExecutedEntry);
133 #endif // ENABLE(FTL_JIT)
134 }
135 
136 void JITCompiler::compileBody()
137 {
138     // We generate the speculative code path, followed by OSR exit code to return
139     // to the old JIT code if speculations fail.
140 
141     bool compiledSpeculative = m_speculative-&gt;compile();
142     ASSERT_UNUSED(compiledSpeculative, compiledSpeculative);
143 }
144 
145 void JITCompiler::compileExceptionHandlers()
146 {
147     if (!m_exceptionChecksWithCallFrameRollback.empty()) {
148         m_exceptionChecksWithCallFrameRollback.link(this);
149 
<span class="line-modified">150         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
151 
152         // lookupExceptionHandlerFromCallerFrame is passed two arguments, the VM and the exec (the CallFrame*).
<span class="line-modified">153         move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);</span>
154         move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
155         addPtr(TrustedImm32(m_graph.stackPointerOffset() * sizeof(Register)), GPRInfo::callFrameRegister, stackPointerRegister);
156 
157 #if CPU(X86)
158         // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!
159         poke(GPRInfo::argumentGPR0);
160         poke(GPRInfo::argumentGPR1, 1);
161 #endif
162         m_calls.append(CallLinkRecord(call(OperationPtrTag), FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandlerFromCallerFrame)));
163 
<span class="line-modified">164         jumpToExceptionHandler(vm());</span>
165     }
166 
167     if (!m_exceptionChecks.empty()) {
168         m_exceptionChecks.link(this);
169 
<span class="line-modified">170         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
171 
172         // lookupExceptionHandler is passed two arguments, the VM and the exec (the CallFrame*).
<span class="line-modified">173         move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);</span>
174         move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
175 
176 #if CPU(X86)
177         // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!
178         poke(GPRInfo::argumentGPR0);
179         poke(GPRInfo::argumentGPR1, 1);
180 #endif
181         m_calls.append(CallLinkRecord(call(OperationPtrTag), FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandler)));
182 
<span class="line-modified">183         jumpToExceptionHandler(vm());</span>
184     }
185 }
186 
187 void JITCompiler::link(LinkBuffer&amp; linkBuffer)
188 {
189     // Link the code, populate data in CodeBlock data structures.
190     m_jitCode-&gt;common.frameRegisterCount = m_graph.frameRegisterCount();
191     m_jitCode-&gt;common.requiredRegisterCountForExit = m_graph.requiredRegisterCountForExit();
192 
193     if (!m_graph.m_plan.inlineCallFrames()-&gt;isEmpty())
194         m_jitCode-&gt;common.inlineCallFrames = m_graph.m_plan.inlineCallFrames();
195 
196 #if USE(JSVALUE32_64)
197     m_jitCode-&gt;common.doubleConstants = WTFMove(m_graph.m_doubleConstants);
198 #endif
199 
200     m_graph.registerFrozenValues();
201 
202     BitVector usedJumpTables;
203     for (Bag&lt;SwitchData&gt;::iterator iter = m_graph.m_switchData.begin(); !!iter; ++iter) {
</pre>
<hr />
<pre>
249         for (iter = table.offsetTable.begin(); iter != end; ++iter)
250             iter-&gt;value.ctiOffset = table.ctiDefault;
251         for (unsigned j = data.cases.size(); j--;) {
252             SwitchCase&amp; myCase = data.cases[j];
253             iter = table.offsetTable.find(myCase.value.stringImpl());
254             RELEASE_ASSERT(iter != end);
255             iter-&gt;value.ctiOffset = linkBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_blockHeads[myCase.target.block-&gt;index]);
256         }
257     }
258 
259     // Link all calls out from the JIT code to their respective functions.
260     for (unsigned i = 0; i &lt; m_calls.size(); ++i)
261         linkBuffer.link(m_calls[i].m_call, m_calls[i].m_function);
262 
263     finalizeInlineCaches(m_getByIds, linkBuffer);
264     finalizeInlineCaches(m_getByIdsWithThis, linkBuffer);
265     finalizeInlineCaches(m_putByIds, linkBuffer);
266     finalizeInlineCaches(m_inByIds, linkBuffer);
267     finalizeInlineCaches(m_instanceOfs, linkBuffer);
268 
<span class="line-modified">269     auto linkCallThunk = FunctionPtr&lt;NoPtrTag&gt;(vm().getCTIStub(linkCallThunkGenerator).retaggedCode&lt;NoPtrTag&gt;());</span>
270     for (auto&amp; record : m_jsCalls) {
271         CallLinkInfo&amp; info = *record.info;
272         linkBuffer.link(record.slowCall, linkCallThunk);
273         info.setCallLocations(
274             CodeLocationLabel&lt;JSInternalPtrTag&gt;(linkBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(record.slowCall)),
275             CodeLocationLabel&lt;JSInternalPtrTag&gt;(linkBuffer.locationOf&lt;JSInternalPtrTag&gt;(record.targetToCheck)),
276             linkBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(record.fastCall));
277     }
278 
279     for (JSDirectCallRecord&amp; record : m_jsDirectCalls) {
280         CallLinkInfo&amp; info = *record.info;
281         linkBuffer.link(record.call, linkBuffer.locationOf&lt;NoPtrTag&gt;(record.slowPath));
282         info.setCallLocations(
283             CodeLocationLabel&lt;JSInternalPtrTag&gt;(),
284             linkBuffer.locationOf&lt;JSInternalPtrTag&gt;(record.slowPath),
285             linkBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(record.call));
286     }
287 
288     for (JSDirectTailCallRecord&amp; record : m_jsDirectTailCalls) {
289         CallLinkInfo&amp; info = *record.info;
290         info.setCallLocations(
291             linkBuffer.locationOf&lt;JSInternalPtrTag&gt;(record.patchableJump),
292             linkBuffer.locationOf&lt;JSInternalPtrTag&gt;(record.slowPath),
293             linkBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(record.call));
294     }
295 
<span class="line-modified">296     MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; osrExitThunk = vm().getCTIStub(osrExitGenerationThunkGenerator);</span>
297     auto target = CodeLocationLabel&lt;JITThunkPtrTag&gt;(osrExitThunk.code());
298     for (unsigned i = 0; i &lt; m_jitCode-&gt;osrExit.size(); ++i) {
299         OSRExitCompilationInfo&amp; info = m_exitCompilationInfo[i];
300         if (!Options::useProbeOSRExit()) {
301             linkBuffer.link(info.m_patchableJump.m_jump, target);
302             OSRExit&amp; exit = m_jitCode-&gt;osrExit[i];
303             exit.m_patchableJumpLocation = linkBuffer.locationOf&lt;JSInternalPtrTag&gt;(info.m_patchableJump);
304         }
305         if (info.m_replacementSource.isSet()) {
306             m_jitCode-&gt;common.jumpReplacements.append(JumpReplacement(
307                 linkBuffer.locationOf&lt;JSInternalPtrTag&gt;(info.m_replacementSource),
308                 linkBuffer.locationOf&lt;OSRExitPtrTag&gt;(info.m_replacementDestination)));
309         }
310     }
311 
312     if (UNLIKELY(m_graph.compilation())) {
313         ASSERT(m_exitSiteLabels.size() == m_jitCode-&gt;osrExit.size());
314         for (unsigned i = 0; i &lt; m_exitSiteLabels.size(); ++i) {
315             Vector&lt;Label&gt;&amp; labels = m_exitSiteLabels[i];
316             Vector&lt;MacroAssemblerCodePtr&lt;JSInternalPtrTag&gt;&gt; addresses;
</pre>
<hr />
<pre>
326     // Link new DFG exception handlers and remove baseline JIT handlers.
327     m_codeBlock-&gt;clearExceptionHandlers();
328     for (unsigned  i = 0; i &lt; m_exceptionHandlerOSRExitCallSites.size(); i++) {
329         OSRExitCompilationInfo&amp; info = m_exceptionHandlerOSRExitCallSites[i].exitInfo;
330         if (info.m_replacementDestination.isSet()) {
331             // If this is is *not* set, it means that we already jumped to the OSR exit in pure generated control flow.
332             // i.e, we explicitly emitted an exceptionCheck that we know will be caught in this machine frame.
333             // If this *is set*, it means we will be landing at this code location from genericUnwind from an
334             // exception thrown in a child call frame.
335             CodeLocationLabel&lt;ExceptionHandlerPtrTag&gt; catchLabel = linkBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(info.m_replacementDestination);
336             HandlerInfo newExceptionHandler = m_exceptionHandlerOSRExitCallSites[i].baselineExceptionHandler;
337             CallSiteIndex callSite = m_exceptionHandlerOSRExitCallSites[i].callSiteIndex;
338             newExceptionHandler.start = callSite.bits();
339             newExceptionHandler.end = callSite.bits() + 1;
340             newExceptionHandler.nativeCode = catchLabel;
341             m_codeBlock-&gt;appendExceptionHandler(newExceptionHandler);
342         }
343     }
344 
345     if (m_pcToCodeOriginMapBuilder.didBuildMapping())
<span class="line-modified">346         m_codeBlock-&gt;setPCToCodeOriginMap(makeUnique&lt;PCToCodeOriginMap&gt;(WTFMove(m_pcToCodeOriginMapBuilder), linkBuffer));</span>
347 }
348 
349 static void emitStackOverflowCheck(JITCompiler&amp; jit, MacroAssembler::JumpList&amp; stackOverflow)
350 {
351     int frameTopOffset = virtualRegisterForLocal(jit.graph().requiredRegisterCountForExecutionAndExit() - 1).offset() * sizeof(Register);
352     unsigned maxFrameSize = -frameTopOffset;
353 
354     jit.addPtr(MacroAssembler::TrustedImm32(frameTopOffset), GPRInfo::callFrameRegister, GPRInfo::regT1);
355     if (UNLIKELY(maxFrameSize &gt; Options::reservedZoneSize()))
356         stackOverflow.append(jit.branchPtr(MacroAssembler::Above, GPRInfo::regT1, GPRInfo::callFrameRegister));
<span class="line-modified">357     stackOverflow.append(jit.branchPtr(MacroAssembler::Above, MacroAssembler::AbsoluteAddress(jit.vm().addressOfSoftStackLimit()), GPRInfo::regT1));</span>
358 }
359 
360 void JITCompiler::compile()
361 {
362     makeCatchOSREntryBuffer();
363 
364     setStartOfCode();
365     compileEntry();
<span class="line-modified">366     m_speculative = makeUnique&lt;SpeculativeJIT&gt;(*this);</span>
367 
368     // Plant a check that sufficient space is available in the JSStack.
369     JumpList stackOverflow;
370     emitStackOverflowCheck(*this, stackOverflow);
371 
372     addPtr(TrustedImm32(-(m_graph.frameRegisterCount() * sizeof(Register))), GPRInfo::callFrameRegister, stackPointerRegister);
373     if (Options::zeroStackFrame())
374         clearStackFrame(GPRInfo::callFrameRegister, stackPointerRegister, GPRInfo::regT0, m_graph.frameRegisterCount() * sizeof(Register));
375     checkStackPointerAlignment();
376     compileSetupRegistersForEntry();
377     compileEntryExecutionFlag();
378     compileBody();
379     setEndOfMainPath();
380 
381     // === Footer code generation ===
382     //
383     // Generate the stack overflow handling; if the stack check in the entry head fails,
384     // we need to call out to a helper function to throw the StackOverflowError.
385     stackOverflow.link(this);
386 
387     emitStoreCodeOrigin(CodeOrigin(0));
388 
389     if (maxFrameExtentForSlowPathCall)
390         addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
391 
392     m_speculative-&gt;callOperationWithCallFrameRollbackOnException(operationThrowStackOverflowError, m_codeBlock);
393 
394     // Generate slow path code.
395     m_speculative-&gt;runSlowPathGenerators(m_pcToCodeOriginMapBuilder);
396     m_pcToCodeOriginMapBuilder.appendItem(labelIgnoringWatchpoints(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
397 
398     compileExceptionHandlers();
399     linkOSRExits();
400 
401     // Create OSR entry trampolines if necessary.
402     m_speculative-&gt;createOSREntries();
403     setEndOfCode();
404 
<span class="line-modified">405     auto linkBuffer = makeUnique&lt;LinkBuffer&gt;(*this, m_codeBlock, JITCompilationCanFail);</span>
406     if (linkBuffer-&gt;didFailToAllocate()) {
<span class="line-modified">407         m_graph.m_plan.setFinalizer(makeUnique&lt;FailedFinalizer&gt;(m_graph.m_plan));</span>
408         return;
409     }
410 
411     link(*linkBuffer);
412     m_speculative-&gt;linkOSREntries(*linkBuffer);
413 
414     m_jitCode-&gt;shrinkToFit();
415     codeBlock()-&gt;shrinkToFit(CodeBlock::LateShrink);
416 
417     disassemble(*linkBuffer);
418 
<span class="line-modified">419     m_graph.m_plan.setFinalizer(makeUnique&lt;JITFinalizer&gt;(</span>
420         m_graph.m_plan, m_jitCode.releaseNonNull(), WTFMove(linkBuffer)));
421 }
422 
423 void JITCompiler::compileFunction()
424 {
425     makeCatchOSREntryBuffer();
426 
427     setStartOfCode();
428     Label entryLabel(this);
429     compileEntry();
430 
431     // === Function header code generation ===
432     // This is the main entry point, without performing an arity check.
433     // If we needed to perform an arity check we will already have moved the return address,
434     // so enter after this.
435     Label fromArityCheck(this);
436     // Plant a check that sufficient space is available in the JSStack.
437     JumpList stackOverflow;
438     emitStackOverflowCheck(*this, stackOverflow);
439 
440     // Move the stack pointer down to accommodate locals
441     addPtr(TrustedImm32(-(m_graph.frameRegisterCount() * sizeof(Register))), GPRInfo::callFrameRegister, stackPointerRegister);
442     if (Options::zeroStackFrame())
443         clearStackFrame(GPRInfo::callFrameRegister, stackPointerRegister, GPRInfo::regT0, m_graph.frameRegisterCount() * sizeof(Register));
444     checkStackPointerAlignment();
445 
446     compileSetupRegistersForEntry();
447     compileEntryExecutionFlag();
448 
449     // === Function body code generation ===
<span class="line-modified">450     m_speculative = makeUnique&lt;SpeculativeJIT&gt;(*this);</span>
451     compileBody();
452     setEndOfMainPath();
453 
454     // === Function footer code generation ===
455     //
456     // Generate code to perform the stack overflow handling (if the stack check in
457     // the function header fails), and generate the entry point with arity check.
458     //
459     // Generate the stack overflow handling; if the stack check in the function head fails,
460     // we need to call out to a helper function to throw the StackOverflowError.
461     stackOverflow.link(this);
462 
463     emitStoreCodeOrigin(CodeOrigin(0));
464 
465     if (maxFrameExtentForSlowPathCall)
466         addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
467 
468     m_speculative-&gt;callOperationWithCallFrameRollbackOnException(operationThrowStackOverflowError, m_codeBlock);
469 
470     // The fast entry point into a function does not check the correct number of arguments
</pre>
<hr />
<pre>
490         branchTest32(Zero, GPRInfo::returnValueGPR).linkTo(fromArityCheck, this);
491         emitStoreCodeOrigin(CodeOrigin(0));
492         move(GPRInfo::returnValueGPR, GPRInfo::argumentGPR0);
493         callArityFixup = nearCall();
494         jump(fromArityCheck);
495     } else
496         arityCheck = entryLabel;
497 
498     // Generate slow path code.
499     m_speculative-&gt;runSlowPathGenerators(m_pcToCodeOriginMapBuilder);
500     m_pcToCodeOriginMapBuilder.appendItem(labelIgnoringWatchpoints(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
501 
502     compileExceptionHandlers();
503     linkOSRExits();
504 
505     // Create OSR entry trampolines if necessary.
506     m_speculative-&gt;createOSREntries();
507     setEndOfCode();
508 
509     // === Link ===
<span class="line-modified">510     auto linkBuffer = makeUnique&lt;LinkBuffer&gt;(*this, m_codeBlock, JITCompilationCanFail);</span>
511     if (linkBuffer-&gt;didFailToAllocate()) {
<span class="line-modified">512         m_graph.m_plan.setFinalizer(makeUnique&lt;FailedFinalizer&gt;(m_graph.m_plan));</span>
513         return;
514     }
515     link(*linkBuffer);
516     m_speculative-&gt;linkOSREntries(*linkBuffer);
517 
518     m_jitCode-&gt;shrinkToFit();
519     codeBlock()-&gt;shrinkToFit(CodeBlock::LateShrink);
520 
521     if (requiresArityFixup)
<span class="line-modified">522         linkBuffer-&gt;link(callArityFixup, FunctionPtr&lt;JITThunkPtrTag&gt;(vm().getCTIStub(arityFixupGenerator).code()));</span>
523 
524     disassemble(*linkBuffer);
525 
526     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; withArityCheck = linkBuffer-&gt;locationOf&lt;JSEntryPtrTag&gt;(arityCheck);
527 
<span class="line-modified">528     m_graph.m_plan.setFinalizer(makeUnique&lt;JITFinalizer&gt;(</span>
529         m_graph.m_plan, m_jitCode.releaseNonNull(), WTFMove(linkBuffer), withArityCheck));
530 }
531 
532 void JITCompiler::disassemble(LinkBuffer&amp; linkBuffer)
533 {
534     if (shouldDumpDisassembly()) {
535         m_disassembler-&gt;dump(linkBuffer);
536         linkBuffer.didAlreadyDisassemble();
537     }
538 
539     if (UNLIKELY(m_graph.m_plan.compilation()))
540         m_disassembler-&gt;reportToProfiler(m_graph.m_plan.compilation(), linkBuffer);
541 }
542 
543 #if USE(JSVALUE32_64)
544 void* JITCompiler::addressOfDoubleConstant(Node* node)
545 {
546     double value = node-&gt;asNumber();
547     int64_t valueBits = bitwise_cast&lt;int64_t&gt;(value);
548     auto it = m_graph.m_doubleConstantsMap.find(valueBits);
549     if (it != m_graph.m_doubleConstantsMap.end())
550         return it-&gt;second;
551 
552     if (!m_graph.m_doubleConstants)
<span class="line-modified">553         m_graph.m_doubleConstants = makeUnique&lt;Bag&lt;double&gt;&gt;();</span>
554 
555     double* addressInConstantPool = m_graph.m_doubleConstants-&gt;add();
556     *addressInConstantPool = value;
557     m_graph.m_doubleConstantsMap[valueBits] = addressInConstantPool;
558     return addressInConstantPool;
559 }
560 #endif
561 
562 void JITCompiler::noticeCatchEntrypoint(BasicBlock&amp; basicBlock, JITCompiler::Label blockHead, LinkBuffer&amp; linkBuffer, Vector&lt;FlushFormat&gt;&amp;&amp; argumentFormats)
563 {
564     RELEASE_ASSERT(basicBlock.isCatchEntrypoint);
565     RELEASE_ASSERT(basicBlock.intersectionOfCFAHasVisited); // An entrypoint is reachable by definition.
566     m_jitCode-&gt;common.appendCatchEntrypoint(basicBlock.bytecodeBegin, linkBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(blockHead), WTFMove(argumentFormats));
567 }
568 
569 void JITCompiler::noticeOSREntry(BasicBlock&amp; basicBlock, JITCompiler::Label blockHead, LinkBuffer&amp; linkBuffer)
570 {
571     RELEASE_ASSERT(!basicBlock.isCatchEntrypoint);
572 
573     // OSR entry is not allowed into blocks deemed unreachable by control flow analysis.
574     if (!basicBlock.intersectionOfCFAHasVisited)
575         return;
576 
577     OSREntryData* entry = m_jitCode-&gt;appendOSREntryData(basicBlock.bytecodeBegin, linkBuffer.locationOf&lt;OSREntryPtrTag&gt;(blockHead));
578 
579     entry-&gt;m_expectedValues = basicBlock.intersectionOfPastValuesAtHead;
580 
581     // Fix the expected values: in our protocol, a dead variable will have an expected
582     // value of (None, []). But the old JIT may stash some values there. So we really
583     // need (Top, TOP).
584     for (size_t argument = 0; argument &lt; basicBlock.variablesAtHead.numberOfArguments(); ++argument) {
585         Node* node = basicBlock.variablesAtHead.argument(argument);
586         if (!node || !node-&gt;shouldGenerate())
<span class="line-modified">587             entry-&gt;m_expectedValues.argument(argument).makeBytecodeTop();</span>
588     }
589     for (size_t local = 0; local &lt; basicBlock.variablesAtHead.numberOfLocals(); ++local) {
590         Node* node = basicBlock.variablesAtHead.local(local);
591         if (!node || !node-&gt;shouldGenerate())
<span class="line-modified">592             entry-&gt;m_expectedValues.local(local).makeBytecodeTop();</span>
593         else {
594             VariableAccessData* variable = node-&gt;variableAccessData();
595             entry-&gt;m_machineStackUsed.set(variable-&gt;machineLocal().toLocal());
596 
597             switch (variable-&gt;flushFormat()) {
598             case FlushedDouble:
599                 entry-&gt;m_localsForcedDouble.set(local);
600                 break;
601             case FlushedInt52:
602                 entry-&gt;m_localsForcedAnyInt.set(local);
603                 break;
604             default:
605                 break;
606             }
607 
608             if (variable-&gt;local() != variable-&gt;machineLocal()) {
609                 entry-&gt;m_reshufflings.append(
610                     OSREntryReshuffling(
611                         variable-&gt;local().offset(), variable-&gt;machineLocal().offset()));
612             }
</pre>
<hr />
<pre>
636     // in the OSR exit value recovery. It&#39;s more defensible for the string concatenation,
637     // then, to not be caught by the for loops&#39; try/catch.
638     // Here is the program I&#39;m speaking about:
639     //
640     // &gt;&gt;&gt;&gt; lets presume &quot;c = a + b&quot; gets hoisted here.
641     // for (var i = 0; i &lt; length; i++) {
642     //     try {
643     //         c = a + b
644     //     } catch(e) {
645     //         If we threw an out of memory error, and we cought the exception
646     //         right here, then &quot;i&quot; would almost certainly be undefined, which
647     //         would make no sense.
648     //         ...
649     //     }
650     // }
651     CodeOrigin opCatchOrigin;
652     HandlerInfo* exceptionHandler;
653     bool willCatchException = m_graph.willCatchExceptionInMachineFrame(m_speculative-&gt;m_currentNode-&gt;origin.forExit, opCatchOrigin, exceptionHandler);
654     if (willCatchException) {
655         unsigned streamIndex = m_speculative-&gt;m_outOfLineStreamIndex ? *m_speculative-&gt;m_outOfLineStreamIndex : m_speculative-&gt;m_stream-&gt;size();
<span class="line-modified">656         MacroAssembler::Jump hadException = emitNonPatchableExceptionCheck(vm());</span>
657         // We assume here that this is called after callOpeartion()/appendCall() is called.
658         appendExceptionHandlingOSRExit(ExceptionCheck, streamIndex, opCatchOrigin, exceptionHandler, m_jitCode-&gt;common.lastCallSite(), hadException);
659     } else
<span class="line-modified">660         m_exceptionChecks.append(emitExceptionCheck(vm()));</span>
661 }
662 
663 CallSiteIndex JITCompiler::recordCallSiteAndGenerateExceptionHandlingOSRExitIfNeeded(const CodeOrigin&amp; callSiteCodeOrigin, unsigned eventStreamIndex)
664 {
665     CodeOrigin opCatchOrigin;
666     HandlerInfo* exceptionHandler;
667     bool willCatchException = m_graph.willCatchExceptionInMachineFrame(callSiteCodeOrigin, opCatchOrigin, exceptionHandler);
668     CallSiteIndex callSite = addCallSite(callSiteCodeOrigin);
669     if (willCatchException)
670         appendExceptionHandlingOSRExit(GenericUnwind, eventStreamIndex, opCatchOrigin, exceptionHandler, callSite);
671     return callSite;
672 }
673 
674 void JITCompiler::setEndOfMainPath()
675 {
676     m_pcToCodeOriginMapBuilder.appendItem(labelIgnoringWatchpoints(), m_speculative-&gt;m_origin.semantic);
677     if (LIKELY(!m_disassembler))
678         return;
679     m_disassembler-&gt;setEndOfMainPath(labelIgnoringWatchpoints());
680 }
681 
682 void JITCompiler::setEndOfCode()
683 {
684     m_pcToCodeOriginMapBuilder.appendItem(labelIgnoringWatchpoints(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
685     if (LIKELY(!m_disassembler))
686         return;
687     m_disassembler-&gt;setEndOfCode(labelIgnoringWatchpoints());
688 }
689 
690 void JITCompiler::makeCatchOSREntryBuffer()
691 {
692     if (m_graph.m_maxLocalsForCatchOSREntry) {
693         uint32_t numberOfLiveLocals = std::max(*m_graph.m_maxLocalsForCatchOSREntry, 1u); // Make sure we always allocate a non-null catchOSREntryBuffer.
<span class="line-modified">694         m_jitCode-&gt;common.catchOSREntryBuffer = vm().scratchBufferForSize(sizeof(JSValue) * numberOfLiveLocals);</span>
695     }
696 }
697 
698 } } // namespace JSC::DFG
699 
700 #endif // ENABLE(DFG_JIT)
</pre>
</td>
</tr>
</table>
<center><a href="DFGJITCode.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGJITCompiler.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>