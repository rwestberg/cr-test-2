diff a/modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/Heap.cpp b/modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/Heap.cpp
--- a/modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/Heap.cpp
+++ b/modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/Heap.cpp
@@ -52,26 +52,26 @@
     RELEASE_BASSERT(vmPageSize() >= vmPageSizePhysical());
 
     initializeLineMetadata();
     initializePageMetadata();
 
-    BASSERT(!PerProcess<Environment>::get()->isDebugHeapEnabled());
+    BASSERT(!Environment::get()->isDebugHeapEnabled());
 
-        Gigacage::ensureGigacage();
+    Gigacage::ensureGigacage();
 #if GIGACAGE_ENABLED
-        if (usingGigacage()) {
-            RELEASE_BASSERT(gigacageBasePtr());
-            uint64_t random[2];
-            cryptoRandom(reinterpret_cast<unsigned char*>(random), sizeof(random));
-            size_t size = roundDownToMultipleOf(vmPageSize(), gigacageSize() - (random[0] % Gigacage::maximumCageSizeReductionForSlide));
-            ptrdiff_t offset = roundDownToMultipleOf(vmPageSize(), random[1] % (gigacageSize() - size));
-            void* base = reinterpret_cast<unsigned char*>(gigacageBasePtr()) + offset;
-            m_largeFree.add(LargeRange(base, size, 0, 0));
-        }
+    if (usingGigacage()) {
+        RELEASE_BASSERT(gigacageBasePtr());
+        uint64_t random[2];
+        cryptoRandom(reinterpret_cast<unsigned char*>(random), sizeof(random));
+        size_t size = roundDownToMultipleOf(vmPageSize(), gigacageSize() - (random[0] % Gigacage::maximumCageSizeReductionForSlide));
+        ptrdiff_t offset = roundDownToMultipleOf(vmPageSize(), random[1] % (gigacageSize() - size));
+        void* base = reinterpret_cast<unsigned char*>(gigacageBasePtr()) + offset;
+        m_largeFree.add(LargeRange(base, size, 0, 0));
+    }
 #endif
 
-    m_scavenger = PerProcess<Scavenger>::get();
+    m_scavenger = Scavenger::get();
 }
 
 bool Heap::usingGigacage()
 {
     return isGigacage(m_kind) && gigacageBasePtr();
@@ -173,17 +173,22 @@
 #if ENABLE_PHYSICAL_PAGE_MAP
     m_physicalPageMap.decommit(range.begin(), range.size());
 #endif
 }
 
-void Heap::scavenge(std::lock_guard<Mutex>& lock, BulkDecommit& decommitter)
+void Heap::scavenge(std::lock_guard<Mutex>& lock, BulkDecommit& decommitter, size_t& deferredDecommits)
 {
     for (auto& list : m_freePages) {
         for (auto* chunk : list) {
             for (auto* page : chunk->freePages()) {
                 if (!page->hasPhysicalPages())
                     continue;
+                if (page->usedSinceLastScavenge()) {
+                    page->clearUsedSinceLastScavenge();
+                    deferredDecommits++;
+                    continue;
+                }
 
                 size_t pageSize = bmalloc::pageSize(&list - &m_freePages[0]);
                 size_t decommitSize = physicalPageSizeSloppy(page->begin()->begin(), pageSize);
                 m_freeableMemory -= decommitSize;
                 m_footprint -= decommitSize;
@@ -200,27 +205,17 @@
         while (!list.isEmpty())
             deallocateSmallChunk(list.pop(), &list - &m_chunkCache[0]);
     }
 
     for (LargeRange& range : m_largeFree) {
-        m_highWatermark = std::min(m_highWatermark, static_cast<void*>(range.begin()));
+        if (range.usedSinceLastScavenge()) {
+            range.clearUsedSinceLastScavenge();
+            deferredDecommits++;
+            continue;
+        }
         decommitLargeRange(lock, range, decommitter);
     }
-
-    m_freeableMemory = 0;
-}
-
-void Heap::scavengeToHighWatermark(std::lock_guard<Mutex>& lock, BulkDecommit& decommitter)
-{
-    void* newHighWaterMark = nullptr;
-    for (LargeRange& range : m_largeFree) {
-        if (range.begin() <= m_highWatermark)
-            newHighWaterMark = std::min(newHighWaterMark, static_cast<void*>(range.begin()));
-        else
-            decommitLargeRange(lock, range, decommitter);
-    }
-    m_highWatermark = newHighWaterMark;
 }
 
 void Heap::deallocateLineCache(std::unique_lock<Mutex>&, LineCache& lineCache)
 {
     for (auto& list : lineCache) {
@@ -247,10 +242,11 @@
 
         m_objectTypes.set(chunk, ObjectType::Small);
 
         forEachPage(chunk, pageSize, [&](SmallPage* page) {
             page->setHasPhysicalPages(true);
+            page->setUsedSinceLastScavenge();
             page->setHasFreeLines(lock, true);
             chunk->freePages().push(page);
         });
 
         m_freeableMemory += chunkSize;
@@ -326,10 +322,11 @@
             page->setHasPhysicalPages(true);
 #if ENABLE_PHYSICAL_PAGE_MAP
             m_physicalPageMap.commit(page->begin()->begin(), pageSize);
 #endif
         }
+        page->setUsedSinceLastScavenge();
 
         return page;
     }();
 
     page->setSizeClass(sizeClass);
@@ -572,22 +569,21 @@
         }
 
         if (usingGigacage())
             return nullptr;
 
-        range = PerProcess<VMHeap>::get()->tryAllocateLargeChunk(alignment, size);
+        range = VMHeap::get()->tryAllocateLargeChunk(alignment, size);
         if (!range)
             return nullptr;
 
         m_largeFree.add(range);
         range = m_largeFree.remove(alignment, size);
     }
 
     m_freeableMemory -= range.totalPhysicalSize();
 
     void* result = splitAndAllocate(lock, range, alignment, size).begin();
-    m_highWatermark = std::max(m_highWatermark, result);
     return result;
 }
 
 void* Heap::allocateLarge(std::unique_lock<Mutex>& lock, size_t alignment, size_t size)
 {
