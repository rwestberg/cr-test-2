<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="MarkedBlock.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="MarkedBlockInlines.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
  2  *  Copyright (C) 1999-2000 Harri Porten (porten@kde.org)
  3  *  Copyright (C) 2001 Peter Kelly (pmk@post.com)
<span class="line-modified">  4  *  Copyright (C) 2003-2018 Apple Inc. All rights reserved.</span>
  5  *
  6  *  This library is free software; you can redistribute it and/or
  7  *  modify it under the terms of the GNU Lesser General Public
  8  *  License as published by the Free Software Foundation; either
  9  *  version 2 of the License, or (at your option) any later version.
 10  *
 11  *  This library is distributed in the hope that it will be useful,
 12  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 13  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 14  *  Lesser General Public License for more details.
 15  *
 16  *  You should have received a copy of the GNU Lesser General Public
 17  *  License along with this library; if not, write to the Free Software
 18  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 19  *
 20  */
 21 
 22 #pragma once
 23 
 24 #include &quot;CellAttributes.h&quot;
</pre>
<hr />
<pre>
107         friend class MarkedBlock;
108         friend struct VerifyMarked;
109     public:
110 
111         ~Handle();
112 
113         MarkedBlock&amp; block();
114         MarkedBlock::Footer&amp; blockFooter();
115 
116         void* cellAlign(void*);
117 
118         bool isEmpty();
119 
120         void lastChanceToFinalize();
121 
122         BlockDirectory* directory() const;
123         Subspace* subspace() const;
124         AlignedMemoryAllocator* alignedMemoryAllocator() const;
125         Heap* heap() const;
126         inline MarkedSpace* space() const;
<span class="line-modified">127         VM* vm() const;</span>
128         WeakSet&amp; weakSet();
129 
130         enum SweepMode { SweepOnly, SweepToFreeList };
131 
132         // Sweeping ensures that destructors get called and removes the block from the unswept
133         // set. Sweeping to free list also removes the block from the empty set, if it was in that
134         // set. Sweeping with SweepOnly may add this block to the empty set, if the block is found
135         // to be empty. The free-list being null implies SweepOnly.
136         //
137         // Note that you need to make sure that the empty bit reflects reality. If it&#39;s not set
138         // and the block is freshly created, then we&#39;ll make the mistake of running destructors in
139         // the block. If it&#39;s not set and the block has nothing marked, then we&#39;ll make the
140         // mistake of making a pop freelist rather than a bump freelist.
141         void sweep(FreeList*);
142 
143         // This is to be called by Subspace.
144         template&lt;typename DestroyFunc&gt;
145         void finishSweepKnowingHeapCellType(FreeList*, const DestroyFunc&amp;);
146 
147         void unsweepWithNoNewlyAllocated();
148 
<span class="line-removed">149         void zap(const FreeList&amp;);</span>
<span class="line-removed">150 </span>
151         void shrink();
152 
153         void visitWeakSet(SlotVisitor&amp;);
154         void reapWeakSet();
155 
156         // While allocating from a free list, MarkedBlock temporarily has bogus
157         // cell liveness data. To restore accurate cell liveness data, call one
158         // of these functions:
159         void didConsumeFreeList(); // Call this once you&#39;ve allocated all the items in the free list.
160         void stopAllocating(const FreeList&amp;);
161         void resumeAllocating(FreeList&amp;); // Call this if you canonicalized a block for some non-collection related purpose.
162 
163         size_t cellSize();
164         inline unsigned cellsPerBlock();
165 
166         const CellAttributes&amp; attributes() const;
167         DestructionMode destruction() const;
168         bool needsDestruction() const;
169         HeapCell::Kind cellKind() const;
170 
</pre>
<hr />
<pre>
183 
184         template &lt;typename Functor&gt; IterationStatus forEachCell(const Functor&amp;);
185         template &lt;typename Functor&gt; inline IterationStatus forEachLiveCell(const Functor&amp;);
186         template &lt;typename Functor&gt; inline IterationStatus forEachDeadCell(const Functor&amp;);
187         template &lt;typename Functor&gt; inline IterationStatus forEachMarkedCell(const Functor&amp;);
188 
189         JS_EXPORT_PRIVATE bool areMarksStale();
190         bool areMarksStaleForSweep();
191 
192         void assertMarksNotStale();
193 
194         bool isFreeListed() const { return m_isFreeListed; }
195 
196         size_t index() const { return m_index; }
197 
198         void removeFromDirectory();
199 
200         void didAddToDirectory(BlockDirectory*, size_t index);
201         void didRemoveFromDirectory();
202 




203         void dumpState(PrintStream&amp;);
204 
205     private:
206         Handle(Heap&amp;, AlignedMemoryAllocator*, void*);
207 
208         enum SweepDestructionMode { BlockHasNoDestructors, BlockHasDestructors, BlockHasDestructorsAndCollectorIsRunning };
209         enum ScribbleMode { DontScribble, Scribble };
210         enum EmptyMode { IsEmpty, NotEmpty };
211         enum NewlyAllocatedMode { HasNewlyAllocated, DoesNotHaveNewlyAllocated };
212         enum MarksMode { MarksStale, MarksNotStale };
213 
214         SweepDestructionMode sweepDestructionMode();
215         EmptyMode emptyMode();
216         ScribbleMode scribbleMode();
217         NewlyAllocatedMode newlyAllocatedMode();
218         MarksMode marksMode();
219 
220         template&lt;bool, EmptyMode, SweepMode, SweepDestructionMode, ScribbleMode, NewlyAllocatedMode, MarksMode, typename DestroyFunc&gt;
221         void specializedSweep(FreeList*, EmptyMode, SweepMode, SweepDestructionMode, ScribbleMode, NewlyAllocatedMode, MarksMode, const DestroyFunc&amp;);
222 
</pre>
<hr />
<pre>
238 
239         MarkedBlock* m_block { nullptr };
240     };
241 
242 private:
243     static constexpr size_t atomAlignmentMask = atomSize - 1;
244 
245     typedef char Atom[atomSize];
246 
247 public:
248     class Footer {
249     public:
250         Footer(VM&amp;, Handle&amp;);
251         ~Footer();
252 
253     private:
254         friend class LLIntOffsetsExtractor;
255         friend class MarkedBlock;
256 
257         Handle&amp; m_handle;


258         VM* m_vm;
259         Subspace* m_subspace;
260 
261         CountingLock m_lock;
262 
263         // The actual mark count can be computed by doing: m_biasedMarkCount - m_markCountBias. Note
264         // that this count is racy. It will accurately detect whether or not exactly zero things were
265         // marked, but if N things got marked, then this may report anything in the range [1, N] (or
266         // before unbiased, it would be [1 + m_markCountBias, N + m_markCountBias].)
267         int16_t m_biasedMarkCount;
268 
269         // We bias the mark count so that if m_biasedMarkCount &gt;= 0 then the block should be retired.
270         // We go to all this trouble to make marking a bit faster: this way, marking knows when to
271         // retire a block using a js/jns on m_biasedMarkCount.
272         //
273         // For example, if a block has room for 100 objects and retirement happens whenever 90% are
274         // live, then m_markCountBias will be -90. This way, when marking begins, this will cause us to
275         // set m_biasedMarkCount to -90 as well, since:
276         //
277         //     m_biasedMarkCount = actualMarkCount + m_markCountBias.
</pre>
<hr />
<pre>
287         //     m_biasedMarkCount != m_markCountBias
288         int16_t m_markCountBias;
289 
290         HeapVersion m_markingVersion;
291         HeapVersion m_newlyAllocatedVersion;
292 
293         Bitmap&lt;atomsPerBlock&gt; m_marks;
294         Bitmap&lt;atomsPerBlock&gt; m_newlyAllocated;
295     };
296 
297 private:
298     Footer&amp; footer();
299     const Footer&amp; footer() const;
300 
301 public:
302     static constexpr size_t endAtom = (blockSize - sizeof(Footer)) / atomSize;
303     static constexpr size_t payloadSize = endAtom * atomSize;
304     static constexpr size_t footerSize = blockSize - payloadSize;
305 
306     static_assert(payloadSize == ((blockSize - sizeof(MarkedBlock::Footer)) &amp; ~(atomSize - 1)), &quot;Payload size computed the alternate way should give the same result&quot;);
<span class="line-modified">307     static_assert(footerSize &gt;= minimumDistanceBetweenCellsFromDifferentOrigins, &quot;Footer is not big enough to create the necessary distance between objects from different origins&quot;);</span>


308 
309     static MarkedBlock::Handle* tryCreate(Heap&amp;, AlignedMemoryAllocator*);
310 
311     Handle&amp; handle();
312     const Handle&amp; handle() const;
313 
<span class="line-modified">314     VM* vm() const;</span>
315     inline Heap* heap() const;
316     inline MarkedSpace* space() const;
317 
318     static bool isAtomAligned(const void*);
319     static MarkedBlock* blockFor(const void*);
320     size_t atomNumber(const void*);
321 
322     size_t markCount();
323 
324     bool isMarked(const void*);
325     bool isMarked(HeapVersion markingVersion, const void*);
326     bool isMarked(const void*, Dependency);
327     bool testAndSetMarked(const void*, Dependency);
328 
329     bool isAtom(const void*);
330     void clearMarked(const void*);
331 
332     bool isNewlyAllocated(const void*);
333     void setNewlyAllocated(const void*);
334     void clearNewlyAllocated(const void*);
</pre>
<hr />
<pre>
359 
360     Dependency aboutToMark(HeapVersion markingVersion);
361 
362 #if ASSERT_DISABLED
363     void assertMarksNotStale() { }
364 #else
365     JS_EXPORT_PRIVATE void assertMarksNotStale();
366 #endif
367 
368     void resetMarks();
369 
370     bool isMarkedRaw(const void* p);
371     HeapVersion markingVersion() const { return footer().m_markingVersion; }
372 
373     const Bitmap&lt;atomsPerBlock&gt;&amp; marks() const;
374 
375     CountingLock&amp; lock() { return footer().m_lock; }
376 
377     Subspace* subspace() const { return footer().m_subspace; }
378 





379     static constexpr size_t offsetOfFooter = endAtom * atomSize;
380 
381 private:
382     MarkedBlock(VM&amp;, Handle&amp;);
383     ~MarkedBlock();
384     Atom* atoms();
385 
386     JS_EXPORT_PRIVATE void aboutToMarkSlow(HeapVersion markingVersion);
387     void clearHasAnyMarked();
388 
389     void noteMarkedSlow();
390 
391     inline bool marksConveyLivenessDuringMarking(HeapVersion markingVersion);
392     inline bool marksConveyLivenessDuringMarking(HeapVersion myMarkingVersion, HeapVersion markingVersion);
393 };
394 
395 inline MarkedBlock::Footer&amp; MarkedBlock::footer()
396 {
397     return *bitwise_cast&lt;MarkedBlock::Footer*&gt;(atoms() + endAtom);
398 }
</pre>
<hr />
<pre>
445 inline MarkedBlock* MarkedBlock::blockFor(const void* p)
446 {
447     return reinterpret_cast&lt;MarkedBlock*&gt;(reinterpret_cast&lt;uintptr_t&gt;(p) &amp; blockMask);
448 }
449 
450 inline BlockDirectory* MarkedBlock::Handle::directory() const
451 {
452     return m_directory;
453 }
454 
455 inline AlignedMemoryAllocator* MarkedBlock::Handle::alignedMemoryAllocator() const
456 {
457     return m_alignedMemoryAllocator;
458 }
459 
460 inline Heap* MarkedBlock::Handle::heap() const
461 {
462     return m_weakSet.heap();
463 }
464 
<span class="line-modified">465 inline VM* MarkedBlock::Handle::vm() const</span>
466 {
467     return m_weakSet.vm();
468 }
469 
<span class="line-modified">470 inline VM* MarkedBlock::vm() const</span>
471 {
<span class="line-modified">472     return footer().m_vm;</span>
473 }
474 
475 inline WeakSet&amp; MarkedBlock::Handle::weakSet()
476 {
477     return m_weakSet;
478 }
479 
480 inline WeakSet&amp; MarkedBlock::weakSet()
481 {
482     return handle().weakSet();
483 }
484 
485 inline void MarkedBlock::Handle::shrink()
486 {
487     m_weakSet.shrink();
488 }
489 
490 inline void MarkedBlock::Handle::visitWeakSet(SlotVisitor&amp; visitor)
491 {
492     return m_weakSet.visit(visitor);
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
  2  *  Copyright (C) 1999-2000 Harri Porten (porten@kde.org)
  3  *  Copyright (C) 2001 Peter Kelly (pmk@post.com)
<span class="line-modified">  4  *  Copyright (C) 2003-2019 Apple Inc. All rights reserved.</span>
  5  *
  6  *  This library is free software; you can redistribute it and/or
  7  *  modify it under the terms of the GNU Lesser General Public
  8  *  License as published by the Free Software Foundation; either
  9  *  version 2 of the License, or (at your option) any later version.
 10  *
 11  *  This library is distributed in the hope that it will be useful,
 12  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 13  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 14  *  Lesser General Public License for more details.
 15  *
 16  *  You should have received a copy of the GNU Lesser General Public
 17  *  License along with this library; if not, write to the Free Software
 18  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 19  *
 20  */
 21 
 22 #pragma once
 23 
 24 #include &quot;CellAttributes.h&quot;
</pre>
<hr />
<pre>
107         friend class MarkedBlock;
108         friend struct VerifyMarked;
109     public:
110 
111         ~Handle();
112 
113         MarkedBlock&amp; block();
114         MarkedBlock::Footer&amp; blockFooter();
115 
116         void* cellAlign(void*);
117 
118         bool isEmpty();
119 
120         void lastChanceToFinalize();
121 
122         BlockDirectory* directory() const;
123         Subspace* subspace() const;
124         AlignedMemoryAllocator* alignedMemoryAllocator() const;
125         Heap* heap() const;
126         inline MarkedSpace* space() const;
<span class="line-modified">127         VM&amp; vm() const;</span>
128         WeakSet&amp; weakSet();
129 
130         enum SweepMode { SweepOnly, SweepToFreeList };
131 
132         // Sweeping ensures that destructors get called and removes the block from the unswept
133         // set. Sweeping to free list also removes the block from the empty set, if it was in that
134         // set. Sweeping with SweepOnly may add this block to the empty set, if the block is found
135         // to be empty. The free-list being null implies SweepOnly.
136         //
137         // Note that you need to make sure that the empty bit reflects reality. If it&#39;s not set
138         // and the block is freshly created, then we&#39;ll make the mistake of running destructors in
139         // the block. If it&#39;s not set and the block has nothing marked, then we&#39;ll make the
140         // mistake of making a pop freelist rather than a bump freelist.
141         void sweep(FreeList*);
142 
143         // This is to be called by Subspace.
144         template&lt;typename DestroyFunc&gt;
145         void finishSweepKnowingHeapCellType(FreeList*, const DestroyFunc&amp;);
146 
147         void unsweepWithNoNewlyAllocated();
148 


149         void shrink();
150 
151         void visitWeakSet(SlotVisitor&amp;);
152         void reapWeakSet();
153 
154         // While allocating from a free list, MarkedBlock temporarily has bogus
155         // cell liveness data. To restore accurate cell liveness data, call one
156         // of these functions:
157         void didConsumeFreeList(); // Call this once you&#39;ve allocated all the items in the free list.
158         void stopAllocating(const FreeList&amp;);
159         void resumeAllocating(FreeList&amp;); // Call this if you canonicalized a block for some non-collection related purpose.
160 
161         size_t cellSize();
162         inline unsigned cellsPerBlock();
163 
164         const CellAttributes&amp; attributes() const;
165         DestructionMode destruction() const;
166         bool needsDestruction() const;
167         HeapCell::Kind cellKind() const;
168 
</pre>
<hr />
<pre>
181 
182         template &lt;typename Functor&gt; IterationStatus forEachCell(const Functor&amp;);
183         template &lt;typename Functor&gt; inline IterationStatus forEachLiveCell(const Functor&amp;);
184         template &lt;typename Functor&gt; inline IterationStatus forEachDeadCell(const Functor&amp;);
185         template &lt;typename Functor&gt; inline IterationStatus forEachMarkedCell(const Functor&amp;);
186 
187         JS_EXPORT_PRIVATE bool areMarksStale();
188         bool areMarksStaleForSweep();
189 
190         void assertMarksNotStale();
191 
192         bool isFreeListed() const { return m_isFreeListed; }
193 
194         size_t index() const { return m_index; }
195 
196         void removeFromDirectory();
197 
198         void didAddToDirectory(BlockDirectory*, size_t index);
199         void didRemoveFromDirectory();
200 
<span class="line-added">201         void* start() const { return &amp;m_block-&gt;atoms()[0]; }</span>
<span class="line-added">202         void* end() const { return &amp;m_block-&gt;atoms()[m_endAtom]; }</span>
<span class="line-added">203         bool contains(void* p) const { return start() &lt;= p &amp;&amp; p &lt; end(); }</span>
<span class="line-added">204 </span>
205         void dumpState(PrintStream&amp;);
206 
207     private:
208         Handle(Heap&amp;, AlignedMemoryAllocator*, void*);
209 
210         enum SweepDestructionMode { BlockHasNoDestructors, BlockHasDestructors, BlockHasDestructorsAndCollectorIsRunning };
211         enum ScribbleMode { DontScribble, Scribble };
212         enum EmptyMode { IsEmpty, NotEmpty };
213         enum NewlyAllocatedMode { HasNewlyAllocated, DoesNotHaveNewlyAllocated };
214         enum MarksMode { MarksStale, MarksNotStale };
215 
216         SweepDestructionMode sweepDestructionMode();
217         EmptyMode emptyMode();
218         ScribbleMode scribbleMode();
219         NewlyAllocatedMode newlyAllocatedMode();
220         MarksMode marksMode();
221 
222         template&lt;bool, EmptyMode, SweepMode, SweepDestructionMode, ScribbleMode, NewlyAllocatedMode, MarksMode, typename DestroyFunc&gt;
223         void specializedSweep(FreeList*, EmptyMode, SweepMode, SweepDestructionMode, ScribbleMode, NewlyAllocatedMode, MarksMode, const DestroyFunc&amp;);
224 
</pre>
<hr />
<pre>
240 
241         MarkedBlock* m_block { nullptr };
242     };
243 
244 private:
245     static constexpr size_t atomAlignmentMask = atomSize - 1;
246 
247     typedef char Atom[atomSize];
248 
249 public:
250     class Footer {
251     public:
252         Footer(VM&amp;, Handle&amp;);
253         ~Footer();
254 
255     private:
256         friend class LLIntOffsetsExtractor;
257         friend class MarkedBlock;
258 
259         Handle&amp; m_handle;
<span class="line-added">260         // m_vm must remain a pointer (instead of a reference) because JSCLLIntOffsetsExtractor</span>
<span class="line-added">261         // will fail otherwise.</span>
262         VM* m_vm;
263         Subspace* m_subspace;
264 
265         CountingLock m_lock;
266 
267         // The actual mark count can be computed by doing: m_biasedMarkCount - m_markCountBias. Note
268         // that this count is racy. It will accurately detect whether or not exactly zero things were
269         // marked, but if N things got marked, then this may report anything in the range [1, N] (or
270         // before unbiased, it would be [1 + m_markCountBias, N + m_markCountBias].)
271         int16_t m_biasedMarkCount;
272 
273         // We bias the mark count so that if m_biasedMarkCount &gt;= 0 then the block should be retired.
274         // We go to all this trouble to make marking a bit faster: this way, marking knows when to
275         // retire a block using a js/jns on m_biasedMarkCount.
276         //
277         // For example, if a block has room for 100 objects and retirement happens whenever 90% are
278         // live, then m_markCountBias will be -90. This way, when marking begins, this will cause us to
279         // set m_biasedMarkCount to -90 as well, since:
280         //
281         //     m_biasedMarkCount = actualMarkCount + m_markCountBias.
</pre>
<hr />
<pre>
291         //     m_biasedMarkCount != m_markCountBias
292         int16_t m_markCountBias;
293 
294         HeapVersion m_markingVersion;
295         HeapVersion m_newlyAllocatedVersion;
296 
297         Bitmap&lt;atomsPerBlock&gt; m_marks;
298         Bitmap&lt;atomsPerBlock&gt; m_newlyAllocated;
299     };
300 
301 private:
302     Footer&amp; footer();
303     const Footer&amp; footer() const;
304 
305 public:
306     static constexpr size_t endAtom = (blockSize - sizeof(Footer)) / atomSize;
307     static constexpr size_t payloadSize = endAtom * atomSize;
308     static constexpr size_t footerSize = blockSize - payloadSize;
309 
310     static_assert(payloadSize == ((blockSize - sizeof(MarkedBlock::Footer)) &amp; ~(atomSize - 1)), &quot;Payload size computed the alternate way should give the same result&quot;);
<span class="line-modified">311     // Some of JSCell types assume that the last JSCell in a MarkedBlock has a subsequent memory region (Footer) that can still safely accessed.</span>
<span class="line-added">312     // For example, JSRopeString assumes that it can safely access up to 2 bytes beyond the JSRopeString cell.</span>
<span class="line-added">313     static_assert(sizeof(Footer) &gt;= sizeof(uint16_t));</span>
314 
315     static MarkedBlock::Handle* tryCreate(Heap&amp;, AlignedMemoryAllocator*);
316 
317     Handle&amp; handle();
318     const Handle&amp; handle() const;
319 
<span class="line-modified">320     VM&amp; vm() const;</span>
321     inline Heap* heap() const;
322     inline MarkedSpace* space() const;
323 
324     static bool isAtomAligned(const void*);
325     static MarkedBlock* blockFor(const void*);
326     size_t atomNumber(const void*);
327 
328     size_t markCount();
329 
330     bool isMarked(const void*);
331     bool isMarked(HeapVersion markingVersion, const void*);
332     bool isMarked(const void*, Dependency);
333     bool testAndSetMarked(const void*, Dependency);
334 
335     bool isAtom(const void*);
336     void clearMarked(const void*);
337 
338     bool isNewlyAllocated(const void*);
339     void setNewlyAllocated(const void*);
340     void clearNewlyAllocated(const void*);
</pre>
<hr />
<pre>
365 
366     Dependency aboutToMark(HeapVersion markingVersion);
367 
368 #if ASSERT_DISABLED
369     void assertMarksNotStale() { }
370 #else
371     JS_EXPORT_PRIVATE void assertMarksNotStale();
372 #endif
373 
374     void resetMarks();
375 
376     bool isMarkedRaw(const void* p);
377     HeapVersion markingVersion() const { return footer().m_markingVersion; }
378 
379     const Bitmap&lt;atomsPerBlock&gt;&amp; marks() const;
380 
381     CountingLock&amp; lock() { return footer().m_lock; }
382 
383     Subspace* subspace() const { return footer().m_subspace; }
384 
<span class="line-added">385     void populatePage() const</span>
<span class="line-added">386     {</span>
<span class="line-added">387         *bitwise_cast&lt;volatile uint8_t*&gt;(&amp;footer());</span>
<span class="line-added">388     }</span>
<span class="line-added">389 </span>
390     static constexpr size_t offsetOfFooter = endAtom * atomSize;
391 
392 private:
393     MarkedBlock(VM&amp;, Handle&amp;);
394     ~MarkedBlock();
395     Atom* atoms();
396 
397     JS_EXPORT_PRIVATE void aboutToMarkSlow(HeapVersion markingVersion);
398     void clearHasAnyMarked();
399 
400     void noteMarkedSlow();
401 
402     inline bool marksConveyLivenessDuringMarking(HeapVersion markingVersion);
403     inline bool marksConveyLivenessDuringMarking(HeapVersion myMarkingVersion, HeapVersion markingVersion);
404 };
405 
406 inline MarkedBlock::Footer&amp; MarkedBlock::footer()
407 {
408     return *bitwise_cast&lt;MarkedBlock::Footer*&gt;(atoms() + endAtom);
409 }
</pre>
<hr />
<pre>
456 inline MarkedBlock* MarkedBlock::blockFor(const void* p)
457 {
458     return reinterpret_cast&lt;MarkedBlock*&gt;(reinterpret_cast&lt;uintptr_t&gt;(p) &amp; blockMask);
459 }
460 
461 inline BlockDirectory* MarkedBlock::Handle::directory() const
462 {
463     return m_directory;
464 }
465 
466 inline AlignedMemoryAllocator* MarkedBlock::Handle::alignedMemoryAllocator() const
467 {
468     return m_alignedMemoryAllocator;
469 }
470 
471 inline Heap* MarkedBlock::Handle::heap() const
472 {
473     return m_weakSet.heap();
474 }
475 
<span class="line-modified">476 inline VM&amp; MarkedBlock::Handle::vm() const</span>
477 {
478     return m_weakSet.vm();
479 }
480 
<span class="line-modified">481 inline VM&amp; MarkedBlock::vm() const</span>
482 {
<span class="line-modified">483     return *footer().m_vm;</span>
484 }
485 
486 inline WeakSet&amp; MarkedBlock::Handle::weakSet()
487 {
488     return m_weakSet;
489 }
490 
491 inline WeakSet&amp; MarkedBlock::weakSet()
492 {
493     return handle().weakSet();
494 }
495 
496 inline void MarkedBlock::Handle::shrink()
497 {
498     m_weakSet.shrink();
499 }
500 
501 inline void MarkedBlock::Handle::visitWeakSet(SlotVisitor&amp; visitor)
502 {
503     return m_weakSet.visit(visitor);
</pre>
</td>
</tr>
</table>
<center><a href="MarkedBlock.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="MarkedBlockInlines.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>