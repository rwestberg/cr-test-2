diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (C) 2011-2018 Apple Inc. All rights reserved.
+ * Copyright (C) 2011-2019 Apple Inc. All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions
  * are met:
  * 1. Redistributions of source code must retain the above copyright
@@ -36,10 +36,13 @@
 #include "SuperSampler.h"
 #include "SweepingScope.h"
 #include <wtf/CommaPrinter.h>
 
 namespace JSC {
+namespace MarkedBlockInternal {
+static constexpr bool verbose = false;
+}
 
 const size_t MarkedBlock::blockSize;
 
 static const bool computeBalance = false;
 static size_t balance;
@@ -61,11 +64,11 @@
 
 MarkedBlock::Handle::Handle(Heap& heap, AlignedMemoryAllocator* alignedMemoryAllocator, void* blockSpace)
     : m_alignedMemoryAllocator(alignedMemoryAllocator)
     , m_weakSet(heap.vm(), CellContainer())
 {
-    m_block = new (NotNull, blockSpace) MarkedBlock(*heap.vm(), *this);
+    m_block = new (NotNull, blockSpace) MarkedBlock(heap.vm(), *this);
 
     m_weakSet.setContainer(*m_block);
 
     heap.didAllocateBlock(blockSize);
 }
@@ -85,11 +88,11 @@
 }
 
 MarkedBlock::MarkedBlock(VM& vm, Handle& handle)
 {
     new (&footer()) Footer(vm, handle);
-    if (false)
+    if (MarkedBlockInternal::verbose)
         dataLog(RawPointer(this), ": Allocated.\n");
 }
 
 MarkedBlock::~MarkedBlock()
 {
@@ -122,24 +125,24 @@
 
 void MarkedBlock::Handle::stopAllocating(const FreeList& freeList)
 {
     auto locker = holdLock(blockFooter().m_lock);
 
-    if (false)
+    if (MarkedBlockInternal::verbose)
         dataLog(RawPointer(this), ": MarkedBlock::Handle::stopAllocating!\n");
     ASSERT(!directory()->isAllocated(NoLockingNecessary, this));
 
     if (!isFreeListed()) {
-        if (false)
+        if (MarkedBlockInternal::verbose)
             dataLog("There ain't no newly allocated.\n");
         // This means that we either didn't use this block at all for allocation since last GC,
         // or someone had already done stopAllocating() before.
         ASSERT(freeList.allocationWillFail());
         return;
     }
 
-    if (false)
+    if (MarkedBlockInternal::verbose)
         dataLog("Free list: ", freeList, "\n");
 
     // Roll back to a coherent state for Heap introspection. Cells newly
     // allocated from our free list are not currently marked, so we need another
     // way to tell what's live vs dead.
@@ -153,14 +156,14 @@
             return IterationStatus::Continue;
         });
 
     freeList.forEach(
         [&] (HeapCell* cell) {
-            if (false)
+            if (MarkedBlockInternal::verbose)
                 dataLog("Free cell: ", RawPointer(cell), "\n");
             if (m_attributes.destruction == NeedsDestruction)
-                cell->zap();
+                cell->zap(HeapCell::StopAllocating);
             block().clearNewlyAllocated(cell);
         });
 
     m_isFreeListed = false;
 }
@@ -181,17 +184,17 @@
 void MarkedBlock::Handle::resumeAllocating(FreeList& freeList)
 {
     {
         auto locker = holdLock(blockFooter().m_lock);
 
-        if (false)
+        if (MarkedBlockInternal::verbose)
             dataLog(RawPointer(this), ": MarkedBlock::Handle::resumeAllocating!\n");
         ASSERT(!directory()->isAllocated(NoLockingNecessary, this));
         ASSERT(!isFreeListed());
 
         if (!block().hasAnyNewlyAllocated()) {
-            if (false)
+            if (MarkedBlockInternal::verbose)
                 dataLog("There ain't no newly allocated.\n");
             // This means we had already exhausted the block when we stopped allocation.
             freeList.clear();
             return;
         }
@@ -200,42 +203,33 @@
     // Re-create our free list from before stopping allocation. Note that this may return an empty
     // freelist, in which case the block will still be Marked!
     sweep(&freeList);
 }
 
-void MarkedBlock::Handle::zap(const FreeList& freeList)
-{
-    freeList.forEach(
-        [&] (HeapCell* cell) {
-            if (m_attributes.destruction == NeedsDestruction)
-                cell->zap();
-        });
-}
-
 void MarkedBlock::aboutToMarkSlow(HeapVersion markingVersion)
 {
-    ASSERT(vm()->heap.objectSpace().isMarking());
+    ASSERT(vm().heap.objectSpace().isMarking());
     auto locker = holdLock(footer().m_lock);
 
     if (!areMarksStale(markingVersion))
         return;
 
     BlockDirectory* directory = handle().directory();
 
     if (handle().directory()->isAllocated(holdLock(directory->bitvectorLock()), &handle())
         || !marksConveyLivenessDuringMarking(markingVersion)) {
-        if (false)
+        if (MarkedBlockInternal::verbose)
             dataLog(RawPointer(this), ": Clearing marks without doing anything else.\n");
         // We already know that the block is full and is already recognized as such, or that the
         // block did not survive the previous GC. So, we can clear mark bits the old fashioned
         // way. Note that it's possible for such a block to have newlyAllocated with an up-to-
         // date version! If it does, then we want to leave the newlyAllocated alone, since that
         // means that we had allocated in this previously empty block but did not fill it up, so
         // we created a newlyAllocated.
         footer().m_marks.clearAll();
     } else {
-        if (false)
+        if (MarkedBlockInternal::verbose)
             dataLog(RawPointer(this), ": Doing things.\n");
         HeapVersion newlyAllocatedVersion = space()->newlyAllocatedVersion();
         if (footer().m_newlyAllocatedVersion == newlyAllocatedVersion) {
             // When do we get here? The block could not have been filled up. The newlyAllocated bits would
             // have had to be created since the end of the last collection. The only things that create
@@ -279,33 +273,33 @@
 }
 
 #if !ASSERT_DISABLED
 void MarkedBlock::assertMarksNotStale()
 {
-    ASSERT(footer().m_markingVersion == vm()->heap.objectSpace().markingVersion());
+    ASSERT(footer().m_markingVersion == vm().heap.objectSpace().markingVersion());
 }
 #endif // !ASSERT_DISABLED
 
 bool MarkedBlock::areMarksStale()
 {
-    return areMarksStale(vm()->heap.objectSpace().markingVersion());
+    return areMarksStale(vm().heap.objectSpace().markingVersion());
 }
 
 bool MarkedBlock::Handle::areMarksStale()
 {
     return m_block->areMarksStale();
 }
 
 bool MarkedBlock::isMarked(const void* p)
 {
-    return isMarked(vm()->heap.objectSpace().markingVersion(), p);
+    return isMarked(vm().heap.objectSpace().markingVersion(), p);
 }
 
 void MarkedBlock::Handle::didConsumeFreeList()
 {
     auto locker = holdLock(blockFooter().m_lock);
-    if (false)
+    if (MarkedBlockInternal::verbose)
         dataLog(RawPointer(this), ": MarkedBlock::Handle::didConsumeFreeList!\n");
     ASSERT(isFreeListed());
     m_isFreeListed = false;
     directory()->setIsAllocated(NoLockingNecessary, this, true);
 }
@@ -375,11 +369,11 @@
 }
 
 #if !ASSERT_DISABLED
 void MarkedBlock::assertValidCell(VM& vm, HeapCell* cell) const
 {
-    RELEASE_ASSERT(&vm == this->vm());
+    RELEASE_ASSERT(&vm == &this->vm());
     RELEASE_ASSERT(const_cast<MarkedBlock*>(this)->handle().cellAlign(cell) == cell);
 }
 #endif
 
 void MarkedBlock::Handle::dumpState(PrintStream& out)
