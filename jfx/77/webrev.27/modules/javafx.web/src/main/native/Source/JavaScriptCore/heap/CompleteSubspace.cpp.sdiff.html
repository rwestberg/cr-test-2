<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/CompleteSubspace.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="CellContainerInlines.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CompleteSubspace.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/CompleteSubspace.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 63     size_t sizeClass = MarkedSpace::s_sizeClassForSizeStep[index];
 64     if (!sizeClass)
 65         return Allocator();
 66 
 67     // This is written in such a way that it&#39;s OK for the JIT threads to end up here if they want
 68     // to generate code that uses some allocator that hadn&#39;t been used yet. Note that a possibly-
 69     // just-as-good solution would be to return null if we&#39;re in the JIT since the JIT treats null
 70     // allocator as &quot;please always take the slow path&quot;. But, that could lead to performance
 71     // surprises and the algorithm here is pretty easy. Only this code has to hold the lock, to
 72     // prevent simultaneously BlockDirectory creations from multiple threads. This code ensures
 73     // that any &quot;forEachAllocator&quot; traversals will only see this allocator after it&#39;s initialized
 74     // enough: it will have
 75     auto locker = holdLock(m_space.directoryLock());
 76     if (Allocator allocator = m_allocatorForSizeStep[index])
 77         return allocator;
 78 
 79     if (false)
 80         dataLog(&quot;Creating BlockDirectory/LocalAllocator for &quot;, m_name, &quot;, &quot;, attributes(), &quot;, &quot;, sizeClass, &quot;.\n&quot;);
 81 
 82     std::unique_ptr&lt;BlockDirectory&gt; uniqueDirectory =
<span class="line-modified"> 83         std::make_unique&lt;BlockDirectory&gt;(m_space.heap(), sizeClass);</span>
 84     BlockDirectory* directory = uniqueDirectory.get();
 85     m_directories.append(WTFMove(uniqueDirectory));
 86 
 87     directory-&gt;setSubspace(this);
 88     m_space.addBlockDirectory(locker, directory);
 89 
 90     std::unique_ptr&lt;LocalAllocator&gt; uniqueLocalAllocator =
<span class="line-modified"> 91         std::make_unique&lt;LocalAllocator&gt;(directory);</span>
 92     LocalAllocator* localAllocator = uniqueLocalAllocator.get();
 93     m_localAllocators.append(WTFMove(uniqueLocalAllocator));
 94 
 95     Allocator allocator(localAllocator);
 96 
 97     index = MarkedSpace::sizeClassToIndex(sizeClass);
 98     for (;;) {
 99         if (MarkedSpace::s_sizeClassForSizeStep[index] != sizeClass)
100             break;
101 
102         m_allocatorForSizeStep[index] = allocator;
103 
104         if (!index--)
105             break;
106     }
107 
108     directory-&gt;setNextDirectoryInSubspace(m_firstDirectory);
109     m_alignedMemoryAllocator-&gt;registerDirectory(directory);
110     WTF::storeStoreFence();
111     m_firstDirectory = directory;
112     return allocator;
113 }
114 
115 void* CompleteSubspace::allocateSlow(VM&amp; vm, size_t size, GCDeferralContext* deferralContext, AllocationFailureMode failureMode)
116 {
117     void* result = tryAllocateSlow(vm, size, deferralContext);
118     if (failureMode == AllocationFailureMode::Assert)
119         RELEASE_ASSERT(result);
120     return result;
121 }
122 
123 void* CompleteSubspace::tryAllocateSlow(VM&amp; vm, size_t size, GCDeferralContext* deferralContext)
124 {
125     if (validateDFGDoesGC)
126         RELEASE_ASSERT(vm.heap.expectDoesGC());
127 
<span class="line-modified">128     sanitizeStackForVM(&amp;vm);</span>
129 
130     if (Allocator allocator = allocatorFor(size, AllocatorForMode::EnsureAllocator))
131         return allocator.allocate(deferralContext, AllocationFailureMode::ReturnNull);
132 
133     if (size &lt;= Options::largeAllocationCutoff()
134         &amp;&amp; size &lt;= MarkedSpace::largeCutoff) {
135         dataLog(&quot;FATAL: attampting to allocate small object using large allocation.\n&quot;);
136         dataLog(&quot;Requested allocation size: &quot;, size, &quot;\n&quot;);
137         RELEASE_ASSERT_NOT_REACHED();
138     }
139 
140     vm.heap.collectIfNecessaryOrDefer(deferralContext);
141 
142     size = WTF::roundUpToMultipleOf&lt;MarkedSpace::sizeStep&gt;(size);
<span class="line-modified">143     LargeAllocation* allocation = LargeAllocation::tryCreate(vm.heap, size, this);</span>
144     if (!allocation)
145         return nullptr;
146 
147     m_space.m_largeAllocations.append(allocation);

148     vm.heap.didAllocate(size);
149     m_space.m_capacity += size;
150 
151     m_largeAllocations.append(allocation);
152 
153     return allocation-&gt;cell();
154 }
155 

















































156 } // namespace JSC
157 
</pre>
</td>
<td>
<hr />
<pre>
 63     size_t sizeClass = MarkedSpace::s_sizeClassForSizeStep[index];
 64     if (!sizeClass)
 65         return Allocator();
 66 
 67     // This is written in such a way that it&#39;s OK for the JIT threads to end up here if they want
 68     // to generate code that uses some allocator that hadn&#39;t been used yet. Note that a possibly-
 69     // just-as-good solution would be to return null if we&#39;re in the JIT since the JIT treats null
 70     // allocator as &quot;please always take the slow path&quot;. But, that could lead to performance
 71     // surprises and the algorithm here is pretty easy. Only this code has to hold the lock, to
 72     // prevent simultaneously BlockDirectory creations from multiple threads. This code ensures
 73     // that any &quot;forEachAllocator&quot; traversals will only see this allocator after it&#39;s initialized
 74     // enough: it will have
 75     auto locker = holdLock(m_space.directoryLock());
 76     if (Allocator allocator = m_allocatorForSizeStep[index])
 77         return allocator;
 78 
 79     if (false)
 80         dataLog(&quot;Creating BlockDirectory/LocalAllocator for &quot;, m_name, &quot;, &quot;, attributes(), &quot;, &quot;, sizeClass, &quot;.\n&quot;);
 81 
 82     std::unique_ptr&lt;BlockDirectory&gt; uniqueDirectory =
<span class="line-modified"> 83         makeUnique&lt;BlockDirectory&gt;(m_space.heap(), sizeClass);</span>
 84     BlockDirectory* directory = uniqueDirectory.get();
 85     m_directories.append(WTFMove(uniqueDirectory));
 86 
 87     directory-&gt;setSubspace(this);
 88     m_space.addBlockDirectory(locker, directory);
 89 
 90     std::unique_ptr&lt;LocalAllocator&gt; uniqueLocalAllocator =
<span class="line-modified"> 91         makeUnique&lt;LocalAllocator&gt;(directory);</span>
 92     LocalAllocator* localAllocator = uniqueLocalAllocator.get();
 93     m_localAllocators.append(WTFMove(uniqueLocalAllocator));
 94 
 95     Allocator allocator(localAllocator);
 96 
 97     index = MarkedSpace::sizeClassToIndex(sizeClass);
 98     for (;;) {
 99         if (MarkedSpace::s_sizeClassForSizeStep[index] != sizeClass)
100             break;
101 
102         m_allocatorForSizeStep[index] = allocator;
103 
104         if (!index--)
105             break;
106     }
107 
108     directory-&gt;setNextDirectoryInSubspace(m_firstDirectory);
109     m_alignedMemoryAllocator-&gt;registerDirectory(directory);
110     WTF::storeStoreFence();
111     m_firstDirectory = directory;
112     return allocator;
113 }
114 
115 void* CompleteSubspace::allocateSlow(VM&amp; vm, size_t size, GCDeferralContext* deferralContext, AllocationFailureMode failureMode)
116 {
117     void* result = tryAllocateSlow(vm, size, deferralContext);
118     if (failureMode == AllocationFailureMode::Assert)
119         RELEASE_ASSERT(result);
120     return result;
121 }
122 
123 void* CompleteSubspace::tryAllocateSlow(VM&amp; vm, size_t size, GCDeferralContext* deferralContext)
124 {
125     if (validateDFGDoesGC)
126         RELEASE_ASSERT(vm.heap.expectDoesGC());
127 
<span class="line-modified">128     sanitizeStackForVM(vm);</span>
129 
130     if (Allocator allocator = allocatorFor(size, AllocatorForMode::EnsureAllocator))
131         return allocator.allocate(deferralContext, AllocationFailureMode::ReturnNull);
132 
133     if (size &lt;= Options::largeAllocationCutoff()
134         &amp;&amp; size &lt;= MarkedSpace::largeCutoff) {
135         dataLog(&quot;FATAL: attampting to allocate small object using large allocation.\n&quot;);
136         dataLog(&quot;Requested allocation size: &quot;, size, &quot;\n&quot;);
137         RELEASE_ASSERT_NOT_REACHED();
138     }
139 
140     vm.heap.collectIfNecessaryOrDefer(deferralContext);
141 
142     size = WTF::roundUpToMultipleOf&lt;MarkedSpace::sizeStep&gt;(size);
<span class="line-modified">143     LargeAllocation* allocation = LargeAllocation::tryCreate(vm.heap, size, this, m_space.m_largeAllocations.size());</span>
144     if (!allocation)
145         return nullptr;
146 
147     m_space.m_largeAllocations.append(allocation);
<span class="line-added">148     ASSERT(allocation-&gt;indexInSpace() == m_space.m_largeAllocations.size() - 1);</span>
149     vm.heap.didAllocate(size);
150     m_space.m_capacity += size;
151 
152     m_largeAllocations.append(allocation);
153 
154     return allocation-&gt;cell();
155 }
156 
<span class="line-added">157 void* CompleteSubspace::reallocateLargeAllocationNonVirtual(VM&amp; vm, HeapCell* oldCell, size_t size, GCDeferralContext* deferralContext, AllocationFailureMode failureMode)</span>
<span class="line-added">158 {</span>
<span class="line-added">159     if (validateDFGDoesGC)</span>
<span class="line-added">160         RELEASE_ASSERT(vm.heap.expectDoesGC());</span>
<span class="line-added">161 </span>
<span class="line-added">162     // The following conditions are met in Butterfly for example.</span>
<span class="line-added">163     ASSERT(oldCell-&gt;isLargeAllocation());</span>
<span class="line-added">164 </span>
<span class="line-added">165     LargeAllocation* oldAllocation = &amp;oldCell-&gt;largeAllocation();</span>
<span class="line-added">166     ASSERT(oldAllocation-&gt;cellSize() &lt;= size);</span>
<span class="line-added">167     ASSERT(oldAllocation-&gt;weakSet().isTriviallyDestructible());</span>
<span class="line-added">168     ASSERT(oldAllocation-&gt;attributes().destruction == DoesNotNeedDestruction);</span>
<span class="line-added">169     ASSERT(oldAllocation-&gt;attributes().cellKind == HeapCell::Auxiliary);</span>
<span class="line-added">170     ASSERT(size &gt; MarkedSpace::largeCutoff);</span>
<span class="line-added">171 </span>
<span class="line-added">172     sanitizeStackForVM(vm);</span>
<span class="line-added">173 </span>
<span class="line-added">174     if (size &lt;= Options::largeAllocationCutoff()</span>
<span class="line-added">175         &amp;&amp; size &lt;= MarkedSpace::largeCutoff) {</span>
<span class="line-added">176         dataLog(&quot;FATAL: attampting to allocate small object using large allocation.\n&quot;);</span>
<span class="line-added">177         dataLog(&quot;Requested allocation size: &quot;, size, &quot;\n&quot;);</span>
<span class="line-added">178         RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added">179     }</span>
<span class="line-added">180 </span>
<span class="line-added">181     vm.heap.collectIfNecessaryOrDefer(deferralContext);</span>
<span class="line-added">182 </span>
<span class="line-added">183     size = WTF::roundUpToMultipleOf&lt;MarkedSpace::sizeStep&gt;(size);</span>
<span class="line-added">184     size_t difference = size - oldAllocation-&gt;cellSize();</span>
<span class="line-added">185     unsigned oldIndexInSpace = oldAllocation-&gt;indexInSpace();</span>
<span class="line-added">186     if (oldAllocation-&gt;isOnList())</span>
<span class="line-added">187         oldAllocation-&gt;remove();</span>
<span class="line-added">188 </span>
<span class="line-added">189     LargeAllocation* allocation = oldAllocation-&gt;tryReallocate(size, this);</span>
<span class="line-added">190     if (!allocation) {</span>
<span class="line-added">191         RELEASE_ASSERT(failureMode != AllocationFailureMode::Assert);</span>
<span class="line-added">192         m_largeAllocations.append(oldAllocation);</span>
<span class="line-added">193         return nullptr;</span>
<span class="line-added">194     }</span>
<span class="line-added">195     ASSERT(oldIndexInSpace == allocation-&gt;indexInSpace());</span>
<span class="line-added">196 </span>
<span class="line-added">197     m_space.m_largeAllocations[oldIndexInSpace] = allocation;</span>
<span class="line-added">198     vm.heap.didAllocate(difference);</span>
<span class="line-added">199     m_space.m_capacity += difference;</span>
<span class="line-added">200 </span>
<span class="line-added">201     m_largeAllocations.append(allocation);</span>
<span class="line-added">202 </span>
<span class="line-added">203     return allocation-&gt;cell();</span>
<span class="line-added">204 }</span>
<span class="line-added">205 </span>
206 } // namespace JSC
207 
</pre>
</td>
</tr>
</table>
<center><a href="CellContainerInlines.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CompleteSubspace.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>