<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter64.asm</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="LowLevelInterpreter32_64.asm.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="../offlineasm/arm.rb.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter64.asm</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  13 # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
  14 # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  15 # PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
  16 # BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  17 # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  18 # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  19 # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  20 # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  21 # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
  22 # THE POSSIBILITY OF SUCH DAMAGE.
  23 
  24 
  25 # Utilities.
  26 
  27 macro nextInstruction()
  28     loadb [PB, PC, 1], t0
  29     leap _g_opcodeMap, t1
  30     jmp [t1, t0, PtrSize], BytecodePtrTag
  31 end
  32 
<span class="line-modified">  33 macro nextInstructionWide()</span>






  34     loadi 1[PB, PC, 1], t0
<span class="line-modified">  35     leap _g_opcodeMapWide, t1</span>
  36     jmp [t1, t0, PtrSize], BytecodePtrTag
  37 end
  38 
  39 macro getuOperandNarrow(opcodeStruct, fieldName, dst)
  40     loadb constexpr %opcodeStruct%_%fieldName%_index[PB, PC, 1], dst
  41 end
  42 
  43 macro getOperandNarrow(opcodeStruct, fieldName, dst)
<span class="line-modified">  44     loadbsp constexpr %opcodeStruct%_%fieldName%_index[PB, PC, 1], dst</span>




  45 end
  46 
<span class="line-modified">  47 macro getuOperandWide(opcodeStruct, fieldName, dst)</span>




  48     loadi constexpr %opcodeStruct%_%fieldName%_index * 4 + 1[PB, PC, 1], dst
  49 end
  50 
<span class="line-modified">  51 macro getOperandWide(opcodeStruct, fieldName, dst)</span>
  52     loadis constexpr %opcodeStruct%_%fieldName%_index * 4 + 1[PB, PC, 1], dst
  53 end
  54 
  55 macro makeReturn(get, dispatch, fn)
  56     fn(macro (value)
  57         move value, t2
  58         get(m_dst, t1)
  59         storeq t2, [cfr, t1, 8]
  60         dispatch()
  61     end)
  62 end
  63 
  64 macro makeReturnProfiled(opcodeStruct, get, metadata, dispatch, fn)
  65     fn(macro (value)
  66         move value, t3
  67         metadata(t1, t2)
  68         valueProfile(opcodeStruct, t1, t3)
  69         get(m_dst, t1)
  70         storeq t3, [cfr, t1, 8]
  71         dispatch()
</pre>
<hr />
<pre>
  92     if X86_64 or ARM64 or ARM64E
  93         call function
  94     elsif X86_64_WIN
  95         # Note: this implementation is only correct if the return type size is &gt; 8 bytes.
  96         # See macro cCall2Void for an implementation when the return type &lt;= 8 bytes.
  97         # On Win64, when the return type is larger than 8 bytes, we need to allocate space on the stack for the return value.
  98         # On entry rcx (a0), should contain a pointer to this stack space. The other parameters are shifted to the right,
  99         # rdx (a1) should contain the first argument, and r8 (a2) should contain the second argument.
 100         # On return, rax contains a pointer to this stack value, and we then need to copy the 16 byte return value into rax (r0) and rdx (r1)
 101         # since the return value is expected to be split between the two.
 102         # See http://msdn.microsoft.com/en-us/library/7572ztz4.aspx
 103         move a1, a2
 104         move a0, a1
 105         subp 48, sp
 106         move sp, a0
 107         addp 32, a0
 108         call function
 109         addp 48, sp
 110         move 8[r0], r1
 111         move [r0], r0
<span class="line-modified"> 112     elsif C_LOOP</span>
 113         cloopCallSlowPath function, a0, a1
 114     else
 115         error
 116     end
 117 end
 118 
 119 macro cCall2Void(function)
<span class="line-modified"> 120     if C_LOOP</span>
 121         cloopCallSlowPathVoid function, a0, a1
 122     elsif X86_64_WIN
 123         # Note: we cannot use the cCall2 macro for Win64 in this case,
 124         # as the Win64 cCall2 implemenation is only correct when the return type size is &gt; 8 bytes.
 125         # On Win64, rcx and rdx are used for passing the first two parameters.
 126         # We also need to make room on the stack for all four parameter registers.
 127         # See http://msdn.microsoft.com/en-us/library/ms235286.aspx
 128         subp 32, sp 
 129         call function
 130         addp 32, sp
 131     else
 132         cCall2(function)
 133     end
 134 end
 135 
 136 # This barely works. arg3 and arg4 should probably be immediates.
 137 macro cCall4(function)
 138     checkStackPointerAlignment(t4, 0xbad0c004)
 139     if X86_64 or ARM64 or ARM64E
 140         call function
</pre>
<hr />
<pre>
 162 
 163     checkStackPointerAlignment(t4, 0xbad0dc01)
 164 
 165     storep vm, VMEntryRecord::m_vm[sp]
 166     loadp VM::topCallFrame[vm], t4
 167     storep t4, VMEntryRecord::m_prevTopCallFrame[sp]
 168     loadp VM::topEntryFrame[vm], t4
 169     storep t4, VMEntryRecord::m_prevTopEntryFrame[sp]
 170     loadp ProtoCallFrame::calleeValue[protoCallFrame], t4
 171     storep t4, VMEntryRecord::m_callee[sp]
 172 
 173     loadi ProtoCallFrame::paddedArgCount[protoCallFrame], t4
 174     addp CallFrameHeaderSlots, t4, t4
 175     lshiftp 3, t4
 176     subp sp, t4, t3
 177     bqbeq sp, t3, .throwStackOverflow
 178 
 179     # Ensure that we have enough additional stack capacity for the incoming args,
 180     # and the frame for the JS code we&#39;re executing. We need to do this check
 181     # before we start copying the args from the protoCallFrame below.
<span class="line-modified"> 182     if C_LOOP</span>
 183         bpaeq t3, VM::m_cloopStackLimit[vm], .stackHeightOK
 184         move entry, t4
 185         move vm, t5
 186         cloopCallSlowPath _llint_stack_check_at_vm_entry, vm, t3
 187         bpeq t0, 0, .stackCheckFailed
 188         move t4, entry
 189         move t5, vm
 190         jmp .stackHeightOK
 191 
 192 .stackCheckFailed:
 193         move t4, entry
 194         move t5, vm
 195         jmp .throwStackOverflow
 196     else
 197         bpb t3, VM::m_softStackLimit[vm], .throwStackOverflow
 198     end
 199 
 200 .stackHeightOK:
 201     move t3, sp
 202     move (constexpr ProtoCallFrame::numberOfRegisters), t3
</pre>
<hr />
<pre>
 268     cCall2(_llint_throw_stack_overflow_error)
 269 
 270     vmEntryRecord(cfr, t4)
 271 
 272     loadp VMEntryRecord::m_vm[t4], vm
 273     loadp VMEntryRecord::m_prevTopCallFrame[t4], extraTempReg
 274     storep extraTempReg, VM::topCallFrame[vm]
 275     loadp VMEntryRecord::m_prevTopEntryFrame[t4], extraTempReg
 276     storep extraTempReg, VM::topEntryFrame[vm]
 277 
 278     subp cfr, CalleeRegisterSaveSize, sp
 279 
 280     popCalleeSaves()
 281     functionEpilogue()
 282     ret
 283 end
 284 
 285 
 286 macro makeJavaScriptCall(entry, temp, unused)
 287     addp 16, sp
<span class="line-modified"> 288     if C_LOOP</span>
 289         cloopCallJSFunction entry
 290     else
 291         call entry, JSEntryPtrTag
 292     end
 293     subp 16, sp
 294 end
 295 
 296 macro makeHostFunctionCall(entry, temp, unused)
 297     move entry, temp
 298     storep cfr, [sp]
 299     move sp, a0
<span class="line-modified"> 300     if C_LOOP</span>
 301         storep lr, 8[sp]
 302         cloopCallNative temp
 303     elsif X86_64_WIN
 304         # We need to allocate 32 bytes on the stack for the shadow space.
 305         subp 32, sp
 306         call temp, JSEntryPtrTag
 307         addp 32, sp
 308     else
 309         call temp, JSEntryPtrTag
 310     end
 311 end
 312 
 313 op(handleUncaughtException, macro ()
 314     loadp Callee[cfr], t3
 315     andp MarkedBlockMask, t3
 316     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
 317     restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(t3, t0)
 318     storep 0, VM::callFrameForCatch[t3]
 319 
 320     loadp VM::topEntryFrame[t3], cfr
</pre>
<hr />
<pre>
 391     loadi ArgumentCount + TagOffset[cfr], PC
 392 end
 393 
 394 macro checkSwitchToJITForLoop()
 395     checkSwitchToJIT(
 396         1,
 397         macro()
 398             storei PC, ArgumentCount + TagOffset[cfr]
 399             prepareStateForCCall()
 400             move cfr, a0
 401             move PC, a1
 402             cCall2(_llint_loop_osr)
 403             btpz r0, .recover
 404             move r1, sp
 405             jmp r0, JSEntryPtrTag
 406         .recover:
 407             loadi ArgumentCount + TagOffset[cfr], PC
 408         end)
 409 end
 410 
<span class="line-modified"> 411 macro uncage(basePtr, mask, ptr, scratch)</span>
<span class="line-modified"> 412     if GIGACAGE_ENABLED and not C_LOOP</span>
 413         loadp basePtr, scratch
 414         btpz scratch, .done
 415         andp mask, ptr
 416         addp scratch, ptr
 417     .done:
 418     end
 419 end
 420 
<span class="line-modified"> 421 macro loadCaged(basePtr, mask, source, dest, scratch)</span>



















 422     loadp source, dest
<span class="line-modified"> 423     uncage(basePtr, mask, dest, scratch)</span>
 424 end
 425 
 426 macro loadVariable(get, fieldName, valueReg)
 427     get(fieldName, valueReg)
 428     loadq [cfr, valueReg, 8], valueReg
 429 end
 430 
 431 # Index and value must be different registers. Index may be clobbered.
 432 macro loadConstantOrVariable(size, index, value)
 433     macro loadNarrow()
 434         bpgteq index, FirstConstantRegisterIndexNarrow, .constant
 435         loadq [cfr, index, 8], value
 436         jmp .done
 437     .constant:
 438         loadp CodeBlock[cfr], value
 439         loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
 440         loadq -(FirstConstantRegisterIndexNarrow * 8)[value, index, 8], value
 441     .done:
 442     end
 443 
<span class="line-modified"> 444     macro loadWide()</span>
<span class="line-modified"> 445         bpgteq index, FirstConstantRegisterIndexWide, .constant</span>
 446         loadq [cfr, index, 8], value
 447         jmp .done
 448     .constant:
 449         loadp CodeBlock[cfr], value
 450         loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
<span class="line-modified"> 451         subp FirstConstantRegisterIndexWide, index</span>











 452         loadq [value, index, 8], value
 453     .done:
 454     end
 455 
<span class="line-modified"> 456     size(loadNarrow, loadWide, macro (load) load() end)</span>
 457 end
 458 
 459 macro loadConstantOrVariableInt32(size, index, value, slow)
 460     loadConstantOrVariable(size, index, value)
 461     bqb value, tagTypeNumber, slow
 462 end
 463 
 464 macro loadConstantOrVariableCell(size, index, value, slow)
 465     loadConstantOrVariable(size, index, value)
 466     btqnz value, tagMask, slow
 467 end
 468 
<span class="line-modified"> 469 macro writeBarrierOnOperandWithReload(size, get, cellFieldName, reloadAfterSlowPath)</span>
<span class="line-removed"> 470     get(cellFieldName, t1)</span>
<span class="line-removed"> 471     loadConstantOrVariableCell(size, t1, t2, .writeBarrierDone)</span>
 472     skipIfIsRememberedOrInEden(
<span class="line-modified"> 473         t2,</span>
 474         macro()
 475             push PB, PC
<span class="line-modified"> 476             move t2, a1 # t2 can be a0 (not on 64 bits, but better safe than sorry)</span>
 477             move cfr, a0
 478             cCall2Void(_llint_write_barrier_slow)
 479             pop PC, PB
 480             reloadAfterSlowPath()
 481         end)






 482 .writeBarrierDone:
 483 end
 484 
 485 macro writeBarrierOnOperand(size, get, cellFieldName)
 486     writeBarrierOnOperandWithReload(size, get, cellFieldName, macro () end)
 487 end
 488 
 489 macro writeBarrierOnOperands(size, get, cellFieldName, valueFieldName)
 490     get(valueFieldName, t1)
 491     loadConstantOrVariableCell(size, t1, t0, .writeBarrierDone)
 492     btpz t0, .writeBarrierDone
 493 
 494     writeBarrierOnOperand(size, get, cellFieldName)
 495 .writeBarrierDone:
 496 end
 497 
 498 macro writeBarrierOnGlobal(size, get, valueFieldName, loadMacro)
 499     get(valueFieldName, t1)
 500     loadConstantOrVariableCell(size, t1, t0, .writeBarrierDone)
 501     btpz t0, .writeBarrierDone
 502 
 503     loadMacro(t3)
<span class="line-modified"> 504     skipIfIsRememberedOrInEden(</span>
<span class="line-removed"> 505         t3,</span>
<span class="line-removed"> 506         macro()</span>
<span class="line-removed"> 507             push PB, PC</span>
<span class="line-removed"> 508             move cfr, a0</span>
<span class="line-removed"> 509             move t3, a1</span>
<span class="line-removed"> 510             cCall2Void(_llint_write_barrier_slow)</span>
<span class="line-removed"> 511             pop PC, PB</span>
<span class="line-removed"> 512         end)</span>
 513 .writeBarrierDone:
 514 end
 515 
 516 macro writeBarrierOnGlobalObject(size, get, valueFieldName)
 517     writeBarrierOnGlobal(size, get, valueFieldName,
 518         macro(registerToStoreGlobal)
 519             loadp CodeBlock[cfr], registerToStoreGlobal
 520             loadp CodeBlock::m_globalObject[registerToStoreGlobal], registerToStoreGlobal
 521         end)
 522 end
 523 
 524 macro writeBarrierOnGlobalLexicalEnvironment(size, get, valueFieldName)
 525     writeBarrierOnGlobal(size, get, valueFieldName,
 526         macro(registerToStoreGlobal)
 527             loadp CodeBlock[cfr], registerToStoreGlobal
 528             loadp CodeBlock::m_globalObject[registerToStoreGlobal], registerToStoreGlobal
 529             loadp JSGlobalObject::m_globalLexicalEnvironment[registerToStoreGlobal], registerToStoreGlobal
 530         end)
 531 end
 532 
</pre>
<hr />
<pre>
 568 
 569 .noError:
 570     move r1, t1 # r1 contains slotsToAdd.
 571     btiz t1, .continue
 572     loadi PayloadOffset + ArgumentCount[cfr], t2
 573     addi CallFrameHeaderSlots, t2
 574 
 575     // Check if there are some unaligned slots we can use
 576     move t1, t3
 577     andi StackAlignmentSlots - 1, t3
 578     btiz t3, .noExtraSlot
 579     move ValueUndefined, t0
 580 .fillExtraSlots:
 581     storeq t0, [cfr, t2, 8]
 582     addi 1, t2
 583     bsubinz 1, t3, .fillExtraSlots
 584     andi ~(StackAlignmentSlots - 1), t1
 585     btiz t1, .continue
 586 
 587 .noExtraSlot:
<span class="line-modified"> 588     if POINTER_PROFILING</span>
<span class="line-modified"> 589         if ARM64 or ARM64E</span>
<span class="line-removed"> 590             loadp 8[cfr], lr</span>
<span class="line-removed"> 591         end</span>
<span class="line-removed"> 592 </span>
 593         addp 16, cfr, t3
 594         untagReturnAddress t3
 595     end
 596 
 597     // Move frame up t1 slots
 598     negq t1
 599     move cfr, t3
 600     subp CalleeSaveSpaceAsVirtualRegisters * 8, t3
 601     addi CalleeSaveSpaceAsVirtualRegisters, t2
 602     move t1, t0
 603     # Adds to sp are always 64-bit on arm64 so we need maintain t0&#39;s high bits.
 604     lshiftq 3, t0
 605     addp t0, cfr
 606     addp t0, sp
 607 .copyLoop:
 608     loadq [t3], t0
 609     storeq t0, [t3, t1, 8]
 610     addp 8, t3
 611     bsubinz 1, t2, .copyLoop
 612 
 613     // Fill new slots with JSUndefined
 614     move t1, t2
 615     move ValueUndefined, t0
 616 .fillLoop:
 617     storeq t0, [t3, t1, 8]
 618     addp 8, t3
 619     baddinz 1, t2, .fillLoop
 620 
<span class="line-modified"> 621     if POINTER_PROFILING</span>
 622         addp 16, cfr, t1
 623         tagReturnAddress t1
<span class="line-modified"> 624 </span>
<span class="line-removed"> 625         if ARM64 or ARM64E</span>
<span class="line-removed"> 626             storep lr, 8[cfr]</span>
<span class="line-removed"> 627         end</span>
 628     end
 629 
 630 .continue:
 631     # Reload CodeBlock and reset PC, since the slow_path clobbered them.
 632     loadp CodeBlock[cfr], t1
 633     loadp CodeBlock::m_instructionsRawPointer[t1], PB
 634     move 0, PC
 635     jmp doneLabel
 636 end
 637 
 638 macro branchIfException(label)
 639     loadp Callee[cfr], t3
 640     andp MarkedBlockMask, t3
 641     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
 642     btpz VM::m_exception[t3], .noException
 643     jmp label
 644 .noException:
 645 end
 646 
 647 # Instruction implementations
 648 _llint_op_enter:
 649     traceExecution()
 650     checkStackPointerAlignment(t2, 0xdead00e1)
<span class="line-modified"> 651     loadp CodeBlock[cfr], t2                // t2&lt;CodeBlock&gt; = cfr.CodeBlock</span>
<span class="line-modified"> 652     loadi CodeBlock::m_numVars[t2], t2      // t2&lt;size_t&gt; = t2&lt;CodeBlock&gt;.m_numVars</span>
 653     subq CalleeSaveSpaceAsVirtualRegisters, t2
 654     move cfr, t1
 655     subq CalleeSaveSpaceAsVirtualRegisters * 8, t1
 656     btiz t2, .opEnterDone
 657     move ValueUndefined, t0
 658     negi t2
 659     sxi2q t2, t2
 660 .opEnterLoop:
 661     storeq t0, [t1, t2, 8]
 662     addq 1, t2
 663     btqnz t2, .opEnterLoop
 664 .opEnterDone:
<span class="line-modified"> 665     callSlowPath(_slow_path_enter)</span>





 666     dispatchOp(narrow, op_enter)
<span class="line-modified"> 667 </span>


 668 
 669 llintOpWithProfile(op_get_argument, OpGetArgument, macro (size, get, dispatch, return)
 670     get(m_index, t2)
 671     loadi PayloadOffset + ArgumentCount[cfr], t0
 672     bilteq t0, t2, .opGetArgumentOutOfBounds
 673     loadq ThisArgumentOffset[cfr, t2, 8], t0
 674     return(t0)
 675 
 676 .opGetArgumentOutOfBounds:
 677     return(ValueUndefined)
 678 end)
 679 
 680 
 681 llintOpWithReturn(op_argument_count, OpArgumentCount, macro (size, get, dispatch, return)
 682     loadi PayloadOffset + ArgumentCount[cfr], t0
 683     subi 1, t0
 684     orq TagTypeNumber, t0
 685     return(t0)
 686 end)
 687 
 688 
 689 llintOpWithReturn(op_get_scope, OpGetScope, macro (size, get, dispatch, return)
 690     loadp Callee[cfr], t0
 691     loadp JSCallee::m_scope[t0], t0
 692     return(t0)
 693 end)
 694 
 695 
 696 llintOpWithMetadata(op_to_this, OpToThis, macro (size, get, dispatch, metadata, return)
 697     get(m_srcDst, t0)
 698     loadq [cfr, t0, 8], t0
 699     btqnz t0, tagMask, .opToThisSlow
 700     bbneq JSCell::m_type[t0], FinalObjectType, .opToThisSlow
<span class="line-modified"> 701     loadStructureWithScratch(t0, t1, t2, t3)</span>
 702     metadata(t2, t3)
<span class="line-modified"> 703     loadp OpToThis::Metadata::m_cachedStructure[t2], t2</span>
<span class="line-modified"> 704     bpneq t1, t2, .opToThisSlow</span>
 705     dispatch()
 706 
 707 .opToThisSlow:
 708     callSlowPath(_slow_path_to_this)
 709     dispatch()
 710 end)
 711 
 712 
 713 llintOp(op_check_tdz, OpCheckTdz, macro (size, get, dispatch)
 714     get(m_targetVirtualRegister, t0)
 715     loadConstantOrVariable(size, t0, t1)
 716     bqneq t1, ValueEmpty, .opNotTDZ
 717     callSlowPath(_slow_path_throw_tdz_error)
 718 
 719 .opNotTDZ:
 720     dispatch()
 721 end)
 722 
 723 
 724 llintOpWithReturn(op_mov, OpMov, macro (size, get, dispatch, return)
</pre>
<hr />
<pre>
 774         cpeq Structure::m_globalObject[t2], t0, t0
 775         jmp .done
 776     .immediate:
 777         andq ~TagBitUndefined, t0
 778         cqeq t0, ValueNull, t0
 779     .done:
 780         fn(t0)
 781         return(t0)
 782     end)
 783 end
 784 
 785 equalNullComparisonOp(op_eq_null, OpEqNull,
 786     macro (value) orq ValueFalse, value end)
 787 
 788 
 789 equalNullComparisonOp(op_neq_null, OpNeqNull,
 790     macro (value) xorq ValueTrue, value end)
 791 
 792 
 793 llintOpWithReturn(op_is_undefined_or_null, OpIsUndefinedOrNull, macro (size, get, dispatch, return)
<span class="line-modified"> 794     get(m_operand, t0)</span>
<span class="line-modified"> 795     loadq [cfr, t0, 8], t0</span>
 796     andq ~TagBitUndefined, t0
 797     cqeq t0, ValueNull, t0
 798     orq ValueFalse, t0
 799     return(t0)
 800 end)
 801 
 802 
 803 macro strictEqOp(opcodeName, opcodeStruct, equalityOperation)
 804     llintOpWithReturn(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, return)
 805         get(m_rhs, t0)
 806         get(m_lhs, t2)
 807         loadConstantOrVariable(size, t0, t1)
 808         loadConstantOrVariable(size, t2, t0)
 809         move t0, t2
 810         orq t1, t2
 811         btqz t2, tagMask, .slow
 812         bqaeq t0, tagTypeNumber, .leftOK
 813         btqnz t0, tagTypeNumber, .slow
 814     .leftOK:
 815         bqaeq t1, tagTypeNumber, .rightOK
</pre>
<hr />
<pre>
1093         bqb t0, tagTypeNumber, .slow
1094         bqb t1, tagTypeNumber, .slow
1095         operation(t1, t0)
1096         orq tagTypeNumber, t0
1097         return(t0)
1098 
1099     .slow:
1100         callSlowPath(_slow_path_%opcodeName%)
1101         dispatch()
1102     end)
1103 end
1104 
1105 macro bitOp(opcodeName, opcodeStruct, operation)
1106     commonBitOp(llintOpWithReturn, opcodeName, opcodeStruct, operation)
1107 end
1108 
1109 macro bitOpProfiled(opcodeName, opcodeStruct, operation)
1110     commonBitOp(llintOpWithProfile, opcodeName, opcodeStruct, operation)
1111 end
1112 
<span class="line-modified">1113 bitOp(lshift, OpLshift,</span>
1114     macro (left, right) lshifti left, right end)
1115 
1116 
1117 bitOp(rshift, OpRshift,
1118     macro (left, right) rshifti left, right end)
1119 
1120 
1121 bitOp(urshift, OpUrshift,
1122     macro (left, right) urshifti left, right end)
1123 
1124 bitOpProfiled(bitand, OpBitand,
1125     macro (left, right) andi left, right end)
1126 
1127 bitOpProfiled(bitor, OpBitor,
1128     macro (left, right) ori left, right end)
1129 
1130 bitOpProfiled(bitxor, OpBitxor,
1131     macro (left, right) xori left, right end)
1132 
1133 llintOpWithProfile(op_bitnot, OpBitnot, macro (size, get, dispatch, return)
</pre>
<hr />
<pre>
1268 llintOpWithMetadata(op_get_by_id_direct, OpGetByIdDirect, macro (size, get, dispatch, metadata, return)
1269     metadata(t2, t0)
1270     get(m_base, t0)
1271     loadConstantOrVariableCell(size, t0, t3, .opGetByIdDirectSlow)
1272     loadi JSCell::m_structureID[t3], t1
1273     loadi OpGetByIdDirect::Metadata::m_structureID[t2], t0
1274     bineq t0, t1, .opGetByIdDirectSlow
1275     loadi OpGetByIdDirect::Metadata::m_offset[t2], t1
1276     loadPropertyAtVariableOffset(t1, t3, t0)
1277     valueProfile(OpGetByIdDirect, t2, t0)
1278     return(t0)
1279 
1280 .opGetByIdDirectSlow:
1281     callSlowPath(_llint_slow_path_get_by_id_direct)
1282     dispatch()
1283 end)
1284 
1285 
1286 llintOpWithMetadata(op_get_by_id, OpGetById, macro (size, get, dispatch, metadata, return)
1287     metadata(t2, t1)
<span class="line-modified">1288     loadb OpGetById::Metadata::m_mode[t2], t1</span>
1289     get(m_base, t0)
1290     loadConstantOrVariableCell(size, t0, t3, .opGetByIdSlow)
1291 
1292 .opGetByIdDefault:
1293     bbneq t1, constexpr GetByIdMode::Default, .opGetByIdProtoLoad
1294     loadi JSCell::m_structureID[t3], t1
1295     loadi OpGetById::Metadata::m_modeMetadata.defaultMode.structureID[t2], t0
1296     bineq t0, t1, .opGetByIdSlow
1297     loadis OpGetById::Metadata::m_modeMetadata.defaultMode.cachedOffset[t2], t1
1298     loadPropertyAtVariableOffset(t1, t3, t0)
1299     valueProfile(OpGetById, t2, t0)
1300     return(t0)
1301 
1302 .opGetByIdProtoLoad:
1303     bbneq t1, constexpr GetByIdMode::ProtoLoad, .opGetByIdArrayLength
1304     loadi JSCell::m_structureID[t3], t1
1305     loadi OpGetById::Metadata::m_modeMetadata.protoLoadMode.structureID[t2], t3
1306     bineq t3, t1, .opGetByIdSlow
1307     loadis OpGetById::Metadata::m_modeMetadata.protoLoadMode.cachedOffset[t2], t1
1308     loadp OpGetById::Metadata::m_modeMetadata.protoLoadMode.cachedSlot[t2], t3
1309     loadPropertyAtVariableOffset(t1, t3, t0)
1310     valueProfile(OpGetById, t2, t0)
1311     return(t0)
1312 
1313 .opGetByIdArrayLength:
1314     bbneq t1, constexpr GetByIdMode::ArrayLength, .opGetByIdUnset
1315     move t3, t0
1316     arrayProfile(OpGetById::Metadata::m_modeMetadata.arrayLengthMode.arrayProfile, t0, t2, t5)
1317     btiz t0, IsArray, .opGetByIdSlow
1318     btiz t0, IndexingShapeMask, .opGetByIdSlow
<span class="line-modified">1319     loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::jsValue, constexpr Gigacage::jsValueGigacageMask, JSObject::m_butterfly[t3], t0, t1)</span>
1320     loadi -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0], t0
1321     bilt t0, 0, .opGetByIdSlow
1322     orq tagTypeNumber, t0
1323     valueProfile(OpGetById, t2, t0)
1324     return(t0)
1325 
1326 .opGetByIdUnset:
1327     loadi JSCell::m_structureID[t3], t1
1328     loadi OpGetById::Metadata::m_modeMetadata.unsetMode.structureID[t2], t0
1329     bineq t0, t1, .opGetByIdSlow
1330     valueProfile(OpGetById, t2, ValueUndefined)
1331     return(ValueUndefined)
1332 
1333 .opGetByIdSlow:
1334     callSlowPath(_llint_slow_path_get_by_id)
1335     dispatch()
1336 end)
1337 
1338 
1339 llintOpWithMetadata(op_put_by_id, OpPutById, macro (size, get, dispatch, metadata, return)
</pre>
<hr />
<pre>
1422     end
1423 
1424     macro finishDoubleGetByVal(result, scratch1, scratch2)
1425         fd2q result, scratch1
1426         subq tagTypeNumber, scratch1
1427         finishGetByVal(scratch1, scratch2)
1428     end
1429 
1430     metadata(t5, t2)
1431 
1432     get(m_base, t2)
1433     loadConstantOrVariableCell(size, t2, t0, .opGetByValSlow)
1434 
1435     move t0, t2
1436     arrayProfile(OpGetByVal::Metadata::m_arrayProfile, t2, t5, t1)
1437 
1438     get(m_property, t3)
1439     loadConstantOrVariableInt32(size, t3, t1, .opGetByValSlow)
1440     sxi2q t1, t1
1441 
<span class="line-modified">1442     loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::jsValue, constexpr Gigacage::jsValueGigacageMask, JSObject::m_butterfly[t0], t3, tagTypeNumber)</span>
1443     move TagTypeNumber, tagTypeNumber
1444 
1445     andi IndexingShapeMask, t2
1446     bieq t2, Int32Shape, .opGetByValIsContiguous
1447     bineq t2, ContiguousShape, .opGetByValNotContiguous
1448 
1449 .opGetByValIsContiguous:
1450     biaeq t1, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t3], .opGetByValSlow
1451     get(m_dst, t0)
1452     loadq [t3, t1, 8], t2
1453     btqz t2, .opGetByValSlow
1454     jmp .opGetByValDone
1455 
1456 .opGetByValNotContiguous:
1457     bineq t2, DoubleShape, .opGetByValNotDouble
1458     biaeq t1, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t3], .opGetByValSlow
1459     get(m_dst, t0)
1460     loadd [t3, t1, 8], ft0
1461     bdnequn ft0, ft0, .opGetByValSlow
1462     fd2q ft0, t2
</pre>
<hr />
<pre>
1466 .opGetByValNotDouble:
1467     subi ArrayStorageShape, t2
1468     bia t2, SlowPutArrayStorageShape - ArrayStorageShape, .opGetByValNotIndexedStorage
1469     biaeq t1, -sizeof IndexingHeader + IndexingHeader::u.lengths.vectorLength[t3], .opGetByValSlow
1470     get(m_dst, t0)
1471     loadq ArrayStorage::m_vector[t3, t1, 8], t2
1472     btqz t2, .opGetByValSlow
1473 
1474 .opGetByValDone:
1475     storeq t2, [cfr, t0, 8]
1476     valueProfile(OpGetByVal, t5, t2)
1477     dispatch()
1478 
1479 .opGetByValNotIndexedStorage:
1480     # First lets check if we even have a typed array. This lets us do some boilerplate up front.
1481     loadb JSCell::m_type[t0], t2
1482     subi FirstTypedArrayType, t2
1483     biaeq t2, NumberOfTypedArrayTypesExcludingDataView, .opGetByValSlow
1484     
1485     # Sweet, now we know that we have a typed array. Do some basic things now.
<span class="line-modified">1486     biaeq t1, JSArrayBufferView::m_length[t0], .opGetByValSlow</span>












1487 
1488     # Now bisect through the various types:
1489     #    Int8ArrayType,
1490     #    Uint8ArrayType,
1491     #    Uint8ClampedArrayType,
1492     #    Int16ArrayType,
1493     #    Uint16ArrayType,
1494     #    Int32ArrayType,
1495     #    Uint32ArrayType,
1496     #    Float32ArrayType,
1497     #    Float64ArrayType,
1498 
1499     bia t2, Uint16ArrayType - FirstTypedArrayType, .opGetByValAboveUint16Array
1500 
1501     # We have one of Int8ArrayType .. Uint16ArrayType.
1502     bia t2, Uint8ClampedArrayType - FirstTypedArrayType, .opGetByValInt16ArrayOrUint16Array
1503 
1504     # We have one of Int8ArrayType ... Uint8ClampedArrayType
1505     bia t2, Int8ArrayType - FirstTypedArrayType, .opGetByValUint8ArrayOrUint8ClampedArray
1506 
1507     # We have Int8ArrayType.
<span class="line-modified">1508     loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)</span>
<span class="line-removed">1509     loadbs [t3, t1], t0</span>
1510     finishIntGetByVal(t0, t1)
1511 
1512 .opGetByValUint8ArrayOrUint8ClampedArray:
1513     bia t2, Uint8ArrayType - FirstTypedArrayType, .opGetByValUint8ClampedArray
1514 
1515     # We have Uint8ArrayType.
<span class="line-removed">1516     loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)</span>
1517     loadb [t3, t1], t0
1518     finishIntGetByVal(t0, t1)
1519 
1520 .opGetByValUint8ClampedArray:
1521     # We have Uint8ClampedArrayType.
<span class="line-removed">1522     loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)</span>
1523     loadb [t3, t1], t0
1524     finishIntGetByVal(t0, t1)
1525 
1526 .opGetByValInt16ArrayOrUint16Array:
1527     # We have either Int16ArrayType or Uint16ClampedArrayType.
1528     bia t2, Int16ArrayType - FirstTypedArrayType, .opGetByValUint16Array
1529 
1530     # We have Int16ArrayType.
<span class="line-modified">1531     loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)</span>
<span class="line-removed">1532     loadhs [t3, t1, 2], t0</span>
1533     finishIntGetByVal(t0, t1)
1534 
1535 .opGetByValUint16Array:
1536     # We have Uint16ArrayType.
<span class="line-removed">1537     loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)</span>
1538     loadh [t3, t1, 2], t0
1539     finishIntGetByVal(t0, t1)
1540 
1541 .opGetByValAboveUint16Array:
1542     # We have one of Int32ArrayType .. Float64ArrayType.
1543     bia t2, Uint32ArrayType - FirstTypedArrayType, .opGetByValFloat32ArrayOrFloat64Array
1544 
1545     # We have either Int32ArrayType or Uint32ArrayType
1546     bia t2, Int32ArrayType - FirstTypedArrayType, .opGetByValUint32Array
1547 
1548     # We have Int32ArrayType.
<span class="line-removed">1549     loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)</span>
1550     loadi [t3, t1, 4], t0
1551     finishIntGetByVal(t0, t1)
1552 
1553 .opGetByValUint32Array:
1554     # We have Uint32ArrayType.
<span class="line-removed">1555     loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)</span>
1556     # This is the hardest part because of large unsigned values.
1557     loadi [t3, t1, 4], t0
1558     bilt t0, 0, .opGetByValSlow # This case is still awkward to implement in LLInt.
1559     finishIntGetByVal(t0, t1)
1560 
1561 .opGetByValFloat32ArrayOrFloat64Array:
1562     # We have one of Float32ArrayType or Float64ArrayType. Sadly, we cannot handle Float32Array
1563     # inline yet. That would require some offlineasm changes.
1564     bieq t2, Float32ArrayType - FirstTypedArrayType, .opGetByValSlow
1565 
1566     # We have Float64ArrayType.
<span class="line-removed">1567     loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, JSArrayBufferView::m_vector[t0], t3, t2)</span>
1568     loadd [t3, t1, 8], ft0
1569     bdnequn ft0, ft0, .opGetByValSlow
1570     finishDoubleGetByVal(ft0, t0, t1)
1571 
1572 .opGetByValSlow:
1573     callSlowPath(_llint_slow_path_get_by_val)
1574     dispatch()
1575 end)
1576 
1577 
1578 macro putByValOp(opcodeName, opcodeStruct)
1579     llintOpWithMetadata(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, metadata, return)
1580         macro contiguousPutByVal(storeCallback)
1581             biaeq t3, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0], .outOfBounds
1582         .storeResult:
1583             get(m_value, t2)
1584             storeCallback(t2, t1, [t0, t3, 8])
1585             dispatch()
1586 
1587         .outOfBounds:
1588             biaeq t3, -sizeof IndexingHeader + IndexingHeader::u.lengths.vectorLength[t0], .opPutByValOutOfBounds
1589             storeb 1, %opcodeStruct%::Metadata::m_arrayProfile.m_mayStoreToHole[t5]
1590             addi 1, t3, t2
1591             storei t2, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0]
1592             jmp .storeResult
1593         end
1594 
1595         get(m_base, t0)
1596         loadConstantOrVariableCell(size, t0, t1, .opPutByValSlow)
1597         move t1, t2
1598         metadata(t5, t0)
1599         arrayProfile(%opcodeStruct%::Metadata::m_arrayProfile, t2, t5, t0)
1600         get(m_property, t0)
1601         loadConstantOrVariableInt32(size, t0, t3, .opPutByValSlow)
1602         sxi2q t3, t3
<span class="line-modified">1603         loadCaged(_g_gigacageBasePtrs + Gigacage::BasePtrs::jsValue, constexpr Gigacage::jsValueGigacageMask, JSObject::m_butterfly[t1], t0, tagTypeNumber)</span>
1604         move TagTypeNumber, tagTypeNumber
1605         btinz t2, CopyOnWrite, .opPutByValSlow
1606         andi IndexingShapeMask, t2
1607         bineq t2, Int32Shape, .opPutByValNotInt32
1608         contiguousPutByVal(
1609             macro (operand, scratch, address)
1610                 loadConstantOrVariable(size, operand, scratch)
1611                 bqb scratch, tagTypeNumber, .opPutByValSlow
1612                 storeq scratch, address
1613                 writeBarrierOnOperands(size, get, m_base, m_value)
1614             end)
1615 
1616     .opPutByValNotInt32:
1617         bineq t2, DoubleShape, .opPutByValNotDouble
1618         contiguousPutByVal(
1619             macro (operand, scratch, address)
1620                 loadConstantOrVariable(size, operand, scratch)
1621                 bqb scratch, tagTypeNumber, .notInt
1622                 ci2d scratch, ft0
1623                 jmp .ready
</pre>
<hr />
<pre>
1712 equalNullJumpOp(jeq_null, OpJeqNull,
1713     macro (structure, value, target) 
1714         btbz value, MasqueradesAsUndefined, .notMasqueradesAsUndefined
1715         loadp CodeBlock[cfr], t0
1716         loadp CodeBlock::m_globalObject[t0], t0
1717         bpeq Structure::m_globalObject[structure], t0, target
1718 .notMasqueradesAsUndefined:
1719     end,
1720     macro (value, target) bqeq value, ValueNull, target end)
1721 
1722 
1723 equalNullJumpOp(jneq_null, OpJneqNull,
1724     macro (structure, value, target) 
1725         btbz value, MasqueradesAsUndefined, target
1726         loadp CodeBlock[cfr], t0
1727         loadp CodeBlock::m_globalObject[t0], t0
1728         bpneq Structure::m_globalObject[structure], t0, target
1729     end,
1730     macro (value, target) bqneq value, ValueNull, target end)
1731 


















1732 
1733 llintOpWithMetadata(op_jneq_ptr, OpJneqPtr, macro (size, get, dispatch, metadata, return)
1734     get(m_value, t0)
1735     getu(size, OpJneqPtr, m_specialPointer, t1)
1736     loadp CodeBlock[cfr], t2
1737     loadp CodeBlock::m_globalObject[t2], t2
1738     loadp JSGlobalObject::m_specialPointers[t2, t1, PtrSize], t1
1739     bpneq t1, [cfr, t0, 8], .opJneqPtrTarget
1740     dispatch()
1741 
1742 .opJneqPtrTarget:
1743     metadata(t5, t0)
1744     storeb 1, OpJneqPtr::Metadata::m_hasJumped[t5]
1745     get(m_targetLabel, t0)
1746     jumpImpl(t0)
1747 end)
1748 
1749 
1750 macro compareJumpOp(opcodeName, opcodeStruct, integerCompare, doubleCompare)
1751     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
</pre>
<hr />
<pre>
1881     btpnz t0, isRopeInPointer, .opSwitchOnRope
1882     bineq StringImpl::m_length[t0], 1, .opSwitchCharFallThrough
1883     loadp StringImpl::m_data8[t0], t1
1884     btinz StringImpl::m_hashAndFlags[t0], HashFlags8BitBuffer, .opSwitchChar8Bit
1885     loadh [t1], t0
1886     jmp .opSwitchCharReady
1887 .opSwitchChar8Bit:
1888     loadb [t1], t0
1889 .opSwitchCharReady:
1890     subi SimpleJumpTable::min[t2], t0
1891     biaeq t0, SimpleJumpTable::branchOffsets + VectorSizeOffset[t2], .opSwitchCharFallThrough
1892     loadp SimpleJumpTable::branchOffsets + VectorBufferOffset[t2], t2
1893     loadis [t2, t0, 4], t1
1894     btiz t1, .opSwitchCharFallThrough
1895     dispatchIndirect(t1)
1896 
1897 .opSwitchCharFallThrough:
1898     jump(m_defaultOffset)
1899 
1900 .opSwitchOnRope:



1901     callSlowPath(_llint_slow_path_switch_char)
1902     nextInstruction()
1903 end)
1904 
1905 
1906 # we assume t5 contains the metadata, and we should not scratch that
1907 macro arrayProfileForCall(opcodeStruct, getu)
1908     getu(m_argv, t3)
1909     negp t3
1910     loadq ThisArgumentOffset[cfr, t3, 8], t0
1911     btqnz t0, tagMask, .done
1912     loadi JSCell::m_structureID[t0], t3
<span class="line-modified">1913     storei t3, %opcodeStruct%::Metadata::m_arrayProfile.m_lastSeenStructureID[t5]</span>
1914 .done:
1915 end
1916 
1917 macro commonCallOp(opcodeName, slowPath, opcodeStruct, prepareCall, prologue)
1918     llintOpWithMetadata(opcodeName, opcodeStruct, macro (size, get, dispatch, metadata, return)
1919         metadata(t5, t0)
1920 
1921         prologue(macro (fieldName, dst)
1922             getu(size, opcodeStruct, fieldName, dst)
1923         end, metadata)
1924 
1925         get(m_callee, t0)
<span class="line-modified">1926         loadp %opcodeStruct%::Metadata::m_callLinkInfo.callee[t5], t2</span>
1927         loadConstantOrVariable(size, t0, t3)
1928         bqneq t3, t2, .opCallSlow
1929         getu(size, opcodeStruct, m_argv, t3)
1930         lshifti 3, t3
1931         negp t3
1932         addp cfr, t3
1933         storeq t2, Callee[t3]
1934         getu(size, opcodeStruct, m_argc, t2)
1935         storei PC, ArgumentCount + TagOffset[cfr]
1936         storei t2, ArgumentCount + PayloadOffset[t3]
1937         move t3, sp
<span class="line-modified">1938         prepareCall(%opcodeStruct%::Metadata::m_callLinkInfo.machineCodeTarget[t5], t2, t3, t4, JSEntryPtrTag)</span>
<span class="line-modified">1939         callTargetFunction(size, opcodeStruct, dispatch, %opcodeStruct%::Metadata::m_callLinkInfo.machineCodeTarget[t5], JSEntryPtrTag)</span>
1940 
1941     .opCallSlow:
1942         slowPathForCall(size, opcodeStruct, dispatch, slowPath, prepareCall)
1943     end)
1944 end
1945 
1946 llintOp(op_ret, OpRet, macro (size, get, dispatch)
1947     checkSwitchToJITForEpilogue()
1948     get(m_value, t2)
1949     loadConstantOrVariable(size, t2, r0)
1950     doReturn()
1951 end)
1952 
1953 
1954 llintOpWithReturn(op_to_primitive, OpToPrimitive, macro (size, get, dispatch, return)
1955     get(m_src, t2)
1956     loadConstantOrVariable(size, t2, t0)
1957     btqnz t0, tagMask, .opToPrimitiveIsImm
1958     bbaeq JSCell::m_type[t0], ObjectType, .opToPrimitiveSlowCase
1959 .opToPrimitiveIsImm:
</pre>
<hr />
<pre>
2035     andp MarkedBlockMask, t1
2036     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
2037     jmp VM::targetMachinePCForThrow[t1], ExceptionHandlerPtrTag
2038 end)
2039 
2040 
2041 op(llint_throw_during_call_trampoline, macro ()
2042     preserveReturnAddressAfterCall(t2)
2043     jmp _llint_throw_from_slow_path_trampoline
2044 end)
2045 
2046 
2047 macro nativeCallTrampoline(executableOffsetToFunction)
2048 
2049     functionPrologue()
2050     storep 0, CodeBlock[cfr]
2051     loadp Callee[cfr], t0
2052     andp MarkedBlockMask, t0, t1
2053     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
2054     storep cfr, VM::topCallFrame[t1]
<span class="line-modified">2055     if ARM64 or ARM64E or C_LOOP</span>
2056         storep lr, ReturnPC[cfr]
2057     end
2058     move cfr, a0
2059     loadp Callee[cfr], t1
2060     loadp JSFunction::m_executable[t1], t1
2061     checkStackPointerAlignment(t3, 0xdead0001)
<span class="line-modified">2062     if C_LOOP</span>
2063         cloopCallNative executableOffsetToFunction[t1]
2064     else
2065         if X86_64_WIN
2066             subp 32, sp
2067             call executableOffsetToFunction[t1], JSEntryPtrTag
2068             addp 32, sp
2069         else
2070             call executableOffsetToFunction[t1], JSEntryPtrTag
2071         end
2072     end
2073 
2074     loadp Callee[cfr], t3
2075     andp MarkedBlockMask, t3
2076     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
2077 
2078     btpnz VM::m_exception[t3], .handleException
2079 
2080     functionEpilogue()
2081     ret
2082 
2083 .handleException:
2084     storep cfr, VM::topCallFrame[t3]
2085     jmp _llint_throw_from_slow_path_trampoline
2086 end
2087 
2088 macro internalFunctionCallTrampoline(offsetOfFunction)
2089     functionPrologue()
2090     storep 0, CodeBlock[cfr]
2091     loadp Callee[cfr], t0
2092     andp MarkedBlockMask, t0, t1
2093     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
2094     storep cfr, VM::topCallFrame[t1]
<span class="line-modified">2095     if ARM64 or ARM64E or C_LOOP</span>
2096         storep lr, ReturnPC[cfr]
2097     end
2098     move cfr, a0
2099     loadp Callee[cfr], t1
2100     checkStackPointerAlignment(t3, 0xdead0001)
<span class="line-modified">2101     if C_LOOP</span>
2102         cloopCallNative offsetOfFunction[t1]
2103     else
2104         if X86_64_WIN
2105             subp 32, sp
2106             call offsetOfFunction[t1], JSEntryPtrTag
2107             addp 32, sp
2108         else
2109             call offsetOfFunction[t1], JSEntryPtrTag
2110         end
2111     end
2112 
2113     loadp Callee[cfr], t3
2114     andp MarkedBlockMask, t3
2115     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
2116 
2117     btpnz VM::m_exception[t3], .handleException
2118 
2119     functionEpilogue()
2120     ret
2121 
</pre>
</td>
<td>
<hr />
<pre>
  13 # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
  14 # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  15 # PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
  16 # BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  17 # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  18 # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  19 # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  20 # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  21 # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
  22 # THE POSSIBILITY OF SUCH DAMAGE.
  23 
  24 
  25 # Utilities.
  26 
  27 macro nextInstruction()
  28     loadb [PB, PC, 1], t0
  29     leap _g_opcodeMap, t1
  30     jmp [t1, t0, PtrSize], BytecodePtrTag
  31 end
  32 
<span class="line-modified">  33 macro nextInstructionWide16()</span>
<span class="line-added">  34     loadh 1[PB, PC, 1], t0</span>
<span class="line-added">  35     leap _g_opcodeMapWide16, t1</span>
<span class="line-added">  36     jmp [t1, t0, PtrSize], BytecodePtrTag</span>
<span class="line-added">  37 end</span>
<span class="line-added">  38 </span>
<span class="line-added">  39 macro nextInstructionWide32()</span>
  40     loadi 1[PB, PC, 1], t0
<span class="line-modified">  41     leap _g_opcodeMapWide32, t1</span>
  42     jmp [t1, t0, PtrSize], BytecodePtrTag
  43 end
  44 
  45 macro getuOperandNarrow(opcodeStruct, fieldName, dst)
  46     loadb constexpr %opcodeStruct%_%fieldName%_index[PB, PC, 1], dst
  47 end
  48 
  49 macro getOperandNarrow(opcodeStruct, fieldName, dst)
<span class="line-modified">  50     loadbsq constexpr %opcodeStruct%_%fieldName%_index[PB, PC, 1], dst</span>
<span class="line-added">  51 end</span>
<span class="line-added">  52 </span>
<span class="line-added">  53 macro getuOperandWide16(opcodeStruct, fieldName, dst)</span>
<span class="line-added">  54     loadh constexpr %opcodeStruct%_%fieldName%_index * 2 + 1[PB, PC, 1], dst</span>
  55 end
  56 
<span class="line-modified">  57 macro getOperandWide16(opcodeStruct, fieldName, dst)</span>
<span class="line-added">  58     loadhsq constexpr %opcodeStruct%_%fieldName%_index * 2 + 1[PB, PC, 1], dst</span>
<span class="line-added">  59 end</span>
<span class="line-added">  60 </span>
<span class="line-added">  61 macro getuOperandWide32(opcodeStruct, fieldName, dst)</span>
  62     loadi constexpr %opcodeStruct%_%fieldName%_index * 4 + 1[PB, PC, 1], dst
  63 end
  64 
<span class="line-modified">  65 macro getOperandWide32(opcodeStruct, fieldName, dst)</span>
  66     loadis constexpr %opcodeStruct%_%fieldName%_index * 4 + 1[PB, PC, 1], dst
  67 end
  68 
  69 macro makeReturn(get, dispatch, fn)
  70     fn(macro (value)
  71         move value, t2
  72         get(m_dst, t1)
  73         storeq t2, [cfr, t1, 8]
  74         dispatch()
  75     end)
  76 end
  77 
  78 macro makeReturnProfiled(opcodeStruct, get, metadata, dispatch, fn)
  79     fn(macro (value)
  80         move value, t3
  81         metadata(t1, t2)
  82         valueProfile(opcodeStruct, t1, t3)
  83         get(m_dst, t1)
  84         storeq t3, [cfr, t1, 8]
  85         dispatch()
</pre>
<hr />
<pre>
 106     if X86_64 or ARM64 or ARM64E
 107         call function
 108     elsif X86_64_WIN
 109         # Note: this implementation is only correct if the return type size is &gt; 8 bytes.
 110         # See macro cCall2Void for an implementation when the return type &lt;= 8 bytes.
 111         # On Win64, when the return type is larger than 8 bytes, we need to allocate space on the stack for the return value.
 112         # On entry rcx (a0), should contain a pointer to this stack space. The other parameters are shifted to the right,
 113         # rdx (a1) should contain the first argument, and r8 (a2) should contain the second argument.
 114         # On return, rax contains a pointer to this stack value, and we then need to copy the 16 byte return value into rax (r0) and rdx (r1)
 115         # since the return value is expected to be split between the two.
 116         # See http://msdn.microsoft.com/en-us/library/7572ztz4.aspx
 117         move a1, a2
 118         move a0, a1
 119         subp 48, sp
 120         move sp, a0
 121         addp 32, a0
 122         call function
 123         addp 48, sp
 124         move 8[r0], r1
 125         move [r0], r0
<span class="line-modified"> 126     elsif C_LOOP or C_LOOP_WIN</span>
 127         cloopCallSlowPath function, a0, a1
 128     else
 129         error
 130     end
 131 end
 132 
 133 macro cCall2Void(function)
<span class="line-modified"> 134     if C_LOOP or C_LOOP_WIN</span>
 135         cloopCallSlowPathVoid function, a0, a1
 136     elsif X86_64_WIN
 137         # Note: we cannot use the cCall2 macro for Win64 in this case,
 138         # as the Win64 cCall2 implemenation is only correct when the return type size is &gt; 8 bytes.
 139         # On Win64, rcx and rdx are used for passing the first two parameters.
 140         # We also need to make room on the stack for all four parameter registers.
 141         # See http://msdn.microsoft.com/en-us/library/ms235286.aspx
 142         subp 32, sp 
 143         call function
 144         addp 32, sp
 145     else
 146         cCall2(function)
 147     end
 148 end
 149 
 150 # This barely works. arg3 and arg4 should probably be immediates.
 151 macro cCall4(function)
 152     checkStackPointerAlignment(t4, 0xbad0c004)
 153     if X86_64 or ARM64 or ARM64E
 154         call function
</pre>
<hr />
<pre>
 176 
 177     checkStackPointerAlignment(t4, 0xbad0dc01)
 178 
 179     storep vm, VMEntryRecord::m_vm[sp]
 180     loadp VM::topCallFrame[vm], t4
 181     storep t4, VMEntryRecord::m_prevTopCallFrame[sp]
 182     loadp VM::topEntryFrame[vm], t4
 183     storep t4, VMEntryRecord::m_prevTopEntryFrame[sp]
 184     loadp ProtoCallFrame::calleeValue[protoCallFrame], t4
 185     storep t4, VMEntryRecord::m_callee[sp]
 186 
 187     loadi ProtoCallFrame::paddedArgCount[protoCallFrame], t4
 188     addp CallFrameHeaderSlots, t4, t4
 189     lshiftp 3, t4
 190     subp sp, t4, t3
 191     bqbeq sp, t3, .throwStackOverflow
 192 
 193     # Ensure that we have enough additional stack capacity for the incoming args,
 194     # and the frame for the JS code we&#39;re executing. We need to do this check
 195     # before we start copying the args from the protoCallFrame below.
<span class="line-modified"> 196     if C_LOOP or C_LOOP_WIN</span>
 197         bpaeq t3, VM::m_cloopStackLimit[vm], .stackHeightOK
 198         move entry, t4
 199         move vm, t5
 200         cloopCallSlowPath _llint_stack_check_at_vm_entry, vm, t3
 201         bpeq t0, 0, .stackCheckFailed
 202         move t4, entry
 203         move t5, vm
 204         jmp .stackHeightOK
 205 
 206 .stackCheckFailed:
 207         move t4, entry
 208         move t5, vm
 209         jmp .throwStackOverflow
 210     else
 211         bpb t3, VM::m_softStackLimit[vm], .throwStackOverflow
 212     end
 213 
 214 .stackHeightOK:
 215     move t3, sp
 216     move (constexpr ProtoCallFrame::numberOfRegisters), t3
</pre>
<hr />
<pre>
 282     cCall2(_llint_throw_stack_overflow_error)
 283 
 284     vmEntryRecord(cfr, t4)
 285 
 286     loadp VMEntryRecord::m_vm[t4], vm
 287     loadp VMEntryRecord::m_prevTopCallFrame[t4], extraTempReg
 288     storep extraTempReg, VM::topCallFrame[vm]
 289     loadp VMEntryRecord::m_prevTopEntryFrame[t4], extraTempReg
 290     storep extraTempReg, VM::topEntryFrame[vm]
 291 
 292     subp cfr, CalleeRegisterSaveSize, sp
 293 
 294     popCalleeSaves()
 295     functionEpilogue()
 296     ret
 297 end
 298 
 299 
 300 macro makeJavaScriptCall(entry, temp, unused)
 301     addp 16, sp
<span class="line-modified"> 302     if C_LOOP or C_LOOP_WIN</span>
 303         cloopCallJSFunction entry
 304     else
 305         call entry, JSEntryPtrTag
 306     end
 307     subp 16, sp
 308 end
 309 
 310 macro makeHostFunctionCall(entry, temp, unused)
 311     move entry, temp
 312     storep cfr, [sp]
 313     move sp, a0
<span class="line-modified"> 314     if C_LOOP or C_LOOP_WIN</span>
 315         storep lr, 8[sp]
 316         cloopCallNative temp
 317     elsif X86_64_WIN
 318         # We need to allocate 32 bytes on the stack for the shadow space.
 319         subp 32, sp
 320         call temp, JSEntryPtrTag
 321         addp 32, sp
 322     else
 323         call temp, JSEntryPtrTag
 324     end
 325 end
 326 
 327 op(handleUncaughtException, macro ()
 328     loadp Callee[cfr], t3
 329     andp MarkedBlockMask, t3
 330     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
 331     restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(t3, t0)
 332     storep 0, VM::callFrameForCatch[t3]
 333 
 334     loadp VM::topEntryFrame[t3], cfr
</pre>
<hr />
<pre>
 405     loadi ArgumentCount + TagOffset[cfr], PC
 406 end
 407 
 408 macro checkSwitchToJITForLoop()
 409     checkSwitchToJIT(
 410         1,
 411         macro()
 412             storei PC, ArgumentCount + TagOffset[cfr]
 413             prepareStateForCCall()
 414             move cfr, a0
 415             move PC, a1
 416             cCall2(_llint_loop_osr)
 417             btpz r0, .recover
 418             move r1, sp
 419             jmp r0, JSEntryPtrTag
 420         .recover:
 421             loadi ArgumentCount + TagOffset[cfr], PC
 422         end)
 423 end
 424 
<span class="line-modified"> 425 macro cage(basePtr, mask, ptr, scratch)</span>
<span class="line-modified"> 426     if GIGACAGE_ENABLED and not (C_LOOP or C_LOOP_WIN)</span>
 427         loadp basePtr, scratch
 428         btpz scratch, .done
 429         andp mask, ptr
 430         addp scratch, ptr
 431     .done:
 432     end
 433 end
 434 
<span class="line-modified"> 435 macro cagedPrimitive(ptr, length, scratch, scratch2)</span>
<span class="line-added"> 436     if ARM64E</span>
<span class="line-added"> 437         const source = scratch2</span>
<span class="line-added"> 438         move ptr, scratch2</span>
<span class="line-added"> 439     else</span>
<span class="line-added"> 440         const source = ptr</span>
<span class="line-added"> 441     end</span>
<span class="line-added"> 442     if GIGACAGE_ENABLED</span>
<span class="line-added"> 443         cage(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, source, scratch)</span>
<span class="line-added"> 444         if ARM64E</span>
<span class="line-added"> 445             const numberOfPACBits = constexpr MacroAssembler::numberOfPACBits</span>
<span class="line-added"> 446             bfiq scratch2, 0, 64 - numberOfPACBits, ptr</span>
<span class="line-added"> 447         end</span>
<span class="line-added"> 448     end</span>
<span class="line-added"> 449     if ARM64E</span>
<span class="line-added"> 450         untagArrayPtr length, ptr</span>
<span class="line-added"> 451     end</span>
<span class="line-added"> 452 end</span>
<span class="line-added"> 453 </span>
<span class="line-added"> 454 macro loadCagedJSValue(source, dest, scratchOrLength)</span>
 455     loadp source, dest
<span class="line-modified"> 456     cage(_g_gigacageBasePtrs + Gigacage::BasePtrs::jsValue, constexpr Gigacage::jsValueGigacageMask, dest, scratchOrLength)</span>
 457 end
 458 
 459 macro loadVariable(get, fieldName, valueReg)
 460     get(fieldName, valueReg)
 461     loadq [cfr, valueReg, 8], valueReg
 462 end
 463 
 464 # Index and value must be different registers. Index may be clobbered.
 465 macro loadConstantOrVariable(size, index, value)
 466     macro loadNarrow()
 467         bpgteq index, FirstConstantRegisterIndexNarrow, .constant
 468         loadq [cfr, index, 8], value
 469         jmp .done
 470     .constant:
 471         loadp CodeBlock[cfr], value
 472         loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
 473         loadq -(FirstConstantRegisterIndexNarrow * 8)[value, index, 8], value
 474     .done:
 475     end
 476 
<span class="line-modified"> 477     macro loadWide16()</span>
<span class="line-modified"> 478         bpgteq index, FirstConstantRegisterIndexWide16, .constant</span>
 479         loadq [cfr, index, 8], value
 480         jmp .done
 481     .constant:
 482         loadp CodeBlock[cfr], value
 483         loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
<span class="line-modified"> 484         loadq -(FirstConstantRegisterIndexWide16 * 8)[value, index, 8], value</span>
<span class="line-added"> 485     .done:</span>
<span class="line-added"> 486     end</span>
<span class="line-added"> 487 </span>
<span class="line-added"> 488     macro loadWide32()</span>
<span class="line-added"> 489         bpgteq index, FirstConstantRegisterIndexWide32, .constant</span>
<span class="line-added"> 490         loadq [cfr, index, 8], value</span>
<span class="line-added"> 491         jmp .done</span>
<span class="line-added"> 492     .constant:</span>
<span class="line-added"> 493         loadp CodeBlock[cfr], value</span>
<span class="line-added"> 494         loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value</span>
<span class="line-added"> 495         subp FirstConstantRegisterIndexWide32, index</span>
 496         loadq [value, index, 8], value
 497     .done:
 498     end
 499 
<span class="line-modified"> 500     size(loadNarrow, loadWide16, loadWide32, macro (load) load() end)</span>
 501 end
 502 
 503 macro loadConstantOrVariableInt32(size, index, value, slow)
 504     loadConstantOrVariable(size, index, value)
 505     bqb value, tagTypeNumber, slow
 506 end
 507 
 508 macro loadConstantOrVariableCell(size, index, value, slow)
 509     loadConstantOrVariable(size, index, value)
 510     btqnz value, tagMask, slow
 511 end
 512 
<span class="line-modified"> 513 macro writeBarrierOnCellWithReload(cell, reloadAfterSlowPath)</span>


 514     skipIfIsRememberedOrInEden(
<span class="line-modified"> 515         cell,</span>
 516         macro()
 517             push PB, PC
<span class="line-modified"> 518             move cell, a1 # cell can be a0</span>
 519             move cfr, a0
 520             cCall2Void(_llint_write_barrier_slow)
 521             pop PC, PB
 522             reloadAfterSlowPath()
 523         end)
<span class="line-added"> 524 end</span>
<span class="line-added"> 525 </span>
<span class="line-added"> 526 macro writeBarrierOnOperandWithReload(size, get, cellFieldName, reloadAfterSlowPath)</span>
<span class="line-added"> 527     get(cellFieldName, t1)</span>
<span class="line-added"> 528     loadConstantOrVariableCell(size, t1, t2, .writeBarrierDone)</span>
<span class="line-added"> 529     writeBarrierOnCellWithReload(t2, reloadAfterSlowPath)</span>
 530 .writeBarrierDone:
 531 end
 532 
 533 macro writeBarrierOnOperand(size, get, cellFieldName)
 534     writeBarrierOnOperandWithReload(size, get, cellFieldName, macro () end)
 535 end
 536 
 537 macro writeBarrierOnOperands(size, get, cellFieldName, valueFieldName)
 538     get(valueFieldName, t1)
 539     loadConstantOrVariableCell(size, t1, t0, .writeBarrierDone)
 540     btpz t0, .writeBarrierDone
 541 
 542     writeBarrierOnOperand(size, get, cellFieldName)
 543 .writeBarrierDone:
 544 end
 545 
 546 macro writeBarrierOnGlobal(size, get, valueFieldName, loadMacro)
 547     get(valueFieldName, t1)
 548     loadConstantOrVariableCell(size, t1, t0, .writeBarrierDone)
 549     btpz t0, .writeBarrierDone
 550 
 551     loadMacro(t3)
<span class="line-modified"> 552     writeBarrierOnCellWithReload(t3, macro() end)</span>








 553 .writeBarrierDone:
 554 end
 555 
 556 macro writeBarrierOnGlobalObject(size, get, valueFieldName)
 557     writeBarrierOnGlobal(size, get, valueFieldName,
 558         macro(registerToStoreGlobal)
 559             loadp CodeBlock[cfr], registerToStoreGlobal
 560             loadp CodeBlock::m_globalObject[registerToStoreGlobal], registerToStoreGlobal
 561         end)
 562 end
 563 
 564 macro writeBarrierOnGlobalLexicalEnvironment(size, get, valueFieldName)
 565     writeBarrierOnGlobal(size, get, valueFieldName,
 566         macro(registerToStoreGlobal)
 567             loadp CodeBlock[cfr], registerToStoreGlobal
 568             loadp CodeBlock::m_globalObject[registerToStoreGlobal], registerToStoreGlobal
 569             loadp JSGlobalObject::m_globalLexicalEnvironment[registerToStoreGlobal], registerToStoreGlobal
 570         end)
 571 end
 572 
</pre>
<hr />
<pre>
 608 
 609 .noError:
 610     move r1, t1 # r1 contains slotsToAdd.
 611     btiz t1, .continue
 612     loadi PayloadOffset + ArgumentCount[cfr], t2
 613     addi CallFrameHeaderSlots, t2
 614 
 615     // Check if there are some unaligned slots we can use
 616     move t1, t3
 617     andi StackAlignmentSlots - 1, t3
 618     btiz t3, .noExtraSlot
 619     move ValueUndefined, t0
 620 .fillExtraSlots:
 621     storeq t0, [cfr, t2, 8]
 622     addi 1, t2
 623     bsubinz 1, t3, .fillExtraSlots
 624     andi ~(StackAlignmentSlots - 1), t1
 625     btiz t1, .continue
 626 
 627 .noExtraSlot:
<span class="line-modified"> 628     if ARM64E</span>
<span class="line-modified"> 629         loadp 8[cfr], lr</span>



 630         addp 16, cfr, t3
 631         untagReturnAddress t3
 632     end
 633 
 634     // Move frame up t1 slots
 635     negq t1
 636     move cfr, t3
 637     subp CalleeSaveSpaceAsVirtualRegisters * 8, t3
 638     addi CalleeSaveSpaceAsVirtualRegisters, t2
 639     move t1, t0
 640     # Adds to sp are always 64-bit on arm64 so we need maintain t0&#39;s high bits.
 641     lshiftq 3, t0
 642     addp t0, cfr
 643     addp t0, sp
 644 .copyLoop:
 645     loadq [t3], t0
 646     storeq t0, [t3, t1, 8]
 647     addp 8, t3
 648     bsubinz 1, t2, .copyLoop
 649 
 650     // Fill new slots with JSUndefined
 651     move t1, t2
 652     move ValueUndefined, t0
 653 .fillLoop:
 654     storeq t0, [t3, t1, 8]
 655     addp 8, t3
 656     baddinz 1, t2, .fillLoop
 657 
<span class="line-modified"> 658     if ARM64E</span>
 659         addp 16, cfr, t1
 660         tagReturnAddress t1
<span class="line-modified"> 661         storep lr, 8[cfr]</span>



 662     end
 663 
 664 .continue:
 665     # Reload CodeBlock and reset PC, since the slow_path clobbered them.
 666     loadp CodeBlock[cfr], t1
 667     loadp CodeBlock::m_instructionsRawPointer[t1], PB
 668     move 0, PC
 669     jmp doneLabel
 670 end
 671 
 672 macro branchIfException(label)
 673     loadp Callee[cfr], t3
 674     andp MarkedBlockMask, t3
 675     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
 676     btpz VM::m_exception[t3], .noException
 677     jmp label
 678 .noException:
 679 end
 680 
 681 # Instruction implementations
 682 _llint_op_enter:
 683     traceExecution()
 684     checkStackPointerAlignment(t2, 0xdead00e1)
<span class="line-modified"> 685     loadp CodeBlock[cfr], t3                // t3&lt;CodeBlock&gt; = cfr.CodeBlock</span>
<span class="line-modified"> 686     loadi CodeBlock::m_numVars[t3], t2      // t2&lt;size_t&gt; = t3&lt;CodeBlock&gt;.m_numVars</span>
 687     subq CalleeSaveSpaceAsVirtualRegisters, t2
 688     move cfr, t1
 689     subq CalleeSaveSpaceAsVirtualRegisters * 8, t1
 690     btiz t2, .opEnterDone
 691     move ValueUndefined, t0
 692     negi t2
 693     sxi2q t2, t2
 694 .opEnterLoop:
 695     storeq t0, [t1, t2, 8]
 696     addq 1, t2
 697     btqnz t2, .opEnterLoop
 698 .opEnterDone:
<span class="line-modified"> 699     writeBarrierOnCellWithReload(t3, macro ()</span>
<span class="line-added"> 700         loadp CodeBlock[cfr], t3 # Reload CodeBlock</span>
<span class="line-added"> 701     end)</span>
<span class="line-added"> 702     loadp CodeBlock::m_vm[t3], t1</span>
<span class="line-added"> 703     btbnz VM::m_traps + VMTraps::m_needTrapHandling[t1], .handleTraps</span>
<span class="line-added"> 704 .afterHandlingTraps:</span>
 705     dispatchOp(narrow, op_enter)
<span class="line-modified"> 706 .handleTraps:</span>
<span class="line-added"> 707     callTrapHandler(_llint_throw_from_slow_path_trampoline)</span>
<span class="line-added"> 708     jmp .afterHandlingTraps</span>
 709 
 710 llintOpWithProfile(op_get_argument, OpGetArgument, macro (size, get, dispatch, return)
 711     get(m_index, t2)
 712     loadi PayloadOffset + ArgumentCount[cfr], t0
 713     bilteq t0, t2, .opGetArgumentOutOfBounds
 714     loadq ThisArgumentOffset[cfr, t2, 8], t0
 715     return(t0)
 716 
 717 .opGetArgumentOutOfBounds:
 718     return(ValueUndefined)
 719 end)
 720 
 721 
 722 llintOpWithReturn(op_argument_count, OpArgumentCount, macro (size, get, dispatch, return)
 723     loadi PayloadOffset + ArgumentCount[cfr], t0
 724     subi 1, t0
 725     orq TagTypeNumber, t0
 726     return(t0)
 727 end)
 728 
 729 
 730 llintOpWithReturn(op_get_scope, OpGetScope, macro (size, get, dispatch, return)
 731     loadp Callee[cfr], t0
 732     loadp JSCallee::m_scope[t0], t0
 733     return(t0)
 734 end)
 735 
 736 
 737 llintOpWithMetadata(op_to_this, OpToThis, macro (size, get, dispatch, metadata, return)
 738     get(m_srcDst, t0)
 739     loadq [cfr, t0, 8], t0
 740     btqnz t0, tagMask, .opToThisSlow
 741     bbneq JSCell::m_type[t0], FinalObjectType, .opToThisSlow
<span class="line-modified"> 742     loadi JSCell::m_structureID[t0], t1</span>
 743     metadata(t2, t3)
<span class="line-modified"> 744     loadi OpToThis::Metadata::m_cachedStructureID[t2], t2</span>
<span class="line-modified"> 745     bineq t1, t2, .opToThisSlow</span>
 746     dispatch()
 747 
 748 .opToThisSlow:
 749     callSlowPath(_slow_path_to_this)
 750     dispatch()
 751 end)
 752 
 753 
 754 llintOp(op_check_tdz, OpCheckTdz, macro (size, get, dispatch)
 755     get(m_targetVirtualRegister, t0)
 756     loadConstantOrVariable(size, t0, t1)
 757     bqneq t1, ValueEmpty, .opNotTDZ
 758     callSlowPath(_slow_path_throw_tdz_error)
 759 
 760 .opNotTDZ:
 761     dispatch()
 762 end)
 763 
 764 
 765 llintOpWithReturn(op_mov, OpMov, macro (size, get, dispatch, return)
</pre>
<hr />
<pre>
 815         cpeq Structure::m_globalObject[t2], t0, t0
 816         jmp .done
 817     .immediate:
 818         andq ~TagBitUndefined, t0
 819         cqeq t0, ValueNull, t0
 820     .done:
 821         fn(t0)
 822         return(t0)
 823     end)
 824 end
 825 
 826 equalNullComparisonOp(op_eq_null, OpEqNull,
 827     macro (value) orq ValueFalse, value end)
 828 
 829 
 830 equalNullComparisonOp(op_neq_null, OpNeqNull,
 831     macro (value) xorq ValueTrue, value end)
 832 
 833 
 834 llintOpWithReturn(op_is_undefined_or_null, OpIsUndefinedOrNull, macro (size, get, dispatch, return)
<span class="line-modified"> 835     get(m_operand, t1)</span>
<span class="line-modified"> 836     loadConstantOrVariable(size, t1, t0)</span>
 837     andq ~TagBitUndefined, t0
 838     cqeq t0, ValueNull, t0
 839     orq ValueFalse, t0
 840     return(t0)
 841 end)
 842 
 843 
 844 macro strictEqOp(opcodeName, opcodeStruct, equalityOperation)
 845     llintOpWithReturn(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, return)
 846         get(m_rhs, t0)
 847         get(m_lhs, t2)
 848         loadConstantOrVariable(size, t0, t1)
 849         loadConstantOrVariable(size, t2, t0)
 850         move t0, t2
 851         orq t1, t2
 852         btqz t2, tagMask, .slow
 853         bqaeq t0, tagTypeNumber, .leftOK
 854         btqnz t0, tagTypeNumber, .slow
 855     .leftOK:
 856         bqaeq t1, tagTypeNumber, .rightOK
</pre>
<hr />
<pre>
1134         bqb t0, tagTypeNumber, .slow
1135         bqb t1, tagTypeNumber, .slow
1136         operation(t1, t0)
1137         orq tagTypeNumber, t0
1138         return(t0)
1139 
1140     .slow:
1141         callSlowPath(_slow_path_%opcodeName%)
1142         dispatch()
1143     end)
1144 end
1145 
1146 macro bitOp(opcodeName, opcodeStruct, operation)
1147     commonBitOp(llintOpWithReturn, opcodeName, opcodeStruct, operation)
1148 end
1149 
1150 macro bitOpProfiled(opcodeName, opcodeStruct, operation)
1151     commonBitOp(llintOpWithProfile, opcodeName, opcodeStruct, operation)
1152 end
1153 
<span class="line-modified">1154 bitOpProfiled(lshift, OpLshift,</span>
1155     macro (left, right) lshifti left, right end)
1156 
1157 
1158 bitOp(rshift, OpRshift,
1159     macro (left, right) rshifti left, right end)
1160 
1161 
1162 bitOp(urshift, OpUrshift,
1163     macro (left, right) urshifti left, right end)
1164 
1165 bitOpProfiled(bitand, OpBitand,
1166     macro (left, right) andi left, right end)
1167 
1168 bitOpProfiled(bitor, OpBitor,
1169     macro (left, right) ori left, right end)
1170 
1171 bitOpProfiled(bitxor, OpBitxor,
1172     macro (left, right) xori left, right end)
1173 
1174 llintOpWithProfile(op_bitnot, OpBitnot, macro (size, get, dispatch, return)
</pre>
<hr />
<pre>
1309 llintOpWithMetadata(op_get_by_id_direct, OpGetByIdDirect, macro (size, get, dispatch, metadata, return)
1310     metadata(t2, t0)
1311     get(m_base, t0)
1312     loadConstantOrVariableCell(size, t0, t3, .opGetByIdDirectSlow)
1313     loadi JSCell::m_structureID[t3], t1
1314     loadi OpGetByIdDirect::Metadata::m_structureID[t2], t0
1315     bineq t0, t1, .opGetByIdDirectSlow
1316     loadi OpGetByIdDirect::Metadata::m_offset[t2], t1
1317     loadPropertyAtVariableOffset(t1, t3, t0)
1318     valueProfile(OpGetByIdDirect, t2, t0)
1319     return(t0)
1320 
1321 .opGetByIdDirectSlow:
1322     callSlowPath(_llint_slow_path_get_by_id_direct)
1323     dispatch()
1324 end)
1325 
1326 
1327 llintOpWithMetadata(op_get_by_id, OpGetById, macro (size, get, dispatch, metadata, return)
1328     metadata(t2, t1)
<span class="line-modified">1329     loadb OpGetById::Metadata::m_modeMetadata.mode[t2], t1</span>
1330     get(m_base, t0)
1331     loadConstantOrVariableCell(size, t0, t3, .opGetByIdSlow)
1332 
1333 .opGetByIdDefault:
1334     bbneq t1, constexpr GetByIdMode::Default, .opGetByIdProtoLoad
1335     loadi JSCell::m_structureID[t3], t1
1336     loadi OpGetById::Metadata::m_modeMetadata.defaultMode.structureID[t2], t0
1337     bineq t0, t1, .opGetByIdSlow
1338     loadis OpGetById::Metadata::m_modeMetadata.defaultMode.cachedOffset[t2], t1
1339     loadPropertyAtVariableOffset(t1, t3, t0)
1340     valueProfile(OpGetById, t2, t0)
1341     return(t0)
1342 
1343 .opGetByIdProtoLoad:
1344     bbneq t1, constexpr GetByIdMode::ProtoLoad, .opGetByIdArrayLength
1345     loadi JSCell::m_structureID[t3], t1
1346     loadi OpGetById::Metadata::m_modeMetadata.protoLoadMode.structureID[t2], t3
1347     bineq t3, t1, .opGetByIdSlow
1348     loadis OpGetById::Metadata::m_modeMetadata.protoLoadMode.cachedOffset[t2], t1
1349     loadp OpGetById::Metadata::m_modeMetadata.protoLoadMode.cachedSlot[t2], t3
1350     loadPropertyAtVariableOffset(t1, t3, t0)
1351     valueProfile(OpGetById, t2, t0)
1352     return(t0)
1353 
1354 .opGetByIdArrayLength:
1355     bbneq t1, constexpr GetByIdMode::ArrayLength, .opGetByIdUnset
1356     move t3, t0
1357     arrayProfile(OpGetById::Metadata::m_modeMetadata.arrayLengthMode.arrayProfile, t0, t2, t5)
1358     btiz t0, IsArray, .opGetByIdSlow
1359     btiz t0, IndexingShapeMask, .opGetByIdSlow
<span class="line-modified">1360     loadCagedJSValue(JSObject::m_butterfly[t3], t0, t1)</span>
1361     loadi -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0], t0
1362     bilt t0, 0, .opGetByIdSlow
1363     orq tagTypeNumber, t0
1364     valueProfile(OpGetById, t2, t0)
1365     return(t0)
1366 
1367 .opGetByIdUnset:
1368     loadi JSCell::m_structureID[t3], t1
1369     loadi OpGetById::Metadata::m_modeMetadata.unsetMode.structureID[t2], t0
1370     bineq t0, t1, .opGetByIdSlow
1371     valueProfile(OpGetById, t2, ValueUndefined)
1372     return(ValueUndefined)
1373 
1374 .opGetByIdSlow:
1375     callSlowPath(_llint_slow_path_get_by_id)
1376     dispatch()
1377 end)
1378 
1379 
1380 llintOpWithMetadata(op_put_by_id, OpPutById, macro (size, get, dispatch, metadata, return)
</pre>
<hr />
<pre>
1463     end
1464 
1465     macro finishDoubleGetByVal(result, scratch1, scratch2)
1466         fd2q result, scratch1
1467         subq tagTypeNumber, scratch1
1468         finishGetByVal(scratch1, scratch2)
1469     end
1470 
1471     metadata(t5, t2)
1472 
1473     get(m_base, t2)
1474     loadConstantOrVariableCell(size, t2, t0, .opGetByValSlow)
1475 
1476     move t0, t2
1477     arrayProfile(OpGetByVal::Metadata::m_arrayProfile, t2, t5, t1)
1478 
1479     get(m_property, t3)
1480     loadConstantOrVariableInt32(size, t3, t1, .opGetByValSlow)
1481     sxi2q t1, t1
1482 
<span class="line-modified">1483     loadCagedJSValue(JSObject::m_butterfly[t0], t3, tagTypeNumber)</span>
1484     move TagTypeNumber, tagTypeNumber
1485 
1486     andi IndexingShapeMask, t2
1487     bieq t2, Int32Shape, .opGetByValIsContiguous
1488     bineq t2, ContiguousShape, .opGetByValNotContiguous
1489 
1490 .opGetByValIsContiguous:
1491     biaeq t1, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t3], .opGetByValSlow
1492     get(m_dst, t0)
1493     loadq [t3, t1, 8], t2
1494     btqz t2, .opGetByValSlow
1495     jmp .opGetByValDone
1496 
1497 .opGetByValNotContiguous:
1498     bineq t2, DoubleShape, .opGetByValNotDouble
1499     biaeq t1, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t3], .opGetByValSlow
1500     get(m_dst, t0)
1501     loadd [t3, t1, 8], ft0
1502     bdnequn ft0, ft0, .opGetByValSlow
1503     fd2q ft0, t2
</pre>
<hr />
<pre>
1507 .opGetByValNotDouble:
1508     subi ArrayStorageShape, t2
1509     bia t2, SlowPutArrayStorageShape - ArrayStorageShape, .opGetByValNotIndexedStorage
1510     biaeq t1, -sizeof IndexingHeader + IndexingHeader::u.lengths.vectorLength[t3], .opGetByValSlow
1511     get(m_dst, t0)
1512     loadq ArrayStorage::m_vector[t3, t1, 8], t2
1513     btqz t2, .opGetByValSlow
1514 
1515 .opGetByValDone:
1516     storeq t2, [cfr, t0, 8]
1517     valueProfile(OpGetByVal, t5, t2)
1518     dispatch()
1519 
1520 .opGetByValNotIndexedStorage:
1521     # First lets check if we even have a typed array. This lets us do some boilerplate up front.
1522     loadb JSCell::m_type[t0], t2
1523     subi FirstTypedArrayType, t2
1524     biaeq t2, NumberOfTypedArrayTypesExcludingDataView, .opGetByValSlow
1525     
1526     # Sweet, now we know that we have a typed array. Do some basic things now.
<span class="line-modified">1527 </span>
<span class="line-added">1528     if ARM64E</span>
<span class="line-added">1529         const length = t6</span>
<span class="line-added">1530         const scratch = t7</span>
<span class="line-added">1531         loadi JSArrayBufferView::m_length[t0], length</span>
<span class="line-added">1532         biaeq t1, length, .opGetByValSlow</span>
<span class="line-added">1533     else</span>
<span class="line-added">1534         # length and scratch are intentionally undefined on this branch because they are not used on other platforms.</span>
<span class="line-added">1535         biaeq t1, JSArrayBufferView::m_length[t0], .opGetByValSlow</span>
<span class="line-added">1536     end</span>
<span class="line-added">1537 </span>
<span class="line-added">1538     loadp JSArrayBufferView::m_vector[t0], t3</span>
<span class="line-added">1539     cagedPrimitive(t3, length, t0, scratch)</span>
1540 
1541     # Now bisect through the various types:
1542     #    Int8ArrayType,
1543     #    Uint8ArrayType,
1544     #    Uint8ClampedArrayType,
1545     #    Int16ArrayType,
1546     #    Uint16ArrayType,
1547     #    Int32ArrayType,
1548     #    Uint32ArrayType,
1549     #    Float32ArrayType,
1550     #    Float64ArrayType,
1551 
1552     bia t2, Uint16ArrayType - FirstTypedArrayType, .opGetByValAboveUint16Array
1553 
1554     # We have one of Int8ArrayType .. Uint16ArrayType.
1555     bia t2, Uint8ClampedArrayType - FirstTypedArrayType, .opGetByValInt16ArrayOrUint16Array
1556 
1557     # We have one of Int8ArrayType ... Uint8ClampedArrayType
1558     bia t2, Int8ArrayType - FirstTypedArrayType, .opGetByValUint8ArrayOrUint8ClampedArray
1559 
1560     # We have Int8ArrayType.
<span class="line-modified">1561     loadbsi [t3, t1], t0</span>

1562     finishIntGetByVal(t0, t1)
1563 
1564 .opGetByValUint8ArrayOrUint8ClampedArray:
1565     bia t2, Uint8ArrayType - FirstTypedArrayType, .opGetByValUint8ClampedArray
1566 
1567     # We have Uint8ArrayType.

1568     loadb [t3, t1], t0
1569     finishIntGetByVal(t0, t1)
1570 
1571 .opGetByValUint8ClampedArray:
1572     # We have Uint8ClampedArrayType.

1573     loadb [t3, t1], t0
1574     finishIntGetByVal(t0, t1)
1575 
1576 .opGetByValInt16ArrayOrUint16Array:
1577     # We have either Int16ArrayType or Uint16ClampedArrayType.
1578     bia t2, Int16ArrayType - FirstTypedArrayType, .opGetByValUint16Array
1579 
1580     # We have Int16ArrayType.
<span class="line-modified">1581     loadhsi [t3, t1, 2], t0</span>

1582     finishIntGetByVal(t0, t1)
1583 
1584 .opGetByValUint16Array:
1585     # We have Uint16ArrayType.

1586     loadh [t3, t1, 2], t0
1587     finishIntGetByVal(t0, t1)
1588 
1589 .opGetByValAboveUint16Array:
1590     # We have one of Int32ArrayType .. Float64ArrayType.
1591     bia t2, Uint32ArrayType - FirstTypedArrayType, .opGetByValFloat32ArrayOrFloat64Array
1592 
1593     # We have either Int32ArrayType or Uint32ArrayType
1594     bia t2, Int32ArrayType - FirstTypedArrayType, .opGetByValUint32Array
1595 
1596     # We have Int32ArrayType.

1597     loadi [t3, t1, 4], t0
1598     finishIntGetByVal(t0, t1)
1599 
1600 .opGetByValUint32Array:
1601     # We have Uint32ArrayType.

1602     # This is the hardest part because of large unsigned values.
1603     loadi [t3, t1, 4], t0
1604     bilt t0, 0, .opGetByValSlow # This case is still awkward to implement in LLInt.
1605     finishIntGetByVal(t0, t1)
1606 
1607 .opGetByValFloat32ArrayOrFloat64Array:
1608     # We have one of Float32ArrayType or Float64ArrayType. Sadly, we cannot handle Float32Array
1609     # inline yet. That would require some offlineasm changes.
1610     bieq t2, Float32ArrayType - FirstTypedArrayType, .opGetByValSlow
1611 
1612     # We have Float64ArrayType.

1613     loadd [t3, t1, 8], ft0
1614     bdnequn ft0, ft0, .opGetByValSlow
1615     finishDoubleGetByVal(ft0, t0, t1)
1616 
1617 .opGetByValSlow:
1618     callSlowPath(_llint_slow_path_get_by_val)
1619     dispatch()
1620 end)
1621 
1622 
1623 macro putByValOp(opcodeName, opcodeStruct)
1624     llintOpWithMetadata(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, metadata, return)
1625         macro contiguousPutByVal(storeCallback)
1626             biaeq t3, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0], .outOfBounds
1627         .storeResult:
1628             get(m_value, t2)
1629             storeCallback(t2, t1, [t0, t3, 8])
1630             dispatch()
1631 
1632         .outOfBounds:
1633             biaeq t3, -sizeof IndexingHeader + IndexingHeader::u.lengths.vectorLength[t0], .opPutByValOutOfBounds
1634             storeb 1, %opcodeStruct%::Metadata::m_arrayProfile.m_mayStoreToHole[t5]
1635             addi 1, t3, t2
1636             storei t2, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0]
1637             jmp .storeResult
1638         end
1639 
1640         get(m_base, t0)
1641         loadConstantOrVariableCell(size, t0, t1, .opPutByValSlow)
1642         move t1, t2
1643         metadata(t5, t0)
1644         arrayProfile(%opcodeStruct%::Metadata::m_arrayProfile, t2, t5, t0)
1645         get(m_property, t0)
1646         loadConstantOrVariableInt32(size, t0, t3, .opPutByValSlow)
1647         sxi2q t3, t3
<span class="line-modified">1648         loadCagedJSValue(JSObject::m_butterfly[t1], t0, tagTypeNumber)</span>
1649         move TagTypeNumber, tagTypeNumber
1650         btinz t2, CopyOnWrite, .opPutByValSlow
1651         andi IndexingShapeMask, t2
1652         bineq t2, Int32Shape, .opPutByValNotInt32
1653         contiguousPutByVal(
1654             macro (operand, scratch, address)
1655                 loadConstantOrVariable(size, operand, scratch)
1656                 bqb scratch, tagTypeNumber, .opPutByValSlow
1657                 storeq scratch, address
1658                 writeBarrierOnOperands(size, get, m_base, m_value)
1659             end)
1660 
1661     .opPutByValNotInt32:
1662         bineq t2, DoubleShape, .opPutByValNotDouble
1663         contiguousPutByVal(
1664             macro (operand, scratch, address)
1665                 loadConstantOrVariable(size, operand, scratch)
1666                 bqb scratch, tagTypeNumber, .notInt
1667                 ci2d scratch, ft0
1668                 jmp .ready
</pre>
<hr />
<pre>
1757 equalNullJumpOp(jeq_null, OpJeqNull,
1758     macro (structure, value, target) 
1759         btbz value, MasqueradesAsUndefined, .notMasqueradesAsUndefined
1760         loadp CodeBlock[cfr], t0
1761         loadp CodeBlock::m_globalObject[t0], t0
1762         bpeq Structure::m_globalObject[structure], t0, target
1763 .notMasqueradesAsUndefined:
1764     end,
1765     macro (value, target) bqeq value, ValueNull, target end)
1766 
1767 
1768 equalNullJumpOp(jneq_null, OpJneqNull,
1769     macro (structure, value, target) 
1770         btbz value, MasqueradesAsUndefined, target
1771         loadp CodeBlock[cfr], t0
1772         loadp CodeBlock::m_globalObject[t0], t0
1773         bpneq Structure::m_globalObject[structure], t0, target
1774     end,
1775     macro (value, target) bqneq value, ValueNull, target end)
1776 
<span class="line-added">1777 macro undefinedOrNullJumpOp(opcodeName, opcodeStruct, fn)</span>
<span class="line-added">1778     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)</span>
<span class="line-added">1779         get(m_value, t1)</span>
<span class="line-added">1780         loadConstantOrVariable(size, t1, t0)</span>
<span class="line-added">1781         andq ~TagBitUndefined, t0</span>
<span class="line-added">1782         fn(t0, .target)</span>
<span class="line-added">1783         dispatch()</span>
<span class="line-added">1784 </span>
<span class="line-added">1785     .target:</span>
<span class="line-added">1786         jump(m_targetLabel)</span>
<span class="line-added">1787     end)</span>
<span class="line-added">1788 end</span>
<span class="line-added">1789 </span>
<span class="line-added">1790 undefinedOrNullJumpOp(jundefined_or_null, OpJundefinedOrNull,</span>
<span class="line-added">1791     macro (value, target) bqeq value, ValueNull, target end)</span>
<span class="line-added">1792 </span>
<span class="line-added">1793 undefinedOrNullJumpOp(jnundefined_or_null, OpJnundefinedOrNull,</span>
<span class="line-added">1794     macro (value, target) bqneq value, ValueNull, target end)</span>
1795 
1796 llintOpWithMetadata(op_jneq_ptr, OpJneqPtr, macro (size, get, dispatch, metadata, return)
1797     get(m_value, t0)
1798     getu(size, OpJneqPtr, m_specialPointer, t1)
1799     loadp CodeBlock[cfr], t2
1800     loadp CodeBlock::m_globalObject[t2], t2
1801     loadp JSGlobalObject::m_specialPointers[t2, t1, PtrSize], t1
1802     bpneq t1, [cfr, t0, 8], .opJneqPtrTarget
1803     dispatch()
1804 
1805 .opJneqPtrTarget:
1806     metadata(t5, t0)
1807     storeb 1, OpJneqPtr::Metadata::m_hasJumped[t5]
1808     get(m_targetLabel, t0)
1809     jumpImpl(t0)
1810 end)
1811 
1812 
1813 macro compareJumpOp(opcodeName, opcodeStruct, integerCompare, doubleCompare)
1814     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
</pre>
<hr />
<pre>
1944     btpnz t0, isRopeInPointer, .opSwitchOnRope
1945     bineq StringImpl::m_length[t0], 1, .opSwitchCharFallThrough
1946     loadp StringImpl::m_data8[t0], t1
1947     btinz StringImpl::m_hashAndFlags[t0], HashFlags8BitBuffer, .opSwitchChar8Bit
1948     loadh [t1], t0
1949     jmp .opSwitchCharReady
1950 .opSwitchChar8Bit:
1951     loadb [t1], t0
1952 .opSwitchCharReady:
1953     subi SimpleJumpTable::min[t2], t0
1954     biaeq t0, SimpleJumpTable::branchOffsets + VectorSizeOffset[t2], .opSwitchCharFallThrough
1955     loadp SimpleJumpTable::branchOffsets + VectorBufferOffset[t2], t2
1956     loadis [t2, t0, 4], t1
1957     btiz t1, .opSwitchCharFallThrough
1958     dispatchIndirect(t1)
1959 
1960 .opSwitchCharFallThrough:
1961     jump(m_defaultOffset)
1962 
1963 .opSwitchOnRope:
<span class="line-added">1964     bineq JSRopeString::m_compactFibers + JSRopeString::CompactFibers::m_length[t1], 1, .opSwitchCharFallThrough</span>
<span class="line-added">1965 </span>
<span class="line-added">1966 .opSwitchOnRopeChar:</span>
1967     callSlowPath(_llint_slow_path_switch_char)
1968     nextInstruction()
1969 end)
1970 
1971 
1972 # we assume t5 contains the metadata, and we should not scratch that
1973 macro arrayProfileForCall(opcodeStruct, getu)
1974     getu(m_argv, t3)
1975     negp t3
1976     loadq ThisArgumentOffset[cfr, t3, 8], t0
1977     btqnz t0, tagMask, .done
1978     loadi JSCell::m_structureID[t0], t3
<span class="line-modified">1979     storei t3, %opcodeStruct%::Metadata::m_callLinkInfo.m_arrayProfile.m_lastSeenStructureID[t5]</span>
1980 .done:
1981 end
1982 
1983 macro commonCallOp(opcodeName, slowPath, opcodeStruct, prepareCall, prologue)
1984     llintOpWithMetadata(opcodeName, opcodeStruct, macro (size, get, dispatch, metadata, return)
1985         metadata(t5, t0)
1986 
1987         prologue(macro (fieldName, dst)
1988             getu(size, opcodeStruct, fieldName, dst)
1989         end, metadata)
1990 
1991         get(m_callee, t0)
<span class="line-modified">1992         loadp %opcodeStruct%::Metadata::m_callLinkInfo.m_calleeOrLastSeenCalleeWithLinkBit[t5], t2</span>
1993         loadConstantOrVariable(size, t0, t3)
1994         bqneq t3, t2, .opCallSlow
1995         getu(size, opcodeStruct, m_argv, t3)
1996         lshifti 3, t3
1997         negp t3
1998         addp cfr, t3
1999         storeq t2, Callee[t3]
2000         getu(size, opcodeStruct, m_argc, t2)
2001         storei PC, ArgumentCount + TagOffset[cfr]
2002         storei t2, ArgumentCount + PayloadOffset[t3]
2003         move t3, sp
<span class="line-modified">2004         prepareCall(%opcodeStruct%::Metadata::m_callLinkInfo.m_machineCodeTarget[t5], t2, t3, t4, JSEntryPtrTag)</span>
<span class="line-modified">2005         callTargetFunction(size, opcodeStruct, dispatch, %opcodeStruct%::Metadata::m_callLinkInfo.m_machineCodeTarget[t5], JSEntryPtrTag)</span>
2006 
2007     .opCallSlow:
2008         slowPathForCall(size, opcodeStruct, dispatch, slowPath, prepareCall)
2009     end)
2010 end
2011 
2012 llintOp(op_ret, OpRet, macro (size, get, dispatch)
2013     checkSwitchToJITForEpilogue()
2014     get(m_value, t2)
2015     loadConstantOrVariable(size, t2, r0)
2016     doReturn()
2017 end)
2018 
2019 
2020 llintOpWithReturn(op_to_primitive, OpToPrimitive, macro (size, get, dispatch, return)
2021     get(m_src, t2)
2022     loadConstantOrVariable(size, t2, t0)
2023     btqnz t0, tagMask, .opToPrimitiveIsImm
2024     bbaeq JSCell::m_type[t0], ObjectType, .opToPrimitiveSlowCase
2025 .opToPrimitiveIsImm:
</pre>
<hr />
<pre>
2101     andp MarkedBlockMask, t1
2102     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
2103     jmp VM::targetMachinePCForThrow[t1], ExceptionHandlerPtrTag
2104 end)
2105 
2106 
2107 op(llint_throw_during_call_trampoline, macro ()
2108     preserveReturnAddressAfterCall(t2)
2109     jmp _llint_throw_from_slow_path_trampoline
2110 end)
2111 
2112 
2113 macro nativeCallTrampoline(executableOffsetToFunction)
2114 
2115     functionPrologue()
2116     storep 0, CodeBlock[cfr]
2117     loadp Callee[cfr], t0
2118     andp MarkedBlockMask, t0, t1
2119     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
2120     storep cfr, VM::topCallFrame[t1]
<span class="line-modified">2121     if ARM64 or ARM64E or C_LOOP or C_LOOP_WIN</span>
2122         storep lr, ReturnPC[cfr]
2123     end
2124     move cfr, a0
2125     loadp Callee[cfr], t1
2126     loadp JSFunction::m_executable[t1], t1
2127     checkStackPointerAlignment(t3, 0xdead0001)
<span class="line-modified">2128     if C_LOOP or C_LOOP_WIN</span>
2129         cloopCallNative executableOffsetToFunction[t1]
2130     else
2131         if X86_64_WIN
2132             subp 32, sp
2133             call executableOffsetToFunction[t1], JSEntryPtrTag
2134             addp 32, sp
2135         else
2136             call executableOffsetToFunction[t1], JSEntryPtrTag
2137         end
2138     end
2139 
2140     loadp Callee[cfr], t3
2141     andp MarkedBlockMask, t3
2142     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
2143 
2144     btpnz VM::m_exception[t3], .handleException
2145 
2146     functionEpilogue()
2147     ret
2148 
2149 .handleException:
2150     storep cfr, VM::topCallFrame[t3]
2151     jmp _llint_throw_from_slow_path_trampoline
2152 end
2153 
2154 macro internalFunctionCallTrampoline(offsetOfFunction)
2155     functionPrologue()
2156     storep 0, CodeBlock[cfr]
2157     loadp Callee[cfr], t0
2158     andp MarkedBlockMask, t0, t1
2159     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
2160     storep cfr, VM::topCallFrame[t1]
<span class="line-modified">2161     if ARM64 or ARM64E or C_LOOP or C_LOOP_WIN</span>
2162         storep lr, ReturnPC[cfr]
2163     end
2164     move cfr, a0
2165     loadp Callee[cfr], t1
2166     checkStackPointerAlignment(t3, 0xdead0001)
<span class="line-modified">2167     if C_LOOP or C_LOOP_WIN</span>
2168         cloopCallNative offsetOfFunction[t1]
2169     else
2170         if X86_64_WIN
2171             subp 32, sp
2172             call offsetOfFunction[t1], JSEntryPtrTag
2173             addp 32, sp
2174         else
2175             call offsetOfFunction[t1], JSEntryPtrTag
2176         end
2177     end
2178 
2179     loadp Callee[cfr], t3
2180     andp MarkedBlockMask, t3
2181     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
2182 
2183     btpnz VM::m_exception[t3], .handleException
2184 
2185     functionEpilogue()
2186     ret
2187 
</pre>
</td>
</tr>
</table>
<center><a href="LowLevelInterpreter32_64.asm.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="../offlineasm/arm.rb.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>