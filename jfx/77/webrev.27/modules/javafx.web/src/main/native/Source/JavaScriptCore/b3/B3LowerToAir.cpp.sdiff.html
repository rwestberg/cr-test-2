<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/B3LowerToAir.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="B3LowerMacrosAfterOptimizations.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="B3MemoryValue.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/B3LowerToAir.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (C) 2015-2017 Apple Inc. All rights reserved.</span>
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
</pre>
<hr />
<pre>
  26 #include &quot;config.h&quot;
  27 #include &quot;B3LowerToAir.h&quot;
  28 
  29 #if ENABLE(B3_JIT)
  30 
  31 #include &quot;AirBlockInsertionSet.h&quot;
  32 #include &quot;AirCCallSpecial.h&quot;
  33 #include &quot;AirCode.h&quot;
  34 #include &quot;AirInsertionSet.h&quot;
  35 #include &quot;AirInstInlines.h&quot;
  36 #include &quot;AirPrintSpecial.h&quot;
  37 #include &quot;AirStackSlot.h&quot;
  38 #include &quot;B3ArgumentRegValue.h&quot;
  39 #include &quot;B3AtomicValue.h&quot;
  40 #include &quot;B3BasicBlockInlines.h&quot;
  41 #include &quot;B3BlockWorklist.h&quot;
  42 #include &quot;B3CCallValue.h&quot;
  43 #include &quot;B3CheckSpecial.h&quot;
  44 #include &quot;B3Commutativity.h&quot;
  45 #include &quot;B3Dominators.h&quot;

  46 #include &quot;B3FenceValue.h&quot;
  47 #include &quot;B3MemoryValueInlines.h&quot;
  48 #include &quot;B3PatchpointSpecial.h&quot;
  49 #include &quot;B3PatchpointValue.h&quot;
  50 #include &quot;B3PhaseScope.h&quot;
  51 #include &quot;B3PhiChildren.h&quot;
  52 #include &quot;B3Procedure.h&quot;
  53 #include &quot;B3SlotBaseValue.h&quot;
  54 #include &quot;B3StackSlot.h&quot;
  55 #include &quot;B3UpsilonValue.h&quot;
  56 #include &quot;B3UseCounts.h&quot;
  57 #include &quot;B3ValueInlines.h&quot;
  58 #include &quot;B3Variable.h&quot;
  59 #include &quot;B3VariableValue.h&quot;
  60 #include &quot;B3WasmAddressValue.h&quot;
  61 #include &lt;wtf/IndexMap.h&gt;
  62 #include &lt;wtf/IndexSet.h&gt;
  63 #include &lt;wtf/ListDump.h&gt;
  64 
  65 #if ASSERT_DISABLED
</pre>
<hr />
<pre>
  98         , m_useCounts(procedure)
  99         , m_phiChildren(procedure)
 100         , m_dominators(procedure.dominators())
 101         , m_procedure(procedure)
 102         , m_code(procedure.code())
 103         , m_blockInsertionSet(m_code)
 104 #if CPU(X86) || CPU(X86_64)
 105         , m_eax(X86Registers::eax)
 106         , m_ecx(X86Registers::ecx)
 107         , m_edx(X86Registers::edx)
 108 #endif
 109     {
 110     }
 111 
 112     void run()
 113     {
 114         using namespace Air;
 115         for (B3::BasicBlock* block : m_procedure)
 116             m_blockToBlock[block] = m_code.addBlock(block-&gt;frequency());
 117 











 118         for (Value* value : m_procedure.values()) {
 119             switch (value-&gt;opcode()) {
 120             case Phi: {






 121                 m_phiToTmp[value] = m_code.newTmp(value-&gt;resultBank());
 122                 if (B3LowerToAirInternal::verbose)
 123                     dataLog(&quot;Phi tmp for &quot;, *value, &quot;: &quot;, m_phiToTmp[value], &quot;\n&quot;);
 124                 break;
 125             }






 126             default:
 127                 break;
 128             }
 129         }
 130 
 131         for (B3::StackSlot* stack : m_procedure.stackSlots())
 132             m_stackToStack.add(stack, m_code.addStackSlot(stack));
<span class="line-modified"> 133         for (Variable* variable : m_procedure.variables())</span>
<span class="line-modified"> 134             m_variableToTmp.add(variable, m_code.newTmp(variable-&gt;bank()));</span>




 135 
 136         // Figure out which blocks are not rare.
 137         m_fastWorklist.push(m_procedure[0]);
 138         while (B3::BasicBlock* block = m_fastWorklist.pop()) {
 139             for (B3::FrequentedBlock&amp; successor : block-&gt;successors()) {
 140                 if (!successor.isRare())
 141                     m_fastWorklist.push(successor.block());
 142             }
 143         }
 144 
 145         m_procedure.resetValueOwners(); // Used by crossesInterference().
 146 
 147         // Lower defs before uses on a global level. This is a good heuristic to lock down a
 148         // hoisted address expression before we duplicate it back into the loop.
 149         for (B3::BasicBlock* block : m_procedure.blocksInPreOrder()) {
 150             m_block = block;
 151 
 152             m_isRare = !m_fastWorklist.saw(block);
 153 
 154             if (B3LowerToAirInternal::verbose)
</pre>
<hr />
<pre>
 380                 return Tmp(GPRInfo::callFrameRegister);
 381 
 382             Tmp&amp; realTmp = m_valueToTmp[value];
 383             if (!realTmp) {
 384                 realTmp = m_code.newTmp(value-&gt;resultBank());
 385                 if (m_procedure.isFastConstant(value-&gt;key()))
 386                     m_code.addFastTmp(realTmp);
 387                 if (B3LowerToAirInternal::verbose)
 388                     dataLog(&quot;Tmp for &quot;, *value, &quot;: &quot;, realTmp, &quot;\n&quot;);
 389             }
 390             tmp = realTmp;
 391         }
 392         return tmp;
 393     }
 394 
 395     ArgPromise tmpPromise(Value* value)
 396     {
 397         return ArgPromise::tmp(value);
 398     }
 399 























 400     bool canBeInternal(Value* value)
 401     {
 402         // If one of the internal things has already been computed, then we don&#39;t want to cause
 403         // it to be recomputed again.
 404         if (m_valueToTmp[value])
 405             return false;
 406 
 407         // We require internals to have only one use - us. It&#39;s not clear if this should be numUses() or
 408         // numUsingInstructions(). Ideally, it would be numUsingInstructions(), except that it&#39;s not clear
 409         // if we&#39;d actually do the right thing when matching over such a DAG pattern. For now, it simply
 410         // doesn&#39;t matter because we don&#39;t implement patterns that would trigger this.
 411         if (m_useCounts.numUses(value) != 1)
 412             return false;
 413 
 414         return true;
 415     }
 416 
 417     // If you ask canBeInternal() and then construct something from that, and you commit to emitting
 418     // that code, then you must commitInternal() on that value. This is tricky, and you only need to
 419     // do it if you&#39;re pattern matching by hand rather than using the patterns language. Long story
</pre>
<hr />
<pre>
 640         return Arg();
 641     }
 642 
 643     Arg bitImm64(Value* value)
 644     {
 645         if (value-&gt;hasInt()) {
 646             int64_t intValue = value-&gt;asInt();
 647             if (Arg::isValidBitImm64Form(intValue))
 648                 return Arg::bitImm64(intValue);
 649         }
 650         return Arg();
 651     }
 652 
 653     Arg immOrTmp(Value* value)
 654     {
 655         if (Arg result = imm(value))
 656             return result;
 657         return tmp(value);
 658     }
 659 















 660     // By convention, we use Oops to mean &quot;I don&#39;t know&quot;.
 661     Air::Opcode tryOpcodeForType(
 662         Air::Opcode opcode32, Air::Opcode opcode64, Air::Opcode opcodeDouble, Air::Opcode opcodeFloat, Type type)
 663     {
 664         Air::Opcode opcode;
<span class="line-modified"> 665         switch (type) {</span>
 666         case Int32:
 667             opcode = opcode32;
 668             break;
 669         case Int64:
 670             opcode = opcode64;
 671             break;
 672         case Float:
 673             opcode = opcodeFloat;
 674             break;
 675         case Double:
 676             opcode = opcodeDouble;
 677             break;
 678         default:
 679             opcode = Air::Oops;
 680             break;
 681         }
 682 
 683         return opcode;
 684     }
 685 
</pre>
<hr />
<pre>
1093                 kind = OPCODE_FOR_WIDTH(Xchg, memory-&gt;accessWidth());
1094                 kind.effects = true;
1095                 Tmp swapTmp = m_code.newTmp(GP);
1096                 append(relaxedMoveForType(memory-&gt;accessType()), tmp(memory-&gt;child(0)), swapTmp);
1097                 append(kind, swapTmp, dest);
1098                 return;
1099             }
1100 
1101             kind = OPCODE_FOR_WIDTH(StoreRel, memory-&gt;accessWidth());
1102         } else
1103             kind = storeOpcode(memory-&gt;accessWidth(), memory-&gt;accessBank());
1104 
1105         kind.effects |= memory-&gt;traps();
1106 
1107         append(createStore(kind, memory-&gt;child(0), dest));
1108     }
1109 
1110     Air::Opcode moveForType(Type type)
1111     {
1112         using namespace Air;
<span class="line-modified">1113         switch (type) {</span>
1114         case Int32:
1115             return Move32;
1116         case Int64:
1117             RELEASE_ASSERT(is64Bit());
1118             return Move;
1119         case Float:
1120             return MoveFloat;
1121         case Double:
1122             return MoveDouble;
1123         case Void:

1124             break;
1125         }
1126         RELEASE_ASSERT_NOT_REACHED();
1127         return Air::Oops;
1128     }
1129 
1130     Air::Opcode relaxedMoveForType(Type type)
1131     {
1132         using namespace Air;
<span class="line-modified">1133         switch (type) {</span>
1134         case Int32:
1135         case Int64:
1136             // For Int32, we could return Move or Move32. It&#39;s a trade-off.
1137             //
1138             // Move32: Using Move32 guarantees that we use the narrower move, but in cases where the
1139             //     register allocator can&#39;t prove that the variables involved are 32-bit, this will
1140             //     disable coalescing.
1141             //
1142             // Move: Using Move guarantees that the register allocator can coalesce normally, but in
1143             //     cases where it can&#39;t prove that the variables are 32-bit and it doesn&#39;t coalesce,
1144             //     this will force us to use a full 64-bit Move instead of the slightly cheaper
1145             //     32-bit Move32.
1146             //
1147             // Coalescing is a lot more profitable than turning Move into Move32. So, it&#39;s better to
1148             // use Move here because in cases where the register allocator cannot prove that
1149             // everything is 32-bit, we still get coalescing.
1150             return Move;
1151         case Float:
1152             // MoveFloat is always coalescable and we never convert MoveDouble to MoveFloat, so we
1153             // should use MoveFloat when we know that the temporaries involved are 32-bit.
1154             return MoveFloat;
1155         case Double:
1156             return MoveDouble;
1157         case Void:

1158             break;
1159         }
1160         RELEASE_ASSERT_NOT_REACHED();
1161         return Air::Oops;
1162     }
1163 
1164 #if ENABLE(MASM_PROBE)
1165     template&lt;typename... Arguments&gt;
1166     void print(Arguments&amp;&amp;... arguments)
1167     {
1168         Value* origin = m_value;
1169         print(origin, std::forward&lt;Arguments&gt;(arguments)...);
1170     }
1171 
1172     template&lt;typename... Arguments&gt;
1173     void print(Value* origin, Arguments&amp;&amp;... arguments)
1174     {
1175         auto printList = Printer::makePrintRecordList(arguments...);
<span class="line-modified">1176         auto printSpecial = static_cast&lt;Air::PrintSpecial*&gt;(m_code.addSpecial(std::make_unique&lt;Air::PrintSpecial&gt;(printList)));</span>
1177         Inst inst(Air::Patch, origin, Arg::special(printSpecial));
1178         Printer::appendAirArgs(inst, std::forward&lt;Arguments&gt;(arguments)...);
1179         append(WTFMove(inst));
1180     }
1181 #endif // ENABLE(MASM_PROBE)
1182 
1183     template&lt;typename... Arguments&gt;
1184     void append(Air::Kind kind, Arguments&amp;&amp;... arguments)
1185     {
1186         m_insts.last().append(Inst(kind, m_value, std::forward&lt;Arguments&gt;(arguments)...));
1187     }
1188 
1189     template&lt;typename... Arguments&gt;
1190     void appendTrapping(Air::Kind kind, Arguments&amp;&amp;... arguments)
1191     {
1192         m_insts.last().append(trappingInst(m_value, kind, m_value, std::forward&lt;Arguments&gt;(arguments)...));
1193     }
1194 
1195     void append(Inst&amp;&amp; inst)
1196     {
</pre>
<hr />
<pre>
1223     // after this will be appended to the previous block.
1224     void splitBlock(Air::BasicBlock*&amp; previousBlock, Air::BasicBlock*&amp; nextBlock)
1225     {
1226         Air::BasicBlock* block = m_blockToBlock[m_block];
1227 
1228         previousBlock = block;
1229         nextBlock = m_blockInsertionSet.insertAfter(block);
1230 
1231         finishAppendingInstructions(nextBlock);
1232         nextBlock-&gt;successors() = block-&gt;successors();
1233         block-&gt;successors().clear();
1234 
1235         m_insts.append(Vector&lt;Inst&gt;());
1236     }
1237 
1238     template&lt;typename T, typename... Arguments&gt;
1239     T* ensureSpecial(T*&amp; field, Arguments&amp;&amp;... arguments)
1240     {
1241         if (!field) {
1242             field = static_cast&lt;T*&gt;(
<span class="line-modified">1243                 m_code.addSpecial(std::make_unique&lt;T&gt;(std::forward&lt;Arguments&gt;(arguments)...)));</span>
1244         }
1245         return field;
1246     }
1247 
1248     template&lt;typename... Arguments&gt;
1249     CheckSpecial* ensureCheckSpecial(Arguments&amp;&amp;... arguments)
1250     {
1251         CheckSpecial::Key key(std::forward&lt;Arguments&gt;(arguments)...);
1252         auto result = m_checkSpecials.add(key, nullptr);
1253         return ensureSpecial(result.iterator-&gt;value, key);
1254     }
1255 
1256     void fillStackmap(Inst&amp; inst, StackmapValue* stackmap, unsigned numSkipped)
1257     {
1258         for (unsigned i = numSkipped; i &lt; stackmap-&gt;numChildren(); ++i) {
1259             ConstrainedValue value = stackmap-&gt;constrainedChild(i);
1260 
1261             Arg arg;
1262             switch (value.rep().kind()) {
1263             case ValueRep::WarmAny:
1264             case ValueRep::ColdAny:
1265             case ValueRep::LateColdAny:
1266                 if (imm(value.value()))
1267                     arg = imm(value.value());
1268                 else if (value.value()-&gt;hasInt64())
1269                     arg = Arg::bigImm(value.value()-&gt;asInt64());
1270                 else if (value.value()-&gt;hasDouble() &amp;&amp; canBeInternal(value.value())) {
1271                     commitInternal(value.value());
1272                     arg = Arg::bigImm(bitwise_cast&lt;int64_t&gt;(value.value()-&gt;asDouble()));



1273                 } else
1274                     arg = tmp(value.value());
1275                 break;
1276             case ValueRep::SomeRegister:

1277                 arg = tmp(value.value());
1278                 break;
1279             case ValueRep::SomeRegisterWithClobber: {
1280                 Tmp dstTmp = m_code.newTmp(value.value()-&gt;resultBank());
1281                 append(relaxedMoveForType(value.value()-&gt;type()), immOrTmp(value.value()), dstTmp);
1282                 arg = dstTmp;
1283                 break;
1284             }
1285             case ValueRep::LateRegister:
1286             case ValueRep::Register:
1287                 stackmap-&gt;earlyClobbered().clear(value.rep().reg());
1288                 arg = Tmp(value.rep().reg());
1289                 append(relaxedMoveForType(value.value()-&gt;type()), immOrTmp(value.value()), arg);
1290                 break;
1291             case ValueRep::StackArgument:
1292                 arg = Arg::callArg(value.rep().offsetFromSP());
1293                 append(trappingInst(m_value, createStore(moveForType(value.value()-&gt;type()), value.value(), arg)));
1294                 break;
1295             default:
1296                 RELEASE_ASSERT_NOT_REACHED();
</pre>
<hr />
<pre>
1443             if (!shouldInvert)
1444                 break;
1445 
1446             FusionResult fusionResult = prepareToFuse(value);
1447             if (fusionResult == CannotFuse)
1448                 break;
1449             commitFusion(value, fusionResult);
1450 
1451             value = value-&gt;child(0);
1452             inverted = !inverted;
1453         }
1454 
1455         auto createRelCond = [&amp;] (
1456             MacroAssembler::RelationalCondition relationalCondition,
1457             MacroAssembler::DoubleCondition doubleCondition) {
1458             Arg relCond = Arg::relCond(relationalCondition).inverted(inverted);
1459             Arg doubleCond = Arg::doubleCond(doubleCondition).inverted(inverted);
1460             Value* left = value-&gt;child(0);
1461             Value* right = value-&gt;child(1);
1462 
<span class="line-modified">1463             if (isInt(value-&gt;child(0)-&gt;type())) {</span>
1464                 Arg rightImm = imm(right);
1465 
1466                 auto tryCompare = [&amp;] (
1467                     Width width, ArgPromise&amp;&amp; left, ArgPromise&amp;&amp; right) -&gt; Inst {
1468                     if (Inst result = compare(width, relCond, left, right))
1469                         return result;
1470                     if (Inst result = compare(width, relCond.flipped(), right, left))
1471                         return result;
1472                     return Inst();
1473                 };
1474 
1475                 auto tryCompareLoadImm = [&amp;] (
1476                     Width width, B3::Opcode loadOpcode, Arg::Signedness signedness) -&gt; Inst {
1477                     if (rightImm &amp;&amp; rightImm.isRepresentableAs(width, signedness)) {
1478                         if (Inst result = tryCompare(width, loadPromise(left, loadOpcode), rightImm)) {
1479                             commitInternal(left);
1480                             return result;
1481                         }
1482                     }
1483                     return Inst();
</pre>
<hr />
<pre>
2107             return true;
2108         if (tryShl(value-&gt;child(1), value-&gt;child(0)))
2109             return true;
2110 
2111         // The remaining pattern is just:
2112         // Add(@x, @y) (only if offset != 0)
2113         if (!offset)
2114             return false;
2115         ASSERT(!m_locked.contains(value-&gt;child(0)));
2116         ASSERT(!m_locked.contains(value-&gt;child(1)));
2117         append(leaOpcode, Arg::index(tmp(value-&gt;child(0)), tmp(value-&gt;child(1)), 1, offset), tmp(m_value));
2118         commitInternal(innerAdd);
2119         return true;
2120     }
2121 
2122     void appendX86Div(B3::Opcode op)
2123     {
2124         using namespace Air;
2125         Air::Opcode convertToDoubleWord;
2126         Air::Opcode div;
<span class="line-modified">2127         switch (m_value-&gt;type()) {</span>
2128         case Int32:
2129             convertToDoubleWord = X86ConvertToDoubleWord32;
2130             div = X86Div32;
2131             break;
2132         case Int64:
2133             convertToDoubleWord = X86ConvertToQuadWord64;
2134             div = X86Div64;
2135             break;
2136         default:
2137             RELEASE_ASSERT_NOT_REACHED();
2138             return;
2139         }
2140 
2141         ASSERT(op == Div || op == Mod);
2142         Tmp result = op == Div ? m_eax : m_edx;
2143 
2144         append(Move, tmp(m_value-&gt;child(0)), m_eax);
2145         append(convertToDoubleWord, m_eax, m_edx);
2146         append(div, m_eax, m_edx, tmp(m_value-&gt;child(1)));
2147         append(Move, result, tmp(m_value));
</pre>
<hr />
<pre>
2431         reloopBlock-&gt;setSuccessors(doneBlock, reloopBlock);
2432     }
2433 
2434     void lower()
2435     {
2436         using namespace Air;
2437         switch (m_value-&gt;opcode()) {
2438         case B3::Nop: {
2439             // Yes, we will totally see Nop&#39;s because some phases will replaceWithNop() instead of
2440             // properly removing things.
2441             return;
2442         }
2443 
2444         case Load: {
2445             MemoryValue* memory = m_value-&gt;as&lt;MemoryValue&gt;();
2446             Air::Kind kind = moveForType(memory-&gt;type());
2447             if (memory-&gt;hasFence()) {
2448                 if (isX86())
2449                     kind.effects = true;
2450                 else {
<span class="line-modified">2451                     switch (memory-&gt;type()) {</span>
2452                     case Int32:
2453                         kind = LoadAcq32;
2454                         break;
2455                     case Int64:
2456                         kind = LoadAcq64;
2457                         break;
2458                     default:
2459                         RELEASE_ASSERT_NOT_REACHED();
2460                         break;
2461                     }
2462                 }
2463             }
2464             append(trappingInst(m_value, kind, m_value, addr(m_value), tmp(m_value)));
2465             return;
2466         }
2467 
2468         case Load8S: {
2469             Air::Kind kind = Load8SignedExtendTo32;
2470             if (m_value-&gt;as&lt;MemoryValue&gt;()-&gt;hasFence()) {
2471                 if (isX86())
</pre>
<hr />
<pre>
2584             Air::Opcode multiplyNegOpcode = tryOpcodeForType(MultiplyNeg32, MultiplyNeg64, m_value-&gt;type());
2585             if (multiplyNegOpcode != Air::Oops
2586                 &amp;&amp; isValidForm(multiplyNegOpcode, Arg::Tmp, Arg::Tmp, Arg::Tmp)
2587                 &amp;&amp; m_value-&gt;child(0)-&gt;opcode() == Mul
2588                 &amp;&amp; canBeInternal(m_value-&gt;child(0))) {
2589                 Value* multiplyOperation = m_value-&gt;child(0);
2590                 Value* multiplyLeft = multiplyOperation-&gt;child(0);
2591                 Value* multiplyRight = multiplyOperation-&gt;child(1);
2592                 if (!m_locked.contains(multiplyLeft) &amp;&amp; !m_locked.contains(multiplyRight)) {
2593                     append(multiplyNegOpcode, tmp(multiplyLeft), tmp(multiplyRight), tmp(m_value));
2594                     commitInternal(multiplyOperation);
2595                     return;
2596                 }
2597             }
2598 
2599             appendUnOp&lt;Neg32, Neg64, NegateDouble, NegateFloat&gt;(m_value-&gt;child(0));
2600             return;
2601         }
2602 
2603         case Mul: {





















2604             appendBinOp&lt;Mul32, Mul64, MulDouble, MulFloat, Commutative&gt;(
2605                 m_value-&gt;child(0), m_value-&gt;child(1));
2606             return;
2607         }
2608 
2609         case Div: {
2610             if (m_value-&gt;isChill())
2611                 RELEASE_ASSERT(isARM64());
<span class="line-modified">2612             if (isInt(m_value-&gt;type()) &amp;&amp; isX86()) {</span>
2613                 appendX86Div(Div);
2614                 return;
2615             }
<span class="line-modified">2616             ASSERT(!isX86() || isFloat(m_value-&gt;type()));</span>
2617 
2618             appendBinOp&lt;Div32, Div64, DivDouble, DivFloat&gt;(m_value-&gt;child(0), m_value-&gt;child(1));
2619             return;
2620         }
2621 
2622         case UDiv: {
<span class="line-modified">2623             if (isInt(m_value-&gt;type()) &amp;&amp; isX86()) {</span>
2624                 appendX86UDiv(UDiv);
2625                 return;
2626             }
2627 
<span class="line-modified">2628             ASSERT(!isX86() &amp;&amp; !isFloat(m_value-&gt;type()));</span>
2629 
2630             appendBinOp&lt;UDiv32, UDiv64, Air::Oops, Air::Oops&gt;(m_value-&gt;child(0), m_value-&gt;child(1));
2631             return;
2632 
2633         }
2634 
2635         case Mod: {
2636             RELEASE_ASSERT(isX86());
2637             RELEASE_ASSERT(!m_value-&gt;isChill());
2638             appendX86Div(Mod);
2639             return;
2640         }
2641 
2642         case UMod: {
2643             RELEASE_ASSERT(isX86());
2644             appendX86UDiv(UMod);
2645             return;
2646         }
2647 
2648         case BitAnd: {
</pre>
<hr />
<pre>
2971 
2972             m_insts.last().append(createCompare(m_value));
2973             return;
2974         }
2975 
2976         case LessThan:
2977         case GreaterThan:
2978         case LessEqual:
2979         case GreaterEqual:
2980         case Above:
2981         case Below:
2982         case AboveEqual:
2983         case BelowEqual:
2984         case EqualOrUnordered: {
2985             m_insts.last().append(createCompare(m_value));
2986             return;
2987         }
2988 
2989         case Select: {
2990             MoveConditionallyConfig config;
<span class="line-modified">2991             if (isInt(m_value-&gt;type())) {</span>
2992                 config.moveConditionally32 = MoveConditionally32;
2993                 config.moveConditionally64 = MoveConditionally64;
2994                 config.moveConditionallyTest32 = MoveConditionallyTest32;
2995                 config.moveConditionallyTest64 = MoveConditionallyTest64;
2996                 config.moveConditionallyDouble = MoveConditionallyDouble;
2997                 config.moveConditionallyFloat = MoveConditionallyFloat;
2998             } else {
2999                 // FIXME: it&#39;s not obvious that these are particularly efficient.
3000                 // https://bugs.webkit.org/show_bug.cgi?id=169251
3001                 config.moveConditionally32 = MoveDoubleConditionally32;
3002                 config.moveConditionally64 = MoveDoubleConditionally64;
3003                 config.moveConditionallyTest32 = MoveDoubleConditionallyTest32;
3004                 config.moveConditionallyTest64 = MoveDoubleConditionallyTest64;
3005                 config.moveConditionallyDouble = MoveDoubleConditionallyDouble;
3006                 config.moveConditionallyFloat = MoveDoubleConditionallyFloat;
3007             }
3008 
3009             m_insts.last().append(createSelect(config));
3010             return;
3011         }
</pre>
<hr />
<pre>
3036             // FIXME: https://bugs.webkit.org/show_bug.cgi?id=151052
3037             inst.args.append(tmp(cCall-&gt;child(0)));
3038 
3039             if (cCall-&gt;type() != Void)
3040                 inst.args.append(tmp(cCall));
3041 
3042             for (unsigned i = 1; i &lt; cCall-&gt;numChildren(); ++i)
3043                 inst.args.append(immOrTmp(cCall-&gt;child(i)));
3044 
3045             m_insts.last().append(WTFMove(inst));
3046             return;
3047         }
3048 
3049         case Patchpoint: {
3050             PatchpointValue* patchpointValue = m_value-&gt;as&lt;PatchpointValue&gt;();
3051             ensureSpecial(m_patchpointSpecial);
3052 
3053             Inst inst(Patch, patchpointValue, Arg::special(m_patchpointSpecial));
3054 
3055             Vector&lt;Inst&gt; after;
<span class="line-modified">3056             if (patchpointValue-&gt;type() != Void) {</span>
<span class="line-modified">3057                 switch (patchpointValue-&gt;resultConstraint.kind()) {</span>
3058                 case ValueRep::WarmAny:
3059                 case ValueRep::ColdAny:
3060                 case ValueRep::LateColdAny:
3061                 case ValueRep::SomeRegister:
3062                 case ValueRep::SomeEarlyRegister:
<span class="line-modified">3063                     inst.args.append(tmp(patchpointValue));</span>
<span class="line-modified">3064                     break;</span>

3065                 case ValueRep::Register: {
<span class="line-modified">3066                     Tmp reg = Tmp(patchpointValue-&gt;resultConstraint.reg());</span>
3067                     inst.args.append(reg);
<span class="line-modified">3068                     after.append(Inst(</span>
<span class="line-modified">3069                         relaxedMoveForType(patchpointValue-&gt;type()), m_value, reg, tmp(patchpointValue)));</span>
<span class="line-removed">3070                     break;</span>
3071                 }
3072                 case ValueRep::StackArgument: {
<span class="line-modified">3073                     Arg arg = Arg::callArg(patchpointValue-&gt;resultConstraint.offsetFromSP());</span>
3074                     inst.args.append(arg);
<span class="line-modified">3075                     after.append(Inst(</span>
<span class="line-modified">3076                         moveForType(patchpointValue-&gt;type()), m_value, arg, tmp(patchpointValue)));</span>
<span class="line-removed">3077                     break;</span>
3078                 }
3079                 default:
3080                     RELEASE_ASSERT_NOT_REACHED();
<span class="line-modified">3081                     break;</span>
3082                 }






3083             }
3084 
3085             fillStackmap(inst, patchpointValue, 0);
<span class="line-modified">3086 </span>
<span class="line-modified">3087             if (patchpointValue-&gt;resultConstraint.isReg())</span>
<span class="line-modified">3088                 patchpointValue-&gt;lateClobbered().clear(patchpointValue-&gt;resultConstraint.reg());</span>

3089 
3090             for (unsigned i = patchpointValue-&gt;numGPScratchRegisters; i--;)
3091                 inst.args.append(m_code.newTmp(GP));
3092             for (unsigned i = patchpointValue-&gt;numFPScratchRegisters; i--;)
3093                 inst.args.append(m_code.newTmp(FP));
3094 
3095             m_insts.last().append(WTFMove(inst));
3096             m_insts.last().appendVector(after);
3097             return;
3098         }
3099 









3100         case CheckAdd:
3101         case CheckSub:
3102         case CheckMul: {
3103             CheckValue* checkValue = m_value-&gt;as&lt;CheckValue&gt;();
3104 
3105             Value* left = checkValue-&gt;child(0);
3106             Value* right = checkValue-&gt;child(1);
3107 
3108             Tmp result = tmp(m_value);
3109 
3110             // Handle checked negation.
3111             if (checkValue-&gt;opcode() == CheckSub &amp;&amp; left-&gt;isInt(0)) {
3112                 append(Move, tmp(right), result);
3113 
3114                 Air::Opcode opcode =
3115                     opcodeForType(BranchNeg32, BranchNeg64, checkValue-&gt;type());
3116                 CheckSpecial* special = ensureCheckSpecial(opcode, 2);
3117 
3118                 Inst inst(Patch, checkValue, Arg::special(special));
3119                 inst.args.append(Arg::resCond(MacroAssembler::Overflow));
</pre>
<hr />
<pre>
3252             switch (value-&gt;boundsType()) {
3253             case WasmBoundsCheckValue::Type::Pinned:
3254                 limit = Arg(value-&gt;bounds().pinnedSize);
3255                 break;
3256 
3257             case WasmBoundsCheckValue::Type::Maximum:
3258                 limit = m_code.newTmp(GP);
3259                 if (imm(value-&gt;bounds().maximum))
3260                     append(Move, imm(value-&gt;bounds().maximum), limit);
3261                 else
3262                     append(Move, Arg::bigImm(value-&gt;bounds().maximum), limit);
3263                 break;
3264             }
3265 
3266             append(Inst(Air::WasmBoundsCheck, value, ptrPlusImm, limit));
3267             return;
3268         }
3269 
3270         case Upsilon: {
3271             Value* value = m_value-&gt;child(0);
<span class="line-modified">3272             append(</span>
<span class="line-modified">3273                 relaxedMoveForType(value-&gt;type()), immOrTmp(value),</span>
<span class="line-modified">3274                 m_phiToTmp[m_value-&gt;as&lt;UpsilonValue&gt;()-&gt;phi()]);</span>









3275             return;
3276         }
3277 
3278         case Phi: {
3279             // Snapshot the value of the Phi. It may change under us because you could do:
3280             // a = Phi()
3281             // Upsilon(@x, ^a)
3282             // @a =&gt; this should get the value of the Phi before the Upsilon, i.e. not @x.
3283 
<span class="line-modified">3284             append(relaxedMoveForType(m_value-&gt;type()), m_phiToTmp[m_value], tmp(m_value));</span>










3285             return;
3286         }
3287 
3288         case Set: {
3289             Value* value = m_value-&gt;child(0);
<span class="line-modified">3290             append(</span>
<span class="line-modified">3291                 relaxedMoveForType(value-&gt;type()), immOrTmp(value),</span>
<span class="line-modified">3292                 m_variableToTmp.get(m_value-&gt;as&lt;VariableValue&gt;()-&gt;variable()));</span>

3293             return;
3294         }
3295 
3296         case Get: {
<span class="line-modified">3297             append(</span>
<span class="line-modified">3298                 relaxedMoveForType(m_value-&gt;type()),</span>
<span class="line-modified">3299                 m_variableToTmp.get(m_value-&gt;as&lt;VariableValue&gt;()-&gt;variable()), tmp(m_value));</span>






3300             return;
3301         }
3302 
3303         case Branch: {
3304             if (canBeInternal(m_value-&gt;child(0))) {
3305                 Value* branchChild = m_value-&gt;child(0);

3306                 switch (branchChild-&gt;opcode()) {












































































3307                 case AtomicWeakCAS:
3308                     commitInternal(branchChild);
3309                     appendCAS(branchChild, false);
3310                     return;
3311 
3312                 case AtomicStrongCAS:
3313                     // A branch is a comparison to zero.
3314                     // FIXME: Teach this to match patterns that arise from subwidth CAS.
3315                     // https://bugs.webkit.org/show_bug.cgi?id=169250
3316                     if (branchChild-&gt;child(0)-&gt;isInt(0)
3317                         &amp;&amp; branchChild-&gt;as&lt;AtomicValue&gt;()-&gt;isCanonicalWidth()) {
3318                         commitInternal(branchChild);
3319                         appendCAS(branchChild, true);
3320                         return;
3321                     }
3322                     break;
3323 
3324                 case Equal:
3325                 case NotEqual:
3326                     // FIXME: Teach this to match patterns that arise from subwidth CAS.
</pre>
<hr />
<pre>
3347 
3348         case B3::Jump: {
3349             append(Air::Jump);
3350             return;
3351         }
3352 
3353         case Identity:
3354         case Opaque: {
3355             ASSERT(tmp(m_value-&gt;child(0)) == tmp(m_value));
3356             return;
3357         }
3358 
3359         case Return: {
3360             if (!m_value-&gt;numChildren()) {
3361                 append(RetVoid);
3362                 return;
3363             }
3364             Value* value = m_value-&gt;child(0);
3365             Tmp returnValueGPR = Tmp(GPRInfo::returnValueGPR);
3366             Tmp returnValueFPR = Tmp(FPRInfo::returnValueFPR);
<span class="line-modified">3367             switch (value-&gt;type()) {</span>
3368             case Void:

3369                 // It&#39;s impossible for a void value to be used as a child. We use RetVoid
3370                 // for void returns.
3371                 RELEASE_ASSERT_NOT_REACHED();
3372                 break;
3373             case Int32:
3374                 append(Move, immOrTmp(value), returnValueGPR);
3375                 append(Ret32, returnValueGPR);
3376                 break;
3377             case Int64:
3378                 append(Move, immOrTmp(value), returnValueGPR);
3379                 append(Ret64, returnValueGPR);
3380                 break;
3381             case Float:
3382                 append(MoveFloat, tmp(value), returnValueFPR);
3383                 append(RetFloat, returnValueFPR);
3384                 break;
3385             case Double:
3386                 append(MoveDouble, tmp(value), returnValueFPR);
3387                 append(RetDouble, returnValueFPR);
3388                 break;
</pre>
<hr />
<pre>
3468                 append(relaxedMoveForType(atomic-&gt;type()), tmp(atomic-&gt;child(0)), tmp(atomic));
3469                 append(opcode, tmp(atomic), address);
3470                 return;
3471             }
3472 
3473             appendGeneralAtomic(Air::Nop);
3474             return;
3475         }
3476 
3477         default:
3478             break;
3479         }
3480 
3481         dataLog(&quot;FATAL: could not lower &quot;, deepDump(m_procedure, m_value), &quot;\n&quot;);
3482         RELEASE_ASSERT_NOT_REACHED();
3483     }
3484 
3485     IndexSet&lt;Value*&gt; m_locked; // These are values that will have no Tmp in Air.
3486     IndexMap&lt;Value*, Tmp&gt; m_valueToTmp; // These are values that must have a Tmp in Air. We say that a Value* with a non-null Tmp is &quot;pinned&quot;.
3487     IndexMap&lt;Value*, Tmp&gt; m_phiToTmp; // Each Phi gets its own Tmp.


3488     IndexMap&lt;B3::BasicBlock*, Air::BasicBlock*&gt; m_blockToBlock;
3489     HashMap&lt;B3::StackSlot*, Air::StackSlot*&gt; m_stackToStack;
<span class="line-modified">3490     HashMap&lt;Variable*, Tmp&gt; m_variableToTmp;</span>
3491 
3492     UseCounts m_useCounts;
3493     PhiChildren m_phiChildren;
3494     BlockWorklist m_fastWorklist;
3495     Dominators&amp; m_dominators;
3496 
3497     Vector&lt;Vector&lt;Inst, 4&gt;&gt; m_insts;
3498     Vector&lt;Inst&gt; m_prologue;
3499 
3500     B3::BasicBlock* m_block;
3501     bool m_isRare;
3502     unsigned m_index;
3503     Value* m_value;
3504 
3505     PatchpointSpecial* m_patchpointSpecial { nullptr };
3506     HashMap&lt;CheckSpecial::Key, CheckSpecial*&gt; m_checkSpecials;
3507 
3508     Procedure&amp; m_procedure;
3509     Code&amp; m_code;
3510 
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (C) 2015-2019 Apple Inc. All rights reserved.</span>
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
</pre>
<hr />
<pre>
  26 #include &quot;config.h&quot;
  27 #include &quot;B3LowerToAir.h&quot;
  28 
  29 #if ENABLE(B3_JIT)
  30 
  31 #include &quot;AirBlockInsertionSet.h&quot;
  32 #include &quot;AirCCallSpecial.h&quot;
  33 #include &quot;AirCode.h&quot;
  34 #include &quot;AirInsertionSet.h&quot;
  35 #include &quot;AirInstInlines.h&quot;
  36 #include &quot;AirPrintSpecial.h&quot;
  37 #include &quot;AirStackSlot.h&quot;
  38 #include &quot;B3ArgumentRegValue.h&quot;
  39 #include &quot;B3AtomicValue.h&quot;
  40 #include &quot;B3BasicBlockInlines.h&quot;
  41 #include &quot;B3BlockWorklist.h&quot;
  42 #include &quot;B3CCallValue.h&quot;
  43 #include &quot;B3CheckSpecial.h&quot;
  44 #include &quot;B3Commutativity.h&quot;
  45 #include &quot;B3Dominators.h&quot;
<span class="line-added">  46 #include &quot;B3ExtractValue.h&quot;</span>
  47 #include &quot;B3FenceValue.h&quot;
  48 #include &quot;B3MemoryValueInlines.h&quot;
  49 #include &quot;B3PatchpointSpecial.h&quot;
  50 #include &quot;B3PatchpointValue.h&quot;
  51 #include &quot;B3PhaseScope.h&quot;
  52 #include &quot;B3PhiChildren.h&quot;
  53 #include &quot;B3Procedure.h&quot;
  54 #include &quot;B3SlotBaseValue.h&quot;
  55 #include &quot;B3StackSlot.h&quot;
  56 #include &quot;B3UpsilonValue.h&quot;
  57 #include &quot;B3UseCounts.h&quot;
  58 #include &quot;B3ValueInlines.h&quot;
  59 #include &quot;B3Variable.h&quot;
  60 #include &quot;B3VariableValue.h&quot;
  61 #include &quot;B3WasmAddressValue.h&quot;
  62 #include &lt;wtf/IndexMap.h&gt;
  63 #include &lt;wtf/IndexSet.h&gt;
  64 #include &lt;wtf/ListDump.h&gt;
  65 
  66 #if ASSERT_DISABLED
</pre>
<hr />
<pre>
  99         , m_useCounts(procedure)
 100         , m_phiChildren(procedure)
 101         , m_dominators(procedure.dominators())
 102         , m_procedure(procedure)
 103         , m_code(procedure.code())
 104         , m_blockInsertionSet(m_code)
 105 #if CPU(X86) || CPU(X86_64)
 106         , m_eax(X86Registers::eax)
 107         , m_ecx(X86Registers::ecx)
 108         , m_edx(X86Registers::edx)
 109 #endif
 110     {
 111     }
 112 
 113     void run()
 114     {
 115         using namespace Air;
 116         for (B3::BasicBlock* block : m_procedure)
 117             m_blockToBlock[block] = m_code.addBlock(block-&gt;frequency());
 118 
<span class="line-added"> 119         auto ensureTupleTmps = [&amp;] (Value* tupleValue, auto&amp; hashTable) {</span>
<span class="line-added"> 120             hashTable.ensure(tupleValue, [&amp;] {</span>
<span class="line-added"> 121                 const auto tuple = m_procedure.tupleForType(tupleValue-&gt;type());</span>
<span class="line-added"> 122                 Vector&lt;Tmp&gt; tmps(tuple.size());</span>
<span class="line-added"> 123 </span>
<span class="line-added"> 124                 for (unsigned i = 0; i &lt; tuple.size(); ++i)</span>
<span class="line-added"> 125                     tmps[i] = tmpForType(tuple[i]);</span>
<span class="line-added"> 126                 return tmps;</span>
<span class="line-added"> 127             });</span>
<span class="line-added"> 128         };</span>
<span class="line-added"> 129 </span>
 130         for (Value* value : m_procedure.values()) {
 131             switch (value-&gt;opcode()) {
 132             case Phi: {
<span class="line-added"> 133                 if (value-&gt;type().isTuple()) {</span>
<span class="line-added"> 134                     ensureTupleTmps(value, m_tuplePhiToTmps);</span>
<span class="line-added"> 135                     ensureTupleTmps(value, m_tupleValueToTmps);</span>
<span class="line-added"> 136                     break;</span>
<span class="line-added"> 137                 }</span>
<span class="line-added"> 138 </span>
 139                 m_phiToTmp[value] = m_code.newTmp(value-&gt;resultBank());
 140                 if (B3LowerToAirInternal::verbose)
 141                     dataLog(&quot;Phi tmp for &quot;, *value, &quot;: &quot;, m_phiToTmp[value], &quot;\n&quot;);
 142                 break;
 143             }
<span class="line-added"> 144             case Get:</span>
<span class="line-added"> 145             case Patchpoint: {</span>
<span class="line-added"> 146                 if (value-&gt;type().isTuple())</span>
<span class="line-added"> 147                     ensureTupleTmps(value, m_tupleValueToTmps);</span>
<span class="line-added"> 148                 break;</span>
<span class="line-added"> 149             }</span>
 150             default:
 151                 break;
 152             }
 153         }
 154 
 155         for (B3::StackSlot* stack : m_procedure.stackSlots())
 156             m_stackToStack.add(stack, m_code.addStackSlot(stack));
<span class="line-modified"> 157         for (Variable* variable : m_procedure.variables()) {</span>
<span class="line-modified"> 158             auto addResult = m_variableToTmps.add(variable, Vector&lt;Tmp, 1&gt;(m_procedure.returnCount(variable-&gt;type())));</span>
<span class="line-added"> 159             ASSERT(addResult.isNewEntry);</span>
<span class="line-added"> 160             for (unsigned i = 0; i &lt; m_procedure.returnCount(variable-&gt;type()); ++i)</span>
<span class="line-added"> 161                 addResult.iterator-&gt;value[i] = tmpForType(variable-&gt;type().isNumeric() ? variable-&gt;type() : m_procedure.extractFromTuple(variable-&gt;type(), i));</span>
<span class="line-added"> 162         }</span>
 163 
 164         // Figure out which blocks are not rare.
 165         m_fastWorklist.push(m_procedure[0]);
 166         while (B3::BasicBlock* block = m_fastWorklist.pop()) {
 167             for (B3::FrequentedBlock&amp; successor : block-&gt;successors()) {
 168                 if (!successor.isRare())
 169                     m_fastWorklist.push(successor.block());
 170             }
 171         }
 172 
 173         m_procedure.resetValueOwners(); // Used by crossesInterference().
 174 
 175         // Lower defs before uses on a global level. This is a good heuristic to lock down a
 176         // hoisted address expression before we duplicate it back into the loop.
 177         for (B3::BasicBlock* block : m_procedure.blocksInPreOrder()) {
 178             m_block = block;
 179 
 180             m_isRare = !m_fastWorklist.saw(block);
 181 
 182             if (B3LowerToAirInternal::verbose)
</pre>
<hr />
<pre>
 408                 return Tmp(GPRInfo::callFrameRegister);
 409 
 410             Tmp&amp; realTmp = m_valueToTmp[value];
 411             if (!realTmp) {
 412                 realTmp = m_code.newTmp(value-&gt;resultBank());
 413                 if (m_procedure.isFastConstant(value-&gt;key()))
 414                     m_code.addFastTmp(realTmp);
 415                 if (B3LowerToAirInternal::verbose)
 416                     dataLog(&quot;Tmp for &quot;, *value, &quot;: &quot;, realTmp, &quot;\n&quot;);
 417             }
 418             tmp = realTmp;
 419         }
 420         return tmp;
 421     }
 422 
 423     ArgPromise tmpPromise(Value* value)
 424     {
 425         return ArgPromise::tmp(value);
 426     }
 427 
<span class="line-added"> 428     Tmp tmpForType(Type type)</span>
<span class="line-added"> 429     {</span>
<span class="line-added"> 430         return m_code.newTmp(bankForType(type));</span>
<span class="line-added"> 431     }</span>
<span class="line-added"> 432 </span>
<span class="line-added"> 433     const Vector&lt;Tmp&gt;&amp; tmpsForTuple(Value* tupleValue)</span>
<span class="line-added"> 434     {</span>
<span class="line-added"> 435         ASSERT(tupleValue-&gt;type().isTuple());</span>
<span class="line-added"> 436 </span>
<span class="line-added"> 437         switch (tupleValue-&gt;opcode()) {</span>
<span class="line-added"> 438         case Phi:</span>
<span class="line-added"> 439         case Patchpoint: {</span>
<span class="line-added"> 440             return m_tupleValueToTmps.find(tupleValue)-&gt;value;</span>
<span class="line-added"> 441         }</span>
<span class="line-added"> 442         case Get:</span>
<span class="line-added"> 443         case Set:</span>
<span class="line-added"> 444             return m_variableToTmps.find(tupleValue-&gt;as&lt;VariableValue&gt;()-&gt;variable())-&gt;value;</span>
<span class="line-added"> 445         default:</span>
<span class="line-added"> 446             break;</span>
<span class="line-added"> 447         }</span>
<span class="line-added"> 448         RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added"> 449     }</span>
<span class="line-added"> 450 </span>
 451     bool canBeInternal(Value* value)
 452     {
 453         // If one of the internal things has already been computed, then we don&#39;t want to cause
 454         // it to be recomputed again.
 455         if (m_valueToTmp[value])
 456             return false;
 457 
 458         // We require internals to have only one use - us. It&#39;s not clear if this should be numUses() or
 459         // numUsingInstructions(). Ideally, it would be numUsingInstructions(), except that it&#39;s not clear
 460         // if we&#39;d actually do the right thing when matching over such a DAG pattern. For now, it simply
 461         // doesn&#39;t matter because we don&#39;t implement patterns that would trigger this.
 462         if (m_useCounts.numUses(value) != 1)
 463             return false;
 464 
 465         return true;
 466     }
 467 
 468     // If you ask canBeInternal() and then construct something from that, and you commit to emitting
 469     // that code, then you must commitInternal() on that value. This is tricky, and you only need to
 470     // do it if you&#39;re pattern matching by hand rather than using the patterns language. Long story
</pre>
<hr />
<pre>
 691         return Arg();
 692     }
 693 
 694     Arg bitImm64(Value* value)
 695     {
 696         if (value-&gt;hasInt()) {
 697             int64_t intValue = value-&gt;asInt();
 698             if (Arg::isValidBitImm64Form(intValue))
 699                 return Arg::bitImm64(intValue);
 700         }
 701         return Arg();
 702     }
 703 
 704     Arg immOrTmp(Value* value)
 705     {
 706         if (Arg result = imm(value))
 707             return result;
 708         return tmp(value);
 709     }
 710 
<span class="line-added"> 711     template&lt;typename Functor&gt;</span>
<span class="line-added"> 712     void forEachImmOrTmp(Value* value, const Functor&amp; func)</span>
<span class="line-added"> 713     {</span>
<span class="line-added"> 714         ASSERT(value-&gt;type() != Void);</span>
<span class="line-added"> 715         if (!value-&gt;type().isTuple()) {</span>
<span class="line-added"> 716             func(immOrTmp(value), value-&gt;type(), 0);</span>
<span class="line-added"> 717             return;</span>
<span class="line-added"> 718         }</span>
<span class="line-added"> 719 </span>
<span class="line-added"> 720         const Vector&lt;Type&gt;&amp; tuple = m_procedure.tupleForType(value-&gt;type());</span>
<span class="line-added"> 721         const auto&amp; tmps = tmpsForTuple(value);</span>
<span class="line-added"> 722         for (unsigned i = 0; i &lt; tuple.size(); ++i)</span>
<span class="line-added"> 723             func(tmps[i], tuple[i], i);</span>
<span class="line-added"> 724     }</span>
<span class="line-added"> 725 </span>
 726     // By convention, we use Oops to mean &quot;I don&#39;t know&quot;.
 727     Air::Opcode tryOpcodeForType(
 728         Air::Opcode opcode32, Air::Opcode opcode64, Air::Opcode opcodeDouble, Air::Opcode opcodeFloat, Type type)
 729     {
 730         Air::Opcode opcode;
<span class="line-modified"> 731         switch (type.kind()) {</span>
 732         case Int32:
 733             opcode = opcode32;
 734             break;
 735         case Int64:
 736             opcode = opcode64;
 737             break;
 738         case Float:
 739             opcode = opcodeFloat;
 740             break;
 741         case Double:
 742             opcode = opcodeDouble;
 743             break;
 744         default:
 745             opcode = Air::Oops;
 746             break;
 747         }
 748 
 749         return opcode;
 750     }
 751 
</pre>
<hr />
<pre>
1159                 kind = OPCODE_FOR_WIDTH(Xchg, memory-&gt;accessWidth());
1160                 kind.effects = true;
1161                 Tmp swapTmp = m_code.newTmp(GP);
1162                 append(relaxedMoveForType(memory-&gt;accessType()), tmp(memory-&gt;child(0)), swapTmp);
1163                 append(kind, swapTmp, dest);
1164                 return;
1165             }
1166 
1167             kind = OPCODE_FOR_WIDTH(StoreRel, memory-&gt;accessWidth());
1168         } else
1169             kind = storeOpcode(memory-&gt;accessWidth(), memory-&gt;accessBank());
1170 
1171         kind.effects |= memory-&gt;traps();
1172 
1173         append(createStore(kind, memory-&gt;child(0), dest));
1174     }
1175 
1176     Air::Opcode moveForType(Type type)
1177     {
1178         using namespace Air;
<span class="line-modified">1179         switch (type.kind()) {</span>
1180         case Int32:
1181             return Move32;
1182         case Int64:
1183             RELEASE_ASSERT(is64Bit());
1184             return Move;
1185         case Float:
1186             return MoveFloat;
1187         case Double:
1188             return MoveDouble;
1189         case Void:
<span class="line-added">1190         case Tuple:</span>
1191             break;
1192         }
1193         RELEASE_ASSERT_NOT_REACHED();
1194         return Air::Oops;
1195     }
1196 
1197     Air::Opcode relaxedMoveForType(Type type)
1198     {
1199         using namespace Air;
<span class="line-modified">1200         switch (type.kind()) {</span>
1201         case Int32:
1202         case Int64:
1203             // For Int32, we could return Move or Move32. It&#39;s a trade-off.
1204             //
1205             // Move32: Using Move32 guarantees that we use the narrower move, but in cases where the
1206             //     register allocator can&#39;t prove that the variables involved are 32-bit, this will
1207             //     disable coalescing.
1208             //
1209             // Move: Using Move guarantees that the register allocator can coalesce normally, but in
1210             //     cases where it can&#39;t prove that the variables are 32-bit and it doesn&#39;t coalesce,
1211             //     this will force us to use a full 64-bit Move instead of the slightly cheaper
1212             //     32-bit Move32.
1213             //
1214             // Coalescing is a lot more profitable than turning Move into Move32. So, it&#39;s better to
1215             // use Move here because in cases where the register allocator cannot prove that
1216             // everything is 32-bit, we still get coalescing.
1217             return Move;
1218         case Float:
1219             // MoveFloat is always coalescable and we never convert MoveDouble to MoveFloat, so we
1220             // should use MoveFloat when we know that the temporaries involved are 32-bit.
1221             return MoveFloat;
1222         case Double:
1223             return MoveDouble;
1224         case Void:
<span class="line-added">1225         case Tuple:</span>
1226             break;
1227         }
1228         RELEASE_ASSERT_NOT_REACHED();
1229         return Air::Oops;
1230     }
1231 
1232 #if ENABLE(MASM_PROBE)
1233     template&lt;typename... Arguments&gt;
1234     void print(Arguments&amp;&amp;... arguments)
1235     {
1236         Value* origin = m_value;
1237         print(origin, std::forward&lt;Arguments&gt;(arguments)...);
1238     }
1239 
1240     template&lt;typename... Arguments&gt;
1241     void print(Value* origin, Arguments&amp;&amp;... arguments)
1242     {
1243         auto printList = Printer::makePrintRecordList(arguments...);
<span class="line-modified">1244         auto printSpecial = static_cast&lt;Air::PrintSpecial*&gt;(m_code.addSpecial(makeUnique&lt;Air::PrintSpecial&gt;(printList)));</span>
1245         Inst inst(Air::Patch, origin, Arg::special(printSpecial));
1246         Printer::appendAirArgs(inst, std::forward&lt;Arguments&gt;(arguments)...);
1247         append(WTFMove(inst));
1248     }
1249 #endif // ENABLE(MASM_PROBE)
1250 
1251     template&lt;typename... Arguments&gt;
1252     void append(Air::Kind kind, Arguments&amp;&amp;... arguments)
1253     {
1254         m_insts.last().append(Inst(kind, m_value, std::forward&lt;Arguments&gt;(arguments)...));
1255     }
1256 
1257     template&lt;typename... Arguments&gt;
1258     void appendTrapping(Air::Kind kind, Arguments&amp;&amp;... arguments)
1259     {
1260         m_insts.last().append(trappingInst(m_value, kind, m_value, std::forward&lt;Arguments&gt;(arguments)...));
1261     }
1262 
1263     void append(Inst&amp;&amp; inst)
1264     {
</pre>
<hr />
<pre>
1291     // after this will be appended to the previous block.
1292     void splitBlock(Air::BasicBlock*&amp; previousBlock, Air::BasicBlock*&amp; nextBlock)
1293     {
1294         Air::BasicBlock* block = m_blockToBlock[m_block];
1295 
1296         previousBlock = block;
1297         nextBlock = m_blockInsertionSet.insertAfter(block);
1298 
1299         finishAppendingInstructions(nextBlock);
1300         nextBlock-&gt;successors() = block-&gt;successors();
1301         block-&gt;successors().clear();
1302 
1303         m_insts.append(Vector&lt;Inst&gt;());
1304     }
1305 
1306     template&lt;typename T, typename... Arguments&gt;
1307     T* ensureSpecial(T*&amp; field, Arguments&amp;&amp;... arguments)
1308     {
1309         if (!field) {
1310             field = static_cast&lt;T*&gt;(
<span class="line-modified">1311                 m_code.addSpecial(makeUnique&lt;T&gt;(std::forward&lt;Arguments&gt;(arguments)...)));</span>
1312         }
1313         return field;
1314     }
1315 
1316     template&lt;typename... Arguments&gt;
1317     CheckSpecial* ensureCheckSpecial(Arguments&amp;&amp;... arguments)
1318     {
1319         CheckSpecial::Key key(std::forward&lt;Arguments&gt;(arguments)...);
1320         auto result = m_checkSpecials.add(key, nullptr);
1321         return ensureSpecial(result.iterator-&gt;value, key);
1322     }
1323 
1324     void fillStackmap(Inst&amp; inst, StackmapValue* stackmap, unsigned numSkipped)
1325     {
1326         for (unsigned i = numSkipped; i &lt; stackmap-&gt;numChildren(); ++i) {
1327             ConstrainedValue value = stackmap-&gt;constrainedChild(i);
1328 
1329             Arg arg;
1330             switch (value.rep().kind()) {
1331             case ValueRep::WarmAny:
1332             case ValueRep::ColdAny:
1333             case ValueRep::LateColdAny:
1334                 if (imm(value.value()))
1335                     arg = imm(value.value());
1336                 else if (value.value()-&gt;hasInt64())
1337                     arg = Arg::bigImm(value.value()-&gt;asInt64());
1338                 else if (value.value()-&gt;hasDouble() &amp;&amp; canBeInternal(value.value())) {
1339                     commitInternal(value.value());
1340                     arg = Arg::bigImm(bitwise_cast&lt;int64_t&gt;(value.value()-&gt;asDouble()));
<span class="line-added">1341                 } else if (value.value()-&gt;hasFloat() &amp;&amp; canBeInternal(value.value())) {</span>
<span class="line-added">1342                     commitInternal(value.value());</span>
<span class="line-added">1343                     arg = Arg::bigImm(static_cast&lt;uint64_t&gt;(bitwise_cast&lt;uint32_t&gt;(value.value()-&gt;asFloat())));</span>
1344                 } else
1345                     arg = tmp(value.value());
1346                 break;
1347             case ValueRep::SomeRegister:
<span class="line-added">1348             case ValueRep::SomeLateRegister:</span>
1349                 arg = tmp(value.value());
1350                 break;
1351             case ValueRep::SomeRegisterWithClobber: {
1352                 Tmp dstTmp = m_code.newTmp(value.value()-&gt;resultBank());
1353                 append(relaxedMoveForType(value.value()-&gt;type()), immOrTmp(value.value()), dstTmp);
1354                 arg = dstTmp;
1355                 break;
1356             }
1357             case ValueRep::LateRegister:
1358             case ValueRep::Register:
1359                 stackmap-&gt;earlyClobbered().clear(value.rep().reg());
1360                 arg = Tmp(value.rep().reg());
1361                 append(relaxedMoveForType(value.value()-&gt;type()), immOrTmp(value.value()), arg);
1362                 break;
1363             case ValueRep::StackArgument:
1364                 arg = Arg::callArg(value.rep().offsetFromSP());
1365                 append(trappingInst(m_value, createStore(moveForType(value.value()-&gt;type()), value.value(), arg)));
1366                 break;
1367             default:
1368                 RELEASE_ASSERT_NOT_REACHED();
</pre>
<hr />
<pre>
1515             if (!shouldInvert)
1516                 break;
1517 
1518             FusionResult fusionResult = prepareToFuse(value);
1519             if (fusionResult == CannotFuse)
1520                 break;
1521             commitFusion(value, fusionResult);
1522 
1523             value = value-&gt;child(0);
1524             inverted = !inverted;
1525         }
1526 
1527         auto createRelCond = [&amp;] (
1528             MacroAssembler::RelationalCondition relationalCondition,
1529             MacroAssembler::DoubleCondition doubleCondition) {
1530             Arg relCond = Arg::relCond(relationalCondition).inverted(inverted);
1531             Arg doubleCond = Arg::doubleCond(doubleCondition).inverted(inverted);
1532             Value* left = value-&gt;child(0);
1533             Value* right = value-&gt;child(1);
1534 
<span class="line-modified">1535             if (value-&gt;child(0)-&gt;type().isInt()) {</span>
1536                 Arg rightImm = imm(right);
1537 
1538                 auto tryCompare = [&amp;] (
1539                     Width width, ArgPromise&amp;&amp; left, ArgPromise&amp;&amp; right) -&gt; Inst {
1540                     if (Inst result = compare(width, relCond, left, right))
1541                         return result;
1542                     if (Inst result = compare(width, relCond.flipped(), right, left))
1543                         return result;
1544                     return Inst();
1545                 };
1546 
1547                 auto tryCompareLoadImm = [&amp;] (
1548                     Width width, B3::Opcode loadOpcode, Arg::Signedness signedness) -&gt; Inst {
1549                     if (rightImm &amp;&amp; rightImm.isRepresentableAs(width, signedness)) {
1550                         if (Inst result = tryCompare(width, loadPromise(left, loadOpcode), rightImm)) {
1551                             commitInternal(left);
1552                             return result;
1553                         }
1554                     }
1555                     return Inst();
</pre>
<hr />
<pre>
2179             return true;
2180         if (tryShl(value-&gt;child(1), value-&gt;child(0)))
2181             return true;
2182 
2183         // The remaining pattern is just:
2184         // Add(@x, @y) (only if offset != 0)
2185         if (!offset)
2186             return false;
2187         ASSERT(!m_locked.contains(value-&gt;child(0)));
2188         ASSERT(!m_locked.contains(value-&gt;child(1)));
2189         append(leaOpcode, Arg::index(tmp(value-&gt;child(0)), tmp(value-&gt;child(1)), 1, offset), tmp(m_value));
2190         commitInternal(innerAdd);
2191         return true;
2192     }
2193 
2194     void appendX86Div(B3::Opcode op)
2195     {
2196         using namespace Air;
2197         Air::Opcode convertToDoubleWord;
2198         Air::Opcode div;
<span class="line-modified">2199         switch (m_value-&gt;type().kind()) {</span>
2200         case Int32:
2201             convertToDoubleWord = X86ConvertToDoubleWord32;
2202             div = X86Div32;
2203             break;
2204         case Int64:
2205             convertToDoubleWord = X86ConvertToQuadWord64;
2206             div = X86Div64;
2207             break;
2208         default:
2209             RELEASE_ASSERT_NOT_REACHED();
2210             return;
2211         }
2212 
2213         ASSERT(op == Div || op == Mod);
2214         Tmp result = op == Div ? m_eax : m_edx;
2215 
2216         append(Move, tmp(m_value-&gt;child(0)), m_eax);
2217         append(convertToDoubleWord, m_eax, m_edx);
2218         append(div, m_eax, m_edx, tmp(m_value-&gt;child(1)));
2219         append(Move, result, tmp(m_value));
</pre>
<hr />
<pre>
2503         reloopBlock-&gt;setSuccessors(doneBlock, reloopBlock);
2504     }
2505 
2506     void lower()
2507     {
2508         using namespace Air;
2509         switch (m_value-&gt;opcode()) {
2510         case B3::Nop: {
2511             // Yes, we will totally see Nop&#39;s because some phases will replaceWithNop() instead of
2512             // properly removing things.
2513             return;
2514         }
2515 
2516         case Load: {
2517             MemoryValue* memory = m_value-&gt;as&lt;MemoryValue&gt;();
2518             Air::Kind kind = moveForType(memory-&gt;type());
2519             if (memory-&gt;hasFence()) {
2520                 if (isX86())
2521                     kind.effects = true;
2522                 else {
<span class="line-modified">2523                     switch (memory-&gt;type().kind()) {</span>
2524                     case Int32:
2525                         kind = LoadAcq32;
2526                         break;
2527                     case Int64:
2528                         kind = LoadAcq64;
2529                         break;
2530                     default:
2531                         RELEASE_ASSERT_NOT_REACHED();
2532                         break;
2533                     }
2534                 }
2535             }
2536             append(trappingInst(m_value, kind, m_value, addr(m_value), tmp(m_value)));
2537             return;
2538         }
2539 
2540         case Load8S: {
2541             Air::Kind kind = Load8SignedExtendTo32;
2542             if (m_value-&gt;as&lt;MemoryValue&gt;()-&gt;hasFence()) {
2543                 if (isX86())
</pre>
<hr />
<pre>
2656             Air::Opcode multiplyNegOpcode = tryOpcodeForType(MultiplyNeg32, MultiplyNeg64, m_value-&gt;type());
2657             if (multiplyNegOpcode != Air::Oops
2658                 &amp;&amp; isValidForm(multiplyNegOpcode, Arg::Tmp, Arg::Tmp, Arg::Tmp)
2659                 &amp;&amp; m_value-&gt;child(0)-&gt;opcode() == Mul
2660                 &amp;&amp; canBeInternal(m_value-&gt;child(0))) {
2661                 Value* multiplyOperation = m_value-&gt;child(0);
2662                 Value* multiplyLeft = multiplyOperation-&gt;child(0);
2663                 Value* multiplyRight = multiplyOperation-&gt;child(1);
2664                 if (!m_locked.contains(multiplyLeft) &amp;&amp; !m_locked.contains(multiplyRight)) {
2665                     append(multiplyNegOpcode, tmp(multiplyLeft), tmp(multiplyRight), tmp(m_value));
2666                     commitInternal(multiplyOperation);
2667                     return;
2668                 }
2669             }
2670 
2671             appendUnOp&lt;Neg32, Neg64, NegateDouble, NegateFloat&gt;(m_value-&gt;child(0));
2672             return;
2673         }
2674 
2675         case Mul: {
<span class="line-added">2676             if (m_value-&gt;type() == Int64</span>
<span class="line-added">2677                 &amp;&amp; isValidForm(MultiplySignExtend32, Arg::Tmp, Arg::Tmp, Arg::Tmp)</span>
<span class="line-added">2678                 &amp;&amp; m_value-&gt;child(0)-&gt;opcode() == SExt32</span>
<span class="line-added">2679                 &amp;&amp; !m_locked.contains(m_value-&gt;child(0))) {</span>
<span class="line-added">2680                 Value* opLeft = m_value-&gt;child(0);</span>
<span class="line-added">2681                 Value* left = opLeft-&gt;child(0);</span>
<span class="line-added">2682                 Value* opRight = m_value-&gt;child(1);</span>
<span class="line-added">2683                 Value* right = nullptr;</span>
<span class="line-added">2684 </span>
<span class="line-added">2685                 if (opRight-&gt;opcode() == SExt32 &amp;&amp; !m_locked.contains(opRight-&gt;child(0))) {</span>
<span class="line-added">2686                     right = opRight-&gt;child(0);</span>
<span class="line-added">2687                 } else if (m_value-&gt;child(1)-&gt;isRepresentableAs&lt;int32_t&gt;() &amp;&amp; !m_locked.contains(m_value-&gt;child(1))) {</span>
<span class="line-added">2688                     // We just use the 64-bit const int as a 32 bit const int directly</span>
<span class="line-added">2689                     right = opRight;</span>
<span class="line-added">2690                 }</span>
<span class="line-added">2691 </span>
<span class="line-added">2692                 if (right) {</span>
<span class="line-added">2693                     append(MultiplySignExtend32, tmp(left), tmp(right), tmp(m_value));</span>
<span class="line-added">2694                     return;</span>
<span class="line-added">2695                 }</span>
<span class="line-added">2696             }</span>
2697             appendBinOp&lt;Mul32, Mul64, MulDouble, MulFloat, Commutative&gt;(
2698                 m_value-&gt;child(0), m_value-&gt;child(1));
2699             return;
2700         }
2701 
2702         case Div: {
2703             if (m_value-&gt;isChill())
2704                 RELEASE_ASSERT(isARM64());
<span class="line-modified">2705             if (m_value-&gt;type().isInt() &amp;&amp; isX86()) {</span>
2706                 appendX86Div(Div);
2707                 return;
2708             }
<span class="line-modified">2709             ASSERT(!isX86() || m_value-&gt;type().isFloat());</span>
2710 
2711             appendBinOp&lt;Div32, Div64, DivDouble, DivFloat&gt;(m_value-&gt;child(0), m_value-&gt;child(1));
2712             return;
2713         }
2714 
2715         case UDiv: {
<span class="line-modified">2716             if (m_value-&gt;type().isInt() &amp;&amp; isX86()) {</span>
2717                 appendX86UDiv(UDiv);
2718                 return;
2719             }
2720 
<span class="line-modified">2721             ASSERT(!isX86() &amp;&amp; !m_value-&gt;type().isFloat());</span>
2722 
2723             appendBinOp&lt;UDiv32, UDiv64, Air::Oops, Air::Oops&gt;(m_value-&gt;child(0), m_value-&gt;child(1));
2724             return;
2725 
2726         }
2727 
2728         case Mod: {
2729             RELEASE_ASSERT(isX86());
2730             RELEASE_ASSERT(!m_value-&gt;isChill());
2731             appendX86Div(Mod);
2732             return;
2733         }
2734 
2735         case UMod: {
2736             RELEASE_ASSERT(isX86());
2737             appendX86UDiv(UMod);
2738             return;
2739         }
2740 
2741         case BitAnd: {
</pre>
<hr />
<pre>
3064 
3065             m_insts.last().append(createCompare(m_value));
3066             return;
3067         }
3068 
3069         case LessThan:
3070         case GreaterThan:
3071         case LessEqual:
3072         case GreaterEqual:
3073         case Above:
3074         case Below:
3075         case AboveEqual:
3076         case BelowEqual:
3077         case EqualOrUnordered: {
3078             m_insts.last().append(createCompare(m_value));
3079             return;
3080         }
3081 
3082         case Select: {
3083             MoveConditionallyConfig config;
<span class="line-modified">3084             if (m_value-&gt;type().isInt()) {</span>
3085                 config.moveConditionally32 = MoveConditionally32;
3086                 config.moveConditionally64 = MoveConditionally64;
3087                 config.moveConditionallyTest32 = MoveConditionallyTest32;
3088                 config.moveConditionallyTest64 = MoveConditionallyTest64;
3089                 config.moveConditionallyDouble = MoveConditionallyDouble;
3090                 config.moveConditionallyFloat = MoveConditionallyFloat;
3091             } else {
3092                 // FIXME: it&#39;s not obvious that these are particularly efficient.
3093                 // https://bugs.webkit.org/show_bug.cgi?id=169251
3094                 config.moveConditionally32 = MoveDoubleConditionally32;
3095                 config.moveConditionally64 = MoveDoubleConditionally64;
3096                 config.moveConditionallyTest32 = MoveDoubleConditionallyTest32;
3097                 config.moveConditionallyTest64 = MoveDoubleConditionallyTest64;
3098                 config.moveConditionallyDouble = MoveDoubleConditionallyDouble;
3099                 config.moveConditionallyFloat = MoveDoubleConditionallyFloat;
3100             }
3101 
3102             m_insts.last().append(createSelect(config));
3103             return;
3104         }
</pre>
<hr />
<pre>
3129             // FIXME: https://bugs.webkit.org/show_bug.cgi?id=151052
3130             inst.args.append(tmp(cCall-&gt;child(0)));
3131 
3132             if (cCall-&gt;type() != Void)
3133                 inst.args.append(tmp(cCall));
3134 
3135             for (unsigned i = 1; i &lt; cCall-&gt;numChildren(); ++i)
3136                 inst.args.append(immOrTmp(cCall-&gt;child(i)));
3137 
3138             m_insts.last().append(WTFMove(inst));
3139             return;
3140         }
3141 
3142         case Patchpoint: {
3143             PatchpointValue* patchpointValue = m_value-&gt;as&lt;PatchpointValue&gt;();
3144             ensureSpecial(m_patchpointSpecial);
3145 
3146             Inst inst(Patch, patchpointValue, Arg::special(m_patchpointSpecial));
3147 
3148             Vector&lt;Inst&gt; after;
<span class="line-modified">3149             auto generateResultOperand = [&amp;] (Type type, ValueRep rep, Tmp tmp) {</span>
<span class="line-modified">3150                 switch (rep.kind()) {</span>
3151                 case ValueRep::WarmAny:
3152                 case ValueRep::ColdAny:
3153                 case ValueRep::LateColdAny:
3154                 case ValueRep::SomeRegister:
3155                 case ValueRep::SomeEarlyRegister:
<span class="line-modified">3156                 case ValueRep::SomeLateRegister:</span>
<span class="line-modified">3157                     inst.args.append(tmp);</span>
<span class="line-added">3158                     return;</span>
3159                 case ValueRep::Register: {
<span class="line-modified">3160                     Tmp reg = Tmp(rep.reg());</span>
3161                     inst.args.append(reg);
<span class="line-modified">3162                     after.append(Inst(relaxedMoveForType(type), m_value, reg, tmp));</span>
<span class="line-modified">3163                     return;</span>

3164                 }
3165                 case ValueRep::StackArgument: {
<span class="line-modified">3166                     Arg arg = Arg::callArg(rep.offsetFromSP());</span>
3167                     inst.args.append(arg);
<span class="line-modified">3168                     after.append(Inst(moveForType(type), m_value, arg, tmp));</span>
<span class="line-modified">3169                     return;</span>

3170                 }
3171                 default:
3172                     RELEASE_ASSERT_NOT_REACHED();
<span class="line-modified">3173                     return;</span>
3174                 }
<span class="line-added">3175             };</span>
<span class="line-added">3176 </span>
<span class="line-added">3177             if (patchpointValue-&gt;type() != Void) {</span>
<span class="line-added">3178                 forEachImmOrTmp(patchpointValue, [&amp;] (Arg arg, Type type, unsigned index) {</span>
<span class="line-added">3179                     generateResultOperand(type, patchpointValue-&gt;resultConstraints[index], arg.tmp());</span>
<span class="line-added">3180                 });</span>
3181             }
3182 
3183             fillStackmap(inst, patchpointValue, 0);
<span class="line-modified">3184             for (auto&amp; constraint : patchpointValue-&gt;resultConstraints) {</span>
<span class="line-modified">3185                 if (constraint.isReg())</span>
<span class="line-modified">3186                     patchpointValue-&gt;lateClobbered().clear(constraint.reg());</span>
<span class="line-added">3187             }</span>
3188 
3189             for (unsigned i = patchpointValue-&gt;numGPScratchRegisters; i--;)
3190                 inst.args.append(m_code.newTmp(GP));
3191             for (unsigned i = patchpointValue-&gt;numFPScratchRegisters; i--;)
3192                 inst.args.append(m_code.newTmp(FP));
3193 
3194             m_insts.last().append(WTFMove(inst));
3195             m_insts.last().appendVector(after);
3196             return;
3197         }
3198 
<span class="line-added">3199         case Extract: {</span>
<span class="line-added">3200             Value* tupleValue = m_value-&gt;child(0);</span>
<span class="line-added">3201             unsigned index = m_value-&gt;as&lt;ExtractValue&gt;()-&gt;index();</span>
<span class="line-added">3202 </span>
<span class="line-added">3203             const auto&amp; tmps = tmpsForTuple(tupleValue);</span>
<span class="line-added">3204             append(relaxedMoveForType(m_value-&gt;type()), tmps[index], tmp(m_value));</span>
<span class="line-added">3205             return;</span>
<span class="line-added">3206         }</span>
<span class="line-added">3207 </span>
3208         case CheckAdd:
3209         case CheckSub:
3210         case CheckMul: {
3211             CheckValue* checkValue = m_value-&gt;as&lt;CheckValue&gt;();
3212 
3213             Value* left = checkValue-&gt;child(0);
3214             Value* right = checkValue-&gt;child(1);
3215 
3216             Tmp result = tmp(m_value);
3217 
3218             // Handle checked negation.
3219             if (checkValue-&gt;opcode() == CheckSub &amp;&amp; left-&gt;isInt(0)) {
3220                 append(Move, tmp(right), result);
3221 
3222                 Air::Opcode opcode =
3223                     opcodeForType(BranchNeg32, BranchNeg64, checkValue-&gt;type());
3224                 CheckSpecial* special = ensureCheckSpecial(opcode, 2);
3225 
3226                 Inst inst(Patch, checkValue, Arg::special(special));
3227                 inst.args.append(Arg::resCond(MacroAssembler::Overflow));
</pre>
<hr />
<pre>
3360             switch (value-&gt;boundsType()) {
3361             case WasmBoundsCheckValue::Type::Pinned:
3362                 limit = Arg(value-&gt;bounds().pinnedSize);
3363                 break;
3364 
3365             case WasmBoundsCheckValue::Type::Maximum:
3366                 limit = m_code.newTmp(GP);
3367                 if (imm(value-&gt;bounds().maximum))
3368                     append(Move, imm(value-&gt;bounds().maximum), limit);
3369                 else
3370                     append(Move, Arg::bigImm(value-&gt;bounds().maximum), limit);
3371                 break;
3372             }
3373 
3374             append(Inst(Air::WasmBoundsCheck, value, ptrPlusImm, limit));
3375             return;
3376         }
3377 
3378         case Upsilon: {
3379             Value* value = m_value-&gt;child(0);
<span class="line-modified">3380             Value* phi = m_value-&gt;as&lt;UpsilonValue&gt;()-&gt;phi();</span>
<span class="line-modified">3381             if (value-&gt;type().isNumeric()) {</span>
<span class="line-modified">3382                 append(relaxedMoveForType(value-&gt;type()), immOrTmp(value), m_phiToTmp[phi]);</span>
<span class="line-added">3383                 return;</span>
<span class="line-added">3384             }</span>
<span class="line-added">3385 </span>
<span class="line-added">3386             const Vector&lt;Type&gt;&amp; tuple = m_procedure.tupleForType(value-&gt;type());</span>
<span class="line-added">3387             const auto&amp; valueTmps = tmpsForTuple(value);</span>
<span class="line-added">3388             const auto&amp; phiTmps = m_tuplePhiToTmps.find(phi)-&gt;value;</span>
<span class="line-added">3389             ASSERT(valueTmps.size() == phiTmps.size());</span>
<span class="line-added">3390             for (unsigned i = 0; i &lt; valueTmps.size(); ++i)</span>
<span class="line-added">3391                 append(relaxedMoveForType(tuple[i]), valueTmps[i], phiTmps[i]);</span>
3392             return;
3393         }
3394 
3395         case Phi: {
3396             // Snapshot the value of the Phi. It may change under us because you could do:
3397             // a = Phi()
3398             // Upsilon(@x, ^a)
3399             // @a =&gt; this should get the value of the Phi before the Upsilon, i.e. not @x.
3400 
<span class="line-modified">3401             if (m_value-&gt;type().isNumeric()) {</span>
<span class="line-added">3402                 append(relaxedMoveForType(m_value-&gt;type()), m_phiToTmp[m_value], tmp(m_value));</span>
<span class="line-added">3403                 return;</span>
<span class="line-added">3404             }</span>
<span class="line-added">3405 </span>
<span class="line-added">3406             const Vector&lt;Type&gt;&amp; tuple = m_procedure.tupleForType(m_value-&gt;type());</span>
<span class="line-added">3407             const auto&amp; valueTmps = tmpsForTuple(m_value);</span>
<span class="line-added">3408             const auto&amp; phiTmps = m_tuplePhiToTmps.find(m_value)-&gt;value;</span>
<span class="line-added">3409             ASSERT(valueTmps.size() == phiTmps.size());</span>
<span class="line-added">3410             for (unsigned i = 0; i &lt; valueTmps.size(); ++i)</span>
<span class="line-added">3411                 append(relaxedMoveForType(tuple[i]), phiTmps[i], valueTmps[i]);</span>
3412             return;
3413         }
3414 
3415         case Set: {
3416             Value* value = m_value-&gt;child(0);
<span class="line-modified">3417             const Vector&lt;Tmp&gt;&amp; variableTmps = m_variableToTmps.get(m_value-&gt;as&lt;VariableValue&gt;()-&gt;variable());</span>
<span class="line-modified">3418             forEachImmOrTmp(value, [&amp;] (Arg immOrTmp, Type type, unsigned index) {</span>
<span class="line-modified">3419                 append(relaxedMoveForType(type), immOrTmp, variableTmps[index]);</span>
<span class="line-added">3420             });</span>
3421             return;
3422         }
3423 
3424         case Get: {
<span class="line-modified">3425             // Snapshot the value of the Get. It may change under us because you could do:</span>
<span class="line-modified">3426             // a = Get(var)</span>
<span class="line-modified">3427             // Set(@x, var)</span>
<span class="line-added">3428             // @a =&gt; this should get the value of the Get before the Set, i.e. not @x.</span>
<span class="line-added">3429 </span>
<span class="line-added">3430             const Vector&lt;Tmp&gt;&amp; variableTmps = m_variableToTmps.get(m_value-&gt;as&lt;VariableValue&gt;()-&gt;variable());</span>
<span class="line-added">3431             forEachImmOrTmp(m_value, [&amp;] (Arg tmp, Type type, unsigned index) {</span>
<span class="line-added">3432                 append(relaxedMoveForType(type), variableTmps[index], tmp.tmp());</span>
<span class="line-added">3433             });</span>
3434             return;
3435         }
3436 
3437         case Branch: {
3438             if (canBeInternal(m_value-&gt;child(0))) {
3439                 Value* branchChild = m_value-&gt;child(0);
<span class="line-added">3440 </span>
3441                 switch (branchChild-&gt;opcode()) {
<span class="line-added">3442                 case BitAnd: {</span>
<span class="line-added">3443                     Value* andValue = branchChild-&gt;child(0);</span>
<span class="line-added">3444                     Value* andMask = branchChild-&gt;child(1);</span>
<span class="line-added">3445                     Air::Opcode opcode = opcodeForType(BranchTestBit32, BranchTestBit64, andValue-&gt;type());</span>
<span class="line-added">3446 </span>
<span class="line-added">3447                     Value* testValue = nullptr;</span>
<span class="line-added">3448                     Value* bitOffset = nullptr;</span>
<span class="line-added">3449                     Value* internalNode = nullptr;</span>
<span class="line-added">3450                     Value* negationNode = nullptr;</span>
<span class="line-added">3451                     bool inverted = false;</span>
<span class="line-added">3452 </span>
<span class="line-added">3453                     // if (~(val &gt;&gt; x)&amp;1)</span>
<span class="line-added">3454                     if (andMask-&gt;isInt(1)</span>
<span class="line-added">3455                         &amp;&amp; andValue-&gt;opcode() == BitXor &amp;&amp; (andValue-&gt;child(1)-&gt;isInt32(-1) || andValue-&gt;child(1)-&gt;isInt64(-1l))</span>
<span class="line-added">3456                         &amp;&amp; (andValue-&gt;child(0)-&gt;opcode() == SShr || andValue-&gt;child(0)-&gt;opcode() == ZShr)) {</span>
<span class="line-added">3457 </span>
<span class="line-added">3458                         negationNode = andValue;</span>
<span class="line-added">3459                         testValue = andValue-&gt;child(0)-&gt;child(0);</span>
<span class="line-added">3460                         bitOffset = andValue-&gt;child(0)-&gt;child(1);</span>
<span class="line-added">3461                         internalNode = andValue-&gt;child(0);</span>
<span class="line-added">3462                         inverted = !inverted;</span>
<span class="line-added">3463                     }</span>
<span class="line-added">3464 </span>
<span class="line-added">3465                     // Turn if ((val &gt;&gt; x)&amp;1) -&gt; Bt val x</span>
<span class="line-added">3466                     if (andMask-&gt;isInt(1) &amp;&amp; (andValue-&gt;opcode() == SShr || andValue-&gt;opcode() == ZShr)) {</span>
<span class="line-added">3467                         testValue = andValue-&gt;child(0);</span>
<span class="line-added">3468                         bitOffset = andValue-&gt;child(1);</span>
<span class="line-added">3469                         internalNode = andValue;</span>
<span class="line-added">3470                     }</span>
<span class="line-added">3471 </span>
<span class="line-added">3472                     // Turn if (val &amp; (1&lt;&lt;x)) -&gt; Bt val x</span>
<span class="line-added">3473                     if ((andMask-&gt;opcode() == Shl) &amp;&amp; andMask-&gt;child(0)-&gt;isInt(1)) {</span>
<span class="line-added">3474                         testValue = andValue;</span>
<span class="line-added">3475                         bitOffset = andMask-&gt;child(1);</span>
<span class="line-added">3476                         internalNode = andMask;</span>
<span class="line-added">3477                     }</span>
<span class="line-added">3478 </span>
<span class="line-added">3479                     // if (~val &amp; (1&lt;&lt;x)) or if ((~val &gt;&gt; x)&amp;1)</span>
<span class="line-added">3480                     if (!negationNode &amp;&amp; testValue &amp;&amp; testValue-&gt;opcode() == BitXor &amp;&amp; (testValue-&gt;child(1)-&gt;isInt32(-1) || testValue-&gt;child(1)-&gt;isInt64(-1l))) {</span>
<span class="line-added">3481                         negationNode = testValue;</span>
<span class="line-added">3482                         testValue = testValue-&gt;child(0);</span>
<span class="line-added">3483                         inverted = !inverted;</span>
<span class="line-added">3484                     }</span>
<span class="line-added">3485 </span>
<span class="line-added">3486                     if (testValue &amp;&amp; bitOffset) {</span>
<span class="line-added">3487                         for (auto&amp; basePromise : Vector&lt;ArgPromise&gt;::from(loadPromise(testValue), tmpPromise(testValue))) {</span>
<span class="line-added">3488                             bool hasLoad = basePromise.kind() != Arg::Tmp;</span>
<span class="line-added">3489                             bool canMakeInternal = (hasLoad ? canBeInternal(testValue) : !m_locked.contains(testValue))</span>
<span class="line-added">3490                                 &amp;&amp; (!negationNode || canBeInternal(negationNode))</span>
<span class="line-added">3491                                 &amp;&amp; (!internalNode || canBeInternal(internalNode));</span>
<span class="line-added">3492 </span>
<span class="line-added">3493                             if (basePromise &amp;&amp; canMakeInternal) {</span>
<span class="line-added">3494                                 if (bitOffset-&gt;hasInt() &amp;&amp; isValidForm(opcode, Arg::ResCond, basePromise.kind(), Arg::Imm)) {</span>
<span class="line-added">3495                                     commitInternal(branchChild);</span>
<span class="line-added">3496                                     commitInternal(internalNode);</span>
<span class="line-added">3497                                     if (hasLoad)</span>
<span class="line-added">3498                                         commitInternal(testValue);</span>
<span class="line-added">3499                                     commitInternal(negationNode);</span>
<span class="line-added">3500                                     append(basePromise.inst(opcode, m_value, Arg::resCond(MacroAssembler::NonZero).inverted(inverted), basePromise.consume(*this), Arg::imm(bitOffset-&gt;asInt())));</span>
<span class="line-added">3501                                     return;</span>
<span class="line-added">3502                                 }</span>
<span class="line-added">3503 </span>
<span class="line-added">3504                                 if (!m_locked.contains(bitOffset) &amp;&amp; isValidForm(opcode, Arg::ResCond, basePromise.kind(), Arg::Tmp)) {</span>
<span class="line-added">3505                                     commitInternal(branchChild);</span>
<span class="line-added">3506                                     commitInternal(internalNode);</span>
<span class="line-added">3507                                     if (hasLoad)</span>
<span class="line-added">3508                                         commitInternal(testValue);</span>
<span class="line-added">3509                                     commitInternal(negationNode);</span>
<span class="line-added">3510                                     append(basePromise.inst(opcode, m_value, Arg::resCond(MacroAssembler::NonZero).inverted(inverted), basePromise.consume(*this), tmp(bitOffset)));</span>
<span class="line-added">3511                                     return;</span>
<span class="line-added">3512                                 }</span>
<span class="line-added">3513                             }</span>
<span class="line-added">3514                         }</span>
<span class="line-added">3515                     }</span>
<span class="line-added">3516                     break;</span>
<span class="line-added">3517                 }</span>
3518                 case AtomicWeakCAS:
3519                     commitInternal(branchChild);
3520                     appendCAS(branchChild, false);
3521                     return;
3522 
3523                 case AtomicStrongCAS:
3524                     // A branch is a comparison to zero.
3525                     // FIXME: Teach this to match patterns that arise from subwidth CAS.
3526                     // https://bugs.webkit.org/show_bug.cgi?id=169250
3527                     if (branchChild-&gt;child(0)-&gt;isInt(0)
3528                         &amp;&amp; branchChild-&gt;as&lt;AtomicValue&gt;()-&gt;isCanonicalWidth()) {
3529                         commitInternal(branchChild);
3530                         appendCAS(branchChild, true);
3531                         return;
3532                     }
3533                     break;
3534 
3535                 case Equal:
3536                 case NotEqual:
3537                     // FIXME: Teach this to match patterns that arise from subwidth CAS.
</pre>
<hr />
<pre>
3558 
3559         case B3::Jump: {
3560             append(Air::Jump);
3561             return;
3562         }
3563 
3564         case Identity:
3565         case Opaque: {
3566             ASSERT(tmp(m_value-&gt;child(0)) == tmp(m_value));
3567             return;
3568         }
3569 
3570         case Return: {
3571             if (!m_value-&gt;numChildren()) {
3572                 append(RetVoid);
3573                 return;
3574             }
3575             Value* value = m_value-&gt;child(0);
3576             Tmp returnValueGPR = Tmp(GPRInfo::returnValueGPR);
3577             Tmp returnValueFPR = Tmp(FPRInfo::returnValueFPR);
<span class="line-modified">3578             switch (value-&gt;type().kind()) {</span>
3579             case Void:
<span class="line-added">3580             case Tuple:</span>
3581                 // It&#39;s impossible for a void value to be used as a child. We use RetVoid
3582                 // for void returns.
3583                 RELEASE_ASSERT_NOT_REACHED();
3584                 break;
3585             case Int32:
3586                 append(Move, immOrTmp(value), returnValueGPR);
3587                 append(Ret32, returnValueGPR);
3588                 break;
3589             case Int64:
3590                 append(Move, immOrTmp(value), returnValueGPR);
3591                 append(Ret64, returnValueGPR);
3592                 break;
3593             case Float:
3594                 append(MoveFloat, tmp(value), returnValueFPR);
3595                 append(RetFloat, returnValueFPR);
3596                 break;
3597             case Double:
3598                 append(MoveDouble, tmp(value), returnValueFPR);
3599                 append(RetDouble, returnValueFPR);
3600                 break;
</pre>
<hr />
<pre>
3680                 append(relaxedMoveForType(atomic-&gt;type()), tmp(atomic-&gt;child(0)), tmp(atomic));
3681                 append(opcode, tmp(atomic), address);
3682                 return;
3683             }
3684 
3685             appendGeneralAtomic(Air::Nop);
3686             return;
3687         }
3688 
3689         default:
3690             break;
3691         }
3692 
3693         dataLog(&quot;FATAL: could not lower &quot;, deepDump(m_procedure, m_value), &quot;\n&quot;);
3694         RELEASE_ASSERT_NOT_REACHED();
3695     }
3696 
3697     IndexSet&lt;Value*&gt; m_locked; // These are values that will have no Tmp in Air.
3698     IndexMap&lt;Value*, Tmp&gt; m_valueToTmp; // These are values that must have a Tmp in Air. We say that a Value* with a non-null Tmp is &quot;pinned&quot;.
3699     IndexMap&lt;Value*, Tmp&gt; m_phiToTmp; // Each Phi gets its own Tmp.
<span class="line-added">3700     HashMap&lt;Value*, Vector&lt;Tmp&gt;&gt; m_tupleValueToTmps; // This is the same as m_valueToTmp for Values that are Tuples.</span>
<span class="line-added">3701     HashMap&lt;Value*, Vector&lt;Tmp&gt;&gt; m_tuplePhiToTmps; // This is the same as m_phiToTmp for Phis that are Tuples.</span>
3702     IndexMap&lt;B3::BasicBlock*, Air::BasicBlock*&gt; m_blockToBlock;
3703     HashMap&lt;B3::StackSlot*, Air::StackSlot*&gt; m_stackToStack;
<span class="line-modified">3704     HashMap&lt;Variable*, Vector&lt;Tmp&gt;&gt; m_variableToTmps;</span>
3705 
3706     UseCounts m_useCounts;
3707     PhiChildren m_phiChildren;
3708     BlockWorklist m_fastWorklist;
3709     Dominators&amp; m_dominators;
3710 
3711     Vector&lt;Vector&lt;Inst, 4&gt;&gt; m_insts;
3712     Vector&lt;Inst&gt; m_prologue;
3713 
3714     B3::BasicBlock* m_block;
3715     bool m_isRare;
3716     unsigned m_index;
3717     Value* m_value;
3718 
3719     PatchpointSpecial* m_patchpointSpecial { nullptr };
3720     HashMap&lt;CheckSpecial::Key, CheckSpecial*&gt; m_checkSpecials;
3721 
3722     Procedure&amp; m_procedure;
3723     Code&amp; m_code;
3724 
</pre>
</td>
</tr>
</table>
<center><a href="B3LowerMacrosAfterOptimizations.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="B3MemoryValue.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>