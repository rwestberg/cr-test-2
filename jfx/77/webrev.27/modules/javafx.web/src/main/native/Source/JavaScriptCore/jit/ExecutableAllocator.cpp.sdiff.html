<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/ExecutableAllocator.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="CallFrameShuffler.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="ExecutableAllocator.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/ExecutableAllocator.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (C) 2008-2018 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;ExecutableAllocator.h&quot;
 28 
 29 #if ENABLE(JIT)
 30 
 31 #include &quot;CodeProfiling.h&quot;
 32 #include &quot;ExecutableAllocationFuzz.h&quot;
 33 #include &quot;JSCInlines.h&quot;

 34 #include &lt;wtf/MetaAllocator.h&gt;
 35 #include &lt;wtf/PageReservation.h&gt;



 36 
 37 #if OS(DARWIN)

 38 #include &lt;sys/mman.h&gt;
 39 #endif
 40 
 41 #if PLATFORM(IOS_FAMILY)
 42 #include &lt;wtf/cocoa/Entitlements.h&gt;
 43 #endif
 44 
 45 #include &quot;LinkBuffer.h&quot;
 46 #include &quot;MacroAssembler.h&quot;
 47 
 48 #if PLATFORM(COCOA)
 49 #define HAVE_REMAP_JIT 1
 50 #endif
 51 
 52 #if HAVE(REMAP_JIT)
 53 #if CPU(ARM64) &amp;&amp; PLATFORM(IOS_FAMILY)
 54 #define USE_EXECUTE_ONLY_JIT_WRITE_FUNCTION 1
 55 #endif
 56 #endif
 57 
</pre>
<hr />
<pre>
 98 static const size_t fixedExecutableMemoryPoolSize = 32 * 1024 * 1024;
 99 #endif
100 
101 #if CPU(ARM)
102 static const double executablePoolReservationFraction = 0.15;
103 #else
104 static const double executablePoolReservationFraction = 0.25;
105 #endif
106 
107 #if ENABLE(SEPARATED_WX_HEAP)
108 JS_EXPORT_PRIVATE bool useFastPermisionsJITCopy { false };
109 JS_EXPORT_PRIVATE JITWriteSeparateHeapsFunction jitWriteSeparateHeapsFunction;
110 #endif
111 
112 #if !USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION) &amp;&amp; HAVE(REMAP_JIT)
113 static uintptr_t startOfFixedWritableMemoryPool;
114 #endif
115 
116 class FixedVMPoolExecutableAllocator;
117 static FixedVMPoolExecutableAllocator* allocator = nullptr;
<span class="line-removed">118 static ExecutableAllocator* executableAllocator = nullptr;</span>
119 
120 static bool s_isJITEnabled = true;
121 static bool isJITEnabled()
122 {
123 #if PLATFORM(IOS_FAMILY) &amp;&amp; (CPU(ARM64) || CPU(ARM))
124     return processHasEntitlement(&quot;dynamic-codesigning&quot;) &amp;&amp; s_isJITEnabled;
125 #else
126     return s_isJITEnabled;
127 #endif
128 }
129 
130 void ExecutableAllocator::setJITEnabled(bool enabled)
131 {
132     ASSERT(!allocator);
133     if (s_isJITEnabled == enabled)
134         return;
135 
136     s_isJITEnabled = enabled;
137 
138 #if PLATFORM(IOS_FAMILY) &amp;&amp; (CPU(ARM64) || CPU(ARM))
139     if (!enabled) {
140         constexpr size_t size = 1;
141         constexpr int protection = PROT_READ | PROT_WRITE | PROT_EXEC;
142         constexpr int flags = MAP_PRIVATE | MAP_ANON | MAP_JIT;
143         constexpr int fd = OSAllocator::JSJITCodePages;
144         void* allocation = mmap(nullptr, size, protection, flags, fd, 0);
145         const void* executableMemoryAllocationFailure = reinterpret_cast&lt;void*&gt;(-1);
146         RELEASE_ASSERT_WITH_MESSAGE(allocation &amp;&amp; allocation != executableMemoryAllocationFailure, &quot;We should not have allocated executable memory before disabling the JIT.&quot;);
147         RELEASE_ASSERT_WITH_MESSAGE(!munmap(allocation, size), &quot;Unmapping executable memory should succeed so we do not have any executable memory in the address space&quot;);
148         RELEASE_ASSERT_WITH_MESSAGE(mmap(nullptr, size, protection, flags, fd, 0) == executableMemoryAllocationFailure, &quot;Allocating executable memory should fail after setJITEnabled(false) is called.&quot;);
149     }
150 #endif
151 }
152 
<span class="line-modified">153 class FixedVMPoolExecutableAllocator : public MetaAllocator {</span>
154     WTF_MAKE_FAST_ALLOCATED;
155 public:
156     FixedVMPoolExecutableAllocator()
157         : MetaAllocator(jitAllocationGranule) // round up all allocations to 32 bytes
158     {
159         if (!isJITEnabled())
160             return;
161 
162         size_t reservationSize;
163         if (Options::jitMemoryReservationSize())
164             reservationSize = Options::jitMemoryReservationSize();
165         else
166             reservationSize = fixedExecutableMemoryPoolSize;
167         reservationSize = std::max(roundUpToMultipleOf(pageSize(), reservationSize), pageSize() * 2);
168 
169         auto tryCreatePageReservation = [] (size_t reservationSize) {
170 #if OS(LINUX)
171             // If we use uncommitted reservation, mmap operation is recorded with small page size in perf command&#39;s output.
172             // This makes the following JIT code logging broken and some of JIT code is not recorded correctly.
173             // To avoid this problem, we use committed reservation if we need perf JITDump logging.
</pre>
<hr />
<pre>
187             os_thread_self_restrict_rwx_to_rx();
188 
189 #else // not ENABLE(FAST_JIT_PERMISSIONS) or ENABLE(SEPARATED_WX_HEAP)
190 #if ENABLE(FAST_JIT_PERMISSIONS)
191             if (os_thread_self_restrict_rwx_is_supported()) {
192                 useFastPermisionsJITCopy = true;
193                 os_thread_self_restrict_rwx_to_rx();
194             } else
195 #endif
196             if (Options::useSeparatedWXHeap()) {
197                 // First page of our JIT allocation is reserved.
198                 ASSERT(reservationSize &gt;= pageSize() * 2);
199                 reservationBase = (void*)((uintptr_t)reservationBase + pageSize());
200                 reservationSize -= pageSize();
201                 initializeSeparatedWXHeaps(m_reservation.base(), pageSize(), reservationBase, reservationSize);
202             }
203 #endif // not ENABLE(FAST_JIT_PERMISSIONS) or ENABLE(SEPARATED_WX_HEAP)
204 
205             addFreshFreeSpace(reservationBase, reservationSize);
206 


207             void* reservationEnd = reinterpret_cast&lt;uint8_t*&gt;(reservationBase) + reservationSize;
208 
209             m_memoryStart = MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt;(tagCodePtr&lt;ExecutableMemoryPtrTag&gt;(reservationBase));
210             m_memoryEnd = MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt;(tagCodePtr&lt;ExecutableMemoryPtrTag&gt;(reservationEnd));
211         }
212     }
213 
214     virtual ~FixedVMPoolExecutableAllocator();
215 
216     void* memoryStart() { return m_memoryStart.untaggedExecutableAddress(); }
217     void* memoryEnd() { return m_memoryEnd.untaggedExecutableAddress(); }
218     bool isJITPC(void* pc) { return memoryStart() &lt;= pc &amp;&amp; pc &lt; memoryEnd(); }
219 
220 protected:
221     FreeSpacePtr allocateNewSpace(size_t&amp;) override
222     {
223         // We&#39;re operating in a fixed pool, so new allocation is always prohibited.
224         return nullptr;
225     }
226 
<span class="line-modified">227     void notifyNeedPage(void* page) override</span>
228     {
229 #if USE(MADV_FREE_FOR_JIT_MEMORY)
230         UNUSED_PARAM(page);

231 #else
<span class="line-modified">232         m_reservation.commit(page, pageSize());</span>
233 #endif
234     }
235 
<span class="line-modified">236     void notifyPageIsFree(void* page) override</span>
237     {
238 #if USE(MADV_FREE_FOR_JIT_MEMORY)
239         for (;;) {
<span class="line-modified">240             int result = madvise(page, pageSize(), MADV_FREE);</span>
241             if (!result)
242                 return;
243             ASSERT(result == -1);
244             if (errno != EAGAIN) {
245                 RELEASE_ASSERT_NOT_REACHED(); // In debug mode, this should be a hard failure.
246                 break; // In release mode, we should just ignore the error - not returning memory to the OS is better than crashing, especially since we _will_ be able to reuse the memory internally anyway.
247             }
248         }
249 #else
<span class="line-modified">250         m_reservation.decommit(page, pageSize());</span>
251 #endif
252     }
253 
254 private:
255 #if OS(DARWIN) &amp;&amp; HAVE(REMAP_JIT)
256     void initializeSeparatedWXHeaps(void* stubBase, size_t stubSize, void* jitBase, size_t jitSize)
257     {
258         mach_vm_address_t writableAddr = 0;
259 
260         // Create a second mapping of the JIT region at a random address.
261         vm_prot_t cur, max;
262         int remapFlags = VM_FLAGS_ANYWHERE;
263 #if defined(VM_FLAGS_RANDOM_ADDR)
264         remapFlags |= VM_FLAGS_RANDOM_ADDR;
265 #endif
266         kern_return_t ret = mach_vm_remap(mach_task_self(), &amp;writableAddr, jitSize, 0,
267             remapFlags,
268             mach_task_self(), (mach_vm_address_t)jitBase, FALSE,
269             &amp;cur, &amp;max, VM_INHERIT_DEFAULT);
270 
</pre>
<hr />
<pre>
387         functionAsInt -= 1;
388         function = reinterpret_cast&lt;void*&gt;(functionAsInt);
389 #endif
390         auto codePtr = MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt;(tagCFunctionPtr&lt;JITThunkPtrTag&gt;(function));
391         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(codePtr);
392     }
393 #endif // CPU(ARM64) &amp;&amp; USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
394 
395 #else // OS(DARWIN) &amp;&amp; HAVE(REMAP_JIT)
396     void initializeSeparatedWXHeaps(void*, size_t, void*, size_t)
397     {
398     }
399 #endif
400 
401 private:
402     PageReservation m_reservation;
403     MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt; m_memoryStart;
404     MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt; m_memoryEnd;
405 };
406 
<span class="line-modified">407 void ExecutableAllocator::initializeAllocator()</span>
<span class="line-removed">408 {</span>
<span class="line-removed">409     ASSERT(!allocator);</span>
<span class="line-removed">410     allocator = new FixedVMPoolExecutableAllocator();</span>
<span class="line-removed">411     CodeProfiling::notifyAllocator(allocator);</span>
<span class="line-removed">412 </span>
<span class="line-removed">413     executableAllocator = new ExecutableAllocator;</span>
<span class="line-removed">414 }</span>
<span class="line-removed">415 </span>
<span class="line-removed">416 ExecutableAllocator&amp; ExecutableAllocator::singleton()</span>
<span class="line-removed">417 {</span>
<span class="line-removed">418     ASSERT(allocator);</span>
<span class="line-removed">419     ASSERT(executableAllocator);</span>
<span class="line-removed">420     return *executableAllocator;</span>
<span class="line-removed">421 }</span>
<span class="line-removed">422 </span>
<span class="line-removed">423 ExecutableAllocator::ExecutableAllocator()</span>
<span class="line-removed">424 {</span>
<span class="line-removed">425     ASSERT(allocator);</span>
<span class="line-removed">426 }</span>
<span class="line-removed">427 </span>
<span class="line-removed">428 ExecutableAllocator::~ExecutableAllocator()</span>
429 {

430 }
431 
<span class="line-modified">432 FixedVMPoolExecutableAllocator::~FixedVMPoolExecutableAllocator()</span>
433 {
<span class="line-modified">434     m_reservation.deallocate();</span>


435 }
436 
437 bool ExecutableAllocator::isValid() const
438 {


439     return !!allocator-&gt;bytesReserved();
440 }
441 
442 bool ExecutableAllocator::underMemoryPressure()
443 {
<span class="line-modified">444     MetaAllocator::Statistics statistics = allocator-&gt;currentStatistics();</span>
<span class="line-modified">445     return statistics.bytesAllocated &gt; statistics.bytesReserved / 2;</span>

446 }
447 
448 double ExecutableAllocator::memoryPressureMultiplier(size_t addedMemoryUsage)
449 {
<span class="line-modified">450     MetaAllocator::Statistics statistics = allocator-&gt;currentStatistics();</span>
<span class="line-modified">451     ASSERT(statistics.bytesAllocated &lt;= statistics.bytesReserved);</span>
<span class="line-modified">452     size_t bytesAllocated = statistics.bytesAllocated + addedMemoryUsage;</span>

453     size_t bytesAvailable = static_cast&lt;size_t&gt;(
<span class="line-modified">454         statistics.bytesReserved * (1 - executablePoolReservationFraction));</span>
455     if (bytesAllocated &gt;= bytesAvailable)
456         bytesAllocated = bytesAvailable;
457     double result = 1.0;
458     size_t divisor = bytesAvailable - bytesAllocated;
459     if (divisor)
460         result = static_cast&lt;double&gt;(bytesAvailable) / divisor;
461     if (result &lt; 1.0)
462         result = 1.0;
463     return result;
464 }
465 
466 RefPtr&lt;ExecutableMemoryHandle&gt; ExecutableAllocator::allocate(size_t sizeInBytes, void* ownerUID, JITCompilationEffort effort)
467 {


468     if (Options::logExecutableAllocation()) {
469         MetaAllocator::Statistics stats = allocator-&gt;currentStatistics();
470         dataLog(&quot;Allocating &quot;, sizeInBytes, &quot; bytes of executable memory with &quot;, stats.bytesAllocated, &quot; bytes allocated, &quot;, stats.bytesReserved, &quot; bytes reserved, and &quot;, stats.bytesCommitted, &quot; committed.\n&quot;);
471     }
472 
473     if (effort != JITCompilationCanFail &amp;&amp; Options::reportMustSucceedExecutableAllocations()) {
474         dataLog(&quot;Allocating &quot;, sizeInBytes, &quot; bytes of executable memory with JITCompilationMustSucceed.\n&quot;);
475         WTFReportBacktrace();
476     }
477 
478     if (effort == JITCompilationCanFail
479         &amp;&amp; doExecutableAllocationFuzzingIfEnabled() == PretendToFailExecutableAllocation)
480         return nullptr;
481 
482     if (effort == JITCompilationCanFail) {
483         // Don&#39;t allow allocations if we are down to reserve.
<span class="line-modified">484         MetaAllocator::Statistics statistics = allocator-&gt;currentStatistics();</span>
<span class="line-removed">485         size_t bytesAllocated = statistics.bytesAllocated + sizeInBytes;</span>
486         size_t bytesAvailable = static_cast&lt;size_t&gt;(
<span class="line-modified">487             statistics.bytesReserved * (1 - executablePoolReservationFraction));</span>
488         if (bytesAllocated &gt; bytesAvailable) {
489             if (Options::logExecutableAllocation())
490                 dataLog(&quot;Allocation failed because bytes allocated &quot;, bytesAllocated,  &quot; &gt; &quot;, bytesAvailable, &quot; bytes available.\n&quot;);
491             return nullptr;
492         }
493     }
494 
495     RefPtr&lt;ExecutableMemoryHandle&gt; result = allocator-&gt;allocate(sizeInBytes, ownerUID);
496     if (!result) {
497         if (effort != JITCompilationCanFail) {
498             dataLog(&quot;Ran out of executable memory while allocating &quot;, sizeInBytes, &quot; bytes.\n&quot;);
499             CRASH();
500         }
501         return nullptr;
502     }
503 
<span class="line-modified">504 #if USE(POINTER_PROFILING)</span>
505     void* start = allocator-&gt;memoryStart();
506     void* end = allocator-&gt;memoryEnd();
507     void* resultStart = result-&gt;start().untaggedPtr();
508     void* resultEnd = result-&gt;end().untaggedPtr();
509     RELEASE_ASSERT(start &lt;= resultStart &amp;&amp; resultStart &lt; end);
510     RELEASE_ASSERT(start &lt; resultEnd &amp;&amp; resultEnd &lt;= end);
511 #endif
512     return result;
513 }
514 
515 bool ExecutableAllocator::isValidExecutableMemory(const AbstractLocker&amp; locker, void* address)
516 {


517     return allocator-&gt;isInAllocatedMemory(locker, address);
518 }
519 
520 Lock&amp; ExecutableAllocator::getLock() const
521 {


522     return allocator-&gt;getLock();
523 }
524 
525 size_t ExecutableAllocator::committedByteCount()
526 {


527     return allocator-&gt;bytesCommitted();
528 }
529 
530 #if ENABLE(META_ALLOCATOR_PROFILE)
531 void ExecutableAllocator::dumpProfile()
532 {


533     allocator-&gt;dumpProfile();
534 }
535 #endif
536 
537 void* startOfFixedExecutableMemoryPoolImpl()
538 {


539     return allocator-&gt;memoryStart();
540 }
541 
542 void* endOfFixedExecutableMemoryPoolImpl()
543 {


544     return allocator-&gt;memoryEnd();
545 }
546 
547 bool isJITPC(void* pc)
548 {
549     return allocator &amp;&amp; allocator-&gt;isJITPC(pc);
550 }
551 










































































552 } // namespace JSC
553 
<span class="line-modified">554 #else // !ENABLE(JIT)</span>
555 
556 namespace JSC {
557 
558 static ExecutableAllocator* executableAllocator;
559 
<span class="line-modified">560 void ExecutableAllocator::initializeAllocator()</span>
561 {
562     executableAllocator = new ExecutableAllocator;
563 }
564 
565 ExecutableAllocator&amp; ExecutableAllocator::singleton()
566 {
567     ASSERT(executableAllocator);
568     return *executableAllocator;
569 }
570 
571 } // namespace JSC
<span class="line-removed">572 </span>
<span class="line-removed">573 #endif // ENABLE(JIT)</span>
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;ExecutableAllocator.h&quot;
 28 
 29 #if ENABLE(JIT)
 30 
 31 #include &quot;CodeProfiling.h&quot;
 32 #include &quot;ExecutableAllocationFuzz.h&quot;
 33 #include &quot;JSCInlines.h&quot;
<span class="line-added"> 34 #include &lt;wtf/FileSystem.h&gt;</span>
 35 #include &lt;wtf/MetaAllocator.h&gt;
 36 #include &lt;wtf/PageReservation.h&gt;
<span class="line-added"> 37 #include &lt;wtf/ProcessID.h&gt;</span>
<span class="line-added"> 38 #include &lt;wtf/SystemTracing.h&gt;</span>
<span class="line-added"> 39 #include &lt;wtf/WorkQueue.h&gt;</span>
 40 
 41 #if OS(DARWIN)
<span class="line-added"> 42 #include &lt;mach/mach_time.h&gt;</span>
 43 #include &lt;sys/mman.h&gt;
 44 #endif
 45 
 46 #if PLATFORM(IOS_FAMILY)
 47 #include &lt;wtf/cocoa/Entitlements.h&gt;
 48 #endif
 49 
 50 #include &quot;LinkBuffer.h&quot;
 51 #include &quot;MacroAssembler.h&quot;
 52 
 53 #if PLATFORM(COCOA)
 54 #define HAVE_REMAP_JIT 1
 55 #endif
 56 
 57 #if HAVE(REMAP_JIT)
 58 #if CPU(ARM64) &amp;&amp; PLATFORM(IOS_FAMILY)
 59 #define USE_EXECUTE_ONLY_JIT_WRITE_FUNCTION 1
 60 #endif
 61 #endif
 62 
</pre>
<hr />
<pre>
103 static const size_t fixedExecutableMemoryPoolSize = 32 * 1024 * 1024;
104 #endif
105 
106 #if CPU(ARM)
107 static const double executablePoolReservationFraction = 0.15;
108 #else
109 static const double executablePoolReservationFraction = 0.25;
110 #endif
111 
112 #if ENABLE(SEPARATED_WX_HEAP)
113 JS_EXPORT_PRIVATE bool useFastPermisionsJITCopy { false };
114 JS_EXPORT_PRIVATE JITWriteSeparateHeapsFunction jitWriteSeparateHeapsFunction;
115 #endif
116 
117 #if !USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION) &amp;&amp; HAVE(REMAP_JIT)
118 static uintptr_t startOfFixedWritableMemoryPool;
119 #endif
120 
121 class FixedVMPoolExecutableAllocator;
122 static FixedVMPoolExecutableAllocator* allocator = nullptr;

123 
124 static bool s_isJITEnabled = true;
125 static bool isJITEnabled()
126 {
127 #if PLATFORM(IOS_FAMILY) &amp;&amp; (CPU(ARM64) || CPU(ARM))
128     return processHasEntitlement(&quot;dynamic-codesigning&quot;) &amp;&amp; s_isJITEnabled;
129 #else
130     return s_isJITEnabled;
131 #endif
132 }
133 
134 void ExecutableAllocator::setJITEnabled(bool enabled)
135 {
136     ASSERT(!allocator);
137     if (s_isJITEnabled == enabled)
138         return;
139 
140     s_isJITEnabled = enabled;
141 
142 #if PLATFORM(IOS_FAMILY) &amp;&amp; (CPU(ARM64) || CPU(ARM))
143     if (!enabled) {
144         constexpr size_t size = 1;
145         constexpr int protection = PROT_READ | PROT_WRITE | PROT_EXEC;
146         constexpr int flags = MAP_PRIVATE | MAP_ANON | MAP_JIT;
147         constexpr int fd = OSAllocator::JSJITCodePages;
148         void* allocation = mmap(nullptr, size, protection, flags, fd, 0);
149         const void* executableMemoryAllocationFailure = reinterpret_cast&lt;void*&gt;(-1);
150         RELEASE_ASSERT_WITH_MESSAGE(allocation &amp;&amp; allocation != executableMemoryAllocationFailure, &quot;We should not have allocated executable memory before disabling the JIT.&quot;);
151         RELEASE_ASSERT_WITH_MESSAGE(!munmap(allocation, size), &quot;Unmapping executable memory should succeed so we do not have any executable memory in the address space&quot;);
152         RELEASE_ASSERT_WITH_MESSAGE(mmap(nullptr, size, protection, flags, fd, 0) == executableMemoryAllocationFailure, &quot;Allocating executable memory should fail after setJITEnabled(false) is called.&quot;);
153     }
154 #endif
155 }
156 
<span class="line-modified">157 class FixedVMPoolExecutableAllocator final : public MetaAllocator {</span>
158     WTF_MAKE_FAST_ALLOCATED;
159 public:
160     FixedVMPoolExecutableAllocator()
161         : MetaAllocator(jitAllocationGranule) // round up all allocations to 32 bytes
162     {
163         if (!isJITEnabled())
164             return;
165 
166         size_t reservationSize;
167         if (Options::jitMemoryReservationSize())
168             reservationSize = Options::jitMemoryReservationSize();
169         else
170             reservationSize = fixedExecutableMemoryPoolSize;
171         reservationSize = std::max(roundUpToMultipleOf(pageSize(), reservationSize), pageSize() * 2);
172 
173         auto tryCreatePageReservation = [] (size_t reservationSize) {
174 #if OS(LINUX)
175             // If we use uncommitted reservation, mmap operation is recorded with small page size in perf command&#39;s output.
176             // This makes the following JIT code logging broken and some of JIT code is not recorded correctly.
177             // To avoid this problem, we use committed reservation if we need perf JITDump logging.
</pre>
<hr />
<pre>
191             os_thread_self_restrict_rwx_to_rx();
192 
193 #else // not ENABLE(FAST_JIT_PERMISSIONS) or ENABLE(SEPARATED_WX_HEAP)
194 #if ENABLE(FAST_JIT_PERMISSIONS)
195             if (os_thread_self_restrict_rwx_is_supported()) {
196                 useFastPermisionsJITCopy = true;
197                 os_thread_self_restrict_rwx_to_rx();
198             } else
199 #endif
200             if (Options::useSeparatedWXHeap()) {
201                 // First page of our JIT allocation is reserved.
202                 ASSERT(reservationSize &gt;= pageSize() * 2);
203                 reservationBase = (void*)((uintptr_t)reservationBase + pageSize());
204                 reservationSize -= pageSize();
205                 initializeSeparatedWXHeaps(m_reservation.base(), pageSize(), reservationBase, reservationSize);
206             }
207 #endif // not ENABLE(FAST_JIT_PERMISSIONS) or ENABLE(SEPARATED_WX_HEAP)
208 
209             addFreshFreeSpace(reservationBase, reservationSize);
210 
<span class="line-added">211             ASSERT(bytesReserved() == reservationSize); // Since our executable memory is fixed-sized, bytesReserved is never changed after initialization.</span>
<span class="line-added">212 </span>
213             void* reservationEnd = reinterpret_cast&lt;uint8_t*&gt;(reservationBase) + reservationSize;
214 
215             m_memoryStart = MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt;(tagCodePtr&lt;ExecutableMemoryPtrTag&gt;(reservationBase));
216             m_memoryEnd = MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt;(tagCodePtr&lt;ExecutableMemoryPtrTag&gt;(reservationEnd));
217         }
218     }
219 
220     virtual ~FixedVMPoolExecutableAllocator();
221 
222     void* memoryStart() { return m_memoryStart.untaggedExecutableAddress(); }
223     void* memoryEnd() { return m_memoryEnd.untaggedExecutableAddress(); }
224     bool isJITPC(void* pc) { return memoryStart() &lt;= pc &amp;&amp; pc &lt; memoryEnd(); }
225 
226 protected:
227     FreeSpacePtr allocateNewSpace(size_t&amp;) override
228     {
229         // We&#39;re operating in a fixed pool, so new allocation is always prohibited.
230         return nullptr;
231     }
232 
<span class="line-modified">233     void notifyNeedPage(void* page, size_t count) override</span>
234     {
235 #if USE(MADV_FREE_FOR_JIT_MEMORY)
236         UNUSED_PARAM(page);
<span class="line-added">237         UNUSED_PARAM(count);</span>
238 #else
<span class="line-modified">239         m_reservation.commit(page, pageSize() * count);</span>
240 #endif
241     }
242 
<span class="line-modified">243     void notifyPageIsFree(void* page, size_t count) override</span>
244     {
245 #if USE(MADV_FREE_FOR_JIT_MEMORY)
246         for (;;) {
<span class="line-modified">247             int result = madvise(page, pageSize() * count, MADV_FREE);</span>
248             if (!result)
249                 return;
250             ASSERT(result == -1);
251             if (errno != EAGAIN) {
252                 RELEASE_ASSERT_NOT_REACHED(); // In debug mode, this should be a hard failure.
253                 break; // In release mode, we should just ignore the error - not returning memory to the OS is better than crashing, especially since we _will_ be able to reuse the memory internally anyway.
254             }
255         }
256 #else
<span class="line-modified">257         m_reservation.decommit(page, pageSize() * count);</span>
258 #endif
259     }
260 
261 private:
262 #if OS(DARWIN) &amp;&amp; HAVE(REMAP_JIT)
263     void initializeSeparatedWXHeaps(void* stubBase, size_t stubSize, void* jitBase, size_t jitSize)
264     {
265         mach_vm_address_t writableAddr = 0;
266 
267         // Create a second mapping of the JIT region at a random address.
268         vm_prot_t cur, max;
269         int remapFlags = VM_FLAGS_ANYWHERE;
270 #if defined(VM_FLAGS_RANDOM_ADDR)
271         remapFlags |= VM_FLAGS_RANDOM_ADDR;
272 #endif
273         kern_return_t ret = mach_vm_remap(mach_task_self(), &amp;writableAddr, jitSize, 0,
274             remapFlags,
275             mach_task_self(), (mach_vm_address_t)jitBase, FALSE,
276             &amp;cur, &amp;max, VM_INHERIT_DEFAULT);
277 
</pre>
<hr />
<pre>
394         functionAsInt -= 1;
395         function = reinterpret_cast&lt;void*&gt;(functionAsInt);
396 #endif
397         auto codePtr = MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt;(tagCFunctionPtr&lt;JITThunkPtrTag&gt;(function));
398         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(codePtr);
399     }
400 #endif // CPU(ARM64) &amp;&amp; USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
401 
402 #else // OS(DARWIN) &amp;&amp; HAVE(REMAP_JIT)
403     void initializeSeparatedWXHeaps(void*, size_t, void*, size_t)
404     {
405     }
406 #endif
407 
408 private:
409     PageReservation m_reservation;
410     MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt; m_memoryStart;
411     MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt; m_memoryEnd;
412 };
413 
<span class="line-modified">414 FixedVMPoolExecutableAllocator::~FixedVMPoolExecutableAllocator()</span>





















415 {
<span class="line-added">416     m_reservation.deallocate();</span>
417 }
418 
<span class="line-modified">419 void ExecutableAllocator::initializeUnderlyingAllocator()</span>
420 {
<span class="line-modified">421     ASSERT(!allocator);</span>
<span class="line-added">422     allocator = new FixedVMPoolExecutableAllocator();</span>
<span class="line-added">423     CodeProfiling::notifyAllocator(allocator);</span>
424 }
425 
426 bool ExecutableAllocator::isValid() const
427 {
<span class="line-added">428     if (!allocator)</span>
<span class="line-added">429         return Base::isValid();</span>
430     return !!allocator-&gt;bytesReserved();
431 }
432 
433 bool ExecutableAllocator::underMemoryPressure()
434 {
<span class="line-modified">435     if (!allocator)</span>
<span class="line-modified">436         return Base::underMemoryPressure();</span>
<span class="line-added">437     return allocator-&gt;bytesAllocated() &gt; allocator-&gt;bytesReserved() / 2;</span>
438 }
439 
440 double ExecutableAllocator::memoryPressureMultiplier(size_t addedMemoryUsage)
441 {
<span class="line-modified">442     if (!allocator)</span>
<span class="line-modified">443         return Base::memoryPressureMultiplier(addedMemoryUsage);</span>
<span class="line-modified">444     ASSERT(allocator-&gt;bytesAllocated() &lt;= allocator-&gt;bytesReserved());</span>
<span class="line-added">445     size_t bytesAllocated = allocator-&gt;bytesAllocated() + addedMemoryUsage;</span>
446     size_t bytesAvailable = static_cast&lt;size_t&gt;(
<span class="line-modified">447         allocator-&gt;bytesReserved() * (1 - executablePoolReservationFraction));</span>
448     if (bytesAllocated &gt;= bytesAvailable)
449         bytesAllocated = bytesAvailable;
450     double result = 1.0;
451     size_t divisor = bytesAvailable - bytesAllocated;
452     if (divisor)
453         result = static_cast&lt;double&gt;(bytesAvailable) / divisor;
454     if (result &lt; 1.0)
455         result = 1.0;
456     return result;
457 }
458 
459 RefPtr&lt;ExecutableMemoryHandle&gt; ExecutableAllocator::allocate(size_t sizeInBytes, void* ownerUID, JITCompilationEffort effort)
460 {
<span class="line-added">461     if (!allocator)</span>
<span class="line-added">462         return Base::allocate(sizeInBytes, ownerUID, effort);</span>
463     if (Options::logExecutableAllocation()) {
464         MetaAllocator::Statistics stats = allocator-&gt;currentStatistics();
465         dataLog(&quot;Allocating &quot;, sizeInBytes, &quot; bytes of executable memory with &quot;, stats.bytesAllocated, &quot; bytes allocated, &quot;, stats.bytesReserved, &quot; bytes reserved, and &quot;, stats.bytesCommitted, &quot; committed.\n&quot;);
466     }
467 
468     if (effort != JITCompilationCanFail &amp;&amp; Options::reportMustSucceedExecutableAllocations()) {
469         dataLog(&quot;Allocating &quot;, sizeInBytes, &quot; bytes of executable memory with JITCompilationMustSucceed.\n&quot;);
470         WTFReportBacktrace();
471     }
472 
473     if (effort == JITCompilationCanFail
474         &amp;&amp; doExecutableAllocationFuzzingIfEnabled() == PretendToFailExecutableAllocation)
475         return nullptr;
476 
477     if (effort == JITCompilationCanFail) {
478         // Don&#39;t allow allocations if we are down to reserve.
<span class="line-modified">479         size_t bytesAllocated = allocator-&gt;bytesAllocated() + sizeInBytes;</span>

480         size_t bytesAvailable = static_cast&lt;size_t&gt;(
<span class="line-modified">481             allocator-&gt;bytesReserved() * (1 - executablePoolReservationFraction));</span>
482         if (bytesAllocated &gt; bytesAvailable) {
483             if (Options::logExecutableAllocation())
484                 dataLog(&quot;Allocation failed because bytes allocated &quot;, bytesAllocated,  &quot; &gt; &quot;, bytesAvailable, &quot; bytes available.\n&quot;);
485             return nullptr;
486         }
487     }
488 
489     RefPtr&lt;ExecutableMemoryHandle&gt; result = allocator-&gt;allocate(sizeInBytes, ownerUID);
490     if (!result) {
491         if (effort != JITCompilationCanFail) {
492             dataLog(&quot;Ran out of executable memory while allocating &quot;, sizeInBytes, &quot; bytes.\n&quot;);
493             CRASH();
494         }
495         return nullptr;
496     }
497 
<span class="line-modified">498 #if CPU(ARM64E)</span>
499     void* start = allocator-&gt;memoryStart();
500     void* end = allocator-&gt;memoryEnd();
501     void* resultStart = result-&gt;start().untaggedPtr();
502     void* resultEnd = result-&gt;end().untaggedPtr();
503     RELEASE_ASSERT(start &lt;= resultStart &amp;&amp; resultStart &lt; end);
504     RELEASE_ASSERT(start &lt; resultEnd &amp;&amp; resultEnd &lt;= end);
505 #endif
506     return result;
507 }
508 
509 bool ExecutableAllocator::isValidExecutableMemory(const AbstractLocker&amp; locker, void* address)
510 {
<span class="line-added">511     if (!allocator)</span>
<span class="line-added">512         return Base::isValidExecutableMemory(locker, address);</span>
513     return allocator-&gt;isInAllocatedMemory(locker, address);
514 }
515 
516 Lock&amp; ExecutableAllocator::getLock() const
517 {
<span class="line-added">518     if (!allocator)</span>
<span class="line-added">519         return Base::getLock();</span>
520     return allocator-&gt;getLock();
521 }
522 
523 size_t ExecutableAllocator::committedByteCount()
524 {
<span class="line-added">525     if (!allocator)</span>
<span class="line-added">526         return Base::committedByteCount();</span>
527     return allocator-&gt;bytesCommitted();
528 }
529 
530 #if ENABLE(META_ALLOCATOR_PROFILE)
531 void ExecutableAllocator::dumpProfile()
532 {
<span class="line-added">533     if (!allocator)</span>
<span class="line-added">534         return;</span>
535     allocator-&gt;dumpProfile();
536 }
537 #endif
538 
539 void* startOfFixedExecutableMemoryPoolImpl()
540 {
<span class="line-added">541     if (!allocator)</span>
<span class="line-added">542         return nullptr;</span>
543     return allocator-&gt;memoryStart();
544 }
545 
546 void* endOfFixedExecutableMemoryPoolImpl()
547 {
<span class="line-added">548     if (!allocator)</span>
<span class="line-added">549         return nullptr;</span>
550     return allocator-&gt;memoryEnd();
551 }
552 
553 bool isJITPC(void* pc)
554 {
555     return allocator &amp;&amp; allocator-&gt;isJITPC(pc);
556 }
557 
<span class="line-added">558 void dumpJITMemory(const void* dst, const void* src, size_t size)</span>
<span class="line-added">559 {</span>
<span class="line-added">560     ASSERT(Options::dumpJITMemoryPath());</span>
<span class="line-added">561 </span>
<span class="line-added">562 #if OS(DARWIN)</span>
<span class="line-added">563     static int fd = -1;</span>
<span class="line-added">564     static uint8_t* buffer;</span>
<span class="line-added">565     static constexpr size_t bufferSize = fixedExecutableMemoryPoolSize;</span>
<span class="line-added">566     static size_t offset = 0;</span>
<span class="line-added">567     static Lock dumpJITMemoryLock;</span>
<span class="line-added">568     static bool needsToFlush = false;</span>
<span class="line-added">569     static auto flush = [](const AbstractLocker&amp;) {</span>
<span class="line-added">570         if (fd == -1) {</span>
<span class="line-added">571             String path = Options::dumpJITMemoryPath();</span>
<span class="line-added">572             path = path.replace(&quot;%pid&quot;, String::number(getCurrentProcessID()));</span>
<span class="line-added">573             fd = open(FileSystem::fileSystemRepresentation(path).data(), O_CREAT | O_TRUNC | O_APPEND | O_WRONLY | O_EXLOCK | O_NONBLOCK, 0666);</span>
<span class="line-added">574             RELEASE_ASSERT(fd != -1);</span>
<span class="line-added">575         }</span>
<span class="line-added">576         write(fd, buffer, offset);</span>
<span class="line-added">577         offset = 0;</span>
<span class="line-added">578         needsToFlush = false;</span>
<span class="line-added">579     };</span>
<span class="line-added">580 </span>
<span class="line-added">581     static std::once_flag once;</span>
<span class="line-added">582     static LazyNeverDestroyed&lt;Ref&lt;WorkQueue&gt;&gt; flushQueue;</span>
<span class="line-added">583     std::call_once(once, [] {</span>
<span class="line-added">584         buffer = bitwise_cast&lt;uint8_t*&gt;(malloc(bufferSize));</span>
<span class="line-added">585         flushQueue.construct(WorkQueue::create(&quot;jsc.dumpJITMemory.queue&quot;, WorkQueue::Type::Serial, WorkQueue::QOS::Background));</span>
<span class="line-added">586         std::atexit([] {</span>
<span class="line-added">587             LockHolder locker(dumpJITMemoryLock);</span>
<span class="line-added">588             flush(locker);</span>
<span class="line-added">589             close(fd);</span>
<span class="line-added">590             fd = -1;</span>
<span class="line-added">591         });</span>
<span class="line-added">592     });</span>
<span class="line-added">593 </span>
<span class="line-added">594     static auto enqueueFlush = [](const AbstractLocker&amp;) {</span>
<span class="line-added">595         if (needsToFlush)</span>
<span class="line-added">596             return;</span>
<span class="line-added">597 </span>
<span class="line-added">598         needsToFlush = true;</span>
<span class="line-added">599         flushQueue.get()-&gt;dispatchAfter(Seconds(Options::dumpJITMemoryFlushInterval()), [] {</span>
<span class="line-added">600             LockHolder locker(dumpJITMemoryLock);</span>
<span class="line-added">601             if (!needsToFlush)</span>
<span class="line-added">602                 return;</span>
<span class="line-added">603             flush(locker);</span>
<span class="line-added">604         });</span>
<span class="line-added">605     };</span>
<span class="line-added">606 </span>
<span class="line-added">607     static auto write = [](const AbstractLocker&amp; locker, const void* src, size_t size) {</span>
<span class="line-added">608         if (UNLIKELY(offset + size &gt; bufferSize))</span>
<span class="line-added">609             flush(locker);</span>
<span class="line-added">610         memcpy(buffer + offset, src, size);</span>
<span class="line-added">611         offset += size;</span>
<span class="line-added">612         enqueueFlush(locker);</span>
<span class="line-added">613     };</span>
<span class="line-added">614 </span>
<span class="line-added">615     LockHolder locker(dumpJITMemoryLock);</span>
<span class="line-added">616     uint64_t time = mach_absolute_time();</span>
<span class="line-added">617     uint64_t dst64 = bitwise_cast&lt;uintptr_t&gt;(dst);</span>
<span class="line-added">618     uint64_t size64 = size;</span>
<span class="line-added">619     TraceScope(DumpJITMemoryStart, DumpJITMemoryStop, time, dst64, size64);</span>
<span class="line-added">620     write(locker, &amp;time, sizeof(time));</span>
<span class="line-added">621     write(locker, &amp;dst64, sizeof(dst64));</span>
<span class="line-added">622     write(locker, &amp;size64, sizeof(size64));</span>
<span class="line-added">623     write(locker, src, size);</span>
<span class="line-added">624 #else</span>
<span class="line-added">625     UNUSED_PARAM(dst);</span>
<span class="line-added">626     UNUSED_PARAM(src);</span>
<span class="line-added">627     UNUSED_PARAM(size);</span>
<span class="line-added">628     RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-added">629 #endif</span>
<span class="line-added">630 }</span>
<span class="line-added">631 </span>
632 } // namespace JSC
633 
<span class="line-modified">634 #endif // ENABLE(JIT)</span>
635 
636 namespace JSC {
637 
638 static ExecutableAllocator* executableAllocator;
639 
<span class="line-modified">640 void ExecutableAllocator::initialize()</span>
641 {
642     executableAllocator = new ExecutableAllocator;
643 }
644 
645 ExecutableAllocator&amp; ExecutableAllocator::singleton()
646 {
647     ASSERT(executableAllocator);
648     return *executableAllocator;
649 }
650 
651 } // namespace JSC


</pre>
</td>
</tr>
</table>
<center><a href="CallFrameShuffler.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="ExecutableAllocator.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>