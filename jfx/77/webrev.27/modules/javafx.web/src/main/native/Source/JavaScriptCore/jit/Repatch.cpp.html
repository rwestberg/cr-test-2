<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/Repatch.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2011-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;Repatch.h&quot;
  28 
  29 #if ENABLE(JIT)
  30 
  31 #include &quot;BinarySwitch.h&quot;
  32 #include &quot;CCallHelpers.h&quot;
  33 #include &quot;CallFrameShuffler.h&quot;
  34 #include &quot;DFGOperations.h&quot;
  35 #include &quot;DFGSpeculativeJIT.h&quot;
  36 #include &quot;DOMJITGetterSetter.h&quot;
  37 #include &quot;DirectArguments.h&quot;
  38 #include &quot;ExecutableBaseInlines.h&quot;
  39 #include &quot;FTLThunks.h&quot;
  40 #include &quot;FullCodeOrigin.h&quot;
  41 #include &quot;FunctionCodeBlock.h&quot;
  42 #include &quot;GCAwareJITStubRoutine.h&quot;
  43 #include &quot;GetterSetter.h&quot;
  44 #include &quot;GetterSetterAccessCase.h&quot;
  45 #include &quot;ICStats.h&quot;
  46 #include &quot;InlineAccess.h&quot;
  47 #include &quot;InstanceOfAccessCase.h&quot;
  48 #include &quot;IntrinsicGetterAccessCase.h&quot;
  49 #include &quot;JIT.h&quot;
  50 #include &quot;JITInlines.h&quot;
  51 #include &quot;JSCInlines.h&quot;
  52 #include &quot;JSModuleNamespaceObject.h&quot;
  53 #include &quot;JSWebAssembly.h&quot;
  54 #include &quot;JSWebAssemblyModule.h&quot;
  55 #include &quot;LinkBuffer.h&quot;
  56 #include &quot;ModuleNamespaceAccessCase.h&quot;
  57 #include &quot;PolymorphicAccess.h&quot;
  58 #include &quot;ScopedArguments.h&quot;
  59 #include &quot;ScratchRegisterAllocator.h&quot;
  60 #include &quot;StackAlignment.h&quot;
  61 #include &quot;StructureRareDataInlines.h&quot;
  62 #include &quot;StructureStubClearingWatchpoint.h&quot;
  63 #include &quot;StructureStubInfo.h&quot;
  64 #include &quot;SuperSampler.h&quot;
  65 #include &quot;ThunkGenerators.h&quot;
  66 #include &quot;WebAssemblyFunction.h&quot;
  67 #include &quot;WebAssemblyToJSCallee.h&quot;
  68 #include &lt;wtf/CommaPrinter.h&gt;
  69 #include &lt;wtf/ListDump.h&gt;
  70 #include &lt;wtf/StringPrintStream.h&gt;
  71 
  72 namespace JSC {
  73 
  74 static FunctionPtr&lt;CFunctionPtrTag&gt; readPutICCallTarget(CodeBlock* codeBlock, CodeLocationCall&lt;JSInternalPtrTag&gt; call)
  75 {
  76     FunctionPtr&lt;OperationPtrTag&gt; target = MacroAssembler::readCallTarget&lt;OperationPtrTag&gt;(call);
  77 #if ENABLE(FTL_JIT)
  78     if (codeBlock-&gt;jitType() == JITType::FTLJIT) {
  79         MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt; thunk = MacroAssemblerCodePtr&lt;OperationPtrTag&gt;::createFromExecutableAddress(target.executableAddress()).retagged&lt;JITThunkPtrTag&gt;();
  80         return codeBlock-&gt;vm().ftlThunks-&gt;keyForSlowPathCallThunk(thunk).callTarget().retagged&lt;CFunctionPtrTag&gt;();
  81     }
  82 #else
  83     UNUSED_PARAM(codeBlock);
  84 #endif // ENABLE(FTL_JIT)
  85     return target.retagged&lt;CFunctionPtrTag&gt;();
  86 }
  87 
  88 void ftlThunkAwareRepatchCall(CodeBlock* codeBlock, CodeLocationCall&lt;JSInternalPtrTag&gt; call, FunctionPtr&lt;CFunctionPtrTag&gt; newCalleeFunction)
  89 {
  90 #if ENABLE(FTL_JIT)
  91     if (codeBlock-&gt;jitType() == JITType::FTLJIT) {
  92         VM&amp; vm = codeBlock-&gt;vm();
  93         FTL::Thunks&amp; thunks = *vm.ftlThunks;
  94         FunctionPtr&lt;OperationPtrTag&gt; target = MacroAssembler::readCallTarget&lt;OperationPtrTag&gt;(call);
  95         auto slowPathThunk = MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt;::createFromExecutableAddress(target.retaggedExecutableAddress&lt;JITThunkPtrTag&gt;());
  96         FTL::SlowPathCallKey key = thunks.keyForSlowPathCallThunk(slowPathThunk);
  97         key = key.withCallTarget(newCalleeFunction);
  98         MacroAssembler::repatchCall(call, FunctionPtr&lt;OperationPtrTag&gt;(thunks.getSlowPathCallThunk(key).retaggedCode&lt;OperationPtrTag&gt;()));
  99         return;
 100     }
 101 #else // ENABLE(FTL_JIT)
 102     UNUSED_PARAM(codeBlock);
 103 #endif // ENABLE(FTL_JIT)
 104     MacroAssembler::repatchCall(call, newCalleeFunction.retagged&lt;OperationPtrTag&gt;());
 105 }
 106 
 107 enum InlineCacheAction {
 108     GiveUpOnCache,
 109     RetryCacheLater,
 110     AttemptToCache
 111 };
 112 
 113 static InlineCacheAction actionForCell(VM&amp; vm, JSCell* cell)
 114 {
 115     Structure* structure = cell-&gt;structure(vm);
 116 
 117     TypeInfo typeInfo = structure-&gt;typeInfo();
 118     if (typeInfo.prohibitsPropertyCaching())
 119         return GiveUpOnCache;
 120 
 121     if (structure-&gt;isUncacheableDictionary()) {
 122         if (structure-&gt;hasBeenFlattenedBefore())
 123             return GiveUpOnCache;
 124         // Flattening could have changed the offset, so return early for another try.
 125         asObject(cell)-&gt;flattenDictionaryObject(vm);
 126         return RetryCacheLater;
 127     }
 128 
 129     if (!structure-&gt;propertyAccessesAreCacheable())
 130         return GiveUpOnCache;
 131 
 132     return AttemptToCache;
 133 }
 134 
 135 static bool forceICFailure(ExecState*)
 136 {
 137     return Options::forceICFailure();
 138 }
 139 
 140 ALWAYS_INLINE static void fireWatchpointsAndClearStubIfNeeded(VM&amp; vm, StructureStubInfo&amp; stubInfo, CodeBlock* codeBlock, AccessGenerationResult&amp; result)
 141 {
 142     if (result.shouldResetStubAndFireWatchpoints()) {
 143         result.fireWatchpoints(vm);
 144         stubInfo.reset(codeBlock);
 145     }
 146 }
 147 
 148 inline FunctionPtr&lt;CFunctionPtrTag&gt; appropriateOptimizingGetByIdFunction(GetByIDKind kind)
 149 {
 150     switch (kind) {
 151     case GetByIDKind::Normal:
 152         return operationGetByIdOptimize;
 153     case GetByIDKind::WithThis:
 154         return operationGetByIdWithThisOptimize;
 155     case GetByIDKind::Try:
 156         return operationTryGetByIdOptimize;
 157     case GetByIDKind::Direct:
 158         return operationGetByIdDirectOptimize;
 159     }
 160     ASSERT_NOT_REACHED();
 161     return operationGetById;
 162 }
 163 
 164 inline FunctionPtr&lt;CFunctionPtrTag&gt; appropriateGetByIdFunction(GetByIDKind kind)
 165 {
 166     switch (kind) {
 167     case GetByIDKind::Normal:
 168         return operationGetById;
 169     case GetByIDKind::WithThis:
 170         return operationGetByIdWithThis;
 171     case GetByIDKind::Try:
 172         return operationTryGetById;
 173     case GetByIDKind::Direct:
 174         return operationGetByIdDirect;
 175     }
 176     ASSERT_NOT_REACHED();
 177     return operationGetById;
 178 }
 179 
 180 static InlineCacheAction tryCacheGetByID(ExecState* exec, JSValue baseValue, const Identifier&amp; propertyName, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, GetByIDKind kind)
 181 {
 182     VM&amp; vm = exec-&gt;vm();
 183     AccessGenerationResult result;
 184 
 185     {
 186         GCSafeConcurrentJSLocker locker(exec-&gt;codeBlock()-&gt;m_lock, exec-&gt;vm().heap);
 187 
 188         if (forceICFailure(exec))
 189             return GiveUpOnCache;
 190 
 191         // FIXME: Cache property access for immediates.
 192         if (!baseValue.isCell())
 193             return GiveUpOnCache;
 194         JSCell* baseCell = baseValue.asCell();
 195 
 196         CodeBlock* codeBlock = exec-&gt;codeBlock();
 197 
 198         std::unique_ptr&lt;AccessCase&gt; newCase;
 199 
 200         if (propertyName == vm.propertyNames-&gt;length) {
 201             if (isJSArray(baseCell)) {
 202                 if (stubInfo.cacheType == CacheType::Unset
 203                     &amp;&amp; slot.slotBase() == baseCell
 204                     &amp;&amp; InlineAccess::isCacheableArrayLength(stubInfo, jsCast&lt;JSArray*&gt;(baseCell))) {
 205 
 206                     bool generatedCodeInline = InlineAccess::generateArrayLength(stubInfo, jsCast&lt;JSArray*&gt;(baseCell));
 207                     if (generatedCodeInline) {
 208                         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
 209                         stubInfo.initArrayLength();
 210                         return RetryCacheLater;
 211                     }
 212                 }
 213 
 214                 newCase = AccessCase::create(vm, codeBlock, AccessCase::ArrayLength);
 215             } else if (isJSString(baseCell)) {
 216                 if (stubInfo.cacheType == CacheType::Unset &amp;&amp; InlineAccess::isCacheableStringLength(stubInfo)) {
 217                     bool generatedCodeInline = InlineAccess::generateStringLength(stubInfo);
 218                     if (generatedCodeInline) {
 219                         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
 220                         stubInfo.initStringLength();
 221                         return RetryCacheLater;
 222                     }
 223                 }
 224 
 225                 newCase = AccessCase::create(vm, codeBlock, AccessCase::StringLength);
 226             }
 227             else if (DirectArguments* arguments = jsDynamicCast&lt;DirectArguments*&gt;(vm, baseCell)) {
 228                 // If there were overrides, then we can handle this as a normal property load! Guarding
 229                 // this with such a check enables us to add an IC case for that load if needed.
 230                 if (!arguments-&gt;overrodeThings())
 231                     newCase = AccessCase::create(vm, codeBlock, AccessCase::DirectArgumentsLength);
 232             } else if (ScopedArguments* arguments = jsDynamicCast&lt;ScopedArguments*&gt;(vm, baseCell)) {
 233                 // Ditto.
 234                 if (!arguments-&gt;overrodeThings())
 235                     newCase = AccessCase::create(vm, codeBlock, AccessCase::ScopedArgumentsLength);
 236             }
 237         }
 238 
 239         if (!propertyName.isSymbol() &amp;&amp; baseCell-&gt;inherits&lt;JSModuleNamespaceObject&gt;(vm) &amp;&amp; !slot.isUnset()) {
 240             if (auto moduleNamespaceSlot = slot.moduleNamespaceSlot())
 241                 newCase = ModuleNamespaceAccessCase::create(vm, codeBlock, jsCast&lt;JSModuleNamespaceObject*&gt;(baseCell), moduleNamespaceSlot-&gt;environment, ScopeOffset(moduleNamespaceSlot-&gt;scopeOffset));
 242         }
 243 
 244         if (!newCase) {
 245             if (!slot.isCacheable() &amp;&amp; !slot.isUnset())
 246                 return GiveUpOnCache;
 247 
 248             ObjectPropertyConditionSet conditionSet;
 249             Structure* structure = baseCell-&gt;structure(vm);
 250 
 251             bool loadTargetFromProxy = false;
 252             if (baseCell-&gt;type() == PureForwardingProxyType) {
 253                 baseValue = jsCast&lt;JSProxy*&gt;(baseCell)-&gt;target();
 254                 baseCell = baseValue.asCell();
 255                 structure = baseCell-&gt;structure(vm);
 256                 loadTargetFromProxy = true;
 257             }
 258 
 259             InlineCacheAction action = actionForCell(vm, baseCell);
 260             if (action != AttemptToCache)
 261                 return action;
 262 
 263             // Optimize self access.
 264             if (stubInfo.cacheType == CacheType::Unset
 265                 &amp;&amp; slot.isCacheableValue()
 266                 &amp;&amp; slot.slotBase() == baseValue
 267                 &amp;&amp; !slot.watchpointSet()
 268                 &amp;&amp; !structure-&gt;needImpurePropertyWatchpoint()
 269                 &amp;&amp; !loadTargetFromProxy) {
 270 
 271                 bool generatedCodeInline = InlineAccess::generateSelfPropertyAccess(stubInfo, structure, slot.cachedOffset());
 272                 if (generatedCodeInline) {
 273                     LOG_IC((ICEvent::GetByIdSelfPatch, structure-&gt;classInfo(), propertyName, slot.slotBase() == baseValue));
 274                     structure-&gt;startWatchingPropertyForReplacements(vm, slot.cachedOffset());
 275                     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
 276                     stubInfo.initGetByIdSelf(codeBlock, structure, slot.cachedOffset());
 277                     return RetryCacheLater;
 278                 }
 279             }
 280 
 281             std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 282 
 283             PropertyOffset offset = slot.isUnset() ? invalidOffset : slot.cachedOffset();
 284 
 285             if (slot.isUnset() || slot.slotBase() != baseValue) {
 286                 if (structure-&gt;typeInfo().prohibitsPropertyCaching())
 287                     return GiveUpOnCache;
 288 
 289                 if (structure-&gt;isDictionary()) {
 290                     if (structure-&gt;hasBeenFlattenedBefore())
 291                         return GiveUpOnCache;
 292                     structure-&gt;flattenDictionaryStructure(vm, jsCast&lt;JSObject*&gt;(baseCell));
 293                 }
 294 
 295                 if (slot.isUnset() &amp;&amp; structure-&gt;typeInfo().getOwnPropertySlotIsImpureForPropertyAbsence())
 296                     return GiveUpOnCache;
 297 
 298                 // If a kind is GetByIDKind::Direct, we do not need to investigate prototype chains further.
 299                 // Cacheability just depends on the head structure.
 300                 if (kind != GetByIDKind::Direct) {
 301                     bool usesPolyProto;
 302                     prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), baseCell, slot, usesPolyProto);
 303                     if (!prototypeAccessChain) {
 304                         // It&#39;s invalid to access this prototype property.
 305                         return GiveUpOnCache;
 306                     }
 307 
 308                     if (!usesPolyProto) {
 309                         // We use ObjectPropertyConditionSet instead for faster accesses.
 310                         prototypeAccessChain = nullptr;
 311 
 312                         // FIXME: Maybe this `if` should be inside generateConditionsForPropertyBlah.
 313                         // https://bugs.webkit.org/show_bug.cgi?id=185215
 314                         if (slot.isUnset()) {
 315                             conditionSet = generateConditionsForPropertyMiss(
 316                                 vm, codeBlock, exec, structure, propertyName.impl());
 317                         } else if (!slot.isCacheableCustom()) {
 318                             conditionSet = generateConditionsForPrototypePropertyHit(
 319                                 vm, codeBlock, exec, structure, slot.slotBase(),
 320                                 propertyName.impl());
 321                         } else {
 322                             conditionSet = generateConditionsForPrototypePropertyHitCustom(
 323                                 vm, codeBlock, exec, structure, slot.slotBase(),
 324                                 propertyName.impl());
 325                         }
 326 
 327                         if (!conditionSet.isValid())
 328                             return GiveUpOnCache;
 329                     }
 330                 }
 331 
 332                 offset = slot.isUnset() ? invalidOffset : slot.cachedOffset();
 333             }
 334 
 335             JSFunction* getter = nullptr;
 336             if (slot.isCacheableGetter())
 337                 getter = jsDynamicCast&lt;JSFunction*&gt;(vm, slot.getterSetter()-&gt;getter());
 338 
 339             Optional&lt;DOMAttributeAnnotation&gt; domAttribute;
 340             if (slot.isCacheableCustom() &amp;&amp; slot.domAttribute())
 341                 domAttribute = slot.domAttribute();
 342 
 343             if (kind == GetByIDKind::Try) {
 344                 AccessCase::AccessType type;
 345                 if (slot.isCacheableValue())
 346                     type = AccessCase::Load;
 347                 else if (slot.isUnset())
 348                     type = AccessCase::Miss;
 349                 else if (slot.isCacheableGetter())
 350                     type = AccessCase::GetGetter;
 351                 else
 352                     RELEASE_ASSERT_NOT_REACHED();
 353 
 354                 newCase = ProxyableAccessCase::create(vm, codeBlock, type, offset, structure, conditionSet, loadTargetFromProxy, slot.watchpointSet(), WTFMove(prototypeAccessChain));
 355             } else if (!loadTargetFromProxy &amp;&amp; getter &amp;&amp; IntrinsicGetterAccessCase::canEmitIntrinsicGetter(getter, structure))
 356                 newCase = IntrinsicGetterAccessCase::create(vm, codeBlock, slot.cachedOffset(), structure, conditionSet, getter, WTFMove(prototypeAccessChain));
 357             else {
 358                 if (slot.isCacheableValue() || slot.isUnset()) {
 359                     newCase = ProxyableAccessCase::create(vm, codeBlock, slot.isUnset() ? AccessCase::Miss : AccessCase::Load,
 360                         offset, structure, conditionSet, loadTargetFromProxy, slot.watchpointSet(), WTFMove(prototypeAccessChain));
 361                 } else {
 362                     AccessCase::AccessType type;
 363                     if (slot.isCacheableGetter())
 364                         type = AccessCase::Getter;
 365                     else if (slot.attributes() &amp; PropertyAttribute::CustomAccessor)
 366                         type = AccessCase::CustomAccessorGetter;
 367                     else
 368                         type = AccessCase::CustomValueGetter;
 369 
 370                     if (kind == GetByIDKind::WithThis &amp;&amp; type == AccessCase::CustomAccessorGetter &amp;&amp; domAttribute)
 371                         return GiveUpOnCache;
 372 
 373                     newCase = GetterSetterAccessCase::create(
 374                         vm, codeBlock, type, offset, structure, conditionSet, loadTargetFromProxy,
 375                         slot.watchpointSet(), slot.isCacheableCustom() ? slot.customGetter() : nullptr,
 376                         slot.isCacheableCustom() &amp;&amp; slot.slotBase() != baseValue ? slot.slotBase() : nullptr,
 377                         domAttribute, WTFMove(prototypeAccessChain));
 378                 }
 379             }
 380         }
 381 
 382         LOG_IC((ICEvent::GetByIdAddAccessCase, baseValue.classInfoOrNull(vm), propertyName, slot.slotBase() == baseValue));
 383 
 384         result = stubInfo.addAccessCase(locker, codeBlock, propertyName, WTFMove(newCase));
 385 
 386         if (result.generatedSomeCode()) {
 387             LOG_IC((ICEvent::GetByIdReplaceWithJump, baseValue.classInfoOrNull(vm), propertyName, slot.slotBase() == baseValue));
 388 
 389             RELEASE_ASSERT(result.code());
 390             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 391         }
 392     }
 393 
 394     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec-&gt;codeBlock(), result);
 395 
 396     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 397 }
 398 
 399 void repatchGetByID(ExecState* exec, JSValue baseValue, const Identifier&amp; propertyName, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, GetByIDKind kind)
 400 {
 401     SuperSamplerScope superSamplerScope(false);
 402 
 403     if (tryCacheGetByID(exec, baseValue, propertyName, slot, stubInfo, kind) == GiveUpOnCache) {
 404         CodeBlock* codeBlock = exec-&gt;codeBlock();
 405         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateGetByIdFunction(kind));
 406     }
 407 }
 408 
 409 static V_JITOperation_ESsiJJI appropriateGenericPutByIdFunction(const PutPropertySlot &amp;slot, PutKind putKind)
 410 {
 411     if (slot.isStrictMode()) {
 412         if (putKind == Direct)
 413             return operationPutByIdDirectStrict;
 414         return operationPutByIdStrict;
 415     }
 416     if (putKind == Direct)
 417         return operationPutByIdDirectNonStrict;
 418     return operationPutByIdNonStrict;
 419 }
 420 
 421 static V_JITOperation_ESsiJJI appropriateOptimizingPutByIdFunction(const PutPropertySlot &amp;slot, PutKind putKind)
 422 {
 423     if (slot.isStrictMode()) {
 424         if (putKind == Direct)
 425             return operationPutByIdDirectStrictOptimize;
 426         return operationPutByIdStrictOptimize;
 427     }
 428     if (putKind == Direct)
 429         return operationPutByIdDirectNonStrictOptimize;
 430     return operationPutByIdNonStrictOptimize;
 431 }
 432 
 433 static InlineCacheAction tryCachePutByID(ExecState* exec, JSValue baseValue, Structure* structure, const Identifier&amp; ident, const PutPropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, PutKind putKind)
 434 {
 435     VM&amp; vm = exec-&gt;vm();
 436     AccessGenerationResult result;
 437     {
 438         GCSafeConcurrentJSLocker locker(exec-&gt;codeBlock()-&gt;m_lock, exec-&gt;vm().heap);
 439 
 440         if (forceICFailure(exec))
 441             return GiveUpOnCache;
 442 
 443         CodeBlock* codeBlock = exec-&gt;codeBlock();
 444 
 445         if (!baseValue.isCell())
 446             return GiveUpOnCache;
 447 
 448         if (!slot.isCacheablePut() &amp;&amp; !slot.isCacheableCustom() &amp;&amp; !slot.isCacheableSetter())
 449             return GiveUpOnCache;
 450 
 451         // FIXME: We should try to do something smarter here...
 452         if (isCopyOnWrite(structure-&gt;indexingMode()))
 453             return GiveUpOnCache;
 454         // We can&#39;t end up storing to a CoW on the prototype since it shouldn&#39;t own properties.
 455         ASSERT(!isCopyOnWrite(slot.base()-&gt;indexingMode()));
 456 
 457         if (!structure-&gt;propertyAccessesAreCacheable())
 458             return GiveUpOnCache;
 459 
 460         std::unique_ptr&lt;AccessCase&gt; newCase;
 461         JSCell* baseCell = baseValue.asCell();
 462 
 463         if (slot.base() == baseValue &amp;&amp; slot.isCacheablePut()) {
 464             if (slot.type() == PutPropertySlot::ExistingProperty) {
 465                 // This assert helps catch bugs if we accidentally forget to disable caching
 466                 // when we transition then store to an existing property. This is common among
 467                 // paths that reify lazy properties. If we reify a lazy property and forget
 468                 // to disable caching, we may come down this path. The Replace IC does not
 469                 // know how to model these types of structure transitions (or any structure
 470                 // transition for that matter).
 471                 RELEASE_ASSERT(baseValue.asCell()-&gt;structure(vm) == structure);
 472 
 473                 structure-&gt;didCachePropertyReplacement(vm, slot.cachedOffset());
 474 
 475                 if (stubInfo.cacheType == CacheType::Unset
 476                     &amp;&amp; InlineAccess::canGenerateSelfPropertyReplace(stubInfo, slot.cachedOffset())
 477                     &amp;&amp; !structure-&gt;needImpurePropertyWatchpoint()) {
 478 
 479                     bool generatedCodeInline = InlineAccess::generateSelfPropertyReplace(stubInfo, structure, slot.cachedOffset());
 480                     if (generatedCodeInline) {
 481                         LOG_IC((ICEvent::PutByIdSelfPatch, structure-&gt;classInfo(), ident, slot.base() == baseValue));
 482                         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingPutByIdFunction(slot, putKind));
 483                         stubInfo.initPutByIdReplace(codeBlock, structure, slot.cachedOffset());
 484                         return RetryCacheLater;
 485                     }
 486                 }
 487 
 488                 newCase = AccessCase::create(vm, codeBlock, AccessCase::Replace, slot.cachedOffset(), structure);
 489             } else {
 490                 ASSERT(slot.type() == PutPropertySlot::NewProperty);
 491 
 492                 if (!structure-&gt;isObject())
 493                     return GiveUpOnCache;
 494 
 495                 if (structure-&gt;isDictionary()) {
 496                     if (structure-&gt;hasBeenFlattenedBefore())
 497                         return GiveUpOnCache;
 498                     structure-&gt;flattenDictionaryStructure(vm, jsCast&lt;JSObject*&gt;(baseValue));
 499                 }
 500 
 501                 PropertyOffset offset;
 502                 Structure* newStructure =
 503                     Structure::addPropertyTransitionToExistingStructureConcurrently(
 504                         structure, ident.impl(), 0, offset);
 505                 if (!newStructure || !newStructure-&gt;propertyAccessesAreCacheable())
 506                     return GiveUpOnCache;
 507 
 508                 ASSERT(newStructure-&gt;previousID() == structure);
 509                 ASSERT(!newStructure-&gt;isDictionary());
 510                 ASSERT(newStructure-&gt;isObject());
 511 
 512                 std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 513                 ObjectPropertyConditionSet conditionSet;
 514                 if (putKind == NotDirect) {
 515                     bool usesPolyProto;
 516                     prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), baseCell, nullptr, usesPolyProto);
 517                     if (!prototypeAccessChain) {
 518                         // It&#39;s invalid to access this prototype property.
 519                         return GiveUpOnCache;
 520                     }
 521 
 522                     if (!usesPolyProto) {
 523                         prototypeAccessChain = nullptr;
 524                         conditionSet =
 525                             generateConditionsForPropertySetterMiss(
 526                                 vm, codeBlock, exec, newStructure, ident.impl());
 527                         if (!conditionSet.isValid())
 528                             return GiveUpOnCache;
 529                     }
 530 
 531                 }
 532 
 533                 newCase = AccessCase::create(vm, codeBlock, offset, structure, newStructure, conditionSet, WTFMove(prototypeAccessChain));
 534             }
 535         } else if (slot.isCacheableCustom() || slot.isCacheableSetter()) {
 536             if (slot.isCacheableCustom()) {
 537                 ObjectPropertyConditionSet conditionSet;
 538                 std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 539 
 540                 if (slot.base() != baseValue) {
 541                     bool usesPolyProto;
 542                     prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), baseCell, slot.base(), usesPolyProto);
 543                     if (!prototypeAccessChain) {
 544                         // It&#39;s invalid to access this prototype property.
 545                         return GiveUpOnCache;
 546                     }
 547 
 548                     if (!usesPolyProto) {
 549                         prototypeAccessChain = nullptr;
 550                         conditionSet =
 551                             generateConditionsForPrototypePropertyHitCustom(
 552                                 vm, codeBlock, exec, structure, slot.base(), ident.impl());
 553                         if (!conditionSet.isValid())
 554                             return GiveUpOnCache;
 555                     }
 556                 }
 557 
 558                 newCase = GetterSetterAccessCase::create(
 559                     vm, codeBlock, slot.isCustomAccessor() ? AccessCase::CustomAccessorSetter : AccessCase::CustomValueSetter, structure, invalidOffset,
 560                     conditionSet, WTFMove(prototypeAccessChain), slot.customSetter(), slot.base() != baseValue ? slot.base() : nullptr);
 561             } else {
 562                 ObjectPropertyConditionSet conditionSet;
 563                 std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 564                 PropertyOffset offset = slot.cachedOffset();
 565 
 566                 if (slot.base() != baseValue) {
 567                     bool usesPolyProto;
 568                     prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), baseCell, slot.base(), usesPolyProto);
 569                     if (!prototypeAccessChain) {
 570                         // It&#39;s invalid to access this prototype property.
 571                         return GiveUpOnCache;
 572                     }
 573 
 574                     if (!usesPolyProto) {
 575                         prototypeAccessChain = nullptr;
 576                         conditionSet =
 577                             generateConditionsForPrototypePropertyHit(
 578                                 vm, codeBlock, exec, structure, slot.base(), ident.impl());
 579                         if (!conditionSet.isValid())
 580                             return GiveUpOnCache;
 581 
 582                         if (!(conditionSet.slotBaseCondition().attributes() &amp; PropertyAttribute::Accessor))
 583                             return GiveUpOnCache;
 584 
 585                         offset = conditionSet.slotBaseCondition().offset();
 586                     }
 587 
 588                 }
 589 
 590                 newCase = GetterSetterAccessCase::create(
 591                     vm, codeBlock, AccessCase::Setter, structure, offset, conditionSet, WTFMove(prototypeAccessChain));
 592             }
 593         }
 594 
 595         LOG_IC((ICEvent::PutByIdAddAccessCase, structure-&gt;classInfo(), ident, slot.base() == baseValue));
 596 
 597         result = stubInfo.addAccessCase(locker, codeBlock, ident, WTFMove(newCase));
 598 
 599         if (result.generatedSomeCode()) {
 600             LOG_IC((ICEvent::PutByIdReplaceWithJump, structure-&gt;classInfo(), ident, slot.base() == baseValue));
 601 
 602             RELEASE_ASSERT(result.code());
 603 
 604             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 605         }
 606     }
 607 
 608     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec-&gt;codeBlock(), result);
 609 
 610     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 611 }
 612 
 613 void repatchPutByID(ExecState* exec, JSValue baseValue, Structure* structure, const Identifier&amp; propertyName, const PutPropertySlot&amp; slot, StructureStubInfo&amp; stubInfo, PutKind putKind)
 614 {
 615     SuperSamplerScope superSamplerScope(false);
 616 
 617     if (tryCachePutByID(exec, baseValue, structure, propertyName, slot, stubInfo, putKind) == GiveUpOnCache) {
 618         CodeBlock* codeBlock = exec-&gt;codeBlock();
 619         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateGenericPutByIdFunction(slot, putKind));
 620     }
 621 }
 622 
 623 static InlineCacheAction tryCacheInByID(
 624     ExecState* exec, JSObject* base, const Identifier&amp; ident,
 625     bool wasFound, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo)
 626 {
 627     VM&amp; vm = exec-&gt;vm();
 628     AccessGenerationResult result;
 629 
 630     {
 631         GCSafeConcurrentJSLocker locker(exec-&gt;codeBlock()-&gt;m_lock, vm.heap);
 632         if (forceICFailure(exec))
 633             return GiveUpOnCache;
 634 
 635         if (!base-&gt;structure(vm)-&gt;propertyAccessesAreCacheable() || (!wasFound &amp;&amp; !base-&gt;structure(vm)-&gt;propertyAccessesAreCacheableForAbsence()))
 636             return GiveUpOnCache;
 637 
 638         if (wasFound) {
 639             if (!slot.isCacheable())
 640                 return GiveUpOnCache;
 641         }
 642 
 643         CodeBlock* codeBlock = exec-&gt;codeBlock();
 644         Structure* structure = base-&gt;structure(vm);
 645 
 646         std::unique_ptr&lt;PolyProtoAccessChain&gt; prototypeAccessChain;
 647         ObjectPropertyConditionSet conditionSet;
 648         if (wasFound) {
 649             InlineCacheAction action = actionForCell(vm, base);
 650             if (action != AttemptToCache)
 651                 return action;
 652 
 653             // Optimize self access.
 654             if (stubInfo.cacheType == CacheType::Unset
 655                 &amp;&amp; slot.isCacheableValue()
 656                 &amp;&amp; slot.slotBase() == base
 657                 &amp;&amp; !slot.watchpointSet()
 658                 &amp;&amp; !structure-&gt;needImpurePropertyWatchpoint()) {
 659                 bool generatedCodeInline = InlineAccess::generateSelfInAccess(stubInfo, structure);
 660                 if (generatedCodeInline) {
 661                     LOG_IC((ICEvent::InByIdSelfPatch, structure-&gt;classInfo(), ident, slot.slotBase() == base));
 662                     structure-&gt;startWatchingPropertyForReplacements(vm, slot.cachedOffset());
 663                     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInByIdOptimize);
 664                     stubInfo.initInByIdSelf(codeBlock, structure, slot.cachedOffset());
 665                     return RetryCacheLater;
 666                 }
 667             }
 668 
 669             if (slot.slotBase() != base) {
 670                 bool usesPolyProto;
 671                 prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), base, slot, usesPolyProto);
 672                 if (!prototypeAccessChain) {
 673                     // It&#39;s invalid to access this prototype property.
 674                     return GiveUpOnCache;
 675                 }
 676                 if (!usesPolyProto) {
 677                     prototypeAccessChain = nullptr;
 678                     conditionSet = generateConditionsForPrototypePropertyHit(
 679                         vm, codeBlock, exec, structure, slot.slotBase(), ident.impl());
 680                 }
 681             }
 682         } else {
 683             bool usesPolyProto;
 684             prototypeAccessChain = PolyProtoAccessChain::create(exec-&gt;lexicalGlobalObject(), base, slot, usesPolyProto);
 685             if (!prototypeAccessChain) {
 686                 // It&#39;s invalid to access this prototype property.
 687                 return GiveUpOnCache;
 688             }
 689 
 690             if (!usesPolyProto) {
 691                 prototypeAccessChain = nullptr;
 692                 conditionSet = generateConditionsForPropertyMiss(
 693                     vm, codeBlock, exec, structure, ident.impl());
 694             }
 695         }
 696         if (!conditionSet.isValid())
 697             return GiveUpOnCache;
 698 
 699         LOG_IC((ICEvent::InAddAccessCase, structure-&gt;classInfo(), ident, slot.slotBase() == base));
 700 
 701         std::unique_ptr&lt;AccessCase&gt; newCase = AccessCase::create(
 702             vm, codeBlock, wasFound ? AccessCase::InHit : AccessCase::InMiss, wasFound ? slot.cachedOffset() : invalidOffset, structure, conditionSet, WTFMove(prototypeAccessChain));
 703 
 704         result = stubInfo.addAccessCase(locker, codeBlock, ident, WTFMove(newCase));
 705 
 706         if (result.generatedSomeCode()) {
 707             LOG_IC((ICEvent::InReplaceWithJump, structure-&gt;classInfo(), ident, slot.slotBase() == base));
 708 
 709             RELEASE_ASSERT(result.code());
 710             InlineAccess::rewireStubAsJump(stubInfo, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 711         }
 712     }
 713 
 714     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, exec-&gt;codeBlock(), result);
 715 
 716     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 717 }
 718 
 719 void repatchInByID(ExecState* exec, JSObject* baseObject, const Identifier&amp; propertyName, bool wasFound, const PropertySlot&amp; slot, StructureStubInfo&amp; stubInfo)
 720 {
 721     SuperSamplerScope superSamplerScope(false);
 722 
 723     if (tryCacheInByID(exec, baseObject, propertyName, wasFound, slot, stubInfo) == GiveUpOnCache) {
 724         CodeBlock* codeBlock = exec-&gt;codeBlock();
 725         ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInById);
 726     }
 727 }
 728 
 729 static InlineCacheAction tryCacheInstanceOf(
 730     ExecState* exec, JSValue valueValue, JSValue prototypeValue, StructureStubInfo&amp; stubInfo,
 731     bool wasFound)
 732 {
 733     VM&amp; vm = exec-&gt;vm();
 734     CodeBlock* codeBlock = exec-&gt;codeBlock();
 735     AccessGenerationResult result;
 736 
 737     RELEASE_ASSERT(valueValue.isCell()); // shouldConsiderCaching rejects non-cells.
 738 
 739     if (forceICFailure(exec))
 740         return GiveUpOnCache;
 741 
 742     {
 743         GCSafeConcurrentJSLocker locker(codeBlock-&gt;m_lock, vm.heap);
 744 
 745         JSCell* value = valueValue.asCell();
 746         Structure* structure = value-&gt;structure(vm);
 747         std::unique_ptr&lt;AccessCase&gt; newCase;
 748         JSObject* prototype = jsDynamicCast&lt;JSObject*&gt;(vm, prototypeValue);
 749         if (prototype) {
 750             if (!jsDynamicCast&lt;JSObject*&gt;(vm, value)) {
 751                 newCase = InstanceOfAccessCase::create(
 752                     vm, codeBlock, AccessCase::InstanceOfMiss, structure, ObjectPropertyConditionSet(),
 753                     prototype);
 754             } else if (structure-&gt;prototypeQueriesAreCacheable()) {
 755                 // FIXME: Teach this to do poly proto.
 756                 // https://bugs.webkit.org/show_bug.cgi?id=185663
 757 
 758                 ObjectPropertyConditionSet conditionSet = generateConditionsForInstanceOf(
 759                     vm, codeBlock, exec, structure, prototype, wasFound);
 760 
 761                 if (conditionSet.isValid()) {
 762                     newCase = InstanceOfAccessCase::create(
 763                         vm, codeBlock,
 764                         wasFound ? AccessCase::InstanceOfHit : AccessCase::InstanceOfMiss,
 765                         structure, conditionSet, prototype);
 766                 }
 767             }
 768         }
 769 
 770         if (!newCase)
 771             newCase = AccessCase::create(vm, codeBlock, AccessCase::InstanceOfGeneric);
 772 
 773         LOG_IC((ICEvent::InstanceOfAddAccessCase, structure-&gt;classInfo(), Identifier()));
 774 
 775         result = stubInfo.addAccessCase(locker, codeBlock, Identifier(), WTFMove(newCase));
 776 
 777         if (result.generatedSomeCode()) {
 778             LOG_IC((ICEvent::InstanceOfReplaceWithJump, structure-&gt;classInfo(), Identifier()));
 779 
 780             RELEASE_ASSERT(result.code());
 781 
 782             MacroAssembler::repatchJump(
 783                 stubInfo.patchableJump(),
 784                 CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(result.code()));
 785         }
 786     }
 787 
 788     fireWatchpointsAndClearStubIfNeeded(vm, stubInfo, codeBlock, result);
 789 
 790     return result.shouldGiveUpNow() ? GiveUpOnCache : RetryCacheLater;
 791 }
 792 
 793 void repatchInstanceOf(
 794     ExecState* exec, JSValue valueValue, JSValue prototypeValue, StructureStubInfo&amp; stubInfo,
 795     bool wasFound)
 796 {
 797     SuperSamplerScope superSamplerScope(false);
 798     if (tryCacheInstanceOf(exec, valueValue, prototypeValue, stubInfo, wasFound) == GiveUpOnCache)
 799         ftlThunkAwareRepatchCall(exec-&gt;codeBlock(), stubInfo.slowPathCallLocation(), operationInstanceOfGeneric);
 800 }
 801 
 802 static void linkSlowFor(VM&amp;, CallLinkInfo&amp; callLinkInfo, MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; codeRef)
 803 {
 804     MacroAssembler::repatchNearCall(callLinkInfo.callReturnLocation(), CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(codeRef.code()));
 805 }
 806 
 807 static void linkSlowFor(VM&amp; vm, CallLinkInfo&amp; callLinkInfo, ThunkGenerator generator)
 808 {
 809     linkSlowFor(vm, callLinkInfo, vm.getCTIStub(generator).retagged&lt;JITStubRoutinePtrTag&gt;());
 810 }
 811 
 812 static void linkSlowFor(VM&amp; vm, CallLinkInfo&amp; callLinkInfo)
 813 {
 814     MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; virtualThunk = virtualThunkFor(vm, callLinkInfo);
 815     linkSlowFor(vm, callLinkInfo, virtualThunk);
 816     callLinkInfo.setSlowStub(createJITStubRoutine(virtualThunk, vm, nullptr, true));
 817 }
 818 
 819 static JSCell* webAssemblyOwner(JSCell* callee)
 820 {
 821 #if ENABLE(WEBASSEMBLY)
 822     // Each WebAssembly.Instance shares the stubs from their WebAssembly.Module, which are therefore the appropriate owner.
 823     return jsCast&lt;WebAssemblyToJSCallee*&gt;(callee)-&gt;module();
 824 #else
 825     UNUSED_PARAM(callee);
 826     RELEASE_ASSERT_NOT_REACHED();
 827     return nullptr;
 828 #endif // ENABLE(WEBASSEMBLY)
 829 }
 830 
 831 void linkFor(
 832     ExecState* exec, CallLinkInfo&amp; callLinkInfo, CodeBlock* calleeCodeBlock,
 833     JSObject* callee, MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr)
 834 {
 835     ASSERT(!callLinkInfo.stub());
 836 
 837     CallFrame* callerFrame = exec-&gt;callerFrame();
 838     // Our caller must have a cell for a callee. When calling
 839     // this from Wasm, we ensure the callee is a cell.
 840     ASSERT(callerFrame-&gt;callee().isCell());
 841 
 842     VM&amp; vm = callerFrame-&gt;vm();
 843     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
 844 
 845     // WebAssembly -&gt; JS stubs don&#39;t have a valid CodeBlock.
 846     JSCell* owner = isWebAssemblyToJSCallee(callerFrame-&gt;callee().asCell()) ? webAssemblyOwner(callerFrame-&gt;callee().asCell()) : callerCodeBlock;
 847     ASSERT(owner);
 848 
 849     ASSERT(!callLinkInfo.isLinked());
 850     callLinkInfo.setCallee(vm, owner, callee);
 851     MacroAssembler::repatchPointer(callLinkInfo.hotPathBegin(), callee);
 852     callLinkInfo.setLastSeenCallee(vm, owner, callee);
 853     if (shouldDumpDisassemblyFor(callerCodeBlock))
 854         dataLog(&quot;Linking call in &quot;, FullCodeOrigin(callerCodeBlock, callLinkInfo.codeOrigin()), &quot; to &quot;, pointerDump(calleeCodeBlock), &quot;, entrypoint at &quot;, codePtr, &quot;\n&quot;);
 855 
 856     MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), CodeLocationLabel&lt;JSEntryPtrTag&gt;(codePtr));
 857 
 858     if (calleeCodeBlock)
 859         calleeCodeBlock-&gt;linkIncomingCall(callerFrame, &amp;callLinkInfo);
 860 
 861     if (callLinkInfo.specializationKind() == CodeForCall &amp;&amp; callLinkInfo.allowStubs()) {
 862         linkSlowFor(vm, callLinkInfo, linkPolymorphicCallThunkGenerator);
 863         return;
 864     }
 865 
 866     linkSlowFor(vm, callLinkInfo);
 867 }
 868 
 869 void linkDirectFor(
 870     ExecState* exec, CallLinkInfo&amp; callLinkInfo, CodeBlock* calleeCodeBlock,
 871     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr)
 872 {
 873     ASSERT(!callLinkInfo.stub());
 874 
 875     CodeBlock* callerCodeBlock = exec-&gt;codeBlock();
 876 
 877     VM&amp; vm = callerCodeBlock-&gt;vm();
 878 
 879     ASSERT(!callLinkInfo.isLinked());
 880     callLinkInfo.setCodeBlock(vm, callerCodeBlock, jsCast&lt;FunctionCodeBlock*&gt;(calleeCodeBlock));
 881     if (shouldDumpDisassemblyFor(callerCodeBlock))
 882         dataLog(&quot;Linking call in &quot;, FullCodeOrigin(callerCodeBlock, callLinkInfo.codeOrigin()), &quot; to &quot;, pointerDump(calleeCodeBlock), &quot;, entrypoint at &quot;, codePtr, &quot;\n&quot;);
 883 
 884     if (callLinkInfo.callType() == CallLinkInfo::DirectTailCall)
 885         MacroAssembler::repatchJumpToNop(callLinkInfo.patchableJump());
 886     MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), CodeLocationLabel&lt;JSEntryPtrTag&gt;(codePtr));
 887 
 888     if (calleeCodeBlock)
 889         calleeCodeBlock-&gt;linkIncomingCall(exec, &amp;callLinkInfo);
 890 }
 891 
 892 void linkSlowFor(
 893     ExecState* exec, CallLinkInfo&amp; callLinkInfo)
 894 {
 895     CodeBlock* callerCodeBlock = exec-&gt;callerFrame()-&gt;codeBlock();
 896     VM&amp; vm = callerCodeBlock-&gt;vm();
 897 
 898     linkSlowFor(vm, callLinkInfo);
 899 }
 900 
 901 static void revertCall(VM&amp; vm, CallLinkInfo&amp; callLinkInfo, MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; codeRef)
 902 {
 903     if (callLinkInfo.isDirect()) {
 904         callLinkInfo.clearCodeBlock();
 905         if (!callLinkInfo.clearedByJettison()) {
 906             if (callLinkInfo.callType() == CallLinkInfo::DirectTailCall)
 907                 MacroAssembler::repatchJump(callLinkInfo.patchableJump(), callLinkInfo.slowPathStart());
 908             else
 909                 MacroAssembler::repatchNearCall(callLinkInfo.hotPathOther(), callLinkInfo.slowPathStart());
 910         }
 911     } else {
 912         if (!callLinkInfo.clearedByJettison()) {
 913             MacroAssembler::revertJumpReplacementToBranchPtrWithPatch(
 914                 MacroAssembler::startOfBranchPtrWithPatchOnRegister(callLinkInfo.hotPathBegin()),
 915                 callLinkInfo.calleeGPR(), 0);
 916             linkSlowFor(vm, callLinkInfo, codeRef);
 917             MacroAssembler::repatchPointer(callLinkInfo.hotPathBegin(), nullptr);
 918         }
 919         callLinkInfo.clearCallee();
 920     }
 921     callLinkInfo.clearSeen();
 922     callLinkInfo.clearStub();
 923     callLinkInfo.clearSlowStub();
 924     if (callLinkInfo.isOnList())
 925         callLinkInfo.remove();
 926 }
 927 
 928 void unlinkFor(VM&amp; vm, CallLinkInfo&amp; callLinkInfo)
 929 {
 930     if (Options::dumpDisassembly())
 931         dataLog(&quot;Unlinking call at &quot;, callLinkInfo.hotPathOther(), &quot;\n&quot;);
 932 
 933     revertCall(vm, callLinkInfo, vm.getCTIStub(linkCallThunkGenerator).retagged&lt;JITStubRoutinePtrTag&gt;());
 934 }
 935 
 936 static void linkVirtualFor(ExecState* exec, CallLinkInfo&amp; callLinkInfo)
 937 {
 938     CallFrame* callerFrame = exec-&gt;callerFrame();
 939     VM&amp; vm = callerFrame-&gt;vm();
 940     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
 941 
 942     if (shouldDumpDisassemblyFor(callerCodeBlock))
 943         dataLog(&quot;Linking virtual call at &quot;, FullCodeOrigin(callerCodeBlock, callerFrame-&gt;codeOrigin()), &quot;\n&quot;);
 944 
 945     MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; virtualThunk = virtualThunkFor(vm, callLinkInfo);
 946     revertCall(vm, callLinkInfo, virtualThunk);
 947     callLinkInfo.setSlowStub(createJITStubRoutine(virtualThunk, vm, nullptr, true));
 948     callLinkInfo.setClearedByVirtual();
 949 }
 950 
 951 namespace {
 952 struct CallToCodePtr {
 953     CCallHelpers::Call call;
 954     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr;
 955 };
 956 } // annonymous namespace
 957 
 958 void linkPolymorphicCall(
 959     ExecState* exec, CallLinkInfo&amp; callLinkInfo, CallVariant newVariant)
 960 {
 961     RELEASE_ASSERT(callLinkInfo.allowStubs());
 962 
 963     CallFrame* callerFrame = exec-&gt;callerFrame();
 964     VM&amp; vm = callerFrame-&gt;vm();
 965 
 966     // During execution of linkPolymorphicCall, we strongly assume that we never do GC.
 967     // GC jettisons CodeBlocks, changes CallLinkInfo etc. and breaks assumption done before and after this call.
 968     DeferGCForAWhile deferGCForAWhile(vm.heap);
 969 
 970     if (!newVariant) {
 971         linkVirtualFor(exec, callLinkInfo);
 972         return;
 973     }
 974 
 975     // Our caller must be have a cell for a callee. When calling
 976     // this from Wasm, we ensure the callee is a cell.
 977     ASSERT(callerFrame-&gt;callee().isCell());
 978 
 979     CodeBlock* callerCodeBlock = callerFrame-&gt;codeBlock();
 980     bool isWebAssembly = isWebAssemblyToJSCallee(callerFrame-&gt;callee().asCell());
 981 
 982     // WebAssembly -&gt; JS stubs don&#39;t have a valid CodeBlock.
 983     JSCell* owner = isWebAssembly ? webAssemblyOwner(callerFrame-&gt;callee().asCell()) : callerCodeBlock;
 984     ASSERT(owner);
 985 
 986     CallVariantList list;
 987     if (PolymorphicCallStubRoutine* stub = callLinkInfo.stub())
 988         list = stub-&gt;variants();
 989     else if (JSObject* oldCallee = callLinkInfo.callee())
 990         list = CallVariantList { CallVariant(oldCallee) };
 991 
 992     list = variantListWithVariant(list, newVariant);
 993 
 994     // If there are any closure calls then it makes sense to treat all of them as closure calls.
 995     // This makes switching on callee cheaper. It also produces profiling that&#39;s easier on the DFG;
 996     // the DFG doesn&#39;t really want to deal with a combination of closure and non-closure callees.
 997     bool isClosureCall = false;
 998     for (CallVariant variant : list)  {
 999         if (variant.isClosureCall()) {
1000             list = despecifiedVariantList(list);
1001             isClosureCall = true;
1002             break;
1003         }
1004     }
1005 
1006     if (isClosureCall)
1007         callLinkInfo.setHasSeenClosure();
1008 
1009     Vector&lt;PolymorphicCallCase&gt; callCases;
1010     Vector&lt;int64_t&gt; caseValues;
1011 
1012     // Figure out what our cases are.
1013     for (CallVariant variant : list) {
1014         CodeBlock* codeBlock = nullptr;
1015         if (variant.executable() &amp;&amp; !variant.executable()-&gt;isHostFunction()) {
1016             ExecutableBase* executable = variant.executable();
1017             codeBlock = jsCast&lt;FunctionExecutable*&gt;(executable)-&gt;codeBlockForCall();
1018             // If we cannot handle a callee, either because we don&#39;t have a CodeBlock or because arity mismatch,
1019             // assume that it&#39;s better for this whole thing to be a virtual call.
1020             if (!codeBlock || exec-&gt;argumentCountIncludingThis() &lt; static_cast&lt;size_t&gt;(codeBlock-&gt;numParameters()) || callLinkInfo.isVarargs()) {
1021                 linkVirtualFor(exec, callLinkInfo);
1022                 return;
1023             }
1024         }
1025 
1026         int64_t newCaseValue = 0;
1027         if (isClosureCall) {
1028             newCaseValue = bitwise_cast&lt;intptr_t&gt;(variant.executable());
1029             // FIXME: We could add a fast path for InternalFunction with closure call.
1030             // https://bugs.webkit.org/show_bug.cgi?id=179311
1031             if (!newCaseValue)
1032                 continue;
1033         } else {
1034             if (auto* function = variant.function())
1035                 newCaseValue = bitwise_cast&lt;intptr_t&gt;(function);
1036             else
1037                 newCaseValue = bitwise_cast&lt;intptr_t&gt;(variant.internalFunction());
1038         }
1039 
1040         if (!ASSERT_DISABLED) {
1041             if (caseValues.contains(newCaseValue)) {
1042                 dataLog(&quot;ERROR: Attempt to add duplicate case value.\n&quot;);
1043                 dataLog(&quot;Existing case values: &quot;);
1044                 CommaPrinter comma;
1045                 for (auto&amp; value : caseValues)
1046                     dataLog(comma, value);
1047                 dataLog(&quot;\n&quot;);
1048                 dataLog(&quot;Attempting to add: &quot;, newCaseValue, &quot;\n&quot;);
1049                 dataLog(&quot;Variant list: &quot;, listDump(callCases), &quot;\n&quot;);
1050                 RELEASE_ASSERT_NOT_REACHED();
1051             }
1052         }
1053 
1054         callCases.append(PolymorphicCallCase(variant, codeBlock));
1055         caseValues.append(newCaseValue);
1056     }
1057     ASSERT(callCases.size() == caseValues.size());
1058 
1059     // If we are over the limit, just use a normal virtual call.
1060     unsigned maxPolymorphicCallVariantListSize;
1061     if (isWebAssembly)
1062         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSizeForWebAssemblyToJS();
1063     else if (callerCodeBlock-&gt;jitType() == JITCode::topTierJIT())
1064         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSizeForTopTier();
1065     else
1066         maxPolymorphicCallVariantListSize = Options::maxPolymorphicCallVariantListSize();
1067 
1068     // We use list.size() instead of callCases.size() because we respect CallVariant size for now.
1069     if (list.size() &gt; maxPolymorphicCallVariantListSize) {
1070         linkVirtualFor(exec, callLinkInfo);
1071         return;
1072     }
1073 
1074     Vector&lt;CallToCodePtr&gt; calls(callCases.size());
1075     UniqueArray&lt;uint32_t&gt; fastCounts;
1076 
1077     if (!isWebAssembly &amp;&amp; callerCodeBlock-&gt;jitType() != JITCode::topTierJIT()) {
1078         fastCounts = makeUniqueArray&lt;uint32_t&gt;(callCases.size());
1079         memset(fastCounts.get(), 0, callCases.size() * sizeof(uint32_t));
1080     }
1081 
1082     GPRReg calleeGPR = callLinkInfo.calleeGPR();
1083 
1084     CCallHelpers stubJit(callerCodeBlock);
1085 
1086     std::unique_ptr&lt;CallFrameShuffler&gt; frameShuffler;
1087     if (callLinkInfo.frameShuffleData()) {
1088         ASSERT(callLinkInfo.isTailCall());
1089         frameShuffler = makeUnique&lt;CallFrameShuffler&gt;(stubJit, *callLinkInfo.frameShuffleData());
1090 #if USE(JSVALUE32_64)
1091         // We would have already checked that the callee is a cell, and we can
1092         // use the additional register this buys us.
1093         frameShuffler-&gt;assumeCalleeIsCell();
1094 #endif
1095         frameShuffler-&gt;lockGPR(calleeGPR);
1096     }
1097 
1098     GPRReg comparisonValueGPR;
1099     if (isClosureCall) {
1100         if (frameShuffler)
1101             comparisonValueGPR = frameShuffler-&gt;acquireGPR();
1102         else
1103             comparisonValueGPR = AssemblyHelpers::selectScratchGPR(calleeGPR);
1104     } else
1105         comparisonValueGPR = calleeGPR;
1106 
1107     GPRReg fastCountsBaseGPR;
1108     if (frameShuffler)
1109         fastCountsBaseGPR = frameShuffler-&gt;acquireGPR();
1110     else {
1111         fastCountsBaseGPR =
1112             AssemblyHelpers::selectScratchGPR(calleeGPR, comparisonValueGPR, GPRInfo::regT3);
1113     }
1114     stubJit.move(CCallHelpers::TrustedImmPtr(fastCounts.get()), fastCountsBaseGPR);
1115 
1116     if (!frameShuffler &amp;&amp; callLinkInfo.isTailCall()) {
1117         // We strongly assume that calleeGPR is not a callee save register in the slow path.
1118         ASSERT(!callerCodeBlock-&gt;calleeSaveRegisters()-&gt;find(calleeGPR));
1119         stubJit.emitRestoreCalleeSaves();
1120     }
1121 
1122     CCallHelpers::JumpList slowPath;
1123     if (isClosureCall) {
1124         // Verify that we have a function and stash the executable in scratchGPR.
1125 #if USE(JSVALUE64)
1126         if (callLinkInfo.isTailCall())
1127             slowPath.append(stubJit.branchIfNotCell(calleeGPR, DoNotHaveTagRegisters));
1128         else
1129             slowPath.append(stubJit.branchIfNotCell(calleeGPR));
1130 #else
1131         // We would have already checked that the callee is a cell.
1132 #endif
1133         // FIXME: We could add a fast path for InternalFunction with closure call.
1134         slowPath.append(stubJit.branchIfNotFunction(calleeGPR));
1135 
1136         stubJit.loadPtr(
1137             CCallHelpers::Address(calleeGPR, JSFunction::offsetOfExecutable()),
1138             comparisonValueGPR);
1139     }
1140 
1141     BinarySwitch binarySwitch(comparisonValueGPR, caseValues, BinarySwitch::IntPtr);
1142     CCallHelpers::JumpList done;
1143     while (binarySwitch.advance(stubJit)) {
1144         size_t caseIndex = binarySwitch.caseIndex();
1145 
1146         CallVariant variant = callCases[caseIndex].variant();
1147 
1148         MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr;
1149         if (variant.executable()) {
1150             ASSERT(variant.executable()-&gt;hasJITCodeForCall());
1151 
1152             codePtr = jsToWasmICCodePtr(vm, callLinkInfo.specializationKind(), variant.function());
1153             if (!codePtr)
1154                 codePtr = variant.executable()-&gt;generatedJITCodeForCall()-&gt;addressForCall(ArityCheckNotRequired);
1155         } else {
1156             ASSERT(variant.internalFunction());
1157             codePtr = vm.getCTIInternalFunctionTrampolineFor(CodeForCall);
1158         }
1159 
1160         if (fastCounts) {
1161             stubJit.add32(
1162                 CCallHelpers::TrustedImm32(1),
1163                 CCallHelpers::Address(fastCountsBaseGPR, caseIndex * sizeof(uint32_t)));
1164         }
1165         if (frameShuffler) {
1166             CallFrameShuffler(stubJit, frameShuffler-&gt;snapshot()).prepareForTailCall();
1167             calls[caseIndex].call = stubJit.nearTailCall();
1168         } else if (callLinkInfo.isTailCall()) {
1169             stubJit.prepareForTailCallSlow();
1170             calls[caseIndex].call = stubJit.nearTailCall();
1171         } else
1172             calls[caseIndex].call = stubJit.nearCall();
1173         calls[caseIndex].codePtr = codePtr;
1174         done.append(stubJit.jump());
1175     }
1176 
1177     slowPath.link(&amp;stubJit);
1178     binarySwitch.fallThrough().link(&amp;stubJit);
1179 
1180     if (frameShuffler) {
1181         frameShuffler-&gt;releaseGPR(calleeGPR);
1182         frameShuffler-&gt;releaseGPR(comparisonValueGPR);
1183         frameShuffler-&gt;releaseGPR(fastCountsBaseGPR);
1184 #if USE(JSVALUE32_64)
1185         frameShuffler-&gt;setCalleeJSValueRegs(JSValueRegs(GPRInfo::regT1, GPRInfo::regT0));
1186 #else
1187         frameShuffler-&gt;setCalleeJSValueRegs(JSValueRegs(GPRInfo::regT0));
1188 #endif
1189         frameShuffler-&gt;prepareForSlowPath();
1190     } else {
1191         stubJit.move(calleeGPR, GPRInfo::regT0);
1192 #if USE(JSVALUE32_64)
1193         stubJit.move(CCallHelpers::TrustedImm32(JSValue::CellTag), GPRInfo::regT1);
1194 #endif
1195     }
1196     stubJit.move(CCallHelpers::TrustedImmPtr(&amp;callLinkInfo), GPRInfo::regT2);
1197     stubJit.move(CCallHelpers::TrustedImmPtr(callLinkInfo.callReturnLocation().untaggedExecutableAddress()), GPRInfo::regT4);
1198 
1199     stubJit.restoreReturnAddressBeforeReturn(GPRInfo::regT4);
1200     AssemblyHelpers::Jump slow = stubJit.jump();
1201 
1202     LinkBuffer patchBuffer(stubJit, owner, JITCompilationCanFail);
1203     if (patchBuffer.didFailToAllocate()) {
1204         linkVirtualFor(exec, callLinkInfo);
1205         return;
1206     }
1207 
1208     RELEASE_ASSERT(callCases.size() == calls.size());
1209     for (CallToCodePtr callToCodePtr : calls) {
1210 #if CPU(ARM_THUMB2)
1211         // Tail call special-casing ensures proper linking on ARM Thumb2, where a tail call jumps to an address
1212         // with a non-decorated bottom bit but a normal call calls an address with a decorated bottom bit.
1213         bool isTailCall = callToCodePtr.call.isFlagSet(CCallHelpers::Call::Tail);
1214         void* target = isTailCall ? callToCodePtr.codePtr.dataLocation() : callToCodePtr.codePtr.executableAddress();
1215         patchBuffer.link(callToCodePtr.call, FunctionPtr&lt;JSEntryPtrTag&gt;(MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt;::createFromExecutableAddress(target)));
1216 #else
1217         patchBuffer.link(callToCodePtr.call, FunctionPtr&lt;JSEntryPtrTag&gt;(callToCodePtr.codePtr));
1218 #endif
1219     }
1220     if (isWebAssembly || JITCode::isOptimizingJIT(callerCodeBlock-&gt;jitType()))
1221         patchBuffer.link(done, callLinkInfo.callReturnLocation().labelAtOffset(0));
1222     else
1223         patchBuffer.link(done, callLinkInfo.hotPathOther().labelAtOffset(0));
1224     patchBuffer.link(slow, CodeLocationLabel&lt;JITThunkPtrTag&gt;(vm.getCTIStub(linkPolymorphicCallThunkGenerator).code()));
1225 
1226     auto stubRoutine = adoptRef(*new PolymorphicCallStubRoutine(
1227         FINALIZE_CODE_FOR(
1228             callerCodeBlock, patchBuffer, JITStubRoutinePtrTag,
1229             &quot;Polymorphic call stub for %s, return point %p, targets %s&quot;,
1230                 isWebAssembly ? &quot;WebAssembly&quot; : toCString(*callerCodeBlock).data(), callLinkInfo.callReturnLocation().labelAtOffset(0).executableAddress(),
1231                 toCString(listDump(callCases)).data()),
1232         vm, owner, exec-&gt;callerFrame(), callLinkInfo, callCases,
1233         WTFMove(fastCounts)));
1234 
1235     MacroAssembler::replaceWithJump(
1236         MacroAssembler::startOfBranchPtrWithPatchOnRegister(callLinkInfo.hotPathBegin()),
1237         CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(stubRoutine-&gt;code().code()));
1238     // The original slow path is unreachable on 64-bits, but still
1239     // reachable on 32-bits since a non-cell callee will always
1240     // trigger the slow path
1241     linkSlowFor(vm, callLinkInfo);
1242 
1243     // If there had been a previous stub routine, that one will die as soon as the GC runs and sees
1244     // that it&#39;s no longer on stack.
1245     callLinkInfo.setStub(WTFMove(stubRoutine));
1246 
1247     // The call link info no longer has a call cache apart from the jump to the polymorphic call
1248     // stub.
1249     if (callLinkInfo.isOnList())
1250         callLinkInfo.remove();
1251 }
1252 
1253 void resetGetByID(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo, GetByIDKind kind)
1254 {
1255     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), appropriateOptimizingGetByIdFunction(kind));
1256     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
1257 }
1258 
1259 void resetPutByID(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo)
1260 {
1261     V_JITOperation_ESsiJJI unoptimizedFunction = reinterpret_cast&lt;V_JITOperation_ESsiJJI&gt;(readPutICCallTarget(codeBlock, stubInfo.slowPathCallLocation()).executableAddress());
1262     V_JITOperation_ESsiJJI optimizedFunction;
1263     if (unoptimizedFunction == operationPutByIdStrict || unoptimizedFunction == operationPutByIdStrictOptimize)
1264         optimizedFunction = operationPutByIdStrictOptimize;
1265     else if (unoptimizedFunction == operationPutByIdNonStrict || unoptimizedFunction == operationPutByIdNonStrictOptimize)
1266         optimizedFunction = operationPutByIdNonStrictOptimize;
1267     else if (unoptimizedFunction == operationPutByIdDirectStrict || unoptimizedFunction == operationPutByIdDirectStrictOptimize)
1268         optimizedFunction = operationPutByIdDirectStrictOptimize;
1269     else {
1270         ASSERT(unoptimizedFunction == operationPutByIdDirectNonStrict || unoptimizedFunction == operationPutByIdDirectNonStrictOptimize);
1271         optimizedFunction = operationPutByIdDirectNonStrictOptimize;
1272     }
1273 
1274     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), optimizedFunction);
1275     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
1276 }
1277 
1278 static void resetPatchableJump(StructureStubInfo&amp; stubInfo)
1279 {
1280     MacroAssembler::repatchJump(stubInfo.patchableJump(), stubInfo.slowPathStartLocation());
1281 }
1282 
1283 void resetInByID(CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo)
1284 {
1285     ftlThunkAwareRepatchCall(codeBlock, stubInfo.slowPathCallLocation(), operationInByIdOptimize);
1286     InlineAccess::rewireStubAsJump(stubInfo, stubInfo.slowPathStartLocation());
1287 }
1288 
1289 void resetInstanceOf(StructureStubInfo&amp; stubInfo)
1290 {
1291     resetPatchableJump(stubInfo);
1292 }
1293 
1294 MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; jsToWasmICCodePtr(VM&amp; vm, CodeSpecializationKind kind, JSObject* callee)
1295 {
1296 #if ENABLE(WEBASSEMBLY)
1297     if (!callee)
1298         return nullptr;
1299     if (kind != CodeForCall)
1300         return nullptr;
1301     if (auto* wasmFunction = jsDynamicCast&lt;WebAssemblyFunction*&gt;(vm, callee))
1302         return wasmFunction-&gt;jsCallEntrypoint();
1303 #else
1304     UNUSED_PARAM(vm);
1305     UNUSED_PARAM(kind);
1306     UNUSED_PARAM(callee);
1307 #endif
1308     return nullptr;
1309 }
1310 
1311 } // namespace JSC
1312 
1313 #endif
    </pre>
  </body>
</html>