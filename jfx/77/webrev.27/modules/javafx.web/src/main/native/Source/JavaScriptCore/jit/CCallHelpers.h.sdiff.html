<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/CCallHelpers.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="BinarySwitch.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CachedRecovery.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/CCallHelpers.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (C) 2011-2018 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
</pre>
<hr />
<pre>
725             move(srcB, destB);
726             move(srcA, destA);
727         } else
728             swap(destA, destB);
729     }
730 
731     void setupResults(JSValueRegs regs)
732     {
733 #if USE(JSVALUE64)
734         move(GPRInfo::returnValueGPR, regs.gpr());
735 #else
736         setupResults(regs.payloadGPR(), regs.tagGPR());
737 #endif
738     }
739 
740     void jumpToExceptionHandler(VM&amp; vm)
741     {
742         // genericUnwind() leaves the handler CallFrame* in vm-&gt;callFrameForCatch,
743         // and the address of the handler in vm-&gt;targetMachinePCForThrow.
744         loadPtr(&amp;vm.targetMachinePCForThrow, GPRInfo::regT1);
<span class="line-modified">745         jump(GPRInfo::regT1, ExceptionHandlerPtrTag);</span>
746     }
747 
748     void prepareForTailCallSlow(GPRReg calleeGPR = InvalidGPRReg)
749     {
750         GPRReg temp1 = calleeGPR == GPRInfo::regT0 ? GPRInfo::regT3 : GPRInfo::regT0;
751         GPRReg temp2 = calleeGPR == GPRInfo::regT1 ? GPRInfo::regT3 : GPRInfo::regT1;
752         GPRReg temp3 = calleeGPR == GPRInfo::regT2 ? GPRInfo::regT3 : GPRInfo::regT2;
753 
754         GPRReg newFramePointer = temp1;
755         GPRReg newFrameSizeGPR = temp2;
756         {
757             // The old frame size is its number of arguments (or number of
758             // parameters in case of arity fixup), plus the frame header size,
759             // aligned
760             GPRReg oldFrameSizeGPR = temp2;
761             {
762                 GPRReg argCountGPR = oldFrameSizeGPR;
763                 load32(Address(framePointerRegister, CallFrameSlot::argumentCount * static_cast&lt;int&gt;(sizeof(Register)) + PayloadOffset), argCountGPR);
764 
765                 {
</pre>
<hr />
<pre>
788 
789             // The new frame size is just the number of arguments plus the
790             // frame header size, aligned
791             ASSERT(newFrameSizeGPR != newFramePointer);
792             load32(Address(stackPointerRegister, CallFrameSlot::argumentCount * static_cast&lt;int&gt;(sizeof(Register)) + PayloadOffset - sizeof(CallerFrameAndPC)),
793                 newFrameSizeGPR);
794             add32(TrustedImm32(stackAlignmentRegisters() + CallFrame::headerSizeInRegisters - 1), newFrameSizeGPR);
795             and32(TrustedImm32(-stackAlignmentRegisters()), newFrameSizeGPR);
796             // We assume &lt; 2^28 arguments
797             mul32(TrustedImm32(sizeof(Register)), newFrameSizeGPR, newFrameSizeGPR);
798         }
799 
800         GPRReg tempGPR = temp3;
801         ASSERT(tempGPR != newFramePointer &amp;&amp; tempGPR != newFrameSizeGPR);
802 
803         // We don&#39;t need the current frame beyond this point. Masquerade as our
804         // caller.
805 #if CPU(ARM_THUMB2) || CPU(ARM64)
806         loadPtr(Address(framePointerRegister, CallFrame::returnPCOffset()), linkRegister);
807         subPtr(TrustedImm32(2 * sizeof(void*)), newFrameSizeGPR);
<span class="line-modified">808 #if USE(POINTER_PROFILING)</span>
809         addPtr(TrustedImm32(sizeof(CallerFrameAndPC)), MacroAssembler::framePointerRegister, tempGPR);
<span class="line-modified">810         untagPtr(linkRegister, tempGPR);</span>
811 #endif
812 #elif CPU(MIPS)
813         loadPtr(Address(framePointerRegister, sizeof(void*)), returnAddressRegister);
814         subPtr(TrustedImm32(2 * sizeof(void*)), newFrameSizeGPR);
815 #elif CPU(X86) || CPU(X86_64)
816         loadPtr(Address(framePointerRegister, sizeof(void*)), tempGPR);
817         push(tempGPR);
818         subPtr(TrustedImm32(sizeof(void*)), newFrameSizeGPR);
819 #else
820         UNREACHABLE_FOR_PLATFORM();
821 #endif
822         subPtr(newFrameSizeGPR, newFramePointer);
823         loadPtr(Address(framePointerRegister), framePointerRegister);
824 
825 
826         // We need to move the newFrameSizeGPR slots above the stack pointer by
827         // newFramePointer registers. We use pointer-sized chunks.
828         MacroAssembler::Label copyLoop(label());
829 
830         subPtr(TrustedImm32(sizeof(void*)), newFrameSizeGPR);
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (C) 2011-2019 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
</pre>
<hr />
<pre>
725             move(srcB, destB);
726             move(srcA, destA);
727         } else
728             swap(destA, destB);
729     }
730 
731     void setupResults(JSValueRegs regs)
732     {
733 #if USE(JSVALUE64)
734         move(GPRInfo::returnValueGPR, regs.gpr());
735 #else
736         setupResults(regs.payloadGPR(), regs.tagGPR());
737 #endif
738     }
739 
740     void jumpToExceptionHandler(VM&amp; vm)
741     {
742         // genericUnwind() leaves the handler CallFrame* in vm-&gt;callFrameForCatch,
743         // and the address of the handler in vm-&gt;targetMachinePCForThrow.
744         loadPtr(&amp;vm.targetMachinePCForThrow, GPRInfo::regT1);
<span class="line-modified">745         farJump(GPRInfo::regT1, ExceptionHandlerPtrTag);</span>
746     }
747 
748     void prepareForTailCallSlow(GPRReg calleeGPR = InvalidGPRReg)
749     {
750         GPRReg temp1 = calleeGPR == GPRInfo::regT0 ? GPRInfo::regT3 : GPRInfo::regT0;
751         GPRReg temp2 = calleeGPR == GPRInfo::regT1 ? GPRInfo::regT3 : GPRInfo::regT1;
752         GPRReg temp3 = calleeGPR == GPRInfo::regT2 ? GPRInfo::regT3 : GPRInfo::regT2;
753 
754         GPRReg newFramePointer = temp1;
755         GPRReg newFrameSizeGPR = temp2;
756         {
757             // The old frame size is its number of arguments (or number of
758             // parameters in case of arity fixup), plus the frame header size,
759             // aligned
760             GPRReg oldFrameSizeGPR = temp2;
761             {
762                 GPRReg argCountGPR = oldFrameSizeGPR;
763                 load32(Address(framePointerRegister, CallFrameSlot::argumentCount * static_cast&lt;int&gt;(sizeof(Register)) + PayloadOffset), argCountGPR);
764 
765                 {
</pre>
<hr />
<pre>
788 
789             // The new frame size is just the number of arguments plus the
790             // frame header size, aligned
791             ASSERT(newFrameSizeGPR != newFramePointer);
792             load32(Address(stackPointerRegister, CallFrameSlot::argumentCount * static_cast&lt;int&gt;(sizeof(Register)) + PayloadOffset - sizeof(CallerFrameAndPC)),
793                 newFrameSizeGPR);
794             add32(TrustedImm32(stackAlignmentRegisters() + CallFrame::headerSizeInRegisters - 1), newFrameSizeGPR);
795             and32(TrustedImm32(-stackAlignmentRegisters()), newFrameSizeGPR);
796             // We assume &lt; 2^28 arguments
797             mul32(TrustedImm32(sizeof(Register)), newFrameSizeGPR, newFrameSizeGPR);
798         }
799 
800         GPRReg tempGPR = temp3;
801         ASSERT(tempGPR != newFramePointer &amp;&amp; tempGPR != newFrameSizeGPR);
802 
803         // We don&#39;t need the current frame beyond this point. Masquerade as our
804         // caller.
805 #if CPU(ARM_THUMB2) || CPU(ARM64)
806         loadPtr(Address(framePointerRegister, CallFrame::returnPCOffset()), linkRegister);
807         subPtr(TrustedImm32(2 * sizeof(void*)), newFrameSizeGPR);
<span class="line-modified">808 #if CPU(ARM64E)</span>
809         addPtr(TrustedImm32(sizeof(CallerFrameAndPC)), MacroAssembler::framePointerRegister, tempGPR);
<span class="line-modified">810         untagPtr(tempGPR, linkRegister);</span>
811 #endif
812 #elif CPU(MIPS)
813         loadPtr(Address(framePointerRegister, sizeof(void*)), returnAddressRegister);
814         subPtr(TrustedImm32(2 * sizeof(void*)), newFrameSizeGPR);
815 #elif CPU(X86) || CPU(X86_64)
816         loadPtr(Address(framePointerRegister, sizeof(void*)), tempGPR);
817         push(tempGPR);
818         subPtr(TrustedImm32(sizeof(void*)), newFrameSizeGPR);
819 #else
820         UNREACHABLE_FOR_PLATFORM();
821 #endif
822         subPtr(newFrameSizeGPR, newFramePointer);
823         loadPtr(Address(framePointerRegister), framePointerRegister);
824 
825 
826         // We need to move the newFrameSizeGPR slots above the stack pointer by
827         // newFramePointer registers. We use pointer-sized chunks.
828         MacroAssembler::Label copyLoop(label());
829 
830         subPtr(TrustedImm32(sizeof(void*)), newFrameSizeGPR);
</pre>
</td>
</tr>
</table>
<center><a href="BinarySwitch.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CachedRecovery.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>