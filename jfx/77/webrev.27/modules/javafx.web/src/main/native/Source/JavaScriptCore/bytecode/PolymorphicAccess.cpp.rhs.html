<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/PolymorphicAccess.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (C) 2014-2018 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;PolymorphicAccess.h&quot;
 28 
 29 #if ENABLE(JIT)
 30 
 31 #include &quot;BinarySwitch.h&quot;
 32 #include &quot;CCallHelpers.h&quot;
 33 #include &quot;CodeBlock.h&quot;
 34 #include &quot;FullCodeOrigin.h&quot;
 35 #include &quot;Heap.h&quot;
 36 #include &quot;JITOperations.h&quot;
 37 #include &quot;JSCInlines.h&quot;
 38 #include &quot;LinkBuffer.h&quot;
 39 #include &quot;StructureStubClearingWatchpoint.h&quot;
 40 #include &quot;StructureStubInfo.h&quot;
 41 #include &quot;SuperSampler.h&quot;
 42 #include &lt;wtf/CommaPrinter.h&gt;
 43 #include &lt;wtf/ListDump.h&gt;
 44 
 45 namespace JSC {
 46 
 47 namespace PolymorphicAccessInternal {
 48 static const bool verbose = false;
 49 }
 50 
 51 void AccessGenerationResult::dump(PrintStream&amp; out) const
 52 {
 53     out.print(m_kind);
 54     if (m_code)
 55         out.print(&quot;:&quot;, m_code);
 56 }
 57 
 58 Watchpoint* AccessGenerationState::addWatchpoint(const ObjectPropertyCondition&amp; condition)
 59 {
 60     return WatchpointsOnStructureStubInfo::ensureReferenceAndAddWatchpoint(
 61         watchpoints, jit-&gt;codeBlock(), stubInfo, condition);
 62 }
 63 
 64 void AccessGenerationState::restoreScratch()
 65 {
 66     allocator-&gt;restoreReusedRegistersByPopping(*jit, preservedReusedRegisterState);
 67 }
 68 
 69 void AccessGenerationState::succeed()
 70 {
 71     restoreScratch();
 72     success.append(jit-&gt;jump());
 73 }
 74 
 75 const RegisterSet&amp; AccessGenerationState::liveRegistersForCall()
 76 {
 77     if (!m_calculatedRegistersForCallAndExceptionHandling)
 78         calculateLiveRegistersForCallAndExceptionHandling();
 79     return m_liveRegistersForCall;
 80 }
 81 
 82 const RegisterSet&amp; AccessGenerationState::liveRegistersToPreserveAtExceptionHandlingCallSite()
 83 {
 84     if (!m_calculatedRegistersForCallAndExceptionHandling)
 85         calculateLiveRegistersForCallAndExceptionHandling();
 86     return m_liveRegistersToPreserveAtExceptionHandlingCallSite;
 87 }
 88 
 89 static RegisterSet calleeSaveRegisters()
 90 {
 91     RegisterSet result = RegisterSet::registersToNotSaveForJSCall();
 92     result.filter(RegisterSet::registersToNotSaveForCCall());
 93     return result;
 94 }
 95 
 96 const RegisterSet&amp; AccessGenerationState::calculateLiveRegistersForCallAndExceptionHandling()
 97 {
 98     if (!m_calculatedRegistersForCallAndExceptionHandling) {
 99         m_calculatedRegistersForCallAndExceptionHandling = true;
100 
101         m_liveRegistersToPreserveAtExceptionHandlingCallSite = jit-&gt;codeBlock()-&gt;jitCode()-&gt;liveRegistersToPreserveAtExceptionHandlingCallSite(jit-&gt;codeBlock(), stubInfo-&gt;callSiteIndex);
102         m_needsToRestoreRegistersIfException = m_liveRegistersToPreserveAtExceptionHandlingCallSite.numberOfSetRegisters() &gt; 0;
103         if (m_needsToRestoreRegistersIfException)
104             RELEASE_ASSERT(JITCode::isOptimizingJIT(jit-&gt;codeBlock()-&gt;jitType()));
105 
106         m_liveRegistersForCall = RegisterSet(m_liveRegistersToPreserveAtExceptionHandlingCallSite, allocator-&gt;usedRegisters());
107         m_liveRegistersForCall.exclude(calleeSaveRegisters());
108     }
109     return m_liveRegistersForCall;
110 }
111 
112 auto AccessGenerationState::preserveLiveRegistersToStackForCall(const RegisterSet&amp; extra) -&gt; SpillState
113 {
114     RegisterSet liveRegisters = liveRegistersForCall();
115     liveRegisters.merge(extra);
116 
117     unsigned extraStackPadding = 0;
118     unsigned numberOfStackBytesUsedForRegisterPreservation = ScratchRegisterAllocator::preserveRegistersToStackForCall(*jit, liveRegisters, extraStackPadding);
119     return SpillState {
120         WTFMove(liveRegisters),
121         numberOfStackBytesUsedForRegisterPreservation
122     };
123 }
124 
125 void AccessGenerationState::restoreLiveRegistersFromStackForCallWithThrownException(const SpillState&amp; spillState)
126 {
127     // Even if we&#39;re a getter, we don&#39;t want to ignore the result value like we normally do
128     // because the getter threw, and therefore, didn&#39;t return a value that means anything.
129     // Instead, we want to restore that register to what it was upon entering the getter
130     // inline cache. The subtlety here is if the base and the result are the same register,
131     // and the getter threw, we want OSR exit to see the original base value, not the result
132     // of the getter call.
133     RegisterSet dontRestore = spillState.spilledRegisters;
134     // As an optimization here, we only need to restore what is live for exception handling.
135     // We can construct the dontRestore set to accomplish this goal by having it contain only
136     // what is live for call but not live for exception handling. By ignoring things that are
137     // only live at the call but not the exception handler, we will only restore things live
138     // at the exception handler.
139     dontRestore.exclude(liveRegistersToPreserveAtExceptionHandlingCallSite());
140     restoreLiveRegistersFromStackForCall(spillState, dontRestore);
141 }
142 
143 void AccessGenerationState::restoreLiveRegistersFromStackForCall(const SpillState&amp; spillState, const RegisterSet&amp; dontRestore)
144 {
145     unsigned extraStackPadding = 0;
146     ScratchRegisterAllocator::restoreRegistersFromStackForCall(*jit, spillState.spilledRegisters, dontRestore, spillState.numberOfStackBytesUsedForRegisterPreservation, extraStackPadding);
147 }
148 
149 CallSiteIndex AccessGenerationState::callSiteIndexForExceptionHandlingOrOriginal()
150 {
151     if (!m_calculatedRegistersForCallAndExceptionHandling)
152         calculateLiveRegistersForCallAndExceptionHandling();
153 
154     if (!m_calculatedCallSiteIndex) {
155         m_calculatedCallSiteIndex = true;
156 
157         if (m_needsToRestoreRegistersIfException)
158             m_callSiteIndex = jit-&gt;codeBlock()-&gt;newExceptionHandlingCallSiteIndex(stubInfo-&gt;callSiteIndex);
159         else
160             m_callSiteIndex = originalCallSiteIndex();
161     }
162 
163     return m_callSiteIndex;
164 }
165 
166 DisposableCallSiteIndex AccessGenerationState::callSiteIndexForExceptionHandling()
167 {
168     RELEASE_ASSERT(m_calculatedRegistersForCallAndExceptionHandling);
169     RELEASE_ASSERT(m_needsToRestoreRegistersIfException);
170     RELEASE_ASSERT(m_calculatedCallSiteIndex);
171     return DisposableCallSiteIndex::fromCallSiteIndex(m_callSiteIndex);
172 }
173 
174 const HandlerInfo&amp; AccessGenerationState::originalExceptionHandler()
175 {
176     if (!m_calculatedRegistersForCallAndExceptionHandling)
177         calculateLiveRegistersForCallAndExceptionHandling();
178 
179     RELEASE_ASSERT(m_needsToRestoreRegistersIfException);
180     HandlerInfo* exceptionHandler = jit-&gt;codeBlock()-&gt;handlerForIndex(stubInfo-&gt;callSiteIndex.bits());
181     RELEASE_ASSERT(exceptionHandler);
182     return *exceptionHandler;
183 }
184 
185 CallSiteIndex AccessGenerationState::originalCallSiteIndex() const { return stubInfo-&gt;callSiteIndex; }
186 
187 void AccessGenerationState::emitExplicitExceptionHandler()
188 {
189     restoreScratch();
<a name="1" id="anc1"></a><span class="line-modified">190     jit-&gt;pushToSave(GPRInfo::regT0);</span>
<span class="line-added">191     jit-&gt;loadPtr(&amp;m_vm.topEntryFrame, GPRInfo::regT0);</span>
<span class="line-added">192     jit-&gt;copyCalleeSavesToEntryFrameCalleeSavesBuffer(GPRInfo::regT0);</span>
<span class="line-added">193     jit-&gt;popToRestore(GPRInfo::regT0);</span>
<span class="line-added">194 </span>
195     if (needsToRestoreRegistersIfException()) {
196         // To the JIT that produces the original exception handling
197         // call site, they will expect the OSR exit to be arrived
198         // at from genericUnwind. Therefore we must model what genericUnwind
199         // does here. I.e, set callFrameForCatch and copy callee saves.
200 
201         jit-&gt;storePtr(GPRInfo::callFrameRegister, m_vm.addressOfCallFrameForCatch());
202         CCallHelpers::Jump jumpToOSRExitExceptionHandler = jit-&gt;jump();
203 
204         // We don&#39;t need to insert a new exception handler in the table
205         // because we&#39;re doing a manual exception check here. i.e, we&#39;ll
206         // never arrive here from genericUnwind().
207         HandlerInfo originalHandler = originalExceptionHandler();
208         jit-&gt;addLinkTask(
209             [=] (LinkBuffer&amp; linkBuffer) {
210                 linkBuffer.link(jumpToOSRExitExceptionHandler, originalHandler.nativeCode);
211             });
212     } else {
213         jit-&gt;setupArguments&lt;decltype(lookupExceptionHandler)&gt;(CCallHelpers::TrustedImmPtr(&amp;m_vm), GPRInfo::callFrameRegister);
214         CCallHelpers::Call lookupExceptionHandlerCall = jit-&gt;call(OperationPtrTag);
215         jit-&gt;addLinkTask(
216             [=] (LinkBuffer&amp; linkBuffer) {
217                 linkBuffer.link(lookupExceptionHandlerCall, FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandler));
218             });
219         jit-&gt;jumpToExceptionHandler(m_vm);
220     }
221 }
222 
223 
224 PolymorphicAccess::PolymorphicAccess() { }
225 PolymorphicAccess::~PolymorphicAccess() { }
226 
227 AccessGenerationResult PolymorphicAccess::addCases(
228     const GCSafeConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo,
229     const Identifier&amp; ident, Vector&lt;std::unique_ptr&lt;AccessCase&gt;, 2&gt; originalCasesToAdd)
230 {
231     SuperSamplerScope superSamplerScope(false);
232 
233     // This method will add the originalCasesToAdd to the list one at a time while preserving the
234     // invariants:
235     // - If a newly added case canReplace() any existing case, then the existing case is removed before
236     //   the new case is added. Removal doesn&#39;t change order of the list. Any number of existing cases
237     //   can be removed via the canReplace() rule.
238     // - Cases in the list always appear in ascending order of time of addition. Therefore, if you
239     //   cascade through the cases in reverse order, you will get the most recent cases first.
240     // - If this method fails (returns null, doesn&#39;t add the cases), then both the previous case list
241     //   and the previous stub are kept intact and the new cases are destroyed. It&#39;s OK to attempt to
242     //   add more things after failure.
243 
244     // First ensure that the originalCasesToAdd doesn&#39;t contain duplicates.
245     Vector&lt;std::unique_ptr&lt;AccessCase&gt;&gt; casesToAdd;
246     for (unsigned i = 0; i &lt; originalCasesToAdd.size(); ++i) {
247         std::unique_ptr&lt;AccessCase&gt; myCase = WTFMove(originalCasesToAdd[i]);
248 
249         // Add it only if it is not replaced by the subsequent cases in the list.
250         bool found = false;
251         for (unsigned j = i + 1; j &lt; originalCasesToAdd.size(); ++j) {
252             if (originalCasesToAdd[j]-&gt;canReplace(*myCase)) {
253                 found = true;
254                 break;
255             }
256         }
257 
258         if (found)
259             continue;
260 
261         casesToAdd.append(WTFMove(myCase));
262     }
263 
264     if (PolymorphicAccessInternal::verbose)
265         dataLog(&quot;casesToAdd: &quot;, listDump(casesToAdd), &quot;\n&quot;);
266 
267     // If there aren&#39;t any cases to add, then fail on the grounds that there&#39;s no point to generating a
268     // new stub that will be identical to the old one. Returning null should tell the caller to just
269     // keep doing what they were doing before.
270     if (casesToAdd.isEmpty())
271         return AccessGenerationResult::MadeNoChanges;
272 
273     if (stubInfo.accessType != AccessType::InstanceOf) {
274         bool shouldReset = false;
275         AccessGenerationResult resetResult(AccessGenerationResult::ResetStubAndFireWatchpoints);
276         auto considerPolyProtoReset = [&amp;] (Structure* a, Structure* b) {
277             if (Structure::shouldConvertToPolyProto(a, b)) {
278                 // For now, we only reset if this is our first time invalidating this watchpoint.
279                 // The reason we don&#39;t immediately fire this watchpoint is that we may be already
280                 // watching the poly proto watchpoint, which if fired, would destroy us. We let
281                 // the person handling the result to do a delayed fire.
282                 ASSERT(a-&gt;rareData()-&gt;sharedPolyProtoWatchpoint().get() == b-&gt;rareData()-&gt;sharedPolyProtoWatchpoint().get());
283                 if (a-&gt;rareData()-&gt;sharedPolyProtoWatchpoint()-&gt;isStillValid()) {
284                     shouldReset = true;
285                     resetResult.addWatchpointToFire(*a-&gt;rareData()-&gt;sharedPolyProtoWatchpoint(), StringFireDetail(&quot;Detected poly proto optimization opportunity.&quot;));
286                 }
287             }
288         };
289 
290         for (auto&amp; caseToAdd : casesToAdd) {
291             for (auto&amp; existingCase : m_list) {
292                 Structure* a = caseToAdd-&gt;structure();
293                 Structure* b = existingCase-&gt;structure();
294                 considerPolyProtoReset(a, b);
295             }
296         }
297         for (unsigned i = 0; i &lt; casesToAdd.size(); ++i) {
298             for (unsigned j = i + 1; j &lt; casesToAdd.size(); ++j) {
299                 Structure* a = casesToAdd[i]-&gt;structure();
300                 Structure* b = casesToAdd[j]-&gt;structure();
301                 considerPolyProtoReset(a, b);
302             }
303         }
304 
305         if (shouldReset)
306             return resetResult;
307     }
308 
309     // Now add things to the new list. Note that at this point, we will still have old cases that
310     // may be replaced by the new ones. That&#39;s fine. We will sort that out when we regenerate.
311     for (auto&amp; caseToAdd : casesToAdd) {
312         commit(locker, vm, m_watchpoints, codeBlock, stubInfo, ident, *caseToAdd);
313         m_list.append(WTFMove(caseToAdd));
314     }
315 
316     if (PolymorphicAccessInternal::verbose)
317         dataLog(&quot;After addCases: m_list: &quot;, listDump(m_list), &quot;\n&quot;);
318 
319     return AccessGenerationResult::Buffered;
320 }
321 
322 AccessGenerationResult PolymorphicAccess::addCase(
323     const GCSafeConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo,
324     const Identifier&amp; ident, std::unique_ptr&lt;AccessCase&gt; newAccess)
325 {
326     Vector&lt;std::unique_ptr&lt;AccessCase&gt;, 2&gt; newAccesses;
327     newAccesses.append(WTFMove(newAccess));
328     return addCases(locker, vm, codeBlock, stubInfo, ident, WTFMove(newAccesses));
329 }
330 
331 bool PolymorphicAccess::visitWeak(VM&amp; vm) const
332 {
333     for (unsigned i = 0; i &lt; size(); ++i) {
334         if (!at(i).visitWeak(vm))
335             return false;
336     }
337     if (Vector&lt;WriteBarrier&lt;JSCell&gt;&gt;* weakReferences = m_weakReferences.get()) {
338         for (WriteBarrier&lt;JSCell&gt;&amp; weakReference : *weakReferences) {
<a name="2" id="anc2"></a><span class="line-modified">339             if (!vm.heap.isMarked(weakReference.get()))</span>
340                 return false;
341         }
342     }
343     return true;
344 }
345 
346 bool PolymorphicAccess::propagateTransitions(SlotVisitor&amp; visitor) const
347 {
348     bool result = true;
349     for (unsigned i = 0; i &lt; size(); ++i)
350         result &amp;= at(i).propagateTransitions(visitor);
351     return result;
352 }
353 
354 void PolymorphicAccess::dump(PrintStream&amp; out) const
355 {
356     out.print(RawPointer(this), &quot;:[&quot;);
357     CommaPrinter comma;
358     for (auto&amp; entry : m_list)
359         out.print(comma, *entry);
360     out.print(&quot;]&quot;);
361 }
362 
363 void PolymorphicAccess::commit(
364     const GCSafeConcurrentJSLocker&amp;, VM&amp; vm, std::unique_ptr&lt;WatchpointsOnStructureStubInfo&gt;&amp; watchpoints, CodeBlock* codeBlock,
365     StructureStubInfo&amp; stubInfo, const Identifier&amp; ident, AccessCase&amp; accessCase)
366 {
367     // NOTE: We currently assume that this is relatively rare. It mainly arises for accesses to
368     // properties on DOM nodes. For sure we cache many DOM node accesses, but even in
369     // Real Pages (TM), we appear to spend most of our time caching accesses to properties on
370     // vanilla objects or exotic objects from within JSC (like Arguments, those are super popular).
371     // Those common kinds of JSC object accesses don&#39;t hit this case.
372 
373     for (WatchpointSet* set : accessCase.commit(vm, ident)) {
374         Watchpoint* watchpoint =
375             WatchpointsOnStructureStubInfo::ensureReferenceAndAddWatchpoint(
376                 watchpoints, codeBlock, &amp;stubInfo, ObjectPropertyCondition());
377 
378         set-&gt;add(watchpoint);
379     }
380 }
381 
382 AccessGenerationResult PolymorphicAccess::regenerate(
383     const GCSafeConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo, const Identifier&amp; ident)
384 {
385     SuperSamplerScope superSamplerScope(false);
386 
387     if (PolymorphicAccessInternal::verbose)
388         dataLog(&quot;Regenerate with m_list: &quot;, listDump(m_list), &quot;\n&quot;);
389 
390     AccessGenerationState state(vm, codeBlock-&gt;globalObject());
391 
392     state.access = this;
393     state.stubInfo = &amp;stubInfo;
394     state.ident = &amp;ident;
395 
396     state.baseGPR = stubInfo.baseGPR();
397     state.thisGPR = stubInfo.patch.thisGPR;
398     state.valueRegs = stubInfo.valueRegs();
399 
400     ScratchRegisterAllocator allocator(stubInfo.patch.usedRegisters);
401     state.allocator = &amp;allocator;
402     allocator.lock(state.baseGPR);
403     if (state.thisGPR != InvalidGPRReg)
404         allocator.lock(state.thisGPR);
405     allocator.lock(state.valueRegs);
406 #if USE(JSVALUE32_64)
407     allocator.lock(stubInfo.patch.baseTagGPR);
408 #endif
409 
410     state.scratchGPR = allocator.allocateScratchGPR();
411 
412     CCallHelpers jit(codeBlock);
413     state.jit = &amp;jit;
414 
415     state.preservedReusedRegisterState =
416         allocator.preserveReusedRegistersByPushing(jit, ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
417 
418     // Regenerating is our opportunity to figure out what our list of cases should look like. We
419     // do this here. The newly produced &#39;cases&#39; list may be smaller than m_list. We don&#39;t edit
420     // m_list in-place because we may still fail, in which case we want the PolymorphicAccess object
421     // to be unmutated. For sure, we want it to hang onto any data structures that may be referenced
422     // from the code of the current stub (aka previous).
423     ListType cases;
424     unsigned srcIndex = 0;
425     unsigned dstIndex = 0;
426     while (srcIndex &lt; m_list.size()) {
427         std::unique_ptr&lt;AccessCase&gt; someCase = WTFMove(m_list[srcIndex++]);
428 
429         // If the case had been generated, then we have to keep the original in m_list in case we
430         // fail to regenerate. That case may have data structures that are used by the code that it
431         // had generated. If the case had not been generated, then we want to remove it from m_list.
432         bool isGenerated = someCase-&gt;state() == AccessCase::Generated;
433 
434         [&amp;] () {
435             if (!someCase-&gt;couldStillSucceed())
436                 return;
437 
438             // Figure out if this is replaced by any later case. Given two cases A and B where A
439             // comes first in the case list, we know that A would have triggered first if we had
440             // generated the cases in a cascade. That&#39;s why this loop asks B-&gt;canReplace(A) but not
441             // A-&gt;canReplace(B). If A-&gt;canReplace(B) was true then A would never have requested
442             // repatching in cases where Repatch.cpp would have then gone on to generate B. If that
443             // did happen by some fluke, then we&#39;d just miss the redundancy here, which wouldn&#39;t be
444             // incorrect - just slow. However, if A&#39;s checks failed and Repatch.cpp concluded that
445             // this new condition could be handled by B and B-&gt;canReplace(A), then this says that we
446             // don&#39;t need A anymore.
447             //
448             // If we can generate a binary switch, then A-&gt;canReplace(B) == B-&gt;canReplace(A). So,
449             // it doesn&#39;t matter that we only do the check in one direction.
450             for (unsigned j = srcIndex; j &lt; m_list.size(); ++j) {
451                 if (m_list[j]-&gt;canReplace(*someCase))
452                     return;
453             }
454 
455             if (isGenerated)
456                 cases.append(someCase-&gt;clone());
457             else
458                 cases.append(WTFMove(someCase));
459         }();
460 
461         if (isGenerated)
462             m_list[dstIndex++] = WTFMove(someCase);
463     }
464     m_list.resize(dstIndex);
465 
466     bool generatedFinalCode = false;
467 
468     // If the resulting set of cases is so big that we would stop caching and this is InstanceOf,
469     // then we want to generate the generic InstanceOf and then stop.
470     if (cases.size() &gt;= Options::maxAccessVariantListSize()
471         &amp;&amp; stubInfo.accessType == AccessType::InstanceOf) {
472         while (!cases.isEmpty())
473             m_list.append(cases.takeLast());
474         cases.append(AccessCase::create(vm, codeBlock, AccessCase::InstanceOfGeneric));
475         generatedFinalCode = true;
476     }
477 
478     if (PolymorphicAccessInternal::verbose)
479         dataLog(&quot;Optimized cases: &quot;, listDump(cases), &quot;\n&quot;);
480 
481     // At this point we&#39;re convinced that &#39;cases&#39; contains the cases that we want to JIT now and we
482     // won&#39;t change that set anymore.
483 
484     bool allGuardedByStructureCheck = true;
485     bool hasJSGetterSetterCall = false;
486     for (auto&amp; newCase : cases) {
487         commit(locker, vm, state.watchpoints, codeBlock, stubInfo, ident, *newCase);
488         allGuardedByStructureCheck &amp;= newCase-&gt;guardedByStructureCheck();
489         if (newCase-&gt;type() == AccessCase::Getter || newCase-&gt;type() == AccessCase::Setter)
490             hasJSGetterSetterCall = true;
491     }
492 
493     if (cases.isEmpty()) {
494         // This is super unlikely, but we make it legal anyway.
495         state.failAndRepatch.append(jit.jump());
496     } else if (!allGuardedByStructureCheck || cases.size() == 1) {
497         // If there are any proxies in the list, we cannot just use a binary switch over the structure.
498         // We need to resort to a cascade. A cascade also happens to be optimal if we only have just
499         // one case.
500         CCallHelpers::JumpList fallThrough;
501 
502         // Cascade through the list, preferring newer entries.
503         for (unsigned i = cases.size(); i--;) {
504             fallThrough.link(&amp;jit);
505             fallThrough.clear();
506             cases[i]-&gt;generateWithGuard(state, fallThrough);
507         }
508         state.failAndRepatch.append(fallThrough);
509     } else {
510         jit.load32(
511             CCallHelpers::Address(state.baseGPR, JSCell::structureIDOffset()),
512             state.scratchGPR);
513 
514         Vector&lt;int64_t&gt; caseValues(cases.size());
515         for (unsigned i = 0; i &lt; cases.size(); ++i)
516             caseValues[i] = bitwise_cast&lt;int32_t&gt;(cases[i]-&gt;structure()-&gt;id());
517 
518         BinarySwitch binarySwitch(state.scratchGPR, caseValues, BinarySwitch::Int32);
519         while (binarySwitch.advance(jit))
520             cases[binarySwitch.caseIndex()]-&gt;generate(state);
521         state.failAndRepatch.append(binarySwitch.fallThrough());
522     }
523 
524     if (!state.failAndIgnore.empty()) {
525         state.failAndIgnore.link(&amp;jit);
526 
527         // Make sure that the inline cache optimization code knows that we are taking slow path because
528         // of something that isn&#39;t patchable. The slow path will decrement &quot;countdown&quot; and will only
529         // patch things if the countdown reaches zero. We increment the slow path count here to ensure
530         // that the slow path does not try to patch.
531 #if CPU(X86) || CPU(X86_64)
532         jit.move(CCallHelpers::TrustedImmPtr(&amp;stubInfo.countdown), state.scratchGPR);
533         jit.add8(CCallHelpers::TrustedImm32(1), CCallHelpers::Address(state.scratchGPR));
534 #else
535         jit.load8(&amp;stubInfo.countdown, state.scratchGPR);
536         jit.add32(CCallHelpers::TrustedImm32(1), state.scratchGPR);
537         jit.store8(state.scratchGPR, &amp;stubInfo.countdown);
538 #endif
539     }
540 
541     CCallHelpers::JumpList failure;
542     if (allocator.didReuseRegisters()) {
543         state.failAndRepatch.link(&amp;jit);
544         state.restoreScratch();
545     } else
546         failure = state.failAndRepatch;
547     failure.append(jit.jump());
548 
549     CodeBlock* codeBlockThatOwnsExceptionHandlers = nullptr;
550     DisposableCallSiteIndex callSiteIndexForExceptionHandling;
551     if (state.needsToRestoreRegistersIfException() &amp;&amp; hasJSGetterSetterCall) {
552         // Emit the exception handler.
553         // Note that this code is only reachable when doing genericUnwind from a pure JS getter/setter .
554         // Note also that this is not reachable from custom getter/setter. Custom getter/setters will have
555         // their own exception handling logic that doesn&#39;t go through genericUnwind.
556         MacroAssembler::Label makeshiftCatchHandler = jit.label();
557 
558         int stackPointerOffset = codeBlock-&gt;stackPointerOffset() * sizeof(EncodedJSValue);
559         AccessGenerationState::SpillState spillStateForJSGetterSetter = state.spillStateForJSGetterSetter();
560         ASSERT(!spillStateForJSGetterSetter.isEmpty());
561         stackPointerOffset -= state.preservedReusedRegisterState.numberOfBytesPreserved;
562         stackPointerOffset -= spillStateForJSGetterSetter.numberOfStackBytesUsedForRegisterPreservation;
563 
564         jit.loadPtr(vm.addressOfCallFrameForCatch(), GPRInfo::callFrameRegister);
565         jit.addPtr(CCallHelpers::TrustedImm32(stackPointerOffset), GPRInfo::callFrameRegister, CCallHelpers::stackPointerRegister);
566 
567         state.restoreLiveRegistersFromStackForCallWithThrownException(spillStateForJSGetterSetter);
568         state.restoreScratch();
569         CCallHelpers::Jump jumpToOSRExitExceptionHandler = jit.jump();
570 
571         HandlerInfo oldHandler = state.originalExceptionHandler();
572         DisposableCallSiteIndex newExceptionHandlingCallSite = state.callSiteIndexForExceptionHandling();
573         jit.addLinkTask(
574             [=] (LinkBuffer&amp; linkBuffer) {
575                 linkBuffer.link(jumpToOSRExitExceptionHandler, oldHandler.nativeCode);
576 
577                 HandlerInfo handlerToRegister = oldHandler;
578                 handlerToRegister.nativeCode = linkBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(makeshiftCatchHandler);
579                 handlerToRegister.start = newExceptionHandlingCallSite.bits();
580                 handlerToRegister.end = newExceptionHandlingCallSite.bits() + 1;
581                 codeBlock-&gt;appendExceptionHandler(handlerToRegister);
582             });
583 
584         // We set these to indicate to the stub to remove itself from the CodeBlock&#39;s
585         // exception handler table when it is deallocated.
586         codeBlockThatOwnsExceptionHandlers = codeBlock;
587         ASSERT(JITCode::isOptimizingJIT(codeBlockThatOwnsExceptionHandlers-&gt;jitType()));
588         callSiteIndexForExceptionHandling = state.callSiteIndexForExceptionHandling();
589     }
590 
591     LinkBuffer linkBuffer(jit, codeBlock, JITCompilationCanFail);
592     if (linkBuffer.didFailToAllocate()) {
593         if (PolymorphicAccessInternal::verbose)
594             dataLog(&quot;Did fail to allocate.\n&quot;);
595         return AccessGenerationResult::GaveUp;
596     }
597 
598     CodeLocationLabel&lt;JSInternalPtrTag&gt; successLabel = stubInfo.doneLocation();
599 
600     linkBuffer.link(state.success, successLabel);
601 
602     linkBuffer.link(failure, stubInfo.slowPathStartLocation());
603 
604     if (PolymorphicAccessInternal::verbose)
605         dataLog(FullCodeOrigin(codeBlock, stubInfo.codeOrigin), &quot;: Generating polymorphic access stub for &quot;, listDump(cases), &quot;\n&quot;);
606 
607     MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; code = FINALIZE_CODE_FOR(
608         codeBlock, linkBuffer, JITStubRoutinePtrTag,
609         &quot;%s&quot;, toCString(&quot;Access stub for &quot;, *codeBlock, &quot; &quot;, stubInfo.codeOrigin, &quot; with return point &quot;, successLabel, &quot;: &quot;, listDump(cases)).data());
610 
611     bool doesCalls = false;
612     Vector&lt;JSCell*&gt; cellsToMark;
613     for (auto&amp; entry : cases)
614         doesCalls |= entry-&gt;doesCalls(&amp;cellsToMark);
615 
616     m_stubRoutine = createJITStubRoutine(code, vm, codeBlock, doesCalls, cellsToMark, codeBlockThatOwnsExceptionHandlers, callSiteIndexForExceptionHandling);
617     m_watchpoints = WTFMove(state.watchpoints);
618     if (!state.weakReferences.isEmpty())
<a name="3" id="anc3"></a><span class="line-modified">619         m_weakReferences = makeUnique&lt;Vector&lt;WriteBarrier&lt;JSCell&gt;&gt;&gt;(WTFMove(state.weakReferences));</span>
620     if (PolymorphicAccessInternal::verbose)
621         dataLog(&quot;Returning: &quot;, code.code(), &quot;\n&quot;);
622 
623     m_list = WTFMove(cases);
624 
625     AccessGenerationResult::Kind resultKind;
626     if (m_list.size() &gt;= Options::maxAccessVariantListSize() || generatedFinalCode)
627         resultKind = AccessGenerationResult::GeneratedFinalCode;
628     else
629         resultKind = AccessGenerationResult::GeneratedNewCode;
630 
631     return AccessGenerationResult(resultKind, code.code());
632 }
633 
634 void PolymorphicAccess::aboutToDie()
635 {
636     if (m_stubRoutine)
637         m_stubRoutine-&gt;aboutToDie();
638 }
639 
640 } // namespace JSC
641 
642 namespace WTF {
643 
644 using namespace JSC;
645 
646 void printInternal(PrintStream&amp; out, AccessGenerationResult::Kind kind)
647 {
648     switch (kind) {
649     case AccessGenerationResult::MadeNoChanges:
650         out.print(&quot;MadeNoChanges&quot;);
651         return;
652     case AccessGenerationResult::GaveUp:
653         out.print(&quot;GaveUp&quot;);
654         return;
655     case AccessGenerationResult::Buffered:
656         out.print(&quot;Buffered&quot;);
657         return;
658     case AccessGenerationResult::GeneratedNewCode:
659         out.print(&quot;GeneratedNewCode&quot;);
660         return;
661     case AccessGenerationResult::GeneratedFinalCode:
662         out.print(&quot;GeneratedFinalCode&quot;);
663         return;
664     case AccessGenerationResult::ResetStubAndFireWatchpoints:
665         out.print(&quot;ResetStubAndFireWatchpoints&quot;);
666         return;
667     }
668 
669     RELEASE_ASSERT_NOT_REACHED();
670 }
671 
672 void printInternal(PrintStream&amp; out, AccessCase::AccessType type)
673 {
674     switch (type) {
675     case AccessCase::Load:
676         out.print(&quot;Load&quot;);
677         return;
678     case AccessCase::Transition:
679         out.print(&quot;Transition&quot;);
680         return;
681     case AccessCase::Replace:
682         out.print(&quot;Replace&quot;);
683         return;
684     case AccessCase::Miss:
685         out.print(&quot;Miss&quot;);
686         return;
687     case AccessCase::GetGetter:
688         out.print(&quot;GetGetter&quot;);
689         return;
690     case AccessCase::Getter:
691         out.print(&quot;Getter&quot;);
692         return;
693     case AccessCase::Setter:
694         out.print(&quot;Setter&quot;);
695         return;
696     case AccessCase::CustomValueGetter:
697         out.print(&quot;CustomValueGetter&quot;);
698         return;
699     case AccessCase::CustomAccessorGetter:
700         out.print(&quot;CustomAccessorGetter&quot;);
701         return;
702     case AccessCase::CustomValueSetter:
703         out.print(&quot;CustomValueSetter&quot;);
704         return;
705     case AccessCase::CustomAccessorSetter:
706         out.print(&quot;CustomAccessorSetter&quot;);
707         return;
708     case AccessCase::IntrinsicGetter:
709         out.print(&quot;IntrinsicGetter&quot;);
710         return;
711     case AccessCase::InHit:
712         out.print(&quot;InHit&quot;);
713         return;
714     case AccessCase::InMiss:
715         out.print(&quot;InMiss&quot;);
716         return;
717     case AccessCase::ArrayLength:
718         out.print(&quot;ArrayLength&quot;);
719         return;
720     case AccessCase::StringLength:
721         out.print(&quot;StringLength&quot;);
722         return;
723     case AccessCase::DirectArgumentsLength:
724         out.print(&quot;DirectArgumentsLength&quot;);
725         return;
726     case AccessCase::ScopedArgumentsLength:
727         out.print(&quot;ScopedArgumentsLength&quot;);
728         return;
729     case AccessCase::ModuleNamespaceLoad:
730         out.print(&quot;ModuleNamespaceLoad&quot;);
731         return;
732     case AccessCase::InstanceOfHit:
733         out.print(&quot;InstanceOfHit&quot;);
734         return;
735     case AccessCase::InstanceOfMiss:
736         out.print(&quot;InstanceOfMiss&quot;);
737         return;
738     case AccessCase::InstanceOfGeneric:
739         out.print(&quot;InstanceOfGeneric&quot;);
740         return;
741     }
742 
743     RELEASE_ASSERT_NOT_REACHED();
744 }
745 
746 void printInternal(PrintStream&amp; out, AccessCase::State state)
747 {
748     switch (state) {
749     case AccessCase::Primordial:
750         out.print(&quot;Primordial&quot;);
751         return;
752     case AccessCase::Committed:
753         out.print(&quot;Committed&quot;);
754         return;
755     case AccessCase::Generated:
756         out.print(&quot;Generated&quot;);
757         return;
758     }
759 
760     RELEASE_ASSERT_NOT_REACHED();
761 }
762 
763 } // namespace WTF
764 
765 #endif // ENABLE(JIT)
766 
767 
<a name="4" id="anc4"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="4" type="hidden" />
</body>
</html>