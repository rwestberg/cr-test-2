<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/air/AirGenerate.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2015-2017 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;AirGenerate.h&quot;
 28 
 29 #if ENABLE(B3_JIT)
 30 
 31 #include &quot;AirAllocateRegistersAndStackAndGenerateCode.h&quot;
 32 #include &quot;AirAllocateRegistersAndStackByLinearScan.h&quot;
 33 #include &quot;AirAllocateRegistersByGraphColoring.h&quot;
 34 #include &quot;AirAllocateStackByGraphColoring.h&quot;
 35 #include &quot;AirCode.h&quot;
 36 #include &quot;AirEliminateDeadCode.h&quot;
 37 #include &quot;AirFixObviousSpills.h&quot;
 38 #include &quot;AirFixPartialRegisterStalls.h&quot;
 39 #include &quot;AirGenerationContext.h&quot;
 40 #include &quot;AirHandleCalleeSaves.h&quot;
 41 #include &quot;AirLiveness.h&quot;
 42 #include &quot;AirLogRegisterPressure.h&quot;
 43 #include &quot;AirLowerAfterRegAlloc.h&quot;
 44 #include &quot;AirLowerEntrySwitch.h&quot;
 45 #include &quot;AirLowerMacros.h&quot;
 46 #include &quot;AirLowerStackArgs.h&quot;
 47 #include &quot;AirOpcodeUtils.h&quot;
 48 #include &quot;AirOptimizeBlockOrder.h&quot;
 49 #include &quot;AirReportUsedRegisters.h&quot;
 50 #include &quot;AirSimplifyCFG.h&quot;
 51 #include &quot;AirStackAllocation.h&quot;
 52 #include &quot;AirTmpMap.h&quot;
 53 #include &quot;AirValidate.h&quot;
 54 #include &quot;B3Common.h&quot;
 55 #include &quot;B3Procedure.h&quot;
 56 #include &quot;B3TimingScope.h&quot;
 57 #include &quot;B3ValueInlines.h&quot;
 58 #include &quot;CCallHelpers.h&quot;
 59 #include &quot;DisallowMacroScratchRegisterUsage.h&quot;
 60 #include &quot;LinkBuffer.h&quot;
 61 #include &lt;wtf/IndexMap.h&gt;
 62 
 63 namespace JSC { namespace B3 { namespace Air {
 64 
 65 void prepareForGeneration(Code&amp; code)
 66 {
 67     TimingScope timingScope(&quot;Air::prepareForGeneration&quot;);
 68 
 69     // If we&#39;re doing super verbose dumping, the phase scope of any phase will already do a dump.
 70     if (shouldDumpIR(AirMode) &amp;&amp; !shouldDumpIRAtEachPhase(AirMode)) {
 71         dataLog(&quot;Initial air:\n&quot;);
 72         dataLog(code);
 73     }
 74 
 75     // We don&#39;t expect the incoming code to have predecessors computed.
 76     code.resetReachability();
 77 
 78     if (shouldValidateIR())
 79         validate(code);
 80 
 81     if (!code.optLevel()) {
 82         lowerMacros(code);
 83 
 84         // FIXME: The name of this phase doesn&#39;t make much sense in O0 since we do this before
 85         // register allocation.
 86         lowerAfterRegAlloc(code);
 87 
 88         // Actually create entrypoints.
 89         lowerEntrySwitch(code);
 90 
 91         // This sorts the basic blocks in Code to achieve an ordering that maximizes the likelihood that a high
 92         // frequency successor is also the fall-through target.
 93         optimizeBlockOrder(code);
 94 
 95         if (shouldValidateIR())
 96             validate(code);
 97 
 98         if (shouldDumpIR(AirMode)) {
 99             dataLog(&quot;Air after &quot;, code.lastPhaseName(), &quot;, before generation:\n&quot;);
100             dataLog(code);
101         }
102 
103         code.m_generateAndAllocateRegisters = makeUnique&lt;GenerateAndAllocateRegisters&gt;(code);
104         code.m_generateAndAllocateRegisters-&gt;prepareForGeneration();
105 
106         return;
107     }
108 
109     simplifyCFG(code);
110 
111     lowerMacros(code);
112 
113     // This is where we run our optimizations and transformations.
114     // FIXME: Add Air optimizations.
115     // https://bugs.webkit.org/show_bug.cgi?id=150456
116 
117     eliminateDeadCode(code);
118 
119     if (code.optLevel() == 1) {
120         // When we&#39;re compiling quickly, we do register and stack allocation in one linear scan
121         // phase. It&#39;s fast because it computes liveness only once.
122         allocateRegistersAndStackByLinearScan(code);
123 
124         if (Options::logAirRegisterPressure()) {
125             dataLog(&quot;Register pressure after register allocation:\n&quot;);
126             logRegisterPressure(code);
127         }
128 
129         // We may still need to do post-allocation lowering. Doing it after both register and
130         // stack allocation is less optimal, but it works fine.
131         lowerAfterRegAlloc(code);
132     } else {
133         // NOTE: B3 -O2 generates code that runs 1.5x-2x faster than code generated by -O1.
134         // Most of this performance benefit is from -O2&#39;s graph coloring register allocation
135         // and stack allocation pipeline, which you see below.
136 
137         // Register allocation for all the Tmps that do not have a corresponding machine
138         // register. After this phase, every Tmp has a reg.
139         allocateRegistersByGraphColoring(code);
140 
141         if (Options::logAirRegisterPressure()) {
142             dataLog(&quot;Register pressure after register allocation:\n&quot;);
143             logRegisterPressure(code);
144         }
145 
146         // This replaces uses of spill slots with registers or constants if possible. It
147         // does this by minimizing the amount that we perturb the already-chosen register
148         // allocation. It may extend the live ranges of registers though.
149         fixObviousSpills(code);
150 
151         lowerAfterRegAlloc(code);
152 
153         // This does first-fit allocation of stack slots using an interference graph plus a
154         // bunch of other optimizations.
155         allocateStackByGraphColoring(code);
156     }
157 
158     // This turns all Stack and CallArg Args into Addr args that use the frame pointer.
159     lowerStackArgs(code);
160 
161     // If we coalesced moves then we can unbreak critical edges. This is the main reason for this
162     // phase.
163     simplifyCFG(code);
164 
165     // This is needed to satisfy a requirement of B3::StackmapValue. This also removes dead
166     // code. We can avoid running this when certain optimizations are disabled.
167     if (code.optLevel() &gt;= 2 || code.needsUsedRegisters())
168         reportUsedRegisters(code);
169 
170     // Attempt to remove false dependencies between instructions created by partial register changes.
171     // This must be executed as late as possible as it depends on the instructions order and register
172     // use. We _must_ run this after reportUsedRegisters(), since that kills variable assignments
173     // that seem dead. Luckily, this phase does not change register liveness, so that&#39;s OK.
174     fixPartialRegisterStalls(code);
175 
176     // Actually create entrypoints.
177     lowerEntrySwitch(code);
178 
179     // The control flow graph can be simplified further after we have lowered EntrySwitch.
180     simplifyCFG(code);
181 
182     // This sorts the basic blocks in Code to achieve an ordering that maximizes the likelihood that a high
183     // frequency successor is also the fall-through target.
184     optimizeBlockOrder(code);
185 
186     if (shouldValidateIR())
187         validate(code);
188 
189     // Do a final dump of Air. Note that we have to do this even if we are doing per-phase dumping,
190     // since the final generation is not a phase.
191     if (shouldDumpIR(AirMode)) {
192         dataLog(&quot;Air after &quot;, code.lastPhaseName(), &quot;, before generation:\n&quot;);
193         dataLog(code);
194     }
195 }
196 
197 static void generateWithAlreadyAllocatedRegisters(Code&amp; code, CCallHelpers&amp; jit)
198 {
199     TimingScope timingScope(&quot;Air::generate&quot;);
200 
201     DisallowMacroScratchRegisterUsage disallowScratch(jit);
202 
203     // And now, we generate code.
204     GenerationContext context;
205     context.code = &amp;code;
206     context.blockLabels.resize(code.size());
207     for (BasicBlock* block : code)
208         context.blockLabels[block] = Box&lt;CCallHelpers::Label&gt;::create();
209     IndexMap&lt;BasicBlock*, CCallHelpers::JumpList&gt; blockJumps(code.size());
210 
211     auto link = [&amp;] (CCallHelpers::Jump jump, BasicBlock* target) {
212         if (context.blockLabels[target]-&gt;isSet()) {
213             jump.linkTo(*context.blockLabels[target], &amp;jit);
214             return;
215         }
216 
217         blockJumps[target].append(jump);
218     };
219 
220     PCToOriginMap&amp; pcToOriginMap = code.proc().pcToOriginMap();
221     auto addItem = [&amp;] (Inst&amp; inst) {
222         if (!inst.origin) {
223             pcToOriginMap.appendItem(jit.labelIgnoringWatchpoints(), Origin());
224             return;
225         }
226         pcToOriginMap.appendItem(jit.labelIgnoringWatchpoints(), inst.origin-&gt;origin());
227     };
228 
229     Disassembler* disassembler = code.disassembler();
230 
231     for (BasicBlock* block : code) {
232         context.currentBlock = block;
233         context.indexInBlock = UINT_MAX;
234         blockJumps[block].link(&amp;jit);
235         CCallHelpers::Label label = jit.label();
236         *context.blockLabels[block] = label;
237 
238         if (disassembler)
239             disassembler-&gt;startBlock(block, jit);
240 
241         if (Optional&lt;unsigned&gt; entrypointIndex = code.entrypointIndex(block)) {
242             ASSERT(code.isEntrypoint(block));
243 
244             if (disassembler)
245                 disassembler-&gt;startEntrypoint(jit);
246 
247             code.prologueGeneratorForEntrypoint(*entrypointIndex)-&gt;run(jit, code);
248 
249             if (disassembler)
250                 disassembler-&gt;endEntrypoint(jit);
251         } else
252             ASSERT(!code.isEntrypoint(block));
253 
254         ASSERT(block-&gt;size() &gt;= 1);
255         for (unsigned i = 0; i &lt; block-&gt;size() - 1; ++i) {
256             context.indexInBlock = i;
257             Inst&amp; inst = block-&gt;at(i);
258             addItem(inst);
259             auto start = jit.labelIgnoringWatchpoints();
260             CCallHelpers::Jump jump = inst.generate(jit, context);
261             ASSERT_UNUSED(jump, !jump.isSet());
262             auto end = jit.labelIgnoringWatchpoints();
263             if (disassembler)
264                 disassembler-&gt;addInst(&amp;inst, start, end);
265         }
266 
267         context.indexInBlock = block-&gt;size() - 1;
268 
269         if (block-&gt;last().kind.opcode == Jump
270             &amp;&amp; block-&gt;successorBlock(0) == code.findNextBlock(block))
271             continue;
272 
273         addItem(block-&gt;last());
274 
275         if (isReturn(block-&gt;last().kind.opcode)) {
276             // We currently don&#39;t represent the full prologue/epilogue in Air, so we need to
277             // have this override.
278             auto start = jit.labelIgnoringWatchpoints();
279             if (code.frameSize()) {
280                 jit.emitRestore(code.calleeSaveRegisterAtOffsetList());
281                 jit.emitFunctionEpilogue();
282             } else
283                 jit.emitFunctionEpilogueWithEmptyFrame();
284             jit.ret();
285             addItem(block-&gt;last());
286             auto end = jit.labelIgnoringWatchpoints();
287             if (disassembler)
288                 disassembler-&gt;addInst(&amp;block-&gt;last(), start, end);
289             continue;
290         }
291 
292         auto start = jit.labelIgnoringWatchpoints();
293         CCallHelpers::Jump jump = block-&gt;last().generate(jit, context);
294         auto end = jit.labelIgnoringWatchpoints();
295         if (disassembler)
296             disassembler-&gt;addInst(&amp;block-&gt;last(), start, end);
297 
298         // The jump won&#39;t be set for patchpoints. It won&#39;t be set for Oops because then it won&#39;t have
299         // any successors.
300         if (jump.isSet()) {
301             switch (block-&gt;numSuccessors()) {
302             case 1:
303                 link(jump, block-&gt;successorBlock(0));
304                 break;
305             case 2:
306                 link(jump, block-&gt;successorBlock(0));
307                 if (block-&gt;successorBlock(1) != code.findNextBlock(block))
308                     link(jit.jump(), block-&gt;successorBlock(1));
309                 break;
310             default:
311                 RELEASE_ASSERT_NOT_REACHED();
312                 break;
313             }
314         }
315         addItem(block-&gt;last());
316     }
317 
318     context.currentBlock = nullptr;
319     context.indexInBlock = UINT_MAX;
320 
321     Vector&lt;CCallHelpers::Label&gt; entrypointLabels(code.numEntrypoints());
322     for (unsigned i = code.numEntrypoints(); i--;)
323         entrypointLabels[i] = *context.blockLabels[code.entrypoint(i).block()];
324     code.setEntrypointLabels(WTFMove(entrypointLabels));
325 
326     pcToOriginMap.appendItem(jit.label(), Origin());
327     // FIXME: Make late paths have Origins: https://bugs.webkit.org/show_bug.cgi?id=153689
328     if (disassembler)
329         disassembler-&gt;startLatePath(jit);
330 
331     for (auto&amp; latePath : context.latePaths)
332         latePath-&gt;run(jit, context);
333 
334     if (disassembler)
335         disassembler-&gt;endLatePath(jit);
336     pcToOriginMap.appendItem(jit.labelIgnoringWatchpoints(), Origin());
337 }
338 
339 void generate(Code&amp; code, CCallHelpers&amp; jit)
340 {
341     if (code.optLevel())
342         generateWithAlreadyAllocatedRegisters(code, jit);
343     else
344         code.m_generateAndAllocateRegisters-&gt;generate(jit);
345 }
346 
347 } } } // namespace JSC::B3::Air
348 
349 #endif // ENABLE(B3_JIT)
    </pre>
  </body>
</html>