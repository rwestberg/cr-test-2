<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter.asm</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 # Copyright (C) 2011-2019 Apple Inc. All rights reserved.
   2 #
   3 # Redistribution and use in source and binary forms, with or without
   4 # modification, are permitted provided that the following conditions
   5 # are met:
   6 # 1. Redistributions of source code must retain the above copyright
   7 #    notice, this list of conditions and the following disclaimer.
   8 # 2. Redistributions in binary form must reproduce the above copyright
   9 #    notice, this list of conditions and the following disclaimer in the
  10 #    documentation and/or other materials provided with the distribution.
  11 #
  12 # THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
  13 # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
  14 # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  15 # PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
  16 # BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  17 # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  18 # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  19 # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  20 # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  21 # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
  22 # THE POSSIBILITY OF SUCH DAMAGE.
  23 
  24 # Crash course on the language that this is written in (which I just call
  25 # &quot;assembly&quot; even though it&#39;s more than that):
  26 #
  27 # - Mostly gas-style operand ordering. The last operand tends to be the
  28 #   destination. So &quot;a := b&quot; is written as &quot;mov b, a&quot;. But unlike gas,
  29 #   comparisons are in-order, so &quot;if (a &lt; b)&quot; is written as
  30 #   &quot;bilt a, b, ...&quot;.
  31 #
  32 # - &quot;b&quot; = byte, &quot;h&quot; = 16-bit word, &quot;i&quot; = 32-bit word, &quot;p&quot; = pointer.
  33 #   For 32-bit, &quot;i&quot; and &quot;p&quot; are interchangeable except when an op supports one
  34 #   but not the other.
  35 #
  36 # - In general, valid operands for macro invocations and instructions are
  37 #   registers (eg &quot;t0&quot;), addresses (eg &quot;4[t0]&quot;), base-index addresses
  38 #   (eg &quot;7[t0, t1, 2]&quot;), absolute addresses (eg &quot;0xa0000000[]&quot;), or labels
  39 #   (eg &quot;_foo&quot; or &quot;.foo&quot;). Macro invocations can also take anonymous
  40 #   macros as operands. Instructions cannot take anonymous macros.
  41 #
  42 # - Labels must have names that begin with either &quot;_&quot; or &quot;.&quot;.  A &quot;.&quot; label
  43 #   is local and gets renamed before code gen to minimize namespace
  44 #   pollution. A &quot;_&quot; label is an extern symbol (i.e. &quot;.globl&quot;). The &quot;_&quot;
  45 #   may or may not be removed during code gen depending on whether the asm
  46 #   conventions for C name mangling on the target platform mandate a &quot;_&quot;
  47 #   prefix.
  48 #
  49 # - A &quot;macro&quot; is a lambda expression, which may be either anonymous or
  50 #   named. But this has caveats. &quot;macro&quot; can take zero or more arguments,
  51 #   which may be macros or any valid operands, but it can only return
  52 #   code. But you can do Turing-complete things via continuation passing
  53 #   style: &quot;macro foo (a, b) b(a, a) end foo(foo, foo)&quot;. Actually, don&#39;t do
  54 #   that, since you&#39;ll just crash the assembler.
  55 #
  56 # - An &quot;if&quot; is a conditional on settings. Any identifier supplied in the
  57 #   predicate of an &quot;if&quot; is assumed to be a #define that is available
  58 #   during code gen. So you can&#39;t use &quot;if&quot; for computation in a macro, but
  59 #   you can use it to select different pieces of code for different
  60 #   platforms.
  61 #
  62 # - Arguments to macros follow lexical scoping rather than dynamic scoping.
  63 #   Const&#39;s also follow lexical scoping and may override (hide) arguments
  64 #   or other consts. All variables (arguments and constants) can be bound
  65 #   to operands. Additionally, arguments (but not constants) can be bound
  66 #   to macros.
  67 
  68 # The following general-purpose registers are available:
  69 #
  70 #  - cfr and sp hold the call frame and (native) stack pointer respectively.
  71 #  They are callee-save registers, and guaranteed to be distinct from all other
  72 #  registers on all architectures.
  73 #
  74 #  - lr is defined on non-X86 architectures (ARM64, ARM64E, ARMv7, MIPS and CLOOP)
  75 #  and holds the return PC
  76 #
  77 #  - pc holds the (native) program counter on 32-bits ARM architectures (ARMv7)
  78 #
  79 #  - t0, t1, t2, t3, t4 and optionally t5 are temporary registers that can get trashed on
  80 #  calls, and are pairwise distinct registers. t4 holds the JS program counter, so use
  81 #  with caution in opcodes (actually, don&#39;t use it in opcodes at all, except as PC).
  82 #
  83 #  - r0 and r1 are the platform&#39;s customary return registers, and thus are
  84 #  two distinct registers
  85 #
  86 #  - a0, a1, a2 and a3 are the platform&#39;s customary argument registers, and
  87 #  thus are pairwise distinct registers. Be mindful that:
  88 #    + On X86, there are no argument registers. a0 and a1 are edx and
  89 #    ecx following the fastcall convention, but you should still use the stack
  90 #    to pass your arguments. The cCall2 and cCall4 macros do this for you.
  91 #    + On X86_64_WIN, you should allocate space on the stack for the arguments,
  92 #    and the return convention is weird for &gt; 8 bytes types. The only place we
  93 #    use &gt; 8 bytes return values is on a cCall, and cCall2 and cCall4 handle
  94 #    this for you.
  95 #
  96 #  - The only registers guaranteed to be caller-saved are r0, r1, a0, a1 and a2, and
  97 #  you should be mindful of that in functions that are called directly from C.
  98 #  If you need more registers, you should push and pop them like a good
  99 #  assembly citizen, because any other register will be callee-saved on X86.
 100 #
 101 # You can additionally assume:
 102 #
 103 #  - a3, t2, t3, t4 and t5 are never return registers; t0, t1, a0, a1 and a2
 104 #  can be return registers.
 105 #
 106 #  - t4 and t5 are never argument registers, t3 can only be a3, t1 can only be
 107 #  a1; but t0 and t2 can be either a0 or a2.
 108 #
 109 #  - On 64 bits, there are callee-save registers named csr0, csr1, ... csrN.
 110 #  The last three csr registers are used used to store the PC base and
 111 #  two special tag values. Don&#39;t use them for anything else.
 112 #
 113 # Additional platform-specific details (you shouldn&#39;t rely on this remaining
 114 # true):
 115 #
 116 #  - For consistency with the baseline JIT, t0 is always r0 (and t1 is always
 117 #  r1 on 32 bits platforms). You should use the r version when you need return
 118 #  registers, and the t version otherwise: code using t0 (or t1) should still
 119 #  work if swapped with e.g. t3, while code using r0 (or r1) should not. There
 120 #  *may* be legacy code relying on this.
 121 #
 122 #  - On all platforms other than X86, t0 can only be a0 and t2 can only be a2.
 123 #
 124 #  - On all platforms other than X86 and X86_64, a2 is not a return register.
 125 #  a2 is r0 on X86 (because we have so few registers) and r1 on X86_64 (because
 126 #  the ABI enforces it).
 127 #
 128 # The following floating-point registers are available:
 129 #
 130 #  - ft0-ft5 are temporary floating-point registers that get trashed on calls,
 131 #  and are pairwise distinct.
 132 #
 133 #  - fa0 and fa1 are the platform&#39;s customary floating-point argument
 134 #  registers, and are both distinct. On 64-bits platforms, fa2 and fa3 are
 135 #  additional floating-point argument registers.
 136 #
 137 #  - fr is the platform&#39;s customary floating-point return register
 138 #
 139 # You can assume that ft1-ft5 or fa1-fa3 are never fr, and that ftX is never
 140 # faY if X != Y.
 141 
 142 # First come the common protocols that both interpreters use. Note that each
 143 # of these must have an ASSERT() in LLIntData.cpp
 144 
 145 # Work-around for the fact that the toolchain&#39;s awareness of armv7k / armv7s
 146 # results in a separate slab in the fat binary, yet the offlineasm doesn&#39;t know
 147 # to expect it.
 148 if ARMv7k
 149 end
 150 if ARMv7s
 151 end
 152 
 153 # These declarations must match interpreter/JSStack.h.
 154 
 155 const PtrSize = constexpr (sizeof(void*))
 156 const MachineRegisterSize = constexpr (sizeof(CPURegister))
 157 const SlotSize = constexpr (sizeof(Register))
 158 
 159 if JSVALUE64
 160     const CallFrameHeaderSlots = 5
 161 else
 162     const CallFrameHeaderSlots = 4
 163     const CallFrameAlignSlots = 1
 164 end
 165 
 166 const JSLexicalEnvironment_variables = (sizeof JSLexicalEnvironment + SlotSize - 1) &amp; ~(SlotSize - 1)
 167 const DirectArguments_storage = (sizeof DirectArguments + SlotSize - 1) &amp; ~(SlotSize - 1)
 168 
 169 const StackAlignment = constexpr (stackAlignmentBytes())
 170 const StackAlignmentSlots = constexpr (stackAlignmentRegisters())
 171 const StackAlignmentMask = StackAlignment - 1
 172 
 173 const CallerFrameAndPCSize = constexpr (sizeof(CallerFrameAndPC))
 174 
 175 const CallerFrame = 0
 176 const ReturnPC = CallerFrame + MachineRegisterSize
 177 const CodeBlock = ReturnPC + MachineRegisterSize
 178 const Callee = CodeBlock + SlotSize
 179 const ArgumentCount = Callee + SlotSize
 180 const ThisArgumentOffset = ArgumentCount + SlotSize
 181 const FirstArgumentOffset = ThisArgumentOffset + SlotSize
 182 const CallFrameHeaderSize = ThisArgumentOffset
 183 
 184 # Some value representation constants.
 185 if JSVALUE64
 186     const TagBitTypeOther = constexpr TagBitTypeOther
 187     const TagBitBool      = constexpr TagBitBool
 188     const TagBitUndefined = constexpr TagBitUndefined
 189     const ValueEmpty      = constexpr ValueEmpty
 190     const ValueFalse      = constexpr ValueFalse
 191     const ValueTrue       = constexpr ValueTrue
 192     const ValueUndefined  = constexpr ValueUndefined
 193     const ValueNull       = constexpr ValueNull
 194     const TagTypeNumber   = constexpr TagTypeNumber
 195     const TagMask         = constexpr TagMask
 196 else
 197     const Int32Tag = constexpr JSValue::Int32Tag
 198     const BooleanTag = constexpr JSValue::BooleanTag
 199     const NullTag = constexpr JSValue::NullTag
 200     const UndefinedTag = constexpr JSValue::UndefinedTag
 201     const CellTag = constexpr JSValue::CellTag
 202     const EmptyValueTag = constexpr JSValue::EmptyValueTag
 203     const DeletedValueTag = constexpr JSValue::DeletedValueTag
 204     const LowestTag = constexpr JSValue::LowestTag
 205 end
 206 
 207 if JSVALUE64
 208     const NumberOfStructureIDEntropyBits = constexpr StructureIDTable::s_numberOfEntropyBits
 209     const StructureEntropyBitsShift = constexpr StructureIDTable::s_entropyBitsShiftForStructurePointer
 210 end
 211 
 212 const CallOpCodeSize = constexpr op_call_length
 213 
 214 const maxFrameExtentForSlowPathCall = constexpr maxFrameExtentForSlowPathCall
 215 
 216 if X86_64 or X86_64_WIN or ARM64 or ARM64E
 217     const CalleeSaveSpaceAsVirtualRegisters = 4
 218 elsif C_LOOP
 219     const CalleeSaveSpaceAsVirtualRegisters = 1
 220 elsif ARMv7
 221     const CalleeSaveSpaceAsVirtualRegisters = 1
 222 elsif MIPS
 223     const CalleeSaveSpaceAsVirtualRegisters = 1
 224 else
 225     const CalleeSaveSpaceAsVirtualRegisters = 0
 226 end
 227 
 228 const CalleeSaveSpaceStackAligned = (CalleeSaveSpaceAsVirtualRegisters * SlotSize + StackAlignment - 1) &amp; ~StackAlignmentMask
 229 
 230 
 231 # Watchpoint states
 232 const ClearWatchpoint = constexpr ClearWatchpoint
 233 const IsWatched = constexpr IsWatched
 234 const IsInvalidated = constexpr IsInvalidated
 235 
 236 # ShadowChicken data
 237 const ShadowChickenTailMarker = constexpr ShadowChicken::Packet::tailMarkerValue
 238 
 239 # ArithProfile data
 240 const ArithProfileInt = constexpr (ArithProfile::observedUnaryInt().bits())
 241 const ArithProfileNumber = constexpr (ArithProfile::observedUnaryNumber().bits())
 242 const ArithProfileIntInt = constexpr (ArithProfile::observedBinaryIntInt().bits())
 243 const ArithProfileNumberInt = constexpr (ArithProfile::observedBinaryNumberInt().bits())
 244 const ArithProfileIntNumber = constexpr (ArithProfile::observedBinaryIntNumber().bits())
 245 const ArithProfileNumberNumber = constexpr (ArithProfile::observedBinaryNumberNumber().bits())
 246 
 247 # Pointer Tags
 248 const BytecodePtrTag = constexpr BytecodePtrTag
 249 const JSEntryPtrTag = constexpr JSEntryPtrTag
 250 const ExceptionHandlerPtrTag = constexpr ExceptionHandlerPtrTag
 251 const NoPtrTag = constexpr NoPtrTag
 252 const SlowPathPtrTag = constexpr SlowPathPtrTag
 253 
 254 # Some register conventions.
 255 if JSVALUE64
 256     # - Use a pair of registers to represent the PC: one register for the
 257     #   base of the bytecodes, and one register for the index.
 258     # - The PC base (or PB for short) must be stored in a callee-save register.
 259     # - C calls are still given the Instruction* rather than the PC index.
 260     #   This requires an add before the call, and a sub after.
 261     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
 262     if ARM64 or ARM64E
 263         const metadataTable = csr6
 264         const PB = csr7
 265         const tagTypeNumber = csr8
 266         const tagMask = csr9
 267     elsif X86_64
 268         const metadataTable = csr1
 269         const PB = csr2
 270         const tagTypeNumber = csr3
 271         const tagMask = csr4
 272     elsif X86_64_WIN
 273         const metadataTable = csr3
 274         const PB = csr4
 275         const tagTypeNumber = csr5
 276         const tagMask = csr6
 277     elsif C_LOOP
 278         const PB = csr0
 279         const tagTypeNumber = csr1
 280         const tagMask = csr2
 281         const metadataTable = csr3
 282     end
 283 
 284 else
 285     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
 286     if C_LOOP
 287         const metadataTable = csr3
 288     elsif ARMv7
 289         const metadataTable = csr0
 290     elsif MIPS
 291         const metadataTable = csr0
 292     else
 293         error
 294     end
 295 end
 296 
 297 macro dispatch(advanceReg)
 298     addp advanceReg, PC
 299     nextInstruction()
 300 end
 301 
 302 macro dispatchIndirect(offsetReg)
 303     dispatch(offsetReg)
 304 end
 305 
 306 macro dispatchOp(size, opcodeName)
 307     macro dispatchNarrow()
 308         dispatch(constexpr %opcodeName%_length)
 309     end
 310 
 311     macro dispatchWide()
 312         dispatch(constexpr %opcodeName%_length * 4 + 1)
 313     end
 314 
 315     size(dispatchNarrow, dispatchWide, macro (dispatch) dispatch() end)
 316 end
 317 
 318 macro getu(size, opcodeStruct, fieldName, dst)
 319     size(getuOperandNarrow, getuOperandWide, macro (getu)
 320         getu(opcodeStruct, fieldName, dst)
 321     end)
 322 end
 323 
 324 macro get(size, opcodeStruct, fieldName, dst)
 325     size(getOperandNarrow, getOperandWide, macro (get)
 326         get(opcodeStruct, fieldName, dst)
 327     end)
 328 end
 329 
 330 macro narrow(narrowFn, wideFn, k)
 331     k(narrowFn)
 332 end
 333 
 334 macro wide(narrowFn, wideFn, k)
 335     k(wideFn)
 336 end
 337 
 338 macro metadata(size, opcode, dst, scratch)
 339     loadi constexpr %opcode%::opcodeID * 4[metadataTable], dst # offset = metadataTable&lt;unsigned*&gt;[opcodeID]
 340     getu(size, opcode, m_metadataID, scratch) # scratch = bytecode.m_metadataID
 341     muli sizeof %opcode%::Metadata, scratch # scratch *= sizeof(Op::Metadata)
 342     addi scratch, dst # offset += scratch
 343     addp metadataTable, dst # return &amp;metadataTable[offset]
 344 end
 345 
 346 macro jumpImpl(targetOffsetReg)
 347     btiz targetOffsetReg, .outOfLineJumpTarget
 348     dispatchIndirect(targetOffsetReg)
 349 .outOfLineJumpTarget:
 350     callSlowPath(_llint_slow_path_out_of_line_jump_target)
 351     nextInstruction()
 352 end
 353 
 354 macro commonOp(label, prologue, fn)
 355 _%label%:
 356     prologue()
 357     fn(narrow)
 358 
 359 _%label%_wide:
 360     prologue()
 361     fn(wide)
 362 end
 363 
 364 macro op(l, fn)
 365     commonOp(l, macro () end, macro (unused)
 366         fn()
 367     end)
 368 end
 369 
 370 macro llintOp(opcodeName, opcodeStruct, fn)
 371     commonOp(llint_%opcodeName%, traceExecution, macro(size)
 372         macro getImpl(fieldName, dst)
 373             get(size, opcodeStruct, fieldName, dst)
 374         end
 375 
 376         macro dispatchImpl()
 377             dispatchOp(size, opcodeName)
 378         end
 379 
 380         fn(size, getImpl, dispatchImpl)
 381     end)
 382 end
 383 
 384 macro llintOpWithReturn(opcodeName, opcodeStruct, fn)
 385     llintOp(opcodeName, opcodeStruct, macro(size, get, dispatch)
 386         makeReturn(get, dispatch, macro (return)
 387             fn(size, get, dispatch, return)
 388         end)
 389     end)
 390 end
 391 
 392 macro llintOpWithMetadata(opcodeName, opcodeStruct, fn)
 393     llintOpWithReturn(opcodeName, opcodeStruct, macro (size, get, dispatch, return)
 394         macro meta(dst, scratch)
 395             metadata(size, opcodeStruct, dst, scratch)
 396         end
 397         fn(size, get, dispatch, meta, return)
 398     end)
 399 end
 400 
 401 macro llintOpWithJump(opcodeName, opcodeStruct, impl)
 402     llintOpWithMetadata(opcodeName, opcodeStruct, macro(size, get, dispatch, metadata, return)
 403         macro jump(fieldName)
 404             get(fieldName, t0)
 405             jumpImpl(t0)
 406         end
 407 
 408         impl(size, get, jump, dispatch)
 409     end)
 410 end
 411 
 412 macro llintOpWithProfile(opcodeName, opcodeStruct, fn)
 413     llintOpWithMetadata(opcodeName, opcodeStruct, macro(size, get, dispatch, metadata, return)
 414         makeReturnProfiled(opcodeStruct, get, metadata, dispatch, macro (returnProfiled)
 415             fn(size, get, dispatch, returnProfiled)
 416         end)
 417     end)
 418 end
 419 
 420 
 421 if X86_64_WIN
 422     const extraTempReg = t0
 423 else
 424     const extraTempReg = t5
 425 end
 426 
 427 # Constants for reasoning about value representation.
 428 const TagOffset = constexpr TagOffset
 429 const PayloadOffset = constexpr PayloadOffset
 430 
 431 # Constant for reasoning about butterflies.
 432 const IsArray                  = constexpr IsArray
 433 const IndexingShapeMask        = constexpr IndexingShapeMask
 434 const NoIndexingShape          = constexpr NoIndexingShape
 435 const Int32Shape               = constexpr Int32Shape
 436 const DoubleShape              = constexpr DoubleShape
 437 const ContiguousShape          = constexpr ContiguousShape
 438 const ArrayStorageShape        = constexpr ArrayStorageShape
 439 const SlowPutArrayStorageShape = constexpr SlowPutArrayStorageShape
 440 const CopyOnWrite              = constexpr CopyOnWrite
 441 
 442 # Type constants.
 443 const StringType = constexpr StringType
 444 const SymbolType = constexpr SymbolType
 445 const ObjectType = constexpr ObjectType
 446 const FinalObjectType = constexpr FinalObjectType
 447 const JSFunctionType = constexpr JSFunctionType
 448 const ArrayType = constexpr ArrayType
 449 const DerivedArrayType = constexpr DerivedArrayType
 450 const ProxyObjectType = constexpr ProxyObjectType
 451 
 452 # The typed array types need to be numbered in a particular order because of the manually written
 453 # switch statement in get_by_val and put_by_val.
 454 const Int8ArrayType = constexpr Int8ArrayType
 455 const Uint8ArrayType = constexpr Uint8ArrayType
 456 const Uint8ClampedArrayType = constexpr Uint8ClampedArrayType
 457 const Int16ArrayType = constexpr Int16ArrayType
 458 const Uint16ArrayType = constexpr Uint16ArrayType
 459 const Int32ArrayType = constexpr Int32ArrayType
 460 const Uint32ArrayType = constexpr Uint32ArrayType
 461 const Float32ArrayType = constexpr Float32ArrayType
 462 const Float64ArrayType = constexpr Float64ArrayType
 463 
 464 const FirstTypedArrayType = constexpr FirstTypedArrayType
 465 const NumberOfTypedArrayTypesExcludingDataView = constexpr NumberOfTypedArrayTypesExcludingDataView
 466 
 467 # Type flags constants.
 468 const MasqueradesAsUndefined = constexpr MasqueradesAsUndefined
 469 const ImplementsDefaultHasInstance = constexpr ImplementsDefaultHasInstance
 470 
 471 # Bytecode operand constants.
 472 const FirstConstantRegisterIndexNarrow = 16
 473 const FirstConstantRegisterIndexWide = constexpr FirstConstantRegisterIndex
 474 
 475 # Code type constants.
 476 const GlobalCode = constexpr GlobalCode
 477 const EvalCode = constexpr EvalCode
 478 const FunctionCode = constexpr FunctionCode
 479 const ModuleCode = constexpr ModuleCode
 480 
 481 # The interpreter steals the tag word of the argument count.
 482 const LLIntReturnPC = ArgumentCount + TagOffset
 483 
 484 # String flags.
 485 const isRopeInPointer = constexpr JSString::isRopeInPointer
 486 const HashFlags8BitBuffer = constexpr StringImpl::s_hashFlag8BitBuffer
 487 
 488 # Copied from PropertyOffset.h
 489 const firstOutOfLineOffset = constexpr firstOutOfLineOffset
 490 
 491 # ResolveType
 492 const GlobalProperty = constexpr GlobalProperty
 493 const GlobalVar = constexpr GlobalVar
 494 const GlobalLexicalVar = constexpr GlobalLexicalVar
 495 const ClosureVar = constexpr ClosureVar
 496 const LocalClosureVar = constexpr LocalClosureVar
 497 const ModuleVar = constexpr ModuleVar
 498 const GlobalPropertyWithVarInjectionChecks = constexpr GlobalPropertyWithVarInjectionChecks
 499 const GlobalVarWithVarInjectionChecks = constexpr GlobalVarWithVarInjectionChecks
 500 const GlobalLexicalVarWithVarInjectionChecks = constexpr GlobalLexicalVarWithVarInjectionChecks
 501 const ClosureVarWithVarInjectionChecks = constexpr ClosureVarWithVarInjectionChecks
 502 
 503 const ResolveTypeMask = constexpr GetPutInfo::typeBits
 504 const InitializationModeMask = constexpr GetPutInfo::initializationBits
 505 const InitializationModeShift = constexpr GetPutInfo::initializationShift
 506 const NotInitialization = constexpr InitializationMode::NotInitialization
 507 
 508 const MarkedBlockSize = constexpr MarkedBlock::blockSize
 509 const MarkedBlockMask = ~(MarkedBlockSize - 1)
 510 const MarkedBlockFooterOffset = constexpr MarkedBlock::offsetOfFooter
 511 
 512 const BlackThreshold = constexpr blackThreshold
 513 
 514 const VectorBufferOffset = Vector::m_buffer
 515 const VectorSizeOffset = Vector::m_size
 516 
 517 # Some common utilities.
 518 macro crash()
 519     if C_LOOP
 520         cloopCrash
 521     else
 522         call _llint_crash
 523     end
 524 end
 525 
 526 macro assert(assertion)
 527     if ASSERT_ENABLED
 528         assertion(.ok)
 529         crash()
 530     .ok:
 531     end
 532 end
 533 
 534 # The probe macro can be used to insert some debugging code without perturbing scalar
 535 # registers. Presently, the probe macro only preserves scalar registers. Hence, the
 536 # C probe callback function should not trash floating point registers.
 537 #
 538 # The macro you pass to probe() can pass whatever registers you like to your probe
 539 # callback function. However, you need to be mindful of which of the registers are
 540 # also used as argument registers, and ensure that you don&#39;t trash the register value
 541 # before storing it in the probe callback argument register that you desire.
 542 #
 543 # Here&#39;s an example of how it&#39;s used:
 544 #
 545 #     probe(
 546 #         macro()
 547 #             move cfr, a0 # pass the ExecState* as arg0.
 548 #             move t0, a1 # pass the value of register t0 as arg1.
 549 #             call _cProbeCallbackFunction # to do whatever you want.
 550 #         end
 551 #     )
 552 #
 553 if X86_64 or ARM64 or ARM64E or ARMv7
 554     macro probe(action)
 555         # save all the registers that the LLInt may use.
 556         if ARM64 or ARM64E or ARMv7
 557             push cfr, lr
 558         end
 559         push a0, a1
 560         push a2, a3
 561         push t0, t1
 562         push t2, t3
 563         push t4, t5
 564         if ARM64 or ARM64E
 565             push csr0, csr1
 566             push csr2, csr3
 567             push csr4, csr5
 568             push csr6, csr7
 569             push csr8, csr9
 570         elsif ARMv7
 571             push csr0
 572         end
 573 
 574         action()
 575 
 576         # restore all the registers we saved previously.
 577         if ARM64 or ARM64E
 578             pop csr9, csr8
 579             pop csr7, csr6
 580             pop csr5, csr4
 581             pop csr3, csr2
 582             pop csr1, csr0
 583         elsif ARMv7
 584             pop csr0
 585         end
 586         pop t5, t4
 587         pop t3, t2
 588         pop t1, t0
 589         pop a3, a2
 590         pop a1, a0
 591         if ARM64 or ARM64E or ARMv7
 592             pop lr, cfr
 593         end
 594     end
 595 else
 596     macro probe(action)
 597     end
 598 end
 599 
 600 macro checkStackPointerAlignment(tempReg, location)
 601     if ASSERT_ENABLED
 602         if ARM64 or ARM64E or C_LOOP
 603             # ARM64 and ARM64E will check for us!
 604             # C_LOOP does not need the alignment, and can use a little perf
 605             # improvement from avoiding useless work.
 606         else
 607             if ARMv7
 608                 # ARM can&#39;t do logical ops with the sp as a source
 609                 move sp, tempReg
 610                 andp StackAlignmentMask, tempReg
 611             else
 612                 andp sp, StackAlignmentMask, tempReg
 613             end
 614             btpz tempReg, .stackPointerOkay
 615             move location, tempReg
 616             break
 617         .stackPointerOkay:
 618         end
 619     end
 620 end
 621 
 622 if C_LOOP or ARM64 or ARM64E or X86_64 or X86_64_WIN
 623     const CalleeSaveRegisterCount = 0
 624 elsif ARMv7
 625     const CalleeSaveRegisterCount = 7
 626 elsif MIPS
 627     const CalleeSaveRegisterCount = 2
 628 elsif X86 or X86_WIN
 629     const CalleeSaveRegisterCount = 3
 630 end
 631 
 632 const CalleeRegisterSaveSize = CalleeSaveRegisterCount * MachineRegisterSize
 633 
 634 # VMEntryTotalFrameSize includes the space for struct VMEntryRecord and the
 635 # callee save registers rounded up to keep the stack aligned
 636 const VMEntryTotalFrameSize = (CalleeRegisterSaveSize + sizeof VMEntryRecord + StackAlignment - 1) &amp; ~StackAlignmentMask
 637 
 638 macro pushCalleeSaves()
 639     if C_LOOP or ARM64 or ARM64E or X86_64 or X86_64_WIN
 640     elsif ARMv7
 641         emit &quot;push {r4-r6, r8-r11}&quot;
 642     elsif MIPS
 643         emit &quot;addiu $sp, $sp, -8&quot;
 644         emit &quot;sw $s0, 0($sp)&quot; # csr0/metaData
 645         emit &quot;sw $s4, 4($sp)&quot;
 646         # save $gp to $s4 so that we can restore it after a function call
 647         emit &quot;move $s4, $gp&quot;
 648     elsif X86
 649         emit &quot;push %esi&quot;
 650         emit &quot;push %edi&quot;
 651         emit &quot;push %ebx&quot;
 652     elsif X86_WIN
 653         emit &quot;push esi&quot;
 654         emit &quot;push edi&quot;
 655         emit &quot;push ebx&quot;
 656     end
 657 end
 658 
 659 macro popCalleeSaves()
 660     if C_LOOP or ARM64 or ARM64E or X86_64 or X86_64_WIN
 661     elsif ARMv7
 662         emit &quot;pop {r4-r6, r8-r11}&quot;
 663     elsif MIPS
 664         emit &quot;lw $s0, 0($sp)&quot;
 665         emit &quot;lw $s4, 4($sp)&quot;
 666         emit &quot;addiu $sp, $sp, 8&quot;
 667     elsif X86
 668         emit &quot;pop %ebx&quot;
 669         emit &quot;pop %edi&quot;
 670         emit &quot;pop %esi&quot;
 671     elsif X86_WIN
 672         emit &quot;pop ebx&quot;
 673         emit &quot;pop edi&quot;
 674         emit &quot;pop esi&quot;
 675     end
 676 end
 677 
 678 macro preserveCallerPCAndCFR()
 679     if C_LOOP or ARMv7 or MIPS
 680         push lr
 681         push cfr
 682     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 683         push cfr
 684     elsif ARM64 or ARM64E
 685         push cfr, lr
 686     else
 687         error
 688     end
 689     move sp, cfr
 690 end
 691 
 692 macro restoreCallerPCAndCFR()
 693     move cfr, sp
 694     if C_LOOP or ARMv7 or MIPS
 695         pop cfr
 696         pop lr
 697     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 698         pop cfr
 699     elsif ARM64 or ARM64E
 700         pop lr, cfr
 701     end
 702 end
 703 
 704 macro preserveCalleeSavesUsedByLLInt()
 705     subp CalleeSaveSpaceStackAligned, sp
 706     if C_LOOP
 707         storep metadataTable, -PtrSize[cfr]
 708     elsif ARMv7 or MIPS
 709         storep metadataTable, -4[cfr]
 710     elsif ARM64 or ARM64E
 711         emit &quot;stp x27, x28, [x29, #-16]&quot;
 712         emit &quot;stp x25, x26, [x29, #-32]&quot;
 713     elsif X86
 714     elsif X86_WIN
 715     elsif X86_64
 716         storep csr4, -8[cfr]
 717         storep csr3, -16[cfr]
 718         storep csr2, -24[cfr]
 719         storep csr1, -32[cfr]
 720     elsif X86_64_WIN
 721         storep csr6, -8[cfr]
 722         storep csr5, -16[cfr]
 723         storep csr4, -24[cfr]
 724         storep csr3, -32[cfr]
 725     end
 726 end
 727 
 728 macro restoreCalleeSavesUsedByLLInt()
 729     if C_LOOP
 730         loadp -PtrSize[cfr], metadataTable
 731     elsif ARMv7 or MIPS
 732         loadp -4[cfr], metadataTable
 733     elsif ARM64 or ARM64E
 734         emit &quot;ldp x25, x26, [x29, #-32]&quot;
 735         emit &quot;ldp x27, x28, [x29, #-16]&quot;
 736     elsif X86
 737     elsif X86_WIN
 738     elsif X86_64
 739         loadp -32[cfr], csr1
 740         loadp -24[cfr], csr2
 741         loadp -16[cfr], csr3
 742         loadp -8[cfr], csr4
 743     elsif X86_64_WIN
 744         loadp -32[cfr], csr3
 745         loadp -24[cfr], csr4
 746         loadp -16[cfr], csr5
 747         loadp -8[cfr], csr6
 748     end
 749 end
 750 
 751 macro copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(vm, temp)
 752     if ARM64 or ARM64E or X86_64 or X86_64_WIN or ARMv7 or MIPS
 753         loadp VM::topEntryFrame[vm], temp
 754         vmEntryRecord(temp, temp)
 755         leap VMEntryRecord::calleeSaveRegistersBuffer[temp], temp
 756         if ARM64 or ARM64E
 757             storeq csr0, [temp]
 758             storeq csr1, 8[temp]
 759             storeq csr2, 16[temp]
 760             storeq csr3, 24[temp]
 761             storeq csr4, 32[temp]
 762             storeq csr5, 40[temp]
 763             storeq csr6, 48[temp]
 764             storeq csr7, 56[temp]
 765             storeq csr8, 64[temp]
 766             storeq csr9, 72[temp]
 767             stored csfr0, 80[temp]
 768             stored csfr1, 88[temp]
 769             stored csfr2, 96[temp]
 770             stored csfr3, 104[temp]
 771             stored csfr4, 112[temp]
 772             stored csfr5, 120[temp]
 773             stored csfr6, 128[temp]
 774             stored csfr7, 136[temp]
 775         elsif X86_64
 776             storeq csr0, [temp]
 777             storeq csr1, 8[temp]
 778             storeq csr2, 16[temp]
 779             storeq csr3, 24[temp]
 780             storeq csr4, 32[temp]
 781         elsif X86_64_WIN
 782             storeq csr0, [temp]
 783             storeq csr1, 8[temp]
 784             storeq csr2, 16[temp]
 785             storeq csr3, 24[temp]
 786             storeq csr4, 32[temp]
 787             storeq csr5, 40[temp]
 788             storeq csr6, 48[temp]
 789         elsif ARMv7 or MIPS
 790             storep csr0, [temp]
 791         end
 792     end
 793 end
 794 
 795 macro restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(vm, temp)
 796     if ARM64 or ARM64E or X86_64 or X86_64_WIN or ARMv7 or MIPS
 797         loadp VM::topEntryFrame[vm], temp
 798         vmEntryRecord(temp, temp)
 799         leap VMEntryRecord::calleeSaveRegistersBuffer[temp], temp
 800         if ARM64 or ARM64E
 801             loadq [temp], csr0
 802             loadq 8[temp], csr1
 803             loadq 16[temp], csr2
 804             loadq 24[temp], csr3
 805             loadq 32[temp], csr4
 806             loadq 40[temp], csr5
 807             loadq 48[temp], csr6
 808             loadq 56[temp], csr7
 809             loadq 64[temp], csr8
 810             loadq 72[temp], csr9
 811             loadd 80[temp], csfr0
 812             loadd 88[temp], csfr1
 813             loadd 96[temp], csfr2
 814             loadd 104[temp], csfr3
 815             loadd 112[temp], csfr4
 816             loadd 120[temp], csfr5
 817             loadd 128[temp], csfr6
 818             loadd 136[temp], csfr7
 819         elsif X86_64
 820             loadq [temp], csr0
 821             loadq 8[temp], csr1
 822             loadq 16[temp], csr2
 823             loadq 24[temp], csr3
 824             loadq 32[temp], csr4
 825         elsif X86_64_WIN
 826             loadq [temp], csr0
 827             loadq 8[temp], csr1
 828             loadq 16[temp], csr2
 829             loadq 24[temp], csr3
 830             loadq 32[temp], csr4
 831             loadq 40[temp], csr5
 832             loadq 48[temp], csr6
 833         elsif ARMv7 or MIPS
 834             loadp [temp], csr0
 835         end
 836     end
 837 end
 838 
 839 macro preserveReturnAddressAfterCall(destinationRegister)
 840     if C_LOOP or ARMv7 or ARM64 or ARM64E or MIPS
 841         # In C_LOOP case, we&#39;re only preserving the bytecode vPC.
 842         move lr, destinationRegister
 843     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 844         pop destinationRegister
 845     else
 846         error
 847     end
 848 end
 849 
 850 macro functionPrologue()
 851     tagReturnAddress sp
 852     if X86 or X86_WIN or X86_64 or X86_64_WIN
 853         push cfr
 854     elsif ARM64 or ARM64E
 855         push cfr, lr
 856     elsif C_LOOP or ARMv7 or MIPS
 857         push lr
 858         push cfr
 859     end
 860     move sp, cfr
 861 end
 862 
 863 macro functionEpilogue()
 864     if X86 or X86_WIN or X86_64 or X86_64_WIN
 865         pop cfr
 866     elsif ARM64 or ARM64E
 867         pop lr, cfr
 868     elsif C_LOOP or ARMv7 or MIPS
 869         pop cfr
 870         pop lr
 871     end
 872 end
 873 
 874 macro vmEntryRecord(entryFramePointer, resultReg)
 875     subp entryFramePointer, VMEntryTotalFrameSize, resultReg
 876 end
 877 
 878 macro getFrameRegisterSizeForCodeBlock(codeBlock, size)
 879     loadi CodeBlock::m_numCalleeLocals[codeBlock], size
 880     lshiftp 3, size
 881     addp maxFrameExtentForSlowPathCall, size
 882 end
 883 
 884 macro restoreStackPointerAfterCall()
 885     loadp CodeBlock[cfr], t2
 886     getFrameRegisterSizeForCodeBlock(t2, t2)
 887     if ARMv7
 888         subp cfr, t2, t2
 889         move t2, sp
 890     else
 891         subp cfr, t2, sp
 892     end
 893 end
 894 
 895 macro traceExecution()
 896     if TRACING
 897         callSlowPath(_llint_trace)
 898     end
 899 end
 900 
 901 macro callTargetFunction(size, opcodeStruct, dispatch, callee, callPtrTag)
 902     if C_LOOP
 903         cloopCallJSFunction callee
 904     else
 905         call callee, callPtrTag
 906     end
 907     restoreStackPointerAfterCall()
 908     dispatchAfterCall(size, opcodeStruct, dispatch)
 909 end
 910 
 911 macro prepareForRegularCall(callee, temp1, temp2, temp3, callPtrTag)
 912     addp CallerFrameAndPCSize, sp
 913 end
 914 
 915 # sp points to the new frame
 916 macro prepareForTailCall(callee, temp1, temp2, temp3, callPtrTag)
 917     restoreCalleeSavesUsedByLLInt()
 918 
 919     loadi PayloadOffset + ArgumentCount[cfr], temp2
 920     loadp CodeBlock[cfr], temp1
 921     loadi CodeBlock::m_numParameters[temp1], temp1
 922     bilteq temp1, temp2, .noArityFixup
 923     move temp1, temp2
 924 
 925 .noArityFixup:
 926     # We assume &lt; 2^28 arguments
 927     muli SlotSize, temp2
 928     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
 929     andi ~StackAlignmentMask, temp2
 930 
 931     move cfr, temp1
 932     addp temp2, temp1
 933 
 934     loadi PayloadOffset + ArgumentCount[sp], temp2
 935     # We assume &lt; 2^28 arguments
 936     muli SlotSize, temp2
 937     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
 938     andi ~StackAlignmentMask, temp2
 939 
 940     if ARMv7 or ARM64 or ARM64E or C_LOOP or MIPS
 941         addp CallerFrameAndPCSize, sp
 942         subi CallerFrameAndPCSize, temp2
 943         loadp CallerFrameAndPC::returnPC[cfr], lr
 944     else
 945         addp PtrSize, sp
 946         subi PtrSize, temp2
 947         loadp PtrSize[cfr], temp3
 948         storep temp3, [sp]
 949     end
 950 
 951     if POINTER_PROFILING
 952         addp 16, cfr, temp3
 953         untagReturnAddress temp3
 954     end
 955 
 956     subp temp2, temp1
 957     loadp [cfr], cfr
 958 
 959 .copyLoop:
 960     if ARM64 and not ADDRESS64
 961         subi MachineRegisterSize, temp2
 962         loadq [sp, temp2, 1], temp3
 963         storeq temp3, [temp1, temp2, 1]
 964         btinz temp2, .copyLoop
 965     else
 966         subi PtrSize, temp2
 967         loadp [sp, temp2, 1], temp3
 968         storep temp3, [temp1, temp2, 1]
 969         btinz temp2, .copyLoop
 970     end
 971 
 972     move temp1, sp
 973     jmp callee, callPtrTag
 974 end
 975 
 976 macro slowPathForCall(size, opcodeStruct, dispatch, slowPath, prepareCall)
 977     callCallSlowPath(
 978         slowPath,
 979         # Those are r0 and r1
 980         macro (callee, calleeFramePtr)
 981             btpz calleeFramePtr, .dontUpdateSP
 982             move calleeFramePtr, sp
 983             prepareCall(callee, t2, t3, t4, SlowPathPtrTag)
 984         .dontUpdateSP:
 985             callTargetFunction(size, opcodeStruct, dispatch, callee, SlowPathPtrTag)
 986         end)
 987 end
 988 
 989 macro arrayProfile(offset, cellAndIndexingType, metadata, scratch)
 990     const cell = cellAndIndexingType
 991     const indexingType = cellAndIndexingType 
 992     loadi JSCell::m_structureID[cell], scratch
 993     storei scratch, offset + ArrayProfile::m_lastSeenStructureID[metadata]
 994     loadb JSCell::m_indexingTypeAndMisc[cell], indexingType
 995 end
 996 
 997 macro skipIfIsRememberedOrInEden(cell, slowPath)
 998     memfence
 999     bba JSCell::m_cellState[cell], BlackThreshold, .done
1000     slowPath()
1001 .done:
1002 end
1003 
1004 macro notifyWrite(set, slow)
1005     bbneq WatchpointSet::m_state[set], IsInvalidated, slow
1006 end
1007 
1008 macro checkSwitchToJIT(increment, action)
1009     loadp CodeBlock[cfr], t0
1010     baddis increment, CodeBlock::m_llintExecuteCounter + BaselineExecutionCounter::m_counter[t0], .continue
1011     action()
1012     .continue:
1013 end
1014 
1015 macro checkSwitchToJITForEpilogue()
1016     checkSwitchToJIT(
1017         10,
1018         macro ()
1019             callSlowPath(_llint_replace)
1020         end)
1021 end
1022 
1023 macro assertNotConstant(size, index)
1024     size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide, macro (FirstConstantRegisterIndex)
1025         assert(macro (ok) bilt index, FirstConstantRegisterIndex, ok end)
1026     end)
1027 end
1028 
1029 macro functionForCallCodeBlockGetter(targetRegister)
1030     if JSVALUE64
1031         loadp Callee[cfr], targetRegister
1032     else
1033         loadp Callee + PayloadOffset[cfr], targetRegister
1034     end
1035     loadp JSFunction::m_executable[targetRegister], targetRegister
1036     loadp FunctionExecutable::m_codeBlockForCall[targetRegister], targetRegister
1037     loadp ExecutableToCodeBlockEdge::m_codeBlock[targetRegister], targetRegister
1038 end
1039 
1040 macro functionForConstructCodeBlockGetter(targetRegister)
1041     if JSVALUE64
1042         loadp Callee[cfr], targetRegister
1043     else
1044         loadp Callee + PayloadOffset[cfr], targetRegister
1045     end
1046     loadp JSFunction::m_executable[targetRegister], targetRegister
1047     loadp FunctionExecutable::m_codeBlockForConstruct[targetRegister], targetRegister
1048     loadp ExecutableToCodeBlockEdge::m_codeBlock[targetRegister], targetRegister
1049 end
1050 
1051 macro notFunctionCodeBlockGetter(targetRegister)
1052     loadp CodeBlock[cfr], targetRegister
1053 end
1054 
1055 macro functionCodeBlockSetter(sourceRegister)
1056     storep sourceRegister, CodeBlock[cfr]
1057 end
1058 
1059 macro notFunctionCodeBlockSetter(sourceRegister)
1060     # Nothing to do!
1061 end
1062 
1063 # Do the bare minimum required to execute code. Sets up the PC, leave the CodeBlock*
1064 # in t1. May also trigger prologue entry OSR.
1065 macro prologue(codeBlockGetter, codeBlockSetter, osrSlowPath, traceSlowPath)
1066     # Set up the call frame and check if we should OSR.
1067     tagReturnAddress sp
1068     preserveCallerPCAndCFR()
1069 
1070     if TRACING
1071         subp maxFrameExtentForSlowPathCall, sp
1072         callSlowPath(traceSlowPath)
1073         addp maxFrameExtentForSlowPathCall, sp
1074     end
1075     codeBlockGetter(t1)
1076     if not C_LOOP
1077         baddis 5, CodeBlock::m_llintExecuteCounter + BaselineExecutionCounter::m_counter[t1], .continue
1078         if JSVALUE64
1079             move cfr, a0
1080             move PC, a1
1081             cCall2(osrSlowPath)
1082         else
1083             # We are after the function prologue, but before we have set up sp from the CodeBlock.
1084             # Temporarily align stack pointer for this call.
1085             subp 8, sp
1086             move cfr, a0
1087             move PC, a1
1088             cCall2(osrSlowPath)
1089             addp 8, sp
1090         end
1091         btpz r0, .recover
1092         move cfr, sp # restore the previous sp
1093         # pop the callerFrame since we will jump to a function that wants to save it
1094         if ARM64 or ARM64E
1095             pop lr, cfr
1096             untagReturnAddress sp
1097         elsif ARMv7 or MIPS
1098             pop cfr
1099             pop lr
1100         else
1101             pop cfr
1102         end
1103         jmp r0, JSEntryPtrTag
1104     .recover:
1105         codeBlockGetter(t1)
1106     .continue:
1107     end
1108 
1109     codeBlockSetter(t1)
1110 
1111     preserveCalleeSavesUsedByLLInt()
1112 
1113     # Set up the PC.
1114     if JSVALUE64
1115         loadp CodeBlock::m_instructionsRawPointer[t1], PB
1116         move 0, PC
1117     else
1118         loadp CodeBlock::m_instructionsRawPointer[t1], PC
1119     end
1120 
1121     # Get new sp in t0 and check stack height.
1122     getFrameRegisterSizeForCodeBlock(t1, t0)
1123     subp cfr, t0, t0
1124     bpa t0, cfr, .needStackCheck
1125     loadp CodeBlock::m_vm[t1], t2
1126     if C_LOOP
1127         bpbeq VM::m_cloopStackLimit[t2], t0, .stackHeightOK
1128     else
1129         bpbeq VM::m_softStackLimit[t2], t0, .stackHeightOK
1130     end
1131 
1132 .needStackCheck:
1133     # Stack height check failed - need to call a slow_path.
1134     # Set up temporary stack pointer for call including callee saves
1135     subp maxFrameExtentForSlowPathCall, sp
1136     callSlowPath(_llint_stack_check)
1137     bpeq r1, 0, .stackHeightOKGetCodeBlock
1138 
1139     # We&#39;re throwing before the frame is fully set up. This frame will be
1140     # ignored by the unwinder. So, let&#39;s restore the callee saves before we
1141     # start unwinding. We need to do this before we change the cfr.
1142     restoreCalleeSavesUsedByLLInt()
1143 
1144     move r1, cfr
1145     jmp _llint_throw_from_slow_path_trampoline
1146 
1147 .stackHeightOKGetCodeBlock:
1148     # Stack check slow path returned that the stack was ok.
1149     # Since they were clobbered, need to get CodeBlock and new sp
1150     codeBlockGetter(t1)
1151     getFrameRegisterSizeForCodeBlock(t1, t0)
1152     subp cfr, t0, t0
1153 
1154 .stackHeightOK:
1155     if X86_64 or ARM64
1156         # We need to start zeroing from sp as it has been adjusted after saving callee saves.
1157         move sp, t2
1158         move t0, sp
1159 .zeroStackLoop:
1160         bpeq sp, t2, .zeroStackDone
1161         subp PtrSize, t2
1162         storep 0, [t2]
1163         jmp .zeroStackLoop
1164 .zeroStackDone:
1165     else
1166         move t0, sp
1167     end
1168 
1169     loadp CodeBlock::m_metadata[t1], metadataTable
1170 
1171     if JSVALUE64
1172         move TagTypeNumber, tagTypeNumber
1173         addq TagBitTypeOther, tagTypeNumber, tagMask
1174     end
1175 end
1176 
1177 # Expects that CodeBlock is in t1, which is what prologue() leaves behind.
1178 # Must call dispatch(0) after calling this.
1179 macro functionInitialization(profileArgSkip)
1180     # Profile the arguments. Unfortunately, we have no choice but to do this. This
1181     # code is pretty horrendous because of the difference in ordering between
1182     # arguments and value profiles, the desire to have a simple loop-down-to-zero
1183     # loop, and the desire to use only three registers so as to preserve the PC and
1184     # the code block. It is likely that this code should be rewritten in a more
1185     # optimal way for architectures that have more than five registers available
1186     # for arbitrary use in the interpreter.
1187     loadi CodeBlock::m_numParameters[t1], t0
1188     addp -profileArgSkip, t0 # Use addi because that&#39;s what has the peephole
1189     assert(macro (ok) bpgteq t0, 0, ok end)
1190     btpz t0, .argumentProfileDone
1191     loadp CodeBlock::m_argumentValueProfiles + RefCountedArray::m_data[t1], t3
1192     btpz t3, .argumentProfileDone # When we can&#39;t JIT, we don&#39;t allocate any argument value profiles.
1193     mulp sizeof ValueProfile, t0, t2 # Aaaaahhhh! Need strength reduction!
1194     lshiftp 3, t0 # offset of last JSValue arguments on the stack.
1195     addp t2, t3 # pointer to end of ValueProfile array in CodeBlock::m_argumentValueProfiles.
1196 .argumentProfileLoop:
1197     if JSVALUE64
1198         loadq ThisArgumentOffset - 8 + profileArgSkip * 8[cfr, t0], t2
1199         subp sizeof ValueProfile, t3
1200         storeq t2, profileArgSkip * sizeof ValueProfile + ValueProfile::m_buckets[t3]
1201     else
1202         loadi ThisArgumentOffset + TagOffset - 8 + profileArgSkip * 8[cfr, t0], t2
1203         subp sizeof ValueProfile, t3
1204         storei t2, profileArgSkip * sizeof ValueProfile + ValueProfile::m_buckets + TagOffset[t3]
1205         loadi ThisArgumentOffset + PayloadOffset - 8 + profileArgSkip * 8[cfr, t0], t2
1206         storei t2, profileArgSkip * sizeof ValueProfile + ValueProfile::m_buckets + PayloadOffset[t3]
1207     end
1208     baddpnz -8, t0, .argumentProfileLoop
1209 .argumentProfileDone:
1210 end
1211 
1212 macro doReturn()
1213     restoreCalleeSavesUsedByLLInt()
1214     restoreCallerPCAndCFR()
1215     ret
1216 end
1217 
1218 # This break instruction is needed so that the synthesized llintPCRangeStart label
1219 # doesn&#39;t point to the exact same location as vmEntryToJavaScript which comes after it.
1220 # Otherwise, libunwind will report vmEntryToJavaScript as llintPCRangeStart in
1221 # stack traces.
1222 
1223     break
1224 
1225 # stub to call into JavaScript or Native functions
1226 # EncodedJSValue vmEntryToJavaScript(void* code, VM* vm, ProtoCallFrame* protoFrame)
1227 # EncodedJSValue vmEntryToNativeFunction(void* code, VM* vm, ProtoCallFrame* protoFrame)
1228 
1229 if C_LOOP
1230     _llint_vm_entry_to_javascript:
1231 else
1232     global _vmEntryToJavaScript
1233     _vmEntryToJavaScript:
1234 end
1235     doVMEntry(makeJavaScriptCall)
1236 
1237 
1238 if C_LOOP
1239     _llint_vm_entry_to_native:
1240 else
1241     global _vmEntryToNative
1242     _vmEntryToNative:
1243 end
1244     doVMEntry(makeHostFunctionCall)
1245 
1246 
1247 if not C_LOOP
1248     # void sanitizeStackForVMImpl(VM* vm)
1249     global _sanitizeStackForVMImpl
1250     _sanitizeStackForVMImpl:
1251         tagReturnAddress sp
1252         # We need three non-aliased caller-save registers. We are guaranteed
1253         # this for a0, a1 and a2 on all architectures.
1254         if X86 or X86_WIN
1255             loadp 4[sp], a0
1256         end
1257         const vm = a0
1258         const address = a1
1259         const zeroValue = a2
1260     
1261         loadp VM::m_lastStackTop[vm], address
1262         bpbeq sp, address, .zeroFillDone
1263     
1264         move 0, zeroValue
1265     .zeroFillLoop:
1266         storep zeroValue, [address]
1267         addp PtrSize, address
1268         bpa sp, address, .zeroFillLoop
1269 
1270     .zeroFillDone:
1271         move sp, address
1272         storep address, VM::m_lastStackTop[vm]
1273         ret
1274     
1275     # VMEntryRecord* vmEntryRecord(const EntryFrame* entryFrame)
1276     global _vmEntryRecord
1277     _vmEntryRecord:
1278         tagReturnAddress sp
1279         if X86 or X86_WIN
1280             loadp 4[sp], a0
1281         end
1282 
1283         vmEntryRecord(a0, r0)
1284         ret
1285 end
1286 
1287 if C_LOOP
1288     # Dummy entry point the C Loop uses to initialize.
1289     _llint_entry:
1290         crash()
1291 else
1292     macro initPCRelative(pcBase)
1293         if X86_64 or X86_64_WIN or X86 or X86_WIN
1294             call _relativePCBase
1295         _relativePCBase:
1296             pop pcBase
1297         elsif ARM64 or ARM64E
1298         elsif ARMv7
1299         _relativePCBase:
1300             move pc, pcBase
1301             subp 3, pcBase   # Need to back up the PC and set the Thumb2 bit
1302         elsif MIPS
1303             la _relativePCBase, pcBase
1304             setcallreg pcBase # needed to set $t9 to the right value for the .cpload created by the label.
1305         _relativePCBase:
1306         end
1307 end
1308 
1309 # The PC base is in t2, as this is what _llint_entry leaves behind through
1310 # initPCRelative(t2)
1311 macro setEntryAddress(index, label)
1312     setEntryAddressCommon(index, label, a0)
1313 end
1314 
1315 macro setEntryAddressWide(index, label)
1316      setEntryAddressCommon(index, label, a1)
1317 end
1318 
1319 macro setEntryAddressCommon(index, label, map)
1320     if X86_64 or X86_64_WIN
1321         leap (label - _relativePCBase)[t2], t3
1322         move index, t4
1323         storep t3, [map, t4, 8]
1324     elsif X86 or X86_WIN
1325         leap (label - _relativePCBase)[t2], t3
1326         move index, t4
1327         storep t3, [map, t4, 4]
1328     elsif ARM64 or ARM64E
1329         pcrtoaddr label, t2
1330         move index, t4
1331         storep t2, [map, t4, PtrSize]
1332     elsif ARMv7
1333         mvlbl (label - _relativePCBase), t4
1334         addp t4, t2, t4
1335         move index, t3
1336         storep t4, [map, t3, 4]
1337     elsif MIPS
1338         la label, t4
1339         la _relativePCBase, t3
1340         subp t3, t4
1341         addp t4, t2, t4
1342         move index, t3
1343         storep t4, [map, t3, 4]
1344     end
1345 end
1346 
1347 global _llint_entry
1348 # Entry point for the llint to initialize.
1349 _llint_entry:
1350     functionPrologue()
1351     pushCalleeSaves()
1352     if X86 or X86_WIN
1353         loadp 20[sp], a0
1354         loadp 24[sp], a1
1355     end
1356 
1357     initPCRelative(t2)
1358 
1359     # Include generated bytecode initialization file.
1360     include InitBytecodes
1361 
1362     popCalleeSaves()
1363     functionEpilogue()
1364     ret
1365 end
1366 
1367 _llint_op_wide:
1368     nextInstructionWide()
1369 
1370 _llint_op_wide_wide:
1371     crash()
1372 
1373 _llint_op_enter_wide:
1374     crash()
1375 
1376 op(llint_program_prologue, macro ()
1377     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1378     dispatch(0)
1379 end)
1380 
1381 
1382 op(llint_module_program_prologue, macro ()
1383     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1384     dispatch(0)
1385 end)
1386 
1387 
1388 op(llint_eval_prologue, macro ()
1389     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1390     dispatch(0)
1391 end)
1392 
1393 
1394 op(llint_function_for_call_prologue, macro ()
1395     prologue(functionForCallCodeBlockGetter, functionCodeBlockSetter, _llint_entry_osr_function_for_call, _llint_trace_prologue_function_for_call)
1396     functionInitialization(0)
1397     dispatch(0)
1398 end)
1399     
1400 
1401 op(llint_function_for_construct_prologue, macro ()
1402     prologue(functionForConstructCodeBlockGetter, functionCodeBlockSetter, _llint_entry_osr_function_for_construct, _llint_trace_prologue_function_for_construct)
1403     functionInitialization(1)
1404     dispatch(0)
1405 end)
1406     
1407 
1408 op(llint_function_for_call_arity_check, macro ()
1409     prologue(functionForCallCodeBlockGetter, functionCodeBlockSetter, _llint_entry_osr_function_for_call_arityCheck, _llint_trace_arityCheck_for_call)
1410     functionArityCheck(.functionForCallBegin, _slow_path_call_arityCheck)
1411 .functionForCallBegin:
1412     functionInitialization(0)
1413     dispatch(0)
1414 end)
1415 
1416 
1417 op(llint_function_for_construct_arity_check, macro ()
1418     prologue(functionForConstructCodeBlockGetter, functionCodeBlockSetter, _llint_entry_osr_function_for_construct_arityCheck, _llint_trace_arityCheck_for_construct)
1419     functionArityCheck(.functionForConstructBegin, _slow_path_construct_arityCheck)
1420 .functionForConstructBegin:
1421     functionInitialization(1)
1422     dispatch(0)
1423 end)
1424 
1425 
1426 # Value-representation-specific code.
1427 if JSVALUE64
1428     include LowLevelInterpreter64
1429 else
1430     include LowLevelInterpreter32_64
1431 end
1432 
1433 
1434 # Value-representation-agnostic code.
1435 macro slowPathOp(opcodeName)
1436     llintOp(op_%opcodeName%, unused, macro (unused, unused, dispatch)
1437         callSlowPath(_slow_path_%opcodeName%)
1438         dispatch()
1439     end)
1440 end
1441 
1442 slowPathOp(create_cloned_arguments)
1443 slowPathOp(create_direct_arguments)
1444 slowPathOp(create_lexical_environment)
1445 slowPathOp(create_rest)
1446 slowPathOp(create_scoped_arguments)
1447 slowPathOp(create_this)
1448 slowPathOp(define_accessor_property)
1449 slowPathOp(define_data_property)
1450 slowPathOp(enumerator_generic_pname)
1451 slowPathOp(enumerator_structure_pname)
1452 slowPathOp(get_by_id_with_this)
1453 slowPathOp(get_by_val_with_this)
1454 slowPathOp(get_direct_pname)
1455 slowPathOp(get_enumerable_length)
1456 slowPathOp(get_property_enumerator)
1457 slowPathOp(greater)
1458 slowPathOp(greatereq)
1459 slowPathOp(has_generic_property)
1460 slowPathOp(has_indexed_property)
1461 slowPathOp(has_structure_property)
1462 slowPathOp(in_by_id)
1463 slowPathOp(in_by_val)
1464 slowPathOp(is_function)
1465 slowPathOp(is_object_or_null)
1466 slowPathOp(less)
1467 slowPathOp(lesseq)
1468 slowPathOp(mod)
1469 slowPathOp(new_array_buffer)
1470 slowPathOp(new_array_with_spread)
1471 slowPathOp(pow)
1472 slowPathOp(push_with_scope)
1473 slowPathOp(put_by_id_with_this)
1474 slowPathOp(put_by_val_with_this)
1475 slowPathOp(resolve_scope_for_hoisting_func_decl_in_eval)
1476 slowPathOp(spread)
1477 slowPathOp(strcat)
1478 slowPathOp(throw_static_error)
1479 slowPathOp(to_index_string)
1480 slowPathOp(typeof)
1481 slowPathOp(unreachable)
1482 
1483 macro llintSlowPathOp(opcodeName)
1484     llintOp(op_%opcodeName%, unused, macro (unused, unused, dispatch)
1485         callSlowPath(_llint_slow_path_%opcodeName%)
1486         dispatch()
1487     end)
1488 end
1489 
1490 llintSlowPathOp(del_by_id)
1491 llintSlowPathOp(del_by_val)
1492 llintSlowPathOp(instanceof)
1493 llintSlowPathOp(instanceof_custom)
1494 llintSlowPathOp(new_array)
1495 llintSlowPathOp(new_array_with_size)
1496 llintSlowPathOp(new_async_func)
1497 llintSlowPathOp(new_async_func_exp)
1498 llintSlowPathOp(new_async_generator_func)
1499 llintSlowPathOp(new_async_generator_func_exp)
1500 llintSlowPathOp(new_func)
1501 llintSlowPathOp(new_func_exp)
1502 llintSlowPathOp(new_generator_func)
1503 llintSlowPathOp(new_generator_func_exp)
1504 llintSlowPathOp(new_object)
1505 llintSlowPathOp(new_regexp)
1506 llintSlowPathOp(put_getter_by_id)
1507 llintSlowPathOp(put_getter_by_val)
1508 llintSlowPathOp(put_getter_setter_by_id)
1509 llintSlowPathOp(put_setter_by_id)
1510 llintSlowPathOp(put_setter_by_val)
1511 llintSlowPathOp(set_function_name)
1512 llintSlowPathOp(super_sampler_begin)
1513 llintSlowPathOp(super_sampler_end)
1514 llintSlowPathOp(throw)
1515 llintSlowPathOp(try_get_by_id)
1516 
1517 llintOp(op_switch_string, unused, macro (unused, unused, unused)
1518     callSlowPath(_llint_slow_path_switch_string)
1519     nextInstruction()
1520 end)
1521 
1522 
1523 equalityComparisonOp(eq, OpEq,
1524     macro (left, right, result) cieq left, right, result end)
1525 
1526 
1527 equalityComparisonOp(neq, OpNeq,
1528     macro (left, right, result) cineq left, right, result end)
1529 
1530 
1531 compareUnsignedOp(below, OpBelow,
1532         macro (left, right, result) cib left, right, result end)
1533 
1534 
1535 compareUnsignedOp(beloweq, OpBeloweq,
1536         macro (left, right, result) cibeq left, right, result end)
1537 
1538 
1539 llintOpWithJump(op_jmp, OpJmp, macro (size, get, jump, dispatch)
1540     jump(m_targetLabel)
1541 end)
1542 
1543 
1544 llintJumpTrueOrFalseOp(
1545     jtrue, OpJtrue,
1546     macro (value, target) btinz value, 1, target end)
1547 
1548 
1549 llintJumpTrueOrFalseOp(
1550     jfalse, OpJfalse,
1551     macro (value, target) btiz value, 1, target end)
1552 
1553 
1554 compareJumpOp(
1555     jless, OpJless,
1556     macro (left, right, target) bilt left, right, target end,
1557     macro (left, right, target) bdlt left, right, target end)
1558 
1559 
1560 compareJumpOp(
1561     jnless, OpJnless,
1562     macro (left, right, target) bigteq left, right, target end,
1563     macro (left, right, target) bdgtequn left, right, target end)
1564 
1565 
1566 compareJumpOp(
1567     jgreater, OpJgreater,
1568     macro (left, right, target) bigt left, right, target end,
1569     macro (left, right, target) bdgt left, right, target end)
1570 
1571 
1572 compareJumpOp(
1573     jngreater, OpJngreater,
1574     macro (left, right, target) bilteq left, right, target end,
1575     macro (left, right, target) bdltequn left, right, target end)
1576 
1577 
1578 compareJumpOp(
1579     jlesseq, OpJlesseq,
1580     macro (left, right, target) bilteq left, right, target end,
1581     macro (left, right, target) bdlteq left, right, target end)
1582 
1583 
1584 compareJumpOp(
1585     jnlesseq, OpJnlesseq,
1586     macro (left, right, target) bigt left, right, target end,
1587     macro (left, right, target) bdgtun left, right, target end)
1588 
1589 
1590 compareJumpOp(
1591     jgreatereq, OpJgreatereq,
1592     macro (left, right, target) bigteq left, right, target end,
1593     macro (left, right, target) bdgteq left, right, target end)
1594 
1595 
1596 compareJumpOp(
1597     jngreatereq, OpJngreatereq,
1598     macro (left, right, target) bilt left, right, target end,
1599     macro (left, right, target) bdltun left, right, target end)
1600 
1601 
1602 equalityJumpOp(
1603     jeq, OpJeq,
1604     macro (left, right, target) bieq left, right, target end)
1605 
1606 
1607 equalityJumpOp(
1608     jneq, OpJneq,
1609     macro (left, right, target) bineq left, right, target end)
1610 
1611 
1612 compareUnsignedJumpOp(
1613     jbelow, OpJbelow,
1614     macro (left, right, target) bib left, right, target end)
1615 
1616 
1617 compareUnsignedJumpOp(
1618     jbeloweq, OpJbeloweq,
1619     macro (left, right, target) bibeq left, right, target end)
1620 
1621 
1622 preOp(inc, OpInc,
1623     macro (value, slow) baddio 1, value, slow end)
1624 
1625 
1626 preOp(dec, OpDec,
1627     macro (value, slow) bsubio 1, value, slow end)
1628 
1629 
1630 llintOp(op_loop_hint, OpLoopHint, macro (unused, unused, dispatch)
1631     checkSwitchToJITForLoop()
1632     dispatch()
1633 end)
1634 
1635 
1636 llintOp(op_check_traps, OpCheckTraps, macro (unused, unused, dispatch)
1637     loadp CodeBlock[cfr], t1
1638     loadp CodeBlock::m_vm[t1], t1
1639     loadb VM::m_traps+VMTraps::m_needTrapHandling[t1], t0
1640     btpnz t0, .handleTraps
1641 .afterHandlingTraps:
1642     dispatch()
1643 .handleTraps:
1644     callTrapHandler(.throwHandler)
1645     jmp .afterHandlingTraps
1646 .throwHandler:
1647     jmp _llint_throw_from_slow_path_trampoline
1648 end)
1649 
1650 
1651 # Returns the packet pointer in t0.
1652 macro acquireShadowChickenPacket(slow)
1653     loadp CodeBlock[cfr], t1
1654     loadp CodeBlock::m_vm[t1], t1
1655     loadp VM::m_shadowChicken[t1], t2
1656     loadp ShadowChicken::m_logCursor[t2], t0
1657     bpaeq t0, ShadowChicken::m_logEnd[t2], slow
1658     addp sizeof ShadowChicken::Packet, t0, t1
1659     storep t1, ShadowChicken::m_logCursor[t2]
1660 end
1661 
1662 
1663 llintOp(op_nop, OpNop, macro (unused, unused, dispatch)
1664     dispatch()
1665 end)
1666 
1667 
1668 # we can&#39;t use callOp because we can&#39;t pass `call` as the opcode name, since it&#39;s an instruction name
1669 commonCallOp(op_call, _llint_slow_path_call, OpCall, prepareForRegularCall, macro (getu, metadata)
1670     arrayProfileForCall(OpCall, getu)
1671 end)
1672 
1673 
1674 macro callOp(opcodeName, opcodeStruct, prepareCall, fn)
1675     commonCallOp(op_%opcodeName%, _llint_slow_path_%opcodeName%, opcodeStruct, prepareCall, fn)
1676 end
1677 
1678 
1679 callOp(tail_call, OpTailCall, prepareForTailCall, macro (getu, metadata)
1680     arrayProfileForCall(OpTailCall, getu)
1681     checkSwitchToJITForEpilogue()
1682     # reload metadata since checkSwitchToJITForEpilogue() might have trashed t5
1683     metadata(t5, t0)
1684 end)
1685 
1686 
1687 callOp(construct, OpConstruct, prepareForRegularCall, macro (getu, metadata) end)
1688 
1689 
1690 macro doCallVarargs(size, opcodeStruct, dispatch, frameSlowPath, slowPath, prepareCall)
1691     callSlowPath(frameSlowPath)
1692     branchIfException(_llint_throw_from_slow_path_trampoline)
1693     # calleeFrame in r1
1694     if JSVALUE64
1695         move r1, sp
1696     else
1697         # The calleeFrame is not stack aligned, move down by CallerFrameAndPCSize to align
1698         if ARMv7
1699             subp r1, CallerFrameAndPCSize, t2
1700             move t2, sp
1701         else
1702             subp r1, CallerFrameAndPCSize, sp
1703         end
1704     end
1705     slowPathForCall(size, opcodeStruct, dispatch, slowPath, prepareCall)
1706 end
1707 
1708 
1709 llintOp(op_call_varargs, OpCallVarargs, macro (size, get, dispatch)
1710     doCallVarargs(size, OpCallVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_call_varargs, prepareForRegularCall)
1711 end)
1712 
1713 llintOp(op_tail_call_varargs, OpTailCallVarargs, macro (size, get, dispatch)
1714     checkSwitchToJITForEpilogue()
1715     # We lie and perform the tail call instead of preparing it since we can&#39;t
1716     # prepare the frame for a call opcode
1717     doCallVarargs(size, OpTailCallVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_tail_call_varargs, prepareForTailCall)
1718 end)
1719 
1720 
1721 llintOp(op_tail_call_forward_arguments, OpTailCallForwardArguments, macro (size, get, dispatch)
1722     checkSwitchToJITForEpilogue()
1723     # We lie and perform the tail call instead of preparing it since we can&#39;t
1724     # prepare the frame for a call opcode
1725     doCallVarargs(size, OpTailCallForwardArguments, dispatch, _llint_slow_path_size_frame_for_forward_arguments, _llint_slow_path_tail_call_forward_arguments, prepareForTailCall)
1726 end)
1727 
1728 
1729 llintOp(op_construct_varargs, OpConstructVarargs, macro (size, get, dispatch)
1730     doCallVarargs(size, OpConstructVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_construct_varargs, prepareForRegularCall)
1731 end)
1732 
1733 
1734 # Eval is executed in one of two modes:
1735 #
1736 # 1) We find that we&#39;re really invoking eval() in which case the
1737 #    execution is perfomed entirely inside the slow_path, and it
1738 #    returns the PC of a function that just returns the return value
1739 #    that the eval returned.
1740 #
1741 # 2) We find that we&#39;re invoking something called eval() that is not
1742 #    the real eval. Then the slow_path returns the PC of the thing to
1743 #    call, and we call it.
1744 #
1745 # This allows us to handle two cases, which would require a total of
1746 # up to four pieces of state that cannot be easily packed into two
1747 # registers (C functions can return up to two registers, easily):
1748 #
1749 # - The call frame register. This may or may not have been modified
1750 #   by the slow_path, but the convention is that it returns it. It&#39;s not
1751 #   totally clear if that&#39;s necessary, since the cfr is callee save.
1752 #   But that&#39;s our style in this here interpreter so we stick with it.
1753 #
1754 # - A bit to say if the slow_path successfully executed the eval and has
1755 #   the return value, or did not execute the eval but has a PC for us
1756 #   to call.
1757 #
1758 # - Either:
1759 #   - The JS return value (two registers), or
1760 #
1761 #   - The PC to call.
1762 #
1763 # It turns out to be easier to just always have this return the cfr
1764 # and a PC to call, and that PC may be a dummy thunk that just
1765 # returns the JS value that the eval returned.
1766 
1767 _llint_op_call_eval:
1768     slowPathForCall(
1769         narrow,
1770         OpCallEval,
1771         macro () dispatchOp(narrow, op_call_eval) end,
1772         _llint_slow_path_call_eval,
1773         prepareForRegularCall)
1774 
1775 _llint_op_call_eval_wide:
1776     slowPathForCall(
1777         wide,
1778         OpCallEval,
1779         macro () dispatchOp(wide, op_call_eval) end,
1780         _llint_slow_path_call_eval_wide,
1781         prepareForRegularCall)
1782 
1783 _llint_generic_return_point:
1784     dispatchAfterCall(narrow, OpCallEval, macro ()
1785         dispatchOp(narrow, op_call_eval)
1786     end)
1787 
1788 _llint_generic_return_point_wide:
1789     dispatchAfterCall(wide, OpCallEval, macro()
1790         dispatchOp(wide, op_call_eval)
1791     end)
1792 
1793 llintOp(op_identity_with_profile, OpIdentityWithProfile, macro (unused, unused, dispatch)
1794     dispatch()
1795 end)
1796 
1797 
1798 llintOp(op_yield, OpYield, macro (unused, unused, unused)
1799     notSupported()
1800 end)
1801 
1802 
1803 llintOp(op_debug, OpDebug, macro (unused, unused, dispatch)
1804     loadp CodeBlock[cfr], t0
1805     loadi CodeBlock::m_debuggerRequests[t0], t0
1806     btiz t0, .opDebugDone
1807     callSlowPath(_llint_slow_path_debug)
1808 .opDebugDone:                    
1809     dispatch()
1810 end)
1811 
1812 
1813 op(llint_native_call_trampoline, macro ()
1814     nativeCallTrampoline(NativeExecutable::m_function)
1815 end)
1816 
1817 
1818 op(llint_native_construct_trampoline, macro ()
1819     nativeCallTrampoline(NativeExecutable::m_constructor)
1820 end)
1821 
1822 
1823 op(llint_internal_function_call_trampoline, macro ()
1824     internalFunctionCallTrampoline(InternalFunction::m_functionForCall)
1825 end)
1826 
1827 
1828 op(llint_internal_function_construct_trampoline, macro ()
1829     internalFunctionCallTrampoline(InternalFunction::m_functionForConstruct)
1830 end)
1831 
1832 
1833 # Lastly, make sure that we can link even though we don&#39;t support all opcodes.
1834 # These opcodes should never arise when using LLInt or either JIT. We assert
1835 # as much.
1836 
1837 macro notSupported()
1838     if ASSERT_ENABLED
1839         crash()
1840     else
1841         # We should use whatever the smallest possible instruction is, just to
1842         # ensure that there is a gap between instruction labels. If multiple
1843         # smallest instructions exist, we should pick the one that is most
1844         # likely result in execution being halted. Currently that is the break
1845         # instruction on all architectures we&#39;re interested in. (Break is int3
1846         # on Intel, which is 1 byte, and bkpt on ARMv7, which is 2 bytes.)
1847         break
1848     end
1849 end
    </pre>
  </body>
</html>