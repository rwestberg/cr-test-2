<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2012-2017 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;LowLevelInterpreter.h&quot;
 28 
 29 #include &quot;LLIntOfflineAsmConfig.h&quot;
 30 #include &lt;wtf/InlineASM.h&gt;
 31 
 32 #if ENABLE(C_LOOP)
 33 #include &quot;Bytecodes.h&quot;
 34 #include &quot;CLoopStackInlines.h&quot;
 35 #include &quot;CodeBlock.h&quot;
 36 #include &quot;CommonSlowPaths.h&quot;
 37 #include &quot;Interpreter.h&quot;
 38 #include &quot;LLIntCLoop.h&quot;
 39 #include &quot;LLIntData.h&quot;
 40 #include &quot;LLIntSlowPaths.h&quot;
 41 #include &quot;JSCInlines.h&quot;
 42 #include &lt;wtf/Assertions.h&gt;
 43 #include &lt;wtf/MathExtras.h&gt;
 44 
 45 using namespace JSC::LLInt;
 46 
 47 // LLInt C Loop opcodes
 48 // ====================
 49 // In the implementation of the C loop, the LLint trampoline glue functions
 50 // (e.g. llint_program_prologue, llint_eval_prologue, etc) are addressed as
 51 // if they are bytecode handlers. That means the names of the trampoline
 52 // functions will be added to the OpcodeID list via the
 53 // FOR_EACH_LLINT_OPCODE_EXTENSION() macro that FOR_EACH_OPCODE_ID()
 54 // includes.
 55 //
 56 // In addition, some JIT trampoline functions which are needed by LLInt
 57 // (e.g. getHostCallReturnValue, ctiOpThrowNotCaught) are also added as
 58 // bytecodes, and the CLoop will provide bytecode handlers for them.
 59 //
 60 // In the CLoop, we can only dispatch indirectly to these bytecodes
 61 // (including the LLInt and JIT extensions). All other dispatches
 62 // (i.e. goto&#39;s) must be to a known label (i.e. local / global labels).
 63 
 64 
 65 // How are the opcodes named?
 66 // ==========================
 67 // Here is a table to show examples of how each of the manifestation of the
 68 // opcodes are named:
 69 //
 70 //   Type:                        Opcode            Trampoline Glue
 71 //                                ======            ===============
 72 //   [In the llint .asm files]
 73 //   llint labels:                llint_op_enter    llint_program_prologue
 74 //
 75 //   OpcodeID:                    op_enter          llint_program
 76 //                                [in Opcode.h]     [in LLIntOpcode.h]
 77 //
 78 //   When using a switch statement dispatch in the CLoop, each &quot;opcode&quot; is
 79 //   a case statement:
 80 //   Opcode:                      case op_enter:    case llint_program_prologue:
 81 //
 82 //   When using a computed goto dispatch in the CLoop, each opcode is a label:
 83 //   Opcode:                      op_enter:         llint_program_prologue:
 84 
 85 
 86 //============================================================================
 87 // Define the opcode dispatch mechanism when using the C loop:
 88 //
 89 
 90 // These are for building a C Loop interpreter:
 91 #define OFFLINE_ASM_BEGIN
 92 #define OFFLINE_ASM_END
 93 
 94 #if ENABLE(OPCODE_TRACING)
 95 #define TRACE_OPCODE(opcode) dataLogF(&quot;   op %s\n&quot;, #opcode)
 96 #else
 97 #define TRACE_OPCODE(opcode)
 98 #endif
 99 
100 // To keep compilers happy in case of unused labels, force usage of the label:
101 #define USE_LABEL(label) \
102     do { \
103         if (false) \
104             goto label; \
105     } while (false)
106 
107 #define OFFLINE_ASM_OPCODE_LABEL(opcode) DEFINE_OPCODE(opcode) USE_LABEL(opcode); TRACE_OPCODE(opcode);
108 
109 #define OFFLINE_ASM_GLOBAL_LABEL(label)  label: USE_LABEL(label);
110 
111 #if ENABLE(LABEL_TRACING)
112 #define TRACE_LABEL(prefix, label) dataLog(#prefix, &quot;: &quot;, #label, &quot;\n&quot;)
113 #else
114 #define TRACE_LABEL(prefix, label) do { } while (false);
115 #endif
116 
117 
118 #if ENABLE(COMPUTED_GOTO_OPCODES)
119 #define OFFLINE_ASM_GLUE_LABEL(label) label: TRACE_LABEL(&quot;OFFLINE_ASM_GLUE_LABEL&quot;, label); USE_LABEL(label);
120 #else
121 #define OFFLINE_ASM_GLUE_LABEL(label)  case label: label: USE_LABEL(label);
122 #endif
123 
124 #define OFFLINE_ASM_LOCAL_LABEL(label) label: TRACE_LABEL(&quot;OFFLINE_ASM_LOCAL_LABEL&quot;, #label); USE_LABEL(label);
125 
126 namespace JSC {
127 
128 //============================================================================
129 // CLoopRegister is the storage for an emulated CPU register.
130 // It defines the policy of how ints smaller than intptr_t are packed into the
131 // pseudo register, as well as hides endianness differences.
132 
133 class CLoopRegister {
134 public:
135     ALWAYS_INLINE intptr_t i() const { return m_value; };
136     ALWAYS_INLINE uintptr_t u() const { return m_value; }
137     ALWAYS_INLINE int32_t i32() const { return m_value; }
138     ALWAYS_INLINE uint32_t u32() const { return m_value; }
139     ALWAYS_INLINE int8_t i8() const { return m_value; }
140     ALWAYS_INLINE uint8_t u8() const { return m_value; }
141 
142     ALWAYS_INLINE intptr_t* ip() const { return bitwise_cast&lt;intptr_t*&gt;(m_value); }
143     ALWAYS_INLINE int8_t* i8p() const { return bitwise_cast&lt;int8_t*&gt;(m_value); }
144     ALWAYS_INLINE void* vp() const { return bitwise_cast&lt;void*&gt;(m_value); }
145     ALWAYS_INLINE const void* cvp() const { return bitwise_cast&lt;const void*&gt;(m_value); }
146     ALWAYS_INLINE CallFrame* callFrame() const { return bitwise_cast&lt;CallFrame*&gt;(m_value); }
147     ALWAYS_INLINE ExecState* execState() const { return bitwise_cast&lt;ExecState*&gt;(m_value); }
148     ALWAYS_INLINE const void* instruction() const { return bitwise_cast&lt;const void*&gt;(m_value); }
149     ALWAYS_INLINE VM* vm() const { return bitwise_cast&lt;VM*&gt;(m_value); }
150     ALWAYS_INLINE JSCell* cell() const { return bitwise_cast&lt;JSCell*&gt;(m_value); }
151     ALWAYS_INLINE ProtoCallFrame* protoCallFrame() const { return bitwise_cast&lt;ProtoCallFrame*&gt;(m_value); }
152     ALWAYS_INLINE NativeFunction nativeFunc() const { return bitwise_cast&lt;NativeFunction&gt;(m_value); }
153 #if USE(JSVALUE64)
154     ALWAYS_INLINE int64_t i64() const { return m_value; }
155     ALWAYS_INLINE uint64_t u64() const { return m_value; }
156     ALWAYS_INLINE EncodedJSValue encodedJSValue() const { return bitwise_cast&lt;EncodedJSValue&gt;(m_value); }
157 #endif
158     ALWAYS_INLINE Opcode opcode() const { return bitwise_cast&lt;Opcode&gt;(m_value); }
159 
160     operator ExecState*() { return bitwise_cast&lt;ExecState*&gt;(m_value); }
161     operator const Instruction*() { return bitwise_cast&lt;const Instruction*&gt;(m_value); }
162     operator JSCell*() { return bitwise_cast&lt;JSCell*&gt;(m_value); }
163     operator ProtoCallFrame*() { return bitwise_cast&lt;ProtoCallFrame*&gt;(m_value); }
164     operator Register*() { return bitwise_cast&lt;Register*&gt;(m_value); }
165     operator VM*() { return bitwise_cast&lt;VM*&gt;(m_value); }
166 
167     template&lt;typename T, typename = std::enable_if_t&lt;sizeof(T) == sizeof(uintptr_t)&gt;&gt;
168     ALWAYS_INLINE void operator=(T value) { m_value = bitwise_cast&lt;uintptr_t&gt;(value); }
169 #if USE(JSVALUE64)
170     ALWAYS_INLINE void operator=(int32_t value) { m_value = static_cast&lt;intptr_t&gt;(value); }
171     ALWAYS_INLINE void operator=(uint32_t value) { m_value = static_cast&lt;uintptr_t&gt;(value); }
172 #endif
173     ALWAYS_INLINE void operator=(int16_t value) { m_value = static_cast&lt;intptr_t&gt;(value); }
174     ALWAYS_INLINE void operator=(uint16_t value) { m_value = static_cast&lt;uintptr_t&gt;(value); }
175     ALWAYS_INLINE void operator=(int8_t value) { m_value = static_cast&lt;intptr_t&gt;(value); }
176     ALWAYS_INLINE void operator=(uint8_t value) { m_value = static_cast&lt;uintptr_t&gt;(value); }
177     ALWAYS_INLINE void operator=(bool value) { m_value = static_cast&lt;uintptr_t&gt;(value); }
178 
179 #if USE(JSVALUE64)
180     ALWAYS_INLINE double bitsAsDouble() const { return bitwise_cast&lt;double&gt;(m_value); }
181     ALWAYS_INLINE int64_t bitsAsInt64() const { return bitwise_cast&lt;int64_t&gt;(m_value); }
182 #endif
183 
184 private:
185     uintptr_t m_value { static_cast&lt;uintptr_t&gt;(0xbadbeef0baddbeef) };
186 };
187 
188 class CLoopDoubleRegister {
189 public:
190     template&lt;typename T&gt;
191     explicit operator T() const { return bitwise_cast&lt;T&gt;(m_value); }
192 
193     ALWAYS_INLINE double d() const { return m_value; }
194     ALWAYS_INLINE int64_t bitsAsInt64() const { return bitwise_cast&lt;int64_t&gt;(m_value); }
195 
196     ALWAYS_INLINE void operator=(double value) { m_value = value; }
197 
198     template&lt;typename T, typename = std::enable_if_t&lt;sizeof(T) == sizeof(uintptr_t) &amp;&amp; std::is_integral&lt;T&gt;::value&gt;&gt;
199     ALWAYS_INLINE void operator=(T value) { m_value = bitwise_cast&lt;double&gt;(value); }
200 
201 private:
202     double m_value;
203 };
204 
205 //============================================================================
206 // Some utilities:
207 //
208 
209 namespace LLInt {
210 
211 #if USE(JSVALUE32_64)
212 static double ints2Double(uint32_t lo, uint32_t hi)
213 {
214     uint64_t value = (static_cast&lt;uint64_t&gt;(hi) &lt;&lt; 32) | lo;
215     return bitwise_cast&lt;double&gt;(value);
216 }
217 
218 static void double2Ints(double val, CLoopRegister&amp; lo, CLoopRegister&amp; hi)
219 {
220     uint64_t value = bitwise_cast&lt;uint64_t&gt;(val);
221     hi = static_cast&lt;uint32_t&gt;(value &gt;&gt; 32);
222     lo = static_cast&lt;uint32_t&gt;(value);
223 }
224 #endif // USE(JSVALUE32_64)
225 
226 static void decodeResult(SlowPathReturnType result, CLoopRegister&amp; t0, CLoopRegister&amp; t1)
227 {
228     const void* t0Result;
229     const void* t1Result;
230     JSC::decodeResult(result, t0Result, t1Result);
231     t0 = t0Result;
232     t1 = t1Result;
233 }
234 
235 } // namespace LLint
236 
237 //============================================================================
238 // The llint C++ interpreter loop:
239 //
240 
241 JSValue CLoop::execute(OpcodeID entryOpcodeID, void* executableAddress, VM* vm, ProtoCallFrame* protoCallFrame, bool isInitializationPass)
242 {
243 #define CAST bitwise_cast
244 
245     // One-time initialization of our address tables. We have to put this code
246     // here because our labels are only in scope inside this function. The
247     // caller (or one of its ancestors) is responsible for ensuring that this
248     // is only called once during the initialization of the VM before threads
249     // are at play.
250     if (UNLIKELY(isInitializationPass)) {
251         Opcode* opcodeMap = LLInt::opcodeMap();
252         Opcode* opcodeMapWide = LLInt::opcodeMapWide();
253 
254 #if ENABLE(COMPUTED_GOTO_OPCODES)
255         #define OPCODE_ENTRY(__opcode, length) \
256             opcodeMap[__opcode] = bitwise_cast&lt;void*&gt;(&amp;&amp;__opcode); \
257             opcodeMapWide[__opcode] = bitwise_cast&lt;void*&gt;(&amp;&amp;__opcode##_wide);
258 
259         #define LLINT_OPCODE_ENTRY(__opcode, length) \
260             opcodeMap[__opcode] = bitwise_cast&lt;void*&gt;(&amp;&amp;__opcode);
261 #else
262         // FIXME: this mapping is unnecessarily expensive in the absence of COMPUTED_GOTO
263         //   narrow opcodes don&#39;t need any mapping and wide opcodes just need to add numOpcodeIDs
264         #define OPCODE_ENTRY(__opcode, length) \
265             opcodeMap[__opcode] = __opcode; \
266             opcodeMapWide[__opcode] = static_cast&lt;OpcodeID&gt;(__opcode##_wide);
267 
268         #define LLINT_OPCODE_ENTRY(__opcode, length) \
269             opcodeMap[__opcode] = __opcode;
270 #endif
271         FOR_EACH_BYTECODE_ID(OPCODE_ENTRY)
272         FOR_EACH_CLOOP_BYTECODE_HELPER_ID(LLINT_OPCODE_ENTRY)
273         FOR_EACH_LLINT_NATIVE_HELPER(LLINT_OPCODE_ENTRY)
274         #undef OPCODE_ENTRY
275         #undef LLINT_OPCODE_ENTRY
276 
277         // Note: we can only set the exceptionInstructions after we have
278         // initialized the opcodeMap above. This is because getCodePtr()
279         // can depend on the opcodeMap.
280         uint8_t* exceptionInstructions = reinterpret_cast&lt;uint8_t*&gt;(LLInt::exceptionInstructions());
281         for (int i = 0; i &lt; maxOpcodeLength + 1; ++i)
282             exceptionInstructions[i] = llint_throw_from_slow_path_trampoline;
283 
284         return JSValue();
285     }
286 
287     // Define the pseudo registers used by the LLINT C Loop backend:
288     ASSERT(sizeof(CLoopRegister) == sizeof(intptr_t));
289 
290     // The CLoop llint backend is initially based on the ARMv7 backend, and
291     // then further enhanced with a few instructions from the x86 backend to
292     // support building for X64 targets. Hence, the shape of the generated
293     // code and the usage convention of registers will look a lot like the
294     // ARMv7 backend&#39;s.
295     //
296     // For example, on a 32-bit build:
297     // 1. Outgoing args will be set up as follows:
298     //    arg1 in t0 (r0 on ARM)
299     //    arg2 in t1 (r1 on ARM)
300     // 2. 32 bit return values will be in t0 (r0 on ARM).
301     // 3. 64 bit return values (e.g. doubles) will be in t0,t1 (r0,r1 on ARM).
302     //
303     // But instead of naming these simulator registers based on their ARM
304     // counterparts, we&#39;ll name them based on their original llint asm names.
305     // This will make it easier to correlate the generated code with the
306     // original llint asm code.
307     //
308     // On a 64-bit build, it more like x64 in that the registers are 64 bit.
309     // Hence:
310     // 1. Outgoing args are still the same: arg1 in t0, arg2 in t1, etc.
311     // 2. 32 bit result values will be in the low 32-bit of t0.
312     // 3. 64 bit result values will be in t0.
313 
314     CLoopRegister t0, t1, t2, t3, t5, sp, cfr, lr, pc;
315 #if USE(JSVALUE64)
316     CLoopRegister pcBase, tagTypeNumber, tagMask;
317 #endif
318     CLoopRegister metadataTable;
319     CLoopDoubleRegister d0, d1;
320 
321     struct StackPointerScope {
322         StackPointerScope(CLoopStack&amp; stack)
323             : m_stack(stack)
324             , m_originalStackPointer(stack.currentStackPointer())
325         { }
326 
327         ~StackPointerScope()
328         {
329             m_stack.setCurrentStackPointer(m_originalStackPointer);
330         }
331 
332     private:
333         CLoopStack&amp; m_stack;
334         void* m_originalStackPointer;
335     };
336 
337     CLoopStack&amp; cloopStack = vm-&gt;interpreter-&gt;cloopStack();
338     StackPointerScope stackPointerScope(cloopStack);
339 
340     lr = getOpcode(llint_return_to_host);
341     sp = cloopStack.currentStackPointer();
342     cfr = vm-&gt;topCallFrame;
343 #ifndef NDEBUG
344     void* startSP = sp.vp();
345     CallFrame* startCFR = cfr.callFrame();
346 #endif
347 
348     // Initialize the incoming args for doVMEntryToJavaScript:
349     t0 = executableAddress;
350     t1 = vm;
351     t2 = protoCallFrame;
352 
353 #if USE(JSVALUE64)
354     // For the ASM llint, JITStubs takes care of this initialization. We do
355     // it explicitly here for the C loop:
356     tagTypeNumber = 0xFFFF000000000000;
357     tagMask = 0xFFFF000000000002;
358 #endif // USE(JSVALUE64)
359 
360     // Interpreter variables for value passing between opcodes and/or helpers:
361     NativeFunction nativeFunc = nullptr;
362     JSValue functionReturnValue;
363     Opcode opcode = getOpcode(entryOpcodeID);
364 
365 #define PUSH(cloopReg) \
366     do { \
367         sp = sp.ip() - 1; \
368         *sp.ip() = cloopReg.i(); \
369     } while (false)
370 
371 #define POP(cloopReg) \
372     do { \
373         cloopReg = *sp.ip(); \
374         sp = sp.ip() + 1; \
375     } while (false)
376 
377 #if ENABLE(OPCODE_STATS)
378 #define RECORD_OPCODE_STATS(__opcode) OpcodeStats::recordInstruction(__opcode)
379 #else
380 #define RECORD_OPCODE_STATS(__opcode)
381 #endif
382 
383 #if USE(JSVALUE32_64)
384 #define FETCH_OPCODE() *pc.i8p
385 #else // USE(JSVALUE64)
386 #define FETCH_OPCODE() *bitwise_cast&lt;OpcodeID*&gt;(pcBase.i8p + pc.i)
387 #endif // USE(JSVALUE64)
388 
389 #define NEXT_INSTRUCTION() \
390     do {                         \
391         opcode = FETCH_OPCODE(); \
392         DISPATCH_OPCODE();       \
393     } while (false)
394 
395 #if ENABLE(COMPUTED_GOTO_OPCODES)
396 
397     //========================================================================
398     // Loop dispatch mechanism using computed goto statements:
399 
400     #define DISPATCH_OPCODE() goto *opcode
401 
402     #define DEFINE_OPCODE(__opcode) \
403         __opcode: \
404             RECORD_OPCODE_STATS(__opcode);
405 
406     // Dispatch to the current PC&#39;s bytecode:
407     DISPATCH_OPCODE();
408 
409 #else // !ENABLE(COMPUTED_GOTO_OPCODES)
410     //========================================================================
411     // Loop dispatch mechanism using a C switch statement:
412 
413     #define DISPATCH_OPCODE() goto dispatchOpcode
414 
415     #define DEFINE_OPCODE(__opcode) \
416         case __opcode: \
417         __opcode: \
418             RECORD_OPCODE_STATS(__opcode);
419 
420     // Dispatch to the current PC&#39;s bytecode:
421     dispatchOpcode:
422     switch (static_cast&lt;unsigned&gt;(opcode))
423 
424 #endif // !ENABLE(COMPUTED_GOTO_OPCODES)
425 
426     //========================================================================
427     // Bytecode handlers:
428     {
429         // This is the file generated by offlineasm, which contains all of the
430         // bytecode handlers for the interpreter, as compiled from
431         // LowLevelInterpreter.asm and its peers.
432 
433         IGNORE_CLANG_WARNINGS_BEGIN(&quot;unreachable-code&quot;)
434         #include &quot;LLIntAssembly.h&quot;
435         IGNORE_CLANG_WARNINGS_END
436 
437         OFFLINE_ASM_GLUE_LABEL(llint_return_to_host)
438         {
439             ASSERT(startSP == sp.vp());
440             ASSERT(startCFR == cfr.callFrame());
441 #if USE(JSVALUE32_64)
442             return JSValue(t1.i(), t0.i()); // returning JSValue(tag, payload);
443 #else
444             return JSValue::decode(t0.encodedJSValue());
445 #endif
446         }
447 
448         // In the ASM llint, getHostCallReturnValue() is a piece of glue
449         // function provided by the JIT (see jit/JITOperations.cpp).
450         // We simulate it here with a pseduo-opcode handler.
451         OFFLINE_ASM_GLUE_LABEL(getHostCallReturnValue)
452         {
453             // The part in getHostCallReturnValueWithExecState():
454             JSValue result = vm-&gt;hostCallReturnValue;
455 #if USE(JSVALUE32_64)
456             t1 = result.tag();
457             t0 = result.payload();
458 #else
459             t0 = JSValue::encode(result);
460 #endif
461             opcode = lr.opcode();
462             DISPATCH_OPCODE();
463         }
464 
465 #if !ENABLE(COMPUTED_GOTO_OPCODES)
466     default:
467         ASSERT(false);
468 #endif
469 
470     } // END bytecode handler cases.
471 
472 #if ENABLE(COMPUTED_GOTO_OPCODES)
473     // Keep the compiler happy so that it doesn&#39;t complain about unused
474     // labels for the LLInt trampoline glue. The labels are automatically
475     // emitted by label macros above, and some of them are referenced by
476     // the llint generated code. Since we can&#39;t tell ahead of time which
477     // will be referenced and which will be not, we&#39;ll just passify the
478     // compiler on all such labels:
479     #define LLINT_OPCODE_ENTRY(__opcode, length) \
480         UNUSED_LABEL(__opcode);
481         FOR_EACH_OPCODE_ID(LLINT_OPCODE_ENTRY);
482     #undef LLINT_OPCODE_ENTRY
483 #endif
484 
485     #undef NEXT_INSTRUCTION
486     #undef DEFINE_OPCODE
487     #undef CHECK_FOR_TIMEOUT
488     #undef CAST
489 
490     return JSValue(); // to suppress a compiler warning.
491 } // Interpreter::llintCLoopExecute()
492 
493 } // namespace JSC
494 
495 #elif !COMPILER(MSVC)
496 
497 //============================================================================
498 // Define the opcode dispatch mechanism when using an ASM loop:
499 //
500 
501 // These are for building an interpreter from generated assembly code:
502 #define OFFLINE_ASM_BEGIN   asm (
503 #define OFFLINE_ASM_END     );
504 
505 #if USE(LLINT_EMBEDDED_OPCODE_ID)
506 #define EMBED_OPCODE_ID_IF_NEEDED(__opcode) &quot;.int &quot; __opcode##_value_string &quot;\n&quot;
507 #else
508 #define EMBED_OPCODE_ID_IF_NEEDED(__opcode)
509 #endif
510 
511 #define OFFLINE_ASM_OPCODE_LABEL(__opcode) \
512     EMBED_OPCODE_ID_IF_NEEDED(__opcode) \
513     OFFLINE_ASM_OPCODE_DEBUG_LABEL(llint_##__opcode) \
514     OFFLINE_ASM_LOCAL_LABEL(llint_##__opcode)
515 
516 #define OFFLINE_ASM_GLUE_LABEL(__opcode)   OFFLINE_ASM_LOCAL_LABEL(__opcode)
517 
518 #if CPU(ARM_THUMB2)
519 #define OFFLINE_ASM_GLOBAL_LABEL(label)          \
520     &quot;.text\n&quot;                                    \
521     &quot;.align 4\n&quot;                                 \
522     &quot;.globl &quot; SYMBOL_STRING(label) &quot;\n&quot;          \
523     HIDE_SYMBOL(label) &quot;\n&quot;                      \
524     &quot;.thumb\n&quot;                                   \
525     &quot;.thumb_func &quot; THUMB_FUNC_PARAM(label) &quot;\n&quot;  \
526     SYMBOL_STRING(label) &quot;:\n&quot;
527 #elif CPU(ARM64)
528 #define OFFLINE_ASM_GLOBAL_LABEL(label)         \
529     &quot;.text\n&quot;                                   \
530     &quot;.align 4\n&quot;                                \
531     &quot;.globl &quot; SYMBOL_STRING(label) &quot;\n&quot;         \
532     HIDE_SYMBOL(label) &quot;\n&quot;                     \
533     SYMBOL_STRING(label) &quot;:\n&quot;
534 #else
535 #define OFFLINE_ASM_GLOBAL_LABEL(label)         \
536     &quot;.text\n&quot;                                   \
537     &quot;.globl &quot; SYMBOL_STRING(label) &quot;\n&quot;         \
538     HIDE_SYMBOL(label) &quot;\n&quot;                     \
539     SYMBOL_STRING(label) &quot;:\n&quot;
540 #endif
541 
542 #define OFFLINE_ASM_LOCAL_LABEL(label)   LOCAL_LABEL_STRING(label) &quot;:\n&quot;
543 
544 #if OS(LINUX)
545 #define OFFLINE_ASM_OPCODE_DEBUG_LABEL(label)  #label &quot;:\n&quot;
546 #else
547 #define OFFLINE_ASM_OPCODE_DEBUG_LABEL(label)
548 #endif
549 
550 // This is a file generated by offlineasm, which contains all of the assembly code
551 // for the interpreter, as compiled from LowLevelInterpreter.asm.
552 #include &quot;LLIntAssembly.h&quot;
553 
554 #endif // ENABLE(C_LOOP)
    </pre>
  </body>
</html>