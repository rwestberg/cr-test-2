<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter.asm</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="LLIntThunks.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="LowLevelInterpreter.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter.asm</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
<span class="line-modified">   1 # Copyright (C) 2011-2019 Apple Inc. All rights reserved.</span>
   2 #
   3 # Redistribution and use in source and binary forms, with or without
   4 # modification, are permitted provided that the following conditions
   5 # are met:
   6 # 1. Redistributions of source code must retain the above copyright
   7 #    notice, this list of conditions and the following disclaimer.
   8 # 2. Redistributions in binary form must reproduce the above copyright
   9 #    notice, this list of conditions and the following disclaimer in the
  10 #    documentation and/or other materials provided with the distribution.
  11 #
  12 # THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
  13 # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
  14 # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  15 # PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
  16 # BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  17 # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  18 # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  19 # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  20 # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  21 # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
</pre>
<hr />
<pre>
  59 #   you can use it to select different pieces of code for different
  60 #   platforms.
  61 #
  62 # - Arguments to macros follow lexical scoping rather than dynamic scoping.
  63 #   Const&#39;s also follow lexical scoping and may override (hide) arguments
  64 #   or other consts. All variables (arguments and constants) can be bound
  65 #   to operands. Additionally, arguments (but not constants) can be bound
  66 #   to macros.
  67 
  68 # The following general-purpose registers are available:
  69 #
  70 #  - cfr and sp hold the call frame and (native) stack pointer respectively.
  71 #  They are callee-save registers, and guaranteed to be distinct from all other
  72 #  registers on all architectures.
  73 #
  74 #  - lr is defined on non-X86 architectures (ARM64, ARM64E, ARMv7, MIPS and CLOOP)
  75 #  and holds the return PC
  76 #
  77 #  - pc holds the (native) program counter on 32-bits ARM architectures (ARMv7)
  78 #
<span class="line-modified">  79 #  - t0, t1, t2, t3, t4 and optionally t5 are temporary registers that can get trashed on</span>
  80 #  calls, and are pairwise distinct registers. t4 holds the JS program counter, so use
  81 #  with caution in opcodes (actually, don&#39;t use it in opcodes at all, except as PC).
  82 #
  83 #  - r0 and r1 are the platform&#39;s customary return registers, and thus are
  84 #  two distinct registers
  85 #
  86 #  - a0, a1, a2 and a3 are the platform&#39;s customary argument registers, and
  87 #  thus are pairwise distinct registers. Be mindful that:
  88 #    + On X86, there are no argument registers. a0 and a1 are edx and
  89 #    ecx following the fastcall convention, but you should still use the stack
  90 #    to pass your arguments. The cCall2 and cCall4 macros do this for you.
  91 #    + On X86_64_WIN, you should allocate space on the stack for the arguments,
  92 #    and the return convention is weird for &gt; 8 bytes types. The only place we
  93 #    use &gt; 8 bytes return values is on a cCall, and cCall2 and cCall4 handle
  94 #    this for you.
  95 #
  96 #  - The only registers guaranteed to be caller-saved are r0, r1, a0, a1 and a2, and
  97 #  you should be mindful of that in functions that are called directly from C.
  98 #  If you need more registers, you should push and pop them like a good
  99 #  assembly citizen, because any other register will be callee-saved on X86.
</pre>
<hr />
<pre>
 164 end
 165 
 166 const JSLexicalEnvironment_variables = (sizeof JSLexicalEnvironment + SlotSize - 1) &amp; ~(SlotSize - 1)
 167 const DirectArguments_storage = (sizeof DirectArguments + SlotSize - 1) &amp; ~(SlotSize - 1)
 168 
 169 const StackAlignment = constexpr (stackAlignmentBytes())
 170 const StackAlignmentSlots = constexpr (stackAlignmentRegisters())
 171 const StackAlignmentMask = StackAlignment - 1
 172 
 173 const CallerFrameAndPCSize = constexpr (sizeof(CallerFrameAndPC))
 174 
 175 const CallerFrame = 0
 176 const ReturnPC = CallerFrame + MachineRegisterSize
 177 const CodeBlock = ReturnPC + MachineRegisterSize
 178 const Callee = CodeBlock + SlotSize
 179 const ArgumentCount = Callee + SlotSize
 180 const ThisArgumentOffset = ArgumentCount + SlotSize
 181 const FirstArgumentOffset = ThisArgumentOffset + SlotSize
 182 const CallFrameHeaderSize = ThisArgumentOffset
 183 



 184 # Some value representation constants.
 185 if JSVALUE64
 186     const TagBitTypeOther = constexpr TagBitTypeOther
 187     const TagBitBool      = constexpr TagBitBool
 188     const TagBitUndefined = constexpr TagBitUndefined
 189     const ValueEmpty      = constexpr ValueEmpty
 190     const ValueFalse      = constexpr ValueFalse
 191     const ValueTrue       = constexpr ValueTrue
 192     const ValueUndefined  = constexpr ValueUndefined
 193     const ValueNull       = constexpr ValueNull
 194     const TagTypeNumber   = constexpr TagTypeNumber
 195     const TagMask         = constexpr TagMask
 196 else
 197     const Int32Tag = constexpr JSValue::Int32Tag
 198     const BooleanTag = constexpr JSValue::BooleanTag
 199     const NullTag = constexpr JSValue::NullTag
 200     const UndefinedTag = constexpr JSValue::UndefinedTag
 201     const CellTag = constexpr JSValue::CellTag
 202     const EmptyValueTag = constexpr JSValue::EmptyValueTag
 203     const DeletedValueTag = constexpr JSValue::DeletedValueTag
 204     const LowestTag = constexpr JSValue::LowestTag
 205 end
 206 
 207 if JSVALUE64
 208     const NumberOfStructureIDEntropyBits = constexpr StructureIDTable::s_numberOfEntropyBits
 209     const StructureEntropyBitsShift = constexpr StructureIDTable::s_entropyBitsShiftForStructurePointer
 210 end
 211 
 212 const CallOpCodeSize = constexpr op_call_length
 213 
 214 const maxFrameExtentForSlowPathCall = constexpr maxFrameExtentForSlowPathCall
 215 
 216 if X86_64 or X86_64_WIN or ARM64 or ARM64E
 217     const CalleeSaveSpaceAsVirtualRegisters = 4
<span class="line-modified"> 218 elsif C_LOOP</span>
 219     const CalleeSaveSpaceAsVirtualRegisters = 1
 220 elsif ARMv7
 221     const CalleeSaveSpaceAsVirtualRegisters = 1
 222 elsif MIPS
 223     const CalleeSaveSpaceAsVirtualRegisters = 1
 224 else
 225     const CalleeSaveSpaceAsVirtualRegisters = 0
 226 end
 227 
 228 const CalleeSaveSpaceStackAligned = (CalleeSaveSpaceAsVirtualRegisters * SlotSize + StackAlignment - 1) &amp; ~StackAlignmentMask
 229 
 230 
 231 # Watchpoint states
 232 const ClearWatchpoint = constexpr ClearWatchpoint
 233 const IsWatched = constexpr IsWatched
 234 const IsInvalidated = constexpr IsInvalidated
 235 
 236 # ShadowChicken data
 237 const ShadowChickenTailMarker = constexpr ShadowChicken::Packet::tailMarkerValue
 238 
</pre>
<hr />
<pre>
 257     #   base of the bytecodes, and one register for the index.
 258     # - The PC base (or PB for short) must be stored in a callee-save register.
 259     # - C calls are still given the Instruction* rather than the PC index.
 260     #   This requires an add before the call, and a sub after.
 261     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
 262     if ARM64 or ARM64E
 263         const metadataTable = csr6
 264         const PB = csr7
 265         const tagTypeNumber = csr8
 266         const tagMask = csr9
 267     elsif X86_64
 268         const metadataTable = csr1
 269         const PB = csr2
 270         const tagTypeNumber = csr3
 271         const tagMask = csr4
 272     elsif X86_64_WIN
 273         const metadataTable = csr3
 274         const PB = csr4
 275         const tagTypeNumber = csr5
 276         const tagMask = csr6
<span class="line-modified"> 277     elsif C_LOOP</span>
 278         const PB = csr0
 279         const tagTypeNumber = csr1
 280         const tagMask = csr2
 281         const metadataTable = csr3
 282     end
 283 
 284 else
 285     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
<span class="line-modified"> 286     if C_LOOP</span>
 287         const metadataTable = csr3
 288     elsif ARMv7
 289         const metadataTable = csr0
 290     elsif MIPS
 291         const metadataTable = csr0
 292     else
 293         error
 294     end
 295 end
 296 
 297 macro dispatch(advanceReg)
 298     addp advanceReg, PC
 299     nextInstruction()
 300 end
 301 
 302 macro dispatchIndirect(offsetReg)
 303     dispatch(offsetReg)
 304 end
 305 
 306 macro dispatchOp(size, opcodeName)
 307     macro dispatchNarrow()
 308         dispatch(constexpr %opcodeName%_length)
 309     end
 310 
<span class="line-modified"> 311     macro dispatchWide()</span>




 312         dispatch(constexpr %opcodeName%_length * 4 + 1)
 313     end
 314 
<span class="line-modified"> 315     size(dispatchNarrow, dispatchWide, macro (dispatch) dispatch() end)</span>
 316 end
 317 
 318 macro getu(size, opcodeStruct, fieldName, dst)
<span class="line-modified"> 319     size(getuOperandNarrow, getuOperandWide, macro (getu)</span>
 320         getu(opcodeStruct, fieldName, dst)
 321     end)
 322 end
 323 
 324 macro get(size, opcodeStruct, fieldName, dst)
<span class="line-modified"> 325     size(getOperandNarrow, getOperandWide, macro (get)</span>
 326         get(opcodeStruct, fieldName, dst)
 327     end)
 328 end
 329 
<span class="line-modified"> 330 macro narrow(narrowFn, wideFn, k)</span>
 331     k(narrowFn)
 332 end
 333 
<span class="line-modified"> 334 macro wide(narrowFn, wideFn, k)</span>
<span class="line-modified"> 335     k(wideFn)</span>




 336 end
 337 
 338 macro metadata(size, opcode, dst, scratch)
<span class="line-modified"> 339     loadi constexpr %opcode%::opcodeID * 4[metadataTable], dst # offset = metadataTable&lt;unsigned*&gt;[opcodeID]</span>



 340     getu(size, opcode, m_metadataID, scratch) # scratch = bytecode.m_metadataID
 341     muli sizeof %opcode%::Metadata, scratch # scratch *= sizeof(Op::Metadata)
 342     addi scratch, dst # offset += scratch
 343     addp metadataTable, dst # return &amp;metadataTable[offset]
 344 end
 345 
 346 macro jumpImpl(targetOffsetReg)
 347     btiz targetOffsetReg, .outOfLineJumpTarget
 348     dispatchIndirect(targetOffsetReg)
 349 .outOfLineJumpTarget:
 350     callSlowPath(_llint_slow_path_out_of_line_jump_target)
 351     nextInstruction()
 352 end
 353 
 354 macro commonOp(label, prologue, fn)
 355 _%label%:
 356     prologue()
 357     fn(narrow)
 358 
<span class="line-modified"> 359 _%label%_wide:</span>





 360     prologue()
<span class="line-modified"> 361     fn(wide)</span>





 362 end
 363 
 364 macro op(l, fn)
<span class="line-modified"> 365     commonOp(l, macro () end, macro (unused)</span>
<span class="line-modified"> 366         fn()</span>
 367     end)
 368 end
 369 
 370 macro llintOp(opcodeName, opcodeStruct, fn)
 371     commonOp(llint_%opcodeName%, traceExecution, macro(size)
 372         macro getImpl(fieldName, dst)
 373             get(size, opcodeStruct, fieldName, dst)
 374         end
 375 
 376         macro dispatchImpl()
 377             dispatchOp(size, opcodeName)
 378         end
 379 
 380         fn(size, getImpl, dispatchImpl)
 381     end)
 382 end
 383 
 384 macro llintOpWithReturn(opcodeName, opcodeStruct, fn)
 385     llintOp(opcodeName, opcodeStruct, macro(size, get, dispatch)
 386         makeReturn(get, dispatch, macro (return)
</pre>
<hr />
<pre>
 452 # The typed array types need to be numbered in a particular order because of the manually written
 453 # switch statement in get_by_val and put_by_val.
 454 const Int8ArrayType = constexpr Int8ArrayType
 455 const Uint8ArrayType = constexpr Uint8ArrayType
 456 const Uint8ClampedArrayType = constexpr Uint8ClampedArrayType
 457 const Int16ArrayType = constexpr Int16ArrayType
 458 const Uint16ArrayType = constexpr Uint16ArrayType
 459 const Int32ArrayType = constexpr Int32ArrayType
 460 const Uint32ArrayType = constexpr Uint32ArrayType
 461 const Float32ArrayType = constexpr Float32ArrayType
 462 const Float64ArrayType = constexpr Float64ArrayType
 463 
 464 const FirstTypedArrayType = constexpr FirstTypedArrayType
 465 const NumberOfTypedArrayTypesExcludingDataView = constexpr NumberOfTypedArrayTypesExcludingDataView
 466 
 467 # Type flags constants.
 468 const MasqueradesAsUndefined = constexpr MasqueradesAsUndefined
 469 const ImplementsDefaultHasInstance = constexpr ImplementsDefaultHasInstance
 470 
 471 # Bytecode operand constants.
<span class="line-modified"> 472 const FirstConstantRegisterIndexNarrow = 16</span>
<span class="line-modified"> 473 const FirstConstantRegisterIndexWide = constexpr FirstConstantRegisterIndex</span>

 474 
 475 # Code type constants.
 476 const GlobalCode = constexpr GlobalCode
 477 const EvalCode = constexpr EvalCode
 478 const FunctionCode = constexpr FunctionCode
 479 const ModuleCode = constexpr ModuleCode
 480 
 481 # The interpreter steals the tag word of the argument count.
 482 const LLIntReturnPC = ArgumentCount + TagOffset
 483 
 484 # String flags.
 485 const isRopeInPointer = constexpr JSString::isRopeInPointer
 486 const HashFlags8BitBuffer = constexpr StringImpl::s_hashFlag8BitBuffer
 487 
 488 # Copied from PropertyOffset.h
 489 const firstOutOfLineOffset = constexpr firstOutOfLineOffset
 490 
 491 # ResolveType
 492 const GlobalProperty = constexpr GlobalProperty
 493 const GlobalVar = constexpr GlobalVar
</pre>
<hr />
<pre>
 499 const GlobalVarWithVarInjectionChecks = constexpr GlobalVarWithVarInjectionChecks
 500 const GlobalLexicalVarWithVarInjectionChecks = constexpr GlobalLexicalVarWithVarInjectionChecks
 501 const ClosureVarWithVarInjectionChecks = constexpr ClosureVarWithVarInjectionChecks
 502 
 503 const ResolveTypeMask = constexpr GetPutInfo::typeBits
 504 const InitializationModeMask = constexpr GetPutInfo::initializationBits
 505 const InitializationModeShift = constexpr GetPutInfo::initializationShift
 506 const NotInitialization = constexpr InitializationMode::NotInitialization
 507 
 508 const MarkedBlockSize = constexpr MarkedBlock::blockSize
 509 const MarkedBlockMask = ~(MarkedBlockSize - 1)
 510 const MarkedBlockFooterOffset = constexpr MarkedBlock::offsetOfFooter
 511 
 512 const BlackThreshold = constexpr blackThreshold
 513 
 514 const VectorBufferOffset = Vector::m_buffer
 515 const VectorSizeOffset = Vector::m_size
 516 
 517 # Some common utilities.
 518 macro crash()
<span class="line-modified"> 519     if C_LOOP</span>
 520         cloopCrash
 521     else
 522         call _llint_crash
 523     end
 524 end
 525 
 526 macro assert(assertion)
 527     if ASSERT_ENABLED
 528         assertion(.ok)
 529         crash()
 530     .ok:
 531     end
 532 end
 533 
 534 # The probe macro can be used to insert some debugging code without perturbing scalar
 535 # registers. Presently, the probe macro only preserves scalar registers. Hence, the
 536 # C probe callback function should not trash floating point registers.
 537 #
 538 # The macro you pass to probe() can pass whatever registers you like to your probe
 539 # callback function. However, you need to be mindful of which of the registers are
</pre>
<hr />
<pre>
 582             pop csr1, csr0
 583         elsif ARMv7
 584             pop csr0
 585         end
 586         pop t5, t4
 587         pop t3, t2
 588         pop t1, t0
 589         pop a3, a2
 590         pop a1, a0
 591         if ARM64 or ARM64E or ARMv7
 592             pop lr, cfr
 593         end
 594     end
 595 else
 596     macro probe(action)
 597     end
 598 end
 599 
 600 macro checkStackPointerAlignment(tempReg, location)
 601     if ASSERT_ENABLED
<span class="line-modified"> 602         if ARM64 or ARM64E or C_LOOP</span>
 603             # ARM64 and ARM64E will check for us!
<span class="line-modified"> 604             # C_LOOP does not need the alignment, and can use a little perf</span>
 605             # improvement from avoiding useless work.
 606         else
 607             if ARMv7
 608                 # ARM can&#39;t do logical ops with the sp as a source
 609                 move sp, tempReg
 610                 andp StackAlignmentMask, tempReg
 611             else
 612                 andp sp, StackAlignmentMask, tempReg
 613             end
 614             btpz tempReg, .stackPointerOkay
 615             move location, tempReg
 616             break
 617         .stackPointerOkay:
 618         end
 619     end
 620 end
 621 
<span class="line-modified"> 622 if C_LOOP or ARM64 or ARM64E or X86_64 or X86_64_WIN</span>
 623     const CalleeSaveRegisterCount = 0
 624 elsif ARMv7
 625     const CalleeSaveRegisterCount = 7
 626 elsif MIPS
 627     const CalleeSaveRegisterCount = 2
 628 elsif X86 or X86_WIN
 629     const CalleeSaveRegisterCount = 3
 630 end
 631 
 632 const CalleeRegisterSaveSize = CalleeSaveRegisterCount * MachineRegisterSize
 633 
 634 # VMEntryTotalFrameSize includes the space for struct VMEntryRecord and the
 635 # callee save registers rounded up to keep the stack aligned
 636 const VMEntryTotalFrameSize = (CalleeRegisterSaveSize + sizeof VMEntryRecord + StackAlignment - 1) &amp; ~StackAlignmentMask
 637 
 638 macro pushCalleeSaves()
<span class="line-modified"> 639     if C_LOOP or ARM64 or ARM64E or X86_64 or X86_64_WIN</span>
 640     elsif ARMv7
 641         emit &quot;push {r4-r6, r8-r11}&quot;
 642     elsif MIPS
 643         emit &quot;addiu $sp, $sp, -8&quot;
 644         emit &quot;sw $s0, 0($sp)&quot; # csr0/metaData
 645         emit &quot;sw $s4, 4($sp)&quot;
 646         # save $gp to $s4 so that we can restore it after a function call
 647         emit &quot;move $s4, $gp&quot;
 648     elsif X86
 649         emit &quot;push %esi&quot;
 650         emit &quot;push %edi&quot;
 651         emit &quot;push %ebx&quot;
 652     elsif X86_WIN
 653         emit &quot;push esi&quot;
 654         emit &quot;push edi&quot;
 655         emit &quot;push ebx&quot;
 656     end
 657 end
 658 
 659 macro popCalleeSaves()
<span class="line-modified"> 660     if C_LOOP or ARM64 or ARM64E or X86_64 or X86_64_WIN</span>
 661     elsif ARMv7
 662         emit &quot;pop {r4-r6, r8-r11}&quot;
 663     elsif MIPS
 664         emit &quot;lw $s0, 0($sp)&quot;
 665         emit &quot;lw $s4, 4($sp)&quot;
 666         emit &quot;addiu $sp, $sp, 8&quot;
 667     elsif X86
 668         emit &quot;pop %ebx&quot;
 669         emit &quot;pop %edi&quot;
 670         emit &quot;pop %esi&quot;
 671     elsif X86_WIN
 672         emit &quot;pop ebx&quot;
 673         emit &quot;pop edi&quot;
 674         emit &quot;pop esi&quot;
 675     end
 676 end
 677 
 678 macro preserveCallerPCAndCFR()
<span class="line-modified"> 679     if C_LOOP or ARMv7 or MIPS</span>
 680         push lr
 681         push cfr
 682     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 683         push cfr
 684     elsif ARM64 or ARM64E
 685         push cfr, lr
 686     else
 687         error
 688     end
 689     move sp, cfr
 690 end
 691 
 692 macro restoreCallerPCAndCFR()
 693     move cfr, sp
<span class="line-modified"> 694     if C_LOOP or ARMv7 or MIPS</span>
 695         pop cfr
 696         pop lr
 697     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 698         pop cfr
 699     elsif ARM64 or ARM64E
 700         pop lr, cfr
 701     end
 702 end
 703 
 704 macro preserveCalleeSavesUsedByLLInt()
 705     subp CalleeSaveSpaceStackAligned, sp
<span class="line-modified"> 706     if C_LOOP</span>
 707         storep metadataTable, -PtrSize[cfr]
 708     elsif ARMv7 or MIPS
 709         storep metadataTable, -4[cfr]
 710     elsif ARM64 or ARM64E
 711         emit &quot;stp x27, x28, [x29, #-16]&quot;
 712         emit &quot;stp x25, x26, [x29, #-32]&quot;
 713     elsif X86
 714     elsif X86_WIN
 715     elsif X86_64
 716         storep csr4, -8[cfr]
 717         storep csr3, -16[cfr]
 718         storep csr2, -24[cfr]
 719         storep csr1, -32[cfr]
 720     elsif X86_64_WIN
 721         storep csr6, -8[cfr]
 722         storep csr5, -16[cfr]
 723         storep csr4, -24[cfr]
 724         storep csr3, -32[cfr]
 725     end
 726 end
 727 
 728 macro restoreCalleeSavesUsedByLLInt()
<span class="line-modified"> 729     if C_LOOP</span>
 730         loadp -PtrSize[cfr], metadataTable
 731     elsif ARMv7 or MIPS
 732         loadp -4[cfr], metadataTable
 733     elsif ARM64 or ARM64E
 734         emit &quot;ldp x25, x26, [x29, #-32]&quot;
 735         emit &quot;ldp x27, x28, [x29, #-16]&quot;
 736     elsif X86
 737     elsif X86_WIN
 738     elsif X86_64
 739         loadp -32[cfr], csr1
 740         loadp -24[cfr], csr2
 741         loadp -16[cfr], csr3
 742         loadp -8[cfr], csr4
 743     elsif X86_64_WIN
 744         loadp -32[cfr], csr3
 745         loadp -24[cfr], csr4
 746         loadp -16[cfr], csr5
 747         loadp -8[cfr], csr6
 748     end
 749 end
</pre>
<hr />
<pre>
 820             loadq [temp], csr0
 821             loadq 8[temp], csr1
 822             loadq 16[temp], csr2
 823             loadq 24[temp], csr3
 824             loadq 32[temp], csr4
 825         elsif X86_64_WIN
 826             loadq [temp], csr0
 827             loadq 8[temp], csr1
 828             loadq 16[temp], csr2
 829             loadq 24[temp], csr3
 830             loadq 32[temp], csr4
 831             loadq 40[temp], csr5
 832             loadq 48[temp], csr6
 833         elsif ARMv7 or MIPS
 834             loadp [temp], csr0
 835         end
 836     end
 837 end
 838 
 839 macro preserveReturnAddressAfterCall(destinationRegister)
<span class="line-modified"> 840     if C_LOOP or ARMv7 or ARM64 or ARM64E or MIPS</span>
<span class="line-modified"> 841         # In C_LOOP case, we&#39;re only preserving the bytecode vPC.</span>
 842         move lr, destinationRegister
 843     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 844         pop destinationRegister
 845     else
 846         error
 847     end
 848 end
 849 
 850 macro functionPrologue()
 851     tagReturnAddress sp
 852     if X86 or X86_WIN or X86_64 or X86_64_WIN
 853         push cfr
 854     elsif ARM64 or ARM64E
 855         push cfr, lr
<span class="line-modified"> 856     elsif C_LOOP or ARMv7 or MIPS</span>
 857         push lr
 858         push cfr
 859     end
 860     move sp, cfr
 861 end
 862 
 863 macro functionEpilogue()
 864     if X86 or X86_WIN or X86_64 or X86_64_WIN
 865         pop cfr
 866     elsif ARM64 or ARM64E
 867         pop lr, cfr
<span class="line-modified"> 868     elsif C_LOOP or ARMv7 or MIPS</span>
 869         pop cfr
 870         pop lr
 871     end
 872 end
 873 
 874 macro vmEntryRecord(entryFramePointer, resultReg)
 875     subp entryFramePointer, VMEntryTotalFrameSize, resultReg
 876 end
 877 
 878 macro getFrameRegisterSizeForCodeBlock(codeBlock, size)
 879     loadi CodeBlock::m_numCalleeLocals[codeBlock], size
 880     lshiftp 3, size
 881     addp maxFrameExtentForSlowPathCall, size
 882 end
 883 
 884 macro restoreStackPointerAfterCall()
 885     loadp CodeBlock[cfr], t2
 886     getFrameRegisterSizeForCodeBlock(t2, t2)
 887     if ARMv7
 888         subp cfr, t2, t2
 889         move t2, sp
 890     else
 891         subp cfr, t2, sp
 892     end
 893 end
 894 
 895 macro traceExecution()
 896     if TRACING
 897         callSlowPath(_llint_trace)
 898     end
 899 end
 900 
 901 macro callTargetFunction(size, opcodeStruct, dispatch, callee, callPtrTag)
<span class="line-modified"> 902     if C_LOOP</span>
 903         cloopCallJSFunction callee
 904     else
 905         call callee, callPtrTag
 906     end
 907     restoreStackPointerAfterCall()
 908     dispatchAfterCall(size, opcodeStruct, dispatch)
 909 end
 910 
 911 macro prepareForRegularCall(callee, temp1, temp2, temp3, callPtrTag)
 912     addp CallerFrameAndPCSize, sp
 913 end
 914 
 915 # sp points to the new frame
 916 macro prepareForTailCall(callee, temp1, temp2, temp3, callPtrTag)
 917     restoreCalleeSavesUsedByLLInt()
 918 
 919     loadi PayloadOffset + ArgumentCount[cfr], temp2
 920     loadp CodeBlock[cfr], temp1
 921     loadi CodeBlock::m_numParameters[temp1], temp1
 922     bilteq temp1, temp2, .noArityFixup
 923     move temp1, temp2
 924 
 925 .noArityFixup:
 926     # We assume &lt; 2^28 arguments
 927     muli SlotSize, temp2
 928     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
 929     andi ~StackAlignmentMask, temp2
 930 
 931     move cfr, temp1
 932     addp temp2, temp1
 933 
 934     loadi PayloadOffset + ArgumentCount[sp], temp2
 935     # We assume &lt; 2^28 arguments
 936     muli SlotSize, temp2
 937     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
 938     andi ~StackAlignmentMask, temp2
 939 
<span class="line-modified"> 940     if ARMv7 or ARM64 or ARM64E or C_LOOP or MIPS</span>
 941         addp CallerFrameAndPCSize, sp
 942         subi CallerFrameAndPCSize, temp2
 943         loadp CallerFrameAndPC::returnPC[cfr], lr
 944     else
 945         addp PtrSize, sp
 946         subi PtrSize, temp2
 947         loadp PtrSize[cfr], temp3
 948         storep temp3, [sp]
 949     end
 950 
<span class="line-modified"> 951     if POINTER_PROFILING</span>
 952         addp 16, cfr, temp3
 953         untagReturnAddress temp3
 954     end
 955 
 956     subp temp2, temp1
 957     loadp [cfr], cfr
 958 
 959 .copyLoop:
 960     if ARM64 and not ADDRESS64
 961         subi MachineRegisterSize, temp2
 962         loadq [sp, temp2, 1], temp3
 963         storeq temp3, [temp1, temp2, 1]
 964         btinz temp2, .copyLoop
 965     else
 966         subi PtrSize, temp2
 967         loadp [sp, temp2, 1], temp3
 968         storep temp3, [temp1, temp2, 1]
 969         btinz temp2, .copyLoop
 970     end
 971 
</pre>
<hr />
<pre>
1004 macro notifyWrite(set, slow)
1005     bbneq WatchpointSet::m_state[set], IsInvalidated, slow
1006 end
1007 
1008 macro checkSwitchToJIT(increment, action)
1009     loadp CodeBlock[cfr], t0
1010     baddis increment, CodeBlock::m_llintExecuteCounter + BaselineExecutionCounter::m_counter[t0], .continue
1011     action()
1012     .continue:
1013 end
1014 
1015 macro checkSwitchToJITForEpilogue()
1016     checkSwitchToJIT(
1017         10,
1018         macro ()
1019             callSlowPath(_llint_replace)
1020         end)
1021 end
1022 
1023 macro assertNotConstant(size, index)
<span class="line-modified">1024     size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide, macro (FirstConstantRegisterIndex)</span>
1025         assert(macro (ok) bilt index, FirstConstantRegisterIndex, ok end)
1026     end)
1027 end
1028 
1029 macro functionForCallCodeBlockGetter(targetRegister)
1030     if JSVALUE64
1031         loadp Callee[cfr], targetRegister
1032     else
1033         loadp Callee + PayloadOffset[cfr], targetRegister
1034     end
1035     loadp JSFunction::m_executable[targetRegister], targetRegister
1036     loadp FunctionExecutable::m_codeBlockForCall[targetRegister], targetRegister
1037     loadp ExecutableToCodeBlockEdge::m_codeBlock[targetRegister], targetRegister
1038 end
1039 
1040 macro functionForConstructCodeBlockGetter(targetRegister)
1041     if JSVALUE64
1042         loadp Callee[cfr], targetRegister
1043     else
1044         loadp Callee + PayloadOffset[cfr], targetRegister
</pre>
<hr />
<pre>
1056     storep sourceRegister, CodeBlock[cfr]
1057 end
1058 
1059 macro notFunctionCodeBlockSetter(sourceRegister)
1060     # Nothing to do!
1061 end
1062 
1063 # Do the bare minimum required to execute code. Sets up the PC, leave the CodeBlock*
1064 # in t1. May also trigger prologue entry OSR.
1065 macro prologue(codeBlockGetter, codeBlockSetter, osrSlowPath, traceSlowPath)
1066     # Set up the call frame and check if we should OSR.
1067     tagReturnAddress sp
1068     preserveCallerPCAndCFR()
1069 
1070     if TRACING
1071         subp maxFrameExtentForSlowPathCall, sp
1072         callSlowPath(traceSlowPath)
1073         addp maxFrameExtentForSlowPathCall, sp
1074     end
1075     codeBlockGetter(t1)
<span class="line-modified">1076     if not C_LOOP</span>
1077         baddis 5, CodeBlock::m_llintExecuteCounter + BaselineExecutionCounter::m_counter[t1], .continue
1078         if JSVALUE64
1079             move cfr, a0
1080             move PC, a1
1081             cCall2(osrSlowPath)
1082         else
1083             # We are after the function prologue, but before we have set up sp from the CodeBlock.
1084             # Temporarily align stack pointer for this call.
1085             subp 8, sp
1086             move cfr, a0
1087             move PC, a1
1088             cCall2(osrSlowPath)
1089             addp 8, sp
1090         end
1091         btpz r0, .recover
1092         move cfr, sp # restore the previous sp
1093         # pop the callerFrame since we will jump to a function that wants to save it
1094         if ARM64 or ARM64E
1095             pop lr, cfr
1096             untagReturnAddress sp
</pre>
<hr />
<pre>
1106     .continue:
1107     end
1108 
1109     codeBlockSetter(t1)
1110 
1111     preserveCalleeSavesUsedByLLInt()
1112 
1113     # Set up the PC.
1114     if JSVALUE64
1115         loadp CodeBlock::m_instructionsRawPointer[t1], PB
1116         move 0, PC
1117     else
1118         loadp CodeBlock::m_instructionsRawPointer[t1], PC
1119     end
1120 
1121     # Get new sp in t0 and check stack height.
1122     getFrameRegisterSizeForCodeBlock(t1, t0)
1123     subp cfr, t0, t0
1124     bpa t0, cfr, .needStackCheck
1125     loadp CodeBlock::m_vm[t1], t2
<span class="line-modified">1126     if C_LOOP</span>
1127         bpbeq VM::m_cloopStackLimit[t2], t0, .stackHeightOK
1128     else
1129         bpbeq VM::m_softStackLimit[t2], t0, .stackHeightOK
1130     end
1131 
1132 .needStackCheck:
1133     # Stack height check failed - need to call a slow_path.
1134     # Set up temporary stack pointer for call including callee saves
1135     subp maxFrameExtentForSlowPathCall, sp
1136     callSlowPath(_llint_stack_check)
1137     bpeq r1, 0, .stackHeightOKGetCodeBlock
1138 
1139     # We&#39;re throwing before the frame is fully set up. This frame will be
1140     # ignored by the unwinder. So, let&#39;s restore the callee saves before we
1141     # start unwinding. We need to do this before we change the cfr.
1142     restoreCalleeSavesUsedByLLInt()
1143 
1144     move r1, cfr
1145     jmp _llint_throw_from_slow_path_trampoline
1146 
</pre>
<hr />
<pre>
1209 .argumentProfileDone:
1210 end
1211 
1212 macro doReturn()
1213     restoreCalleeSavesUsedByLLInt()
1214     restoreCallerPCAndCFR()
1215     ret
1216 end
1217 
1218 # This break instruction is needed so that the synthesized llintPCRangeStart label
1219 # doesn&#39;t point to the exact same location as vmEntryToJavaScript which comes after it.
1220 # Otherwise, libunwind will report vmEntryToJavaScript as llintPCRangeStart in
1221 # stack traces.
1222 
1223     break
1224 
1225 # stub to call into JavaScript or Native functions
1226 # EncodedJSValue vmEntryToJavaScript(void* code, VM* vm, ProtoCallFrame* protoFrame)
1227 # EncodedJSValue vmEntryToNativeFunction(void* code, VM* vm, ProtoCallFrame* protoFrame)
1228 
<span class="line-modified">1229 if C_LOOP</span>
1230     _llint_vm_entry_to_javascript:
1231 else
1232     global _vmEntryToJavaScript
1233     _vmEntryToJavaScript:
1234 end
1235     doVMEntry(makeJavaScriptCall)
1236 
1237 
<span class="line-modified">1238 if C_LOOP</span>
1239     _llint_vm_entry_to_native:
1240 else
1241     global _vmEntryToNative
1242     _vmEntryToNative:
1243 end
1244     doVMEntry(makeHostFunctionCall)
1245 
1246 
<span class="line-modified">1247 if not C_LOOP</span>
1248     # void sanitizeStackForVMImpl(VM* vm)
1249     global _sanitizeStackForVMImpl
1250     _sanitizeStackForVMImpl:
1251         tagReturnAddress sp
1252         # We need three non-aliased caller-save registers. We are guaranteed
1253         # this for a0, a1 and a2 on all architectures.
1254         if X86 or X86_WIN
1255             loadp 4[sp], a0
1256         end
1257         const vm = a0
1258         const address = a1
1259         const zeroValue = a2
1260     
1261         loadp VM::m_lastStackTop[vm], address
1262         bpbeq sp, address, .zeroFillDone
1263     
1264         move 0, zeroValue
1265     .zeroFillLoop:
1266         storep zeroValue, [address]
1267         addp PtrSize, address
1268         bpa sp, address, .zeroFillLoop
1269 
1270     .zeroFillDone:
1271         move sp, address
1272         storep address, VM::m_lastStackTop[vm]
1273         ret
1274     
1275     # VMEntryRecord* vmEntryRecord(const EntryFrame* entryFrame)
1276     global _vmEntryRecord
1277     _vmEntryRecord:
1278         tagReturnAddress sp
1279         if X86 or X86_WIN
1280             loadp 4[sp], a0
1281         end
1282 
1283         vmEntryRecord(a0, r0)
1284         ret
1285 end
1286 
<span class="line-modified">1287 if C_LOOP</span>
1288     # Dummy entry point the C Loop uses to initialize.
1289     _llint_entry:
1290         crash()
1291 else
1292     macro initPCRelative(pcBase)
1293         if X86_64 or X86_64_WIN or X86 or X86_WIN
1294             call _relativePCBase
1295         _relativePCBase:
1296             pop pcBase
1297         elsif ARM64 or ARM64E
1298         elsif ARMv7
1299         _relativePCBase:
1300             move pc, pcBase
1301             subp 3, pcBase   # Need to back up the PC and set the Thumb2 bit
1302         elsif MIPS
1303             la _relativePCBase, pcBase
1304             setcallreg pcBase # needed to set $t9 to the right value for the .cpload created by the label.
1305         _relativePCBase:
1306         end
1307 end
1308 
<span class="line-modified">1309 # The PC base is in t2, as this is what _llint_entry leaves behind through</span>
<span class="line-modified">1310 # initPCRelative(t2)</span>
1311 macro setEntryAddress(index, label)
1312     setEntryAddressCommon(index, label, a0)
1313 end
1314 
<span class="line-modified">1315 macro setEntryAddressWide(index, label)</span>
1316      setEntryAddressCommon(index, label, a1)
1317 end
1318 




1319 macro setEntryAddressCommon(index, label, map)
<span class="line-modified">1320     if X86_64 or X86_64_WIN</span>
<span class="line-modified">1321         leap (label - _relativePCBase)[t2], t3</span>
<span class="line-modified">1322         move index, t4</span>
<span class="line-modified">1323         storep t3, [map, t4, 8]</span>




1324     elsif X86 or X86_WIN
<span class="line-modified">1325         leap (label - _relativePCBase)[t2], t3</span>
<span class="line-modified">1326         move index, t4</span>
<span class="line-modified">1327         storep t3, [map, t4, 4]</span>
1328     elsif ARM64 or ARM64E
<span class="line-modified">1329         pcrtoaddr label, t2</span>
1330         move index, t4
<span class="line-modified">1331         storep t2, [map, t4, PtrSize]</span>
1332     elsif ARMv7
1333         mvlbl (label - _relativePCBase), t4
<span class="line-modified">1334         addp t4, t2, t4</span>
<span class="line-modified">1335         move index, t3</span>
<span class="line-modified">1336         storep t4, [map, t3, 4]</span>
1337     elsif MIPS
1338         la label, t4
1339         la _relativePCBase, t3
1340         subp t3, t4
<span class="line-modified">1341         addp t4, t2, t4</span>
<span class="line-modified">1342         move index, t3</span>
<span class="line-modified">1343         storep t4, [map, t3, 4]</span>
1344     end
1345 end
1346 
1347 global _llint_entry
1348 # Entry point for the llint to initialize.
1349 _llint_entry:
1350     functionPrologue()
1351     pushCalleeSaves()
1352     if X86 or X86_WIN
1353         loadp 20[sp], a0
1354         loadp 24[sp], a1

1355     end
1356 
<span class="line-modified">1357     initPCRelative(t2)</span>
1358 
1359     # Include generated bytecode initialization file.
1360     include InitBytecodes
1361 
1362     popCalleeSaves()
1363     functionEpilogue()
1364     ret
1365 end
1366 
<span class="line-modified">1367 _llint_op_wide:</span>
<span class="line-modified">1368     nextInstructionWide()</span>
1369 
<span class="line-modified">1370 _llint_op_wide_wide:</span>




1371     crash()
1372 
<span class="line-modified">1373 _llint_op_enter_wide:</span>
1374     crash()





1375 
1376 op(llint_program_prologue, macro ()
1377     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1378     dispatch(0)
1379 end)
1380 
1381 
1382 op(llint_module_program_prologue, macro ()
1383     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1384     dispatch(0)
1385 end)
1386 
1387 
1388 op(llint_eval_prologue, macro ()
1389     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1390     dispatch(0)
1391 end)
1392 
1393 
1394 op(llint_function_for_call_prologue, macro ()
</pre>
<hr />
<pre>
1611 
1612 compareUnsignedJumpOp(
1613     jbelow, OpJbelow,
1614     macro (left, right, target) bib left, right, target end)
1615 
1616 
1617 compareUnsignedJumpOp(
1618     jbeloweq, OpJbeloweq,
1619     macro (left, right, target) bibeq left, right, target end)
1620 
1621 
1622 preOp(inc, OpInc,
1623     macro (value, slow) baddio 1, value, slow end)
1624 
1625 
1626 preOp(dec, OpDec,
1627     macro (value, slow) bsubio 1, value, slow end)
1628 
1629 
1630 llintOp(op_loop_hint, OpLoopHint, macro (unused, unused, dispatch)
<span class="line-modified">1631     checkSwitchToJITForLoop()</span>
<span class="line-removed">1632     dispatch()</span>
<span class="line-removed">1633 end)</span>
<span class="line-removed">1634 </span>
<span class="line-removed">1635 </span>
<span class="line-removed">1636 llintOp(op_check_traps, OpCheckTraps, macro (unused, unused, dispatch)</span>
1637     loadp CodeBlock[cfr], t1
1638     loadp CodeBlock::m_vm[t1], t1
<span class="line-modified">1639     loadb VM::m_traps+VMTraps::m_needTrapHandling[t1], t0</span>
<span class="line-removed">1640     btpnz t0, .handleTraps</span>
1641 .afterHandlingTraps:

1642     dispatch()
1643 .handleTraps:
<span class="line-modified">1644     callTrapHandler(.throwHandler)</span>
1645     jmp .afterHandlingTraps
<span class="line-removed">1646 .throwHandler:</span>
<span class="line-removed">1647     jmp _llint_throw_from_slow_path_trampoline</span>
1648 end)
1649 
1650 
1651 # Returns the packet pointer in t0.
1652 macro acquireShadowChickenPacket(slow)
1653     loadp CodeBlock[cfr], t1
1654     loadp CodeBlock::m_vm[t1], t1
1655     loadp VM::m_shadowChicken[t1], t2
1656     loadp ShadowChicken::m_logCursor[t2], t0
1657     bpaeq t0, ShadowChicken::m_logEnd[t2], slow
1658     addp sizeof ShadowChicken::Packet, t0, t1
1659     storep t1, ShadowChicken::m_logCursor[t2]
1660 end
1661 
1662 
1663 llintOp(op_nop, OpNop, macro (unused, unused, dispatch)
1664     dispatch()
1665 end)
1666 
1667 
</pre>
<hr />
<pre>
1755 #   the return value, or did not execute the eval but has a PC for us
1756 #   to call.
1757 #
1758 # - Either:
1759 #   - The JS return value (two registers), or
1760 #
1761 #   - The PC to call.
1762 #
1763 # It turns out to be easier to just always have this return the cfr
1764 # and a PC to call, and that PC may be a dummy thunk that just
1765 # returns the JS value that the eval returned.
1766 
1767 _llint_op_call_eval:
1768     slowPathForCall(
1769         narrow,
1770         OpCallEval,
1771         macro () dispatchOp(narrow, op_call_eval) end,
1772         _llint_slow_path_call_eval,
1773         prepareForRegularCall)
1774 
<span class="line-modified">1775 _llint_op_call_eval_wide:</span>
1776     slowPathForCall(
<span class="line-modified">1777         wide,</span>
1778         OpCallEval,
<span class="line-modified">1779         macro () dispatchOp(wide, op_call_eval) end,</span>
<span class="line-modified">1780         _llint_slow_path_call_eval_wide,</span>








1781         prepareForRegularCall)
1782 
<span class="line-removed">1783 _llint_generic_return_point:</span>
<span class="line-removed">1784     dispatchAfterCall(narrow, OpCallEval, macro ()</span>
<span class="line-removed">1785         dispatchOp(narrow, op_call_eval)</span>
<span class="line-removed">1786     end)</span>
1787 
<span class="line-modified">1788 _llint_generic_return_point_wide:</span>
<span class="line-modified">1789     dispatchAfterCall(wide, OpCallEval, macro()</span>
<span class="line-modified">1790         dispatchOp(wide, op_call_eval)</span>
1791     end)


1792 
1793 llintOp(op_identity_with_profile, OpIdentityWithProfile, macro (unused, unused, dispatch)
1794     dispatch()
1795 end)
1796 
1797 
1798 llintOp(op_yield, OpYield, macro (unused, unused, unused)
1799     notSupported()
1800 end)
1801 
1802 





1803 llintOp(op_debug, OpDebug, macro (unused, unused, dispatch)
1804     loadp CodeBlock[cfr], t0
1805     loadi CodeBlock::m_debuggerRequests[t0], t0
1806     btiz t0, .opDebugDone
1807     callSlowPath(_llint_slow_path_debug)
1808 .opDebugDone:                    
1809     dispatch()
1810 end)
1811 
1812 
1813 op(llint_native_call_trampoline, macro ()
1814     nativeCallTrampoline(NativeExecutable::m_function)
1815 end)
1816 
1817 
1818 op(llint_native_construct_trampoline, macro ()
1819     nativeCallTrampoline(NativeExecutable::m_constructor)
1820 end)
1821 
1822 
</pre>
</td>
<td>
<hr />
<pre>
<span class="line-modified">   1 # Copyrsght (C) 2011-2019 Apple Inc. All rights reserved.</span>
   2 #
   3 # Redistribution and use in source and binary forms, with or without
   4 # modification, are permitted provided that the following conditions
   5 # are met:
   6 # 1. Redistributions of source code must retain the above copyright
   7 #    notice, this list of conditions and the following disclaimer.
   8 # 2. Redistributions in binary form must reproduce the above copyright
   9 #    notice, this list of conditions and the following disclaimer in the
  10 #    documentation and/or other materials provided with the distribution.
  11 #
  12 # THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
  13 # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
  14 # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  15 # PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
  16 # BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  17 # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  18 # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  19 # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  20 # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  21 # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
</pre>
<hr />
<pre>
  59 #   you can use it to select different pieces of code for different
  60 #   platforms.
  61 #
  62 # - Arguments to macros follow lexical scoping rather than dynamic scoping.
  63 #   Const&#39;s also follow lexical scoping and may override (hide) arguments
  64 #   or other consts. All variables (arguments and constants) can be bound
  65 #   to operands. Additionally, arguments (but not constants) can be bound
  66 #   to macros.
  67 
  68 # The following general-purpose registers are available:
  69 #
  70 #  - cfr and sp hold the call frame and (native) stack pointer respectively.
  71 #  They are callee-save registers, and guaranteed to be distinct from all other
  72 #  registers on all architectures.
  73 #
  74 #  - lr is defined on non-X86 architectures (ARM64, ARM64E, ARMv7, MIPS and CLOOP)
  75 #  and holds the return PC
  76 #
  77 #  - pc holds the (native) program counter on 32-bits ARM architectures (ARMv7)
  78 #
<span class="line-modified">  79 #  - t0, t1, t2, t3, t4, and optionally t5, t6, and t7 are temporary registers that can get trashed on</span>
  80 #  calls, and are pairwise distinct registers. t4 holds the JS program counter, so use
  81 #  with caution in opcodes (actually, don&#39;t use it in opcodes at all, except as PC).
  82 #
  83 #  - r0 and r1 are the platform&#39;s customary return registers, and thus are
  84 #  two distinct registers
  85 #
  86 #  - a0, a1, a2 and a3 are the platform&#39;s customary argument registers, and
  87 #  thus are pairwise distinct registers. Be mindful that:
  88 #    + On X86, there are no argument registers. a0 and a1 are edx and
  89 #    ecx following the fastcall convention, but you should still use the stack
  90 #    to pass your arguments. The cCall2 and cCall4 macros do this for you.
  91 #    + On X86_64_WIN, you should allocate space on the stack for the arguments,
  92 #    and the return convention is weird for &gt; 8 bytes types. The only place we
  93 #    use &gt; 8 bytes return values is on a cCall, and cCall2 and cCall4 handle
  94 #    this for you.
  95 #
  96 #  - The only registers guaranteed to be caller-saved are r0, r1, a0, a1 and a2, and
  97 #  you should be mindful of that in functions that are called directly from C.
  98 #  If you need more registers, you should push and pop them like a good
  99 #  assembly citizen, because any other register will be callee-saved on X86.
</pre>
<hr />
<pre>
 164 end
 165 
 166 const JSLexicalEnvironment_variables = (sizeof JSLexicalEnvironment + SlotSize - 1) &amp; ~(SlotSize - 1)
 167 const DirectArguments_storage = (sizeof DirectArguments + SlotSize - 1) &amp; ~(SlotSize - 1)
 168 
 169 const StackAlignment = constexpr (stackAlignmentBytes())
 170 const StackAlignmentSlots = constexpr (stackAlignmentRegisters())
 171 const StackAlignmentMask = StackAlignment - 1
 172 
 173 const CallerFrameAndPCSize = constexpr (sizeof(CallerFrameAndPC))
 174 
 175 const CallerFrame = 0
 176 const ReturnPC = CallerFrame + MachineRegisterSize
 177 const CodeBlock = ReturnPC + MachineRegisterSize
 178 const Callee = CodeBlock + SlotSize
 179 const ArgumentCount = Callee + SlotSize
 180 const ThisArgumentOffset = ArgumentCount + SlotSize
 181 const FirstArgumentOffset = ThisArgumentOffset + SlotSize
 182 const CallFrameHeaderSize = ThisArgumentOffset
 183 
<span class="line-added"> 184 const MetadataOffsetTable16Offset = 0</span>
<span class="line-added"> 185 const MetadataOffsetTable32Offset = constexpr UnlinkedMetadataTable::s_offset16TableSize</span>
<span class="line-added"> 186 </span>
 187 # Some value representation constants.
 188 if JSVALUE64
 189     const TagBitTypeOther = constexpr TagBitTypeOther
 190     const TagBitBool      = constexpr TagBitBool
 191     const TagBitUndefined = constexpr TagBitUndefined
 192     const ValueEmpty      = constexpr ValueEmpty
 193     const ValueFalse      = constexpr ValueFalse
 194     const ValueTrue       = constexpr ValueTrue
 195     const ValueUndefined  = constexpr ValueUndefined
 196     const ValueNull       = constexpr ValueNull
 197     const TagTypeNumber   = constexpr TagTypeNumber
 198     const TagMask         = constexpr TagMask
 199 else
 200     const Int32Tag = constexpr JSValue::Int32Tag
 201     const BooleanTag = constexpr JSValue::BooleanTag
 202     const NullTag = constexpr JSValue::NullTag
 203     const UndefinedTag = constexpr JSValue::UndefinedTag
 204     const CellTag = constexpr JSValue::CellTag
 205     const EmptyValueTag = constexpr JSValue::EmptyValueTag
 206     const DeletedValueTag = constexpr JSValue::DeletedValueTag
 207     const LowestTag = constexpr JSValue::LowestTag
 208 end
 209 
 210 if JSVALUE64
 211     const NumberOfStructureIDEntropyBits = constexpr StructureIDTable::s_numberOfEntropyBits
 212     const StructureEntropyBitsShift = constexpr StructureIDTable::s_entropyBitsShiftForStructurePointer
 213 end
 214 
 215 const CallOpCodeSize = constexpr op_call_length
 216 
 217 const maxFrameExtentForSlowPathCall = constexpr maxFrameExtentForSlowPathCall
 218 
 219 if X86_64 or X86_64_WIN or ARM64 or ARM64E
 220     const CalleeSaveSpaceAsVirtualRegisters = 4
<span class="line-modified"> 221 elsif C_LOOP or C_LOOP_WIN</span>
 222     const CalleeSaveSpaceAsVirtualRegisters = 1
 223 elsif ARMv7
 224     const CalleeSaveSpaceAsVirtualRegisters = 1
 225 elsif MIPS
 226     const CalleeSaveSpaceAsVirtualRegisters = 1
 227 else
 228     const CalleeSaveSpaceAsVirtualRegisters = 0
 229 end
 230 
 231 const CalleeSaveSpaceStackAligned = (CalleeSaveSpaceAsVirtualRegisters * SlotSize + StackAlignment - 1) &amp; ~StackAlignmentMask
 232 
 233 
 234 # Watchpoint states
 235 const ClearWatchpoint = constexpr ClearWatchpoint
 236 const IsWatched = constexpr IsWatched
 237 const IsInvalidated = constexpr IsInvalidated
 238 
 239 # ShadowChicken data
 240 const ShadowChickenTailMarker = constexpr ShadowChicken::Packet::tailMarkerValue
 241 
</pre>
<hr />
<pre>
 260     #   base of the bytecodes, and one register for the index.
 261     # - The PC base (or PB for short) must be stored in a callee-save register.
 262     # - C calls are still given the Instruction* rather than the PC index.
 263     #   This requires an add before the call, and a sub after.
 264     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
 265     if ARM64 or ARM64E
 266         const metadataTable = csr6
 267         const PB = csr7
 268         const tagTypeNumber = csr8
 269         const tagMask = csr9
 270     elsif X86_64
 271         const metadataTable = csr1
 272         const PB = csr2
 273         const tagTypeNumber = csr3
 274         const tagMask = csr4
 275     elsif X86_64_WIN
 276         const metadataTable = csr3
 277         const PB = csr4
 278         const tagTypeNumber = csr5
 279         const tagMask = csr6
<span class="line-modified"> 280     elsif C_LOOP or C_LOOP_WIN</span>
 281         const PB = csr0
 282         const tagTypeNumber = csr1
 283         const tagMask = csr2
 284         const metadataTable = csr3
 285     end
 286 
 287 else
 288     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
<span class="line-modified"> 289     if C_LOOP or C_LOOP_WIN</span>
 290         const metadataTable = csr3
 291     elsif ARMv7
 292         const metadataTable = csr0
 293     elsif MIPS
 294         const metadataTable = csr0
 295     else
 296         error
 297     end
 298 end
 299 
 300 macro dispatch(advanceReg)
 301     addp advanceReg, PC
 302     nextInstruction()
 303 end
 304 
 305 macro dispatchIndirect(offsetReg)
 306     dispatch(offsetReg)
 307 end
 308 
 309 macro dispatchOp(size, opcodeName)
 310     macro dispatchNarrow()
 311         dispatch(constexpr %opcodeName%_length)
 312     end
 313 
<span class="line-modified"> 314     macro dispatchWide16()</span>
<span class="line-added"> 315         dispatch(constexpr %opcodeName%_length * 2 + 1)</span>
<span class="line-added"> 316     end</span>
<span class="line-added"> 317 </span>
<span class="line-added"> 318     macro dispatchWide32()</span>
 319         dispatch(constexpr %opcodeName%_length * 4 + 1)
 320     end
 321 
<span class="line-modified"> 322     size(dispatchNarrow, dispatchWide16, dispatchWide32, macro (dispatch) dispatch() end)</span>
 323 end
 324 
 325 macro getu(size, opcodeStruct, fieldName, dst)
<span class="line-modified"> 326     size(getuOperandNarrow, getuOperandWide16, getuOperandWide32, macro (getu)</span>
 327         getu(opcodeStruct, fieldName, dst)
 328     end)
 329 end
 330 
 331 macro get(size, opcodeStruct, fieldName, dst)
<span class="line-modified"> 332     size(getOperandNarrow, getOperandWide16, getOperandWide32, macro (get)</span>
 333         get(opcodeStruct, fieldName, dst)
 334     end)
 335 end
 336 
<span class="line-modified"> 337 macro narrow(narrowFn, wide16Fn, wide32Fn, k)</span>
 338     k(narrowFn)
 339 end
 340 
<span class="line-modified"> 341 macro wide16(narrowFn, wide16Fn, wide32Fn, k)</span>
<span class="line-modified"> 342     k(wide16Fn)</span>
<span class="line-added"> 343 end</span>
<span class="line-added"> 344 </span>
<span class="line-added"> 345 macro wide32(narrowFn, wide16Fn, wide32Fn, k)</span>
<span class="line-added"> 346     k(wide32Fn)</span>
 347 end
 348 
 349 macro metadata(size, opcode, dst, scratch)
<span class="line-modified"> 350     loadh (constexpr %opcode%::opcodeID * 2 + MetadataOffsetTable16Offset)[metadataTable], dst # offset = metadataTable&lt;uint16_t*&gt;[opcodeID]</span>
<span class="line-added"> 351     btinz dst, .setUpOffset</span>
<span class="line-added"> 352     loadi (constexpr %opcode%::opcodeID * 4 + MetadataOffsetTable32Offset)[metadataTable], dst # offset = metadataTable&lt;uint32_t*&gt;[opcodeID]</span>
<span class="line-added"> 353 .setUpOffset:</span>
 354     getu(size, opcode, m_metadataID, scratch) # scratch = bytecode.m_metadataID
 355     muli sizeof %opcode%::Metadata, scratch # scratch *= sizeof(Op::Metadata)
 356     addi scratch, dst # offset += scratch
 357     addp metadataTable, dst # return &amp;metadataTable[offset]
 358 end
 359 
 360 macro jumpImpl(targetOffsetReg)
 361     btiz targetOffsetReg, .outOfLineJumpTarget
 362     dispatchIndirect(targetOffsetReg)
 363 .outOfLineJumpTarget:
 364     callSlowPath(_llint_slow_path_out_of_line_jump_target)
 365     nextInstruction()
 366 end
 367 
 368 macro commonOp(label, prologue, fn)
 369 _%label%:
 370     prologue()
 371     fn(narrow)
 372 
<span class="line-modified"> 373 # FIXME: We cannot enable wide16 bytecode in Windows CLoop. With MSVC, as CLoop::execute gets larger code</span>
<span class="line-added"> 374 # size, CLoop::execute gets higher stack height requirement. This makes CLoop::execute takes 160KB stack</span>
<span class="line-added"> 375 # per call, causes stack overflow error easily. For now, we disable wide16 optimization for Windows CLoop.</span>
<span class="line-added"> 376 # https://bugs.webkit.org/show_bug.cgi?id=198283</span>
<span class="line-added"> 377 if not C_LOOP_WIN</span>
<span class="line-added"> 378 _%label%_wide16:</span>
 379     prologue()
<span class="line-modified"> 380     fn(wide16)</span>
<span class="line-added"> 381 end</span>
<span class="line-added"> 382 </span>
<span class="line-added"> 383 _%label%_wide32:</span>
<span class="line-added"> 384     prologue()</span>
<span class="line-added"> 385     fn(wide32)</span>
 386 end
 387 
 388 macro op(l, fn)
<span class="line-modified"> 389     commonOp(l, macro () end, macro (size)</span>
<span class="line-modified"> 390         size(fn, macro() end, macro() end, macro(gen) gen() end)</span>
 391     end)
 392 end
 393 
 394 macro llintOp(opcodeName, opcodeStruct, fn)
 395     commonOp(llint_%opcodeName%, traceExecution, macro(size)
 396         macro getImpl(fieldName, dst)
 397             get(size, opcodeStruct, fieldName, dst)
 398         end
 399 
 400         macro dispatchImpl()
 401             dispatchOp(size, opcodeName)
 402         end
 403 
 404         fn(size, getImpl, dispatchImpl)
 405     end)
 406 end
 407 
 408 macro llintOpWithReturn(opcodeName, opcodeStruct, fn)
 409     llintOp(opcodeName, opcodeStruct, macro(size, get, dispatch)
 410         makeReturn(get, dispatch, macro (return)
</pre>
<hr />
<pre>
 476 # The typed array types need to be numbered in a particular order because of the manually written
 477 # switch statement in get_by_val and put_by_val.
 478 const Int8ArrayType = constexpr Int8ArrayType
 479 const Uint8ArrayType = constexpr Uint8ArrayType
 480 const Uint8ClampedArrayType = constexpr Uint8ClampedArrayType
 481 const Int16ArrayType = constexpr Int16ArrayType
 482 const Uint16ArrayType = constexpr Uint16ArrayType
 483 const Int32ArrayType = constexpr Int32ArrayType
 484 const Uint32ArrayType = constexpr Uint32ArrayType
 485 const Float32ArrayType = constexpr Float32ArrayType
 486 const Float64ArrayType = constexpr Float64ArrayType
 487 
 488 const FirstTypedArrayType = constexpr FirstTypedArrayType
 489 const NumberOfTypedArrayTypesExcludingDataView = constexpr NumberOfTypedArrayTypesExcludingDataView
 490 
 491 # Type flags constants.
 492 const MasqueradesAsUndefined = constexpr MasqueradesAsUndefined
 493 const ImplementsDefaultHasInstance = constexpr ImplementsDefaultHasInstance
 494 
 495 # Bytecode operand constants.
<span class="line-modified"> 496 const FirstConstantRegisterIndexNarrow = constexpr FirstConstantRegisterIndex8</span>
<span class="line-modified"> 497 const FirstConstantRegisterIndexWide16 = constexpr FirstConstantRegisterIndex16</span>
<span class="line-added"> 498 const FirstConstantRegisterIndexWide32 = constexpr FirstConstantRegisterIndex</span>
 499 
 500 # Code type constants.
 501 const GlobalCode = constexpr GlobalCode
 502 const EvalCode = constexpr EvalCode
 503 const FunctionCode = constexpr FunctionCode
 504 const ModuleCode = constexpr ModuleCode
 505 
 506 # The interpreter steals the tag word of the argument count.
 507 const LLIntReturnPC = ArgumentCount + TagOffset
 508 
 509 # String flags.
 510 const isRopeInPointer = constexpr JSString::isRopeInPointer
 511 const HashFlags8BitBuffer = constexpr StringImpl::s_hashFlag8BitBuffer
 512 
 513 # Copied from PropertyOffset.h
 514 const firstOutOfLineOffset = constexpr firstOutOfLineOffset
 515 
 516 # ResolveType
 517 const GlobalProperty = constexpr GlobalProperty
 518 const GlobalVar = constexpr GlobalVar
</pre>
<hr />
<pre>
 524 const GlobalVarWithVarInjectionChecks = constexpr GlobalVarWithVarInjectionChecks
 525 const GlobalLexicalVarWithVarInjectionChecks = constexpr GlobalLexicalVarWithVarInjectionChecks
 526 const ClosureVarWithVarInjectionChecks = constexpr ClosureVarWithVarInjectionChecks
 527 
 528 const ResolveTypeMask = constexpr GetPutInfo::typeBits
 529 const InitializationModeMask = constexpr GetPutInfo::initializationBits
 530 const InitializationModeShift = constexpr GetPutInfo::initializationShift
 531 const NotInitialization = constexpr InitializationMode::NotInitialization
 532 
 533 const MarkedBlockSize = constexpr MarkedBlock::blockSize
 534 const MarkedBlockMask = ~(MarkedBlockSize - 1)
 535 const MarkedBlockFooterOffset = constexpr MarkedBlock::offsetOfFooter
 536 
 537 const BlackThreshold = constexpr blackThreshold
 538 
 539 const VectorBufferOffset = Vector::m_buffer
 540 const VectorSizeOffset = Vector::m_size
 541 
 542 # Some common utilities.
 543 macro crash()
<span class="line-modified"> 544     if C_LOOP or C_LOOP_WIN</span>
 545         cloopCrash
 546     else
 547         call _llint_crash
 548     end
 549 end
 550 
 551 macro assert(assertion)
 552     if ASSERT_ENABLED
 553         assertion(.ok)
 554         crash()
 555     .ok:
 556     end
 557 end
 558 
 559 # The probe macro can be used to insert some debugging code without perturbing scalar
 560 # registers. Presently, the probe macro only preserves scalar registers. Hence, the
 561 # C probe callback function should not trash floating point registers.
 562 #
 563 # The macro you pass to probe() can pass whatever registers you like to your probe
 564 # callback function. However, you need to be mindful of which of the registers are
</pre>
<hr />
<pre>
 607             pop csr1, csr0
 608         elsif ARMv7
 609             pop csr0
 610         end
 611         pop t5, t4
 612         pop t3, t2
 613         pop t1, t0
 614         pop a3, a2
 615         pop a1, a0
 616         if ARM64 or ARM64E or ARMv7
 617             pop lr, cfr
 618         end
 619     end
 620 else
 621     macro probe(action)
 622     end
 623 end
 624 
 625 macro checkStackPointerAlignment(tempReg, location)
 626     if ASSERT_ENABLED
<span class="line-modified"> 627         if ARM64 or ARM64E or C_LOOP or C_LOOP_WIN</span>
 628             # ARM64 and ARM64E will check for us!
<span class="line-modified"> 629             # C_LOOP or C_LOOP_WIN does not need the alignment, and can use a little perf</span>
 630             # improvement from avoiding useless work.
 631         else
 632             if ARMv7
 633                 # ARM can&#39;t do logical ops with the sp as a source
 634                 move sp, tempReg
 635                 andp StackAlignmentMask, tempReg
 636             else
 637                 andp sp, StackAlignmentMask, tempReg
 638             end
 639             btpz tempReg, .stackPointerOkay
 640             move location, tempReg
 641             break
 642         .stackPointerOkay:
 643         end
 644     end
 645 end
 646 
<span class="line-modified"> 647 if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN</span>
 648     const CalleeSaveRegisterCount = 0
 649 elsif ARMv7
 650     const CalleeSaveRegisterCount = 7
 651 elsif MIPS
 652     const CalleeSaveRegisterCount = 2
 653 elsif X86 or X86_WIN
 654     const CalleeSaveRegisterCount = 3
 655 end
 656 
 657 const CalleeRegisterSaveSize = CalleeSaveRegisterCount * MachineRegisterSize
 658 
 659 # VMEntryTotalFrameSize includes the space for struct VMEntryRecord and the
 660 # callee save registers rounded up to keep the stack aligned
 661 const VMEntryTotalFrameSize = (CalleeRegisterSaveSize + sizeof VMEntryRecord + StackAlignment - 1) &amp; ~StackAlignmentMask
 662 
 663 macro pushCalleeSaves()
<span class="line-modified"> 664     if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN</span>
 665     elsif ARMv7
 666         emit &quot;push {r4-r6, r8-r11}&quot;
 667     elsif MIPS
 668         emit &quot;addiu $sp, $sp, -8&quot;
 669         emit &quot;sw $s0, 0($sp)&quot; # csr0/metaData
 670         emit &quot;sw $s4, 4($sp)&quot;
 671         # save $gp to $s4 so that we can restore it after a function call
 672         emit &quot;move $s4, $gp&quot;
 673     elsif X86
 674         emit &quot;push %esi&quot;
 675         emit &quot;push %edi&quot;
 676         emit &quot;push %ebx&quot;
 677     elsif X86_WIN
 678         emit &quot;push esi&quot;
 679         emit &quot;push edi&quot;
 680         emit &quot;push ebx&quot;
 681     end
 682 end
 683 
 684 macro popCalleeSaves()
<span class="line-modified"> 685     if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN</span>
 686     elsif ARMv7
 687         emit &quot;pop {r4-r6, r8-r11}&quot;
 688     elsif MIPS
 689         emit &quot;lw $s0, 0($sp)&quot;
 690         emit &quot;lw $s4, 4($sp)&quot;
 691         emit &quot;addiu $sp, $sp, 8&quot;
 692     elsif X86
 693         emit &quot;pop %ebx&quot;
 694         emit &quot;pop %edi&quot;
 695         emit &quot;pop %esi&quot;
 696     elsif X86_WIN
 697         emit &quot;pop ebx&quot;
 698         emit &quot;pop edi&quot;
 699         emit &quot;pop esi&quot;
 700     end
 701 end
 702 
 703 macro preserveCallerPCAndCFR()
<span class="line-modified"> 704     if C_LOOP or C_LOOP_WIN or ARMv7 or MIPS</span>
 705         push lr
 706         push cfr
 707     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 708         push cfr
 709     elsif ARM64 or ARM64E
 710         push cfr, lr
 711     else
 712         error
 713     end
 714     move sp, cfr
 715 end
 716 
 717 macro restoreCallerPCAndCFR()
 718     move cfr, sp
<span class="line-modified"> 719     if C_LOOP or C_LOOP_WIN or ARMv7 or MIPS</span>
 720         pop cfr
 721         pop lr
 722     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 723         pop cfr
 724     elsif ARM64 or ARM64E
 725         pop lr, cfr
 726     end
 727 end
 728 
 729 macro preserveCalleeSavesUsedByLLInt()
 730     subp CalleeSaveSpaceStackAligned, sp
<span class="line-modified"> 731     if C_LOOP or C_LOOP_WIN</span>
 732         storep metadataTable, -PtrSize[cfr]
 733     elsif ARMv7 or MIPS
 734         storep metadataTable, -4[cfr]
 735     elsif ARM64 or ARM64E
 736         emit &quot;stp x27, x28, [x29, #-16]&quot;
 737         emit &quot;stp x25, x26, [x29, #-32]&quot;
 738     elsif X86
 739     elsif X86_WIN
 740     elsif X86_64
 741         storep csr4, -8[cfr]
 742         storep csr3, -16[cfr]
 743         storep csr2, -24[cfr]
 744         storep csr1, -32[cfr]
 745     elsif X86_64_WIN
 746         storep csr6, -8[cfr]
 747         storep csr5, -16[cfr]
 748         storep csr4, -24[cfr]
 749         storep csr3, -32[cfr]
 750     end
 751 end
 752 
 753 macro restoreCalleeSavesUsedByLLInt()
<span class="line-modified"> 754     if C_LOOP or C_LOOP_WIN</span>
 755         loadp -PtrSize[cfr], metadataTable
 756     elsif ARMv7 or MIPS
 757         loadp -4[cfr], metadataTable
 758     elsif ARM64 or ARM64E
 759         emit &quot;ldp x25, x26, [x29, #-32]&quot;
 760         emit &quot;ldp x27, x28, [x29, #-16]&quot;
 761     elsif X86
 762     elsif X86_WIN
 763     elsif X86_64
 764         loadp -32[cfr], csr1
 765         loadp -24[cfr], csr2
 766         loadp -16[cfr], csr3
 767         loadp -8[cfr], csr4
 768     elsif X86_64_WIN
 769         loadp -32[cfr], csr3
 770         loadp -24[cfr], csr4
 771         loadp -16[cfr], csr5
 772         loadp -8[cfr], csr6
 773     end
 774 end
</pre>
<hr />
<pre>
 845             loadq [temp], csr0
 846             loadq 8[temp], csr1
 847             loadq 16[temp], csr2
 848             loadq 24[temp], csr3
 849             loadq 32[temp], csr4
 850         elsif X86_64_WIN
 851             loadq [temp], csr0
 852             loadq 8[temp], csr1
 853             loadq 16[temp], csr2
 854             loadq 24[temp], csr3
 855             loadq 32[temp], csr4
 856             loadq 40[temp], csr5
 857             loadq 48[temp], csr6
 858         elsif ARMv7 or MIPS
 859             loadp [temp], csr0
 860         end
 861     end
 862 end
 863 
 864 macro preserveReturnAddressAfterCall(destinationRegister)
<span class="line-modified"> 865     if C_LOOP or C_LOOP_WIN or ARMv7 or ARM64 or ARM64E or MIPS</span>
<span class="line-modified"> 866         # In C_LOOP or C_LOOP_WIN case, we&#39;re only preserving the bytecode vPC.</span>
 867         move lr, destinationRegister
 868     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 869         pop destinationRegister
 870     else
 871         error
 872     end
 873 end
 874 
 875 macro functionPrologue()
 876     tagReturnAddress sp
 877     if X86 or X86_WIN or X86_64 or X86_64_WIN
 878         push cfr
 879     elsif ARM64 or ARM64E
 880         push cfr, lr
<span class="line-modified"> 881     elsif C_LOOP or C_LOOP_WIN or ARMv7 or MIPS</span>
 882         push lr
 883         push cfr
 884     end
 885     move sp, cfr
 886 end
 887 
 888 macro functionEpilogue()
 889     if X86 or X86_WIN or X86_64 or X86_64_WIN
 890         pop cfr
 891     elsif ARM64 or ARM64E
 892         pop lr, cfr
<span class="line-modified"> 893     elsif C_LOOP or C_LOOP_WIN or ARMv7 or MIPS</span>
 894         pop cfr
 895         pop lr
 896     end
 897 end
 898 
 899 macro vmEntryRecord(entryFramePointer, resultReg)
 900     subp entryFramePointer, VMEntryTotalFrameSize, resultReg
 901 end
 902 
 903 macro getFrameRegisterSizeForCodeBlock(codeBlock, size)
 904     loadi CodeBlock::m_numCalleeLocals[codeBlock], size
 905     lshiftp 3, size
 906     addp maxFrameExtentForSlowPathCall, size
 907 end
 908 
 909 macro restoreStackPointerAfterCall()
 910     loadp CodeBlock[cfr], t2
 911     getFrameRegisterSizeForCodeBlock(t2, t2)
 912     if ARMv7
 913         subp cfr, t2, t2
 914         move t2, sp
 915     else
 916         subp cfr, t2, sp
 917     end
 918 end
 919 
 920 macro traceExecution()
 921     if TRACING
 922         callSlowPath(_llint_trace)
 923     end
 924 end
 925 
 926 macro callTargetFunction(size, opcodeStruct, dispatch, callee, callPtrTag)
<span class="line-modified"> 927     if C_LOOP or C_LOOP_WIN</span>
 928         cloopCallJSFunction callee
 929     else
 930         call callee, callPtrTag
 931     end
 932     restoreStackPointerAfterCall()
 933     dispatchAfterCall(size, opcodeStruct, dispatch)
 934 end
 935 
 936 macro prepareForRegularCall(callee, temp1, temp2, temp3, callPtrTag)
 937     addp CallerFrameAndPCSize, sp
 938 end
 939 
 940 # sp points to the new frame
 941 macro prepareForTailCall(callee, temp1, temp2, temp3, callPtrTag)
 942     restoreCalleeSavesUsedByLLInt()
 943 
 944     loadi PayloadOffset + ArgumentCount[cfr], temp2
 945     loadp CodeBlock[cfr], temp1
 946     loadi CodeBlock::m_numParameters[temp1], temp1
 947     bilteq temp1, temp2, .noArityFixup
 948     move temp1, temp2
 949 
 950 .noArityFixup:
 951     # We assume &lt; 2^28 arguments
 952     muli SlotSize, temp2
 953     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
 954     andi ~StackAlignmentMask, temp2
 955 
 956     move cfr, temp1
 957     addp temp2, temp1
 958 
 959     loadi PayloadOffset + ArgumentCount[sp], temp2
 960     # We assume &lt; 2^28 arguments
 961     muli SlotSize, temp2
 962     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
 963     andi ~StackAlignmentMask, temp2
 964 
<span class="line-modified"> 965     if ARMv7 or ARM64 or ARM64E or C_LOOP or C_LOOP_WIN or MIPS</span>
 966         addp CallerFrameAndPCSize, sp
 967         subi CallerFrameAndPCSize, temp2
 968         loadp CallerFrameAndPC::returnPC[cfr], lr
 969     else
 970         addp PtrSize, sp
 971         subi PtrSize, temp2
 972         loadp PtrSize[cfr], temp3
 973         storep temp3, [sp]
 974     end
 975 
<span class="line-modified"> 976     if ARM64E</span>
 977         addp 16, cfr, temp3
 978         untagReturnAddress temp3
 979     end
 980 
 981     subp temp2, temp1
 982     loadp [cfr], cfr
 983 
 984 .copyLoop:
 985     if ARM64 and not ADDRESS64
 986         subi MachineRegisterSize, temp2
 987         loadq [sp, temp2, 1], temp3
 988         storeq temp3, [temp1, temp2, 1]
 989         btinz temp2, .copyLoop
 990     else
 991         subi PtrSize, temp2
 992         loadp [sp, temp2, 1], temp3
 993         storep temp3, [temp1, temp2, 1]
 994         btinz temp2, .copyLoop
 995     end
 996 
</pre>
<hr />
<pre>
1029 macro notifyWrite(set, slow)
1030     bbneq WatchpointSet::m_state[set], IsInvalidated, slow
1031 end
1032 
1033 macro checkSwitchToJIT(increment, action)
1034     loadp CodeBlock[cfr], t0
1035     baddis increment, CodeBlock::m_llintExecuteCounter + BaselineExecutionCounter::m_counter[t0], .continue
1036     action()
1037     .continue:
1038 end
1039 
1040 macro checkSwitchToJITForEpilogue()
1041     checkSwitchToJIT(
1042         10,
1043         macro ()
1044             callSlowPath(_llint_replace)
1045         end)
1046 end
1047 
1048 macro assertNotConstant(size, index)
<span class="line-modified">1049     size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide16, FirstConstantRegisterIndexWide32, macro (FirstConstantRegisterIndex)</span>
1050         assert(macro (ok) bilt index, FirstConstantRegisterIndex, ok end)
1051     end)
1052 end
1053 
1054 macro functionForCallCodeBlockGetter(targetRegister)
1055     if JSVALUE64
1056         loadp Callee[cfr], targetRegister
1057     else
1058         loadp Callee + PayloadOffset[cfr], targetRegister
1059     end
1060     loadp JSFunction::m_executable[targetRegister], targetRegister
1061     loadp FunctionExecutable::m_codeBlockForCall[targetRegister], targetRegister
1062     loadp ExecutableToCodeBlockEdge::m_codeBlock[targetRegister], targetRegister
1063 end
1064 
1065 macro functionForConstructCodeBlockGetter(targetRegister)
1066     if JSVALUE64
1067         loadp Callee[cfr], targetRegister
1068     else
1069         loadp Callee + PayloadOffset[cfr], targetRegister
</pre>
<hr />
<pre>
1081     storep sourceRegister, CodeBlock[cfr]
1082 end
1083 
1084 macro notFunctionCodeBlockSetter(sourceRegister)
1085     # Nothing to do!
1086 end
1087 
1088 # Do the bare minimum required to execute code. Sets up the PC, leave the CodeBlock*
1089 # in t1. May also trigger prologue entry OSR.
1090 macro prologue(codeBlockGetter, codeBlockSetter, osrSlowPath, traceSlowPath)
1091     # Set up the call frame and check if we should OSR.
1092     tagReturnAddress sp
1093     preserveCallerPCAndCFR()
1094 
1095     if TRACING
1096         subp maxFrameExtentForSlowPathCall, sp
1097         callSlowPath(traceSlowPath)
1098         addp maxFrameExtentForSlowPathCall, sp
1099     end
1100     codeBlockGetter(t1)
<span class="line-modified">1101     if not (C_LOOP or C_LOOP_WIN)</span>
1102         baddis 5, CodeBlock::m_llintExecuteCounter + BaselineExecutionCounter::m_counter[t1], .continue
1103         if JSVALUE64
1104             move cfr, a0
1105             move PC, a1
1106             cCall2(osrSlowPath)
1107         else
1108             # We are after the function prologue, but before we have set up sp from the CodeBlock.
1109             # Temporarily align stack pointer for this call.
1110             subp 8, sp
1111             move cfr, a0
1112             move PC, a1
1113             cCall2(osrSlowPath)
1114             addp 8, sp
1115         end
1116         btpz r0, .recover
1117         move cfr, sp # restore the previous sp
1118         # pop the callerFrame since we will jump to a function that wants to save it
1119         if ARM64 or ARM64E
1120             pop lr, cfr
1121             untagReturnAddress sp
</pre>
<hr />
<pre>
1131     .continue:
1132     end
1133 
1134     codeBlockSetter(t1)
1135 
1136     preserveCalleeSavesUsedByLLInt()
1137 
1138     # Set up the PC.
1139     if JSVALUE64
1140         loadp CodeBlock::m_instructionsRawPointer[t1], PB
1141         move 0, PC
1142     else
1143         loadp CodeBlock::m_instructionsRawPointer[t1], PC
1144     end
1145 
1146     # Get new sp in t0 and check stack height.
1147     getFrameRegisterSizeForCodeBlock(t1, t0)
1148     subp cfr, t0, t0
1149     bpa t0, cfr, .needStackCheck
1150     loadp CodeBlock::m_vm[t1], t2
<span class="line-modified">1151     if C_LOOP or C_LOOP_WIN</span>
1152         bpbeq VM::m_cloopStackLimit[t2], t0, .stackHeightOK
1153     else
1154         bpbeq VM::m_softStackLimit[t2], t0, .stackHeightOK
1155     end
1156 
1157 .needStackCheck:
1158     # Stack height check failed - need to call a slow_path.
1159     # Set up temporary stack pointer for call including callee saves
1160     subp maxFrameExtentForSlowPathCall, sp
1161     callSlowPath(_llint_stack_check)
1162     bpeq r1, 0, .stackHeightOKGetCodeBlock
1163 
1164     # We&#39;re throwing before the frame is fully set up. This frame will be
1165     # ignored by the unwinder. So, let&#39;s restore the callee saves before we
1166     # start unwinding. We need to do this before we change the cfr.
1167     restoreCalleeSavesUsedByLLInt()
1168 
1169     move r1, cfr
1170     jmp _llint_throw_from_slow_path_trampoline
1171 
</pre>
<hr />
<pre>
1234 .argumentProfileDone:
1235 end
1236 
1237 macro doReturn()
1238     restoreCalleeSavesUsedByLLInt()
1239     restoreCallerPCAndCFR()
1240     ret
1241 end
1242 
1243 # This break instruction is needed so that the synthesized llintPCRangeStart label
1244 # doesn&#39;t point to the exact same location as vmEntryToJavaScript which comes after it.
1245 # Otherwise, libunwind will report vmEntryToJavaScript as llintPCRangeStart in
1246 # stack traces.
1247 
1248     break
1249 
1250 # stub to call into JavaScript or Native functions
1251 # EncodedJSValue vmEntryToJavaScript(void* code, VM* vm, ProtoCallFrame* protoFrame)
1252 # EncodedJSValue vmEntryToNativeFunction(void* code, VM* vm, ProtoCallFrame* protoFrame)
1253 
<span class="line-modified">1254 if C_LOOP or C_LOOP_WIN</span>
1255     _llint_vm_entry_to_javascript:
1256 else
1257     global _vmEntryToJavaScript
1258     _vmEntryToJavaScript:
1259 end
1260     doVMEntry(makeJavaScriptCall)
1261 
1262 
<span class="line-modified">1263 if C_LOOP or C_LOOP_WIN</span>
1264     _llint_vm_entry_to_native:
1265 else
1266     global _vmEntryToNative
1267     _vmEntryToNative:
1268 end
1269     doVMEntry(makeHostFunctionCall)
1270 
1271 
<span class="line-modified">1272 if not (C_LOOP or C_LOOP_WIN)</span>
1273     # void sanitizeStackForVMImpl(VM* vm)
1274     global _sanitizeStackForVMImpl
1275     _sanitizeStackForVMImpl:
1276         tagReturnAddress sp
1277         # We need three non-aliased caller-save registers. We are guaranteed
1278         # this for a0, a1 and a2 on all architectures.
1279         if X86 or X86_WIN
1280             loadp 4[sp], a0
1281         end
1282         const vm = a0
1283         const address = a1
1284         const zeroValue = a2
1285     
1286         loadp VM::m_lastStackTop[vm], address
1287         bpbeq sp, address, .zeroFillDone
1288     
1289         move 0, zeroValue
1290     .zeroFillLoop:
1291         storep zeroValue, [address]
1292         addp PtrSize, address
1293         bpa sp, address, .zeroFillLoop
1294 
1295     .zeroFillDone:
1296         move sp, address
1297         storep address, VM::m_lastStackTop[vm]
1298         ret
1299     
1300     # VMEntryRecord* vmEntryRecord(const EntryFrame* entryFrame)
1301     global _vmEntryRecord
1302     _vmEntryRecord:
1303         tagReturnAddress sp
1304         if X86 or X86_WIN
1305             loadp 4[sp], a0
1306         end
1307 
1308         vmEntryRecord(a0, r0)
1309         ret
1310 end
1311 
<span class="line-modified">1312 if C_LOOP or C_LOOP_WIN</span>
1313     # Dummy entry point the C Loop uses to initialize.
1314     _llint_entry:
1315         crash()
1316 else
1317     macro initPCRelative(pcBase)
1318         if X86_64 or X86_64_WIN or X86 or X86_WIN
1319             call _relativePCBase
1320         _relativePCBase:
1321             pop pcBase
1322         elsif ARM64 or ARM64E
1323         elsif ARMv7
1324         _relativePCBase:
1325             move pc, pcBase
1326             subp 3, pcBase   # Need to back up the PC and set the Thumb2 bit
1327         elsif MIPS
1328             la _relativePCBase, pcBase
1329             setcallreg pcBase # needed to set $t9 to the right value for the .cpload created by the label.
1330         _relativePCBase:
1331         end
1332 end
1333 
<span class="line-modified">1334 # The PC base is in t3, as this is what _llint_entry leaves behind through</span>
<span class="line-modified">1335 # initPCRelative(t3)</span>
1336 macro setEntryAddress(index, label)
1337     setEntryAddressCommon(index, label, a0)
1338 end
1339 
<span class="line-modified">1340 macro setEntryAddressWide16(index, label)</span>
1341      setEntryAddressCommon(index, label, a1)
1342 end
1343 
<span class="line-added">1344 macro setEntryAddressWide32(index, label)</span>
<span class="line-added">1345      setEntryAddressCommon(index, label, a2)</span>
<span class="line-added">1346 end</span>
<span class="line-added">1347 </span>
1348 macro setEntryAddressCommon(index, label, map)
<span class="line-modified">1349     if X86_64</span>
<span class="line-modified">1350         leap (label - _relativePCBase)[t3], t4</span>
<span class="line-modified">1351         move index, t5</span>
<span class="line-modified">1352         storep t4, [map, t5, 8]</span>
<span class="line-added">1353     elsif X86_64_WIN</span>
<span class="line-added">1354         leap (label - _relativePCBase)[t3], t4</span>
<span class="line-added">1355         move index, t0</span>
<span class="line-added">1356         storep t4, [map, t0, 8]</span>
1357     elsif X86 or X86_WIN
<span class="line-modified">1358         leap (label - _relativePCBase)[t3], t4</span>
<span class="line-modified">1359         move index, t5</span>
<span class="line-modified">1360         storep t4, [map, t5, 4]</span>
1361     elsif ARM64 or ARM64E
<span class="line-modified">1362         pcrtoaddr label, t3</span>
1363         move index, t4
<span class="line-modified">1364         storep t3, [map, t4, PtrSize]</span>
1365     elsif ARMv7
1366         mvlbl (label - _relativePCBase), t4
<span class="line-modified">1367         addp t4, t3, t4</span>
<span class="line-modified">1368         move index, t5</span>
<span class="line-modified">1369         storep t4, [map, t5, 4]</span>
1370     elsif MIPS
1371         la label, t4
1372         la _relativePCBase, t3
1373         subp t3, t4
<span class="line-modified">1374         addp t4, t3, t4</span>
<span class="line-modified">1375         move index, t5</span>
<span class="line-modified">1376         storep t4, [map, t5, 4]</span>
1377     end
1378 end
1379 
1380 global _llint_entry
1381 # Entry point for the llint to initialize.
1382 _llint_entry:
1383     functionPrologue()
1384     pushCalleeSaves()
1385     if X86 or X86_WIN
1386         loadp 20[sp], a0
1387         loadp 24[sp], a1
<span class="line-added">1388         loadp 28[sp], a2</span>
1389     end
1390 
<span class="line-modified">1391     initPCRelative(t3)</span>
1392 
1393     # Include generated bytecode initialization file.
1394     include InitBytecodes
1395 
1396     popCalleeSaves()
1397     functionEpilogue()
1398     ret
1399 end
1400 
<span class="line-modified">1401 _llint_op_wide16:</span>
<span class="line-modified">1402     nextInstructionWide16()</span>
1403 
<span class="line-modified">1404 _llint_op_wide32:</span>
<span class="line-added">1405     nextInstructionWide32()</span>
<span class="line-added">1406 </span>
<span class="line-added">1407 macro noWide(label)</span>
<span class="line-added">1408 _llint_%label%_wide16:</span>
1409     crash()
1410 
<span class="line-modified">1411 _llint_%label%_wide32:</span>
1412     crash()
<span class="line-added">1413 end</span>
<span class="line-added">1414 </span>
<span class="line-added">1415 noWide(op_wide16)</span>
<span class="line-added">1416 noWide(op_wide32)</span>
<span class="line-added">1417 noWide(op_enter)</span>
1418 
1419 op(llint_program_prologue, macro ()
1420     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1421     dispatch(0)
1422 end)
1423 
1424 
1425 op(llint_module_program_prologue, macro ()
1426     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1427     dispatch(0)
1428 end)
1429 
1430 
1431 op(llint_eval_prologue, macro ()
1432     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1433     dispatch(0)
1434 end)
1435 
1436 
1437 op(llint_function_for_call_prologue, macro ()
</pre>
<hr />
<pre>
1654 
1655 compareUnsignedJumpOp(
1656     jbelow, OpJbelow,
1657     macro (left, right, target) bib left, right, target end)
1658 
1659 
1660 compareUnsignedJumpOp(
1661     jbeloweq, OpJbeloweq,
1662     macro (left, right, target) bibeq left, right, target end)
1663 
1664 
1665 preOp(inc, OpInc,
1666     macro (value, slow) baddio 1, value, slow end)
1667 
1668 
1669 preOp(dec, OpDec,
1670     macro (value, slow) bsubio 1, value, slow end)
1671 
1672 
1673 llintOp(op_loop_hint, OpLoopHint, macro (unused, unused, dispatch)
<span class="line-modified">1674     # CheckTraps.</span>





1675     loadp CodeBlock[cfr], t1
1676     loadp CodeBlock::m_vm[t1], t1
<span class="line-modified">1677     btbnz VM::m_traps + VMTraps::m_needTrapHandling[t1], .handleTraps</span>

1678 .afterHandlingTraps:
<span class="line-added">1679     checkSwitchToJITForLoop()</span>
1680     dispatch()
1681 .handleTraps:
<span class="line-modified">1682     callTrapHandler(_llint_throw_from_slow_path_trampoline)</span>
1683     jmp .afterHandlingTraps


1684 end)
1685 
1686 
1687 # Returns the packet pointer in t0.
1688 macro acquireShadowChickenPacket(slow)
1689     loadp CodeBlock[cfr], t1
1690     loadp CodeBlock::m_vm[t1], t1
1691     loadp VM::m_shadowChicken[t1], t2
1692     loadp ShadowChicken::m_logCursor[t2], t0
1693     bpaeq t0, ShadowChicken::m_logEnd[t2], slow
1694     addp sizeof ShadowChicken::Packet, t0, t1
1695     storep t1, ShadowChicken::m_logCursor[t2]
1696 end
1697 
1698 
1699 llintOp(op_nop, OpNop, macro (unused, unused, dispatch)
1700     dispatch()
1701 end)
1702 
1703 
</pre>
<hr />
<pre>
1791 #   the return value, or did not execute the eval but has a PC for us
1792 #   to call.
1793 #
1794 # - Either:
1795 #   - The JS return value (two registers), or
1796 #
1797 #   - The PC to call.
1798 #
1799 # It turns out to be easier to just always have this return the cfr
1800 # and a PC to call, and that PC may be a dummy thunk that just
1801 # returns the JS value that the eval returned.
1802 
1803 _llint_op_call_eval:
1804     slowPathForCall(
1805         narrow,
1806         OpCallEval,
1807         macro () dispatchOp(narrow, op_call_eval) end,
1808         _llint_slow_path_call_eval,
1809         prepareForRegularCall)
1810 
<span class="line-modified">1811 _llint_op_call_eval_wide16:</span>
1812     slowPathForCall(
<span class="line-modified">1813         wide16,</span>
1814         OpCallEval,
<span class="line-modified">1815         macro () dispatchOp(wide16, op_call_eval) end,</span>
<span class="line-modified">1816         _llint_slow_path_call_eval_wide16,</span>
<span class="line-added">1817         prepareForRegularCall)</span>
<span class="line-added">1818 </span>
<span class="line-added">1819 _llint_op_call_eval_wide32:</span>
<span class="line-added">1820     slowPathForCall(</span>
<span class="line-added">1821         wide32,</span>
<span class="line-added">1822         OpCallEval,</span>
<span class="line-added">1823         macro () dispatchOp(wide32, op_call_eval) end,</span>
<span class="line-added">1824         _llint_slow_path_call_eval_wide32,</span>
1825         prepareForRegularCall)
1826 




1827 
<span class="line-modified">1828 commonOp(llint_generic_return_point, macro () end, macro (size)</span>
<span class="line-modified">1829     dispatchAfterCall(size, OpCallEval, macro ()</span>
<span class="line-modified">1830         dispatchOp(size, op_call_eval)</span>
1831     end)
<span class="line-added">1832 end)</span>
<span class="line-added">1833 </span>
1834 
1835 llintOp(op_identity_with_profile, OpIdentityWithProfile, macro (unused, unused, dispatch)
1836     dispatch()
1837 end)
1838 
1839 
1840 llintOp(op_yield, OpYield, macro (unused, unused, unused)
1841     notSupported()
1842 end)
1843 
1844 
<span class="line-added">1845 llintOp(op_create_generator_frame_environment, OpYield, macro (unused, unused, unused)</span>
<span class="line-added">1846     notSupported()</span>
<span class="line-added">1847 end)</span>
<span class="line-added">1848 </span>
<span class="line-added">1849 </span>
1850 llintOp(op_debug, OpDebug, macro (unused, unused, dispatch)
1851     loadp CodeBlock[cfr], t0
1852     loadi CodeBlock::m_debuggerRequests[t0], t0
1853     btiz t0, .opDebugDone
1854     callSlowPath(_llint_slow_path_debug)
1855 .opDebugDone:                    
1856     dispatch()
1857 end)
1858 
1859 
1860 op(llint_native_call_trampoline, macro ()
1861     nativeCallTrampoline(NativeExecutable::m_function)
1862 end)
1863 
1864 
1865 op(llint_native_construct_trampoline, macro ()
1866     nativeCallTrampoline(NativeExecutable::m_constructor)
1867 end)
1868 
1869 
</pre>
</td>
</tr>
</table>
<center><a href="LLIntThunks.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="LowLevelInterpreter.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>