<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter64.asm</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 # Copyright (C) 2011-2019 Apple Inc. All rights reserved.
   2 #
   3 # Redistribution and use in source and binary forms, with or without
   4 # modification, are permitted provided that the following conditions
   5 # are met:
   6 # 1. Redistributions of source code must retain the above copyright
   7 #    notice, this list of conditions and the following disclaimer.
   8 # 2. Redistributions in binary form must reproduce the above copyright
   9 #    notice, this list of conditions and the following disclaimer in the
  10 #    documentation and/or other materials provided with the distribution.
  11 #
  12 # THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
  13 # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
  14 # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  15 # PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
  16 # BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  17 # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  18 # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  19 # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  20 # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  21 # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
  22 # THE POSSIBILITY OF SUCH DAMAGE.
  23 
  24 
  25 # Utilities.
  26 
  27 macro nextInstruction()
  28     loadb [PB, PC, 1], t0
  29     leap _g_opcodeMap, t1
  30     jmp [t1, t0, PtrSize], BytecodePtrTag
  31 end
  32 
  33 macro nextInstructionWide16()
  34     loadh 1[PB, PC, 1], t0
  35     leap _g_opcodeMapWide16, t1
  36     jmp [t1, t0, PtrSize], BytecodePtrTag
  37 end
  38 
  39 macro nextInstructionWide32()
  40     loadi 1[PB, PC, 1], t0
  41     leap _g_opcodeMapWide32, t1
  42     jmp [t1, t0, PtrSize], BytecodePtrTag
  43 end
  44 
  45 macro getuOperandNarrow(opcodeStruct, fieldName, dst)
  46     loadb constexpr %opcodeStruct%_%fieldName%_index[PB, PC, 1], dst
  47 end
  48 
  49 macro getOperandNarrow(opcodeStruct, fieldName, dst)
  50     loadbsq constexpr %opcodeStruct%_%fieldName%_index[PB, PC, 1], dst
  51 end
  52 
  53 macro getuOperandWide16(opcodeStruct, fieldName, dst)
  54     loadh constexpr %opcodeStruct%_%fieldName%_index * 2 + 1[PB, PC, 1], dst
  55 end
  56 
  57 macro getOperandWide16(opcodeStruct, fieldName, dst)
  58     loadhsq constexpr %opcodeStruct%_%fieldName%_index * 2 + 1[PB, PC, 1], dst
  59 end
  60 
  61 macro getuOperandWide32(opcodeStruct, fieldName, dst)
  62     loadi constexpr %opcodeStruct%_%fieldName%_index * 4 + 1[PB, PC, 1], dst
  63 end
  64 
  65 macro getOperandWide32(opcodeStruct, fieldName, dst)
  66     loadis constexpr %opcodeStruct%_%fieldName%_index * 4 + 1[PB, PC, 1], dst
  67 end
  68 
  69 macro makeReturn(get, dispatch, fn)
  70     fn(macro (value)
  71         move value, t2
  72         get(m_dst, t1)
  73         storeq t2, [cfr, t1, 8]
  74         dispatch()
  75     end)
  76 end
  77 
  78 macro makeReturnProfiled(opcodeStruct, get, metadata, dispatch, fn)
  79     fn(macro (value)
  80         move value, t3
  81         metadata(t1, t2)
  82         valueProfile(opcodeStruct, t1, t3)
  83         get(m_dst, t1)
  84         storeq t3, [cfr, t1, 8]
  85         dispatch()
  86     end)
  87 end
  88 
  89 macro valueProfile(opcodeStruct, metadata, value)
  90     storeq value, %opcodeStruct%::Metadata::m_profile.m_buckets[metadata]
  91 end
  92 
  93 macro dispatchAfterCall(size, opcodeStruct, dispatch)
  94     loadi ArgumentCount + TagOffset[cfr], PC
  95     loadp CodeBlock[cfr], PB
  96     loadp CodeBlock::m_instructionsRawPointer[PB], PB
  97     get(size, opcodeStruct, m_dst, t1)
  98     storeq r0, [cfr, t1, 8]
  99     metadata(size, opcodeStruct, t2, t1)
 100     valueProfile(opcodeStruct, t2, r0)
 101     dispatch()
 102 end
 103 
 104 macro cCall2(function)
 105     checkStackPointerAlignment(t4, 0xbad0c002)
 106     if X86_64 or ARM64 or ARM64E
 107         call function
 108     elsif X86_64_WIN
 109         # Note: this implementation is only correct if the return type size is &gt; 8 bytes.
 110         # See macro cCall2Void for an implementation when the return type &lt;= 8 bytes.
 111         # On Win64, when the return type is larger than 8 bytes, we need to allocate space on the stack for the return value.
 112         # On entry rcx (a0), should contain a pointer to this stack space. The other parameters are shifted to the right,
 113         # rdx (a1) should contain the first argument, and r8 (a2) should contain the second argument.
 114         # On return, rax contains a pointer to this stack value, and we then need to copy the 16 byte return value into rax (r0) and rdx (r1)
 115         # since the return value is expected to be split between the two.
 116         # See http://msdn.microsoft.com/en-us/library/7572ztz4.aspx
 117         move a1, a2
 118         move a0, a1
 119         subp 48, sp
 120         move sp, a0
 121         addp 32, a0
 122         call function
 123         addp 48, sp
 124         move 8[r0], r1
 125         move [r0], r0
 126     elsif C_LOOP or C_LOOP_WIN
 127         cloopCallSlowPath function, a0, a1
 128     else
 129         error
 130     end
 131 end
 132 
 133 macro cCall2Void(function)
 134     if C_LOOP or C_LOOP_WIN
 135         cloopCallSlowPathVoid function, a0, a1
 136     elsif X86_64_WIN
 137         # Note: we cannot use the cCall2 macro for Win64 in this case,
 138         # as the Win64 cCall2 implemenation is only correct when the return type size is &gt; 8 bytes.
 139         # On Win64, rcx and rdx are used for passing the first two parameters.
 140         # We also need to make room on the stack for all four parameter registers.
 141         # See http://msdn.microsoft.com/en-us/library/ms235286.aspx
 142         subp 32, sp 
 143         call function
 144         addp 32, sp
 145     else
 146         cCall2(function)
 147     end
 148 end
 149 
 150 # This barely works. arg3 and arg4 should probably be immediates.
 151 macro cCall4(function)
 152     checkStackPointerAlignment(t4, 0xbad0c004)
 153     if X86_64 or ARM64 or ARM64E
 154         call function
 155     elsif X86_64_WIN
 156         # On Win64, rcx, rdx, r8, and r9 are used for passing the first four parameters.
 157         # We also need to make room on the stack for all four parameter registers.
 158         # See http://msdn.microsoft.com/en-us/library/ms235286.aspx
 159         subp 64, sp
 160         call function
 161         addp 64, sp
 162     else
 163         error
 164     end
 165 end
 166 
 167 macro doVMEntry(makeCall)
 168     functionPrologue()
 169     pushCalleeSaves()
 170 
 171     const entry = a0
 172     const vm = a1
 173     const protoCallFrame = a2
 174 
 175     vmEntryRecord(cfr, sp)
 176 
 177     checkStackPointerAlignment(t4, 0xbad0dc01)
 178 
 179     storep vm, VMEntryRecord::m_vm[sp]
 180     loadp VM::topCallFrame[vm], t4
 181     storep t4, VMEntryRecord::m_prevTopCallFrame[sp]
 182     loadp VM::topEntryFrame[vm], t4
 183     storep t4, VMEntryRecord::m_prevTopEntryFrame[sp]
 184     loadp ProtoCallFrame::calleeValue[protoCallFrame], t4
 185     storep t4, VMEntryRecord::m_callee[sp]
 186 
 187     loadi ProtoCallFrame::paddedArgCount[protoCallFrame], t4
 188     addp CallFrameHeaderSlots, t4, t4
 189     lshiftp 3, t4
 190     subp sp, t4, t3
 191     bqbeq sp, t3, .throwStackOverflow
 192 
 193     # Ensure that we have enough additional stack capacity for the incoming args,
 194     # and the frame for the JS code we&#39;re executing. We need to do this check
 195     # before we start copying the args from the protoCallFrame below.
 196     if C_LOOP or C_LOOP_WIN
 197         bpaeq t3, VM::m_cloopStackLimit[vm], .stackHeightOK
 198         move entry, t4
 199         move vm, t5
 200         cloopCallSlowPath _llint_stack_check_at_vm_entry, vm, t3
 201         bpeq t0, 0, .stackCheckFailed
 202         move t4, entry
 203         move t5, vm
 204         jmp .stackHeightOK
 205 
 206 .stackCheckFailed:
 207         move t4, entry
 208         move t5, vm
 209         jmp .throwStackOverflow
 210     else
 211         bpb t3, VM::m_softStackLimit[vm], .throwStackOverflow
 212     end
 213 
 214 .stackHeightOK:
 215     move t3, sp
 216     move (constexpr ProtoCallFrame::numberOfRegisters), t3
 217 
 218 .copyHeaderLoop:
 219     # Copy the CodeBlock/Callee/ArgumentCount/|this| from protoCallFrame into the callee frame.
 220     subi 1, t3
 221     loadq [protoCallFrame, t3, 8], extraTempReg
 222     storeq extraTempReg, CodeBlock[sp, t3, 8]
 223     btinz t3, .copyHeaderLoop
 224 
 225     loadi PayloadOffset + ProtoCallFrame::argCountAndCodeOriginValue[protoCallFrame], t4
 226     subi 1, t4
 227     loadi ProtoCallFrame::paddedArgCount[protoCallFrame], extraTempReg
 228     subi 1, extraTempReg
 229 
 230     bieq t4, extraTempReg, .copyArgs
 231     move ValueUndefined, t3
 232 .fillExtraArgsLoop:
 233     subi 1, extraTempReg
 234     storeq t3, ThisArgumentOffset + 8[sp, extraTempReg, 8]
 235     bineq t4, extraTempReg, .fillExtraArgsLoop
 236 
 237 .copyArgs:
 238     loadp ProtoCallFrame::args[protoCallFrame], t3
 239 
 240 .copyArgsLoop:
 241     btiz t4, .copyArgsDone
 242     subi 1, t4
 243     loadq [t3, t4, 8], extraTempReg
 244     storeq extraTempReg, ThisArgumentOffset + 8[sp, t4, 8]
 245     jmp .copyArgsLoop
 246 
 247 .copyArgsDone:
 248     if ARM64 or ARM64E
 249         move sp, t4
 250         storep t4, VM::topCallFrame[vm]
 251     else
 252         storep sp, VM::topCallFrame[vm]
 253     end
 254     storep cfr, VM::topEntryFrame[vm]
 255 
 256     checkStackPointerAlignment(extraTempReg, 0xbad0dc02)
 257 
 258     makeCall(entry, t3, t4)
 259 
 260     # We may have just made a call into a JS function, so we can&#39;t rely on sp
 261     # for anything but the fact that our own locals (ie the VMEntryRecord) are
 262     # not below it. It also still has to be aligned, though.
 263     checkStackPointerAlignment(t2, 0xbad0dc03)
 264 
 265     vmEntryRecord(cfr, t4)
 266 
 267     loadp VMEntryRecord::m_vm[t4], vm
 268     loadp VMEntryRecord::m_prevTopCallFrame[t4], t2
 269     storep t2, VM::topCallFrame[vm]
 270     loadp VMEntryRecord::m_prevTopEntryFrame[t4], t2
 271     storep t2, VM::topEntryFrame[vm]
 272 
 273     subp cfr, CalleeRegisterSaveSize, sp
 274 
 275     popCalleeSaves()
 276     functionEpilogue()
 277     ret
 278 
 279 .throwStackOverflow:
 280     move vm, a0
 281     move protoCallFrame, a1
 282     cCall2(_llint_throw_stack_overflow_error)
 283 
 284     vmEntryRecord(cfr, t4)
 285 
 286     loadp VMEntryRecord::m_vm[t4], vm
 287     loadp VMEntryRecord::m_prevTopCallFrame[t4], extraTempReg
 288     storep extraTempReg, VM::topCallFrame[vm]
 289     loadp VMEntryRecord::m_prevTopEntryFrame[t4], extraTempReg
 290     storep extraTempReg, VM::topEntryFrame[vm]
 291 
 292     subp cfr, CalleeRegisterSaveSize, sp
 293 
 294     popCalleeSaves()
 295     functionEpilogue()
 296     ret
 297 end
 298 
 299 
 300 macro makeJavaScriptCall(entry, temp, unused)
 301     addp 16, sp
 302     if C_LOOP or C_LOOP_WIN
 303         cloopCallJSFunction entry
 304     else
 305         call entry, JSEntryPtrTag
 306     end
 307     subp 16, sp
 308 end
 309 
 310 macro makeHostFunctionCall(entry, temp, unused)
 311     move entry, temp
 312     storep cfr, [sp]
 313     move sp, a0
 314     if C_LOOP or C_LOOP_WIN
 315         storep lr, 8[sp]
 316         cloopCallNative temp
 317     elsif X86_64_WIN
 318         # We need to allocate 32 bytes on the stack for the shadow space.
 319         subp 32, sp
 320         call temp, JSEntryPtrTag
 321         addp 32, sp
 322     else
 323         call temp, JSEntryPtrTag
 324     end
 325 end
 326 
 327 op(handleUncaughtException, macro ()
 328     loadp Callee[cfr], t3
 329     andp MarkedBlockMask, t3
 330     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
 331     restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(t3, t0)
 332     storep 0, VM::callFrameForCatch[t3]
 333 
 334     loadp VM::topEntryFrame[t3], cfr
 335     vmEntryRecord(cfr, t2)
 336 
 337     loadp VMEntryRecord::m_vm[t2], t3
 338     loadp VMEntryRecord::m_prevTopCallFrame[t2], extraTempReg
 339     storep extraTempReg, VM::topCallFrame[t3]
 340     loadp VMEntryRecord::m_prevTopEntryFrame[t2], extraTempReg
 341     storep extraTempReg, VM::topEntryFrame[t3]
 342 
 343     subp cfr, CalleeRegisterSaveSize, sp
 344 
 345     popCalleeSaves()
 346     functionEpilogue()
 347     ret
 348 end)
 349 
 350 
 351 macro prepareStateForCCall()
 352     addp PB, PC
 353 end
 354 
 355 macro restoreStateAfterCCall()
 356     move r0, PC
 357     subp PB, PC
 358 end
 359 
 360 macro callSlowPath(slowPath)
 361     prepareStateForCCall()
 362     move cfr, a0
 363     move PC, a1
 364     cCall2(slowPath)
 365     restoreStateAfterCCall()
 366 end
 367 
 368 macro traceOperand(fromWhere, operand)
 369     prepareStateForCCall()
 370     move fromWhere, a2
 371     move operand, a3
 372     move cfr, a0
 373     move PC, a1
 374     cCall4(_llint_trace_operand)
 375     restoreStateAfterCCall()
 376 end
 377 
 378 macro traceValue(fromWhere, operand)
 379     prepareStateForCCall()
 380     move fromWhere, a2
 381     move operand, a3
 382     move cfr, a0
 383     move PC, a1
 384     cCall4(_llint_trace_value)
 385     restoreStateAfterCCall()
 386 end
 387 
 388 # Call a slow path for call call opcodes.
 389 macro callCallSlowPath(slowPath, action)
 390     storei PC, ArgumentCount + TagOffset[cfr]
 391     prepareStateForCCall()
 392     move cfr, a0
 393     move PC, a1
 394     cCall2(slowPath)
 395     action(r0, r1)
 396 end
 397 
 398 macro callTrapHandler(throwHandler)
 399     storei PC, ArgumentCount + TagOffset[cfr]
 400     prepareStateForCCall()
 401     move cfr, a0
 402     move PC, a1
 403     cCall2(_llint_slow_path_handle_traps)
 404     btpnz r0, throwHandler
 405     loadi ArgumentCount + TagOffset[cfr], PC
 406 end
 407 
 408 macro checkSwitchToJITForLoop()
 409     checkSwitchToJIT(
 410         1,
 411         macro()
 412             storei PC, ArgumentCount + TagOffset[cfr]
 413             prepareStateForCCall()
 414             move cfr, a0
 415             move PC, a1
 416             cCall2(_llint_loop_osr)
 417             btpz r0, .recover
 418             move r1, sp
 419             jmp r0, JSEntryPtrTag
 420         .recover:
 421             loadi ArgumentCount + TagOffset[cfr], PC
 422         end)
 423 end
 424 
 425 macro cage(basePtr, mask, ptr, scratch)
 426     if GIGACAGE_ENABLED and not (C_LOOP or C_LOOP_WIN)
 427         loadp basePtr, scratch
 428         btpz scratch, .done
 429         andp mask, ptr
 430         addp scratch, ptr
 431     .done:
 432     end
 433 end
 434 
 435 macro cagedPrimitive(ptr, length, scratch, scratch2)
 436     if ARM64E
 437         const source = scratch2
 438         move ptr, scratch2
 439     else
 440         const source = ptr
 441     end
 442     if GIGACAGE_ENABLED
 443         cage(_g_gigacageBasePtrs + Gigacage::BasePtrs::primitive, constexpr Gigacage::primitiveGigacageMask, source, scratch)
 444         if ARM64E
 445             const numberOfPACBits = constexpr MacroAssembler::numberOfPACBits
 446             bfiq scratch2, 0, 64 - numberOfPACBits, ptr
 447         end
 448     end
 449     if ARM64E
 450         untagArrayPtr length, ptr
 451     end
 452 end
 453 
 454 macro loadCagedJSValue(source, dest, scratchOrLength)
 455     loadp source, dest
 456     cage(_g_gigacageBasePtrs + Gigacage::BasePtrs::jsValue, constexpr Gigacage::jsValueGigacageMask, dest, scratchOrLength)
 457 end
 458 
 459 macro loadVariable(get, fieldName, valueReg)
 460     get(fieldName, valueReg)
 461     loadq [cfr, valueReg, 8], valueReg
 462 end
 463 
 464 # Index and value must be different registers. Index may be clobbered.
 465 macro loadConstantOrVariable(size, index, value)
 466     macro loadNarrow()
 467         bpgteq index, FirstConstantRegisterIndexNarrow, .constant
 468         loadq [cfr, index, 8], value
 469         jmp .done
 470     .constant:
 471         loadp CodeBlock[cfr], value
 472         loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
 473         loadq -(FirstConstantRegisterIndexNarrow * 8)[value, index, 8], value
 474     .done:
 475     end
 476 
 477     macro loadWide16()
 478         bpgteq index, FirstConstantRegisterIndexWide16, .constant
 479         loadq [cfr, index, 8], value
 480         jmp .done
 481     .constant:
 482         loadp CodeBlock[cfr], value
 483         loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
 484         loadq -(FirstConstantRegisterIndexWide16 * 8)[value, index, 8], value
 485     .done:
 486     end
 487 
 488     macro loadWide32()
 489         bpgteq index, FirstConstantRegisterIndexWide32, .constant
 490         loadq [cfr, index, 8], value
 491         jmp .done
 492     .constant:
 493         loadp CodeBlock[cfr], value
 494         loadp CodeBlock::m_constantRegisters + VectorBufferOffset[value], value
 495         subp FirstConstantRegisterIndexWide32, index
 496         loadq [value, index, 8], value
 497     .done:
 498     end
 499 
 500     size(loadNarrow, loadWide16, loadWide32, macro (load) load() end)
 501 end
 502 
 503 macro loadConstantOrVariableInt32(size, index, value, slow)
 504     loadConstantOrVariable(size, index, value)
 505     bqb value, tagTypeNumber, slow
 506 end
 507 
 508 macro loadConstantOrVariableCell(size, index, value, slow)
 509     loadConstantOrVariable(size, index, value)
 510     btqnz value, tagMask, slow
 511 end
 512 
 513 macro writeBarrierOnCellWithReload(cell, reloadAfterSlowPath)
 514     skipIfIsRememberedOrInEden(
 515         cell,
 516         macro()
 517             push PB, PC
 518             move cell, a1 # cell can be a0
 519             move cfr, a0
 520             cCall2Void(_llint_write_barrier_slow)
 521             pop PC, PB
 522             reloadAfterSlowPath()
 523         end)
 524 end
 525 
 526 macro writeBarrierOnOperandWithReload(size, get, cellFieldName, reloadAfterSlowPath)
 527     get(cellFieldName, t1)
 528     loadConstantOrVariableCell(size, t1, t2, .writeBarrierDone)
 529     writeBarrierOnCellWithReload(t2, reloadAfterSlowPath)
 530 .writeBarrierDone:
 531 end
 532 
 533 macro writeBarrierOnOperand(size, get, cellFieldName)
 534     writeBarrierOnOperandWithReload(size, get, cellFieldName, macro () end)
 535 end
 536 
 537 macro writeBarrierOnOperands(size, get, cellFieldName, valueFieldName)
 538     get(valueFieldName, t1)
 539     loadConstantOrVariableCell(size, t1, t0, .writeBarrierDone)
 540     btpz t0, .writeBarrierDone
 541 
 542     writeBarrierOnOperand(size, get, cellFieldName)
 543 .writeBarrierDone:
 544 end
 545 
 546 macro writeBarrierOnGlobal(size, get, valueFieldName, loadMacro)
 547     get(valueFieldName, t1)
 548     loadConstantOrVariableCell(size, t1, t0, .writeBarrierDone)
 549     btpz t0, .writeBarrierDone
 550 
 551     loadMacro(t3)
 552     writeBarrierOnCellWithReload(t3, macro() end)
 553 .writeBarrierDone:
 554 end
 555 
 556 macro writeBarrierOnGlobalObject(size, get, valueFieldName)
 557     writeBarrierOnGlobal(size, get, valueFieldName,
 558         macro(registerToStoreGlobal)
 559             loadp CodeBlock[cfr], registerToStoreGlobal
 560             loadp CodeBlock::m_globalObject[registerToStoreGlobal], registerToStoreGlobal
 561         end)
 562 end
 563 
 564 macro writeBarrierOnGlobalLexicalEnvironment(size, get, valueFieldName)
 565     writeBarrierOnGlobal(size, get, valueFieldName,
 566         macro(registerToStoreGlobal)
 567             loadp CodeBlock[cfr], registerToStoreGlobal
 568             loadp CodeBlock::m_globalObject[registerToStoreGlobal], registerToStoreGlobal
 569             loadp JSGlobalObject::m_globalLexicalEnvironment[registerToStoreGlobal], registerToStoreGlobal
 570         end)
 571 end
 572 
 573 macro structureIDToStructureWithScratch(structureIDThenStructure, scratch, scratch2)
 574     loadp CodeBlock[cfr], scratch
 575     move structureIDThenStructure, scratch2
 576     loadp CodeBlock::m_vm[scratch], scratch
 577     rshifti NumberOfStructureIDEntropyBits, scratch2
 578     loadp VM::heap + Heap::m_structureIDTable + StructureIDTable::m_table[scratch], scratch
 579     loadp [scratch, scratch2, PtrSize], scratch2
 580     lshiftp StructureEntropyBitsShift, structureIDThenStructure
 581     xorp scratch2, structureIDThenStructure
 582 end
 583 
 584 macro loadStructureWithScratch(cell, structure, scratch, scratch2)
 585     loadi JSCell::m_structureID[cell], structure
 586     structureIDToStructureWithScratch(structure, scratch, scratch2)
 587 end
 588 
 589 # Entrypoints into the interpreter.
 590 
 591 # Expects that CodeBlock is in t1, which is what prologue() leaves behind.
 592 macro functionArityCheck(doneLabel, slowPath)
 593     loadi PayloadOffset + ArgumentCount[cfr], t0
 594     biaeq t0, CodeBlock::m_numParameters[t1], doneLabel
 595     prepareStateForCCall()
 596     move cfr, a0
 597     move PC, a1
 598     cCall2(slowPath)   # This slowPath has the protocol: r0 = 0 =&gt; no error, r0 != 0 =&gt; error
 599     btiz r0, .noError
 600 
 601     # We&#39;re throwing before the frame is fully set up. This frame will be
 602     # ignored by the unwinder. So, let&#39;s restore the callee saves before we
 603     # start unwinding. We need to do this before we change the cfr.
 604     restoreCalleeSavesUsedByLLInt()
 605 
 606     move r1, cfr   # r1 contains caller frame
 607     jmp _llint_throw_from_slow_path_trampoline
 608 
 609 .noError:
 610     move r1, t1 # r1 contains slotsToAdd.
 611     btiz t1, .continue
 612     loadi PayloadOffset + ArgumentCount[cfr], t2
 613     addi CallFrameHeaderSlots, t2
 614 
 615     // Check if there are some unaligned slots we can use
 616     move t1, t3
 617     andi StackAlignmentSlots - 1, t3
 618     btiz t3, .noExtraSlot
 619     move ValueUndefined, t0
 620 .fillExtraSlots:
 621     storeq t0, [cfr, t2, 8]
 622     addi 1, t2
 623     bsubinz 1, t3, .fillExtraSlots
 624     andi ~(StackAlignmentSlots - 1), t1
 625     btiz t1, .continue
 626 
 627 .noExtraSlot:
 628     if ARM64E
 629         loadp 8[cfr], lr
 630         addp 16, cfr, t3
 631         untagReturnAddress t3
 632     end
 633 
 634     // Move frame up t1 slots
 635     negq t1
 636     move cfr, t3
 637     subp CalleeSaveSpaceAsVirtualRegisters * 8, t3
 638     addi CalleeSaveSpaceAsVirtualRegisters, t2
 639     move t1, t0
 640     # Adds to sp are always 64-bit on arm64 so we need maintain t0&#39;s high bits.
 641     lshiftq 3, t0
 642     addp t0, cfr
 643     addp t0, sp
 644 .copyLoop:
 645     loadq [t3], t0
 646     storeq t0, [t3, t1, 8]
 647     addp 8, t3
 648     bsubinz 1, t2, .copyLoop
 649 
 650     // Fill new slots with JSUndefined
 651     move t1, t2
 652     move ValueUndefined, t0
 653 .fillLoop:
 654     storeq t0, [t3, t1, 8]
 655     addp 8, t3
 656     baddinz 1, t2, .fillLoop
 657 
 658     if ARM64E
 659         addp 16, cfr, t1
 660         tagReturnAddress t1
 661         storep lr, 8[cfr]
 662     end
 663 
 664 .continue:
 665     # Reload CodeBlock and reset PC, since the slow_path clobbered them.
 666     loadp CodeBlock[cfr], t1
 667     loadp CodeBlock::m_instructionsRawPointer[t1], PB
 668     move 0, PC
 669     jmp doneLabel
 670 end
 671 
 672 macro branchIfException(label)
 673     loadp Callee[cfr], t3
 674     andp MarkedBlockMask, t3
 675     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
 676     btpz VM::m_exception[t3], .noException
 677     jmp label
 678 .noException:
 679 end
 680 
 681 # Instruction implementations
 682 _llint_op_enter:
 683     traceExecution()
 684     checkStackPointerAlignment(t2, 0xdead00e1)
 685     loadp CodeBlock[cfr], t3                // t3&lt;CodeBlock&gt; = cfr.CodeBlock
 686     loadi CodeBlock::m_numVars[t3], t2      // t2&lt;size_t&gt; = t3&lt;CodeBlock&gt;.m_numVars
 687     subq CalleeSaveSpaceAsVirtualRegisters, t2
 688     move cfr, t1
 689     subq CalleeSaveSpaceAsVirtualRegisters * 8, t1
 690     btiz t2, .opEnterDone
 691     move ValueUndefined, t0
 692     negi t2
 693     sxi2q t2, t2
 694 .opEnterLoop:
 695     storeq t0, [t1, t2, 8]
 696     addq 1, t2
 697     btqnz t2, .opEnterLoop
 698 .opEnterDone:
 699     writeBarrierOnCellWithReload(t3, macro ()
 700         loadp CodeBlock[cfr], t3 # Reload CodeBlock
 701     end)
 702     loadp CodeBlock::m_vm[t3], t1
 703     btbnz VM::m_traps + VMTraps::m_needTrapHandling[t1], .handleTraps
 704 .afterHandlingTraps:
 705     dispatchOp(narrow, op_enter)
 706 .handleTraps:
 707     callTrapHandler(_llint_throw_from_slow_path_trampoline)
 708     jmp .afterHandlingTraps
 709 
 710 llintOpWithProfile(op_get_argument, OpGetArgument, macro (size, get, dispatch, return)
 711     get(m_index, t2)
 712     loadi PayloadOffset + ArgumentCount[cfr], t0
 713     bilteq t0, t2, .opGetArgumentOutOfBounds
 714     loadq ThisArgumentOffset[cfr, t2, 8], t0
 715     return(t0)
 716 
 717 .opGetArgumentOutOfBounds:
 718     return(ValueUndefined)
 719 end)
 720 
 721 
 722 llintOpWithReturn(op_argument_count, OpArgumentCount, macro (size, get, dispatch, return)
 723     loadi PayloadOffset + ArgumentCount[cfr], t0
 724     subi 1, t0
 725     orq TagTypeNumber, t0
 726     return(t0)
 727 end)
 728 
 729 
 730 llintOpWithReturn(op_get_scope, OpGetScope, macro (size, get, dispatch, return)
 731     loadp Callee[cfr], t0
 732     loadp JSCallee::m_scope[t0], t0
 733     return(t0)
 734 end)
 735 
 736 
 737 llintOpWithMetadata(op_to_this, OpToThis, macro (size, get, dispatch, metadata, return)
 738     get(m_srcDst, t0)
 739     loadq [cfr, t0, 8], t0
 740     btqnz t0, tagMask, .opToThisSlow
 741     bbneq JSCell::m_type[t0], FinalObjectType, .opToThisSlow
 742     loadi JSCell::m_structureID[t0], t1
 743     metadata(t2, t3)
 744     loadi OpToThis::Metadata::m_cachedStructureID[t2], t2
 745     bineq t1, t2, .opToThisSlow
 746     dispatch()
 747 
 748 .opToThisSlow:
 749     callSlowPath(_slow_path_to_this)
 750     dispatch()
 751 end)
 752 
 753 
 754 llintOp(op_check_tdz, OpCheckTdz, macro (size, get, dispatch)
 755     get(m_targetVirtualRegister, t0)
 756     loadConstantOrVariable(size, t0, t1)
 757     bqneq t1, ValueEmpty, .opNotTDZ
 758     callSlowPath(_slow_path_throw_tdz_error)
 759 
 760 .opNotTDZ:
 761     dispatch()
 762 end)
 763 
 764 
 765 llintOpWithReturn(op_mov, OpMov, macro (size, get, dispatch, return)
 766     get(m_src, t1)
 767     loadConstantOrVariable(size, t1, t2)
 768     return(t2)
 769 end)
 770 
 771 
 772 llintOpWithReturn(op_not, OpNot, macro (size, get, dispatch, return)
 773     get(m_operand, t0)
 774     loadConstantOrVariable(size, t0, t2)
 775     xorq ValueFalse, t2
 776     btqnz t2, ~1, .opNotSlow
 777     xorq ValueTrue, t2
 778     return(t2)
 779 
 780 .opNotSlow:
 781     callSlowPath(_slow_path_not)
 782     dispatch()
 783 end)
 784 
 785 
 786 macro equalityComparisonOp(opcodeName, opcodeStruct, integerComparison)
 787     llintOpWithReturn(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, return)
 788         get(m_rhs, t0)
 789         get(m_lhs, t2)
 790         loadConstantOrVariableInt32(size, t0, t1, .slow)
 791         loadConstantOrVariableInt32(size, t2, t0, .slow)
 792         integerComparison(t0, t1, t0)
 793         orq ValueFalse, t0
 794         return(t0)
 795 
 796     .slow:
 797         callSlowPath(_slow_path_%opcodeName%)
 798         dispatch()
 799     end)
 800 end
 801 
 802 
 803 macro equalNullComparisonOp(opcodeName, opcodeStruct, fn)
 804     llintOpWithReturn(opcodeName, opcodeStruct, macro (size, get, dispatch, return)
 805         get(m_operand, t0)
 806         loadq [cfr, t0, 8], t0
 807         btqnz t0, tagMask, .immediate
 808         btbnz JSCell::m_flags[t0], MasqueradesAsUndefined, .masqueradesAsUndefined
 809         move 0, t0
 810         jmp .done
 811     .masqueradesAsUndefined:
 812         loadStructureWithScratch(t0, t2, t1, t3)
 813         loadp CodeBlock[cfr], t0
 814         loadp CodeBlock::m_globalObject[t0], t0
 815         cpeq Structure::m_globalObject[t2], t0, t0
 816         jmp .done
 817     .immediate:
 818         andq ~TagBitUndefined, t0
 819         cqeq t0, ValueNull, t0
 820     .done:
 821         fn(t0)
 822         return(t0)
 823     end)
 824 end
 825 
 826 equalNullComparisonOp(op_eq_null, OpEqNull,
 827     macro (value) orq ValueFalse, value end)
 828 
 829 
 830 equalNullComparisonOp(op_neq_null, OpNeqNull,
 831     macro (value) xorq ValueTrue, value end)
 832 
 833 
 834 llintOpWithReturn(op_is_undefined_or_null, OpIsUndefinedOrNull, macro (size, get, dispatch, return)
 835     get(m_operand, t1)
 836     loadConstantOrVariable(size, t1, t0)
 837     andq ~TagBitUndefined, t0
 838     cqeq t0, ValueNull, t0
 839     orq ValueFalse, t0
 840     return(t0)
 841 end)
 842 
 843 
 844 macro strictEqOp(opcodeName, opcodeStruct, equalityOperation)
 845     llintOpWithReturn(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, return)
 846         get(m_rhs, t0)
 847         get(m_lhs, t2)
 848         loadConstantOrVariable(size, t0, t1)
 849         loadConstantOrVariable(size, t2, t0)
 850         move t0, t2
 851         orq t1, t2
 852         btqz t2, tagMask, .slow
 853         bqaeq t0, tagTypeNumber, .leftOK
 854         btqnz t0, tagTypeNumber, .slow
 855     .leftOK:
 856         bqaeq t1, tagTypeNumber, .rightOK
 857         btqnz t1, tagTypeNumber, .slow
 858     .rightOK:
 859         equalityOperation(t0, t1, t0)
 860         orq ValueFalse, t0
 861         return(t0)
 862 
 863     .slow:
 864         callSlowPath(_slow_path_%opcodeName%)
 865         dispatch()
 866     end)
 867 end
 868 
 869 
 870 strictEqOp(stricteq, OpStricteq,
 871     macro (left, right, result) cqeq left, right, result end)
 872 
 873 
 874 strictEqOp(nstricteq, OpNstricteq,
 875     macro (left, right, result) cqneq left, right, result end)
 876 
 877 
 878 macro strictEqualityJumpOp(opcodeName, opcodeStruct, equalityOperation)
 879     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
 880         get(m_lhs, t2)
 881         get(m_rhs, t3)
 882         loadConstantOrVariable(size, t2, t0)
 883         loadConstantOrVariable(size, t3, t1)
 884         move t0, t2
 885         orq t1, t2
 886         btqz t2, tagMask, .slow
 887         bqaeq t0, tagTypeNumber, .leftOK
 888         btqnz t0, tagTypeNumber, .slow
 889     .leftOK:
 890         bqaeq t1, tagTypeNumber, .rightOK
 891         btqnz t1, tagTypeNumber, .slow
 892     .rightOK:
 893         equalityOperation(t0, t1, .jumpTarget)
 894         dispatch()
 895 
 896     .jumpTarget:
 897         jump(m_targetLabel)
 898 
 899     .slow:
 900         callSlowPath(_llint_slow_path_%opcodeName%)
 901         nextInstruction()
 902     end)
 903 end
 904 
 905 
 906 strictEqualityJumpOp(jstricteq, OpJstricteq,
 907     macro (left, right, target) bqeq left, right, target end)
 908 
 909 
 910 strictEqualityJumpOp(jnstricteq, OpJnstricteq,
 911     macro (left, right, target) bqneq left, right, target end)
 912 
 913 
 914 macro preOp(opcodeName, opcodeStruct, arithmeticOperation)
 915     llintOp(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch)
 916         get(m_srcDst, t0)
 917         loadq [cfr, t0, 8], t1
 918         bqb t1, tagTypeNumber, .slow
 919         arithmeticOperation(t1, .slow)
 920         orq tagTypeNumber, t1
 921         storeq t1, [cfr, t0, 8]
 922         dispatch()
 923     .slow:
 924         callSlowPath(_slow_path_%opcodeName%)
 925         dispatch()
 926     end)
 927 end
 928 
 929 llintOpWithProfile(op_to_number, OpToNumber, macro (size, get, dispatch, return)
 930     get(m_operand, t0)
 931     loadConstantOrVariable(size, t0, t2)
 932     bqaeq t2, tagTypeNumber, .opToNumberIsImmediate
 933     btqz t2, tagTypeNumber, .opToNumberSlow
 934 .opToNumberIsImmediate:
 935     return(t2)
 936 
 937 .opToNumberSlow:
 938     callSlowPath(_slow_path_to_number)
 939     dispatch()
 940 end)
 941 
 942 
 943 llintOpWithReturn(op_to_string, OpToString, macro (size, get, dispatch, return)
 944     get(m_operand, t1)
 945     loadConstantOrVariable(size, t1, t0)
 946     btqnz t0, tagMask, .opToStringSlow
 947     bbneq JSCell::m_type[t0], StringType, .opToStringSlow
 948 .opToStringIsString:
 949     return(t0)
 950 
 951 .opToStringSlow:
 952     callSlowPath(_slow_path_to_string)
 953     dispatch()
 954 end)
 955 
 956 
 957 llintOpWithProfile(op_to_object, OpToObject, macro (size, get, dispatch, return)
 958     get(m_operand, t0)
 959     loadConstantOrVariable(size, t0, t2)
 960     btqnz t2, tagMask, .opToObjectSlow
 961     bbb JSCell::m_type[t2], ObjectType, .opToObjectSlow
 962     return(t2)
 963 
 964 .opToObjectSlow:
 965     callSlowPath(_slow_path_to_object)
 966     dispatch()
 967 end)
 968 
 969 
 970 llintOpWithMetadata(op_negate, OpNegate, macro (size, get, dispatch, metadata, return)
 971     get(m_operand, t0)
 972     loadConstantOrVariable(size, t0, t3)
 973     metadata(t1, t2)
 974     loadi OpNegate::Metadata::m_arithProfile + ArithProfile::m_bits[t1], t2
 975     bqb t3, tagTypeNumber, .opNegateNotInt
 976     btiz t3, 0x7fffffff, .opNegateSlow
 977     negi t3
 978     orq tagTypeNumber, t3
 979     ori ArithProfileInt, t2
 980     storei t2, OpNegate::Metadata::m_arithProfile + ArithProfile::m_bits[t1]
 981     return(t3)
 982 .opNegateNotInt:
 983     btqz t3, tagTypeNumber, .opNegateSlow
 984     xorq 0x8000000000000000, t3
 985     ori ArithProfileNumber, t2
 986     storei t2, OpNegate::Metadata::m_arithProfile + ArithProfile::m_bits[t1]
 987     return(t3)
 988 
 989 .opNegateSlow:
 990     callSlowPath(_slow_path_negate)
 991     dispatch()
 992 end)
 993 
 994 
 995 macro binaryOpCustomStore(opcodeName, opcodeStruct, integerOperationAndStore, doubleOperation)
 996     llintOpWithMetadata(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, metadata, return)
 997         metadata(t5, t0)
 998 
 999         macro profile(type)
1000             ori type, %opcodeStruct%::Metadata::m_arithProfile + ArithProfile::m_bits[t5]
1001         end
1002 
1003         get(m_rhs, t0)
1004         get(m_lhs, t2)
1005         loadConstantOrVariable(size, t0, t1)
1006         loadConstantOrVariable(size, t2, t0)
1007         bqb t0, tagTypeNumber, .op1NotInt
1008         bqb t1, tagTypeNumber, .op2NotInt
1009         get(m_dst, t2)
1010         integerOperationAndStore(t1, t0, .slow, t2)
1011 
1012         profile(ArithProfileIntInt)
1013         dispatch()
1014 
1015     .op1NotInt:
1016         # First operand is definitely not an int, the second operand could be anything.
1017         btqz t0, tagTypeNumber, .slow
1018         bqaeq t1, tagTypeNumber, .op1NotIntOp2Int
1019         btqz t1, tagTypeNumber, .slow
1020         addq tagTypeNumber, t1
1021         fq2d t1, ft1
1022         profile(ArithProfileNumberNumber)
1023         jmp .op1NotIntReady
1024     .op1NotIntOp2Int:
1025         profile(ArithProfileNumberInt)
1026         ci2d t1, ft1
1027     .op1NotIntReady:
1028         get(m_dst, t2)
1029         addq tagTypeNumber, t0
1030         fq2d t0, ft0
1031         doubleOperation(ft1, ft0)
1032         fd2q ft0, t0
1033         subq tagTypeNumber, t0
1034         storeq t0, [cfr, t2, 8]
1035         dispatch()
1036 
1037     .op2NotInt:
1038         # First operand is definitely an int, the second is definitely not.
1039         get(m_dst, t2)
1040         btqz t1, tagTypeNumber, .slow
1041         profile(ArithProfileIntNumber)
1042         ci2d t0, ft0
1043         addq tagTypeNumber, t1
1044         fq2d t1, ft1
1045         doubleOperation(ft1, ft0)
1046         fd2q ft0, t0
1047         subq tagTypeNumber, t0
1048         storeq t0, [cfr, t2, 8]
1049         dispatch()
1050 
1051     .slow:
1052         callSlowPath(_slow_path_%opcodeName%)
1053         dispatch()
1054     end)
1055 end
1056 
1057 if X86_64 or X86_64_WIN
1058     binaryOpCustomStore(div, OpDiv,
1059         macro (left, right, slow, index)
1060             # Assume t3 is scratchable.
1061             btiz left, slow
1062             bineq left, -1, .notNeg2TwoThe31DivByNeg1
1063             bieq right, -2147483648, .slow
1064         .notNeg2TwoThe31DivByNeg1:
1065             btinz right, .intOK
1066             bilt left, 0, slow
1067         .intOK:
1068             move left, t3
1069             move right, t0
1070             cdqi
1071             idivi t3
1072             btinz t1, slow
1073             orq tagTypeNumber, t0
1074             storeq t0, [cfr, index, 8]
1075         end,
1076         macro (left, right) divd left, right end)
1077 else
1078     slowPathOp(div)
1079 end
1080 
1081 
1082 binaryOpCustomStore(mul, OpMul,
1083     macro (left, right, slow, index)
1084         # Assume t3 is scratchable.
1085         move right, t3
1086         bmulio left, t3, slow
1087         btinz t3, .done
1088         bilt left, 0, slow
1089         bilt right, 0, slow
1090     .done:
1091         orq tagTypeNumber, t3
1092         storeq t3, [cfr, index, 8]
1093     end,
1094     macro (left, right) muld left, right end)
1095 
1096 
1097 macro binaryOp(opcodeName, opcodeStruct, integerOperation, doubleOperation)
1098     binaryOpCustomStore(opcodeName, opcodeStruct,
1099         macro (left, right, slow, index)
1100             integerOperation(left, right, slow)
1101             orq tagTypeNumber, right
1102             storeq right, [cfr, index, 8]
1103         end,
1104         doubleOperation)
1105 end
1106 
1107 binaryOp(add, OpAdd,
1108     macro (left, right, slow) baddio left, right, slow end,
1109     macro (left, right) addd left, right end)
1110 
1111 
1112 binaryOp(sub, OpSub,
1113     macro (left, right, slow) bsubio left, right, slow end,
1114     macro (left, right) subd left, right end)
1115 
1116 
1117 llintOpWithReturn(op_unsigned, OpUnsigned, macro (size, get, dispatch, return)
1118     get(m_operand, t1)
1119     loadConstantOrVariable(size, t1, t2)
1120     bilt t2, 0, .opUnsignedSlow
1121     return(t2)
1122 .opUnsignedSlow:
1123     callSlowPath(_slow_path_unsigned)
1124     dispatch()
1125 end)
1126 
1127 
1128 macro commonBitOp(opKind, opcodeName, opcodeStruct, operation)
1129     opKind(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, return)
1130         get(m_rhs, t0)
1131         get(m_lhs, t2)
1132         loadConstantOrVariable(size, t0, t1)
1133         loadConstantOrVariable(size, t2, t0)
1134         bqb t0, tagTypeNumber, .slow
1135         bqb t1, tagTypeNumber, .slow
1136         operation(t1, t0)
1137         orq tagTypeNumber, t0
1138         return(t0)
1139 
1140     .slow:
1141         callSlowPath(_slow_path_%opcodeName%)
1142         dispatch()
1143     end)
1144 end
1145 
1146 macro bitOp(opcodeName, opcodeStruct, operation)
1147     commonBitOp(llintOpWithReturn, opcodeName, opcodeStruct, operation)
1148 end
1149 
1150 macro bitOpProfiled(opcodeName, opcodeStruct, operation)
1151     commonBitOp(llintOpWithProfile, opcodeName, opcodeStruct, operation)
1152 end
1153 
1154 bitOpProfiled(lshift, OpLshift,
1155     macro (left, right) lshifti left, right end)
1156 
1157 
1158 bitOp(rshift, OpRshift,
1159     macro (left, right) rshifti left, right end)
1160 
1161 
1162 bitOp(urshift, OpUrshift,
1163     macro (left, right) urshifti left, right end)
1164 
1165 bitOpProfiled(bitand, OpBitand,
1166     macro (left, right) andi left, right end)
1167 
1168 bitOpProfiled(bitor, OpBitor,
1169     macro (left, right) ori left, right end)
1170 
1171 bitOpProfiled(bitxor, OpBitxor,
1172     macro (left, right) xori left, right end)
1173 
1174 llintOpWithProfile(op_bitnot, OpBitnot, macro (size, get, dispatch, return)
1175     get(m_operand, t0)
1176     loadConstantOrVariableInt32(size, t0, t3, .opBitNotSlow)
1177     noti t3
1178     orq tagTypeNumber, t3
1179     return(t3)
1180 .opBitNotSlow:
1181     callSlowPath(_slow_path_bitnot)
1182     dispatch()
1183 end)
1184 
1185 
1186 llintOp(op_overrides_has_instance, OpOverridesHasInstance, macro (size, get, dispatch)
1187     get(m_dst, t3)
1188 
1189     get(m_hasInstanceValue, t1)
1190     loadConstantOrVariable(size, t1, t0)
1191     loadp CodeBlock[cfr], t2
1192     loadp CodeBlock::m_globalObject[t2], t2
1193     loadp JSGlobalObject::m_functionProtoHasInstanceSymbolFunction[t2], t2
1194     bqneq t0, t2, .opOverridesHasInstanceNotDefaultSymbol
1195 
1196     get(m_constructor, t1)
1197     loadConstantOrVariable(size, t1, t0)
1198     tbz JSCell::m_flags[t0], ImplementsDefaultHasInstance, t1
1199     orq ValueFalse, t1
1200     storeq t1, [cfr, t3, 8]
1201     dispatch()
1202 
1203 .opOverridesHasInstanceNotDefaultSymbol:
1204     storeq ValueTrue, [cfr, t3, 8]
1205     dispatch()
1206 end)
1207 
1208 
1209 llintOpWithReturn(op_is_empty, OpIsEmpty, macro (size, get, dispatch, return)
1210     get(m_operand, t1)
1211     loadConstantOrVariable(size, t1, t0)
1212     cqeq t0, ValueEmpty, t3
1213     orq ValueFalse, t3
1214     return(t3)
1215 end)
1216 
1217 
1218 llintOpWithReturn(op_is_undefined, OpIsUndefined, macro (size, get, dispatch, return)
1219     get(m_operand, t1)
1220     loadConstantOrVariable(size, t1, t0)
1221     btqz t0, tagMask, .opIsUndefinedCell
1222     cqeq t0, ValueUndefined, t3
1223     orq ValueFalse, t3
1224     return(t3)
1225 .opIsUndefinedCell:
1226     btbnz JSCell::m_flags[t0], MasqueradesAsUndefined, .masqueradesAsUndefined
1227     move ValueFalse, t1
1228     return(t1)
1229 .masqueradesAsUndefined:
1230     loadStructureWithScratch(t0, t3, t1, t2)
1231     loadp CodeBlock[cfr], t1
1232     loadp CodeBlock::m_globalObject[t1], t1
1233     cpeq Structure::m_globalObject[t3], t1, t0
1234     orq ValueFalse, t0
1235     return(t0)
1236 end)
1237 
1238 
1239 llintOpWithReturn(op_is_boolean, OpIsBoolean, macro (size, get, dispatch, return)
1240     get(m_operand, t1)
1241     loadConstantOrVariable(size, t1, t0)
1242     xorq ValueFalse, t0
1243     tqz t0, ~1, t0
1244     orq ValueFalse, t0
1245     return(t0)
1246 end)
1247 
1248 
1249 llintOpWithReturn(op_is_number, OpIsNumber, macro (size, get, dispatch, return)
1250     get(m_operand, t1)
1251     loadConstantOrVariable(size, t1, t0)
1252     tqnz t0, tagTypeNumber, t1
1253     orq ValueFalse, t1
1254     return(t1)
1255 end)
1256 
1257 
1258 llintOpWithReturn(op_is_cell_with_type, OpIsCellWithType, macro (size, get, dispatch, return)
1259     getu(size, OpIsCellWithType, m_type, t0)
1260     get(m_operand, t1)
1261     loadConstantOrVariable(size, t1, t3)
1262     btqnz t3, tagMask, .notCellCase
1263     cbeq JSCell::m_type[t3], t0, t1
1264     orq ValueFalse, t1
1265     return(t1)
1266 .notCellCase:
1267     return(ValueFalse)
1268 end)
1269 
1270 
1271 llintOpWithReturn(op_is_object, OpIsObject, macro (size, get, dispatch, return)
1272     get(m_operand, t1)
1273     loadConstantOrVariable(size, t1, t0)
1274     btqnz t0, tagMask, .opIsObjectNotCell
1275     cbaeq JSCell::m_type[t0], ObjectType, t1
1276     orq ValueFalse, t1
1277     return(t1)
1278 .opIsObjectNotCell:
1279     return(ValueFalse)
1280 end)
1281 
1282 
1283 macro loadPropertyAtVariableOffset(propertyOffsetAsInt, objectAndStorage, value)
1284     bilt propertyOffsetAsInt, firstOutOfLineOffset, .isInline
1285     loadp JSObject::m_butterfly[objectAndStorage], objectAndStorage
1286     negi propertyOffsetAsInt
1287     sxi2q propertyOffsetAsInt, propertyOffsetAsInt
1288     jmp .ready
1289 .isInline:
1290     addp sizeof JSObject - (firstOutOfLineOffset - 2) * 8, objectAndStorage
1291 .ready:
1292     loadq (firstOutOfLineOffset - 2) * 8[objectAndStorage, propertyOffsetAsInt, 8], value
1293 end
1294 
1295 
1296 macro storePropertyAtVariableOffset(propertyOffsetAsInt, objectAndStorage, value)
1297     bilt propertyOffsetAsInt, firstOutOfLineOffset, .isInline
1298     loadp JSObject::m_butterfly[objectAndStorage], objectAndStorage
1299     negi propertyOffsetAsInt
1300     sxi2q propertyOffsetAsInt, propertyOffsetAsInt
1301     jmp .ready
1302 .isInline:
1303     addp sizeof JSObject - (firstOutOfLineOffset - 2) * 8, objectAndStorage
1304 .ready:
1305     storeq value, (firstOutOfLineOffset - 2) * 8[objectAndStorage, propertyOffsetAsInt, 8]
1306 end
1307 
1308 
1309 llintOpWithMetadata(op_get_by_id_direct, OpGetByIdDirect, macro (size, get, dispatch, metadata, return)
1310     metadata(t2, t0)
1311     get(m_base, t0)
1312     loadConstantOrVariableCell(size, t0, t3, .opGetByIdDirectSlow)
1313     loadi JSCell::m_structureID[t3], t1
1314     loadi OpGetByIdDirect::Metadata::m_structureID[t2], t0
1315     bineq t0, t1, .opGetByIdDirectSlow
1316     loadi OpGetByIdDirect::Metadata::m_offset[t2], t1
1317     loadPropertyAtVariableOffset(t1, t3, t0)
1318     valueProfile(OpGetByIdDirect, t2, t0)
1319     return(t0)
1320 
1321 .opGetByIdDirectSlow:
1322     callSlowPath(_llint_slow_path_get_by_id_direct)
1323     dispatch()
1324 end)
1325 
1326 
1327 llintOpWithMetadata(op_get_by_id, OpGetById, macro (size, get, dispatch, metadata, return)
1328     metadata(t2, t1)
1329     loadb OpGetById::Metadata::m_modeMetadata.mode[t2], t1
1330     get(m_base, t0)
1331     loadConstantOrVariableCell(size, t0, t3, .opGetByIdSlow)
1332 
1333 .opGetByIdDefault:
1334     bbneq t1, constexpr GetByIdMode::Default, .opGetByIdProtoLoad
1335     loadi JSCell::m_structureID[t3], t1
1336     loadi OpGetById::Metadata::m_modeMetadata.defaultMode.structureID[t2], t0
1337     bineq t0, t1, .opGetByIdSlow
1338     loadis OpGetById::Metadata::m_modeMetadata.defaultMode.cachedOffset[t2], t1
1339     loadPropertyAtVariableOffset(t1, t3, t0)
1340     valueProfile(OpGetById, t2, t0)
1341     return(t0)
1342 
1343 .opGetByIdProtoLoad:
1344     bbneq t1, constexpr GetByIdMode::ProtoLoad, .opGetByIdArrayLength
1345     loadi JSCell::m_structureID[t3], t1
1346     loadi OpGetById::Metadata::m_modeMetadata.protoLoadMode.structureID[t2], t3
1347     bineq t3, t1, .opGetByIdSlow
1348     loadis OpGetById::Metadata::m_modeMetadata.protoLoadMode.cachedOffset[t2], t1
1349     loadp OpGetById::Metadata::m_modeMetadata.protoLoadMode.cachedSlot[t2], t3
1350     loadPropertyAtVariableOffset(t1, t3, t0)
1351     valueProfile(OpGetById, t2, t0)
1352     return(t0)
1353 
1354 .opGetByIdArrayLength:
1355     bbneq t1, constexpr GetByIdMode::ArrayLength, .opGetByIdUnset
1356     move t3, t0
1357     arrayProfile(OpGetById::Metadata::m_modeMetadata.arrayLengthMode.arrayProfile, t0, t2, t5)
1358     btiz t0, IsArray, .opGetByIdSlow
1359     btiz t0, IndexingShapeMask, .opGetByIdSlow
1360     loadCagedJSValue(JSObject::m_butterfly[t3], t0, t1)
1361     loadi -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0], t0
1362     bilt t0, 0, .opGetByIdSlow
1363     orq tagTypeNumber, t0
1364     valueProfile(OpGetById, t2, t0)
1365     return(t0)
1366 
1367 .opGetByIdUnset:
1368     loadi JSCell::m_structureID[t3], t1
1369     loadi OpGetById::Metadata::m_modeMetadata.unsetMode.structureID[t2], t0
1370     bineq t0, t1, .opGetByIdSlow
1371     valueProfile(OpGetById, t2, ValueUndefined)
1372     return(ValueUndefined)
1373 
1374 .opGetByIdSlow:
1375     callSlowPath(_llint_slow_path_get_by_id)
1376     dispatch()
1377 end)
1378 
1379 
1380 llintOpWithMetadata(op_put_by_id, OpPutById, macro (size, get, dispatch, metadata, return)
1381     get(m_base, t3)
1382     loadConstantOrVariableCell(size, t3, t0, .opPutByIdSlow)
1383     metadata(t5, t2)
1384     loadi OpPutById::Metadata::m_oldStructureID[t5], t2
1385     bineq t2, JSCell::m_structureID[t0], .opPutByIdSlow
1386 
1387     # At this point, we have:
1388     # t0 -&gt; object base
1389     # t2 -&gt; current structure ID
1390     # t5 -&gt; metadata
1391 
1392     loadi OpPutById::Metadata::m_newStructureID[t5], t1
1393     btiz t1, .opPutByIdNotTransition
1394 
1395     # This is the transition case. t1 holds the new structureID. t2 holds the old structure ID.
1396     # If we have a chain, we need to check it. t0 is the base. We may clobber t1 to use it as
1397     # scratch.
1398     loadp OpPutById::Metadata::m_structureChain[t5], t3
1399     btpz t3, .opPutByIdTransitionDirect
1400 
1401     structureIDToStructureWithScratch(t2, t1, t3)
1402 
1403     # reload the StructureChain since we used t3 as a scratch above
1404     loadp OpPutById::Metadata::m_structureChain[t5], t3
1405 
1406     loadp StructureChain::m_vector[t3], t3
1407     assert(macro (ok) btpnz t3, ok end)
1408 
1409     loadq Structure::m_prototype[t2], t2
1410     bqeq t2, ValueNull, .opPutByIdTransitionChainDone
1411 .opPutByIdTransitionChainLoop:
1412     # At this point, t2 contains a prototye, and [t3] contains the Structure* that we want that
1413     # prototype to have. We don&#39;t want to have to load the Structure* for t2. Instead, we load
1414     # the Structure* from [t3], and then we compare its id to the id in the header of t2.
1415     loadp [t3], t1
1416     loadi JSCell::m_structureID[t2], t2
1417     # Now, t1 has the Structure* and t2 has the StructureID that we want that Structure* to have.
1418     bineq t2, Structure::m_blob + StructureIDBlob::u.fields.structureID[t1], .opPutByIdSlow
1419     addp PtrSize, t3
1420     loadq Structure::m_prototype[t1], t2
1421     bqneq t2, ValueNull, .opPutByIdTransitionChainLoop
1422 
1423 .opPutByIdTransitionChainDone:
1424     # Reload the new structure, since we clobbered it above.
1425     loadi OpPutById::Metadata::m_newStructureID[t5], t1
1426 
1427 .opPutByIdTransitionDirect:
1428     storei t1, JSCell::m_structureID[t0]
1429     writeBarrierOnOperandWithReload(size, get, m_base, macro ()
1430         # Reload metadata into t5
1431         metadata(t5, t1)
1432         # Reload base into t0
1433         get(m_base, t1)
1434         loadConstantOrVariable(size, t1, t0)
1435     end)
1436 
1437 .opPutByIdNotTransition:
1438     # The only thing live right now is t0, which holds the base.
1439     get(m_value, t1)
1440     loadConstantOrVariable(size, t1, t2)
1441     loadi OpPutById::Metadata::m_offset[t5], t1
1442     storePropertyAtVariableOffset(t1, t0, t2)
1443     writeBarrierOnOperands(size, get, m_base, m_value)
1444     dispatch()
1445 
1446 .opPutByIdSlow:
1447     callSlowPath(_llint_slow_path_put_by_id)
1448     dispatch()
1449 end)
1450 
1451 
1452 llintOpWithMetadata(op_get_by_val, OpGetByVal, macro (size, get, dispatch, metadata, return)
1453     macro finishGetByVal(result, scratch)
1454         get(m_dst, scratch)
1455         storeq result, [cfr, scratch, 8]
1456         valueProfile(OpGetByVal, t5, result)
1457         dispatch()
1458     end
1459 
1460     macro finishIntGetByVal(result, scratch)
1461         orq tagTypeNumber, result
1462         finishGetByVal(result, scratch)
1463     end
1464 
1465     macro finishDoubleGetByVal(result, scratch1, scratch2)
1466         fd2q result, scratch1
1467         subq tagTypeNumber, scratch1
1468         finishGetByVal(scratch1, scratch2)
1469     end
1470 
1471     metadata(t5, t2)
1472 
1473     get(m_base, t2)
1474     loadConstantOrVariableCell(size, t2, t0, .opGetByValSlow)
1475 
1476     move t0, t2
1477     arrayProfile(OpGetByVal::Metadata::m_arrayProfile, t2, t5, t1)
1478 
1479     get(m_property, t3)
1480     loadConstantOrVariableInt32(size, t3, t1, .opGetByValSlow)
1481     sxi2q t1, t1
1482 
1483     loadCagedJSValue(JSObject::m_butterfly[t0], t3, tagTypeNumber)
1484     move TagTypeNumber, tagTypeNumber
1485 
1486     andi IndexingShapeMask, t2
1487     bieq t2, Int32Shape, .opGetByValIsContiguous
1488     bineq t2, ContiguousShape, .opGetByValNotContiguous
1489 
1490 .opGetByValIsContiguous:
1491     biaeq t1, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t3], .opGetByValSlow
1492     get(m_dst, t0)
1493     loadq [t3, t1, 8], t2
1494     btqz t2, .opGetByValSlow
1495     jmp .opGetByValDone
1496 
1497 .opGetByValNotContiguous:
1498     bineq t2, DoubleShape, .opGetByValNotDouble
1499     biaeq t1, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t3], .opGetByValSlow
1500     get(m_dst, t0)
1501     loadd [t3, t1, 8], ft0
1502     bdnequn ft0, ft0, .opGetByValSlow
1503     fd2q ft0, t2
1504     subq tagTypeNumber, t2
1505     jmp .opGetByValDone
1506     
1507 .opGetByValNotDouble:
1508     subi ArrayStorageShape, t2
1509     bia t2, SlowPutArrayStorageShape - ArrayStorageShape, .opGetByValNotIndexedStorage
1510     biaeq t1, -sizeof IndexingHeader + IndexingHeader::u.lengths.vectorLength[t3], .opGetByValSlow
1511     get(m_dst, t0)
1512     loadq ArrayStorage::m_vector[t3, t1, 8], t2
1513     btqz t2, .opGetByValSlow
1514 
1515 .opGetByValDone:
1516     storeq t2, [cfr, t0, 8]
1517     valueProfile(OpGetByVal, t5, t2)
1518     dispatch()
1519 
1520 .opGetByValNotIndexedStorage:
1521     # First lets check if we even have a typed array. This lets us do some boilerplate up front.
1522     loadb JSCell::m_type[t0], t2
1523     subi FirstTypedArrayType, t2
1524     biaeq t2, NumberOfTypedArrayTypesExcludingDataView, .opGetByValSlow
1525     
1526     # Sweet, now we know that we have a typed array. Do some basic things now.
1527 
1528     if ARM64E
1529         const length = t6
1530         const scratch = t7
1531         loadi JSArrayBufferView::m_length[t0], length
1532         biaeq t1, length, .opGetByValSlow
1533     else
1534         # length and scratch are intentionally undefined on this branch because they are not used on other platforms.
1535         biaeq t1, JSArrayBufferView::m_length[t0], .opGetByValSlow
1536     end
1537 
1538     loadp JSArrayBufferView::m_vector[t0], t3
1539     cagedPrimitive(t3, length, t0, scratch)
1540 
1541     # Now bisect through the various types:
1542     #    Int8ArrayType,
1543     #    Uint8ArrayType,
1544     #    Uint8ClampedArrayType,
1545     #    Int16ArrayType,
1546     #    Uint16ArrayType,
1547     #    Int32ArrayType,
1548     #    Uint32ArrayType,
1549     #    Float32ArrayType,
1550     #    Float64ArrayType,
1551 
1552     bia t2, Uint16ArrayType - FirstTypedArrayType, .opGetByValAboveUint16Array
1553 
1554     # We have one of Int8ArrayType .. Uint16ArrayType.
1555     bia t2, Uint8ClampedArrayType - FirstTypedArrayType, .opGetByValInt16ArrayOrUint16Array
1556 
1557     # We have one of Int8ArrayType ... Uint8ClampedArrayType
1558     bia t2, Int8ArrayType - FirstTypedArrayType, .opGetByValUint8ArrayOrUint8ClampedArray
1559 
1560     # We have Int8ArrayType.
1561     loadbsi [t3, t1], t0
1562     finishIntGetByVal(t0, t1)
1563 
1564 .opGetByValUint8ArrayOrUint8ClampedArray:
1565     bia t2, Uint8ArrayType - FirstTypedArrayType, .opGetByValUint8ClampedArray
1566 
1567     # We have Uint8ArrayType.
1568     loadb [t3, t1], t0
1569     finishIntGetByVal(t0, t1)
1570 
1571 .opGetByValUint8ClampedArray:
1572     # We have Uint8ClampedArrayType.
1573     loadb [t3, t1], t0
1574     finishIntGetByVal(t0, t1)
1575 
1576 .opGetByValInt16ArrayOrUint16Array:
1577     # We have either Int16ArrayType or Uint16ClampedArrayType.
1578     bia t2, Int16ArrayType - FirstTypedArrayType, .opGetByValUint16Array
1579 
1580     # We have Int16ArrayType.
1581     loadhsi [t3, t1, 2], t0
1582     finishIntGetByVal(t0, t1)
1583 
1584 .opGetByValUint16Array:
1585     # We have Uint16ArrayType.
1586     loadh [t3, t1, 2], t0
1587     finishIntGetByVal(t0, t1)
1588 
1589 .opGetByValAboveUint16Array:
1590     # We have one of Int32ArrayType .. Float64ArrayType.
1591     bia t2, Uint32ArrayType - FirstTypedArrayType, .opGetByValFloat32ArrayOrFloat64Array
1592 
1593     # We have either Int32ArrayType or Uint32ArrayType
1594     bia t2, Int32ArrayType - FirstTypedArrayType, .opGetByValUint32Array
1595 
1596     # We have Int32ArrayType.
1597     loadi [t3, t1, 4], t0
1598     finishIntGetByVal(t0, t1)
1599 
1600 .opGetByValUint32Array:
1601     # We have Uint32ArrayType.
1602     # This is the hardest part because of large unsigned values.
1603     loadi [t3, t1, 4], t0
1604     bilt t0, 0, .opGetByValSlow # This case is still awkward to implement in LLInt.
1605     finishIntGetByVal(t0, t1)
1606 
1607 .opGetByValFloat32ArrayOrFloat64Array:
1608     # We have one of Float32ArrayType or Float64ArrayType. Sadly, we cannot handle Float32Array
1609     # inline yet. That would require some offlineasm changes.
1610     bieq t2, Float32ArrayType - FirstTypedArrayType, .opGetByValSlow
1611 
1612     # We have Float64ArrayType.
1613     loadd [t3, t1, 8], ft0
1614     bdnequn ft0, ft0, .opGetByValSlow
1615     finishDoubleGetByVal(ft0, t0, t1)
1616 
1617 .opGetByValSlow:
1618     callSlowPath(_llint_slow_path_get_by_val)
1619     dispatch()
1620 end)
1621 
1622 
1623 macro putByValOp(opcodeName, opcodeStruct)
1624     llintOpWithMetadata(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, metadata, return)
1625         macro contiguousPutByVal(storeCallback)
1626             biaeq t3, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0], .outOfBounds
1627         .storeResult:
1628             get(m_value, t2)
1629             storeCallback(t2, t1, [t0, t3, 8])
1630             dispatch()
1631 
1632         .outOfBounds:
1633             biaeq t3, -sizeof IndexingHeader + IndexingHeader::u.lengths.vectorLength[t0], .opPutByValOutOfBounds
1634             storeb 1, %opcodeStruct%::Metadata::m_arrayProfile.m_mayStoreToHole[t5]
1635             addi 1, t3, t2
1636             storei t2, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0]
1637             jmp .storeResult
1638         end
1639 
1640         get(m_base, t0)
1641         loadConstantOrVariableCell(size, t0, t1, .opPutByValSlow)
1642         move t1, t2
1643         metadata(t5, t0)
1644         arrayProfile(%opcodeStruct%::Metadata::m_arrayProfile, t2, t5, t0)
1645         get(m_property, t0)
1646         loadConstantOrVariableInt32(size, t0, t3, .opPutByValSlow)
1647         sxi2q t3, t3
1648         loadCagedJSValue(JSObject::m_butterfly[t1], t0, tagTypeNumber)
1649         move TagTypeNumber, tagTypeNumber
1650         btinz t2, CopyOnWrite, .opPutByValSlow
1651         andi IndexingShapeMask, t2
1652         bineq t2, Int32Shape, .opPutByValNotInt32
1653         contiguousPutByVal(
1654             macro (operand, scratch, address)
1655                 loadConstantOrVariable(size, operand, scratch)
1656                 bqb scratch, tagTypeNumber, .opPutByValSlow
1657                 storeq scratch, address
1658                 writeBarrierOnOperands(size, get, m_base, m_value)
1659             end)
1660 
1661     .opPutByValNotInt32:
1662         bineq t2, DoubleShape, .opPutByValNotDouble
1663         contiguousPutByVal(
1664             macro (operand, scratch, address)
1665                 loadConstantOrVariable(size, operand, scratch)
1666                 bqb scratch, tagTypeNumber, .notInt
1667                 ci2d scratch, ft0
1668                 jmp .ready
1669             .notInt:
1670                 addq tagTypeNumber, scratch
1671                 fq2d scratch, ft0
1672                 bdnequn ft0, ft0, .opPutByValSlow
1673             .ready:
1674                 stored ft0, address
1675                 writeBarrierOnOperands(size, get, m_base, m_value)
1676             end)
1677 
1678     .opPutByValNotDouble:
1679         bineq t2, ContiguousShape, .opPutByValNotContiguous
1680         contiguousPutByVal(
1681             macro (operand, scratch, address)
1682                 loadConstantOrVariable(size, operand, scratch)
1683                 storeq scratch, address
1684                 writeBarrierOnOperands(size, get, m_base, m_value)
1685             end)
1686 
1687     .opPutByValNotContiguous:
1688         bineq t2, ArrayStorageShape, .opPutByValSlow
1689         biaeq t3, -sizeof IndexingHeader + IndexingHeader::u.lengths.vectorLength[t0], .opPutByValOutOfBounds
1690         btqz ArrayStorage::m_vector[t0, t3, 8], .opPutByValArrayStorageEmpty
1691     .opPutByValArrayStorageStoreResult:
1692         get(m_value, t2)
1693         loadConstantOrVariable(size, t2, t1)
1694         storeq t1, ArrayStorage::m_vector[t0, t3, 8]
1695         writeBarrierOnOperands(size, get, m_base, m_value)
1696         dispatch()
1697 
1698     .opPutByValArrayStorageEmpty:
1699         storeb 1, %opcodeStruct%::Metadata::m_arrayProfile.m_mayStoreToHole[t5]
1700         addi 1, ArrayStorage::m_numValuesInVector[t0]
1701         bib t3, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0], .opPutByValArrayStorageStoreResult
1702         addi 1, t3, t1
1703         storei t1, -sizeof IndexingHeader + IndexingHeader::u.lengths.publicLength[t0]
1704         jmp .opPutByValArrayStorageStoreResult
1705 
1706     .opPutByValOutOfBounds:
1707         storeb 1, %opcodeStruct%::Metadata::m_arrayProfile.m_outOfBounds[t5]
1708     .opPutByValSlow:
1709         callSlowPath(_llint_slow_path_%opcodeName%)
1710         dispatch()
1711     end)
1712 end
1713 
1714 putByValOp(put_by_val, OpPutByVal)
1715 
1716 putByValOp(put_by_val_direct, OpPutByValDirect)
1717 
1718 
1719 macro llintJumpTrueOrFalseOp(opcodeName, opcodeStruct, conditionOp)
1720     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
1721         get(m_condition, t1)
1722         loadConstantOrVariable(size, t1, t0)
1723         btqnz t0, ~0xf, .slow
1724         conditionOp(t0, .target)
1725         dispatch()
1726 
1727     .target:
1728         jump(m_targetLabel)
1729 
1730     .slow:
1731         callSlowPath(_llint_slow_path_%opcodeName%)
1732         nextInstruction()
1733     end)
1734 end
1735 
1736 
1737 macro equalNullJumpOp(opcodeName, opcodeStruct, cellHandler, immediateHandler)
1738     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
1739         get(m_value, t0)
1740         assertNotConstant(size, t0)
1741         loadq [cfr, t0, 8], t0
1742         btqnz t0, tagMask, .immediate
1743         loadStructureWithScratch(t0, t2, t1, t3)
1744         cellHandler(t2, JSCell::m_flags[t0], .target)
1745         dispatch()
1746 
1747     .target:
1748         jump(m_targetLabel)
1749 
1750     .immediate:
1751         andq ~TagBitUndefined, t0
1752         immediateHandler(t0, .target)
1753         dispatch()
1754     end)
1755 end
1756 
1757 equalNullJumpOp(jeq_null, OpJeqNull,
1758     macro (structure, value, target) 
1759         btbz value, MasqueradesAsUndefined, .notMasqueradesAsUndefined
1760         loadp CodeBlock[cfr], t0
1761         loadp CodeBlock::m_globalObject[t0], t0
1762         bpeq Structure::m_globalObject[structure], t0, target
1763 .notMasqueradesAsUndefined:
1764     end,
1765     macro (value, target) bqeq value, ValueNull, target end)
1766 
1767 
1768 equalNullJumpOp(jneq_null, OpJneqNull,
1769     macro (structure, value, target) 
1770         btbz value, MasqueradesAsUndefined, target
1771         loadp CodeBlock[cfr], t0
1772         loadp CodeBlock::m_globalObject[t0], t0
1773         bpneq Structure::m_globalObject[structure], t0, target
1774     end,
1775     macro (value, target) bqneq value, ValueNull, target end)
1776 
1777 macro undefinedOrNullJumpOp(opcodeName, opcodeStruct, fn)
1778     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
1779         get(m_value, t1)
1780         loadConstantOrVariable(size, t1, t0)
1781         andq ~TagBitUndefined, t0
1782         fn(t0, .target)
1783         dispatch()
1784 
1785     .target:
1786         jump(m_targetLabel)
1787     end)
1788 end
1789 
1790 undefinedOrNullJumpOp(jundefined_or_null, OpJundefinedOrNull,
1791     macro (value, target) bqeq value, ValueNull, target end)
1792 
1793 undefinedOrNullJumpOp(jnundefined_or_null, OpJnundefinedOrNull,
1794     macro (value, target) bqneq value, ValueNull, target end)
1795 
1796 llintOpWithMetadata(op_jneq_ptr, OpJneqPtr, macro (size, get, dispatch, metadata, return)
1797     get(m_value, t0)
1798     getu(size, OpJneqPtr, m_specialPointer, t1)
1799     loadp CodeBlock[cfr], t2
1800     loadp CodeBlock::m_globalObject[t2], t2
1801     loadp JSGlobalObject::m_specialPointers[t2, t1, PtrSize], t1
1802     bpneq t1, [cfr, t0, 8], .opJneqPtrTarget
1803     dispatch()
1804 
1805 .opJneqPtrTarget:
1806     metadata(t5, t0)
1807     storeb 1, OpJneqPtr::Metadata::m_hasJumped[t5]
1808     get(m_targetLabel, t0)
1809     jumpImpl(t0)
1810 end)
1811 
1812 
1813 macro compareJumpOp(opcodeName, opcodeStruct, integerCompare, doubleCompare)
1814     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
1815         get(m_lhs, t2)
1816         get(m_rhs, t3)
1817         loadConstantOrVariable(size, t2, t0)
1818         loadConstantOrVariable(size, t3, t1)
1819         bqb t0, tagTypeNumber, .op1NotInt
1820         bqb t1, tagTypeNumber, .op2NotInt
1821         integerCompare(t0, t1, .jumpTarget)
1822         dispatch()
1823 
1824     .op1NotInt:
1825         btqz t0, tagTypeNumber, .slow
1826         bqb t1, tagTypeNumber, .op1NotIntOp2NotInt
1827         ci2d t1, ft1
1828         jmp .op1NotIntReady
1829     .op1NotIntOp2NotInt:
1830         btqz t1, tagTypeNumber, .slow
1831         addq tagTypeNumber, t1
1832         fq2d t1, ft1
1833     .op1NotIntReady:
1834         addq tagTypeNumber, t0
1835         fq2d t0, ft0
1836         doubleCompare(ft0, ft1, .jumpTarget)
1837         dispatch()
1838 
1839     .op2NotInt:
1840         ci2d t0, ft0
1841         btqz t1, tagTypeNumber, .slow
1842         addq tagTypeNumber, t1
1843         fq2d t1, ft1
1844         doubleCompare(ft0, ft1, .jumpTarget)
1845         dispatch()
1846 
1847     .jumpTarget:
1848         jump(m_targetLabel)
1849 
1850     .slow:
1851         callSlowPath(_llint_slow_path_%opcodeName%)
1852         nextInstruction()
1853     end)
1854 end
1855 
1856 
1857 macro equalityJumpOp(opcodeName, opcodeStruct, integerComparison)
1858     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
1859         get(m_lhs, t2)
1860         get(m_rhs, t3)
1861         loadConstantOrVariableInt32(size, t2, t0, .slow)
1862         loadConstantOrVariableInt32(size, t3, t1, .slow)
1863         integerComparison(t0, t1, .jumpTarget)
1864         dispatch()
1865 
1866     .jumpTarget:
1867         jump(m_targetLabel)
1868 
1869     .slow:
1870         callSlowPath(_llint_slow_path_%opcodeName%)
1871         nextInstruction()
1872     end)
1873 end
1874 
1875 
1876 macro compareUnsignedJumpOp(opcodeName, opcodeStruct, integerCompareMacro)
1877     llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
1878         get(m_lhs, t2)
1879         get(m_rhs, t3)
1880         loadConstantOrVariable(size, t2, t0)
1881         loadConstantOrVariable(size, t3, t1)
1882         integerCompareMacro(t0, t1, .jumpTarget)
1883         dispatch()
1884 
1885     .jumpTarget:
1886         jump(m_targetLabel)
1887     end)
1888 end
1889 
1890 
1891 macro compareUnsignedOp(opcodeName, opcodeStruct, integerCompareAndSet)
1892     llintOpWithReturn(op_%opcodeName%, opcodeStruct, macro (size, get, dispatch, return)
1893         get(m_lhs, t2)
1894         get(m_rhs,  t0)
1895         loadConstantOrVariable(size, t0, t1)
1896         loadConstantOrVariable(size, t2, t0)
1897         integerCompareAndSet(t0, t1, t0)
1898         orq ValueFalse, t0
1899         return(t0)
1900     end)
1901 end
1902 
1903 
1904 llintOpWithJump(op_switch_imm, OpSwitchImm, macro (size, get, jump, dispatch)
1905     get(m_scrutinee, t2)
1906     getu(size, OpSwitchImm, m_tableIndex, t3)
1907     loadConstantOrVariable(size, t2, t1)
1908     loadp CodeBlock[cfr], t2
1909     loadp CodeBlock::m_rareData[t2], t2
1910     muli sizeof SimpleJumpTable, t3
1911     loadp CodeBlock::RareData::m_switchJumpTables + VectorBufferOffset[t2], t2
1912     addp t3, t2
1913     bqb t1, tagTypeNumber, .opSwitchImmNotInt
1914     subi SimpleJumpTable::min[t2], t1
1915     biaeq t1, SimpleJumpTable::branchOffsets + VectorSizeOffset[t2], .opSwitchImmFallThrough
1916     loadp SimpleJumpTable::branchOffsets + VectorBufferOffset[t2], t3
1917     loadis [t3, t1, 4], t1
1918     btiz t1, .opSwitchImmFallThrough
1919     dispatchIndirect(t1)
1920 
1921 .opSwitchImmNotInt:
1922     btqnz t1, tagTypeNumber, .opSwitchImmSlow   # Go slow if it&#39;s a double.
1923 .opSwitchImmFallThrough:
1924     jump(m_defaultOffset)
1925 
1926 .opSwitchImmSlow:
1927     callSlowPath(_llint_slow_path_switch_imm)
1928     nextInstruction()
1929 end)
1930 
1931 
1932 llintOpWithJump(op_switch_char, OpSwitchChar, macro (size, get, jump, dispatch)
1933     get(m_scrutinee, t2)
1934     getu(size, OpSwitchChar, m_tableIndex, t3)
1935     loadConstantOrVariable(size, t2, t1)
1936     loadp CodeBlock[cfr], t2
1937     loadp CodeBlock::m_rareData[t2], t2
1938     muli sizeof SimpleJumpTable, t3
1939     loadp CodeBlock::RareData::m_switchJumpTables + VectorBufferOffset[t2], t2
1940     addp t3, t2
1941     btqnz t1, tagMask, .opSwitchCharFallThrough
1942     bbneq JSCell::m_type[t1], StringType, .opSwitchCharFallThrough
1943     loadp JSString::m_fiber[t1], t0
1944     btpnz t0, isRopeInPointer, .opSwitchOnRope
1945     bineq StringImpl::m_length[t0], 1, .opSwitchCharFallThrough
1946     loadp StringImpl::m_data8[t0], t1
1947     btinz StringImpl::m_hashAndFlags[t0], HashFlags8BitBuffer, .opSwitchChar8Bit
1948     loadh [t1], t0
1949     jmp .opSwitchCharReady
1950 .opSwitchChar8Bit:
1951     loadb [t1], t0
1952 .opSwitchCharReady:
1953     subi SimpleJumpTable::min[t2], t0
1954     biaeq t0, SimpleJumpTable::branchOffsets + VectorSizeOffset[t2], .opSwitchCharFallThrough
1955     loadp SimpleJumpTable::branchOffsets + VectorBufferOffset[t2], t2
1956     loadis [t2, t0, 4], t1
1957     btiz t1, .opSwitchCharFallThrough
1958     dispatchIndirect(t1)
1959 
1960 .opSwitchCharFallThrough:
1961     jump(m_defaultOffset)
1962 
1963 .opSwitchOnRope:
1964     bineq JSRopeString::m_compactFibers + JSRopeString::CompactFibers::m_length[t1], 1, .opSwitchCharFallThrough
1965 
1966 .opSwitchOnRopeChar:
1967     callSlowPath(_llint_slow_path_switch_char)
1968     nextInstruction()
1969 end)
1970 
1971 
1972 # we assume t5 contains the metadata, and we should not scratch that
1973 macro arrayProfileForCall(opcodeStruct, getu)
1974     getu(m_argv, t3)
1975     negp t3
1976     loadq ThisArgumentOffset[cfr, t3, 8], t0
1977     btqnz t0, tagMask, .done
1978     loadi JSCell::m_structureID[t0], t3
1979     storei t3, %opcodeStruct%::Metadata::m_callLinkInfo.m_arrayProfile.m_lastSeenStructureID[t5]
1980 .done:
1981 end
1982 
1983 macro commonCallOp(opcodeName, slowPath, opcodeStruct, prepareCall, prologue)
1984     llintOpWithMetadata(opcodeName, opcodeStruct, macro (size, get, dispatch, metadata, return)
1985         metadata(t5, t0)
1986 
1987         prologue(macro (fieldName, dst)
1988             getu(size, opcodeStruct, fieldName, dst)
1989         end, metadata)
1990 
1991         get(m_callee, t0)
1992         loadp %opcodeStruct%::Metadata::m_callLinkInfo.m_calleeOrLastSeenCalleeWithLinkBit[t5], t2
1993         loadConstantOrVariable(size, t0, t3)
1994         bqneq t3, t2, .opCallSlow
1995         getu(size, opcodeStruct, m_argv, t3)
1996         lshifti 3, t3
1997         negp t3
1998         addp cfr, t3
1999         storeq t2, Callee[t3]
2000         getu(size, opcodeStruct, m_argc, t2)
2001         storei PC, ArgumentCount + TagOffset[cfr]
2002         storei t2, ArgumentCount + PayloadOffset[t3]
2003         move t3, sp
2004         prepareCall(%opcodeStruct%::Metadata::m_callLinkInfo.m_machineCodeTarget[t5], t2, t3, t4, JSEntryPtrTag)
2005         callTargetFunction(size, opcodeStruct, dispatch, %opcodeStruct%::Metadata::m_callLinkInfo.m_machineCodeTarget[t5], JSEntryPtrTag)
2006 
2007     .opCallSlow:
2008         slowPathForCall(size, opcodeStruct, dispatch, slowPath, prepareCall)
2009     end)
2010 end
2011 
2012 llintOp(op_ret, OpRet, macro (size, get, dispatch)
2013     checkSwitchToJITForEpilogue()
2014     get(m_value, t2)
2015     loadConstantOrVariable(size, t2, r0)
2016     doReturn()
2017 end)
2018 
2019 
2020 llintOpWithReturn(op_to_primitive, OpToPrimitive, macro (size, get, dispatch, return)
2021     get(m_src, t2)
2022     loadConstantOrVariable(size, t2, t0)
2023     btqnz t0, tagMask, .opToPrimitiveIsImm
2024     bbaeq JSCell::m_type[t0], ObjectType, .opToPrimitiveSlowCase
2025 .opToPrimitiveIsImm:
2026     return(t0)
2027 
2028 .opToPrimitiveSlowCase:
2029     callSlowPath(_slow_path_to_primitive)
2030     dispatch()
2031 end)
2032 
2033 
2034 commonOp(llint_op_catch, macro() end, macro (size)
2035     # This is where we end up from the JIT&#39;s throw trampoline (because the
2036     # machine code return address will be set to _llint_op_catch), and from
2037     # the interpreter&#39;s throw trampoline (see _llint_throw_trampoline).
2038     # The throwing code must have known that we were throwing to the interpreter,
2039     # and have set VM::targetInterpreterPCForThrow.
2040     loadp Callee[cfr], t3
2041     andp MarkedBlockMask, t3
2042     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
2043     restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(t3, t0)
2044     loadp VM::callFrameForCatch[t3], cfr
2045     storep 0, VM::callFrameForCatch[t3]
2046     restoreStackPointerAfterCall()
2047 
2048     loadp CodeBlock[cfr], PB
2049     loadp CodeBlock::m_metadata[PB], metadataTable
2050     loadp CodeBlock::m_instructionsRawPointer[PB], PB
2051     loadp VM::targetInterpreterPCForThrow[t3], PC
2052     subp PB, PC
2053 
2054     callSlowPath(_llint_slow_path_check_if_exception_is_uncatchable_and_notify_profiler)
2055     bpeq r1, 0, .isCatchableException
2056     jmp _llint_throw_from_slow_path_trampoline
2057 
2058 .isCatchableException:
2059     loadp Callee[cfr], t3
2060     andp MarkedBlockMask, t3
2061     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
2062 
2063     loadp VM::m_exception[t3], t0
2064     storep 0, VM::m_exception[t3]
2065     get(size, OpCatch, m_exception, t2)
2066     storeq t0, [cfr, t2, 8]
2067 
2068     loadq Exception::m_value[t0], t3
2069     get(size, OpCatch, m_thrownValue, t2)
2070     storeq t3, [cfr, t2, 8]
2071 
2072     traceExecution()
2073 
2074     callSlowPath(_llint_slow_path_profile_catch)
2075 
2076     dispatchOp(size, op_catch)
2077 end)
2078 
2079 
2080 llintOp(op_end, OpEnd, macro (size, get, dispatch)
2081     checkSwitchToJITForEpilogue()
2082     get(m_value, t0)
2083     assertNotConstant(size, t0)
2084     loadq [cfr, t0, 8], r0
2085     doReturn()
2086 end)
2087 
2088 
2089 op(llint_throw_from_slow_path_trampoline, macro ()
2090     loadp Callee[cfr], t1
2091     andp MarkedBlockMask, t1
2092     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
2093     copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(t1, t2)
2094 
2095     callSlowPath(_llint_slow_path_handle_exception)
2096 
2097     # When throwing from the interpreter (i.e. throwing from LLIntSlowPaths), so
2098     # the throw target is not necessarily interpreted code, we come to here.
2099     # This essentially emulates the JIT&#39;s throwing protocol.
2100     loadp Callee[cfr], t1
2101     andp MarkedBlockMask, t1
2102     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
2103     jmp VM::targetMachinePCForThrow[t1], ExceptionHandlerPtrTag
2104 end)
2105 
2106 
2107 op(llint_throw_during_call_trampoline, macro ()
2108     preserveReturnAddressAfterCall(t2)
2109     jmp _llint_throw_from_slow_path_trampoline
2110 end)
2111 
2112 
2113 macro nativeCallTrampoline(executableOffsetToFunction)
2114 
2115     functionPrologue()
2116     storep 0, CodeBlock[cfr]
2117     loadp Callee[cfr], t0
2118     andp MarkedBlockMask, t0, t1
2119     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
2120     storep cfr, VM::topCallFrame[t1]
2121     if ARM64 or ARM64E or C_LOOP or C_LOOP_WIN
2122         storep lr, ReturnPC[cfr]
2123     end
2124     move cfr, a0
2125     loadp Callee[cfr], t1
2126     loadp JSFunction::m_executable[t1], t1
2127     checkStackPointerAlignment(t3, 0xdead0001)
2128     if C_LOOP or C_LOOP_WIN
2129         cloopCallNative executableOffsetToFunction[t1]
2130     else
2131         if X86_64_WIN
2132             subp 32, sp
2133             call executableOffsetToFunction[t1], JSEntryPtrTag
2134             addp 32, sp
2135         else
2136             call executableOffsetToFunction[t1], JSEntryPtrTag
2137         end
2138     end
2139 
2140     loadp Callee[cfr], t3
2141     andp MarkedBlockMask, t3
2142     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
2143 
2144     btpnz VM::m_exception[t3], .handleException
2145 
2146     functionEpilogue()
2147     ret
2148 
2149 .handleException:
2150     storep cfr, VM::topCallFrame[t3]
2151     jmp _llint_throw_from_slow_path_trampoline
2152 end
2153 
2154 macro internalFunctionCallTrampoline(offsetOfFunction)
2155     functionPrologue()
2156     storep 0, CodeBlock[cfr]
2157     loadp Callee[cfr], t0
2158     andp MarkedBlockMask, t0, t1
2159     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
2160     storep cfr, VM::topCallFrame[t1]
2161     if ARM64 or ARM64E or C_LOOP or C_LOOP_WIN
2162         storep lr, ReturnPC[cfr]
2163     end
2164     move cfr, a0
2165     loadp Callee[cfr], t1
2166     checkStackPointerAlignment(t3, 0xdead0001)
2167     if C_LOOP or C_LOOP_WIN
2168         cloopCallNative offsetOfFunction[t1]
2169     else
2170         if X86_64_WIN
2171             subp 32, sp
2172             call offsetOfFunction[t1], JSEntryPtrTag
2173             addp 32, sp
2174         else
2175             call offsetOfFunction[t1], JSEntryPtrTag
2176         end
2177     end
2178 
2179     loadp Callee[cfr], t3
2180     andp MarkedBlockMask, t3
2181     loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
2182 
2183     btpnz VM::m_exception[t3], .handleException
2184 
2185     functionEpilogue()
2186     ret
2187 
2188 .handleException:
2189     storep cfr, VM::topCallFrame[t3]
2190     jmp _llint_throw_from_slow_path_trampoline
2191 end
2192 
2193 macro varInjectionCheck(slowPath, scratch)
2194     loadp CodeBlock[cfr], scratch
2195     loadp CodeBlock::m_globalObject[scratch], scratch
2196     loadp JSGlobalObject::m_varInjectionWatchpoint[scratch], scratch
2197     bbeq WatchpointSet::m_state[scratch], IsInvalidated, slowPath
2198 end
2199 
2200 llintOpWithMetadata(op_resolve_scope, OpResolveScope, macro (size, get, dispatch, metadata, return)
2201     metadata(t5, t0)
2202 
2203     macro getConstantScope(dst)
2204         loadp OpResolveScope::Metadata::m_constantScope[t5], dst
2205     end
2206 
2207     macro returnConstantScope()
2208         getConstantScope(t0)
2209         return(t0)
2210     end
2211 
2212     macro globalLexicalBindingEpochCheck(slowPath, globalObject, scratch)
2213         loadi OpResolveScope::Metadata::m_globalLexicalBindingEpoch[t5], scratch
2214         bineq JSGlobalObject::m_globalLexicalBindingEpoch[globalObject], scratch, slowPath
2215     end
2216 
2217     macro resolveScope()
2218         loadi OpResolveScope::Metadata::m_localScopeDepth[t5], t2
2219         get(m_scope, t0)
2220         loadq [cfr, t0, 8], t0
2221         btiz t2, .resolveScopeLoopEnd
2222 
2223     .resolveScopeLoop:
2224         loadp JSScope::m_next[t0], t0
2225         subi 1, t2
2226         btinz t2, .resolveScopeLoop
2227 
2228     .resolveScopeLoopEnd:
2229         return(t0)
2230     end
2231 
2232     loadi OpResolveScope::Metadata::m_resolveType[t5], t0
2233 
2234 #rGlobalProperty:
2235     bineq t0, GlobalProperty, .rGlobalVar
2236     getConstantScope(t0)
2237     globalLexicalBindingEpochCheck(.rDynamic, t0, t2)
2238     return(t0)
2239 
2240 .rGlobalVar:
2241     bineq t0, GlobalVar, .rGlobalLexicalVar
2242     returnConstantScope()
2243 
2244 .rGlobalLexicalVar:
2245     bineq t0, GlobalLexicalVar, .rClosureVar
2246     returnConstantScope()
2247 
2248 .rClosureVar:
2249     bineq t0, ClosureVar, .rModuleVar
2250     resolveScope()
2251 
2252 .rModuleVar:
2253     bineq t0, ModuleVar, .rGlobalPropertyWithVarInjectionChecks
2254     returnConstantScope()
2255 
2256 .rGlobalPropertyWithVarInjectionChecks:
2257     bineq t0, GlobalPropertyWithVarInjectionChecks, .rGlobalVarWithVarInjectionChecks
2258     varInjectionCheck(.rDynamic, t2)
2259     getConstantScope(t0)
2260     globalLexicalBindingEpochCheck(.rDynamic, t0, t2)
2261     return(t0)
2262 
2263 .rGlobalVarWithVarInjectionChecks:
2264     bineq t0, GlobalVarWithVarInjectionChecks, .rGlobalLexicalVarWithVarInjectionChecks
2265     varInjectionCheck(.rDynamic, t2)
2266     returnConstantScope()
2267 
2268 .rGlobalLexicalVarWithVarInjectionChecks:
2269     bineq t0, GlobalLexicalVarWithVarInjectionChecks, .rClosureVarWithVarInjectionChecks
2270     varInjectionCheck(.rDynamic, t2)
2271     returnConstantScope()
2272 
2273 .rClosureVarWithVarInjectionChecks:
2274     bineq t0, ClosureVarWithVarInjectionChecks, .rDynamic
2275     varInjectionCheck(.rDynamic, t2)
2276     resolveScope()
2277 
2278 .rDynamic:
2279     callSlowPath(_slow_path_resolve_scope)
2280     dispatch()
2281 end)
2282 
2283 
2284 macro loadWithStructureCheck(opcodeStruct, get, slowPath)
2285     get(m_scope, t0)
2286     loadq [cfr, t0, 8], t0
2287     loadStructureWithScratch(t0, t2, t1, t3)
2288     loadp %opcodeStruct%::Metadata::m_structure[t5], t1
2289     bpneq t2, t1, slowPath
2290 end
2291 
2292 llintOpWithMetadata(op_get_from_scope, OpGetFromScope, macro (size, get, dispatch, metadata, return)
2293     metadata(t5, t0)
2294 
2295     macro getProperty()
2296         loadp OpGetFromScope::Metadata::m_operand[t5], t1
2297         loadPropertyAtVariableOffset(t1, t0, t2)
2298         valueProfile(OpGetFromScope, t5, t2)
2299         return(t2)
2300     end
2301 
2302     macro getGlobalVar(tdzCheckIfNecessary)
2303         loadp OpGetFromScope::Metadata::m_operand[t5], t0
2304         loadq [t0], t0
2305         tdzCheckIfNecessary(t0)
2306         valueProfile(OpGetFromScope, t5, t0)
2307         return(t0)
2308     end
2309 
2310     macro getClosureVar()
2311         loadp OpGetFromScope::Metadata::m_operand[t5], t1
2312         loadq JSLexicalEnvironment_variables[t0, t1, 8], t0
2313         valueProfile(OpGetFromScope, t5, t0)
2314         return(t0)
2315     end
2316 
2317     loadi OpGetFromScope::Metadata::m_getPutInfo + GetPutInfo::m_operand[t5], t0
2318     andi ResolveTypeMask, t0
2319 
2320 #gGlobalProperty:
2321     bineq t0, GlobalProperty, .gGlobalVar
2322     loadWithStructureCheck(OpGetFromScope, get, .gDynamic) # This structure check includes lexical binding epoch check since when the epoch is changed, scope will be changed too.
2323     getProperty()
2324 
2325 .gGlobalVar:
2326     bineq t0, GlobalVar, .gGlobalLexicalVar
2327     getGlobalVar(macro(v) end)
2328 
2329 .gGlobalLexicalVar:
2330     bineq t0, GlobalLexicalVar, .gClosureVar
2331     getGlobalVar(
2332         macro (value)
2333             bqeq value, ValueEmpty, .gDynamic
2334         end)
2335 
2336 .gClosureVar:
2337     bineq t0, ClosureVar, .gGlobalPropertyWithVarInjectionChecks
2338     loadVariable(get, m_scope, t0)
2339     getClosureVar()
2340 
2341 .gGlobalPropertyWithVarInjectionChecks:
2342     bineq t0, GlobalPropertyWithVarInjectionChecks, .gGlobalVarWithVarInjectionChecks
2343     loadWithStructureCheck(OpGetFromScope, get, .gDynamic) # This structure check includes lexical binding epoch check since when the epoch is changed, scope will be changed too.
2344     getProperty()
2345 
2346 .gGlobalVarWithVarInjectionChecks:
2347     bineq t0, GlobalVarWithVarInjectionChecks, .gGlobalLexicalVarWithVarInjectionChecks
2348     varInjectionCheck(.gDynamic, t2)
2349     getGlobalVar(macro(v) end)
2350 
2351 .gGlobalLexicalVarWithVarInjectionChecks:
2352     bineq t0, GlobalLexicalVarWithVarInjectionChecks, .gClosureVarWithVarInjectionChecks
2353     varInjectionCheck(.gDynamic, t2)
2354     getGlobalVar(
2355         macro (value)
2356             bqeq value, ValueEmpty, .gDynamic
2357         end)
2358 
2359 .gClosureVarWithVarInjectionChecks:
2360     bineq t0, ClosureVarWithVarInjectionChecks, .gDynamic
2361     varInjectionCheck(.gDynamic, t2)
2362     loadVariable(get, m_scope, t0)
2363     getClosureVar()
2364 
2365 .gDynamic:
2366     callSlowPath(_llint_slow_path_get_from_scope)
2367     dispatch()
2368 end)
2369 
2370 
2371 llintOpWithMetadata(op_put_to_scope, OpPutToScope, macro (size, get, dispatch, metadata, return)
2372     macro putProperty()
2373         get(m_value, t1)
2374         loadConstantOrVariable(size, t1, t2)
2375         loadp OpPutToScope::Metadata::m_operand[t5], t1
2376         storePropertyAtVariableOffset(t1, t0, t2)
2377     end
2378 
2379     macro putGlobalVariable()
2380         get(m_value, t0)
2381         loadConstantOrVariable(size, t0, t1)
2382         loadp OpPutToScope::Metadata::m_watchpointSet[t5], t2
2383         btpz t2, .noVariableWatchpointSet
2384         notifyWrite(t2, .pDynamic)
2385     .noVariableWatchpointSet:
2386         loadp OpPutToScope::Metadata::m_operand[t5], t0
2387         storeq t1, [t0]
2388     end
2389 
2390     macro putClosureVar()
2391         get(m_value, t1)
2392         loadConstantOrVariable(size, t1, t2)
2393         loadp OpPutToScope::Metadata::m_operand[t5], t1
2394         storeq t2, JSLexicalEnvironment_variables[t0, t1, 8]
2395     end
2396 
2397     macro putLocalClosureVar()
2398         get(m_value, t1)
2399         loadConstantOrVariable(size, t1, t2)
2400         loadp OpPutToScope::Metadata::m_watchpointSet[t5], t3
2401         btpz t3, .noVariableWatchpointSet
2402         notifyWrite(t3, .pDynamic)
2403     .noVariableWatchpointSet:
2404         loadp OpPutToScope::Metadata::m_operand[t5], t1
2405         storeq t2, JSLexicalEnvironment_variables[t0, t1, 8]
2406     end
2407 
2408     macro checkTDZInGlobalPutToScopeIfNecessary()
2409         loadi OpPutToScope::Metadata::m_getPutInfo + GetPutInfo::m_operand[t5], t0
2410         andi InitializationModeMask, t0
2411         rshifti InitializationModeShift, t0
2412         bineq t0, NotInitialization, .noNeedForTDZCheck
2413         loadp OpPutToScope::Metadata::m_operand[t5], t0
2414         loadq [t0], t0
2415         bqeq t0, ValueEmpty, .pDynamic
2416     .noNeedForTDZCheck:
2417     end
2418 
2419     metadata(t5, t0)
2420     loadi OpPutToScope::Metadata::m_getPutInfo + GetPutInfo::m_operand[t5], t0
2421     andi ResolveTypeMask, t0
2422 
2423 #pLocalClosureVar:
2424     bineq t0, LocalClosureVar, .pGlobalProperty
2425     loadVariable(get, m_scope, t0)
2426     putLocalClosureVar()
2427     writeBarrierOnOperands(size, get, m_scope, m_value)
2428     dispatch()
2429 
2430 .pGlobalProperty:
2431     bineq t0, GlobalProperty, .pGlobalVar
2432     loadWithStructureCheck(OpPutToScope, get, .pDynamic) # This structure check includes lexical binding epoch check since when the epoch is changed, scope will be changed too.
2433     putProperty()
2434     writeBarrierOnOperands(size, get, m_scope, m_value)
2435     dispatch()
2436 
2437 .pGlobalVar:
2438     bineq t0, GlobalVar, .pGlobalLexicalVar
2439     putGlobalVariable()
2440     writeBarrierOnGlobalObject(size, get, m_value)
2441     dispatch()
2442 
2443 .pGlobalLexicalVar:
2444     bineq t0, GlobalLexicalVar, .pClosureVar
2445     checkTDZInGlobalPutToScopeIfNecessary()
2446     putGlobalVariable()
2447     writeBarrierOnGlobalLexicalEnvironment(size, get, m_value)
2448     dispatch()
2449 
2450 .pClosureVar:
2451     bineq t0, ClosureVar, .pGlobalPropertyWithVarInjectionChecks
2452     loadVariable(get, m_scope, t0)
2453     putClosureVar()
2454     writeBarrierOnOperands(size, get, m_scope, m_value)
2455     dispatch()
2456 
2457 .pGlobalPropertyWithVarInjectionChecks:
2458     bineq t0, GlobalPropertyWithVarInjectionChecks, .pGlobalVarWithVarInjectionChecks
2459     loadWithStructureCheck(OpPutToScope, get, .pDynamic) # This structure check includes lexical binding epoch check since when the epoch is changed, scope will be changed too.
2460     putProperty()
2461     writeBarrierOnOperands(size, get, m_scope, m_value)
2462     dispatch()
2463 
2464 .pGlobalVarWithVarInjectionChecks:
2465     bineq t0, GlobalVarWithVarInjectionChecks, .pGlobalLexicalVarWithVarInjectionChecks
2466     varInjectionCheck(.pDynamic, t2)
2467     putGlobalVariable()
2468     writeBarrierOnGlobalObject(size, get, m_value)
2469     dispatch()
2470 
2471 .pGlobalLexicalVarWithVarInjectionChecks:
2472     bineq t0, GlobalLexicalVarWithVarInjectionChecks, .pClosureVarWithVarInjectionChecks
2473     varInjectionCheck(.pDynamic, t2)
2474     checkTDZInGlobalPutToScopeIfNecessary()
2475     putGlobalVariable()
2476     writeBarrierOnGlobalLexicalEnvironment(size, get, m_value)
2477     dispatch()
2478 
2479 .pClosureVarWithVarInjectionChecks:
2480     bineq t0, ClosureVarWithVarInjectionChecks, .pModuleVar
2481     varInjectionCheck(.pDynamic, t2)
2482     loadVariable(get, m_scope, t0)
2483     putClosureVar()
2484     writeBarrierOnOperands(size, get, m_scope, m_value)
2485     dispatch()
2486 
2487 .pModuleVar:
2488     bineq t0, ModuleVar, .pDynamic
2489     callSlowPath(_slow_path_throw_strict_mode_readonly_property_write_error)
2490     dispatch()
2491 
2492 .pDynamic:
2493     callSlowPath(_llint_slow_path_put_to_scope)
2494     dispatch()
2495 end)
2496 
2497 
2498 llintOpWithProfile(op_get_from_arguments, OpGetFromArguments, macro (size, get, dispatch, return)
2499     loadVariable(get, m_arguments, t0)
2500     getu(size, OpGetFromArguments, m_index, t1)
2501     loadq DirectArguments_storage[t0, t1, 8], t0
2502     return(t0)
2503 end)
2504 
2505 
2506 llintOp(op_put_to_arguments, OpPutToArguments, macro (size, get, dispatch)
2507     loadVariable(get, m_arguments, t0)
2508     getu(size, OpPutToArguments, m_index, t1)
2509     get(m_value, t3)
2510     loadConstantOrVariable(size, t3, t2)
2511     storeq t2, DirectArguments_storage[t0, t1, 8]
2512     writeBarrierOnOperands(size, get, m_arguments, m_value)
2513     dispatch()
2514 end)
2515 
2516 
2517 llintOpWithReturn(op_get_parent_scope, OpGetParentScope, macro (size, get, dispatch, return)
2518     loadVariable(get, m_scope, t0)
2519     loadp JSScope::m_next[t0], t0
2520     return(t0)
2521 end)
2522 
2523 
2524 llintOpWithMetadata(op_profile_type, OpProfileType, macro (size, get, dispatch, metadata, return)
2525     loadp CodeBlock[cfr], t1
2526     loadp CodeBlock::m_vm[t1], t1
2527     # t1 is holding the pointer to the typeProfilerLog.
2528     loadp VM::m_typeProfilerLog[t1], t1
2529     # t2 is holding the pointer to the current log entry.
2530     loadp TypeProfilerLog::m_currentLogEntryPtr[t1], t2
2531 
2532     # t0 is holding the JSValue argument.
2533     get(m_targetVirtualRegister, t3)
2534     loadConstantOrVariable(size, t3, t0)
2535 
2536     bqeq t0, ValueEmpty, .opProfileTypeDone
2537     # Store the JSValue onto the log entry.
2538     storeq t0, TypeProfilerLog::LogEntry::value[t2]
2539     
2540     # Store the TypeLocation onto the log entry.
2541     metadata(t5, t3)
2542     loadp OpProfileType::Metadata::m_typeLocation[t5], t3
2543     storep t3, TypeProfilerLog::LogEntry::location[t2]
2544 
2545     btqz t0, tagMask, .opProfileTypeIsCell
2546     storei 0, TypeProfilerLog::LogEntry::structureID[t2]
2547     jmp .opProfileTypeSkipIsCell
2548 .opProfileTypeIsCell:
2549     loadi JSCell::m_structureID[t0], t3
2550     storei t3, TypeProfilerLog::LogEntry::structureID[t2]
2551 .opProfileTypeSkipIsCell:
2552     
2553     # Increment the current log entry.
2554     addp sizeof TypeProfilerLog::LogEntry, t2
2555     storep t2, TypeProfilerLog::m_currentLogEntryPtr[t1]
2556 
2557     loadp TypeProfilerLog::m_logEndPtr[t1], t1
2558     bpneq t2, t1, .opProfileTypeDone
2559     callSlowPath(_slow_path_profile_type_clear_log)
2560 
2561 .opProfileTypeDone:
2562     dispatch()
2563 end)
2564 
2565 
2566 llintOpWithMetadata(op_profile_control_flow, OpProfileControlFlow, macro (size, get, dispatch, metadata, return)
2567     metadata(t5, t0)
2568     loadp OpProfileControlFlow::Metadata::m_basicBlockLocation[t5], t0
2569     addq 1, BasicBlockLocation::m_executionCount[t0]
2570     dispatch()
2571 end)
2572 
2573 
2574 llintOpWithReturn(op_get_rest_length, OpGetRestLength, macro (size, get, dispatch, return)
2575     loadi PayloadOffset + ArgumentCount[cfr], t0
2576     subi 1, t0
2577     getu(size, OpGetRestLength, m_numParametersToSkip, t1)
2578     bilteq t0, t1, .storeZero
2579     subi t1, t0
2580     jmp .boxUp
2581 .storeZero:
2582     move 0, t0
2583 .boxUp:
2584     orq tagTypeNumber, t0
2585     return(t0)
2586 end)
2587 
2588 
2589 llintOp(op_log_shadow_chicken_prologue, OpLogShadowChickenPrologue, macro (size, get, dispatch)
2590     acquireShadowChickenPacket(.opLogShadowChickenPrologueSlow)
2591     storep cfr, ShadowChicken::Packet::frame[t0]
2592     loadp CallerFrame[cfr], t1
2593     storep t1, ShadowChicken::Packet::callerFrame[t0]
2594     loadp Callee[cfr], t1
2595     storep t1, ShadowChicken::Packet::callee[t0]
2596     loadVariable(get, m_scope, t1)
2597     storep t1, ShadowChicken::Packet::scope[t0]
2598     dispatch()
2599 .opLogShadowChickenPrologueSlow:
2600     callSlowPath(_llint_slow_path_log_shadow_chicken_prologue)
2601     dispatch()
2602 end)
2603 
2604 
2605 llintOp(op_log_shadow_chicken_tail, OpLogShadowChickenTail, macro (size, get, dispatch)
2606     acquireShadowChickenPacket(.opLogShadowChickenTailSlow)
2607     storep cfr, ShadowChicken::Packet::frame[t0]
2608     storep ShadowChickenTailMarker, ShadowChicken::Packet::callee[t0]
2609     loadVariable(get, m_thisValue, t1)
2610     storep t1, ShadowChicken::Packet::thisValue[t0]
2611     loadVariable(get, m_scope, t1)
2612     storep t1, ShadowChicken::Packet::scope[t0]
2613     loadp CodeBlock[cfr], t1
2614     storep t1, ShadowChicken::Packet::codeBlock[t0]
2615     storei PC, ShadowChicken::Packet::callSiteIndex[t0]
2616     dispatch()
2617 .opLogShadowChickenTailSlow:
2618     callSlowPath(_llint_slow_path_log_shadow_chicken_tail)
2619     dispatch()
2620 end)
    </pre>
  </body>
</html>