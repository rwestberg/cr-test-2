<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/PolymorphicAccess.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (C) 2014-2018 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;PolymorphicAccess.h&quot;
 28 
 29 #if ENABLE(JIT)
 30 
 31 #include &quot;BinarySwitch.h&quot;
 32 #include &quot;CCallHelpers.h&quot;
 33 #include &quot;CodeBlock.h&quot;
 34 #include &quot;FullCodeOrigin.h&quot;
 35 #include &quot;Heap.h&quot;
 36 #include &quot;JITOperations.h&quot;
 37 #include &quot;JSCInlines.h&quot;
 38 #include &quot;LinkBuffer.h&quot;
 39 #include &quot;StructureStubClearingWatchpoint.h&quot;
 40 #include &quot;StructureStubInfo.h&quot;
 41 #include &quot;SuperSampler.h&quot;
 42 #include &lt;wtf/CommaPrinter.h&gt;
 43 #include &lt;wtf/ListDump.h&gt;
 44 
 45 namespace JSC {
 46 
 47 namespace PolymorphicAccessInternal {
 48 static const bool verbose = false;
 49 }
 50 
 51 void AccessGenerationResult::dump(PrintStream&amp; out) const
 52 {
 53     out.print(m_kind);
 54     if (m_code)
 55         out.print(&quot;:&quot;, m_code);
 56 }
 57 
 58 Watchpoint* AccessGenerationState::addWatchpoint(const ObjectPropertyCondition&amp; condition)
 59 {
 60     return WatchpointsOnStructureStubInfo::ensureReferenceAndAddWatchpoint(
 61         watchpoints, jit-&gt;codeBlock(), stubInfo, condition);
 62 }
 63 
 64 void AccessGenerationState::restoreScratch()
 65 {
 66     allocator-&gt;restoreReusedRegistersByPopping(*jit, preservedReusedRegisterState);
 67 }
 68 
 69 void AccessGenerationState::succeed()
 70 {
 71     restoreScratch();
 72     success.append(jit-&gt;jump());
 73 }
 74 
 75 const RegisterSet&amp; AccessGenerationState::liveRegistersForCall()
 76 {
 77     if (!m_calculatedRegistersForCallAndExceptionHandling)
 78         calculateLiveRegistersForCallAndExceptionHandling();
 79     return m_liveRegistersForCall;
 80 }
 81 
 82 const RegisterSet&amp; AccessGenerationState::liveRegistersToPreserveAtExceptionHandlingCallSite()
 83 {
 84     if (!m_calculatedRegistersForCallAndExceptionHandling)
 85         calculateLiveRegistersForCallAndExceptionHandling();
 86     return m_liveRegistersToPreserveAtExceptionHandlingCallSite;
 87 }
 88 
 89 static RegisterSet calleeSaveRegisters()
 90 {
 91     RegisterSet result = RegisterSet::registersToNotSaveForJSCall();
 92     result.filter(RegisterSet::registersToNotSaveForCCall());
 93     return result;
 94 }
 95 
 96 const RegisterSet&amp; AccessGenerationState::calculateLiveRegistersForCallAndExceptionHandling()
 97 {
 98     if (!m_calculatedRegistersForCallAndExceptionHandling) {
 99         m_calculatedRegistersForCallAndExceptionHandling = true;
100 
101         m_liveRegistersToPreserveAtExceptionHandlingCallSite = jit-&gt;codeBlock()-&gt;jitCode()-&gt;liveRegistersToPreserveAtExceptionHandlingCallSite(jit-&gt;codeBlock(), stubInfo-&gt;callSiteIndex);
102         m_needsToRestoreRegistersIfException = m_liveRegistersToPreserveAtExceptionHandlingCallSite.numberOfSetRegisters() &gt; 0;
103         if (m_needsToRestoreRegistersIfException)
104             RELEASE_ASSERT(JITCode::isOptimizingJIT(jit-&gt;codeBlock()-&gt;jitType()));
105 
106         m_liveRegistersForCall = RegisterSet(m_liveRegistersToPreserveAtExceptionHandlingCallSite, allocator-&gt;usedRegisters());
107         m_liveRegistersForCall.exclude(calleeSaveRegisters());
108     }
109     return m_liveRegistersForCall;
110 }
111 
112 auto AccessGenerationState::preserveLiveRegistersToStackForCall(const RegisterSet&amp; extra) -&gt; SpillState
113 {
114     RegisterSet liveRegisters = liveRegistersForCall();
115     liveRegisters.merge(extra);
116 
117     unsigned extraStackPadding = 0;
118     unsigned numberOfStackBytesUsedForRegisterPreservation = ScratchRegisterAllocator::preserveRegistersToStackForCall(*jit, liveRegisters, extraStackPadding);
119     return SpillState {
120         WTFMove(liveRegisters),
121         numberOfStackBytesUsedForRegisterPreservation
122     };
123 }
124 
125 void AccessGenerationState::restoreLiveRegistersFromStackForCallWithThrownException(const SpillState&amp; spillState)
126 {
127     // Even if we&#39;re a getter, we don&#39;t want to ignore the result value like we normally do
128     // because the getter threw, and therefore, didn&#39;t return a value that means anything.
129     // Instead, we want to restore that register to what it was upon entering the getter
130     // inline cache. The subtlety here is if the base and the result are the same register,
131     // and the getter threw, we want OSR exit to see the original base value, not the result
132     // of the getter call.
133     RegisterSet dontRestore = spillState.spilledRegisters;
134     // As an optimization here, we only need to restore what is live for exception handling.
135     // We can construct the dontRestore set to accomplish this goal by having it contain only
136     // what is live for call but not live for exception handling. By ignoring things that are
137     // only live at the call but not the exception handler, we will only restore things live
138     // at the exception handler.
139     dontRestore.exclude(liveRegistersToPreserveAtExceptionHandlingCallSite());
140     restoreLiveRegistersFromStackForCall(spillState, dontRestore);
141 }
142 
143 void AccessGenerationState::restoreLiveRegistersFromStackForCall(const SpillState&amp; spillState, const RegisterSet&amp; dontRestore)
144 {
145     unsigned extraStackPadding = 0;
146     ScratchRegisterAllocator::restoreRegistersFromStackForCall(*jit, spillState.spilledRegisters, dontRestore, spillState.numberOfStackBytesUsedForRegisterPreservation, extraStackPadding);
147 }
148 
149 CallSiteIndex AccessGenerationState::callSiteIndexForExceptionHandlingOrOriginal()
150 {
151     if (!m_calculatedRegistersForCallAndExceptionHandling)
152         calculateLiveRegistersForCallAndExceptionHandling();
153 
154     if (!m_calculatedCallSiteIndex) {
155         m_calculatedCallSiteIndex = true;
156 
157         if (m_needsToRestoreRegistersIfException)
158             m_callSiteIndex = jit-&gt;codeBlock()-&gt;newExceptionHandlingCallSiteIndex(stubInfo-&gt;callSiteIndex);
159         else
160             m_callSiteIndex = originalCallSiteIndex();
161     }
162 
163     return m_callSiteIndex;
164 }
165 
166 DisposableCallSiteIndex AccessGenerationState::callSiteIndexForExceptionHandling()
167 {
168     RELEASE_ASSERT(m_calculatedRegistersForCallAndExceptionHandling);
169     RELEASE_ASSERT(m_needsToRestoreRegistersIfException);
170     RELEASE_ASSERT(m_calculatedCallSiteIndex);
171     return DisposableCallSiteIndex::fromCallSiteIndex(m_callSiteIndex);
172 }
173 
174 const HandlerInfo&amp; AccessGenerationState::originalExceptionHandler()
175 {
176     if (!m_calculatedRegistersForCallAndExceptionHandling)
177         calculateLiveRegistersForCallAndExceptionHandling();
178 
179     RELEASE_ASSERT(m_needsToRestoreRegistersIfException);
180     HandlerInfo* exceptionHandler = jit-&gt;codeBlock()-&gt;handlerForIndex(stubInfo-&gt;callSiteIndex.bits());
181     RELEASE_ASSERT(exceptionHandler);
182     return *exceptionHandler;
183 }
184 
185 CallSiteIndex AccessGenerationState::originalCallSiteIndex() const { return stubInfo-&gt;callSiteIndex; }
186 
187 void AccessGenerationState::emitExplicitExceptionHandler()
188 {
189     restoreScratch();
<a name="1" id="anc1"></a><span class="line-modified">190     jit-&gt;copyCalleeSavesToEntryFrameCalleeSavesBuffer(m_vm.topEntryFrame);</span>




191     if (needsToRestoreRegistersIfException()) {
192         // To the JIT that produces the original exception handling
193         // call site, they will expect the OSR exit to be arrived
194         // at from genericUnwind. Therefore we must model what genericUnwind
195         // does here. I.e, set callFrameForCatch and copy callee saves.
196 
197         jit-&gt;storePtr(GPRInfo::callFrameRegister, m_vm.addressOfCallFrameForCatch());
198         CCallHelpers::Jump jumpToOSRExitExceptionHandler = jit-&gt;jump();
199 
200         // We don&#39;t need to insert a new exception handler in the table
201         // because we&#39;re doing a manual exception check here. i.e, we&#39;ll
202         // never arrive here from genericUnwind().
203         HandlerInfo originalHandler = originalExceptionHandler();
204         jit-&gt;addLinkTask(
205             [=] (LinkBuffer&amp; linkBuffer) {
206                 linkBuffer.link(jumpToOSRExitExceptionHandler, originalHandler.nativeCode);
207             });
208     } else {
209         jit-&gt;setupArguments&lt;decltype(lookupExceptionHandler)&gt;(CCallHelpers::TrustedImmPtr(&amp;m_vm), GPRInfo::callFrameRegister);
210         CCallHelpers::Call lookupExceptionHandlerCall = jit-&gt;call(OperationPtrTag);
211         jit-&gt;addLinkTask(
212             [=] (LinkBuffer&amp; linkBuffer) {
213                 linkBuffer.link(lookupExceptionHandlerCall, FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandler));
214             });
215         jit-&gt;jumpToExceptionHandler(m_vm);
216     }
217 }
218 
219 
220 PolymorphicAccess::PolymorphicAccess() { }
221 PolymorphicAccess::~PolymorphicAccess() { }
222 
223 AccessGenerationResult PolymorphicAccess::addCases(
224     const GCSafeConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo,
225     const Identifier&amp; ident, Vector&lt;std::unique_ptr&lt;AccessCase&gt;, 2&gt; originalCasesToAdd)
226 {
227     SuperSamplerScope superSamplerScope(false);
228 
229     // This method will add the originalCasesToAdd to the list one at a time while preserving the
230     // invariants:
231     // - If a newly added case canReplace() any existing case, then the existing case is removed before
232     //   the new case is added. Removal doesn&#39;t change order of the list. Any number of existing cases
233     //   can be removed via the canReplace() rule.
234     // - Cases in the list always appear in ascending order of time of addition. Therefore, if you
235     //   cascade through the cases in reverse order, you will get the most recent cases first.
236     // - If this method fails (returns null, doesn&#39;t add the cases), then both the previous case list
237     //   and the previous stub are kept intact and the new cases are destroyed. It&#39;s OK to attempt to
238     //   add more things after failure.
239 
240     // First ensure that the originalCasesToAdd doesn&#39;t contain duplicates.
241     Vector&lt;std::unique_ptr&lt;AccessCase&gt;&gt; casesToAdd;
242     for (unsigned i = 0; i &lt; originalCasesToAdd.size(); ++i) {
243         std::unique_ptr&lt;AccessCase&gt; myCase = WTFMove(originalCasesToAdd[i]);
244 
245         // Add it only if it is not replaced by the subsequent cases in the list.
246         bool found = false;
247         for (unsigned j = i + 1; j &lt; originalCasesToAdd.size(); ++j) {
248             if (originalCasesToAdd[j]-&gt;canReplace(*myCase)) {
249                 found = true;
250                 break;
251             }
252         }
253 
254         if (found)
255             continue;
256 
257         casesToAdd.append(WTFMove(myCase));
258     }
259 
260     if (PolymorphicAccessInternal::verbose)
261         dataLog(&quot;casesToAdd: &quot;, listDump(casesToAdd), &quot;\n&quot;);
262 
263     // If there aren&#39;t any cases to add, then fail on the grounds that there&#39;s no point to generating a
264     // new stub that will be identical to the old one. Returning null should tell the caller to just
265     // keep doing what they were doing before.
266     if (casesToAdd.isEmpty())
267         return AccessGenerationResult::MadeNoChanges;
268 
269     if (stubInfo.accessType != AccessType::InstanceOf) {
270         bool shouldReset = false;
271         AccessGenerationResult resetResult(AccessGenerationResult::ResetStubAndFireWatchpoints);
272         auto considerPolyProtoReset = [&amp;] (Structure* a, Structure* b) {
273             if (Structure::shouldConvertToPolyProto(a, b)) {
274                 // For now, we only reset if this is our first time invalidating this watchpoint.
275                 // The reason we don&#39;t immediately fire this watchpoint is that we may be already
276                 // watching the poly proto watchpoint, which if fired, would destroy us. We let
277                 // the person handling the result to do a delayed fire.
278                 ASSERT(a-&gt;rareData()-&gt;sharedPolyProtoWatchpoint().get() == b-&gt;rareData()-&gt;sharedPolyProtoWatchpoint().get());
279                 if (a-&gt;rareData()-&gt;sharedPolyProtoWatchpoint()-&gt;isStillValid()) {
280                     shouldReset = true;
281                     resetResult.addWatchpointToFire(*a-&gt;rareData()-&gt;sharedPolyProtoWatchpoint(), StringFireDetail(&quot;Detected poly proto optimization opportunity.&quot;));
282                 }
283             }
284         };
285 
286         for (auto&amp; caseToAdd : casesToAdd) {
287             for (auto&amp; existingCase : m_list) {
288                 Structure* a = caseToAdd-&gt;structure();
289                 Structure* b = existingCase-&gt;structure();
290                 considerPolyProtoReset(a, b);
291             }
292         }
293         for (unsigned i = 0; i &lt; casesToAdd.size(); ++i) {
294             for (unsigned j = i + 1; j &lt; casesToAdd.size(); ++j) {
295                 Structure* a = casesToAdd[i]-&gt;structure();
296                 Structure* b = casesToAdd[j]-&gt;structure();
297                 considerPolyProtoReset(a, b);
298             }
299         }
300 
301         if (shouldReset)
302             return resetResult;
303     }
304 
305     // Now add things to the new list. Note that at this point, we will still have old cases that
306     // may be replaced by the new ones. That&#39;s fine. We will sort that out when we regenerate.
307     for (auto&amp; caseToAdd : casesToAdd) {
308         commit(locker, vm, m_watchpoints, codeBlock, stubInfo, ident, *caseToAdd);
309         m_list.append(WTFMove(caseToAdd));
310     }
311 
312     if (PolymorphicAccessInternal::verbose)
313         dataLog(&quot;After addCases: m_list: &quot;, listDump(m_list), &quot;\n&quot;);
314 
315     return AccessGenerationResult::Buffered;
316 }
317 
318 AccessGenerationResult PolymorphicAccess::addCase(
319     const GCSafeConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo,
320     const Identifier&amp; ident, std::unique_ptr&lt;AccessCase&gt; newAccess)
321 {
322     Vector&lt;std::unique_ptr&lt;AccessCase&gt;, 2&gt; newAccesses;
323     newAccesses.append(WTFMove(newAccess));
324     return addCases(locker, vm, codeBlock, stubInfo, ident, WTFMove(newAccesses));
325 }
326 
327 bool PolymorphicAccess::visitWeak(VM&amp; vm) const
328 {
329     for (unsigned i = 0; i &lt; size(); ++i) {
330         if (!at(i).visitWeak(vm))
331             return false;
332     }
333     if (Vector&lt;WriteBarrier&lt;JSCell&gt;&gt;* weakReferences = m_weakReferences.get()) {
334         for (WriteBarrier&lt;JSCell&gt;&amp; weakReference : *weakReferences) {
<a name="2" id="anc2"></a><span class="line-modified">335             if (!Heap::isMarked(weakReference.get()))</span>
336                 return false;
337         }
338     }
339     return true;
340 }
341 
342 bool PolymorphicAccess::propagateTransitions(SlotVisitor&amp; visitor) const
343 {
344     bool result = true;
345     for (unsigned i = 0; i &lt; size(); ++i)
346         result &amp;= at(i).propagateTransitions(visitor);
347     return result;
348 }
349 
350 void PolymorphicAccess::dump(PrintStream&amp; out) const
351 {
352     out.print(RawPointer(this), &quot;:[&quot;);
353     CommaPrinter comma;
354     for (auto&amp; entry : m_list)
355         out.print(comma, *entry);
356     out.print(&quot;]&quot;);
357 }
358 
359 void PolymorphicAccess::commit(
360     const GCSafeConcurrentJSLocker&amp;, VM&amp; vm, std::unique_ptr&lt;WatchpointsOnStructureStubInfo&gt;&amp; watchpoints, CodeBlock* codeBlock,
361     StructureStubInfo&amp; stubInfo, const Identifier&amp; ident, AccessCase&amp; accessCase)
362 {
363     // NOTE: We currently assume that this is relatively rare. It mainly arises for accesses to
364     // properties on DOM nodes. For sure we cache many DOM node accesses, but even in
365     // Real Pages (TM), we appear to spend most of our time caching accesses to properties on
366     // vanilla objects or exotic objects from within JSC (like Arguments, those are super popular).
367     // Those common kinds of JSC object accesses don&#39;t hit this case.
368 
369     for (WatchpointSet* set : accessCase.commit(vm, ident)) {
370         Watchpoint* watchpoint =
371             WatchpointsOnStructureStubInfo::ensureReferenceAndAddWatchpoint(
372                 watchpoints, codeBlock, &amp;stubInfo, ObjectPropertyCondition());
373 
374         set-&gt;add(watchpoint);
375     }
376 }
377 
378 AccessGenerationResult PolymorphicAccess::regenerate(
379     const GCSafeConcurrentJSLocker&amp; locker, VM&amp; vm, CodeBlock* codeBlock, StructureStubInfo&amp; stubInfo, const Identifier&amp; ident)
380 {
381     SuperSamplerScope superSamplerScope(false);
382 
383     if (PolymorphicAccessInternal::verbose)
384         dataLog(&quot;Regenerate with m_list: &quot;, listDump(m_list), &quot;\n&quot;);
385 
386     AccessGenerationState state(vm, codeBlock-&gt;globalObject());
387 
388     state.access = this;
389     state.stubInfo = &amp;stubInfo;
390     state.ident = &amp;ident;
391 
392     state.baseGPR = stubInfo.baseGPR();
393     state.thisGPR = stubInfo.patch.thisGPR;
394     state.valueRegs = stubInfo.valueRegs();
395 
396     ScratchRegisterAllocator allocator(stubInfo.patch.usedRegisters);
397     state.allocator = &amp;allocator;
398     allocator.lock(state.baseGPR);
399     if (state.thisGPR != InvalidGPRReg)
400         allocator.lock(state.thisGPR);
401     allocator.lock(state.valueRegs);
402 #if USE(JSVALUE32_64)
403     allocator.lock(stubInfo.patch.baseTagGPR);
404 #endif
405 
406     state.scratchGPR = allocator.allocateScratchGPR();
407 
408     CCallHelpers jit(codeBlock);
409     state.jit = &amp;jit;
410 
411     state.preservedReusedRegisterState =
412         allocator.preserveReusedRegistersByPushing(jit, ScratchRegisterAllocator::ExtraStackSpace::NoExtraSpace);
413 
414     // Regenerating is our opportunity to figure out what our list of cases should look like. We
415     // do this here. The newly produced &#39;cases&#39; list may be smaller than m_list. We don&#39;t edit
416     // m_list in-place because we may still fail, in which case we want the PolymorphicAccess object
417     // to be unmutated. For sure, we want it to hang onto any data structures that may be referenced
418     // from the code of the current stub (aka previous).
419     ListType cases;
420     unsigned srcIndex = 0;
421     unsigned dstIndex = 0;
422     while (srcIndex &lt; m_list.size()) {
423         std::unique_ptr&lt;AccessCase&gt; someCase = WTFMove(m_list[srcIndex++]);
424 
425         // If the case had been generated, then we have to keep the original in m_list in case we
426         // fail to regenerate. That case may have data structures that are used by the code that it
427         // had generated. If the case had not been generated, then we want to remove it from m_list.
428         bool isGenerated = someCase-&gt;state() == AccessCase::Generated;
429 
430         [&amp;] () {
431             if (!someCase-&gt;couldStillSucceed())
432                 return;
433 
434             // Figure out if this is replaced by any later case. Given two cases A and B where A
435             // comes first in the case list, we know that A would have triggered first if we had
436             // generated the cases in a cascade. That&#39;s why this loop asks B-&gt;canReplace(A) but not
437             // A-&gt;canReplace(B). If A-&gt;canReplace(B) was true then A would never have requested
438             // repatching in cases where Repatch.cpp would have then gone on to generate B. If that
439             // did happen by some fluke, then we&#39;d just miss the redundancy here, which wouldn&#39;t be
440             // incorrect - just slow. However, if A&#39;s checks failed and Repatch.cpp concluded that
441             // this new condition could be handled by B and B-&gt;canReplace(A), then this says that we
442             // don&#39;t need A anymore.
443             //
444             // If we can generate a binary switch, then A-&gt;canReplace(B) == B-&gt;canReplace(A). So,
445             // it doesn&#39;t matter that we only do the check in one direction.
446             for (unsigned j = srcIndex; j &lt; m_list.size(); ++j) {
447                 if (m_list[j]-&gt;canReplace(*someCase))
448                     return;
449             }
450 
451             if (isGenerated)
452                 cases.append(someCase-&gt;clone());
453             else
454                 cases.append(WTFMove(someCase));
455         }();
456 
457         if (isGenerated)
458             m_list[dstIndex++] = WTFMove(someCase);
459     }
460     m_list.resize(dstIndex);
461 
462     bool generatedFinalCode = false;
463 
464     // If the resulting set of cases is so big that we would stop caching and this is InstanceOf,
465     // then we want to generate the generic InstanceOf and then stop.
466     if (cases.size() &gt;= Options::maxAccessVariantListSize()
467         &amp;&amp; stubInfo.accessType == AccessType::InstanceOf) {
468         while (!cases.isEmpty())
469             m_list.append(cases.takeLast());
470         cases.append(AccessCase::create(vm, codeBlock, AccessCase::InstanceOfGeneric));
471         generatedFinalCode = true;
472     }
473 
474     if (PolymorphicAccessInternal::verbose)
475         dataLog(&quot;Optimized cases: &quot;, listDump(cases), &quot;\n&quot;);
476 
477     // At this point we&#39;re convinced that &#39;cases&#39; contains the cases that we want to JIT now and we
478     // won&#39;t change that set anymore.
479 
480     bool allGuardedByStructureCheck = true;
481     bool hasJSGetterSetterCall = false;
482     for (auto&amp; newCase : cases) {
483         commit(locker, vm, state.watchpoints, codeBlock, stubInfo, ident, *newCase);
484         allGuardedByStructureCheck &amp;= newCase-&gt;guardedByStructureCheck();
485         if (newCase-&gt;type() == AccessCase::Getter || newCase-&gt;type() == AccessCase::Setter)
486             hasJSGetterSetterCall = true;
487     }
488 
489     if (cases.isEmpty()) {
490         // This is super unlikely, but we make it legal anyway.
491         state.failAndRepatch.append(jit.jump());
492     } else if (!allGuardedByStructureCheck || cases.size() == 1) {
493         // If there are any proxies in the list, we cannot just use a binary switch over the structure.
494         // We need to resort to a cascade. A cascade also happens to be optimal if we only have just
495         // one case.
496         CCallHelpers::JumpList fallThrough;
497 
498         // Cascade through the list, preferring newer entries.
499         for (unsigned i = cases.size(); i--;) {
500             fallThrough.link(&amp;jit);
501             fallThrough.clear();
502             cases[i]-&gt;generateWithGuard(state, fallThrough);
503         }
504         state.failAndRepatch.append(fallThrough);
505     } else {
506         jit.load32(
507             CCallHelpers::Address(state.baseGPR, JSCell::structureIDOffset()),
508             state.scratchGPR);
509 
510         Vector&lt;int64_t&gt; caseValues(cases.size());
511         for (unsigned i = 0; i &lt; cases.size(); ++i)
512             caseValues[i] = bitwise_cast&lt;int32_t&gt;(cases[i]-&gt;structure()-&gt;id());
513 
514         BinarySwitch binarySwitch(state.scratchGPR, caseValues, BinarySwitch::Int32);
515         while (binarySwitch.advance(jit))
516             cases[binarySwitch.caseIndex()]-&gt;generate(state);
517         state.failAndRepatch.append(binarySwitch.fallThrough());
518     }
519 
520     if (!state.failAndIgnore.empty()) {
521         state.failAndIgnore.link(&amp;jit);
522 
523         // Make sure that the inline cache optimization code knows that we are taking slow path because
524         // of something that isn&#39;t patchable. The slow path will decrement &quot;countdown&quot; and will only
525         // patch things if the countdown reaches zero. We increment the slow path count here to ensure
526         // that the slow path does not try to patch.
527 #if CPU(X86) || CPU(X86_64)
528         jit.move(CCallHelpers::TrustedImmPtr(&amp;stubInfo.countdown), state.scratchGPR);
529         jit.add8(CCallHelpers::TrustedImm32(1), CCallHelpers::Address(state.scratchGPR));
530 #else
531         jit.load8(&amp;stubInfo.countdown, state.scratchGPR);
532         jit.add32(CCallHelpers::TrustedImm32(1), state.scratchGPR);
533         jit.store8(state.scratchGPR, &amp;stubInfo.countdown);
534 #endif
535     }
536 
537     CCallHelpers::JumpList failure;
538     if (allocator.didReuseRegisters()) {
539         state.failAndRepatch.link(&amp;jit);
540         state.restoreScratch();
541     } else
542         failure = state.failAndRepatch;
543     failure.append(jit.jump());
544 
545     CodeBlock* codeBlockThatOwnsExceptionHandlers = nullptr;
546     DisposableCallSiteIndex callSiteIndexForExceptionHandling;
547     if (state.needsToRestoreRegistersIfException() &amp;&amp; hasJSGetterSetterCall) {
548         // Emit the exception handler.
549         // Note that this code is only reachable when doing genericUnwind from a pure JS getter/setter .
550         // Note also that this is not reachable from custom getter/setter. Custom getter/setters will have
551         // their own exception handling logic that doesn&#39;t go through genericUnwind.
552         MacroAssembler::Label makeshiftCatchHandler = jit.label();
553 
554         int stackPointerOffset = codeBlock-&gt;stackPointerOffset() * sizeof(EncodedJSValue);
555         AccessGenerationState::SpillState spillStateForJSGetterSetter = state.spillStateForJSGetterSetter();
556         ASSERT(!spillStateForJSGetterSetter.isEmpty());
557         stackPointerOffset -= state.preservedReusedRegisterState.numberOfBytesPreserved;
558         stackPointerOffset -= spillStateForJSGetterSetter.numberOfStackBytesUsedForRegisterPreservation;
559 
560         jit.loadPtr(vm.addressOfCallFrameForCatch(), GPRInfo::callFrameRegister);
561         jit.addPtr(CCallHelpers::TrustedImm32(stackPointerOffset), GPRInfo::callFrameRegister, CCallHelpers::stackPointerRegister);
562 
563         state.restoreLiveRegistersFromStackForCallWithThrownException(spillStateForJSGetterSetter);
564         state.restoreScratch();
565         CCallHelpers::Jump jumpToOSRExitExceptionHandler = jit.jump();
566 
567         HandlerInfo oldHandler = state.originalExceptionHandler();
568         DisposableCallSiteIndex newExceptionHandlingCallSite = state.callSiteIndexForExceptionHandling();
569         jit.addLinkTask(
570             [=] (LinkBuffer&amp; linkBuffer) {
571                 linkBuffer.link(jumpToOSRExitExceptionHandler, oldHandler.nativeCode);
572 
573                 HandlerInfo handlerToRegister = oldHandler;
574                 handlerToRegister.nativeCode = linkBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(makeshiftCatchHandler);
575                 handlerToRegister.start = newExceptionHandlingCallSite.bits();
576                 handlerToRegister.end = newExceptionHandlingCallSite.bits() + 1;
577                 codeBlock-&gt;appendExceptionHandler(handlerToRegister);
578             });
579 
580         // We set these to indicate to the stub to remove itself from the CodeBlock&#39;s
581         // exception handler table when it is deallocated.
582         codeBlockThatOwnsExceptionHandlers = codeBlock;
583         ASSERT(JITCode::isOptimizingJIT(codeBlockThatOwnsExceptionHandlers-&gt;jitType()));
584         callSiteIndexForExceptionHandling = state.callSiteIndexForExceptionHandling();
585     }
586 
587     LinkBuffer linkBuffer(jit, codeBlock, JITCompilationCanFail);
588     if (linkBuffer.didFailToAllocate()) {
589         if (PolymorphicAccessInternal::verbose)
590             dataLog(&quot;Did fail to allocate.\n&quot;);
591         return AccessGenerationResult::GaveUp;
592     }
593 
594     CodeLocationLabel&lt;JSInternalPtrTag&gt; successLabel = stubInfo.doneLocation();
595 
596     linkBuffer.link(state.success, successLabel);
597 
598     linkBuffer.link(failure, stubInfo.slowPathStartLocation());
599 
600     if (PolymorphicAccessInternal::verbose)
601         dataLog(FullCodeOrigin(codeBlock, stubInfo.codeOrigin), &quot;: Generating polymorphic access stub for &quot;, listDump(cases), &quot;\n&quot;);
602 
603     MacroAssemblerCodeRef&lt;JITStubRoutinePtrTag&gt; code = FINALIZE_CODE_FOR(
604         codeBlock, linkBuffer, JITStubRoutinePtrTag,
605         &quot;%s&quot;, toCString(&quot;Access stub for &quot;, *codeBlock, &quot; &quot;, stubInfo.codeOrigin, &quot; with return point &quot;, successLabel, &quot;: &quot;, listDump(cases)).data());
606 
607     bool doesCalls = false;
608     Vector&lt;JSCell*&gt; cellsToMark;
609     for (auto&amp; entry : cases)
610         doesCalls |= entry-&gt;doesCalls(&amp;cellsToMark);
611 
612     m_stubRoutine = createJITStubRoutine(code, vm, codeBlock, doesCalls, cellsToMark, codeBlockThatOwnsExceptionHandlers, callSiteIndexForExceptionHandling);
613     m_watchpoints = WTFMove(state.watchpoints);
614     if (!state.weakReferences.isEmpty())
<a name="3" id="anc3"></a><span class="line-modified">615         m_weakReferences = std::make_unique&lt;Vector&lt;WriteBarrier&lt;JSCell&gt;&gt;&gt;(WTFMove(state.weakReferences));</span>
616     if (PolymorphicAccessInternal::verbose)
617         dataLog(&quot;Returning: &quot;, code.code(), &quot;\n&quot;);
618 
619     m_list = WTFMove(cases);
620 
621     AccessGenerationResult::Kind resultKind;
622     if (m_list.size() &gt;= Options::maxAccessVariantListSize() || generatedFinalCode)
623         resultKind = AccessGenerationResult::GeneratedFinalCode;
624     else
625         resultKind = AccessGenerationResult::GeneratedNewCode;
626 
627     return AccessGenerationResult(resultKind, code.code());
628 }
629 
630 void PolymorphicAccess::aboutToDie()
631 {
632     if (m_stubRoutine)
633         m_stubRoutine-&gt;aboutToDie();
634 }
635 
636 } // namespace JSC
637 
638 namespace WTF {
639 
640 using namespace JSC;
641 
642 void printInternal(PrintStream&amp; out, AccessGenerationResult::Kind kind)
643 {
644     switch (kind) {
645     case AccessGenerationResult::MadeNoChanges:
646         out.print(&quot;MadeNoChanges&quot;);
647         return;
648     case AccessGenerationResult::GaveUp:
649         out.print(&quot;GaveUp&quot;);
650         return;
651     case AccessGenerationResult::Buffered:
652         out.print(&quot;Buffered&quot;);
653         return;
654     case AccessGenerationResult::GeneratedNewCode:
655         out.print(&quot;GeneratedNewCode&quot;);
656         return;
657     case AccessGenerationResult::GeneratedFinalCode:
658         out.print(&quot;GeneratedFinalCode&quot;);
659         return;
660     case AccessGenerationResult::ResetStubAndFireWatchpoints:
661         out.print(&quot;ResetStubAndFireWatchpoints&quot;);
662         return;
663     }
664 
665     RELEASE_ASSERT_NOT_REACHED();
666 }
667 
668 void printInternal(PrintStream&amp; out, AccessCase::AccessType type)
669 {
670     switch (type) {
671     case AccessCase::Load:
672         out.print(&quot;Load&quot;);
673         return;
674     case AccessCase::Transition:
675         out.print(&quot;Transition&quot;);
676         return;
677     case AccessCase::Replace:
678         out.print(&quot;Replace&quot;);
679         return;
680     case AccessCase::Miss:
681         out.print(&quot;Miss&quot;);
682         return;
683     case AccessCase::GetGetter:
684         out.print(&quot;GetGetter&quot;);
685         return;
686     case AccessCase::Getter:
687         out.print(&quot;Getter&quot;);
688         return;
689     case AccessCase::Setter:
690         out.print(&quot;Setter&quot;);
691         return;
692     case AccessCase::CustomValueGetter:
693         out.print(&quot;CustomValueGetter&quot;);
694         return;
695     case AccessCase::CustomAccessorGetter:
696         out.print(&quot;CustomAccessorGetter&quot;);
697         return;
698     case AccessCase::CustomValueSetter:
699         out.print(&quot;CustomValueSetter&quot;);
700         return;
701     case AccessCase::CustomAccessorSetter:
702         out.print(&quot;CustomAccessorSetter&quot;);
703         return;
704     case AccessCase::IntrinsicGetter:
705         out.print(&quot;IntrinsicGetter&quot;);
706         return;
707     case AccessCase::InHit:
708         out.print(&quot;InHit&quot;);
709         return;
710     case AccessCase::InMiss:
711         out.print(&quot;InMiss&quot;);
712         return;
713     case AccessCase::ArrayLength:
714         out.print(&quot;ArrayLength&quot;);
715         return;
716     case AccessCase::StringLength:
717         out.print(&quot;StringLength&quot;);
718         return;
719     case AccessCase::DirectArgumentsLength:
720         out.print(&quot;DirectArgumentsLength&quot;);
721         return;
722     case AccessCase::ScopedArgumentsLength:
723         out.print(&quot;ScopedArgumentsLength&quot;);
724         return;
725     case AccessCase::ModuleNamespaceLoad:
726         out.print(&quot;ModuleNamespaceLoad&quot;);
727         return;
728     case AccessCase::InstanceOfHit:
729         out.print(&quot;InstanceOfHit&quot;);
730         return;
731     case AccessCase::InstanceOfMiss:
732         out.print(&quot;InstanceOfMiss&quot;);
733         return;
734     case AccessCase::InstanceOfGeneric:
735         out.print(&quot;InstanceOfGeneric&quot;);
736         return;
737     }
738 
739     RELEASE_ASSERT_NOT_REACHED();
740 }
741 
742 void printInternal(PrintStream&amp; out, AccessCase::State state)
743 {
744     switch (state) {
745     case AccessCase::Primordial:
746         out.print(&quot;Primordial&quot;);
747         return;
748     case AccessCase::Committed:
749         out.print(&quot;Committed&quot;);
750         return;
751     case AccessCase::Generated:
752         out.print(&quot;Generated&quot;);
753         return;
754     }
755 
756     RELEASE_ASSERT_NOT_REACHED();
757 }
758 
759 } // namespace WTF
760 
761 #endif // ENABLE(JIT)
762 
763 
<a name="4" id="anc4"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="4" type="hidden" />
</body>
</html>