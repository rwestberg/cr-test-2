<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/Heap.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="HandleSet.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="Heap.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/Heap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   5  *  This library is free software; you can redistribute it and/or
   6  *  modify it under the terms of the GNU Lesser General Public
   7  *  License as published by the Free Software Foundation; either
   8  *  version 2 of the License, or (at your option) any later version.
   9  *
  10  *  This library is distributed in the hope that it will be useful,
  11  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
  12  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
  13  *  Lesser General Public License for more details.
  14  *
  15  *  You should have received a copy of the GNU Lesser General Public
  16  *  License along with this library; if not, write to the Free Software
  17  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
  18  *
  19  */
  20 
  21 #include &quot;config.h&quot;
  22 #include &quot;Heap.h&quot;
  23 
  24 #include &quot;BlockDirectoryInlines.h&quot;

  25 #include &quot;CodeBlock.h&quot;
  26 #include &quot;CodeBlockSetInlines.h&quot;
  27 #include &quot;CollectingScope.h&quot;
  28 #include &quot;ConservativeRoots.h&quot;
  29 #include &quot;DFGWorklistInlines.h&quot;
  30 #include &quot;EdenGCActivityCallback.h&quot;
  31 #include &quot;Exception.h&quot;
  32 #include &quot;FullGCActivityCallback.h&quot;

  33 #include &quot;GCActivityCallback.h&quot;
  34 #include &quot;GCIncomingRefCountedSetInlines.h&quot;
  35 #include &quot;GCSegmentedArrayInlines.h&quot;
  36 #include &quot;GCTypeMap.h&quot;
  37 #include &quot;HasOwnPropertyCache.h&quot;
  38 #include &quot;HeapHelperPool.h&quot;
  39 #include &quot;HeapIterationScope.h&quot;
  40 #include &quot;HeapProfiler.h&quot;
  41 #include &quot;HeapSnapshot.h&quot;
  42 #include &quot;HeapVerifier.h&quot;
  43 #include &quot;IncrementalSweeper.h&quot;
  44 #include &quot;InferredValueInlines.h&quot;
  45 #include &quot;Interpreter.h&quot;
  46 #include &quot;IsoCellSetInlines.h&quot;
  47 #include &quot;JITStubRoutineSet.h&quot;
  48 #include &quot;JITWorklist.h&quot;
  49 #include &quot;JSCInlines.h&quot;
  50 #include &quot;JSGlobalObject.h&quot;
  51 #include &quot;JSLock.h&quot;
  52 #include &quot;JSVirtualMachineInternal.h&quot;
  53 #include &quot;JSWeakMap.h&quot;

  54 #include &quot;JSWeakSet.h&quot;
  55 #include &quot;JSWebAssemblyCodeBlock.h&quot;
  56 #include &quot;MachineStackMarker.h&quot;
  57 #include &quot;MarkStackMergingConstraint.h&quot;
  58 #include &quot;MarkedSpaceInlines.h&quot;
  59 #include &quot;MarkingConstraintSet.h&quot;
  60 #include &quot;PreventCollectionScope.h&quot;
  61 #include &quot;SamplingProfiler.h&quot;
  62 #include &quot;ShadowChicken.h&quot;
  63 #include &quot;SpaceTimeMutatorScheduler.h&quot;
  64 #include &quot;StochasticSpaceTimeMutatorScheduler.h&quot;
  65 #include &quot;StopIfNecessaryTimer.h&quot;
  66 #include &quot;SubspaceInlines.h&quot;
  67 #include &quot;SuperSampler.h&quot;
  68 #include &quot;SweepingScope.h&quot;

  69 #include &quot;SynchronousStopTheWorldMutatorScheduler.h&quot;
  70 #include &quot;TypeProfiler.h&quot;
  71 #include &quot;TypeProfilerLog.h&quot;
  72 #include &quot;UnlinkedCodeBlock.h&quot;
  73 #include &quot;VM.h&quot;
  74 #include &quot;VisitCounter.h&quot;
  75 #include &quot;WasmMemory.h&quot;
  76 #include &quot;WeakMapImplInlines.h&quot;
  77 #include &quot;WeakSetInlines.h&quot;
  78 #include &lt;algorithm&gt;
  79 #include &lt;wtf/ListDump.h&gt;
  80 #include &lt;wtf/MainThread.h&gt;
  81 #include &lt;wtf/ParallelVectorIterator.h&gt;
  82 #include &lt;wtf/ProcessID.h&gt;
  83 #include &lt;wtf/RAMSize.h&gt;
  84 #include &lt;wtf/SimpleStats.h&gt;
  85 #include &lt;wtf/Threading.h&gt;
  86 
  87 #if PLATFORM(IOS_FAMILY)
  88 #include &lt;bmalloc/bmalloc.h&gt;
  89 #endif
  90 
  91 #if USE(FOUNDATION)
  92 #include &lt;wtf/spi/cocoa/objcSPI.h&gt;
  93 #endif
  94 
<span class="line-modified">  95 #if USE(GLIB)</span>
  96 #include &quot;JSCGLibWrapperObject.h&quot;
  97 #endif
  98 
  99 namespace JSC {
 100 
 101 namespace {
 102 
 103 bool verboseStop = false;
 104 
 105 double maxPauseMS(double thisPauseMS)
 106 {
 107     static double maxPauseMS;
 108     maxPauseMS = std::max(thisPauseMS, maxPauseMS);
 109     return maxPauseMS;
 110 }
 111 
 112 size_t minHeapSize(HeapType heapType, size_t ramSize)
 113 {
 114     if (heapType == LargeHeap) {
 115         double result = std::min(
</pre>
<hr />
<pre>
 123 size_t proportionalHeapSize(size_t heapSize, size_t ramSize)
 124 {
 125     if (VM::isInMiniMode())
 126         return Options::miniVMHeapGrowthFactor() * heapSize;
 127 
 128 #if PLATFORM(IOS_FAMILY)
 129     size_t memoryFootprint = bmalloc::api::memoryFootprint();
 130     if (memoryFootprint &lt; ramSize * Options::smallHeapRAMFraction())
 131         return Options::smallHeapGrowthFactor() * heapSize;
 132     if (memoryFootprint &lt; ramSize * Options::mediumHeapRAMFraction())
 133         return Options::mediumHeapGrowthFactor() * heapSize;
 134 #else
 135     if (heapSize &lt; ramSize * Options::smallHeapRAMFraction())
 136         return Options::smallHeapGrowthFactor() * heapSize;
 137     if (heapSize &lt; ramSize * Options::mediumHeapRAMFraction())
 138         return Options::mediumHeapGrowthFactor() * heapSize;
 139 #endif
 140     return Options::largeHeapGrowthFactor() * heapSize;
 141 }
 142 
<span class="line-modified"> 143 bool isValidSharedInstanceThreadState(VM* vm)</span>
 144 {
<span class="line-modified"> 145     return vm-&gt;currentThreadIsHoldingAPILock();</span>
 146 }
 147 
<span class="line-modified"> 148 bool isValidThreadState(VM* vm)</span>
 149 {
<span class="line-modified"> 150     if (vm-&gt;atomicStringTable() != WTF::Thread::current().atomicStringTable())</span>
 151         return false;
 152 
<span class="line-modified"> 153     if (vm-&gt;isSharedInstance() &amp;&amp; !isValidSharedInstanceThreadState(vm))</span>
 154         return false;
 155 
 156     return true;
 157 }
 158 
 159 void recordType(VM&amp; vm, TypeCountSet&amp; set, JSCell* cell)
 160 {
 161     const char* typeName = &quot;[unknown]&quot;;
 162     const ClassInfo* info = cell-&gt;classInfo(vm);
 163     if (info &amp;&amp; info-&gt;className)
 164         typeName = info-&gt;className;
 165     set.add(typeName);
 166 }
 167 
 168 bool measurePhaseTiming()
 169 {
 170     return false;
 171 }
 172 
 173 HashMap&lt;const char*, GCTypeMap&lt;SimpleStats&gt;&gt;&amp; timingStats()
</pre>
<hr />
<pre>
 213     }
 214 
 215     ~TimingScope()
 216     {
 217         if (measurePhaseTiming()) {
 218             MonotonicTime after = MonotonicTime::now();
 219             Seconds timing = after - m_before;
 220             SimpleStats&amp; stats = timingStats(m_name, *m_scope);
 221             stats.add(timing.milliseconds());
 222             dataLog(&quot;[GC:&quot;, *m_scope, &quot;] &quot;, m_name, &quot; took: &quot;, timing.milliseconds(), &quot;ms (average &quot;, stats.mean(), &quot;ms).\n&quot;);
 223         }
 224     }
 225 private:
 226     Optional&lt;CollectionScope&gt; m_scope;
 227     MonotonicTime m_before;
 228     const char* m_name;
 229 };
 230 
 231 } // anonymous namespace
 232 
<span class="line-modified"> 233 class Heap::Thread : public AutomaticThread {</span>
 234 public:
<span class="line-modified"> 235     Thread(const AbstractLocker&amp; locker, Heap&amp; heap)</span>
 236         : AutomaticThread(locker, heap.m_threadLock, heap.m_threadCondition.copyRef())
 237         , m_heap(heap)
 238     {
 239     }
 240 
 241     const char* name() const override
 242     {
 243         return &quot;JSC Heap Collector Thread&quot;;
 244     }
 245 
 246 protected:
 247     PollResult poll(const AbstractLocker&amp; locker) override
 248     {
 249         if (m_heap.m_threadShouldStop) {
 250             m_heap.notifyThreadStopping(locker);
 251             return PollResult::Stop;
 252         }
<span class="line-modified"> 253         if (m_heap.shouldCollectInCollectorThread(locker))</span>

 254             return PollResult::Work;


 255         return PollResult::Wait;
 256     }
 257 
 258     WorkResult work() override
 259     {
 260         m_heap.collectInCollectorThread();
 261         return WorkResult::Continue;
 262     }
 263 
 264     void threadDidStart() override
 265     {
<span class="line-modified"> 266         WTF::registerGCThread(GCThreadType::Main);</span>





 267     }
 268 
 269 private:
 270     Heap&amp; m_heap;
 271 };
 272 
<span class="line-modified"> 273 Heap::Heap(VM* vm, HeapType heapType)</span>
 274     : m_heapType(heapType)
 275     , m_ramSize(Options::forceRAMSize() ? Options::forceRAMSize() : ramSize())
 276     , m_minBytesPerCycle(minHeapSize(m_heapType, m_ramSize))
 277     , m_maxEdenSize(m_minBytesPerCycle)
 278     , m_maxHeapSize(m_minBytesPerCycle)
 279     , m_objectSpace(this)
<span class="line-modified"> 280     , m_machineThreads(std::make_unique&lt;MachineThreads&gt;())</span>
<span class="line-modified"> 281     , m_collectorSlotVisitor(std::make_unique&lt;SlotVisitor&gt;(*this, &quot;C&quot;))</span>
<span class="line-modified"> 282     , m_mutatorSlotVisitor(std::make_unique&lt;SlotVisitor&gt;(*this, &quot;M&quot;))</span>
<span class="line-modified"> 283     , m_mutatorMarkStack(std::make_unique&lt;MarkStackArray&gt;())</span>
<span class="line-modified"> 284     , m_raceMarkStack(std::make_unique&lt;MarkStackArray&gt;())</span>
<span class="line-modified"> 285     , m_constraintSet(std::make_unique&lt;MarkingConstraintSet&gt;(*this))</span>
 286     , m_handleSet(vm)
<span class="line-modified"> 287     , m_codeBlocks(std::make_unique&lt;CodeBlockSet&gt;())</span>
<span class="line-modified"> 288     , m_jitStubRoutines(std::make_unique&lt;JITStubRoutineSet&gt;())</span>
 289     , m_vm(vm)
 290     // We seed with 10ms so that GCActivityCallback::didAllocate doesn&#39;t continuously
 291     // schedule the timer if we&#39;ve never done a collection.
 292     , m_fullActivityCallback(GCActivityCallback::tryCreateFullTimer(this))
 293     , m_edenActivityCallback(GCActivityCallback::tryCreateEdenTimer(this))
 294     , m_sweeper(adoptRef(*new IncrementalSweeper(this)))
 295     , m_stopIfNecessaryTimer(adoptRef(*new StopIfNecessaryTimer(vm)))
<span class="line-modified"> 296     , m_sharedCollectorMarkStack(std::make_unique&lt;MarkStackArray&gt;())</span>
<span class="line-modified"> 297     , m_sharedMutatorMarkStack(std::make_unique&lt;MarkStackArray&gt;())</span>
 298     , m_helperClient(&amp;heapHelperPool())
 299     , m_threadLock(Box&lt;Lock&gt;::create())
 300     , m_threadCondition(AutomaticThreadCondition::create())
 301 {
 302     m_worldState.store(0);
 303 








 304     if (Options::useConcurrentGC()) {
 305         if (Options::useStochasticMutatorScheduler())
<span class="line-modified"> 306             m_scheduler = std::make_unique&lt;StochasticSpaceTimeMutatorScheduler&gt;(*this);</span>
 307         else
<span class="line-modified"> 308             m_scheduler = std::make_unique&lt;SpaceTimeMutatorScheduler&gt;(*this);</span>
 309     } else {
 310         // We simulate turning off concurrent GC by making the scheduler say that the world
 311         // should always be stopped when the collector is running.
<span class="line-modified"> 312         m_scheduler = std::make_unique&lt;SynchronousStopTheWorldMutatorScheduler&gt;();</span>
 313     }
 314 
 315     if (Options::verifyHeap())
<span class="line-modified"> 316         m_verifier = std::make_unique&lt;HeapVerifier&gt;(this, Options::numberOfGCCyclesToRecordForVerification());</span>
 317 
 318     m_collectorSlotVisitor-&gt;optimizeForStoppedMutator();
 319 
 320     // When memory is critical, allow allocating 25% of the amount above the critical threshold before collecting.
 321     size_t memoryAboveCriticalThreshold = static_cast&lt;size_t&gt;(static_cast&lt;double&gt;(m_ramSize) * (1.0 - Options::criticalGCMemoryThreshold()));
 322     m_maxEdenSizeWhenCritical = memoryAboveCriticalThreshold / 4;
 323 
 324     LockHolder locker(*m_threadLock);
<span class="line-modified"> 325     m_thread = adoptRef(new Thread(locker, *this));</span>
 326 }
 327 
 328 Heap::~Heap()
 329 {



 330     forEachSlotVisitor(
 331         [&amp;] (SlotVisitor&amp; visitor) {
 332             visitor.clearMarkStacks();
 333         });
 334     m_mutatorMarkStack-&gt;clear();
 335     m_raceMarkStack-&gt;clear();
 336 
 337     for (WeakBlock* block : m_logicallyEmptyWeakBlocks)
 338         WeakBlock::destroy(*this, block);
 339 }
 340 
 341 bool Heap::isPagedOut(MonotonicTime deadline)
 342 {
 343     return m_objectSpace.isPagedOut(deadline);
 344 }
 345 
























 346 // The VM is being destroyed and the collector will never run again.
 347 // Run all pending finalizers now because we won&#39;t get another chance.
 348 void Heap::lastChanceToFinalize()
 349 {
 350     MonotonicTime before;
 351     if (Options::logGC()) {
 352         before = MonotonicTime::now();
 353         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: shutdown &quot;);
 354     }
 355 
 356     m_isShuttingDown = true;
 357 
<span class="line-modified"> 358     RELEASE_ASSERT(!m_vm-&gt;entryScope);</span>
 359     RELEASE_ASSERT(m_mutatorState == MutatorState::Running);
 360 
 361     if (m_collectContinuouslyThread) {
 362         {
 363             LockHolder locker(m_collectContinuouslyLock);
 364             m_shouldStopCollectingContinuously = true;
 365             m_collectContinuouslyCondition.notifyOne();
 366         }
 367         m_collectContinuouslyThread-&gt;waitForCompletion();
 368     }
 369 
 370     if (Options::logGC())
 371         dataLog(&quot;1&quot;);
 372 
 373     // Prevent new collections from being started. This is probably not even necessary, since we&#39;re not
 374     // going to call into anything that starts collections. Still, this makes the algorithm more
 375     // obviously sound.
 376     m_isSafeToCollect = false;
 377 
 378     if (Options::logGC())
</pre>
<hr />
<pre>
 406 
 407     // Carefully bring the thread down.
 408     bool stopped = false;
 409     {
 410         LockHolder locker(*m_threadLock);
 411         stopped = m_thread-&gt;tryStop(locker);
 412         m_threadShouldStop = true;
 413         if (!stopped)
 414             m_threadCondition-&gt;notifyOne(locker);
 415     }
 416 
 417     if (Options::logGC())
 418         dataLog(&quot;4&quot;);
 419 
 420     if (!stopped)
 421         m_thread-&gt;join();
 422 
 423     if (Options::logGC())
 424         dataLog(&quot;5 &quot;);
 425 



 426     m_arrayBuffers.lastChanceToFinalize();
 427     m_objectSpace.stopAllocatingForGood();
 428     m_objectSpace.lastChanceToFinalize();
 429     releaseDelayedReleasedObjects();
 430 
 431     sweepAllLogicallyEmptyWeakBlocks();
 432 
 433     m_objectSpace.freeMemory();
 434 
 435     if (Options::logGC())
 436         dataLog((MonotonicTime::now() - before).milliseconds(), &quot;ms]\n&quot;);
 437 }
 438 
 439 void Heap::releaseDelayedReleasedObjects()
 440 {
<span class="line-modified"> 441 #if USE(FOUNDATION) || USE(GLIB)</span>
 442     // We need to guard against the case that releasing an object can create more objects due to the
 443     // release calling into JS. When those JS call(s) exit and all locks are being dropped we end up
 444     // back here and could try to recursively release objects. We guard that with a recursive entry
 445     // count. Only the initial call will release objects, recursive calls simple return and let the
 446     // the initial call to the function take care of any objects created during release time.
 447     // This also means that we need to loop until there are no objects in m_delayedReleaseObjects
 448     // and use a temp Vector for the actual releasing.
 449     if (!m_delayedReleaseRecursionCount++) {
 450         while (!m_delayedReleaseObjects.isEmpty()) {
<span class="line-modified"> 451             ASSERT(m_vm-&gt;currentThreadIsHoldingAPILock());</span>
 452 
 453             auto objectsToRelease = WTFMove(m_delayedReleaseObjects);
 454 
 455             {
 456                 // We need to drop locks before calling out to arbitrary code.
 457                 JSLock::DropAllLocks dropAllLocks(m_vm);
 458 
 459 #if USE(FOUNDATION)
 460                 void* context = objc_autoreleasePoolPush();
 461 #endif
 462                 objectsToRelease.clear();
 463 #if USE(FOUNDATION)
 464                 objc_autoreleasePoolPop(context);
 465 #endif
 466             }
 467         }
 468     }
 469     m_delayedReleaseRecursionCount--;
 470 #endif
 471 }
</pre>
<hr />
<pre>
 504 void Heap::reportAbandonedObjectGraph()
 505 {
 506     // Our clients don&#39;t know exactly how much memory they
 507     // are abandoning so we just guess for them.
 508     size_t abandonedBytes = static_cast&lt;size_t&gt;(0.1 * capacity());
 509 
 510     // We want to accelerate the next collection. Because memory has just
 511     // been abandoned, the next collection has the potential to
 512     // be more profitable. Since allocation is the trigger for collection,
 513     // we hasten the next collection by pretending that we&#39;ve allocated more memory.
 514     if (m_fullActivityCallback) {
 515         m_fullActivityCallback-&gt;didAllocate(*this,
 516             m_sizeAfterLastCollect - m_sizeAfterLastFullCollect + m_bytesAllocatedThisCycle + m_bytesAbandonedSinceLastFullCollect);
 517     }
 518     m_bytesAbandonedSinceLastFullCollect += abandonedBytes;
 519 }
 520 
 521 void Heap::protect(JSValue k)
 522 {
 523     ASSERT(k);
<span class="line-modified"> 524     ASSERT(m_vm-&gt;currentThreadIsHoldingAPILock());</span>
 525 
 526     if (!k.isCell())
 527         return;
 528 
 529     m_protectedValues.add(k.asCell());
 530 }
 531 
 532 bool Heap::unprotect(JSValue k)
 533 {
 534     ASSERT(k);
<span class="line-modified"> 535     ASSERT(m_vm-&gt;currentThreadIsHoldingAPILock());</span>
 536 
 537     if (!k.isCell())
 538         return false;
 539 
 540     return m_protectedValues.remove(k.asCell());
 541 }
 542 
 543 void Heap::addReference(JSCell* cell, ArrayBuffer* buffer)
 544 {
 545     if (m_arrayBuffers.addReference(cell, buffer)) {
 546         collectIfNecessaryOrDefer();
 547         didAllocate(buffer-&gt;gcSizeEstimateInBytes());
 548     }
 549 }
 550 
 551 template&lt;typename CellType, typename CellSet&gt;
 552 void Heap::finalizeMarkedUnconditionalFinalizers(CellSet&amp; cellSet)
 553 {
 554     cellSet.forEachMarkedCell(
 555         [&amp;] (HeapCell* cell, HeapCell::Kind) {
<span class="line-modified"> 556             static_cast&lt;CellType*&gt;(cell)-&gt;finalizeUnconditionally(*vm());</span>
 557         });
 558 }
 559 
 560 void Heap::finalizeUnconditionalFinalizers()
 561 {
<span class="line-modified"> 562     if (vm()-&gt;m_inferredValueSpace)</span>
<span class="line-modified"> 563         finalizeMarkedUnconditionalFinalizers&lt;InferredValue&gt;(vm()-&gt;m_inferredValueSpace-&gt;space);</span>
<span class="line-modified"> 564     vm()-&gt;forEachCodeBlockSpace(</span>

 565         [&amp;] (auto&amp; space) {
 566             this-&gt;finalizeMarkedUnconditionalFinalizers&lt;CodeBlock&gt;(space.set);
 567         });
<span class="line-modified"> 568     finalizeMarkedUnconditionalFinalizers&lt;ExecutableToCodeBlockEdge&gt;(vm()-&gt;executableToCodeBlockEdgesWithFinalizers);</span>
<span class="line-modified"> 569     if (vm()-&gt;m_weakSetSpace)</span>
<span class="line-modified"> 570         finalizeMarkedUnconditionalFinalizers&lt;JSWeakSet&gt;(*vm()-&gt;m_weakSetSpace);</span>
<span class="line-modified"> 571     if (vm()-&gt;m_weakMapSpace)</span>
<span class="line-modified"> 572         finalizeMarkedUnconditionalFinalizers&lt;JSWeakMap&gt;(*vm()-&gt;m_weakMapSpace);</span>
<span class="line-modified"> 573     if (vm()-&gt;m_errorInstanceSpace)</span>
<span class="line-modified"> 574         finalizeMarkedUnconditionalFinalizers&lt;ErrorInstance&gt;(*vm()-&gt;m_errorInstanceSpace);</span>




 575 
 576 #if ENABLE(WEBASSEMBLY)
<span class="line-modified"> 577     if (vm()-&gt;m_webAssemblyCodeBlockSpace)</span>
<span class="line-modified"> 578         finalizeMarkedUnconditionalFinalizers&lt;JSWebAssemblyCodeBlock&gt;(*vm()-&gt;m_webAssemblyCodeBlockSpace);</span>
 579 #endif
 580 }
 581 
 582 void Heap::willStartIterating()
 583 {
 584     m_objectSpace.willStartIterating();
 585 }
 586 
 587 void Heap::didFinishIterating()
 588 {
 589     m_objectSpace.didFinishIterating();
 590 }
 591 
 592 void Heap::completeAllJITPlans()
 593 {
 594     if (!VM::canUseJIT())
 595         return;
 596 #if ENABLE(JIT)
<span class="line-modified"> 597     JITWorklist::ensureGlobalWorklist().completeAllForVM(*m_vm);</span>
 598 #endif // ENABLE(JIT)
<span class="line-modified"> 599     DFG::completeAllPlansForVM(*m_vm);</span>
 600 }
 601 
 602 template&lt;typename Func&gt;
 603 void Heap::iterateExecutingAndCompilingCodeBlocks(const Func&amp; func)
 604 {
 605     m_codeBlocks-&gt;iterateCurrentlyExecuting(func);
 606     if (VM::canUseJIT())
<span class="line-modified"> 607         DFG::iterateCodeBlocksForGC(*m_vm, func);</span>
 608 }
 609 
 610 template&lt;typename Func&gt;
 611 void Heap::iterateExecutingAndCompilingCodeBlocksWithoutHoldingLocks(const Func&amp; func)
 612 {
 613     Vector&lt;CodeBlock*, 256&gt; codeBlocks;
 614     iterateExecutingAndCompilingCodeBlocks(
 615         [&amp;] (CodeBlock* codeBlock) {
 616             codeBlocks.append(codeBlock);
 617         });
 618     for (CodeBlock* codeBlock : codeBlocks)
 619         func(codeBlock);
 620 }
 621 
 622 void Heap::assertMarkStacksEmpty()
 623 {
 624     bool ok = true;
 625 
 626     if (!m_sharedCollectorMarkStack-&gt;isEmpty()) {
 627         dataLog(&quot;FATAL: Shared collector mark stack not empty! It has &quot;, m_sharedCollectorMarkStack-&gt;size(), &quot; elements.\n&quot;);
</pre>
<hr />
<pre>
 636     forEachSlotVisitor(
 637         [&amp;] (SlotVisitor&amp; visitor) {
 638             if (visitor.isEmpty())
 639                 return;
 640 
 641             dataLog(&quot;FATAL: Visitor &quot;, RawPointer(&amp;visitor), &quot; is not empty!\n&quot;);
 642             ok = false;
 643         });
 644 
 645     RELEASE_ASSERT(ok);
 646 }
 647 
 648 void Heap::gatherStackRoots(ConservativeRoots&amp; roots)
 649 {
 650     m_machineThreads-&gt;gatherConservativeRoots(roots, *m_jitStubRoutines, *m_codeBlocks, m_currentThreadState, m_currentThread);
 651 }
 652 
 653 void Heap::gatherJSStackRoots(ConservativeRoots&amp; roots)
 654 {
 655 #if ENABLE(C_LOOP)
<span class="line-modified"> 656     m_vm-&gt;interpreter-&gt;cloopStack().gatherConservativeRoots(roots, *m_jitStubRoutines, *m_codeBlocks);</span>
 657 #else
 658     UNUSED_PARAM(roots);
 659 #endif
 660 }
 661 
 662 void Heap::gatherScratchBufferRoots(ConservativeRoots&amp; roots)
 663 {
 664 #if ENABLE(DFG_JIT)
 665     if (!VM::canUseJIT())
 666         return;
<span class="line-modified"> 667     m_vm-&gt;gatherScratchBufferRoots(roots);</span>
 668 #else
 669     UNUSED_PARAM(roots);
 670 #endif
 671 }
 672 
 673 void Heap::beginMarking()
 674 {
 675     TimingScope timingScope(*this, &quot;Heap::beginMarking&quot;);
 676     m_jitStubRoutines-&gt;clearMarks();
 677     m_objectSpace.beginMarking();
 678     setMutatorShouldBeFenced(true);
 679 }
 680 
 681 void Heap::removeDeadCompilerWorklistEntries()
 682 {
 683 #if ENABLE(DFG_JIT)
 684     if (!VM::canUseJIT())
 685         return;
 686     for (unsigned i = DFG::numberOfWorklists(); i--;)
<span class="line-modified"> 687         DFG::existingWorklistForIndex(i).removeDeadPlans(*m_vm);</span>
 688 #endif
 689 }
 690 
<span class="line-modified"> 691 bool Heap::isHeapSnapshotting() const</span>
 692 {
<span class="line-modified"> 693     HeapProfiler* heapProfiler = m_vm-&gt;heapProfiler();</span>
 694     if (UNLIKELY(heapProfiler))
<span class="line-modified"> 695         return heapProfiler-&gt;activeSnapshotBuilder();</span>
 696     return false;
 697 }
 698 
<span class="line-modified"> 699 struct GatherHeapSnapshotData : MarkedBlock::CountFunctor {</span>
<span class="line-modified"> 700     GatherHeapSnapshotData(VM&amp; vm, HeapSnapshotBuilder&amp; builder)</span>
 701         : m_vm(vm)
<span class="line-modified"> 702         , m_builder(builder)</span>
 703     {
 704     }
 705 
 706     IterationStatus operator()(HeapCell* heapCell, HeapCell::Kind kind) const
 707     {
 708         if (isJSCellKind(kind)) {
 709             JSCell* cell = static_cast&lt;JSCell*&gt;(heapCell);
<span class="line-modified"> 710             cell-&gt;methodTable(m_vm)-&gt;heapSnapshot(cell, m_builder);</span>
 711         }
 712         return IterationStatus::Continue;
 713     }
 714 
 715     VM&amp; m_vm;
<span class="line-modified"> 716     HeapSnapshotBuilder&amp; m_builder;</span>
 717 };
 718 
<span class="line-modified"> 719 void Heap::gatherExtraHeapSnapshotData(HeapProfiler&amp; heapProfiler)</span>
 720 {
<span class="line-modified"> 721     if (HeapSnapshotBuilder* builder = heapProfiler.activeSnapshotBuilder()) {</span>
 722         HeapIterationScope heapIterationScope(*this);
<span class="line-modified"> 723         GatherHeapSnapshotData functor(*m_vm, *builder);</span>
 724         m_objectSpace.forEachLiveCell(heapIterationScope, functor);
 725     }
 726 }
 727 
 728 struct RemoveDeadHeapSnapshotNodes : MarkedBlock::CountFunctor {
 729     RemoveDeadHeapSnapshotNodes(HeapSnapshot&amp; snapshot)
 730         : m_snapshot(snapshot)
 731     {
 732     }
 733 
 734     IterationStatus operator()(HeapCell* cell, HeapCell::Kind kind) const
 735     {
 736         if (isJSCellKind(kind))
 737             m_snapshot.sweepCell(static_cast&lt;JSCell*&gt;(cell));
 738         return IterationStatus::Continue;
 739     }
 740 
 741     HeapSnapshot&amp; m_snapshot;
 742 };
 743 
</pre>
<hr />
<pre>
 827             JSCell* cell = static_cast&lt;JSCell*&gt;(heapCell);
 828             if (cell-&gt;isObject() &amp;&amp; asObject(cell)-&gt;isGlobalObject())
 829                 result++;
 830             return IterationStatus::Continue;
 831         });
 832     return result;
 833 }
 834 
 835 size_t Heap::protectedObjectCount()
 836 {
 837     size_t result = 0;
 838     forEachProtectedCell(
 839         [&amp;] (JSCell*) {
 840             result++;
 841         });
 842     return result;
 843 }
 844 
 845 std::unique_ptr&lt;TypeCountSet&gt; Heap::protectedObjectTypeCounts()
 846 {
<span class="line-modified"> 847     std::unique_ptr&lt;TypeCountSet&gt; result = std::make_unique&lt;TypeCountSet&gt;();</span>
 848     forEachProtectedCell(
 849         [&amp;] (JSCell* cell) {
<span class="line-modified"> 850             recordType(*vm(), *result, cell);</span>
 851         });
 852     return result;
 853 }
 854 
 855 std::unique_ptr&lt;TypeCountSet&gt; Heap::objectTypeCounts()
 856 {
<span class="line-modified"> 857     std::unique_ptr&lt;TypeCountSet&gt; result = std::make_unique&lt;TypeCountSet&gt;();</span>
 858     HeapIterationScope iterationScope(*this);
 859     m_objectSpace.forEachLiveCell(
 860         iterationScope,
 861         [&amp;] (HeapCell* cell, HeapCell::Kind kind) -&gt; IterationStatus {
 862             if (isJSCellKind(kind))
<span class="line-modified"> 863                 recordType(*vm(), *result, static_cast&lt;JSCell*&gt;(cell));</span>
 864             return IterationStatus::Continue;
 865         });
 866     return result;
 867 }
 868 
 869 void Heap::deleteAllCodeBlocks(DeleteAllCodeEffort effort)
 870 {
 871     if (m_collectionScope &amp;&amp; effort == DeleteAllCodeIfNotCollecting)
 872         return;
 873 
<span class="line-modified"> 874     VM&amp; vm = *m_vm;</span>
 875     PreventCollectionScope preventCollectionScope(*this);
 876 
 877     // If JavaScript is running, it&#39;s not safe to delete all JavaScript code, since
 878     // we&#39;ll end up returning to deleted code.
 879     RELEASE_ASSERT(!vm.entryScope);
 880     RELEASE_ASSERT(!m_collectionScope);
 881 
 882     completeAllJITPlans();
 883 
 884     vm.forEachScriptExecutableSpace(
 885         [&amp;] (auto&amp; spaceAndSet) {
 886             HeapIterationScope heapIterationScope(*this);
 887             auto&amp; set = spaceAndSet.set;
 888             set.forEachLiveCell(
 889                 [&amp;] (HeapCell* cell, HeapCell::Kind) {
 890                     ScriptExecutable* executable = static_cast&lt;ScriptExecutable*&gt;(cell);
 891                     executable-&gt;clearCode(set);
 892                 });
 893         });
 894 
</pre>
<hr />
<pre>
 899         // VM. This could leave Wasm in an inconsistent state where it has an IC that
 900         // points into a CodeBlock that could be dead. The IC will still succeed because
 901         // it uses a callee check, but then it will call into dead code.
 902         HeapIterationScope heapIterationScope(*this);
 903         if (vm.m_webAssemblyCodeBlockSpace) {
 904             vm.m_webAssemblyCodeBlockSpace-&gt;forEachLiveCell([&amp;] (HeapCell* cell, HeapCell::Kind kind) {
 905                 ASSERT_UNUSED(kind, kind == HeapCell::JSCell);
 906                 JSWebAssemblyCodeBlock* codeBlock = static_cast&lt;JSWebAssemblyCodeBlock*&gt;(cell);
 907                 codeBlock-&gt;clearJSCallICs(vm);
 908             });
 909         }
 910     }
 911 #endif
 912 }
 913 
 914 void Heap::deleteAllUnlinkedCodeBlocks(DeleteAllCodeEffort effort)
 915 {
 916     if (m_collectionScope &amp;&amp; effort == DeleteAllCodeIfNotCollecting)
 917         return;
 918 
<span class="line-modified"> 919     VM&amp; vm = *m_vm;</span>
 920     PreventCollectionScope preventCollectionScope(*this);
 921 
 922     RELEASE_ASSERT(!m_collectionScope);
 923 
 924     HeapIterationScope heapIterationScope(*this);
 925     vm.unlinkedFunctionExecutableSpace.set.forEachLiveCell(
 926         [&amp;] (HeapCell* cell, HeapCell::Kind) {
 927             UnlinkedFunctionExecutable* executable = static_cast&lt;UnlinkedFunctionExecutable*&gt;(cell);
 928             executable-&gt;clearCode(vm);
 929         });
 930 }
 931 
 932 void Heap::deleteUnmarkedCompiledCode()
 933 {
<span class="line-modified"> 934     vm()-&gt;forEachScriptExecutableSpace([] (auto&amp; space) { space.space.sweep(); });</span>
<span class="line-modified"> 935     vm()-&gt;forEachCodeBlockSpace([] (auto&amp; space) { space.space.sweep(); }); // Sweeping must occur before deleting stubs, otherwise the stubs might still think they&#39;re alive as they get deleted.</span>
 936     m_jitStubRoutines-&gt;deleteUnmarkedJettisonedStubRoutines();
 937 }
 938 
 939 void Heap::addToRememberedSet(const JSCell* constCell)
 940 {
 941     JSCell* cell = const_cast&lt;JSCell*&gt;(constCell);
 942     ASSERT(cell);
 943     ASSERT(!Options::useConcurrentJIT() || !isCompilationThread());
 944     m_barriersExecuted++;
 945     if (m_mutatorShouldBeFenced) {
 946         WTF::loadLoadFence();
 947         if (!isMarked(cell)) {
 948             // During a full collection a store into an unmarked object that had surivived past
 949             // collections will manifest as a store to an unmarked PossiblyBlack object. If the
 950             // object gets marked at some time after this then it will go down the normal marking
 951             // path. So, we don&#39;t have to remember this object. We could return here. But we go
 952             // further and attempt to re-white the object.
 953 
 954             RELEASE_ASSERT(m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full);
 955 
</pre>
<hr />
<pre>
 964                 //
 965                 // In this case we would have made the object white again, even though it should
 966                 // be black. This check lets us correct our mistake. This relies on the fact that
 967                 // isMarked converges monotonically to true.
 968                 if (isMarked(cell)) {
 969                     // It&#39;s difficult to work out whether the object should be grey or black at
 970                     // this point. We say black conservatively.
 971                     cell-&gt;setCellState(CellState::PossiblyBlack);
 972                 }
 973 
 974                 // Either way, we can return. Most likely, the object was not marked, and so the
 975                 // object is now labeled white. This means that future barrier executions will not
 976                 // fire. In the unlikely event that the object had become marked, we can still
 977                 // return anyway, since we proved that the object was not marked at the time that
 978                 // we executed this slow path.
 979             }
 980 
 981             return;
 982         }
 983     } else
<span class="line-modified"> 984         ASSERT(Heap::isMarked(cell));</span>
 985     // It could be that the object was *just* marked. This means that the collector may set the
 986     // state to DefinitelyGrey and then to PossiblyOldOrBlack at any time. It&#39;s OK for us to
 987     // race with the collector here. If we win then this is accurate because the object _will_
 988     // get scanned again. If we lose then someone else will barrier the object again. That would
 989     // be unfortunate but not the end of the world.
 990     cell-&gt;setCellState(CellState::PossiblyGrey);
 991     m_mutatorMarkStack-&gt;append(cell);
 992 }
 993 
 994 void Heap::sweepSynchronously()
 995 {
 996     MonotonicTime before { };
 997     if (Options::logGC()) {
 998         dataLog(&quot;Full sweep: &quot;, capacity() / 1024, &quot;kb &quot;);
 999         before = MonotonicTime::now();
1000     }
1001     m_objectSpace.sweep();
1002     m_objectSpace.shrink();
1003     if (Options::logGC()) {
1004         MonotonicTime after = MonotonicTime::now();
</pre>
<hr />
<pre>
1109             return;
1110         case RunCurrentPhaseResult::Continue:
1111             break;
1112         case RunCurrentPhaseResult::NeedCurrentThreadState:
1113             RELEASE_ASSERT_NOT_REACHED();
1114             break;
1115         }
1116     }
1117 }
1118 
1119 ALWAYS_INLINE int asInt(CollectorPhase phase)
1120 {
1121     return static_cast&lt;int&gt;(phase);
1122 }
1123 
1124 void Heap::checkConn(GCConductor conn)
1125 {
1126     unsigned worldState = m_worldState.load();
1127     switch (conn) {
1128     case GCConductor::Mutator:
<span class="line-modified">1129         RELEASE_ASSERT(worldState &amp; mutatorHasConnBit, worldState, asInt(m_lastPhase), asInt(m_currentPhase), asInt(m_nextPhase), vm()-&gt;id(), VM::numberOfIDs(), vm()-&gt;isEntered());</span>
1130         return;
1131     case GCConductor::Collector:
<span class="line-modified">1132         RELEASE_ASSERT(!(worldState &amp; mutatorHasConnBit), worldState, asInt(m_lastPhase), asInt(m_currentPhase), asInt(m_nextPhase), vm()-&gt;id(), VM::numberOfIDs(), vm()-&gt;isEntered());</span>
1133         return;
1134     }
1135     RELEASE_ASSERT_NOT_REACHED();
1136 }
1137 
1138 auto Heap::runCurrentPhase(GCConductor conn, CurrentThreadState* currentThreadState) -&gt; RunCurrentPhaseResult
1139 {
1140     checkConn(conn);
1141     m_currentThreadState = currentThreadState;
<span class="line-modified">1142     m_currentThread = &amp;WTF::Thread::current();</span>
1143 
1144     if (conn == GCConductor::Mutator)
1145         sanitizeStackForVM(vm());
1146 
1147     // If the collector transfers the conn to the mutator, it leaves us in between phases.
1148     if (!finishChangingPhase(conn)) {
1149         // A mischevious mutator could repeatedly relinquish the conn back to us. We try to avoid doing
1150         // this, but it&#39;s probably not the end of the world if it did happen.
1151         if (false)
1152             dataLog(&quot;Conn bounce-back.\n&quot;);
1153         return RunCurrentPhaseResult::Finished;
1154     }
1155 
1156     bool result = false;
1157     switch (m_currentPhase) {
1158     case CollectorPhase::NotRunning:
1159         result = runNotRunningPhase(conn);
1160         break;
1161 
1162     case CollectorPhase::Begin:
</pre>
<hr />
<pre>
1236         m_collectorSlotVisitor-&gt;clearMarkStacks();
1237         m_mutatorMarkStack-&gt;clear();
1238     }
1239 
1240     RELEASE_ASSERT(m_raceMarkStack-&gt;isEmpty());
1241 
1242     beginMarking();
1243 
1244     forEachSlotVisitor(
1245         [&amp;] (SlotVisitor&amp; visitor) {
1246             visitor.didStartMarking();
1247         });
1248 
1249     m_parallelMarkersShouldExit = false;
1250 
1251     m_helperClient.setFunction(
1252         [this] () {
1253             SlotVisitor* slotVisitor;
1254             {
1255                 LockHolder locker(m_parallelSlotVisitorLock);
<span class="line-modified">1256                 if (m_availableParallelSlotVisitors.isEmpty()) {</span>
<span class="line-removed">1257                     std::unique_ptr&lt;SlotVisitor&gt; newVisitor = std::make_unique&lt;SlotVisitor&gt;(</span>
<span class="line-removed">1258                         *this, toCString(&quot;P&quot;, m_parallelSlotVisitors.size() + 1));</span>
<span class="line-removed">1259 </span>
<span class="line-removed">1260                     if (Options::optimizeParallelSlotVisitorsForStoppedMutator())</span>
<span class="line-removed">1261                         newVisitor-&gt;optimizeForStoppedMutator();</span>
<span class="line-removed">1262 </span>
<span class="line-removed">1263                     newVisitor-&gt;didStartMarking();</span>
<span class="line-removed">1264 </span>
<span class="line-removed">1265                     slotVisitor = newVisitor.get();</span>
<span class="line-removed">1266                     m_parallelSlotVisitors.append(WTFMove(newVisitor));</span>
<span class="line-removed">1267                 } else</span>
1268                 slotVisitor = m_availableParallelSlotVisitors.takeLast();
1269             }
1270 
<span class="line-modified">1271             WTF::registerGCThread(GCThreadType::Helper);</span>
1272 
1273             {
1274                 ParallelModeEnabler parallelModeEnabler(*slotVisitor);
1275                 slotVisitor-&gt;drainFromShared(SlotVisitor::SlaveDrain);
1276             }
1277 
1278             {
1279                 LockHolder locker(m_parallelSlotVisitorLock);
1280                 m_availableParallelSlotVisitors.append(slotVisitor);
1281             }
1282         });
1283 
1284     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1285 
1286     m_constraintSet-&gt;didStartMarking();
1287 
1288     m_scheduler-&gt;beginCollection();
1289     if (Options::logGC())
1290         m_scheduler-&gt;log();
1291 
</pre>
<hr />
<pre>
1447     {
1448         auto locker = holdLock(m_markingMutex);
1449         m_parallelMarkersShouldExit = true;
1450         m_markingConditionVariable.notifyAll();
1451     }
1452     m_helperClient.finish();
1453 
1454     iterateExecutingAndCompilingCodeBlocks(
1455         [&amp;] (CodeBlock* codeBlock) {
1456             writeBarrier(codeBlock);
1457         });
1458 
1459     updateObjectCounts();
1460     endMarking();
1461 
1462     if (UNLIKELY(m_verifier)) {
1463         m_verifier-&gt;gatherLiveCells(HeapVerifier::Phase::AfterMarking);
1464         m_verifier-&gt;verify(HeapVerifier::Phase::AfterMarking);
1465     }
1466 
<span class="line-modified">1467     if (vm()-&gt;typeProfiler())</span>
<span class="line-modified">1468         vm()-&gt;typeProfiler()-&gt;invalidateTypeSetCache();</span>
<span class="line-removed">1469 </span>
<span class="line-removed">1470     if (ValueProfile* profile = vm()-&gt;noJITValueProfileSingleton.get())</span>
<span class="line-removed">1471         *profile = ValueProfile(0);</span>
1472 
1473     reapWeakHandles();
1474     pruneStaleEntriesFromWeakGCMaps();
1475     sweepArrayBuffers();
1476     snapshotUnswept();
1477     finalizeUnconditionalFinalizers();
1478     removeDeadCompilerWorklistEntries();
1479     notifyIncrementalSweeper();
1480 
1481     m_codeBlocks-&gt;iterateCurrentlyExecuting(
1482         [&amp;] (CodeBlock* codeBlock) {
1483             writeBarrier(codeBlock);
1484         });
1485     m_codeBlocks-&gt;clearCurrentlyExecuting();
1486 
1487     m_objectSpace.prepareForAllocation();
1488     updateAllocationLimits();
1489 
1490     if (UNLIKELY(m_verifier)) {
1491         m_verifier-&gt;trimDeadCells();
</pre>
<hr />
<pre>
1590         dataLog(&quot;FATAL: world already stopped.\n&quot;);
1591         RELEASE_ASSERT_NOT_REACHED();
1592     }
1593 
1594     if (m_mutatorDidRun)
1595         m_mutatorExecutionVersion++;
1596 
1597     m_mutatorDidRun = false;
1598 
1599     suspendCompilerThreads();
1600     m_worldIsStopped = true;
1601 
1602     forEachSlotVisitor(
1603         [&amp;] (SlotVisitor&amp; slotVisitor) {
1604             slotVisitor.updateMutatorIsStopped(NoLockingNecessary);
1605         });
1606 
1607 #if ENABLE(JIT)
1608     if (VM::canUseJIT()) {
1609         DeferGCForAWhile awhile(*this);
<span class="line-modified">1610         if (JITWorklist::ensureGlobalWorklist().completeAllForVM(*m_vm)</span>
1611             &amp;&amp; conn == GCConductor::Collector)
1612             setGCDidJIT();
1613     }
1614 #endif // ENABLE(JIT)
1615     UNUSED_PARAM(conn);
1616 
<span class="line-modified">1617     if (auto* shadowChicken = vm()-&gt;shadowChicken())</span>
<span class="line-modified">1618         shadowChicken-&gt;update(*vm(), vm()-&gt;topCallFrame);</span>
1619 
1620     m_structureIDTable.flushOldTables();
1621     m_objectSpace.stopAllocating();
1622 
1623     m_stopTime = MonotonicTime::now();
1624 }
1625 
1626 NEVER_INLINE void Heap::resumeThePeriphery()
1627 {
1628     // Calling resumeAllocating does the Right Thing depending on whether this is the end of a
1629     // collection cycle or this is just a concurrent phase within a collection cycle:
1630     // - At end of collection cycle: it&#39;s a no-op because prepareForAllocation already cleared the
1631     //   last active block.
1632     // - During collection cycle: it reinstates the last active block.
1633     m_objectSpace.resumeAllocating();
1634 
1635     m_barriersExecuted = 0;
1636 
1637     if (!m_worldIsStopped) {
1638         dataLog(&quot;Fatal: collector does not believe that the world is stopped.\n&quot;);
</pre>
<hr />
<pre>
1651     forEachSlotVisitor(
1652         [&amp;] (SlotVisitor&amp; slotVisitor) {
1653             slotVisitorsToUpdate.append(&amp;slotVisitor);
1654         });
1655 
1656     for (unsigned countdown = 40; !slotVisitorsToUpdate.isEmpty() &amp;&amp; countdown--;) {
1657         for (unsigned index = 0; index &lt; slotVisitorsToUpdate.size(); ++index) {
1658             SlotVisitor&amp; slotVisitor = *slotVisitorsToUpdate[index];
1659             bool remove = false;
1660             if (slotVisitor.hasAcknowledgedThatTheMutatorIsResumed())
1661                 remove = true;
1662             else if (auto locker = tryHoldLock(slotVisitor.rightToRun())) {
1663                 slotVisitor.updateMutatorIsStopped(locker);
1664                 remove = true;
1665             }
1666             if (remove) {
1667                 slotVisitorsToUpdate[index--] = slotVisitorsToUpdate.last();
1668                 slotVisitorsToUpdate.takeLast();
1669             }
1670         }
<span class="line-modified">1671         WTF::Thread::yield();</span>
1672     }
1673 
1674     for (SlotVisitor* slotVisitor : slotVisitorsToUpdate)
1675         slotVisitor-&gt;updateMutatorIsStopped();
1676 
1677     resumeCompilerThreads();
1678 }
1679 
1680 bool Heap::stopTheMutator()
1681 {
1682     for (;;) {
1683         unsigned oldState = m_worldState.load();
1684         if (oldState &amp; stoppedBit) {
1685             RELEASE_ASSERT(!(oldState &amp; hasAccessBit));
1686             RELEASE_ASSERT(!(oldState &amp; mutatorWaitingBit));
1687             RELEASE_ASSERT(!(oldState &amp; mutatorHasConnBit));
1688             return true;
1689         }
1690 
1691         if (oldState &amp; mutatorHasConnBit) {
</pre>
<hr />
<pre>
2039     m_threadIsStopping = true;
2040     clearMutatorWaiting();
2041     ParkingLot::unparkAll(&amp;m_worldState);
2042 }
2043 
2044 void Heap::finalize()
2045 {
2046     MonotonicTime before;
2047     if (Options::logGC()) {
2048         before = MonotonicTime::now();
2049         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: finalize &quot;);
2050     }
2051 
2052     {
2053         SweepingScope sweepingScope(*this);
2054         deleteUnmarkedCompiledCode();
2055         deleteSourceProviderCaches();
2056         sweepInFinalize();
2057     }
2058 
<span class="line-modified">2059     if (HasOwnPropertyCache* cache = vm()-&gt;hasOwnPropertyCache())</span>
2060         cache-&gt;clear();
2061 
2062     immutableButterflyToStringCache.clear();
2063 
2064     for (const HeapFinalizerCallback&amp; callback : m_heapFinalizerCallbacks)
<span class="line-modified">2065         callback.run(*vm());</span>
2066 
2067     if (shouldSweepSynchronously())
2068         sweepSynchronously();
2069 
2070     if (Options::logGC()) {
2071         MonotonicTime after = MonotonicTime::now();
2072         dataLog((after - before).milliseconds(), &quot;ms]\n&quot;);
2073     }
2074 }
2075 
2076 Heap::Ticket Heap::requestCollection(GCRequest request)
2077 {
2078     stopIfNecessary();
2079 
<span class="line-modified">2080     ASSERT(vm()-&gt;currentThreadIsHoldingAPILock());</span>
<span class="line-modified">2081     RELEASE_ASSERT(vm()-&gt;atomicStringTable() == WTF::Thread::current().atomicStringTable());</span>
2082 
2083     LockHolder locker(*m_threadLock);
2084     // We may be able to steal the conn. That only works if the collector is definitely not running
2085     // right now. This is an optimization that prevents the collector thread from ever starting in most
2086     // cases.
2087     ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
<span class="line-modified">2088     if ((m_lastServedTicket == m_lastGrantedTicket) &amp;&amp; (m_currentPhase == CollectorPhase::NotRunning)) {</span>
2089         if (false)
2090             dataLog(&quot;Taking the conn.\n&quot;);
2091         m_worldState.exchangeOr(mutatorHasConnBit);
2092     }
2093 
2094     m_requests.append(request);
2095     m_lastGrantedTicket++;
2096     if (!(m_worldState.load() &amp; mutatorHasConnBit))
2097         m_threadCondition-&gt;notifyOne(locker);
2098     return m_lastGrantedTicket;
2099 }
2100 
2101 void Heap::waitForCollection(Ticket ticket)
2102 {
2103     waitForCollector(
2104         [&amp;] (const AbstractLocker&amp;) -&gt; bool {
2105             return m_lastServedTicket &gt;= ticket;
2106         });
2107 }
2108 
2109 void Heap::sweepInFinalize()
2110 {
2111     m_objectSpace.sweepLargeAllocations();
<span class="line-modified">2112     vm()-&gt;eagerlySweptDestructibleObjectSpace.sweep();</span>
2113 }
2114 
2115 void Heap::suspendCompilerThreads()
2116 {
2117 #if ENABLE(DFG_JIT)
2118     // We ensure the worklists so that it&#39;s not possible for the mutator to start a new worklist
2119     // after we have suspended the ones that he had started before. That&#39;s not very expensive since
2120     // the worklists use AutomaticThreads anyway.
2121     if (!VM::canUseJIT())
2122         return;
2123     for (unsigned i = DFG::numberOfWorklists(); i--;)
2124         DFG::ensureWorklistForIndex(i).suspendAllThreads();
2125 #endif
2126 }
2127 
2128 void Heap::willStartCollection()
2129 {
2130     if (Options::logGC())
2131         dataLog(&quot;=&gt; &quot;);
2132 
</pre>
<hr />
<pre>
2169 void Heap::prepareForMarking()
2170 {
2171     m_objectSpace.prepareForMarking();
2172 }
2173 
2174 void Heap::reapWeakHandles()
2175 {
2176     m_objectSpace.reapWeakSets();
2177 }
2178 
2179 void Heap::pruneStaleEntriesFromWeakGCMaps()
2180 {
2181     if (!m_collectionScope || m_collectionScope.value() != CollectionScope::Full)
2182         return;
2183     for (WeakGCMapBase* weakGCMap : m_weakGCMaps)
2184         weakGCMap-&gt;pruneStaleEntries();
2185 }
2186 
2187 void Heap::sweepArrayBuffers()
2188 {
<span class="line-modified">2189     m_arrayBuffers.sweep();</span>
2190 }
2191 
2192 void Heap::snapshotUnswept()
2193 {
2194     TimingScope timingScope(*this, &quot;Heap::snapshotUnswept&quot;);
2195     m_objectSpace.snapshotUnswept();
2196 }
2197 
2198 void Heap::deleteSourceProviderCaches()
2199 {
2200     if (m_lastCollectionScope &amp;&amp; m_lastCollectionScope.value() == CollectionScope::Full)
<span class="line-modified">2201         m_vm-&gt;clearSourceProviderCaches();</span>
2202 }
2203 
2204 void Heap::notifyIncrementalSweeper()
2205 {
2206     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2207         if (!m_logicallyEmptyWeakBlocks.isEmpty())
2208             m_indexOfNextLogicallyEmptyWeakBlockToSweep = 0;
2209     }
2210 
2211     m_sweeper-&gt;startSweeping(*this);
2212 }
2213 
2214 void Heap::updateAllocationLimits()
2215 {
2216     static const bool verbose = false;
2217 
2218     if (verbose) {
2219         dataLog(&quot;\n&quot;);
2220         dataLog(&quot;bytesAllocatedThisCycle = &quot;, m_bytesAllocatedThisCycle, &quot;\n&quot;);
2221     }
</pre>
<hr />
<pre>
2301         dataLog(&quot;sizeAfterLastCollect = &quot;, m_sizeAfterLastCollect, &quot;\n&quot;);
2302     m_bytesAllocatedThisCycle = 0;
2303 
2304     if (Options::logGC())
2305         dataLog(&quot;=&gt; &quot;, currentHeapSize / 1024, &quot;kb, &quot;);
2306 }
2307 
2308 void Heap::didFinishCollection()
2309 {
2310     m_afterGC = MonotonicTime::now();
2311     CollectionScope scope = *m_collectionScope;
2312     if (scope == CollectionScope::Full)
2313         m_lastFullGCLength = m_afterGC - m_beforeGC;
2314     else
2315         m_lastEdenGCLength = m_afterGC - m_beforeGC;
2316 
2317 #if ENABLE(RESOURCE_USAGE)
2318     ASSERT(externalMemorySize() &lt;= extraMemorySize());
2319 #endif
2320 
<span class="line-modified">2321     if (HeapProfiler* heapProfiler = m_vm-&gt;heapProfiler()) {</span>
<span class="line-modified">2322         gatherExtraHeapSnapshotData(*heapProfiler);</span>
2323         removeDeadHeapSnapshotNodes(*heapProfiler);
2324     }
2325 
2326     if (UNLIKELY(m_verifier))
2327         m_verifier-&gt;endGC();
2328 
2329     RELEASE_ASSERT(m_collectionScope);
2330     m_lastCollectionScope = m_collectionScope;
2331     m_collectionScope = WTF::nullopt;
2332 
2333     for (auto* observer : m_observers)
2334         observer-&gt;didGarbageCollect(scope);
2335 }
2336 
2337 void Heap::resumeCompilerThreads()
2338 {
2339 #if ENABLE(DFG_JIT)
2340     if (!VM::canUseJIT())
2341         return;
2342     for (unsigned i = DFG::numberOfWorklists(); i--;)
</pre>
<hr />
<pre>
2504 void Heap::forEachCodeBlockIgnoringJITPlansImpl(const AbstractLocker&amp; locker, const ScopedLambda&lt;void(CodeBlock*)&gt;&amp; func)
2505 {
2506     return m_codeBlocks-&gt;iterate(locker, func);
2507 }
2508 
2509 void Heap::writeBarrierSlowPath(const JSCell* from)
2510 {
2511     if (UNLIKELY(mutatorShouldBeFenced())) {
2512         // In this case, the barrierThreshold is the tautological threshold, so from could still be
2513         // not black. But we can&#39;t know for sure until we fire off a fence.
2514         WTF::storeLoadFence();
2515         if (from-&gt;cellState() != CellState::PossiblyBlack)
2516             return;
2517     }
2518 
2519     addToRememberedSet(from);
2520 }
2521 
2522 bool Heap::isCurrentThreadBusy()
2523 {
<span class="line-modified">2524     return mayBeGCThread() || mutatorState() != MutatorState::Running;</span>
2525 }
2526 
2527 void Heap::reportExtraMemoryVisited(size_t size)
2528 {
2529     size_t* counter = &amp;m_extraMemorySize;
2530 
2531     for (;;) {
2532         size_t oldSize = *counter;
2533         // FIXME: Change this to use SaturatedArithmetic when available.
2534         // https://bugs.webkit.org/show_bug.cgi?id=170411
2535         Checked&lt;size_t, RecordOverflow&gt; checkedNewSize = oldSize;
2536         checkedNewSize += size;
2537         size_t newSize = UNLIKELY(checkedNewSize.hasOverflowed()) ? std::numeric_limits&lt;size_t&gt;::max() : checkedNewSize.unsafeGet();
2538         if (WTF::atomicCompareExchangeWeakRelaxed(counter, oldSize, newSize))
2539             return;
2540     }
2541 }
2542 
2543 #if ENABLE(RESOURCE_USAGE)
2544 void Heap::reportExternalMemoryVisited(size_t size)
</pre>
<hr />
<pre>
2643 void Heap::didFreeBlock(size_t capacity)
2644 {
2645 #if ENABLE(RESOURCE_USAGE)
2646     m_blockBytesAllocated -= capacity;
2647 #else
2648     UNUSED_PARAM(capacity);
2649 #endif
2650 }
2651 
2652 void Heap::addCoreConstraints()
2653 {
2654     m_constraintSet-&gt;add(
2655         &quot;Cs&quot;, &quot;Conservative Scan&quot;,
2656         [this, lastVersion = static_cast&lt;uint64_t&gt;(0)] (SlotVisitor&amp; slotVisitor) mutable {
2657             bool shouldNotProduceWork = lastVersion == m_phaseVersion;
2658             if (shouldNotProduceWork)
2659                 return;
2660 
2661             TimingScope preConvergenceTimingScope(*this, &quot;Constraint: conservative scan&quot;);
2662             m_objectSpace.prepareForConservativeScan();

2663 
2664             {
2665                 ConservativeRoots conservativeRoots(*this);
2666                 SuperSamplerScope superSamplerScope(false);
2667 
2668                 gatherStackRoots(conservativeRoots);
2669                 gatherJSStackRoots(conservativeRoots);
2670                 gatherScratchBufferRoots(conservativeRoots);
2671 
2672                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ConservativeScan);
2673                 slotVisitor.append(conservativeRoots);
2674             }
2675             if (VM::canUseJIT()) {
2676                 // JITStubRoutines must be visited after scanning ConservativeRoots since JITStubRoutines depend on the hook executed during gathering ConservativeRoots.
2677                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::JITStubRoutines);
2678                 m_jitStubRoutines-&gt;traceMarkedStubRoutines(slotVisitor);
2679             }
2680 
2681             lastVersion = m_phaseVersion;
2682         },
2683         ConstraintVolatility::GreyedByExecution);
2684 
2685     m_constraintSet-&gt;add(
2686         &quot;Msr&quot;, &quot;Misc Small Roots&quot;,
2687         [this] (SlotVisitor&amp; slotVisitor) {
2688 
2689 #if JSC_OBJC_API_ENABLED
<span class="line-modified">2690             scanExternalRememberedSet(*m_vm, slotVisitor);</span>
2691 #endif
<span class="line-modified">2692             if (m_vm-&gt;smallStrings.needsToBeVisited(*m_collectionScope)) {</span>
2693                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::StrongReferences);
<span class="line-modified">2694                 m_vm-&gt;smallStrings.visitStrongReferences(slotVisitor);</span>
2695             }
2696 
2697             {
2698                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ProtectedValues);
2699                 for (auto&amp; pair : m_protectedValues)
2700                     slotVisitor.appendUnbarriered(pair.key);
2701             }
2702 
2703             if (m_markListSet &amp;&amp; m_markListSet-&gt;size()) {
2704                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ConservativeScan);
2705                 MarkedArgumentBuffer::markLists(slotVisitor, *m_markListSet);
2706             }
2707 
2708             {
2709                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::VMExceptions);
<span class="line-modified">2710                 slotVisitor.appendUnbarriered(m_vm-&gt;exception());</span>
<span class="line-modified">2711                 slotVisitor.appendUnbarriered(m_vm-&gt;lastException());</span>
2712             }
2713         },
2714         ConstraintVolatility::GreyedByExecution);
2715 
2716     m_constraintSet-&gt;add(
2717         &quot;Sh&quot;, &quot;Strong Handles&quot;,
2718         [this] (SlotVisitor&amp; slotVisitor) {
2719             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::StrongHandles);
2720             m_handleSet.visitStrongHandles(slotVisitor);
2721         },
2722         ConstraintVolatility::GreyedByExecution);
2723 
2724     m_constraintSet-&gt;add(
2725         &quot;D&quot;, &quot;Debugger&quot;,
2726         [this] (SlotVisitor&amp; slotVisitor) {
2727             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::Debugger);
2728 
2729 #if ENABLE(SAMPLING_PROFILER)
<span class="line-modified">2730             if (SamplingProfiler* samplingProfiler = m_vm-&gt;samplingProfiler()) {</span>
2731                 LockHolder locker(samplingProfiler-&gt;getLock());
2732                 samplingProfiler-&gt;processUnverifiedStackTraces();
2733                 samplingProfiler-&gt;visit(slotVisitor);
2734                 if (Options::logGC() == GCLogging::Verbose)
2735                     dataLog(&quot;Sampling Profiler data:\n&quot;, slotVisitor);
2736             }
2737 #endif // ENABLE(SAMPLING_PROFILER)
2738 
<span class="line-modified">2739             if (m_vm-&gt;typeProfiler())</span>
<span class="line-modified">2740                 m_vm-&gt;typeProfilerLog()-&gt;visit(slotVisitor);</span>
2741 
<span class="line-modified">2742             if (auto* shadowChicken = m_vm-&gt;shadowChicken())</span>
2743                 shadowChicken-&gt;visitChildren(slotVisitor);
2744         },
2745         ConstraintVolatility::GreyedByExecution);
2746 
2747     m_constraintSet-&gt;add(
2748         &quot;Ws&quot;, &quot;Weak Sets&quot;,
2749         [this] (SlotVisitor&amp; slotVisitor) {
2750             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::WeakSets);
2751             m_objectSpace.visitWeakSets(slotVisitor);
2752         },
2753         ConstraintVolatility::GreyedByMarking);
2754 
2755     m_constraintSet-&gt;add(
2756         &quot;O&quot;, &quot;Output&quot;,
2757         [] (SlotVisitor&amp; slotVisitor) {
2758             VM&amp; vm = slotVisitor.vm();
2759 
2760             auto callOutputConstraint = [] (SlotVisitor&amp; slotVisitor, HeapCell* heapCell, HeapCell::Kind) {
2761                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::Output);
2762                 VM&amp; vm = slotVisitor.vm();
</pre>
<hr />
<pre>
2771             add(vm.executableToCodeBlockEdgesWithConstraints);
2772             if (vm.m_weakMapSpace)
2773                 add(*vm.m_weakMapSpace);
2774         },
2775         ConstraintVolatility::GreyedByMarking,
2776         ConstraintParallelism::Parallel);
2777 
2778 #if ENABLE(DFG_JIT)
2779     if (VM::canUseJIT()) {
2780         m_constraintSet-&gt;add(
2781             &quot;Dw&quot;, &quot;DFG Worklists&quot;,
2782             [this] (SlotVisitor&amp; slotVisitor) {
2783                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::DFGWorkLists);
2784 
2785                 for (unsigned i = DFG::numberOfWorklists(); i--;)
2786                     DFG::existingWorklistForIndex(i).visitWeakReferences(slotVisitor);
2787 
2788                 // FIXME: This is almost certainly unnecessary.
2789                 // https://bugs.webkit.org/show_bug.cgi?id=166829
2790                 DFG::iterateCodeBlocksForGC(
<span class="line-modified">2791                     *m_vm,</span>
2792                     [&amp;] (CodeBlock* codeBlock) {
2793                         slotVisitor.appendUnbarriered(codeBlock);
2794                     });
2795 
2796                 if (Options::logGC() == GCLogging::Verbose)
2797                     dataLog(&quot;DFG Worklists:\n&quot;, slotVisitor);
2798             },
2799             ConstraintVolatility::GreyedByMarking);
2800     }
2801 #endif
2802 
2803     m_constraintSet-&gt;add(
2804         &quot;Cb&quot;, &quot;CodeBlocks&quot;,
2805         [this] (SlotVisitor&amp; slotVisitor) {
2806             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::CodeBlocks);
2807             iterateExecutingAndCompilingCodeBlocksWithoutHoldingLocks(
2808                 [&amp;] (CodeBlock* codeBlock) {
2809                     // Visit the CodeBlock as a constraint only if it&#39;s black.
<span class="line-modified">2810                     if (Heap::isMarked(codeBlock)</span>
2811                         &amp;&amp; codeBlock-&gt;cellState() == CellState::PossiblyBlack)
2812                         slotVisitor.visitAsConstraint(codeBlock);
2813                 });
2814         },
2815         ConstraintVolatility::SeldomGreyed);
2816 
<span class="line-modified">2817     m_constraintSet-&gt;add(std::make_unique&lt;MarkStackMergingConstraint&gt;(*this));</span>
2818 }
2819 
2820 void Heap::addMarkingConstraint(std::unique_ptr&lt;MarkingConstraint&gt; constraint)
2821 {
2822     PreventCollectionScope preventCollectionScope(*this);
2823     m_constraintSet-&gt;add(WTFMove(constraint));
2824 }
2825 
2826 void Heap::notifyIsSafeToCollect()
2827 {
2828     MonotonicTime before;
2829     if (Options::logGC()) {
2830         before = MonotonicTime::now();
2831         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: starting &quot;);
2832     }
2833 
2834     addCoreConstraints();
2835 
2836     m_isSafeToCollect = true;
2837 
2838     if (Options::collectContinuously()) {
<span class="line-modified">2839         m_collectContinuouslyThread = WTF::Thread::create(</span>
2840             &quot;JSC DEBUG Continuous GC&quot;,
2841             [this] () {
2842                 MonotonicTime initialTime = MonotonicTime::now();
2843                 Seconds period = Seconds::fromMilliseconds(Options::collectContinuouslyPeriodMS());
2844                 while (!m_shouldStopCollectingContinuously) {
2845                     {
2846                         LockHolder locker(*m_threadLock);
2847                         if (m_requests.isEmpty()) {
2848                             m_requests.append(WTF::nullopt);
2849                             m_lastGrantedTicket++;
2850                             m_threadCondition-&gt;notifyOne(locker);
2851                         }
2852                     }
2853 
2854                     {
2855                         LockHolder locker(m_collectContinuouslyLock);
2856                         Seconds elapsed = MonotonicTime::now() - initialTime;
2857                         Seconds elapsedInPeriod = elapsed % period;
2858                         MonotonicTime timeToWakeUp =
2859                             initialTime + elapsed - elapsedInPeriod + period;
</pre>
</td>
<td>
<hr />
<pre>
   5  *  This library is free software; you can redistribute it and/or
   6  *  modify it under the terms of the GNU Lesser General Public
   7  *  License as published by the Free Software Foundation; either
   8  *  version 2 of the License, or (at your option) any later version.
   9  *
  10  *  This library is distributed in the hope that it will be useful,
  11  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
  12  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
  13  *  Lesser General Public License for more details.
  14  *
  15  *  You should have received a copy of the GNU Lesser General Public
  16  *  License along with this library; if not, write to the Free Software
  17  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
  18  *
  19  */
  20 
  21 #include &quot;config.h&quot;
  22 #include &quot;Heap.h&quot;
  23 
  24 #include &quot;BlockDirectoryInlines.h&quot;
<span class="line-added">  25 #include &quot;BuiltinExecutables.h&quot;</span>
  26 #include &quot;CodeBlock.h&quot;
  27 #include &quot;CodeBlockSetInlines.h&quot;
  28 #include &quot;CollectingScope.h&quot;
  29 #include &quot;ConservativeRoots.h&quot;
  30 #include &quot;DFGWorklistInlines.h&quot;
  31 #include &quot;EdenGCActivityCallback.h&quot;
  32 #include &quot;Exception.h&quot;
  33 #include &quot;FullGCActivityCallback.h&quot;
<span class="line-added">  34 #include &quot;FunctionExecutableInlines.h&quot;</span>
  35 #include &quot;GCActivityCallback.h&quot;
  36 #include &quot;GCIncomingRefCountedSetInlines.h&quot;
  37 #include &quot;GCSegmentedArrayInlines.h&quot;
  38 #include &quot;GCTypeMap.h&quot;
  39 #include &quot;HasOwnPropertyCache.h&quot;
  40 #include &quot;HeapHelperPool.h&quot;
  41 #include &quot;HeapIterationScope.h&quot;
  42 #include &quot;HeapProfiler.h&quot;
  43 #include &quot;HeapSnapshot.h&quot;
  44 #include &quot;HeapVerifier.h&quot;
  45 #include &quot;IncrementalSweeper.h&quot;
  46 #include &quot;InferredValueInlines.h&quot;
  47 #include &quot;Interpreter.h&quot;
  48 #include &quot;IsoCellSetInlines.h&quot;
  49 #include &quot;JITStubRoutineSet.h&quot;
  50 #include &quot;JITWorklist.h&quot;
  51 #include &quot;JSCInlines.h&quot;
  52 #include &quot;JSGlobalObject.h&quot;
  53 #include &quot;JSLock.h&quot;
  54 #include &quot;JSVirtualMachineInternal.h&quot;
  55 #include &quot;JSWeakMap.h&quot;
<span class="line-added">  56 #include &quot;JSWeakObjectRef.h&quot;</span>
  57 #include &quot;JSWeakSet.h&quot;
  58 #include &quot;JSWebAssemblyCodeBlock.h&quot;
  59 #include &quot;MachineStackMarker.h&quot;
  60 #include &quot;MarkStackMergingConstraint.h&quot;
  61 #include &quot;MarkedSpaceInlines.h&quot;
  62 #include &quot;MarkingConstraintSet.h&quot;
  63 #include &quot;PreventCollectionScope.h&quot;
  64 #include &quot;SamplingProfiler.h&quot;
  65 #include &quot;ShadowChicken.h&quot;
  66 #include &quot;SpaceTimeMutatorScheduler.h&quot;
  67 #include &quot;StochasticSpaceTimeMutatorScheduler.h&quot;
  68 #include &quot;StopIfNecessaryTimer.h&quot;
  69 #include &quot;SubspaceInlines.h&quot;
  70 #include &quot;SuperSampler.h&quot;
  71 #include &quot;SweepingScope.h&quot;
<span class="line-added">  72 #include &quot;SymbolTableInlines.h&quot;</span>
  73 #include &quot;SynchronousStopTheWorldMutatorScheduler.h&quot;
  74 #include &quot;TypeProfiler.h&quot;
  75 #include &quot;TypeProfilerLog.h&quot;
  76 #include &quot;UnlinkedCodeBlock.h&quot;
  77 #include &quot;VM.h&quot;
  78 #include &quot;VisitCounter.h&quot;
  79 #include &quot;WasmMemory.h&quot;
  80 #include &quot;WeakMapImplInlines.h&quot;
  81 #include &quot;WeakSetInlines.h&quot;
  82 #include &lt;algorithm&gt;
  83 #include &lt;wtf/ListDump.h&gt;
  84 #include &lt;wtf/MainThread.h&gt;
  85 #include &lt;wtf/ParallelVectorIterator.h&gt;
  86 #include &lt;wtf/ProcessID.h&gt;
  87 #include &lt;wtf/RAMSize.h&gt;
  88 #include &lt;wtf/SimpleStats.h&gt;
  89 #include &lt;wtf/Threading.h&gt;
  90 
  91 #if PLATFORM(IOS_FAMILY)
  92 #include &lt;bmalloc/bmalloc.h&gt;
  93 #endif
  94 
  95 #if USE(FOUNDATION)
  96 #include &lt;wtf/spi/cocoa/objcSPI.h&gt;
  97 #endif
  98 
<span class="line-modified">  99 #ifdef JSC_GLIB_API_ENABLED</span>
 100 #include &quot;JSCGLibWrapperObject.h&quot;
 101 #endif
 102 
 103 namespace JSC {
 104 
 105 namespace {
 106 
 107 bool verboseStop = false;
 108 
 109 double maxPauseMS(double thisPauseMS)
 110 {
 111     static double maxPauseMS;
 112     maxPauseMS = std::max(thisPauseMS, maxPauseMS);
 113     return maxPauseMS;
 114 }
 115 
 116 size_t minHeapSize(HeapType heapType, size_t ramSize)
 117 {
 118     if (heapType == LargeHeap) {
 119         double result = std::min(
</pre>
<hr />
<pre>
 127 size_t proportionalHeapSize(size_t heapSize, size_t ramSize)
 128 {
 129     if (VM::isInMiniMode())
 130         return Options::miniVMHeapGrowthFactor() * heapSize;
 131 
 132 #if PLATFORM(IOS_FAMILY)
 133     size_t memoryFootprint = bmalloc::api::memoryFootprint();
 134     if (memoryFootprint &lt; ramSize * Options::smallHeapRAMFraction())
 135         return Options::smallHeapGrowthFactor() * heapSize;
 136     if (memoryFootprint &lt; ramSize * Options::mediumHeapRAMFraction())
 137         return Options::mediumHeapGrowthFactor() * heapSize;
 138 #else
 139     if (heapSize &lt; ramSize * Options::smallHeapRAMFraction())
 140         return Options::smallHeapGrowthFactor() * heapSize;
 141     if (heapSize &lt; ramSize * Options::mediumHeapRAMFraction())
 142         return Options::mediumHeapGrowthFactor() * heapSize;
 143 #endif
 144     return Options::largeHeapGrowthFactor() * heapSize;
 145 }
 146 
<span class="line-modified"> 147 bool isValidSharedInstanceThreadState(VM&amp; vm)</span>
 148 {
<span class="line-modified"> 149     return vm.currentThreadIsHoldingAPILock();</span>
 150 }
 151 
<span class="line-modified"> 152 bool isValidThreadState(VM&amp; vm)</span>
 153 {
<span class="line-modified"> 154     if (vm.atomStringTable() != Thread::current().atomStringTable())</span>
 155         return false;
 156 
<span class="line-modified"> 157     if (vm.isSharedInstance() &amp;&amp; !isValidSharedInstanceThreadState(vm))</span>
 158         return false;
 159 
 160     return true;
 161 }
 162 
 163 void recordType(VM&amp; vm, TypeCountSet&amp; set, JSCell* cell)
 164 {
 165     const char* typeName = &quot;[unknown]&quot;;
 166     const ClassInfo* info = cell-&gt;classInfo(vm);
 167     if (info &amp;&amp; info-&gt;className)
 168         typeName = info-&gt;className;
 169     set.add(typeName);
 170 }
 171 
 172 bool measurePhaseTiming()
 173 {
 174     return false;
 175 }
 176 
 177 HashMap&lt;const char*, GCTypeMap&lt;SimpleStats&gt;&gt;&amp; timingStats()
</pre>
<hr />
<pre>
 217     }
 218 
 219     ~TimingScope()
 220     {
 221         if (measurePhaseTiming()) {
 222             MonotonicTime after = MonotonicTime::now();
 223             Seconds timing = after - m_before;
 224             SimpleStats&amp; stats = timingStats(m_name, *m_scope);
 225             stats.add(timing.milliseconds());
 226             dataLog(&quot;[GC:&quot;, *m_scope, &quot;] &quot;, m_name, &quot; took: &quot;, timing.milliseconds(), &quot;ms (average &quot;, stats.mean(), &quot;ms).\n&quot;);
 227         }
 228     }
 229 private:
 230     Optional&lt;CollectionScope&gt; m_scope;
 231     MonotonicTime m_before;
 232     const char* m_name;
 233 };
 234 
 235 } // anonymous namespace
 236 
<span class="line-modified"> 237 class Heap::HeapThread : public AutomaticThread {</span>
 238 public:
<span class="line-modified"> 239     HeapThread(const AbstractLocker&amp; locker, Heap&amp; heap)</span>
 240         : AutomaticThread(locker, heap.m_threadLock, heap.m_threadCondition.copyRef())
 241         , m_heap(heap)
 242     {
 243     }
 244 
 245     const char* name() const override
 246     {
 247         return &quot;JSC Heap Collector Thread&quot;;
 248     }
 249 
 250 protected:
 251     PollResult poll(const AbstractLocker&amp; locker) override
 252     {
 253         if (m_heap.m_threadShouldStop) {
 254             m_heap.notifyThreadStopping(locker);
 255             return PollResult::Stop;
 256         }
<span class="line-modified"> 257         if (m_heap.shouldCollectInCollectorThread(locker)) {</span>
<span class="line-added"> 258             m_heap.m_collectorThreadIsRunning = true;</span>
 259             return PollResult::Work;
<span class="line-added"> 260         }</span>
<span class="line-added"> 261         m_heap.m_collectorThreadIsRunning = false;</span>
 262         return PollResult::Wait;
 263     }
 264 
 265     WorkResult work() override
 266     {
 267         m_heap.collectInCollectorThread();
 268         return WorkResult::Continue;
 269     }
 270 
 271     void threadDidStart() override
 272     {
<span class="line-modified"> 273         Thread::registerGCThread(GCThreadType::Main);</span>
<span class="line-added"> 274     }</span>
<span class="line-added"> 275 </span>
<span class="line-added"> 276     void threadIsStopping(const AbstractLocker&amp;) override</span>
<span class="line-added"> 277     {</span>
<span class="line-added"> 278         m_heap.m_collectorThreadIsRunning = false;</span>
 279     }
 280 
 281 private:
 282     Heap&amp; m_heap;
 283 };
 284 
<span class="line-modified"> 285 Heap::Heap(VM&amp; vm, HeapType heapType)</span>
 286     : m_heapType(heapType)
 287     , m_ramSize(Options::forceRAMSize() ? Options::forceRAMSize() : ramSize())
 288     , m_minBytesPerCycle(minHeapSize(m_heapType, m_ramSize))
 289     , m_maxEdenSize(m_minBytesPerCycle)
 290     , m_maxHeapSize(m_minBytesPerCycle)
 291     , m_objectSpace(this)
<span class="line-modified"> 292     , m_machineThreads(makeUnique&lt;MachineThreads&gt;())</span>
<span class="line-modified"> 293     , m_collectorSlotVisitor(makeUnique&lt;SlotVisitor&gt;(*this, &quot;C&quot;))</span>
<span class="line-modified"> 294     , m_mutatorSlotVisitor(makeUnique&lt;SlotVisitor&gt;(*this, &quot;M&quot;))</span>
<span class="line-modified"> 295     , m_mutatorMarkStack(makeUnique&lt;MarkStackArray&gt;())</span>
<span class="line-modified"> 296     , m_raceMarkStack(makeUnique&lt;MarkStackArray&gt;())</span>
<span class="line-modified"> 297     , m_constraintSet(makeUnique&lt;MarkingConstraintSet&gt;(*this))</span>
 298     , m_handleSet(vm)
<span class="line-modified"> 299     , m_codeBlocks(makeUnique&lt;CodeBlockSet&gt;())</span>
<span class="line-modified"> 300     , m_jitStubRoutines(makeUnique&lt;JITStubRoutineSet&gt;())</span>
 301     , m_vm(vm)
 302     // We seed with 10ms so that GCActivityCallback::didAllocate doesn&#39;t continuously
 303     // schedule the timer if we&#39;ve never done a collection.
 304     , m_fullActivityCallback(GCActivityCallback::tryCreateFullTimer(this))
 305     , m_edenActivityCallback(GCActivityCallback::tryCreateEdenTimer(this))
 306     , m_sweeper(adoptRef(*new IncrementalSweeper(this)))
 307     , m_stopIfNecessaryTimer(adoptRef(*new StopIfNecessaryTimer(vm)))
<span class="line-modified"> 308     , m_sharedCollectorMarkStack(makeUnique&lt;MarkStackArray&gt;())</span>
<span class="line-modified"> 309     , m_sharedMutatorMarkStack(makeUnique&lt;MarkStackArray&gt;())</span>
 310     , m_helperClient(&amp;heapHelperPool())
 311     , m_threadLock(Box&lt;Lock&gt;::create())
 312     , m_threadCondition(AutomaticThreadCondition::create())
 313 {
 314     m_worldState.store(0);
 315 
<span class="line-added"> 316     for (unsigned i = 0, numberOfParallelThreads = heapHelperPool().numberOfThreads(); i &lt; numberOfParallelThreads; ++i) {</span>
<span class="line-added"> 317         std::unique_ptr&lt;SlotVisitor&gt; visitor = makeUnique&lt;SlotVisitor&gt;(*this, toCString(&quot;P&quot;, i + 1));</span>
<span class="line-added"> 318         if (Options::optimizeParallelSlotVisitorsForStoppedMutator())</span>
<span class="line-added"> 319             visitor-&gt;optimizeForStoppedMutator();</span>
<span class="line-added"> 320         m_availableParallelSlotVisitors.append(visitor.get());</span>
<span class="line-added"> 321         m_parallelSlotVisitors.append(WTFMove(visitor));</span>
<span class="line-added"> 322     }</span>
<span class="line-added"> 323 </span>
 324     if (Options::useConcurrentGC()) {
 325         if (Options::useStochasticMutatorScheduler())
<span class="line-modified"> 326             m_scheduler = makeUnique&lt;StochasticSpaceTimeMutatorScheduler&gt;(*this);</span>
 327         else
<span class="line-modified"> 328             m_scheduler = makeUnique&lt;SpaceTimeMutatorScheduler&gt;(*this);</span>
 329     } else {
 330         // We simulate turning off concurrent GC by making the scheduler say that the world
 331         // should always be stopped when the collector is running.
<span class="line-modified"> 332         m_scheduler = makeUnique&lt;SynchronousStopTheWorldMutatorScheduler&gt;();</span>
 333     }
 334 
 335     if (Options::verifyHeap())
<span class="line-modified"> 336         m_verifier = makeUnique&lt;HeapVerifier&gt;(this, Options::numberOfGCCyclesToRecordForVerification());</span>
 337 
 338     m_collectorSlotVisitor-&gt;optimizeForStoppedMutator();
 339 
 340     // When memory is critical, allow allocating 25% of the amount above the critical threshold before collecting.
 341     size_t memoryAboveCriticalThreshold = static_cast&lt;size_t&gt;(static_cast&lt;double&gt;(m_ramSize) * (1.0 - Options::criticalGCMemoryThreshold()));
 342     m_maxEdenSizeWhenCritical = memoryAboveCriticalThreshold / 4;
 343 
 344     LockHolder locker(*m_threadLock);
<span class="line-modified"> 345     m_thread = adoptRef(new HeapThread(locker, *this));</span>
 346 }
 347 
 348 Heap::~Heap()
 349 {
<span class="line-added"> 350     // Scribble m_worldState to make it clear that the heap has already been destroyed if we crash in checkConn</span>
<span class="line-added"> 351     m_worldState.store(0xbadbeeffu);</span>
<span class="line-added"> 352 </span>
 353     forEachSlotVisitor(
 354         [&amp;] (SlotVisitor&amp; visitor) {
 355             visitor.clearMarkStacks();
 356         });
 357     m_mutatorMarkStack-&gt;clear();
 358     m_raceMarkStack-&gt;clear();
 359 
 360     for (WeakBlock* block : m_logicallyEmptyWeakBlocks)
 361         WeakBlock::destroy(*this, block);
 362 }
 363 
 364 bool Heap::isPagedOut(MonotonicTime deadline)
 365 {
 366     return m_objectSpace.isPagedOut(deadline);
 367 }
 368 
<span class="line-added"> 369 void Heap::dumpHeapStatisticsAtVMDestruction()</span>
<span class="line-added"> 370 {</span>
<span class="line-added"> 371     unsigned counter = 0;</span>
<span class="line-added"> 372     m_objectSpace.forEachBlock([&amp;] (MarkedBlock::Handle* block) {</span>
<span class="line-added"> 373         unsigned live = 0;</span>
<span class="line-added"> 374         block-&gt;forEachCell([&amp;] (HeapCell* cell, HeapCell::Kind) {</span>
<span class="line-added"> 375             if (cell-&gt;isLive())</span>
<span class="line-added"> 376                 live++;</span>
<span class="line-added"> 377             return IterationStatus::Continue;</span>
<span class="line-added"> 378         });</span>
<span class="line-added"> 379         dataLogLn(&quot;[&quot;, counter++, &quot;] &quot;, block-&gt;cellSize(), &quot;, &quot;, live, &quot; / &quot;, block-&gt;cellsPerBlock(), &quot; &quot;, static_cast&lt;double&gt;(live) / block-&gt;cellsPerBlock() * 100, &quot;% &quot;, block-&gt;attributes(), &quot; &quot;, block-&gt;subspace()-&gt;name());</span>
<span class="line-added"> 380         block-&gt;forEachCell([&amp;] (HeapCell* heapCell, HeapCell::Kind kind) {</span>
<span class="line-added"> 381             if (heapCell-&gt;isLive() &amp;&amp; kind == HeapCell::Kind::JSCell) {</span>
<span class="line-added"> 382                 auto* cell = static_cast&lt;JSCell*&gt;(heapCell);</span>
<span class="line-added"> 383                 if (cell-&gt;isObject())</span>
<span class="line-added"> 384                     dataLogLn(&quot;    &quot;, JSValue((JSObject*)cell));</span>
<span class="line-added"> 385                 else</span>
<span class="line-added"> 386                     dataLogLn(&quot;    &quot;, *cell);</span>
<span class="line-added"> 387             }</span>
<span class="line-added"> 388             return IterationStatus::Continue;</span>
<span class="line-added"> 389         });</span>
<span class="line-added"> 390     });</span>
<span class="line-added"> 391 }</span>
<span class="line-added"> 392 </span>
 393 // The VM is being destroyed and the collector will never run again.
 394 // Run all pending finalizers now because we won&#39;t get another chance.
 395 void Heap::lastChanceToFinalize()
 396 {
 397     MonotonicTime before;
 398     if (Options::logGC()) {
 399         before = MonotonicTime::now();
 400         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: shutdown &quot;);
 401     }
 402 
 403     m_isShuttingDown = true;
 404 
<span class="line-modified"> 405     RELEASE_ASSERT(!m_vm.entryScope);</span>
 406     RELEASE_ASSERT(m_mutatorState == MutatorState::Running);
 407 
 408     if (m_collectContinuouslyThread) {
 409         {
 410             LockHolder locker(m_collectContinuouslyLock);
 411             m_shouldStopCollectingContinuously = true;
 412             m_collectContinuouslyCondition.notifyOne();
 413         }
 414         m_collectContinuouslyThread-&gt;waitForCompletion();
 415     }
 416 
 417     if (Options::logGC())
 418         dataLog(&quot;1&quot;);
 419 
 420     // Prevent new collections from being started. This is probably not even necessary, since we&#39;re not
 421     // going to call into anything that starts collections. Still, this makes the algorithm more
 422     // obviously sound.
 423     m_isSafeToCollect = false;
 424 
 425     if (Options::logGC())
</pre>
<hr />
<pre>
 453 
 454     // Carefully bring the thread down.
 455     bool stopped = false;
 456     {
 457         LockHolder locker(*m_threadLock);
 458         stopped = m_thread-&gt;tryStop(locker);
 459         m_threadShouldStop = true;
 460         if (!stopped)
 461             m_threadCondition-&gt;notifyOne(locker);
 462     }
 463 
 464     if (Options::logGC())
 465         dataLog(&quot;4&quot;);
 466 
 467     if (!stopped)
 468         m_thread-&gt;join();
 469 
 470     if (Options::logGC())
 471         dataLog(&quot;5 &quot;);
 472 
<span class="line-added"> 473     if (UNLIKELY(Options::dumpHeapStatisticsAtVMDestruction()))</span>
<span class="line-added"> 474         dumpHeapStatisticsAtVMDestruction();</span>
<span class="line-added"> 475 </span>
 476     m_arrayBuffers.lastChanceToFinalize();
 477     m_objectSpace.stopAllocatingForGood();
 478     m_objectSpace.lastChanceToFinalize();
 479     releaseDelayedReleasedObjects();
 480 
 481     sweepAllLogicallyEmptyWeakBlocks();
 482 
 483     m_objectSpace.freeMemory();
 484 
 485     if (Options::logGC())
 486         dataLog((MonotonicTime::now() - before).milliseconds(), &quot;ms]\n&quot;);
 487 }
 488 
 489 void Heap::releaseDelayedReleasedObjects()
 490 {
<span class="line-modified"> 491 #if USE(FOUNDATION) || defined(JSC_GLIB_API_ENABLED)</span>
 492     // We need to guard against the case that releasing an object can create more objects due to the
 493     // release calling into JS. When those JS call(s) exit and all locks are being dropped we end up
 494     // back here and could try to recursively release objects. We guard that with a recursive entry
 495     // count. Only the initial call will release objects, recursive calls simple return and let the
 496     // the initial call to the function take care of any objects created during release time.
 497     // This also means that we need to loop until there are no objects in m_delayedReleaseObjects
 498     // and use a temp Vector for the actual releasing.
 499     if (!m_delayedReleaseRecursionCount++) {
 500         while (!m_delayedReleaseObjects.isEmpty()) {
<span class="line-modified"> 501             ASSERT(m_vm.currentThreadIsHoldingAPILock());</span>
 502 
 503             auto objectsToRelease = WTFMove(m_delayedReleaseObjects);
 504 
 505             {
 506                 // We need to drop locks before calling out to arbitrary code.
 507                 JSLock::DropAllLocks dropAllLocks(m_vm);
 508 
 509 #if USE(FOUNDATION)
 510                 void* context = objc_autoreleasePoolPush();
 511 #endif
 512                 objectsToRelease.clear();
 513 #if USE(FOUNDATION)
 514                 objc_autoreleasePoolPop(context);
 515 #endif
 516             }
 517         }
 518     }
 519     m_delayedReleaseRecursionCount--;
 520 #endif
 521 }
</pre>
<hr />
<pre>
 554 void Heap::reportAbandonedObjectGraph()
 555 {
 556     // Our clients don&#39;t know exactly how much memory they
 557     // are abandoning so we just guess for them.
 558     size_t abandonedBytes = static_cast&lt;size_t&gt;(0.1 * capacity());
 559 
 560     // We want to accelerate the next collection. Because memory has just
 561     // been abandoned, the next collection has the potential to
 562     // be more profitable. Since allocation is the trigger for collection,
 563     // we hasten the next collection by pretending that we&#39;ve allocated more memory.
 564     if (m_fullActivityCallback) {
 565         m_fullActivityCallback-&gt;didAllocate(*this,
 566             m_sizeAfterLastCollect - m_sizeAfterLastFullCollect + m_bytesAllocatedThisCycle + m_bytesAbandonedSinceLastFullCollect);
 567     }
 568     m_bytesAbandonedSinceLastFullCollect += abandonedBytes;
 569 }
 570 
 571 void Heap::protect(JSValue k)
 572 {
 573     ASSERT(k);
<span class="line-modified"> 574     ASSERT(m_vm.currentThreadIsHoldingAPILock());</span>
 575 
 576     if (!k.isCell())
 577         return;
 578 
 579     m_protectedValues.add(k.asCell());
 580 }
 581 
 582 bool Heap::unprotect(JSValue k)
 583 {
 584     ASSERT(k);
<span class="line-modified"> 585     ASSERT(m_vm.currentThreadIsHoldingAPILock());</span>
 586 
 587     if (!k.isCell())
 588         return false;
 589 
 590     return m_protectedValues.remove(k.asCell());
 591 }
 592 
 593 void Heap::addReference(JSCell* cell, ArrayBuffer* buffer)
 594 {
 595     if (m_arrayBuffers.addReference(cell, buffer)) {
 596         collectIfNecessaryOrDefer();
 597         didAllocate(buffer-&gt;gcSizeEstimateInBytes());
 598     }
 599 }
 600 
 601 template&lt;typename CellType, typename CellSet&gt;
 602 void Heap::finalizeMarkedUnconditionalFinalizers(CellSet&amp; cellSet)
 603 {
 604     cellSet.forEachMarkedCell(
 605         [&amp;] (HeapCell* cell, HeapCell::Kind) {
<span class="line-modified"> 606             static_cast&lt;CellType*&gt;(cell)-&gt;finalizeUnconditionally(vm());</span>
 607         });
 608 }
 609 
 610 void Heap::finalizeUnconditionalFinalizers()
 611 {
<span class="line-modified"> 612     vm().builtinExecutables()-&gt;finalizeUnconditionally();</span>
<span class="line-modified"> 613     finalizeMarkedUnconditionalFinalizers&lt;FunctionExecutable&gt;(vm().functionExecutableSpace.space);</span>
<span class="line-modified"> 614     finalizeMarkedUnconditionalFinalizers&lt;SymbolTable&gt;(vm().symbolTableSpace);</span>
<span class="line-added"> 615     vm().forEachCodeBlockSpace(</span>
 616         [&amp;] (auto&amp; space) {
 617             this-&gt;finalizeMarkedUnconditionalFinalizers&lt;CodeBlock&gt;(space.set);
 618         });
<span class="line-modified"> 619     finalizeMarkedUnconditionalFinalizers&lt;ExecutableToCodeBlockEdge&gt;(vm().executableToCodeBlockEdgesWithFinalizers);</span>
<span class="line-modified"> 620     finalizeMarkedUnconditionalFinalizers&lt;StructureRareData&gt;(vm().structureRareDataSpace);</span>
<span class="line-modified"> 621     finalizeMarkedUnconditionalFinalizers&lt;UnlinkedFunctionExecutable&gt;(vm().unlinkedFunctionExecutableSpace.set);</span>
<span class="line-modified"> 622     if (vm().m_weakSetSpace)</span>
<span class="line-modified"> 623         finalizeMarkedUnconditionalFinalizers&lt;JSWeakSet&gt;(*vm().m_weakSetSpace);</span>
<span class="line-modified"> 624     if (vm().m_weakMapSpace)</span>
<span class="line-modified"> 625         finalizeMarkedUnconditionalFinalizers&lt;JSWeakMap&gt;(*vm().m_weakMapSpace);</span>
<span class="line-added"> 626     if (vm().m_weakObjectRefSpace)</span>
<span class="line-added"> 627         finalizeMarkedUnconditionalFinalizers&lt;JSWeakObjectRef&gt;(*vm().m_weakObjectRefSpace);</span>
<span class="line-added"> 628     if (vm().m_errorInstanceSpace)</span>
<span class="line-added"> 629         finalizeMarkedUnconditionalFinalizers&lt;ErrorInstance&gt;(*vm().m_errorInstanceSpace);</span>
 630 
 631 #if ENABLE(WEBASSEMBLY)
<span class="line-modified"> 632     if (vm().m_webAssemblyCodeBlockSpace)</span>
<span class="line-modified"> 633         finalizeMarkedUnconditionalFinalizers&lt;JSWebAssemblyCodeBlock&gt;(*vm().m_webAssemblyCodeBlockSpace);</span>
 634 #endif
 635 }
 636 
 637 void Heap::willStartIterating()
 638 {
 639     m_objectSpace.willStartIterating();
 640 }
 641 
 642 void Heap::didFinishIterating()
 643 {
 644     m_objectSpace.didFinishIterating();
 645 }
 646 
 647 void Heap::completeAllJITPlans()
 648 {
 649     if (!VM::canUseJIT())
 650         return;
 651 #if ENABLE(JIT)
<span class="line-modified"> 652     JITWorklist::ensureGlobalWorklist().completeAllForVM(m_vm);</span>
 653 #endif // ENABLE(JIT)
<span class="line-modified"> 654     DFG::completeAllPlansForVM(m_vm);</span>
 655 }
 656 
 657 template&lt;typename Func&gt;
 658 void Heap::iterateExecutingAndCompilingCodeBlocks(const Func&amp; func)
 659 {
 660     m_codeBlocks-&gt;iterateCurrentlyExecuting(func);
 661     if (VM::canUseJIT())
<span class="line-modified"> 662         DFG::iterateCodeBlocksForGC(m_vm, func);</span>
 663 }
 664 
 665 template&lt;typename Func&gt;
 666 void Heap::iterateExecutingAndCompilingCodeBlocksWithoutHoldingLocks(const Func&amp; func)
 667 {
 668     Vector&lt;CodeBlock*, 256&gt; codeBlocks;
 669     iterateExecutingAndCompilingCodeBlocks(
 670         [&amp;] (CodeBlock* codeBlock) {
 671             codeBlocks.append(codeBlock);
 672         });
 673     for (CodeBlock* codeBlock : codeBlocks)
 674         func(codeBlock);
 675 }
 676 
 677 void Heap::assertMarkStacksEmpty()
 678 {
 679     bool ok = true;
 680 
 681     if (!m_sharedCollectorMarkStack-&gt;isEmpty()) {
 682         dataLog(&quot;FATAL: Shared collector mark stack not empty! It has &quot;, m_sharedCollectorMarkStack-&gt;size(), &quot; elements.\n&quot;);
</pre>
<hr />
<pre>
 691     forEachSlotVisitor(
 692         [&amp;] (SlotVisitor&amp; visitor) {
 693             if (visitor.isEmpty())
 694                 return;
 695 
 696             dataLog(&quot;FATAL: Visitor &quot;, RawPointer(&amp;visitor), &quot; is not empty!\n&quot;);
 697             ok = false;
 698         });
 699 
 700     RELEASE_ASSERT(ok);
 701 }
 702 
 703 void Heap::gatherStackRoots(ConservativeRoots&amp; roots)
 704 {
 705     m_machineThreads-&gt;gatherConservativeRoots(roots, *m_jitStubRoutines, *m_codeBlocks, m_currentThreadState, m_currentThread);
 706 }
 707 
 708 void Heap::gatherJSStackRoots(ConservativeRoots&amp; roots)
 709 {
 710 #if ENABLE(C_LOOP)
<span class="line-modified"> 711     m_vm.interpreter-&gt;cloopStack().gatherConservativeRoots(roots, *m_jitStubRoutines, *m_codeBlocks);</span>
 712 #else
 713     UNUSED_PARAM(roots);
 714 #endif
 715 }
 716 
 717 void Heap::gatherScratchBufferRoots(ConservativeRoots&amp; roots)
 718 {
 719 #if ENABLE(DFG_JIT)
 720     if (!VM::canUseJIT())
 721         return;
<span class="line-modified"> 722     m_vm.gatherScratchBufferRoots(roots);</span>
 723 #else
 724     UNUSED_PARAM(roots);
 725 #endif
 726 }
 727 
 728 void Heap::beginMarking()
 729 {
 730     TimingScope timingScope(*this, &quot;Heap::beginMarking&quot;);
 731     m_jitStubRoutines-&gt;clearMarks();
 732     m_objectSpace.beginMarking();
 733     setMutatorShouldBeFenced(true);
 734 }
 735 
 736 void Heap::removeDeadCompilerWorklistEntries()
 737 {
 738 #if ENABLE(DFG_JIT)
 739     if (!VM::canUseJIT())
 740         return;
 741     for (unsigned i = DFG::numberOfWorklists(); i--;)
<span class="line-modified"> 742         DFG::existingWorklistForIndex(i).removeDeadPlans(m_vm);</span>
 743 #endif
 744 }
 745 
<span class="line-modified"> 746 bool Heap::isAnalyzingHeap() const</span>
 747 {
<span class="line-modified"> 748     HeapProfiler* heapProfiler = m_vm.heapProfiler();</span>
 749     if (UNLIKELY(heapProfiler))
<span class="line-modified"> 750         return heapProfiler-&gt;activeHeapAnalyzer();</span>
 751     return false;
 752 }
 753 
<span class="line-modified"> 754 struct GatherExtraHeapData : MarkedBlock::CountFunctor {</span>
<span class="line-modified"> 755     GatherExtraHeapData(VM&amp; vm, HeapAnalyzer&amp; analyzer)</span>
 756         : m_vm(vm)
<span class="line-modified"> 757         , m_analyzer(analyzer)</span>
 758     {
 759     }
 760 
 761     IterationStatus operator()(HeapCell* heapCell, HeapCell::Kind kind) const
 762     {
 763         if (isJSCellKind(kind)) {
 764             JSCell* cell = static_cast&lt;JSCell*&gt;(heapCell);
<span class="line-modified"> 765             cell-&gt;methodTable(m_vm)-&gt;analyzeHeap(cell, m_analyzer);</span>
 766         }
 767         return IterationStatus::Continue;
 768     }
 769 
 770     VM&amp; m_vm;
<span class="line-modified"> 771     HeapAnalyzer&amp; m_analyzer;</span>
 772 };
 773 
<span class="line-modified"> 774 void Heap::gatherExtraHeapData(HeapProfiler&amp; heapProfiler)</span>
 775 {
<span class="line-modified"> 776     if (auto* analyzer = heapProfiler.activeHeapAnalyzer()) {</span>
 777         HeapIterationScope heapIterationScope(*this);
<span class="line-modified"> 778         GatherExtraHeapData functor(m_vm, *analyzer);</span>
 779         m_objectSpace.forEachLiveCell(heapIterationScope, functor);
 780     }
 781 }
 782 
 783 struct RemoveDeadHeapSnapshotNodes : MarkedBlock::CountFunctor {
 784     RemoveDeadHeapSnapshotNodes(HeapSnapshot&amp; snapshot)
 785         : m_snapshot(snapshot)
 786     {
 787     }
 788 
 789     IterationStatus operator()(HeapCell* cell, HeapCell::Kind kind) const
 790     {
 791         if (isJSCellKind(kind))
 792             m_snapshot.sweepCell(static_cast&lt;JSCell*&gt;(cell));
 793         return IterationStatus::Continue;
 794     }
 795 
 796     HeapSnapshot&amp; m_snapshot;
 797 };
 798 
</pre>
<hr />
<pre>
 882             JSCell* cell = static_cast&lt;JSCell*&gt;(heapCell);
 883             if (cell-&gt;isObject() &amp;&amp; asObject(cell)-&gt;isGlobalObject())
 884                 result++;
 885             return IterationStatus::Continue;
 886         });
 887     return result;
 888 }
 889 
 890 size_t Heap::protectedObjectCount()
 891 {
 892     size_t result = 0;
 893     forEachProtectedCell(
 894         [&amp;] (JSCell*) {
 895             result++;
 896         });
 897     return result;
 898 }
 899 
 900 std::unique_ptr&lt;TypeCountSet&gt; Heap::protectedObjectTypeCounts()
 901 {
<span class="line-modified"> 902     std::unique_ptr&lt;TypeCountSet&gt; result = makeUnique&lt;TypeCountSet&gt;();</span>
 903     forEachProtectedCell(
 904         [&amp;] (JSCell* cell) {
<span class="line-modified"> 905             recordType(vm(), *result, cell);</span>
 906         });
 907     return result;
 908 }
 909 
 910 std::unique_ptr&lt;TypeCountSet&gt; Heap::objectTypeCounts()
 911 {
<span class="line-modified"> 912     std::unique_ptr&lt;TypeCountSet&gt; result = makeUnique&lt;TypeCountSet&gt;();</span>
 913     HeapIterationScope iterationScope(*this);
 914     m_objectSpace.forEachLiveCell(
 915         iterationScope,
 916         [&amp;] (HeapCell* cell, HeapCell::Kind kind) -&gt; IterationStatus {
 917             if (isJSCellKind(kind))
<span class="line-modified"> 918                 recordType(vm(), *result, static_cast&lt;JSCell*&gt;(cell));</span>
 919             return IterationStatus::Continue;
 920         });
 921     return result;
 922 }
 923 
 924 void Heap::deleteAllCodeBlocks(DeleteAllCodeEffort effort)
 925 {
 926     if (m_collectionScope &amp;&amp; effort == DeleteAllCodeIfNotCollecting)
 927         return;
 928 
<span class="line-modified"> 929     VM&amp; vm = m_vm;</span>
 930     PreventCollectionScope preventCollectionScope(*this);
 931 
 932     // If JavaScript is running, it&#39;s not safe to delete all JavaScript code, since
 933     // we&#39;ll end up returning to deleted code.
 934     RELEASE_ASSERT(!vm.entryScope);
 935     RELEASE_ASSERT(!m_collectionScope);
 936 
 937     completeAllJITPlans();
 938 
 939     vm.forEachScriptExecutableSpace(
 940         [&amp;] (auto&amp; spaceAndSet) {
 941             HeapIterationScope heapIterationScope(*this);
 942             auto&amp; set = spaceAndSet.set;
 943             set.forEachLiveCell(
 944                 [&amp;] (HeapCell* cell, HeapCell::Kind) {
 945                     ScriptExecutable* executable = static_cast&lt;ScriptExecutable*&gt;(cell);
 946                     executable-&gt;clearCode(set);
 947                 });
 948         });
 949 
</pre>
<hr />
<pre>
 954         // VM. This could leave Wasm in an inconsistent state where it has an IC that
 955         // points into a CodeBlock that could be dead. The IC will still succeed because
 956         // it uses a callee check, but then it will call into dead code.
 957         HeapIterationScope heapIterationScope(*this);
 958         if (vm.m_webAssemblyCodeBlockSpace) {
 959             vm.m_webAssemblyCodeBlockSpace-&gt;forEachLiveCell([&amp;] (HeapCell* cell, HeapCell::Kind kind) {
 960                 ASSERT_UNUSED(kind, kind == HeapCell::JSCell);
 961                 JSWebAssemblyCodeBlock* codeBlock = static_cast&lt;JSWebAssemblyCodeBlock*&gt;(cell);
 962                 codeBlock-&gt;clearJSCallICs(vm);
 963             });
 964         }
 965     }
 966 #endif
 967 }
 968 
 969 void Heap::deleteAllUnlinkedCodeBlocks(DeleteAllCodeEffort effort)
 970 {
 971     if (m_collectionScope &amp;&amp; effort == DeleteAllCodeIfNotCollecting)
 972         return;
 973 
<span class="line-modified"> 974     VM&amp; vm = m_vm;</span>
 975     PreventCollectionScope preventCollectionScope(*this);
 976 
 977     RELEASE_ASSERT(!m_collectionScope);
 978 
 979     HeapIterationScope heapIterationScope(*this);
 980     vm.unlinkedFunctionExecutableSpace.set.forEachLiveCell(
 981         [&amp;] (HeapCell* cell, HeapCell::Kind) {
 982             UnlinkedFunctionExecutable* executable = static_cast&lt;UnlinkedFunctionExecutable*&gt;(cell);
 983             executable-&gt;clearCode(vm);
 984         });
 985 }
 986 
 987 void Heap::deleteUnmarkedCompiledCode()
 988 {
<span class="line-modified"> 989     vm().forEachScriptExecutableSpace([] (auto&amp; space) { space.space.sweep(); });</span>
<span class="line-modified"> 990     vm().forEachCodeBlockSpace([] (auto&amp; space) { space.space.sweep(); }); // Sweeping must occur before deleting stubs, otherwise the stubs might still think they&#39;re alive as they get deleted.</span>
 991     m_jitStubRoutines-&gt;deleteUnmarkedJettisonedStubRoutines();
 992 }
 993 
 994 void Heap::addToRememberedSet(const JSCell* constCell)
 995 {
 996     JSCell* cell = const_cast&lt;JSCell*&gt;(constCell);
 997     ASSERT(cell);
 998     ASSERT(!Options::useConcurrentJIT() || !isCompilationThread());
 999     m_barriersExecuted++;
1000     if (m_mutatorShouldBeFenced) {
1001         WTF::loadLoadFence();
1002         if (!isMarked(cell)) {
1003             // During a full collection a store into an unmarked object that had surivived past
1004             // collections will manifest as a store to an unmarked PossiblyBlack object. If the
1005             // object gets marked at some time after this then it will go down the normal marking
1006             // path. So, we don&#39;t have to remember this object. We could return here. But we go
1007             // further and attempt to re-white the object.
1008 
1009             RELEASE_ASSERT(m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full);
1010 
</pre>
<hr />
<pre>
1019                 //
1020                 // In this case we would have made the object white again, even though it should
1021                 // be black. This check lets us correct our mistake. This relies on the fact that
1022                 // isMarked converges monotonically to true.
1023                 if (isMarked(cell)) {
1024                     // It&#39;s difficult to work out whether the object should be grey or black at
1025                     // this point. We say black conservatively.
1026                     cell-&gt;setCellState(CellState::PossiblyBlack);
1027                 }
1028 
1029                 // Either way, we can return. Most likely, the object was not marked, and so the
1030                 // object is now labeled white. This means that future barrier executions will not
1031                 // fire. In the unlikely event that the object had become marked, we can still
1032                 // return anyway, since we proved that the object was not marked at the time that
1033                 // we executed this slow path.
1034             }
1035 
1036             return;
1037         }
1038     } else
<span class="line-modified">1039         ASSERT(isMarked(cell));</span>
1040     // It could be that the object was *just* marked. This means that the collector may set the
1041     // state to DefinitelyGrey and then to PossiblyOldOrBlack at any time. It&#39;s OK for us to
1042     // race with the collector here. If we win then this is accurate because the object _will_
1043     // get scanned again. If we lose then someone else will barrier the object again. That would
1044     // be unfortunate but not the end of the world.
1045     cell-&gt;setCellState(CellState::PossiblyGrey);
1046     m_mutatorMarkStack-&gt;append(cell);
1047 }
1048 
1049 void Heap::sweepSynchronously()
1050 {
1051     MonotonicTime before { };
1052     if (Options::logGC()) {
1053         dataLog(&quot;Full sweep: &quot;, capacity() / 1024, &quot;kb &quot;);
1054         before = MonotonicTime::now();
1055     }
1056     m_objectSpace.sweep();
1057     m_objectSpace.shrink();
1058     if (Options::logGC()) {
1059         MonotonicTime after = MonotonicTime::now();
</pre>
<hr />
<pre>
1164             return;
1165         case RunCurrentPhaseResult::Continue:
1166             break;
1167         case RunCurrentPhaseResult::NeedCurrentThreadState:
1168             RELEASE_ASSERT_NOT_REACHED();
1169             break;
1170         }
1171     }
1172 }
1173 
1174 ALWAYS_INLINE int asInt(CollectorPhase phase)
1175 {
1176     return static_cast&lt;int&gt;(phase);
1177 }
1178 
1179 void Heap::checkConn(GCConductor conn)
1180 {
1181     unsigned worldState = m_worldState.load();
1182     switch (conn) {
1183     case GCConductor::Mutator:
<span class="line-modified">1184         RELEASE_ASSERT(worldState &amp; mutatorHasConnBit, worldState, asInt(m_lastPhase), asInt(m_currentPhase), asInt(m_nextPhase), vm().id(), VM::numberOfIDs(), vm().isEntered());</span>
1185         return;
1186     case GCConductor::Collector:
<span class="line-modified">1187         RELEASE_ASSERT(!(worldState &amp; mutatorHasConnBit), worldState, asInt(m_lastPhase), asInt(m_currentPhase), asInt(m_nextPhase), vm().id(), VM::numberOfIDs(), vm().isEntered());</span>
1188         return;
1189     }
1190     RELEASE_ASSERT_NOT_REACHED();
1191 }
1192 
1193 auto Heap::runCurrentPhase(GCConductor conn, CurrentThreadState* currentThreadState) -&gt; RunCurrentPhaseResult
1194 {
1195     checkConn(conn);
1196     m_currentThreadState = currentThreadState;
<span class="line-modified">1197     m_currentThread = &amp;Thread::current();</span>
1198 
1199     if (conn == GCConductor::Mutator)
1200         sanitizeStackForVM(vm());
1201 
1202     // If the collector transfers the conn to the mutator, it leaves us in between phases.
1203     if (!finishChangingPhase(conn)) {
1204         // A mischevious mutator could repeatedly relinquish the conn back to us. We try to avoid doing
1205         // this, but it&#39;s probably not the end of the world if it did happen.
1206         if (false)
1207             dataLog(&quot;Conn bounce-back.\n&quot;);
1208         return RunCurrentPhaseResult::Finished;
1209     }
1210 
1211     bool result = false;
1212     switch (m_currentPhase) {
1213     case CollectorPhase::NotRunning:
1214         result = runNotRunningPhase(conn);
1215         break;
1216 
1217     case CollectorPhase::Begin:
</pre>
<hr />
<pre>
1291         m_collectorSlotVisitor-&gt;clearMarkStacks();
1292         m_mutatorMarkStack-&gt;clear();
1293     }
1294 
1295     RELEASE_ASSERT(m_raceMarkStack-&gt;isEmpty());
1296 
1297     beginMarking();
1298 
1299     forEachSlotVisitor(
1300         [&amp;] (SlotVisitor&amp; visitor) {
1301             visitor.didStartMarking();
1302         });
1303 
1304     m_parallelMarkersShouldExit = false;
1305 
1306     m_helperClient.setFunction(
1307         [this] () {
1308             SlotVisitor* slotVisitor;
1309             {
1310                 LockHolder locker(m_parallelSlotVisitorLock);
<span class="line-modified">1311                 RELEASE_ASSERT_WITH_MESSAGE(!m_availableParallelSlotVisitors.isEmpty(), &quot;Parallel SlotVisitors are allocated apriori&quot;);</span>











1312                 slotVisitor = m_availableParallelSlotVisitors.takeLast();
1313             }
1314 
<span class="line-modified">1315             Thread::registerGCThread(GCThreadType::Helper);</span>
1316 
1317             {
1318                 ParallelModeEnabler parallelModeEnabler(*slotVisitor);
1319                 slotVisitor-&gt;drainFromShared(SlotVisitor::SlaveDrain);
1320             }
1321 
1322             {
1323                 LockHolder locker(m_parallelSlotVisitorLock);
1324                 m_availableParallelSlotVisitors.append(slotVisitor);
1325             }
1326         });
1327 
1328     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1329 
1330     m_constraintSet-&gt;didStartMarking();
1331 
1332     m_scheduler-&gt;beginCollection();
1333     if (Options::logGC())
1334         m_scheduler-&gt;log();
1335 
</pre>
<hr />
<pre>
1491     {
1492         auto locker = holdLock(m_markingMutex);
1493         m_parallelMarkersShouldExit = true;
1494         m_markingConditionVariable.notifyAll();
1495     }
1496     m_helperClient.finish();
1497 
1498     iterateExecutingAndCompilingCodeBlocks(
1499         [&amp;] (CodeBlock* codeBlock) {
1500             writeBarrier(codeBlock);
1501         });
1502 
1503     updateObjectCounts();
1504     endMarking();
1505 
1506     if (UNLIKELY(m_verifier)) {
1507         m_verifier-&gt;gatherLiveCells(HeapVerifier::Phase::AfterMarking);
1508         m_verifier-&gt;verify(HeapVerifier::Phase::AfterMarking);
1509     }
1510 
<span class="line-modified">1511     if (vm().typeProfiler())</span>
<span class="line-modified">1512         vm().typeProfiler()-&gt;invalidateTypeSetCache(vm());</span>



1513 
1514     reapWeakHandles();
1515     pruneStaleEntriesFromWeakGCMaps();
1516     sweepArrayBuffers();
1517     snapshotUnswept();
1518     finalizeUnconditionalFinalizers();
1519     removeDeadCompilerWorklistEntries();
1520     notifyIncrementalSweeper();
1521 
1522     m_codeBlocks-&gt;iterateCurrentlyExecuting(
1523         [&amp;] (CodeBlock* codeBlock) {
1524             writeBarrier(codeBlock);
1525         });
1526     m_codeBlocks-&gt;clearCurrentlyExecuting();
1527 
1528     m_objectSpace.prepareForAllocation();
1529     updateAllocationLimits();
1530 
1531     if (UNLIKELY(m_verifier)) {
1532         m_verifier-&gt;trimDeadCells();
</pre>
<hr />
<pre>
1631         dataLog(&quot;FATAL: world already stopped.\n&quot;);
1632         RELEASE_ASSERT_NOT_REACHED();
1633     }
1634 
1635     if (m_mutatorDidRun)
1636         m_mutatorExecutionVersion++;
1637 
1638     m_mutatorDidRun = false;
1639 
1640     suspendCompilerThreads();
1641     m_worldIsStopped = true;
1642 
1643     forEachSlotVisitor(
1644         [&amp;] (SlotVisitor&amp; slotVisitor) {
1645             slotVisitor.updateMutatorIsStopped(NoLockingNecessary);
1646         });
1647 
1648 #if ENABLE(JIT)
1649     if (VM::canUseJIT()) {
1650         DeferGCForAWhile awhile(*this);
<span class="line-modified">1651         if (JITWorklist::ensureGlobalWorklist().completeAllForVM(m_vm)</span>
1652             &amp;&amp; conn == GCConductor::Collector)
1653             setGCDidJIT();
1654     }
1655 #endif // ENABLE(JIT)
1656     UNUSED_PARAM(conn);
1657 
<span class="line-modified">1658     if (auto* shadowChicken = vm().shadowChicken())</span>
<span class="line-modified">1659         shadowChicken-&gt;update(vm(), vm().topCallFrame);</span>
1660 
1661     m_structureIDTable.flushOldTables();
1662     m_objectSpace.stopAllocating();
1663 
1664     m_stopTime = MonotonicTime::now();
1665 }
1666 
1667 NEVER_INLINE void Heap::resumeThePeriphery()
1668 {
1669     // Calling resumeAllocating does the Right Thing depending on whether this is the end of a
1670     // collection cycle or this is just a concurrent phase within a collection cycle:
1671     // - At end of collection cycle: it&#39;s a no-op because prepareForAllocation already cleared the
1672     //   last active block.
1673     // - During collection cycle: it reinstates the last active block.
1674     m_objectSpace.resumeAllocating();
1675 
1676     m_barriersExecuted = 0;
1677 
1678     if (!m_worldIsStopped) {
1679         dataLog(&quot;Fatal: collector does not believe that the world is stopped.\n&quot;);
</pre>
<hr />
<pre>
1692     forEachSlotVisitor(
1693         [&amp;] (SlotVisitor&amp; slotVisitor) {
1694             slotVisitorsToUpdate.append(&amp;slotVisitor);
1695         });
1696 
1697     for (unsigned countdown = 40; !slotVisitorsToUpdate.isEmpty() &amp;&amp; countdown--;) {
1698         for (unsigned index = 0; index &lt; slotVisitorsToUpdate.size(); ++index) {
1699             SlotVisitor&amp; slotVisitor = *slotVisitorsToUpdate[index];
1700             bool remove = false;
1701             if (slotVisitor.hasAcknowledgedThatTheMutatorIsResumed())
1702                 remove = true;
1703             else if (auto locker = tryHoldLock(slotVisitor.rightToRun())) {
1704                 slotVisitor.updateMutatorIsStopped(locker);
1705                 remove = true;
1706             }
1707             if (remove) {
1708                 slotVisitorsToUpdate[index--] = slotVisitorsToUpdate.last();
1709                 slotVisitorsToUpdate.takeLast();
1710             }
1711         }
<span class="line-modified">1712         Thread::yield();</span>
1713     }
1714 
1715     for (SlotVisitor* slotVisitor : slotVisitorsToUpdate)
1716         slotVisitor-&gt;updateMutatorIsStopped();
1717 
1718     resumeCompilerThreads();
1719 }
1720 
1721 bool Heap::stopTheMutator()
1722 {
1723     for (;;) {
1724         unsigned oldState = m_worldState.load();
1725         if (oldState &amp; stoppedBit) {
1726             RELEASE_ASSERT(!(oldState &amp; hasAccessBit));
1727             RELEASE_ASSERT(!(oldState &amp; mutatorWaitingBit));
1728             RELEASE_ASSERT(!(oldState &amp; mutatorHasConnBit));
1729             return true;
1730         }
1731 
1732         if (oldState &amp; mutatorHasConnBit) {
</pre>
<hr />
<pre>
2080     m_threadIsStopping = true;
2081     clearMutatorWaiting();
2082     ParkingLot::unparkAll(&amp;m_worldState);
2083 }
2084 
2085 void Heap::finalize()
2086 {
2087     MonotonicTime before;
2088     if (Options::logGC()) {
2089         before = MonotonicTime::now();
2090         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: finalize &quot;);
2091     }
2092 
2093     {
2094         SweepingScope sweepingScope(*this);
2095         deleteUnmarkedCompiledCode();
2096         deleteSourceProviderCaches();
2097         sweepInFinalize();
2098     }
2099 
<span class="line-modified">2100     if (HasOwnPropertyCache* cache = vm().hasOwnPropertyCache())</span>
2101         cache-&gt;clear();
2102 
2103     immutableButterflyToStringCache.clear();
2104 
2105     for (const HeapFinalizerCallback&amp; callback : m_heapFinalizerCallbacks)
<span class="line-modified">2106         callback.run(vm());</span>
2107 
2108     if (shouldSweepSynchronously())
2109         sweepSynchronously();
2110 
2111     if (Options::logGC()) {
2112         MonotonicTime after = MonotonicTime::now();
2113         dataLog((after - before).milliseconds(), &quot;ms]\n&quot;);
2114     }
2115 }
2116 
2117 Heap::Ticket Heap::requestCollection(GCRequest request)
2118 {
2119     stopIfNecessary();
2120 
<span class="line-modified">2121     ASSERT(vm().currentThreadIsHoldingAPILock());</span>
<span class="line-modified">2122     RELEASE_ASSERT(vm().atomStringTable() == Thread::current().atomStringTable());</span>
2123 
2124     LockHolder locker(*m_threadLock);
2125     // We may be able to steal the conn. That only works if the collector is definitely not running
2126     // right now. This is an optimization that prevents the collector thread from ever starting in most
2127     // cases.
2128     ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
<span class="line-modified">2129     if ((m_lastServedTicket == m_lastGrantedTicket) &amp;&amp; !m_collectorThreadIsRunning) {</span>
2130         if (false)
2131             dataLog(&quot;Taking the conn.\n&quot;);
2132         m_worldState.exchangeOr(mutatorHasConnBit);
2133     }
2134 
2135     m_requests.append(request);
2136     m_lastGrantedTicket++;
2137     if (!(m_worldState.load() &amp; mutatorHasConnBit))
2138         m_threadCondition-&gt;notifyOne(locker);
2139     return m_lastGrantedTicket;
2140 }
2141 
2142 void Heap::waitForCollection(Ticket ticket)
2143 {
2144     waitForCollector(
2145         [&amp;] (const AbstractLocker&amp;) -&gt; bool {
2146             return m_lastServedTicket &gt;= ticket;
2147         });
2148 }
2149 
2150 void Heap::sweepInFinalize()
2151 {
2152     m_objectSpace.sweepLargeAllocations();
<span class="line-modified">2153     vm().eagerlySweptDestructibleObjectSpace.sweep();</span>
2154 }
2155 
2156 void Heap::suspendCompilerThreads()
2157 {
2158 #if ENABLE(DFG_JIT)
2159     // We ensure the worklists so that it&#39;s not possible for the mutator to start a new worklist
2160     // after we have suspended the ones that he had started before. That&#39;s not very expensive since
2161     // the worklists use AutomaticThreads anyway.
2162     if (!VM::canUseJIT())
2163         return;
2164     for (unsigned i = DFG::numberOfWorklists(); i--;)
2165         DFG::ensureWorklistForIndex(i).suspendAllThreads();
2166 #endif
2167 }
2168 
2169 void Heap::willStartCollection()
2170 {
2171     if (Options::logGC())
2172         dataLog(&quot;=&gt; &quot;);
2173 
</pre>
<hr />
<pre>
2210 void Heap::prepareForMarking()
2211 {
2212     m_objectSpace.prepareForMarking();
2213 }
2214 
2215 void Heap::reapWeakHandles()
2216 {
2217     m_objectSpace.reapWeakSets();
2218 }
2219 
2220 void Heap::pruneStaleEntriesFromWeakGCMaps()
2221 {
2222     if (!m_collectionScope || m_collectionScope.value() != CollectionScope::Full)
2223         return;
2224     for (WeakGCMapBase* weakGCMap : m_weakGCMaps)
2225         weakGCMap-&gt;pruneStaleEntries();
2226 }
2227 
2228 void Heap::sweepArrayBuffers()
2229 {
<span class="line-modified">2230     m_arrayBuffers.sweep(vm());</span>
2231 }
2232 
2233 void Heap::snapshotUnswept()
2234 {
2235     TimingScope timingScope(*this, &quot;Heap::snapshotUnswept&quot;);
2236     m_objectSpace.snapshotUnswept();
2237 }
2238 
2239 void Heap::deleteSourceProviderCaches()
2240 {
2241     if (m_lastCollectionScope &amp;&amp; m_lastCollectionScope.value() == CollectionScope::Full)
<span class="line-modified">2242         m_vm.clearSourceProviderCaches();</span>
2243 }
2244 
2245 void Heap::notifyIncrementalSweeper()
2246 {
2247     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2248         if (!m_logicallyEmptyWeakBlocks.isEmpty())
2249             m_indexOfNextLogicallyEmptyWeakBlockToSweep = 0;
2250     }
2251 
2252     m_sweeper-&gt;startSweeping(*this);
2253 }
2254 
2255 void Heap::updateAllocationLimits()
2256 {
2257     static const bool verbose = false;
2258 
2259     if (verbose) {
2260         dataLog(&quot;\n&quot;);
2261         dataLog(&quot;bytesAllocatedThisCycle = &quot;, m_bytesAllocatedThisCycle, &quot;\n&quot;);
2262     }
</pre>
<hr />
<pre>
2342         dataLog(&quot;sizeAfterLastCollect = &quot;, m_sizeAfterLastCollect, &quot;\n&quot;);
2343     m_bytesAllocatedThisCycle = 0;
2344 
2345     if (Options::logGC())
2346         dataLog(&quot;=&gt; &quot;, currentHeapSize / 1024, &quot;kb, &quot;);
2347 }
2348 
2349 void Heap::didFinishCollection()
2350 {
2351     m_afterGC = MonotonicTime::now();
2352     CollectionScope scope = *m_collectionScope;
2353     if (scope == CollectionScope::Full)
2354         m_lastFullGCLength = m_afterGC - m_beforeGC;
2355     else
2356         m_lastEdenGCLength = m_afterGC - m_beforeGC;
2357 
2358 #if ENABLE(RESOURCE_USAGE)
2359     ASSERT(externalMemorySize() &lt;= extraMemorySize());
2360 #endif
2361 
<span class="line-modified">2362     if (HeapProfiler* heapProfiler = m_vm.heapProfiler()) {</span>
<span class="line-modified">2363         gatherExtraHeapData(*heapProfiler);</span>
2364         removeDeadHeapSnapshotNodes(*heapProfiler);
2365     }
2366 
2367     if (UNLIKELY(m_verifier))
2368         m_verifier-&gt;endGC();
2369 
2370     RELEASE_ASSERT(m_collectionScope);
2371     m_lastCollectionScope = m_collectionScope;
2372     m_collectionScope = WTF::nullopt;
2373 
2374     for (auto* observer : m_observers)
2375         observer-&gt;didGarbageCollect(scope);
2376 }
2377 
2378 void Heap::resumeCompilerThreads()
2379 {
2380 #if ENABLE(DFG_JIT)
2381     if (!VM::canUseJIT())
2382         return;
2383     for (unsigned i = DFG::numberOfWorklists(); i--;)
</pre>
<hr />
<pre>
2545 void Heap::forEachCodeBlockIgnoringJITPlansImpl(const AbstractLocker&amp; locker, const ScopedLambda&lt;void(CodeBlock*)&gt;&amp; func)
2546 {
2547     return m_codeBlocks-&gt;iterate(locker, func);
2548 }
2549 
2550 void Heap::writeBarrierSlowPath(const JSCell* from)
2551 {
2552     if (UNLIKELY(mutatorShouldBeFenced())) {
2553         // In this case, the barrierThreshold is the tautological threshold, so from could still be
2554         // not black. But we can&#39;t know for sure until we fire off a fence.
2555         WTF::storeLoadFence();
2556         if (from-&gt;cellState() != CellState::PossiblyBlack)
2557             return;
2558     }
2559 
2560     addToRememberedSet(from);
2561 }
2562 
2563 bool Heap::isCurrentThreadBusy()
2564 {
<span class="line-modified">2565     return Thread::mayBeGCThread() || mutatorState() != MutatorState::Running;</span>
2566 }
2567 
2568 void Heap::reportExtraMemoryVisited(size_t size)
2569 {
2570     size_t* counter = &amp;m_extraMemorySize;
2571 
2572     for (;;) {
2573         size_t oldSize = *counter;
2574         // FIXME: Change this to use SaturatedArithmetic when available.
2575         // https://bugs.webkit.org/show_bug.cgi?id=170411
2576         Checked&lt;size_t, RecordOverflow&gt; checkedNewSize = oldSize;
2577         checkedNewSize += size;
2578         size_t newSize = UNLIKELY(checkedNewSize.hasOverflowed()) ? std::numeric_limits&lt;size_t&gt;::max() : checkedNewSize.unsafeGet();
2579         if (WTF::atomicCompareExchangeWeakRelaxed(counter, oldSize, newSize))
2580             return;
2581     }
2582 }
2583 
2584 #if ENABLE(RESOURCE_USAGE)
2585 void Heap::reportExternalMemoryVisited(size_t size)
</pre>
<hr />
<pre>
2684 void Heap::didFreeBlock(size_t capacity)
2685 {
2686 #if ENABLE(RESOURCE_USAGE)
2687     m_blockBytesAllocated -= capacity;
2688 #else
2689     UNUSED_PARAM(capacity);
2690 #endif
2691 }
2692 
2693 void Heap::addCoreConstraints()
2694 {
2695     m_constraintSet-&gt;add(
2696         &quot;Cs&quot;, &quot;Conservative Scan&quot;,
2697         [this, lastVersion = static_cast&lt;uint64_t&gt;(0)] (SlotVisitor&amp; slotVisitor) mutable {
2698             bool shouldNotProduceWork = lastVersion == m_phaseVersion;
2699             if (shouldNotProduceWork)
2700                 return;
2701 
2702             TimingScope preConvergenceTimingScope(*this, &quot;Constraint: conservative scan&quot;);
2703             m_objectSpace.prepareForConservativeScan();
<span class="line-added">2704             m_jitStubRoutines-&gt;prepareForConservativeScan();</span>
2705 
2706             {
2707                 ConservativeRoots conservativeRoots(*this);
2708                 SuperSamplerScope superSamplerScope(false);
2709 
2710                 gatherStackRoots(conservativeRoots);
2711                 gatherJSStackRoots(conservativeRoots);
2712                 gatherScratchBufferRoots(conservativeRoots);
2713 
2714                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ConservativeScan);
2715                 slotVisitor.append(conservativeRoots);
2716             }
2717             if (VM::canUseJIT()) {
2718                 // JITStubRoutines must be visited after scanning ConservativeRoots since JITStubRoutines depend on the hook executed during gathering ConservativeRoots.
2719                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::JITStubRoutines);
2720                 m_jitStubRoutines-&gt;traceMarkedStubRoutines(slotVisitor);
2721             }
2722 
2723             lastVersion = m_phaseVersion;
2724         },
2725         ConstraintVolatility::GreyedByExecution);
2726 
2727     m_constraintSet-&gt;add(
2728         &quot;Msr&quot;, &quot;Misc Small Roots&quot;,
2729         [this] (SlotVisitor&amp; slotVisitor) {
2730 
2731 #if JSC_OBJC_API_ENABLED
<span class="line-modified">2732             scanExternalRememberedSet(m_vm, slotVisitor);</span>
2733 #endif
<span class="line-modified">2734             if (m_vm.smallStrings.needsToBeVisited(*m_collectionScope)) {</span>
2735                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::StrongReferences);
<span class="line-modified">2736                 m_vm.smallStrings.visitStrongReferences(slotVisitor);</span>
2737             }
2738 
2739             {
2740                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ProtectedValues);
2741                 for (auto&amp; pair : m_protectedValues)
2742                     slotVisitor.appendUnbarriered(pair.key);
2743             }
2744 
2745             if (m_markListSet &amp;&amp; m_markListSet-&gt;size()) {
2746                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ConservativeScan);
2747                 MarkedArgumentBuffer::markLists(slotVisitor, *m_markListSet);
2748             }
2749 
2750             {
2751                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::VMExceptions);
<span class="line-modified">2752                 slotVisitor.appendUnbarriered(m_vm.exception());</span>
<span class="line-modified">2753                 slotVisitor.appendUnbarriered(m_vm.lastException());</span>
2754             }
2755         },
2756         ConstraintVolatility::GreyedByExecution);
2757 
2758     m_constraintSet-&gt;add(
2759         &quot;Sh&quot;, &quot;Strong Handles&quot;,
2760         [this] (SlotVisitor&amp; slotVisitor) {
2761             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::StrongHandles);
2762             m_handleSet.visitStrongHandles(slotVisitor);
2763         },
2764         ConstraintVolatility::GreyedByExecution);
2765 
2766     m_constraintSet-&gt;add(
2767         &quot;D&quot;, &quot;Debugger&quot;,
2768         [this] (SlotVisitor&amp; slotVisitor) {
2769             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::Debugger);
2770 
2771 #if ENABLE(SAMPLING_PROFILER)
<span class="line-modified">2772             if (SamplingProfiler* samplingProfiler = m_vm.samplingProfiler()) {</span>
2773                 LockHolder locker(samplingProfiler-&gt;getLock());
2774                 samplingProfiler-&gt;processUnverifiedStackTraces();
2775                 samplingProfiler-&gt;visit(slotVisitor);
2776                 if (Options::logGC() == GCLogging::Verbose)
2777                     dataLog(&quot;Sampling Profiler data:\n&quot;, slotVisitor);
2778             }
2779 #endif // ENABLE(SAMPLING_PROFILER)
2780 
<span class="line-modified">2781             if (m_vm.typeProfiler())</span>
<span class="line-modified">2782                 m_vm.typeProfilerLog()-&gt;visit(slotVisitor);</span>
2783 
<span class="line-modified">2784             if (auto* shadowChicken = m_vm.shadowChicken())</span>
2785                 shadowChicken-&gt;visitChildren(slotVisitor);
2786         },
2787         ConstraintVolatility::GreyedByExecution);
2788 
2789     m_constraintSet-&gt;add(
2790         &quot;Ws&quot;, &quot;Weak Sets&quot;,
2791         [this] (SlotVisitor&amp; slotVisitor) {
2792             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::WeakSets);
2793             m_objectSpace.visitWeakSets(slotVisitor);
2794         },
2795         ConstraintVolatility::GreyedByMarking);
2796 
2797     m_constraintSet-&gt;add(
2798         &quot;O&quot;, &quot;Output&quot;,
2799         [] (SlotVisitor&amp; slotVisitor) {
2800             VM&amp; vm = slotVisitor.vm();
2801 
2802             auto callOutputConstraint = [] (SlotVisitor&amp; slotVisitor, HeapCell* heapCell, HeapCell::Kind) {
2803                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::Output);
2804                 VM&amp; vm = slotVisitor.vm();
</pre>
<hr />
<pre>
2813             add(vm.executableToCodeBlockEdgesWithConstraints);
2814             if (vm.m_weakMapSpace)
2815                 add(*vm.m_weakMapSpace);
2816         },
2817         ConstraintVolatility::GreyedByMarking,
2818         ConstraintParallelism::Parallel);
2819 
2820 #if ENABLE(DFG_JIT)
2821     if (VM::canUseJIT()) {
2822         m_constraintSet-&gt;add(
2823             &quot;Dw&quot;, &quot;DFG Worklists&quot;,
2824             [this] (SlotVisitor&amp; slotVisitor) {
2825                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::DFGWorkLists);
2826 
2827                 for (unsigned i = DFG::numberOfWorklists(); i--;)
2828                     DFG::existingWorklistForIndex(i).visitWeakReferences(slotVisitor);
2829 
2830                 // FIXME: This is almost certainly unnecessary.
2831                 // https://bugs.webkit.org/show_bug.cgi?id=166829
2832                 DFG::iterateCodeBlocksForGC(
<span class="line-modified">2833                     m_vm,</span>
2834                     [&amp;] (CodeBlock* codeBlock) {
2835                         slotVisitor.appendUnbarriered(codeBlock);
2836                     });
2837 
2838                 if (Options::logGC() == GCLogging::Verbose)
2839                     dataLog(&quot;DFG Worklists:\n&quot;, slotVisitor);
2840             },
2841             ConstraintVolatility::GreyedByMarking);
2842     }
2843 #endif
2844 
2845     m_constraintSet-&gt;add(
2846         &quot;Cb&quot;, &quot;CodeBlocks&quot;,
2847         [this] (SlotVisitor&amp; slotVisitor) {
2848             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::CodeBlocks);
2849             iterateExecutingAndCompilingCodeBlocksWithoutHoldingLocks(
2850                 [&amp;] (CodeBlock* codeBlock) {
2851                     // Visit the CodeBlock as a constraint only if it&#39;s black.
<span class="line-modified">2852                     if (isMarked(codeBlock)</span>
2853                         &amp;&amp; codeBlock-&gt;cellState() == CellState::PossiblyBlack)
2854                         slotVisitor.visitAsConstraint(codeBlock);
2855                 });
2856         },
2857         ConstraintVolatility::SeldomGreyed);
2858 
<span class="line-modified">2859     m_constraintSet-&gt;add(makeUnique&lt;MarkStackMergingConstraint&gt;(*this));</span>
2860 }
2861 
2862 void Heap::addMarkingConstraint(std::unique_ptr&lt;MarkingConstraint&gt; constraint)
2863 {
2864     PreventCollectionScope preventCollectionScope(*this);
2865     m_constraintSet-&gt;add(WTFMove(constraint));
2866 }
2867 
2868 void Heap::notifyIsSafeToCollect()
2869 {
2870     MonotonicTime before;
2871     if (Options::logGC()) {
2872         before = MonotonicTime::now();
2873         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: starting &quot;);
2874     }
2875 
2876     addCoreConstraints();
2877 
2878     m_isSafeToCollect = true;
2879 
2880     if (Options::collectContinuously()) {
<span class="line-modified">2881         m_collectContinuouslyThread = Thread::create(</span>
2882             &quot;JSC DEBUG Continuous GC&quot;,
2883             [this] () {
2884                 MonotonicTime initialTime = MonotonicTime::now();
2885                 Seconds period = Seconds::fromMilliseconds(Options::collectContinuouslyPeriodMS());
2886                 while (!m_shouldStopCollectingContinuously) {
2887                     {
2888                         LockHolder locker(*m_threadLock);
2889                         if (m_requests.isEmpty()) {
2890                             m_requests.append(WTF::nullopt);
2891                             m_lastGrantedTicket++;
2892                             m_threadCondition-&gt;notifyOne(locker);
2893                         }
2894                     }
2895 
2896                     {
2897                         LockHolder locker(m_collectContinuouslyLock);
2898                         Seconds elapsed = MonotonicTime::now() - initialTime;
2899                         Seconds elapsedInPeriod = elapsed % period;
2900                         MonotonicTime timeToWakeUp =
2901                             initialTime + elapsed - elapsedInPeriod + period;
</pre>
</td>
</tr>
</table>
<center><a href="HandleSet.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="Heap.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>