<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/WebCore/Modules/mediasource/SourceBuffer.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2013 Google Inc. All rights reserved.
   3  * Copyright (C) 2013-2019 Apple Inc. All rights reserved.
   4  *
   5  * Redistribution and use in source and binary forms, with or without
   6  * modification, are permitted provided that the following conditions are
   7  * met:
   8  *
   9  *     * Redistributions of source code must retain the above copyright
  10  * notice, this list of conditions and the following disclaimer.
  11  *     * Redistributions in binary form must reproduce the above
  12  * copyright notice, this list of conditions and the following disclaimer
  13  * in the documentation and/or other materials provided with the
  14  * distribution.
  15  *     * Neither the name of Google Inc. nor the names of its
  16  * contributors may be used to endorse or promote products derived from
  17  * this software without specific prior written permission.
  18  *
  19  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
  20  * &quot;AS IS&quot; AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
  21  * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
  22  * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
  23  * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
  24  * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
  25  * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
  26  * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
  27  * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  28  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  29  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  30  */
  31 
  32 #include &quot;config.h&quot;
  33 #include &quot;SourceBuffer.h&quot;
  34 
  35 #if ENABLE(MEDIA_SOURCE)
  36 
  37 #include &quot;AudioTrackList.h&quot;
  38 #include &quot;BufferSource.h&quot;
  39 #include &quot;Event.h&quot;
  40 #include &quot;EventNames.h&quot;
  41 #include &quot;GenericEventQueue.h&quot;
  42 #include &quot;HTMLMediaElement.h&quot;
  43 #include &quot;InbandTextTrack.h&quot;
  44 #include &quot;Logging.h&quot;
  45 #include &quot;MediaDescription.h&quot;
  46 #include &quot;MediaSample.h&quot;
  47 #include &quot;MediaSource.h&quot;
  48 #include &quot;SampleMap.h&quot;
  49 #include &quot;SourceBufferList.h&quot;
  50 #include &quot;SourceBufferPrivate.h&quot;
  51 #include &quot;TextTrackList.h&quot;
  52 #include &quot;TimeRanges.h&quot;
  53 #include &quot;VideoTrackList.h&quot;
  54 #include &lt;JavaScriptCore/JSCInlines.h&gt;
  55 #include &lt;JavaScriptCore/JSLock.h&gt;
  56 #include &lt;JavaScriptCore/VM.h&gt;
  57 #include &lt;limits&gt;
  58 #include &lt;wtf/CheckedArithmetic.h&gt;
  59 #include &lt;wtf/IsoMallocInlines.h&gt;
  60 
  61 namespace WebCore {
  62 
  63 WTF_MAKE_ISO_ALLOCATED_IMPL(SourceBuffer);
  64 
  65 static const double ExponentialMovingAverageCoefficient = 0.1;
  66 
  67 struct SourceBuffer::TrackBuffer {
  68     MediaTime lastDecodeTimestamp;
  69     MediaTime greatestDecodeDuration;
  70     MediaTime lastFrameDuration;
  71     MediaTime highestPresentationTimestamp;
  72     MediaTime lastEnqueuedPresentationTime;
  73     MediaTime minimumEnqueuedPresentationTime;
  74     DecodeOrderSampleMap::KeyType lastEnqueuedDecodeKey;
  75     MediaTime lastEnqueuedDecodeDuration;
  76     MediaTime roundedTimestampOffset;
  77     uint32_t lastFrameTimescale { 0 };
  78     bool needRandomAccessFlag { true };
  79     bool enabled { false };
  80     bool needsReenqueueing { false };
  81     bool needsMinimumUpcomingPresentationTimeUpdating { false };
  82     SampleMap samples;
  83     DecodeOrderSampleMap::MapType decodeQueue;
  84     RefPtr&lt;MediaDescription&gt; description;
  85     PlatformTimeRanges buffered;
  86 
  87     TrackBuffer()
  88         : lastDecodeTimestamp(MediaTime::invalidTime())
  89         , greatestDecodeDuration(MediaTime::invalidTime())
  90         , lastFrameDuration(MediaTime::invalidTime())
  91         , highestPresentationTimestamp(MediaTime::invalidTime())
  92         , lastEnqueuedPresentationTime(MediaTime::invalidTime())
  93         , lastEnqueuedDecodeKey({MediaTime::invalidTime(), MediaTime::invalidTime()})
  94         , lastEnqueuedDecodeDuration(MediaTime::invalidTime())
  95     {
  96     }
  97 };
  98 
  99 Ref&lt;SourceBuffer&gt; SourceBuffer::create(Ref&lt;SourceBufferPrivate&gt;&amp;&amp; sourceBufferPrivate, MediaSource* source)
 100 {
 101     auto sourceBuffer = adoptRef(*new SourceBuffer(WTFMove(sourceBufferPrivate), source));
 102     sourceBuffer-&gt;suspendIfNeeded();
 103     return sourceBuffer;
 104 }
 105 
 106 SourceBuffer::SourceBuffer(Ref&lt;SourceBufferPrivate&gt;&amp;&amp; sourceBufferPrivate, MediaSource* source)
 107     : ActiveDOMObject(source-&gt;scriptExecutionContext())
 108     , m_private(WTFMove(sourceBufferPrivate))
 109     , m_source(source)
 110     , m_asyncEventQueue(*this)
 111     , m_appendBufferTimer(*this, &amp;SourceBuffer::appendBufferTimerFired)
 112     , m_appendWindowStart(MediaTime::zeroTime())
 113     , m_appendWindowEnd(MediaTime::positiveInfiniteTime())
 114     , m_groupStartTimestamp(MediaTime::invalidTime())
 115     , m_groupEndTimestamp(MediaTime::zeroTime())
 116     , m_buffered(TimeRanges::create())
 117     , m_appendState(WaitingForSegment)
 118     , m_timeOfBufferingMonitor(MonotonicTime::now())
 119     , m_pendingRemoveStart(MediaTime::invalidTime())
 120     , m_pendingRemoveEnd(MediaTime::invalidTime())
 121     , m_removeTimer(*this, &amp;SourceBuffer::removeTimerFired)
 122 #if !RELEASE_LOG_DISABLED
 123     , m_logger(m_private-&gt;sourceBufferLogger())
 124     , m_logIdentifier(m_private-&gt;sourceBufferLogIdentifier())
 125 #endif
 126 {
 127     ASSERT(m_source);
 128     ALWAYS_LOG(LOGIDENTIFIER);
 129 
 130     m_private-&gt;setClient(this);
 131 }
 132 
 133 SourceBuffer::~SourceBuffer()
 134 {
 135     ASSERT(isRemoved());
 136     ALWAYS_LOG(LOGIDENTIFIER);
 137 
 138     m_private-&gt;setClient(nullptr);
 139 }
 140 
 141 ExceptionOr&lt;Ref&lt;TimeRanges&gt;&gt; SourceBuffer::buffered() const
 142 {
 143     // Section 3.1 buffered attribute steps.
 144     // https://dvcs.w3.org/hg/html-media/raw-file/default/media-source/media-source.html#attributes-1
 145     // 1. If this object has been removed from the sourceBuffers attribute of the parent media source then throw an
 146     //    InvalidStateError exception and abort these steps.
 147     if (isRemoved())
 148         return Exception { InvalidStateError };
 149 
 150     // 2. Return a new static normalized TimeRanges object for the media segments buffered.
 151     return m_buffered-&gt;copy();
 152 }
 153 
 154 double SourceBuffer::timestampOffset() const
 155 {
 156     return m_timestampOffset.toDouble();
 157 }
 158 
 159 ExceptionOr&lt;void&gt; SourceBuffer::setTimestampOffset(double offset)
 160 {
 161     // Section 3.1 timestampOffset attribute setter steps.
 162     // https://dvcs.w3.org/hg/html-media/raw-file/default/media-source/media-source.html#attributes-1
 163     // 1. Let new timestamp offset equal the new value being assigned to this attribute.
 164     // 2. If this object has been removed from the sourceBuffers attribute of the parent media source, then throw an
 165     //    InvalidStateError exception and abort these steps.
 166     // 3. If the updating attribute equals true, then throw an InvalidStateError exception and abort these steps.
 167     if (isRemoved() || m_updating)
 168         return Exception { InvalidStateError };
 169 
 170     // 4. If the readyState attribute of the parent media source is in the &quot;ended&quot; state then run the following steps:
 171     // 4.1 Set the readyState attribute of the parent media source to &quot;open&quot;
 172     // 4.2 Queue a task to fire a simple event named sourceopen at the parent media source.
 173     m_source-&gt;openIfInEndedState();
 174 
 175     // 5. If the append state equals PARSING_MEDIA_SEGMENT, then throw an InvalidStateError and abort these steps.
 176     if (m_appendState == ParsingMediaSegment)
 177         return Exception { InvalidStateError };
 178 
 179     MediaTime newTimestampOffset = MediaTime::createWithDouble(offset);
 180 
 181     // 6. If the mode attribute equals &quot;sequence&quot;, then set the group start timestamp to new timestamp offset.
 182     if (m_mode == AppendMode::Sequence)
 183         m_groupStartTimestamp = newTimestampOffset;
 184 
 185     // 7. Update the attribute to the new value.
 186     m_timestampOffset = newTimestampOffset;
 187 
 188     for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
 189         trackBuffer.lastFrameTimescale = 0;
 190         trackBuffer.roundedTimestampOffset = MediaTime::invalidTime();
 191     }
 192 
 193     return { };
 194 }
 195 
 196 double SourceBuffer::appendWindowStart() const
 197 {
 198     return m_appendWindowStart.toDouble();
 199 }
 200 
 201 ExceptionOr&lt;void&gt; SourceBuffer::setAppendWindowStart(double newValue)
 202 {
 203     // Section 3.1 appendWindowStart attribute setter steps.
 204     // W3C Editor&#39;s Draft 16 September 2016
 205     // https://rawgit.com/w3c/media-source/45627646344eea0170dd1cbc5a3d508ca751abb8/media-source-respec.html#dom-sourcebuffer-appendwindowstart
 206     // 1. If this object has been removed from the sourceBuffers attribute of the parent media source,
 207     //    then throw an InvalidStateError  exception and abort these steps.
 208     // 2. If the updating attribute equals true, then throw an InvalidStateError exception and abort these steps.
 209     if (isRemoved() || m_updating)
 210         return Exception { InvalidStateError };
 211 
 212     // 3. If the new value is less than 0 or greater than or equal to appendWindowEnd then
 213     //    throw an TypeError exception and abort these steps.
 214     if (newValue &lt; 0 || newValue &gt;= m_appendWindowEnd.toDouble())
 215         return Exception { TypeError };
 216 
 217     // 4. Update the attribute to the new value.
 218     m_appendWindowStart = MediaTime::createWithDouble(newValue);
 219 
 220     return { };
 221 }
 222 
 223 double SourceBuffer::appendWindowEnd() const
 224 {
 225     return m_appendWindowEnd.toDouble();
 226 }
 227 
 228 ExceptionOr&lt;void&gt; SourceBuffer::setAppendWindowEnd(double newValue)
 229 {
 230     // Section 3.1 appendWindowEnd attribute setter steps.
 231     // W3C Editor&#39;s Draft 16 September 2016
 232     // https://rawgit.com/w3c/media-source/45627646344eea0170dd1cbc5a3d508ca751abb8/media-source-respec.html#dom-sourcebuffer-appendwindowend
 233     // 1. If this object has been removed from the sourceBuffers attribute of the parent media source,
 234     //    then throw an InvalidStateError exception and abort these steps.
 235     // 2. If the updating attribute equals true, then throw an InvalidStateError exception and abort these steps.
 236     if (isRemoved() || m_updating)
 237         return Exception { InvalidStateError };
 238 
 239     // 3. If the new value equals NaN, then throw an TypeError and abort these steps.
 240     // 4. If the new value is less than or equal to appendWindowStart then throw an TypeError exception
 241     //    and abort these steps.
 242     if (std::isnan(newValue) || newValue &lt;= m_appendWindowStart.toDouble())
 243         return Exception { TypeError };
 244 
 245     // 5.. Update the attribute to the new value.
 246     m_appendWindowEnd = MediaTime::createWithDouble(newValue);
 247 
 248     return { };
 249 }
 250 
 251 ExceptionOr&lt;void&gt; SourceBuffer::appendBuffer(const BufferSource&amp; data)
 252 {
 253     return appendBufferInternal(static_cast&lt;const unsigned char*&gt;(data.data()), data.length());
 254 }
 255 
 256 void SourceBuffer::resetParserState()
 257 {
 258     // Section 3.5.2 Reset Parser State algorithm steps.
 259     // http://www.w3.org/TR/2014/CR-media-source-20140717/#sourcebuffer-reset-parser-state
 260     // 1. If the append state equals PARSING_MEDIA_SEGMENT and the input buffer contains some complete coded frames,
 261     //    then run the coded frame processing algorithm until all of these complete coded frames have been processed.
 262     // FIXME: If any implementation will work in pulling mode (instead of async push to SourceBufferPrivate, and forget)
 263     //     this should be handled somehow either here, or in m_private-&gt;abort();
 264 
 265     // 2. Unset the last decode timestamp on all track buffers.
 266     // 3. Unset the last frame duration on all track buffers.
 267     // 4. Unset the highest presentation timestamp on all track buffers.
 268     // 5. Set the need random access point flag on all track buffers to true.
 269     for (auto&amp; trackBufferPair : m_trackBufferMap.values()) {
 270         trackBufferPair.lastDecodeTimestamp = MediaTime::invalidTime();
 271         trackBufferPair.greatestDecodeDuration = MediaTime::invalidTime();
 272         trackBufferPair.lastFrameDuration = MediaTime::invalidTime();
 273         trackBufferPair.highestPresentationTimestamp = MediaTime::invalidTime();
 274         trackBufferPair.needRandomAccessFlag = true;
 275     }
 276     // 6. Remove all bytes from the input buffer.
 277     // Note: this is handled by abortIfUpdating()
 278     // 7. Set append state to WAITING_FOR_SEGMENT.
 279     m_appendState = WaitingForSegment;
 280 
 281     m_private-&gt;resetParserState();
 282 }
 283 
 284 ExceptionOr&lt;void&gt; SourceBuffer::abort()
 285 {
 286     // Section 3.2 abort() method steps.
 287     // https://rawgit.com/w3c/media-source/45627646344eea0170dd1cbc5a3d508ca751abb8/media-source-respec.html#dom-sourcebuffer-abort
 288     // 1. If this object has been removed from the sourceBuffers attribute of the parent media source
 289     //    then throw an InvalidStateError exception and abort these steps.
 290     // 2. If the readyState attribute of the parent media source is not in the &quot;open&quot; state
 291     //    then throw an InvalidStateError exception and abort these steps.
 292     if (isRemoved() || !m_source-&gt;isOpen())
 293         return Exception { InvalidStateError };
 294 
 295     // 3. If the range removal algorithm is running, then throw an InvalidStateError exception and abort these steps.
 296     if (m_removeTimer.isActive())
 297         return Exception { InvalidStateError };
 298 
 299     // 4. If the sourceBuffer.updating attribute equals true, then run the following steps: ...
 300     abortIfUpdating();
 301 
 302     // 5. Run the reset parser state algorithm.
 303     resetParserState();
 304 
 305     // 6. Set appendWindowStart to the presentation start time.
 306     m_appendWindowStart = MediaTime::zeroTime();
 307 
 308     // 7. Set appendWindowEnd to positive Infinity.
 309     m_appendWindowEnd = MediaTime::positiveInfiniteTime();
 310 
 311     return { };
 312 }
 313 
 314 ExceptionOr&lt;void&gt; SourceBuffer::remove(double start, double end)
 315 {
 316     return remove(MediaTime::createWithDouble(start), MediaTime::createWithDouble(end));
 317 }
 318 
 319 ExceptionOr&lt;void&gt; SourceBuffer::remove(const MediaTime&amp; start, const MediaTime&amp; end)
 320 {
 321     DEBUG_LOG(LOGIDENTIFIER, &quot;start = &quot;, start, &quot;, end = &quot;, end);
 322 
 323     // https://rawgit.com/w3c/media-source/45627646344eea0170dd1cbc5a3d508ca751abb8/media-source-respec.html#dom-sourcebuffer-remove
 324     // Section 3.2 remove() method steps.
 325     // 1. If this object has been removed from the sourceBuffers attribute of the parent media source then throw
 326     //    an InvalidStateError exception and abort these steps.
 327     // 2. If the updating attribute equals true, then throw an InvalidStateError exception and abort these steps.
 328     if (isRemoved() || m_updating)
 329         return Exception { InvalidStateError };
 330 
 331     // 3. If duration equals NaN, then throw a TypeError exception and abort these steps.
 332     // 4. If start is negative or greater than duration, then throw a TypeError exception and abort these steps.
 333     // 5. If end is less than or equal to start or end equals NaN, then throw a TypeError exception and abort these steps.
 334     if (m_source-&gt;duration().isInvalid()
 335         || end.isInvalid()
 336         || start.isInvalid()
 337         || start &lt; MediaTime::zeroTime()
 338         || start &gt; m_source-&gt;duration()
 339         || end &lt;= start) {
 340         return Exception { TypeError };
 341     }
 342 
 343     // 6. If the readyState attribute of the parent media source is in the &quot;ended&quot; state then run the following steps:
 344     // 6.1. Set the readyState attribute of the parent media source to &quot;open&quot;
 345     // 6.2. Queue a task to fire a simple event named sourceopen at the parent media source .
 346     m_source-&gt;openIfInEndedState();
 347 
 348     // 7. Run the range removal algorithm with start and end as the start and end of the removal range.
 349     rangeRemoval(start, end);
 350 
 351     return { };
 352 }
 353 
 354 void SourceBuffer::rangeRemoval(const MediaTime&amp; start, const MediaTime&amp; end)
 355 {
 356     // 3.5.7 Range Removal
 357     // https://rawgit.com/w3c/media-source/7bbe4aa33c61ec025bc7acbd80354110f6a000f9/media-source.html#sourcebuffer-range-removal
 358     // 1. Let start equal the starting presentation timestamp for the removal range.
 359     // 2. Let end equal the end presentation timestamp for the removal range.
 360     // 3. Set the updating attribute to true.
 361     m_updating = true;
 362 
 363     // 4. Queue a task to fire a simple event named updatestart at this SourceBuffer object.
 364     scheduleEvent(eventNames().updatestartEvent);
 365 
 366     // 5. Return control to the caller and run the rest of the steps asynchronously.
 367     m_pendingRemoveStart = start;
 368     m_pendingRemoveEnd = end;
 369     m_removeTimer.startOneShot(0_s);
 370 }
 371 
 372 ExceptionOr&lt;void&gt; SourceBuffer::changeType(const String&amp; type)
 373 {
 374     // changeType() proposed API. See issue #155: &lt;https://github.com/w3c/media-source/issues/155&gt;
 375     // https://rawgit.com/wicg/media-source/codec-switching/index.html#dom-sourcebuffer-changetype
 376 
 377     // 1. If type is an empty string then throw a TypeError exception and abort these steps.
 378     if (type.isEmpty())
 379         return Exception { TypeError };
 380 
 381     // 2. If this object has been removed from the sourceBuffers attribute of the parent media source,
 382     // then throw an InvalidStateError exception and abort these steps.
 383     // 3. If the updating attribute equals true, then throw an InvalidStateError exception and abort these steps.
 384     if (isRemoved() || m_updating)
 385         return Exception { InvalidStateError };
 386 
 387     // 4. If type contains a MIME type that is not supported or contains a MIME type that is not supported with
 388     // the types specified (currently or previously) of SourceBuffer objects in the sourceBuffers attribute of
 389     // the parent media source, then throw a NotSupportedError exception and abort these steps.
 390     ContentType contentType(type);
 391     if (!m_private-&gt;canSwitchToType(contentType))
 392         return Exception { NotSupportedError };
 393 
 394     // 5. If the readyState attribute of the parent media source is in the &quot;ended&quot; state then run the following
 395     // steps:
 396     // 5.1. Set the readyState attribute of the parent media source to &quot;open&quot;
 397     // 5.2. Queue a task to fire a simple event named sourceopen at the parent media source.
 398     m_source-&gt;openIfInEndedState();
 399 
 400     // 6. Run the reset parser state algorithm.
 401     resetParserState();
 402 
 403     // 7. Update the generate timestamps flag on this SourceBuffer object to the value in the &quot;Generate Timestamps
 404     // Flag&quot; column of the byte stream format registry [MSE-REGISTRY] entry that is associated with type.
 405     setShouldGenerateTimestamps(MediaSource::contentTypeShouldGenerateTimestamps(contentType));
 406 
 407     // ↳ If the generate timestamps flag equals true:
 408     // Set the mode attribute on this SourceBuffer object to &quot;sequence&quot;, including running the associated steps
 409     // for that attribute being set.
 410     if (m_shouldGenerateTimestamps)
 411         setMode(AppendMode::Sequence);
 412 
 413     // ↳ Otherwise:
 414     // Keep the previous value of the mode attribute on this SourceBuffer object, without running any associated
 415     // steps for that attribute being set.
 416     // NOTE: No-op.
 417 
 418     // 9. Set pending initialization segment for changeType flag to true.
 419     m_pendingInitializationSegmentForChangeType = true;
 420 
 421     return { };
 422 }
 423 
 424 void SourceBuffer::abortIfUpdating()
 425 {
 426     // Section 3.2 abort() method step 4 substeps.
 427     // https://rawgit.com/w3c/media-source/45627646344eea0170dd1cbc5a3d508ca751abb8/media-source-respec.html#dom-sourcebuffer-abort
 428 
 429     if (!m_updating)
 430         return;
 431 
 432     // 4.1. Abort the buffer append algorithm if it is running.
 433     m_appendBufferTimer.stop();
 434     m_pendingAppendData.clear();
 435     m_private-&gt;abort();
 436 
 437     // 4.2. Set the updating attribute to false.
 438     m_updating = false;
 439 
 440     // 4.3. Queue a task to fire a simple event named abort at this SourceBuffer object.
 441     scheduleEvent(eventNames().abortEvent);
 442 
 443     // 4.4. Queue a task to fire a simple event named updateend at this SourceBuffer object.
 444     scheduleEvent(eventNames().updateendEvent);
 445 }
 446 
 447 MediaTime SourceBuffer::highestPresentationTimestamp() const
 448 {
 449     MediaTime highestTime;
 450     for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
 451         auto lastSampleIter = trackBuffer.samples.presentationOrder().rbegin();
 452         if (lastSampleIter == trackBuffer.samples.presentationOrder().rend())
 453             continue;
 454         highestTime = std::max(highestTime, lastSampleIter-&gt;first);
 455     }
 456     return highestTime;
 457 }
 458 
 459 void SourceBuffer::readyStateChanged()
 460 {
 461     updateBufferedFromTrackBuffers();
 462 }
 463 
 464 void SourceBuffer::removedFromMediaSource()
 465 {
 466     if (isRemoved())
 467         return;
 468 
 469     abortIfUpdating();
 470 
 471     for (auto&amp; trackBufferPair : m_trackBufferMap.values()) {
 472         trackBufferPair.samples.clear();
 473         trackBufferPair.decodeQueue.clear();
 474     }
 475 
 476     m_private-&gt;removedFromMediaSource();
 477     m_source = nullptr;
 478 }
 479 
 480 void SourceBuffer::seekToTime(const MediaTime&amp; time)
 481 {
 482     ALWAYS_LOG(LOGIDENTIFIER, time);
 483 
 484     for (auto&amp; trackBufferPair : m_trackBufferMap) {
 485         TrackBuffer&amp; trackBuffer = trackBufferPair.value;
 486         const AtomString&amp; trackID = trackBufferPair.key;
 487 
 488         trackBuffer.needsReenqueueing = true;
 489         reenqueueMediaForTime(trackBuffer, trackID, time);
 490     }
 491 }
 492 
 493 MediaTime SourceBuffer::sourceBufferPrivateFastSeekTimeForMediaTime(const MediaTime&amp; targetTime, const MediaTime&amp; negativeThreshold, const MediaTime&amp; positiveThreshold)
 494 {
 495     MediaTime seekTime = targetTime;
 496     MediaTime lowerBoundTime = targetTime - negativeThreshold;
 497     MediaTime upperBoundTime = targetTime + positiveThreshold;
 498 
 499     for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
 500         // Find the sample which contains the target time time.
 501         auto futureSyncSampleIterator = trackBuffer.samples.decodeOrder().findSyncSampleAfterPresentationTime(targetTime, positiveThreshold);
 502         auto pastSyncSampleIterator = trackBuffer.samples.decodeOrder().findSyncSamplePriorToPresentationTime(targetTime, negativeThreshold);
 503         auto upperBound = trackBuffer.samples.decodeOrder().end();
 504         auto lowerBound = trackBuffer.samples.decodeOrder().rend();
 505 
 506         if (futureSyncSampleIterator == upperBound &amp;&amp; pastSyncSampleIterator == lowerBound)
 507             continue;
 508 
 509         MediaTime futureSeekTime = MediaTime::positiveInfiniteTime();
 510         if (futureSyncSampleIterator != upperBound) {
 511             RefPtr&lt;MediaSample&gt;&amp; sample = futureSyncSampleIterator-&gt;second;
 512             futureSeekTime = sample-&gt;presentationTime();
 513         }
 514 
 515         MediaTime pastSeekTime = MediaTime::negativeInfiniteTime();
 516         if (pastSyncSampleIterator != lowerBound) {
 517             RefPtr&lt;MediaSample&gt;&amp; sample = pastSyncSampleIterator-&gt;second;
 518             pastSeekTime = sample-&gt;presentationTime();
 519         }
 520 
 521         MediaTime trackSeekTime = abs(targetTime - futureSeekTime) &lt; abs(targetTime - pastSeekTime) ? futureSeekTime : pastSeekTime;
 522         if (abs(targetTime - trackSeekTime) &gt; abs(targetTime - seekTime))
 523             seekTime = trackSeekTime;
 524     }
 525 
 526     return seekTime;
 527 }
 528 
 529 bool SourceBuffer::hasPendingActivity() const
 530 {
 531     return m_source || m_asyncEventQueue.hasPendingEvents();
 532 }
 533 
 534 void SourceBuffer::suspend(ReasonForSuspension reason)
 535 {
 536     switch (reason) {
 537     case ReasonForSuspension::PageCache:
 538     case ReasonForSuspension::PageWillBeSuspended:
 539         m_asyncEventQueue.suspend();
 540         break;
 541     case ReasonForSuspension::JavaScriptDebuggerPaused:
 542     case ReasonForSuspension::WillDeferLoading:
 543         // Do nothing, we don&#39;t pause media playback in these cases.
 544         break;
 545     }
 546 }
 547 
 548 void SourceBuffer::resume()
 549 {
 550     m_asyncEventQueue.resume();
 551 }
 552 
 553 void SourceBuffer::stop()
 554 {
 555     m_asyncEventQueue.close();
 556     m_appendBufferTimer.stop();
 557     m_removeTimer.stop();
 558 }
 559 
 560 bool SourceBuffer::canSuspendForDocumentSuspension() const
 561 {
 562     return !hasPendingActivity();
 563 }
 564 
 565 const char* SourceBuffer::activeDOMObjectName() const
 566 {
 567     return &quot;SourceBuffer&quot;;
 568 }
 569 
 570 bool SourceBuffer::isRemoved() const
 571 {
 572     return !m_source;
 573 }
 574 
 575 void SourceBuffer::scheduleEvent(const AtomString&amp; eventName)
 576 {
 577     auto event = Event::create(eventName, Event::CanBubble::No, Event::IsCancelable::No);
 578     event-&gt;setTarget(this);
 579 
 580     m_asyncEventQueue.enqueueEvent(WTFMove(event));
 581 }
 582 
 583 ExceptionOr&lt;void&gt; SourceBuffer::appendBufferInternal(const unsigned char* data, unsigned size)
 584 {
 585     // Section 3.2 appendBuffer()
 586     // https://dvcs.w3.org/hg/html-media/raw-file/default/media-source/media-source.html#widl-SourceBuffer-appendBuffer-void-ArrayBufferView-data
 587 
 588     // Step 1 is enforced by the caller.
 589     // 2. Run the prepare append algorithm.
 590     // Section 3.5.4 Prepare AppendAlgorithm
 591 
 592     // 1. If the SourceBuffer has been removed from the sourceBuffers attribute of the parent media source
 593     // then throw an InvalidStateError exception and abort these steps.
 594     // 2. If the updating attribute equals true, then throw an InvalidStateError exception and abort these steps.
 595     if (isRemoved() || m_updating)
 596         return Exception { InvalidStateError };
 597 
 598     // 3. If the readyState attribute of the parent media source is in the &quot;ended&quot; state then run the following steps:
 599     // 3.1. Set the readyState attribute of the parent media source to &quot;open&quot;
 600     // 3.2. Queue a task to fire a simple event named sourceopen at the parent media source .
 601     m_source-&gt;openIfInEndedState();
 602 
 603     // 4. Run the coded frame eviction algorithm.
 604     evictCodedFrames(size);
 605 
 606     // FIXME: enable this code when MSE libraries have been updated to support it.
 607 #if USE(GSTREAMER)
 608     // 5. If the buffer full flag equals true, then throw a QuotaExceededError exception and abort these step.
 609     if (m_bufferFull) {
 610         ERROR_LOG(LOGIDENTIFIER, &quot;buffer full, failing with QuotaExceededError error&quot;);
 611         return Exception { QuotaExceededError };
 612     }
 613 #endif
 614 
 615     // NOTE: Return to 3.2 appendBuffer()
 616     // 3. Add data to the end of the input buffer.
 617     m_pendingAppendData.append(data, size);
 618 
 619     // 4. Set the updating attribute to true.
 620     m_updating = true;
 621 
 622     // 5. Queue a task to fire a simple event named updatestart at this SourceBuffer object.
 623     scheduleEvent(eventNames().updatestartEvent);
 624 
 625     // 6. Asynchronously run the buffer append algorithm.
 626     m_appendBufferTimer.startOneShot(0_s);
 627 
 628     reportExtraMemoryAllocated();
 629 
 630     return { };
 631 }
 632 
 633 void SourceBuffer::appendBufferTimerFired()
 634 {
 635     if (isRemoved())
 636         return;
 637 
 638     ASSERT(m_updating);
 639 
 640     // Section 3.5.5 Buffer Append Algorithm
 641     // https://dvcs.w3.org/hg/html-media/raw-file/default/media-source/media-source.html#sourcebuffer-buffer-append
 642 
 643     // 1. Run the segment parser loop algorithm.
 644 
 645     // Section 3.5.1 Segment Parser Loop
 646     // https://dvcs.w3.org/hg/html-media/raw-file/tip/media-source/media-source.html#sourcebuffer-segment-parser-loop
 647     // When the segment parser loop algorithm is invoked, run the following steps:
 648 
 649     // 1. Loop Top: If the input buffer is empty, then jump to the need more data step below.
 650     if (!m_pendingAppendData.size()) {
 651         sourceBufferPrivateAppendComplete(AppendSucceeded);
 652         return;
 653     }
 654 
 655     // Manually clear out the m_pendingAppendData Vector, in case the platform implementation
 656     // rejects appending the buffer for whatever reason.
 657     // FIXME: The implementation should guarantee the move from this Vector, and we should
 658     // assert here to confirm that. See https://bugs.webkit.org/show_bug.cgi?id=178003.
 659     m_private-&gt;append(WTFMove(m_pendingAppendData));
 660     m_pendingAppendData.clear();
 661 }
 662 
 663 void SourceBuffer::sourceBufferPrivateAppendComplete(AppendResult result)
 664 {
 665     if (isRemoved())
 666         return;
 667 
 668     // Resolve the changes it TrackBuffers&#39; buffered ranges
 669     // into the SourceBuffer&#39;s buffered ranges
 670     updateBufferedFromTrackBuffers();
 671 
 672     // Section 3.5.5 Buffer Append Algorithm, ctd.
 673     // https://dvcs.w3.org/hg/html-media/raw-file/default/media-source/media-source.html#sourcebuffer-buffer-append
 674 
 675     // 2. If the input buffer contains bytes that violate the SourceBuffer byte stream format specification,
 676     // then run the append error algorithm with the decode error parameter set to true and abort this algorithm.
 677     if (result == ParsingFailed) {
 678         ERROR_LOG(LOGIDENTIFIER, &quot;ParsingFailed&quot;);
 679         appendError(true);
 680         return;
 681     }
 682 
 683     // NOTE: Steps 3 - 6 enforced by sourceBufferPrivateDidReceiveInitializationSegment() and
 684     // sourceBufferPrivateDidReceiveSample below.
 685 
 686     // 7. Need more data: Return control to the calling algorithm.
 687 
 688     // NOTE: return to Section 3.5.5
 689     // 2.If the segment parser loop algorithm in the previous step was aborted, then abort this algorithm.
 690     if (result != AppendSucceeded)
 691         return;
 692 
 693     // 3. Set the updating attribute to false.
 694     m_updating = false;
 695 
 696     // 4. Queue a task to fire a simple event named update at this SourceBuffer object.
 697     scheduleEvent(eventNames().updateEvent);
 698 
 699     // 5. Queue a task to fire a simple event named updateend at this SourceBuffer object.
 700     scheduleEvent(eventNames().updateendEvent);
 701 
 702     if (m_source)
 703         m_source-&gt;monitorSourceBuffers();
 704 
 705     MediaTime currentMediaTime = m_source-&gt;currentTime();
 706     for (auto&amp; trackBufferPair : m_trackBufferMap) {
 707         TrackBuffer&amp; trackBuffer = trackBufferPair.value;
 708         const AtomString&amp; trackID = trackBufferPair.key;
 709 
 710         if (trackBuffer.needsReenqueueing) {
 711             DEBUG_LOG(LOGIDENTIFIER, &quot;reenqueuing at time &quot;, currentMediaTime);
 712             reenqueueMediaForTime(trackBuffer, trackID, currentMediaTime);
 713         } else
 714             provideMediaData(trackBuffer, trackID);
 715     }
 716 
 717     reportExtraMemoryAllocated();
 718     if (extraMemoryCost() &gt; this-&gt;maximumBufferSize())
 719         m_bufferFull = true;
 720 
 721     DEBUG_LOG(LOGIDENTIFIER);
 722 }
 723 
 724 void SourceBuffer::sourceBufferPrivateDidReceiveRenderingError(int error)
 725 {
 726 #if RELEASE_LOG_DISABLED
 727     UNUSED_PARAM(error);
 728 #endif
 729 
 730     ERROR_LOG(LOGIDENTIFIER, error);
 731 
 732     if (!isRemoved())
 733         m_source-&gt;streamEndedWithError(MediaSource::EndOfStreamError::Decode);
 734 }
 735 
 736 static bool decodeTimeComparator(const PresentationOrderSampleMap::MapType::value_type&amp; a, const PresentationOrderSampleMap::MapType::value_type&amp; b)
 737 {
 738     return a.second-&gt;decodeTime() &lt; b.second-&gt;decodeTime();
 739 }
 740 
 741 static PlatformTimeRanges removeSamplesFromTrackBuffer(const DecodeOrderSampleMap::MapType&amp; samples, SourceBuffer::TrackBuffer&amp; trackBuffer, const SourceBuffer* buffer, const char* logPrefix)
 742 {
 743 #if !RELEASE_LOG_DISABLED
 744     MediaTime earliestSample = MediaTime::positiveInfiniteTime();
 745     MediaTime latestSample = MediaTime::zeroTime();
 746     size_t bytesRemoved = 0;
 747     auto logIdentifier = WTF::Logger::LogSiteIdentifier(buffer-&gt;logClassName(), logPrefix, buffer-&gt;logIdentifier());
 748     auto&amp; logger = buffer-&gt;logger();
 749     auto willLog = logger.willLog(buffer-&gt;logChannel(), WTFLogLevel::Debug);
 750 #else
 751     UNUSED_PARAM(logPrefix);
 752     UNUSED_PARAM(buffer);
 753 #endif
 754 
 755     PlatformTimeRanges erasedRanges;
 756     for (const auto&amp; sampleIt : samples) {
 757         const DecodeOrderSampleMap::KeyType&amp; decodeKey = sampleIt.first;
 758 #if !RELEASE_LOG_DISABLED
 759         size_t startBufferSize = trackBuffer.samples.sizeInBytes();
 760 #endif
 761 
 762         const RefPtr&lt;MediaSample&gt;&amp; sample = sampleIt.second;
 763 
 764 #if !RELEASE_LOG_DISABLED
 765         if (willLog)
 766             logger.debug(buffer-&gt;logChannel(), logIdentifier, &quot;removing sample &quot;, *sampleIt.second);
 767 #endif
 768 
 769         // Remove the erased samples from the TrackBuffer sample map.
 770         trackBuffer.samples.removeSample(sample.get());
 771 
 772         // Also remove the erased samples from the TrackBuffer decodeQueue.
 773         trackBuffer.decodeQueue.erase(decodeKey);
 774 
 775         auto startTime = sample-&gt;presentationTime();
 776         auto endTime = startTime + sample-&gt;duration();
 777         erasedRanges.add(startTime, endTime);
 778 
 779 #if !RELEASE_LOG_DISABLED
 780         bytesRemoved += startBufferSize - trackBuffer.samples.sizeInBytes();
 781         if (startTime &lt; earliestSample)
 782             earliestSample = startTime;
 783         if (endTime &gt; latestSample)
 784             latestSample = endTime;
 785 #endif
 786     }
 787 
 788     // Because we may have added artificial padding in the buffered ranges when adding samples, we may
 789     // need to remove that padding when removing those same samples. Walk over the erased ranges looking
 790     // for unbuffered areas and expand erasedRanges to encompass those areas.
 791     PlatformTimeRanges additionalErasedRanges;
 792     for (unsigned i = 0; i &lt; erasedRanges.length(); ++i) {
 793         auto erasedStart = erasedRanges.start(i);
 794         auto erasedEnd = erasedRanges.end(i);
 795         auto startIterator = trackBuffer.samples.presentationOrder().reverseFindSampleBeforePresentationTime(erasedStart);
 796         if (startIterator == trackBuffer.samples.presentationOrder().rend())
 797             additionalErasedRanges.add(MediaTime::zeroTime(), erasedStart);
 798         else {
 799             auto&amp; previousSample = *startIterator-&gt;second;
 800             if (previousSample.presentationTime() + previousSample.duration() &lt; erasedStart)
 801                 additionalErasedRanges.add(previousSample.presentationTime() + previousSample.duration(), erasedStart);
 802         }
 803 
 804         auto endIterator = trackBuffer.samples.presentationOrder().findSampleStartingOnOrAfterPresentationTime(erasedEnd);
 805         if (endIterator == trackBuffer.samples.presentationOrder().end())
 806             additionalErasedRanges.add(erasedEnd, MediaTime::positiveInfiniteTime());
 807         else {
 808             auto&amp; nextSample = *endIterator-&gt;second;
 809             if (nextSample.presentationTime() &gt; erasedEnd)
 810                 additionalErasedRanges.add(erasedEnd, nextSample.presentationTime());
 811         }
 812     }
 813     if (additionalErasedRanges.length())
 814         erasedRanges.unionWith(additionalErasedRanges);
 815 
 816 #if !RELEASE_LOG_DISABLED
 817     if (bytesRemoved &amp;&amp; willLog)
 818         logger.debug(buffer-&gt;logChannel(), logIdentifier, &quot;removed &quot;, bytesRemoved, &quot;, start = &quot;, earliestSample, &quot;, end = &quot;, latestSample);
 819 #endif
 820 
 821     return erasedRanges;
 822 }
 823 
 824 void SourceBuffer::removeCodedFrames(const MediaTime&amp; start, const MediaTime&amp; end)
 825 {
 826     DEBUG_LOG(LOGIDENTIFIER, &quot;start = &quot;, start, &quot;, end = &quot;, end);
 827 
 828     // 3.5.9 Coded Frame Removal Algorithm
 829     // https://dvcs.w3.org/hg/html-media/raw-file/tip/media-source/media-source.html#sourcebuffer-coded-frame-removal
 830 
 831     // 1. Let start be the starting presentation timestamp for the removal range.
 832     MediaTime durationMediaTime = m_source-&gt;duration();
 833     MediaTime currentMediaTime = m_source-&gt;currentTime();
 834 
 835     // 2. Let end be the end presentation timestamp for the removal range.
 836     // 3. For each track buffer in this source buffer, run the following steps:
 837     for (auto&amp; trackBufferKeyValue : m_trackBufferMap) {
 838         TrackBuffer&amp; trackBuffer = trackBufferKeyValue.value;
 839         AtomString trackID = trackBufferKeyValue.key;
 840 
 841         // 3.1. Let remove end timestamp be the current value of duration
 842         // 3.2 If this track buffer has a random access point timestamp that is greater than or equal to end, then update
 843         // remove end timestamp to that random access point timestamp.
 844         // NOTE: Step 3.2 will be incorrect for any random access point timestamp whose decode time is later than the sample at end,
 845         // but whose presentation time is less than the sample at end. Skip this step until step 3.3 below.
 846 
 847         // NOTE: To handle MediaSamples which may be an amalgamation of multiple shorter samples, find samples whose presentation
 848         // interval straddles the start and end times, and divide them if possible:
 849         auto divideSampleIfPossibleAtPresentationTime = [&amp;] (const MediaTime&amp; time) {
 850             auto sampleIterator = trackBuffer.samples.presentationOrder().findSampleContainingPresentationTime(time);
 851             if (sampleIterator == trackBuffer.samples.presentationOrder().end())
 852                 return;
 853             RefPtr&lt;MediaSample&gt; sample = sampleIterator-&gt;second;
 854             if (!sample-&gt;isDivisable())
 855                 return;
 856             std::pair&lt;RefPtr&lt;MediaSample&gt;, RefPtr&lt;MediaSample&gt;&gt; replacementSamples = sample-&gt;divide(time);
 857             if (!replacementSamples.first || !replacementSamples.second)
 858                 return;
 859             DEBUG_LOG(LOGIDENTIFIER, &quot;splitting sample &quot;, *sample, &quot; into &quot;, *replacementSamples.first, &quot; and &quot;, *replacementSamples.second);
 860             trackBuffer.samples.removeSample(sample.get());
 861             trackBuffer.samples.addSample(*replacementSamples.first);
 862             trackBuffer.samples.addSample(*replacementSamples.second);
 863         };
 864         divideSampleIfPossibleAtPresentationTime(start);
 865         divideSampleIfPossibleAtPresentationTime(end);
 866 
 867         auto removePresentationStart = trackBuffer.samples.presentationOrder().findSampleContainingOrAfterPresentationTime(start);
 868         auto removePresentationEnd = trackBuffer.samples.presentationOrder().findSampleStartingOnOrAfterPresentationTime(end);
 869         if (removePresentationStart == removePresentationEnd)
 870             continue;
 871 
 872         // 3.3 Remove all media data, from this track buffer, that contain starting timestamps greater than or equal to
 873         // start and less than the remove end timestamp.
 874         // NOTE: frames must be removed in decode order, so that all dependant frames between the frame to be removed
 875         // and the next sync sample frame are removed. But we must start from the first sample in decode order, not
 876         // presentation order.
 877         auto minmaxDecodeTimeIterPair = std::minmax_element(removePresentationStart, removePresentationEnd, decodeTimeComparator);
 878         auto&amp; firstSample = *minmaxDecodeTimeIterPair.first-&gt;second;
 879         auto&amp; lastSample = *minmaxDecodeTimeIterPair.second-&gt;second;
 880         auto removeDecodeStart = trackBuffer.samples.decodeOrder().findSampleWithDecodeKey({firstSample.decodeTime(), firstSample.presentationTime()});
 881         auto removeDecodeLast = trackBuffer.samples.decodeOrder().findSampleWithDecodeKey({lastSample.decodeTime(), lastSample.presentationTime()});
 882         auto removeDecodeEnd = trackBuffer.samples.decodeOrder().findSyncSampleAfterDecodeIterator(removeDecodeLast);
 883 
 884         DecodeOrderSampleMap::MapType erasedSamples(removeDecodeStart, removeDecodeEnd);
 885         PlatformTimeRanges erasedRanges = removeSamplesFromTrackBuffer(erasedSamples, trackBuffer, this, &quot;removeCodedFrames&quot;);
 886 
 887         // Only force the TrackBuffer to re-enqueue if the removed ranges overlap with enqueued and possibly
 888         // not yet displayed samples.
 889         if (trackBuffer.lastEnqueuedPresentationTime.isValid() &amp;&amp; currentMediaTime &lt; trackBuffer.lastEnqueuedPresentationTime) {
 890             PlatformTimeRanges possiblyEnqueuedRanges(currentMediaTime, trackBuffer.lastEnqueuedPresentationTime);
 891             possiblyEnqueuedRanges.intersectWith(erasedRanges);
 892             if (possiblyEnqueuedRanges.length()) {
 893                 trackBuffer.needsReenqueueing = true;
 894                 DEBUG_LOG(LOGIDENTIFIER, &quot;the range in removeCodedFrames() includes already enqueued samples, reenqueueing from &quot;, currentMediaTime);
 895                 reenqueueMediaForTime(trackBuffer, trackID, currentMediaTime);
 896             }
 897         }
 898 
 899         erasedRanges.invert();
 900         trackBuffer.buffered.intersectWith(erasedRanges);
 901         setBufferedDirty(true);
 902 
 903         // 3.4 If this object is in activeSourceBuffers, the current playback position is greater than or equal to start
 904         // and less than the remove end timestamp, and HTMLMediaElement.readyState is greater than HAVE_METADATA, then set
 905         // the HTMLMediaElement.readyState attribute to HAVE_METADATA and stall playback.
 906         if (m_active &amp;&amp; currentMediaTime &gt;= start &amp;&amp; currentMediaTime &lt; end &amp;&amp; m_private-&gt;readyState() &gt; MediaPlayer::HaveMetadata)
 907             m_private-&gt;setReadyState(MediaPlayer::HaveMetadata);
 908     }
 909 
 910     updateBufferedFromTrackBuffers();
 911 
 912     // 4. If buffer full flag equals true and this object is ready to accept more bytes, then set the buffer full flag to false.
 913     // No-op
 914 
 915     LOG(Media, &quot;SourceBuffer::removeCodedFrames(%p) - buffered = %s&quot;, this, toString(m_buffered-&gt;ranges()).utf8().data());
 916 }
 917 
 918 void SourceBuffer::removeTimerFired()
 919 {
 920     if (isRemoved())
 921         return;
 922 
 923     ASSERT(m_updating);
 924     ASSERT(m_pendingRemoveStart.isValid());
 925     ASSERT(m_pendingRemoveStart &lt; m_pendingRemoveEnd);
 926 
 927     // Section 3.5.7 Range Removal
 928     // http://w3c.github.io/media-source/#sourcebuffer-range-removal
 929 
 930     // 6. Run the coded frame removal algorithm with start and end as the start and end of the removal range.
 931     removeCodedFrames(m_pendingRemoveStart, m_pendingRemoveEnd);
 932 
 933     // 7. Set the updating attribute to false.
 934     m_updating = false;
 935     m_pendingRemoveStart = MediaTime::invalidTime();
 936     m_pendingRemoveEnd = MediaTime::invalidTime();
 937 
 938     // 8. Queue a task to fire a simple event named update at this SourceBuffer object.
 939     scheduleEvent(eventNames().updateEvent);
 940 
 941     // 9. Queue a task to fire a simple event named updateend at this SourceBuffer object.
 942     scheduleEvent(eventNames().updateendEvent);
 943 }
 944 
 945 void SourceBuffer::evictCodedFrames(size_t newDataSize)
 946 {
 947     // 3.5.13 Coded Frame Eviction Algorithm
 948     // http://www.w3.org/TR/media-source/#sourcebuffer-coded-frame-eviction
 949 
 950     if (isRemoved())
 951         return;
 952 
 953     // This algorithm is run to free up space in this source buffer when new data is appended.
 954     // 1. Let new data equal the data that is about to be appended to this SourceBuffer.
 955     // 2. If the buffer full flag equals false, then abort these steps.
 956     if (!m_bufferFull)
 957         return;
 958 
 959     size_t maximumBufferSize = this-&gt;maximumBufferSize();
 960 
 961     // 3. Let removal ranges equal a list of presentation time ranges that can be evicted from
 962     // the presentation to make room for the new data.
 963 
 964     // NOTE: begin by removing data from the beginning of the buffered ranges, 30 seconds at
 965     // a time, up to 30 seconds before currentTime.
 966     MediaTime thirtySeconds = MediaTime(30, 1);
 967     MediaTime currentTime = m_source-&gt;currentTime();
 968     MediaTime maximumRangeEnd = currentTime - thirtySeconds;
 969 
 970 #if !RELEASE_LOG_DISABLED
 971     DEBUG_LOG(LOGIDENTIFIER, &quot;currentTime = &quot;, m_source-&gt;currentTime(), &quot;, require &quot;, extraMemoryCost() + newDataSize, &quot; bytes, maximum buffer size is &quot;, maximumBufferSize);
 972     size_t initialBufferedSize = extraMemoryCost();
 973 #endif
 974 
 975     MediaTime rangeStart = MediaTime::zeroTime();
 976     MediaTime rangeEnd = rangeStart + thirtySeconds;
 977     while (rangeStart &lt; maximumRangeEnd) {
 978         // 4. For each range in removal ranges, run the coded frame removal algorithm with start and
 979         // end equal to the removal range start and end timestamp respectively.
 980         removeCodedFrames(rangeStart, std::min(rangeEnd, maximumRangeEnd));
 981         if (extraMemoryCost() + newDataSize &lt; maximumBufferSize) {
 982             m_bufferFull = false;
 983             break;
 984         }
 985 
 986         rangeStart += thirtySeconds;
 987         rangeEnd += thirtySeconds;
 988     }
 989 
 990 #if !RELEASE_LOG_DISABLED
 991     if (!m_bufferFull) {
 992         DEBUG_LOG(LOGIDENTIFIER, &quot;evicted &quot;, initialBufferedSize - extraMemoryCost());
 993         return;
 994     }
 995 #endif
 996 
 997     // If there still isn&#39;t enough free space and there buffers in time ranges after the current range (ie. there is a gap after
 998     // the current buffered range), delete 30 seconds at a time from duration back to the current time range or 30 seconds after
 999     // currenTime whichever we hit first.
1000     auto buffered = m_buffered-&gt;ranges();
1001     size_t currentTimeRange = buffered.find(currentTime);
1002     if (currentTimeRange == buffered.length() - 1) {
1003 #if !RELEASE_LOG_DISABLED
1004         ERROR_LOG(LOGIDENTIFIER, &quot;FAILED to free enough after evicting &quot;, initialBufferedSize - extraMemoryCost());
1005 #endif
1006         return;
1007     }
1008 
1009     MediaTime minimumRangeStart = currentTime + thirtySeconds;
1010 
1011     rangeEnd = m_source-&gt;duration();
1012     rangeStart = rangeEnd - thirtySeconds;
1013     while (rangeStart &gt; minimumRangeStart) {
1014 
1015         // Do not evict data from the time range that contains currentTime.
1016         size_t startTimeRange = buffered.find(rangeStart);
1017         if (currentTimeRange != notFound &amp;&amp; startTimeRange == currentTimeRange) {
1018             size_t endTimeRange = buffered.find(rangeEnd);
1019             if (currentTimeRange != notFound &amp;&amp; endTimeRange == currentTimeRange)
1020                 break;
1021 
1022             rangeEnd = buffered.start(endTimeRange);
1023         }
1024 
1025         // 4. For each range in removal ranges, run the coded frame removal algorithm with start and
1026         // end equal to the removal range start and end timestamp respectively.
1027         removeCodedFrames(std::max(minimumRangeStart, rangeStart), rangeEnd);
1028         if (extraMemoryCost() + newDataSize &lt; maximumBufferSize) {
1029             m_bufferFull = false;
1030             break;
1031         }
1032 
1033         rangeStart -= thirtySeconds;
1034         rangeEnd -= thirtySeconds;
1035     }
1036 
1037 #if !RELEASE_LOG_DISABLED
1038     if (m_bufferFull)
1039         ERROR_LOG(LOGIDENTIFIER, &quot;FAILED to free enough after evicting &quot;, initialBufferedSize - extraMemoryCost());
1040     else
1041         DEBUG_LOG(LOGIDENTIFIER, &quot;evicted &quot;, initialBufferedSize - extraMemoryCost());
1042 #endif
1043 }
1044 
1045 size_t SourceBuffer::maximumBufferSize() const
1046 {
1047     if (isRemoved())
1048         return 0;
1049 
1050     auto* element = m_source-&gt;mediaElement();
1051     if (!element)
1052         return 0;
1053 
1054     return element-&gt;maximumSourceBufferSize(*this);
1055 }
1056 
1057 VideoTrackList&amp; SourceBuffer::videoTracks()
1058 {
1059     if (!m_videoTracks)
1060         m_videoTracks = VideoTrackList::create(m_source-&gt;mediaElement(), scriptExecutionContext());
1061     return *m_videoTracks;
1062 }
1063 
1064 AudioTrackList&amp; SourceBuffer::audioTracks()
1065 {
1066     if (!m_audioTracks)
1067         m_audioTracks = AudioTrackList::create(m_source-&gt;mediaElement(), scriptExecutionContext());
1068     return *m_audioTracks;
1069 }
1070 
1071 TextTrackList&amp; SourceBuffer::textTracks()
1072 {
1073     if (!m_textTracks)
1074         m_textTracks = TextTrackList::create(m_source-&gt;mediaElement(), scriptExecutionContext());
1075     return *m_textTracks;
1076 }
1077 
1078 void SourceBuffer::setActive(bool active)
1079 {
1080     if (m_active == active)
1081         return;
1082 
1083     m_active = active;
1084     m_private-&gt;setActive(active);
1085     if (!isRemoved())
1086         m_source-&gt;sourceBufferDidChangeActiveState(*this, active);
1087 }
1088 
1089 void SourceBuffer::sourceBufferPrivateDidReceiveInitializationSegment(const InitializationSegment&amp; segment)
1090 {
1091     if (isRemoved())
1092         return;
1093 
1094     ALWAYS_LOG(LOGIDENTIFIER);
1095 
1096     // 3.5.8 Initialization Segment Received (ctd)
1097     // https://rawgit.com/w3c/media-source/c3ad59c7a370d04430969ba73d18dc9bcde57a33/index.html#sourcebuffer-init-segment-received [Editor&#39;s Draft 09 January 2015]
1098 
1099     // 1. Update the duration attribute if it currently equals NaN:
1100     if (m_source-&gt;duration().isInvalid()) {
1101         // ↳ If the initialization segment contains a duration:
1102         //   Run the duration change algorithm with new duration set to the duration in the initialization segment.
1103         // ↳ Otherwise:
1104         //   Run the duration change algorithm with new duration set to positive Infinity.
1105         if (segment.duration.isValid() &amp;&amp; !segment.duration.isIndefinite())
1106             m_source-&gt;setDurationInternal(segment.duration);
1107         else
1108             m_source-&gt;setDurationInternal(MediaTime::positiveInfiniteTime());
1109     }
1110 
1111     // 2. If the initialization segment has no audio, video, or text tracks, then run the append error algorithm
1112     // with the decode error parameter set to true and abort these steps.
1113     if (segment.audioTracks.isEmpty() &amp;&amp; segment.videoTracks.isEmpty() &amp;&amp; segment.textTracks.isEmpty()) {
1114         appendError(true);
1115         return;
1116     }
1117 
1118     // 3. If the first initialization segment flag is true, then run the following steps:
1119     if (m_receivedFirstInitializationSegment) {
1120 
1121         // 3.1. Verify the following properties. If any of the checks fail then run the append error algorithm
1122         // with the decode error parameter set to true and abort these steps.
1123         if (!validateInitializationSegment(segment)) {
1124             appendError(true);
1125             return;
1126         }
1127         // 3.2 Add the appropriate track descriptions from this initialization segment to each of the track buffers.
1128         ASSERT(segment.audioTracks.size() == audioTracks().length());
1129         for (auto&amp; audioTrackInfo : segment.audioTracks) {
1130             if (audioTracks().length() == 1) {
1131                 audioTracks().item(0)-&gt;setPrivate(*audioTrackInfo.track);
1132                 break;
1133             }
1134 
1135             auto audioTrack = audioTracks().getTrackById(audioTrackInfo.track-&gt;id());
1136             ASSERT(audioTrack);
1137             audioTrack-&gt;setPrivate(*audioTrackInfo.track);
1138         }
1139 
1140         ASSERT(segment.videoTracks.size() == videoTracks().length());
1141         for (auto&amp; videoTrackInfo : segment.videoTracks) {
1142             if (videoTracks().length() == 1) {
1143                 videoTracks().item(0)-&gt;setPrivate(*videoTrackInfo.track);
1144                 break;
1145             }
1146 
1147             auto videoTrack = videoTracks().getTrackById(videoTrackInfo.track-&gt;id());
1148             ASSERT(videoTrack);
1149             videoTrack-&gt;setPrivate(*videoTrackInfo.track);
1150         }
1151 
1152         ASSERT(segment.textTracks.size() == textTracks().length());
1153         for (auto&amp; textTrackInfo : segment.textTracks) {
1154             if (textTracks().length() == 1) {
1155                 downcast&lt;InbandTextTrack&gt;(*textTracks().item(0)).setPrivate(*textTrackInfo.track);
1156                 break;
1157             }
1158 
1159             auto textTrack = textTracks().getTrackById(textTrackInfo.track-&gt;id());
1160             ASSERT(textTrack);
1161             downcast&lt;InbandTextTrack&gt;(*textTrack).setPrivate(*textTrackInfo.track);
1162         }
1163 
1164         // 3.3 Set the need random access point flag on all track buffers to true.
1165         for (auto&amp; trackBuffer : m_trackBufferMap.values())
1166             trackBuffer.needRandomAccessFlag = true;
1167     }
1168 
1169     // 4. Let active track flag equal false.
1170     bool activeTrackFlag = false;
1171 
1172     // 5. If the first initialization segment flag is false, then run the following steps:
1173     if (!m_receivedFirstInitializationSegment) {
1174         // 5.1 If the initialization segment contains tracks with codecs the user agent does not support,
1175         // then run the append error algorithm with the decode error parameter set to true and abort these steps.
1176         // NOTE: This check is the responsibility of the SourceBufferPrivate.
1177 
1178         // 5.2 For each audio track in the initialization segment, run following steps:
1179         for (auto&amp; audioTrackInfo : segment.audioTracks) {
1180             // FIXME: Implement steps 5.2.1-5.2.8.1 as per Editor&#39;s Draft 09 January 2015, and reorder this
1181             // 5.2.1 Let new audio track be a new AudioTrack object.
1182             // 5.2.2 Generate a unique ID and assign it to the id property on new video track.
1183             auto newAudioTrack = AudioTrack::create(*this, *audioTrackInfo.track);
1184             newAudioTrack-&gt;setSourceBuffer(this);
1185 
1186             // 5.2.3 If audioTracks.length equals 0, then run the following steps:
1187             if (!audioTracks().length()) {
1188                 // 5.2.3.1 Set the enabled property on new audio track to true.
1189                 newAudioTrack-&gt;setEnabled(true);
1190 
1191                 // 5.2.3.2 Set active track flag to true.
1192                 activeTrackFlag = true;
1193             }
1194 
1195             // 5.2.4 Add new audio track to the audioTracks attribute on this SourceBuffer object.
1196             // 5.2.5 Queue a task to fire a trusted event named addtrack, that does not bubble and is
1197             // not cancelable, and that uses the TrackEvent interface, at the AudioTrackList object
1198             // referenced by the audioTracks attribute on this SourceBuffer object.
1199             audioTracks().append(newAudioTrack.copyRef());
1200 
1201             // 5.2.6 Add new audio track to the audioTracks attribute on the HTMLMediaElement.
1202             // 5.2.7 Queue a task to fire a trusted event named addtrack, that does not bubble and is
1203             // not cancelable, and that uses the TrackEvent interface, at the AudioTrackList object
1204             // referenced by the audioTracks attribute on the HTMLMediaElement.
1205             m_source-&gt;mediaElement()-&gt;ensureAudioTracks().append(newAudioTrack.copyRef());
1206 
1207             // 5.2.8 Create a new track buffer to store coded frames for this track.
1208             ASSERT(!m_trackBufferMap.contains(newAudioTrack-&gt;id()));
1209             auto&amp; trackBuffer = m_trackBufferMap.add(newAudioTrack-&gt;id(), TrackBuffer()).iterator-&gt;value;
1210 
1211             // 5.2.9 Add the track description for this track to the track buffer.
1212             trackBuffer.description = audioTrackInfo.description;
1213 
1214             m_audioCodecs.append(trackBuffer.description-&gt;codec());
1215         }
1216 
1217         // 5.3 For each video track in the initialization segment, run following steps:
1218         for (auto&amp; videoTrackInfo : segment.videoTracks) {
1219             // FIXME: Implement steps 5.3.1-5.3.8.1 as per Editor&#39;s Draft 09 January 2015, and reorder this
1220             // 5.3.1 Let new video track be a new VideoTrack object.
1221             // 5.3.2 Generate a unique ID and assign it to the id property on new video track.
1222             auto newVideoTrack = VideoTrack::create(*this, *videoTrackInfo.track);
1223             newVideoTrack-&gt;setSourceBuffer(this);
1224 
1225             // 5.3.3 If videoTracks.length equals 0, then run the following steps:
1226             if (!videoTracks().length()) {
1227                 // 5.3.3.1 Set the selected property on new video track to true.
1228                 newVideoTrack-&gt;setSelected(true);
1229 
1230                 // 5.3.3.2 Set active track flag to true.
1231                 activeTrackFlag = true;
1232             }
1233 
1234             // 5.3.4 Add new video track to the videoTracks attribute on this SourceBuffer object.
1235             // 5.3.5 Queue a task to fire a trusted event named addtrack, that does not bubble and is
1236             // not cancelable, and that uses the TrackEvent interface, at the VideoTrackList object
1237             // referenced by the videoTracks attribute on this SourceBuffer object.
1238             videoTracks().append(newVideoTrack.copyRef());
1239 
1240             // 5.3.6 Add new video track to the videoTracks attribute on the HTMLMediaElement.
1241             // 5.3.7 Queue a task to fire a trusted event named addtrack, that does not bubble and is
1242             // not cancelable, and that uses the TrackEvent interface, at the VideoTrackList object
1243             // referenced by the videoTracks attribute on the HTMLMediaElement.
1244             m_source-&gt;mediaElement()-&gt;ensureVideoTracks().append(newVideoTrack.copyRef());
1245 
1246             // 5.3.8 Create a new track buffer to store coded frames for this track.
1247             ASSERT(!m_trackBufferMap.contains(newVideoTrack-&gt;id()));
1248             auto&amp; trackBuffer = m_trackBufferMap.add(newVideoTrack-&gt;id(), TrackBuffer()).iterator-&gt;value;
1249 
1250             // 5.3.9 Add the track description for this track to the track buffer.
1251             trackBuffer.description = videoTrackInfo.description;
1252 
1253             m_videoCodecs.append(trackBuffer.description-&gt;codec());
1254         }
1255 
1256         // 5.4 For each text track in the initialization segment, run following steps:
1257         for (auto&amp; textTrackInfo : segment.textTracks) {
1258             auto&amp; textTrackPrivate = *textTrackInfo.track;
1259 
1260             // FIXME: Implement steps 5.4.1-5.4.8.1 as per Editor&#39;s Draft 09 January 2015, and reorder this
1261             // 5.4.1 Let new text track be a new TextTrack object with its properties populated with the
1262             // appropriate information from the initialization segment.
1263             auto newTextTrack = InbandTextTrack::create(*scriptExecutionContext(), *this, textTrackPrivate);
1264 
1265             // 5.4.2 If the mode property on new text track equals &quot;showing&quot; or &quot;hidden&quot;, then set active
1266             // track flag to true.
1267             if (textTrackPrivate.mode() != InbandTextTrackPrivate::Disabled)
1268                 activeTrackFlag = true;
1269 
1270             // 5.4.3 Add new text track to the textTracks attribute on this SourceBuffer object.
1271             // 5.4.4 Queue a task to fire a trusted event named addtrack, that does not bubble and is
1272             // not cancelable, and that uses the TrackEvent interface, at textTracks attribute on this
1273             // SourceBuffer object.
1274             textTracks().append(newTextTrack.get());
1275 
1276             // 5.4.5 Add new text track to the textTracks attribute on the HTMLMediaElement.
1277             // 5.4.6 Queue a task to fire a trusted event named addtrack, that does not bubble and is
1278             // not cancelable, and that uses the TrackEvent interface, at the TextTrackList object
1279             // referenced by the textTracks attribute on the HTMLMediaElement.
1280             m_source-&gt;mediaElement()-&gt;ensureTextTracks().append(WTFMove(newTextTrack));
1281 
1282             // 5.4.7 Create a new track buffer to store coded frames for this track.
1283             ASSERT(!m_trackBufferMap.contains(textTrackPrivate.id()));
1284             auto&amp; trackBuffer = m_trackBufferMap.add(textTrackPrivate.id(), TrackBuffer()).iterator-&gt;value;
1285 
1286             // 5.4.8 Add the track description for this track to the track buffer.
1287             trackBuffer.description = textTrackInfo.description;
1288 
1289             m_textCodecs.append(trackBuffer.description-&gt;codec());
1290         }
1291 
1292         // 5.5 If active track flag equals true, then run the following steps:
1293         if (activeTrackFlag) {
1294             // 5.5.1 Add this SourceBuffer to activeSourceBuffers.
1295             // 5.5.2 Queue a task to fire a simple event named addsourcebuffer at activeSourceBuffers
1296             setActive(true);
1297         }
1298 
1299         // 5.6 Set first initialization segment flag to true.
1300         m_receivedFirstInitializationSegment = true;
1301     }
1302 
1303     // (Note: Issue #155 adds this step after step 5:)
1304     // 6. Set  pending initialization segment for changeType flag  to false.
1305     m_pendingInitializationSegmentForChangeType = false;
1306 
1307     // 6. If the HTMLMediaElement.readyState attribute is HAVE_NOTHING, then run the following steps:
1308     if (m_private-&gt;readyState() == MediaPlayer::HaveNothing) {
1309         // 6.1 If one or more objects in sourceBuffers have first initialization segment flag set to false, then abort these steps.
1310         for (auto&amp; sourceBuffer : *m_source-&gt;sourceBuffers()) {
1311             if (!sourceBuffer-&gt;m_receivedFirstInitializationSegment)
1312                 return;
1313         }
1314 
1315         // 6.2 Set the HTMLMediaElement.readyState attribute to HAVE_METADATA.
1316         // 6.3 Queue a task to fire a simple event named loadedmetadata at the media element.
1317         m_private-&gt;setReadyState(MediaPlayer::HaveMetadata);
1318     }
1319 
1320     // 7. If the active track flag equals true and the HTMLMediaElement.readyState
1321     // attribute is greater than HAVE_CURRENT_DATA, then set the HTMLMediaElement.readyState
1322     // attribute to HAVE_METADATA.
1323     if (activeTrackFlag &amp;&amp; m_private-&gt;readyState() &gt; MediaPlayer::HaveCurrentData)
1324         m_private-&gt;setReadyState(MediaPlayer::HaveMetadata);
1325 }
1326 
1327 bool SourceBuffer::validateInitializationSegment(const InitializationSegment&amp; segment)
1328 {
1329     // FIXME: ordering of all 3.5.X (X&gt;=7) functions needs to be updated to post-[24 July 2014 Editor&#39;s Draft] version
1330     // 3.5.8 Initialization Segment Received (ctd)
1331     // https://rawgit.com/w3c/media-source/c3ad59c7a370d04430969ba73d18dc9bcde57a33/index.html#sourcebuffer-init-segment-received [Editor&#39;s Draft 09 January 2015]
1332 
1333     // Note: those are checks from step 3.1
1334     //   * The number of audio, video, and text tracks match what was in the first initialization segment.
1335     if (segment.audioTracks.size() != audioTracks().length()
1336         || segment.videoTracks.size() != videoTracks().length()
1337         || segment.textTracks.size() != textTracks().length())
1338         return false;
1339 
1340     //   * The codecs for each track, match what was specified in the first initialization segment.
1341     // (Note: Issue #155 strikes out this check. For broad compatibility when this experimental feature
1342     // is not enabled, only perform this check if the &quot;pending initialization segment for changeType flag&quot;
1343     // is not set.)
1344     for (auto&amp; audioTrackInfo : segment.audioTracks) {
1345         if (m_audioCodecs.contains(audioTrackInfo.description-&gt;codec()))
1346             continue;
1347 
1348         if (!m_pendingInitializationSegmentForChangeType)
1349             return false;
1350 
1351         m_audioCodecs.append(audioTrackInfo.description-&gt;codec());
1352     }
1353 
1354     for (auto&amp; videoTrackInfo : segment.videoTracks) {
1355         if (m_videoCodecs.contains(videoTrackInfo.description-&gt;codec()))
1356             continue;
1357 
1358         if (!m_pendingInitializationSegmentForChangeType)
1359             return false;
1360 
1361         m_videoCodecs.append(videoTrackInfo.description-&gt;codec());
1362     }
1363 
1364     for (auto&amp; textTrackInfo : segment.textTracks) {
1365         if (m_textCodecs.contains(textTrackInfo.description-&gt;codec()))
1366             continue;
1367 
1368         if (!m_pendingInitializationSegmentForChangeType)
1369             return false;
1370 
1371         m_textCodecs.append(textTrackInfo.description-&gt;codec());
1372     }
1373 
1374     //   * If more than one track for a single type are present (ie 2 audio tracks), then the Track
1375     //   IDs match the ones in the first initialization segment.
1376     if (segment.audioTracks.size() &gt;= 2) {
1377         for (auto&amp; audioTrackInfo : segment.audioTracks) {
1378             if (!m_trackBufferMap.contains(audioTrackInfo.track-&gt;id()))
1379                 return false;
1380         }
1381     }
1382 
1383     if (segment.videoTracks.size() &gt;= 2) {
1384         for (auto&amp; videoTrackInfo : segment.videoTracks) {
1385             if (!m_trackBufferMap.contains(videoTrackInfo.track-&gt;id()))
1386                 return false;
1387         }
1388     }
1389 
1390     if (segment.textTracks.size() &gt;= 2) {
1391         for (auto&amp; textTrackInfo : segment.videoTracks) {
1392             if (!m_trackBufferMap.contains(textTrackInfo.track-&gt;id()))
1393                 return false;
1394         }
1395     }
1396 
1397     return true;
1398 }
1399 
1400 class SampleLessThanComparator {
1401 public:
1402     bool operator()(std::pair&lt;MediaTime, RefPtr&lt;MediaSample&gt;&gt; value1, std::pair&lt;MediaTime, RefPtr&lt;MediaSample&gt;&gt; value2)
1403     {
1404         return value1.first &lt; value2.first;
1405     }
1406 
1407     bool operator()(MediaTime value1, std::pair&lt;MediaTime, RefPtr&lt;MediaSample&gt;&gt; value2)
1408     {
1409         return value1 &lt; value2.first;
1410     }
1411 
1412     bool operator()(std::pair&lt;MediaTime, RefPtr&lt;MediaSample&gt;&gt; value1, MediaTime value2)
1413     {
1414         return value1.first &lt; value2;
1415     }
1416 };
1417 
1418 void SourceBuffer::appendError(bool decodeErrorParam)
1419 {
1420     // 3.5.3 Append Error Algorithm
1421     // https://rawgit.com/w3c/media-source/c3ad59c7a370d04430969ba73d18dc9bcde57a33/index.html#sourcebuffer-append-error [Editor&#39;s Draft 09 January 2015]
1422 
1423     ASSERT(m_updating);
1424     // 1. Run the reset parser state algorithm.
1425     resetParserState();
1426 
1427     // 2. Set the updating attribute to false.
1428     m_updating = false;
1429 
1430     // 3. Queue a task to fire a simple event named error at this SourceBuffer object.
1431     scheduleEvent(eventNames().errorEvent);
1432 
1433     // 4. Queue a task to fire a simple event named updateend at this SourceBuffer object.
1434     scheduleEvent(eventNames().updateendEvent);
1435 
1436     // 5. If decode error is true, then run the end of stream algorithm with the error parameter set to &quot;decode&quot;.
1437     if (decodeErrorParam)
1438         m_source-&gt;streamEndedWithError(MediaSource::EndOfStreamError::Decode);
1439 }
1440 
1441 void SourceBuffer::sourceBufferPrivateDidReceiveSample(MediaSample&amp; sample)
1442 {
1443     if (isRemoved())
1444         return;
1445 
1446     // 3.5.1 Segment Parser Loop
1447     // 6.1 If the first initialization segment received flag is false, (Note: Issue # 155 &amp; changeType()
1448     // algorithm) or the  pending initialization segment for changeType flag  is true, (End note)
1449     // then run the append error algorithm
1450     //     with the decode error parameter set to true and abort this algorithm.
1451     // Note: current design makes SourceBuffer somehow ignorant of append state - it&#39;s more a thing
1452     //  of SourceBufferPrivate. That&#39;s why this check can&#39;t really be done in appendInternal.
1453     //  unless we force some kind of design with state machine switching.
1454     if (!m_receivedFirstInitializationSegment || m_pendingInitializationSegmentForChangeType) {
1455         appendError(true);
1456         return;
1457     }
1458 
1459     // 3.5.8 Coded Frame Processing
1460     // http://www.w3.org/TR/media-source/#sourcebuffer-coded-frame-processing
1461 
1462     // When complete coded frames have been parsed by the segment parser loop then the following steps
1463     // are run:
1464     // 1. For each coded frame in the media segment run the following steps:
1465     // 1.1. Loop Top
1466     do {
1467         MediaTime presentationTimestamp;
1468         MediaTime decodeTimestamp;
1469 
1470         // NOTE: this is out-of-order, but we need the timescale from the
1471         // sample&#39;s duration for timestamp generation.
1472         // 1.2 Let frame duration be a double precision floating point representation of the coded frame&#39;s
1473         // duration in seconds.
1474         MediaTime frameDuration = sample.duration();
1475 
1476         if (m_shouldGenerateTimestamps) {
1477             // ↳ If generate timestamps flag equals true:
1478             // 1. Let presentation timestamp equal 0.
1479             // NOTE: Use the duration timscale for the presentation timestamp, as this will eliminate
1480             // timescale rounding when generating timestamps.
1481             presentationTimestamp = { 0, frameDuration.timeScale() };
1482 
1483             // 2. Let decode timestamp equal 0.
1484             decodeTimestamp = { 0, frameDuration.timeScale() };
1485         } else {
1486             // ↳ Otherwise:
1487             // 1. Let presentation timestamp be a double precision floating point representation of
1488             // the coded frame&#39;s presentation timestamp in seconds.
1489             presentationTimestamp = sample.presentationTime();
1490 
1491             // 2. Let decode timestamp be a double precision floating point representation of the coded frame&#39;s
1492             // decode timestamp in seconds.
1493             decodeTimestamp = sample.decodeTime();
1494         }
1495 
1496         // 1.3 If mode equals &quot;sequence&quot; and group start timestamp is set, then run the following steps:
1497         if (m_mode == AppendMode::Sequence &amp;&amp; m_groupStartTimestamp.isValid()) {
1498             // 1.3.1 Set timestampOffset equal to group start timestamp - presentation timestamp.
1499             m_timestampOffset = m_groupStartTimestamp;
1500 
1501             for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
1502                 trackBuffer.lastFrameTimescale = 0;
1503                 trackBuffer.roundedTimestampOffset = MediaTime::invalidTime();
1504             }
1505 
1506             // 1.3.2 Set group end timestamp equal to group start timestamp.
1507             m_groupEndTimestamp = m_groupStartTimestamp;
1508 
1509             // 1.3.3 Set the need random access point flag on all track buffers to true.
1510             for (auto&amp; trackBuffer : m_trackBufferMap.values())
1511                 trackBuffer.needRandomAccessFlag = true;
1512 
1513             // 1.3.4 Unset group start timestamp.
1514             m_groupStartTimestamp = MediaTime::invalidTime();
1515         }
1516 
1517         // NOTE: this is out-of-order, but we need TrackBuffer to be able to cache the results of timestamp offset rounding
1518         // 1.5 Let track buffer equal the track buffer that the coded frame will be added to.
1519         AtomString trackID = sample.trackID();
1520         auto it = m_trackBufferMap.find(trackID);
1521         if (it == m_trackBufferMap.end()) {
1522             // The client managed to append a sample with a trackID not present in the initialization
1523             // segment. This would be a good place to post an message to the developer console.
1524             didDropSample();
1525             return;
1526         }
1527         TrackBuffer&amp; trackBuffer = it-&gt;value;
1528 
1529         MediaTime microsecond(1, 1000000);
1530 
1531         auto roundTowardsTimeScaleWithRoundingMargin = [] (const MediaTime&amp; time, uint32_t timeScale, const MediaTime&amp; roundingMargin) {
1532             while (true) {
1533                 MediaTime roundedTime = time.toTimeScale(timeScale);
1534                 if (abs(roundedTime - time) &lt; roundingMargin || timeScale &gt;= MediaTime::MaximumTimeScale)
1535                     return roundedTime;
1536 
1537                 if (!WTF::safeMultiply(timeScale, 2, timeScale) || timeScale &gt; MediaTime::MaximumTimeScale)
1538                     timeScale = MediaTime::MaximumTimeScale;
1539             }
1540         };
1541 
1542         // 1.4 If timestampOffset is not 0, then run the following steps:
1543         if (m_timestampOffset) {
1544             if (!trackBuffer.roundedTimestampOffset.isValid() || presentationTimestamp.timeScale() != trackBuffer.lastFrameTimescale) {
1545                 trackBuffer.lastFrameTimescale = presentationTimestamp.timeScale();
1546                 trackBuffer.roundedTimestampOffset = roundTowardsTimeScaleWithRoundingMargin(m_timestampOffset, trackBuffer.lastFrameTimescale, microsecond);
1547             }
1548 
1549             // 1.4.1 Add timestampOffset to the presentation timestamp.
1550             presentationTimestamp += trackBuffer.roundedTimestampOffset;
1551 
1552             // 1.4.2 Add timestampOffset to the decode timestamp.
1553             decodeTimestamp += trackBuffer.roundedTimestampOffset;
1554         }
1555 
1556         // 1.6 ↳ If last decode timestamp for track buffer is set and decode timestamp is less than last
1557         // decode timestamp:
1558         // OR
1559         // ↳ If last decode timestamp for track buffer is set and the difference between decode timestamp and
1560         // last decode timestamp is greater than 2 times last frame duration:
1561         MediaTime decodeDurationToCheck = trackBuffer.greatestDecodeDuration;
1562 
1563         if (decodeDurationToCheck.isValid() &amp;&amp; trackBuffer.lastFrameDuration.isValid()
1564             &amp;&amp; (trackBuffer.lastFrameDuration &gt; decodeDurationToCheck))
1565             decodeDurationToCheck = trackBuffer.lastFrameDuration;
1566 
1567         if (trackBuffer.lastDecodeTimestamp.isValid() &amp;&amp; (decodeTimestamp &lt; trackBuffer.lastDecodeTimestamp
1568             || (decodeDurationToCheck.isValid() &amp;&amp; abs(decodeTimestamp - trackBuffer.lastDecodeTimestamp) &gt; (decodeDurationToCheck * 2)))) {
1569 
1570             // 1.6.1:
1571             if (m_mode == AppendMode::Segments) {
1572                 // ↳ If mode equals &quot;segments&quot;:
1573                 // Set group end timestamp to presentation timestamp.
1574                 m_groupEndTimestamp = presentationTimestamp;
1575             } else {
1576                 // ↳ If mode equals &quot;sequence&quot;:
1577                 // Set group start timestamp equal to the group end timestamp.
1578                 m_groupStartTimestamp = m_groupEndTimestamp;
1579             }
1580 
1581             for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
1582                 // 1.6.2 Unset the last decode timestamp on all track buffers.
1583                 trackBuffer.lastDecodeTimestamp = MediaTime::invalidTime();
1584                 // 1.6.3 Unset the last frame duration on all track buffers.
1585                 trackBuffer.greatestDecodeDuration = MediaTime::invalidTime();
1586                 trackBuffer.lastFrameDuration = MediaTime::invalidTime();
1587                 // 1.6.4 Unset the highest presentation timestamp on all track buffers.
1588                 trackBuffer.highestPresentationTimestamp = MediaTime::invalidTime();
1589                 // 1.6.5 Set the need random access point flag on all track buffers to true.
1590                 trackBuffer.needRandomAccessFlag = true;
1591             }
1592 
1593             // 1.6.6 Jump to the Loop Top step above to restart processing of the current coded frame.
1594             continue;
1595         }
1596 
1597         if (m_mode == AppendMode::Sequence) {
1598             // Use the generated timestamps instead of the sample&#39;s timestamps.
1599             sample.setTimestamps(presentationTimestamp, decodeTimestamp);
1600         } else if (trackBuffer.roundedTimestampOffset) {
1601             // Reflect the timestamp offset into the sample.
1602             sample.offsetTimestampsBy(trackBuffer.roundedTimestampOffset);
1603         }
1604 
1605         DEBUG_LOG(LOGIDENTIFIER, sample);
1606 
1607         // 1.7 Let frame end timestamp equal the sum of presentation timestamp and frame duration.
1608         MediaTime frameEndTimestamp = presentationTimestamp + frameDuration;
1609 
1610         // 1.8 If presentation timestamp is less than appendWindowStart, then set the need random access
1611         // point flag to true, drop the coded frame, and jump to the top of the loop to start processing
1612         // the next coded frame.
1613         // 1.9 If frame end timestamp is greater than appendWindowEnd, then set the need random access
1614         // point flag to true, drop the coded frame, and jump to the top of the loop to start processing
1615         // the next coded frame.
1616         if (presentationTimestamp &lt; m_appendWindowStart || frameEndTimestamp &gt; m_appendWindowEnd) {
1617             trackBuffer.needRandomAccessFlag = true;
1618             didDropSample();
1619             return;
1620         }
1621 
1622 
1623         // 1.10 If the decode timestamp is less than the presentation start time, then run the end of stream
1624         // algorithm with the error parameter set to &quot;decode&quot;, and abort these steps.
1625         // NOTE: Until &lt;https://www.w3.org/Bugs/Public/show_bug.cgi?id=27487&gt; is resolved, we will only check
1626         // the presentation timestamp.
1627         MediaTime presentationStartTime = MediaTime::zeroTime();
1628         if (presentationTimestamp &lt; presentationStartTime) {
1629             ERROR_LOG(LOGIDENTIFIER, &quot;failing because presentationTimestamp (&quot;, presentationTimestamp, &quot;) &lt; presentationStartTime (&quot;, presentationStartTime, &quot;)&quot;);
1630             m_source-&gt;streamEndedWithError(MediaSource::EndOfStreamError::Decode);
1631             return;
1632         }
1633 
1634         // 1.11 If the need random access point flag on track buffer equals true, then run the following steps:
1635         if (trackBuffer.needRandomAccessFlag) {
1636             // 1.11.1 If the coded frame is not a random access point, then drop the coded frame and jump
1637             // to the top of the loop to start processing the next coded frame.
1638             if (!sample.isSync()) {
1639                 didDropSample();
1640                 return;
1641             }
1642 
1643             // 1.11.2 Set the need random access point flag on track buffer to false.
1644             trackBuffer.needRandomAccessFlag = false;
1645         }
1646 
1647         // 1.12 Let spliced audio frame be an unset variable for holding audio splice information
1648         // 1.13 Let spliced timed text frame be an unset variable for holding timed text splice information
1649         // FIXME: Add support for sample splicing.
1650 
1651         SampleMap erasedSamples;
1652 
1653         // 1.14 If last decode timestamp for track buffer is unset and presentation timestamp falls
1654         // falls within the presentation interval of a coded frame in track buffer, then run the
1655         // following steps:
1656         if (trackBuffer.lastDecodeTimestamp.isInvalid()) {
1657             auto iter = trackBuffer.samples.presentationOrder().findSampleContainingPresentationTime(presentationTimestamp);
1658             if (iter != trackBuffer.samples.presentationOrder().end()) {
1659                 // 1.14.1 Let overlapped frame be the coded frame in track buffer that matches the condition above.
1660                 RefPtr&lt;MediaSample&gt; overlappedFrame = iter-&gt;second;
1661 
1662                 // 1.14.2 If track buffer contains audio coded frames:
1663                 // Run the audio splice frame algorithm and if a splice frame is returned, assign it to
1664                 // spliced audio frame.
1665                 // FIXME: Add support for sample splicing.
1666 
1667                 // If track buffer contains video coded frames:
1668                 if (trackBuffer.description &amp;&amp; trackBuffer.description-&gt;isVideo()) {
1669                     // 1.14.2.1 Let overlapped frame presentation timestamp equal the presentation timestamp
1670                     // of overlapped frame.
1671                     MediaTime overlappedFramePresentationTimestamp = overlappedFrame-&gt;presentationTime();
1672 
1673                     // 1.14.2.2 Let remove window timestamp equal overlapped frame presentation timestamp
1674                     // plus 1 microsecond.
1675                     MediaTime removeWindowTimestamp = overlappedFramePresentationTimestamp + microsecond;
1676 
1677                     // 1.14.2.3 If the presentation timestamp is less than the remove window timestamp,
1678                     // then remove overlapped frame and any coded frames that depend on it from track buffer.
1679                     if (presentationTimestamp &lt; removeWindowTimestamp)
1680                         erasedSamples.addSample(*iter-&gt;second);
1681                 }
1682 
1683                 // If track buffer contains timed text coded frames:
1684                 // Run the text splice frame algorithm and if a splice frame is returned, assign it to spliced timed text frame.
1685                 // FIXME: Add support for sample splicing.
1686             }
1687         }
1688 
1689         // 1.15 Remove existing coded frames in track buffer:
1690         // If highest presentation timestamp for track buffer is not set:
1691         if (trackBuffer.highestPresentationTimestamp.isInvalid()) {
1692             // Remove all coded frames from track buffer that have a presentation timestamp greater than or
1693             // equal to presentation timestamp and less than frame end timestamp.
1694             auto iter_pair = trackBuffer.samples.presentationOrder().findSamplesBetweenPresentationTimes(presentationTimestamp, frameEndTimestamp);
1695             if (iter_pair.first != trackBuffer.samples.presentationOrder().end())
1696                 erasedSamples.addRange(iter_pair.first, iter_pair.second);
1697         }
1698 
1699         // There are many files out there where the frame times are not perfectly contiguous and may have small overlaps
1700         // between the beginning of a frame and the end of the previous one; therefore a tolerance is needed whenever
1701         // durations are considered.
1702         // For instance, most WebM files are muxed rounded to the millisecond (the default TimecodeScale of the format)
1703         // but their durations use a finer timescale (causing a sub-millisecond overlap). More rarely, there are also
1704         // MP4 files with slightly off tfdt boxes, presenting a similar problem at the beginning of each fragment.
1705         const MediaTime contiguousFrameTolerance = MediaTime(1, 1000);
1706 
1707         // If highest presentation timestamp for track buffer is set and less than or equal to presentation timestamp
1708         if (trackBuffer.highestPresentationTimestamp.isValid() &amp;&amp; trackBuffer.highestPresentationTimestamp - contiguousFrameTolerance &lt;= presentationTimestamp) {
1709             // Remove all coded frames from track buffer that have a presentation timestamp greater than highest
1710             // presentation timestamp and less than or equal to frame end timestamp.
1711             do {
1712                 // NOTE: Searching from the end of the trackBuffer will be vastly more efficient if the search range is
1713                 // near the end of the buffered range. Use a linear-backwards search if the search range is within one
1714                 // frame duration of the end:
1715                 unsigned bufferedLength = trackBuffer.buffered.length();
1716                 if (!bufferedLength)
1717                     break;
1718 
1719                 MediaTime highestBufferedTime = trackBuffer.buffered.maximumBufferedTime();
1720                 MediaTime eraseBeginTime = trackBuffer.highestPresentationTimestamp - contiguousFrameTolerance;
1721                 MediaTime eraseEndTime = frameEndTimestamp - contiguousFrameTolerance;
1722 
1723                 PresentationOrderSampleMap::iterator_range range;
1724                 if (highestBufferedTime - trackBuffer.highestPresentationTimestamp &lt; trackBuffer.lastFrameDuration)
1725                     // If the new frame is at the end of the buffered ranges, perform a sequential scan from end (O(1)).
1726                     range = trackBuffer.samples.presentationOrder().findSamplesBetweenPresentationTimesFromEnd(eraseBeginTime, eraseEndTime);
1727                 else
1728                     // In any other case, perform a binary search (O(log(n)).
1729                     range = trackBuffer.samples.presentationOrder().findSamplesBetweenPresentationTimes(eraseBeginTime, eraseEndTime);
1730 
1731                 if (range.first != trackBuffer.samples.presentationOrder().end())
1732                     erasedSamples.addRange(range.first, range.second);
1733             } while(false);
1734         }
1735 
1736         // 1.16 Remove decoding dependencies of the coded frames removed in the previous step:
1737         DecodeOrderSampleMap::MapType dependentSamples;
1738         if (!erasedSamples.empty()) {
1739             // If detailed information about decoding dependencies is available:
1740             // FIXME: Add support for detailed dependency information
1741 
1742             // Otherwise: Remove all coded frames between the coded frames removed in the previous step
1743             // and the next random access point after those removed frames.
1744             auto firstDecodeIter = trackBuffer.samples.decodeOrder().findSampleWithDecodeKey(erasedSamples.decodeOrder().begin()-&gt;first);
1745             auto lastDecodeIter = trackBuffer.samples.decodeOrder().findSampleWithDecodeKey(erasedSamples.decodeOrder().rbegin()-&gt;first);
1746             auto nextSyncIter = trackBuffer.samples.decodeOrder().findSyncSampleAfterDecodeIterator(lastDecodeIter);
1747             dependentSamples.insert(firstDecodeIter, nextSyncIter);
1748 
1749             // NOTE: in the case of b-frames, the previous step may leave in place samples whose presentation
1750             // timestamp &lt; presentationTime, but whose decode timestamp &gt;= decodeTime. These will eventually cause
1751             // a decode error if left in place, so remove these samples as well.
1752             DecodeOrderSampleMap::KeyType decodeKey(sample.decodeTime(), sample.presentationTime());
1753             auto samplesWithHigherDecodeTimes = trackBuffer.samples.decodeOrder().findSamplesBetweenDecodeKeys(decodeKey, erasedSamples.decodeOrder().begin()-&gt;first);
1754             if (samplesWithHigherDecodeTimes.first != samplesWithHigherDecodeTimes.second)
1755                 dependentSamples.insert(samplesWithHigherDecodeTimes.first, samplesWithHigherDecodeTimes.second);
1756 
1757             PlatformTimeRanges erasedRanges = removeSamplesFromTrackBuffer(dependentSamples, trackBuffer, this, &quot;sourceBufferPrivateDidReceiveSample&quot;);
1758 
1759             // Only force the TrackBuffer to re-enqueue if the removed ranges overlap with enqueued and possibly
1760             // not yet displayed samples.
1761             MediaTime currentMediaTime = m_source-&gt;currentTime();
1762             if (trackBuffer.lastEnqueuedPresentationTime.isValid() &amp;&amp; currentMediaTime &lt; trackBuffer.lastEnqueuedPresentationTime) {
1763                 PlatformTimeRanges possiblyEnqueuedRanges(currentMediaTime, trackBuffer.lastEnqueuedPresentationTime);
1764                 possiblyEnqueuedRanges.intersectWith(erasedRanges);
1765                 if (possiblyEnqueuedRanges.length())
1766                     trackBuffer.needsReenqueueing = true;
1767             }
1768 
1769             erasedRanges.invert();
1770             trackBuffer.buffered.intersectWith(erasedRanges);
1771             setBufferedDirty(true);
1772         }
1773 
1774         // 1.17 If spliced audio frame is set:
1775         // Add spliced audio frame to the track buffer.
1776         // If spliced timed text frame is set:
1777         // Add spliced timed text frame to the track buffer.
1778         // FIXME: Add support for sample splicing.
1779 
1780         // Otherwise:
1781         // Add the coded frame with the presentation timestamp, decode timestamp, and frame duration to the track buffer.
1782         trackBuffer.samples.addSample(sample);
1783 
1784         // Note: The terminology here is confusing: &quot;enqueuing&quot; means providing a frame to the inner media framework.
1785         // First, frames are inserted in the decode queue; later, at the end of the append all the frames in the decode
1786         // queue are &quot;enqueued&quot; (sent to the inner media framework) in `provideMediaData()`.
1787         //
1788         // In order to check whether a frame should be added to the decode queue we check whether it starts after the
1789         // lastEnqueuedDecodeKey.
1790         DecodeOrderSampleMap::KeyType decodeKey(sample.decodeTime(), sample.presentationTime());
1791         if (trackBuffer.lastEnqueuedDecodeKey.first.isInvalid() || decodeKey &gt; trackBuffer.lastEnqueuedDecodeKey) {
1792             trackBuffer.decodeQueue.insert(DecodeOrderSampleMap::MapType::value_type(decodeKey, &amp;sample));
1793 
1794             if (trackBuffer.minimumEnqueuedPresentationTime.isValid() &amp;&amp; sample.presentationTime() &lt; trackBuffer.minimumEnqueuedPresentationTime)
1795                 trackBuffer.needsMinimumUpcomingPresentationTimeUpdating = true;
1796         }
1797 
1798         // NOTE: the spec considers &quot;Coded Frame Duration&quot; to be the presentation duration, but this is not necessarily equal
1799         // to the decoded duration. When comparing deltas between decode timestamps, the decode duration, not the presentation.
1800         if (trackBuffer.lastDecodeTimestamp.isValid()) {
1801             MediaTime lastDecodeDuration = decodeTimestamp - trackBuffer.lastDecodeTimestamp;
1802             if (!trackBuffer.greatestDecodeDuration.isValid() || lastDecodeDuration &gt; trackBuffer.greatestDecodeDuration)
1803                 trackBuffer.greatestDecodeDuration = lastDecodeDuration;
1804         }
1805 
1806         // 1.18 Set last decode timestamp for track buffer to decode timestamp.
1807         trackBuffer.lastDecodeTimestamp = decodeTimestamp;
1808 
1809         // 1.19 Set last frame duration for track buffer to frame duration.
1810         trackBuffer.lastFrameDuration = frameDuration;
1811 
1812         // 1.20 If highest presentation timestamp for track buffer is unset or frame end timestamp is greater
1813         // than highest presentation timestamp, then set highest presentation timestamp for track buffer
1814         // to frame end timestamp.
1815         if (trackBuffer.highestPresentationTimestamp.isInvalid() || frameEndTimestamp &gt; trackBuffer.highestPresentationTimestamp)
1816             trackBuffer.highestPresentationTimestamp = frameEndTimestamp;
1817 
1818         // 1.21 If frame end timestamp is greater than group end timestamp, then set group end timestamp equal
1819         // to frame end timestamp.
1820         if (m_groupEndTimestamp.isInvalid() || frameEndTimestamp &gt; m_groupEndTimestamp)
1821             m_groupEndTimestamp = frameEndTimestamp;
1822 
1823         // 1.22 If generate timestamps flag equals true, then set timestampOffset equal to frame end timestamp.
1824         if (m_shouldGenerateTimestamps) {
1825             m_timestampOffset = frameEndTimestamp;
1826             for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
1827                 trackBuffer.lastFrameTimescale = 0;
1828                 trackBuffer.roundedTimestampOffset = MediaTime::invalidTime();
1829             }
1830         }
1831 
1832         // Eliminate small gaps between buffered ranges by coalescing
1833         // disjoint ranges separated by less than a &quot;fudge factor&quot;.
1834         auto presentationEndTime = presentationTimestamp + frameDuration;
1835         auto nearestToPresentationStartTime = trackBuffer.buffered.nearest(presentationTimestamp);
1836         if (nearestToPresentationStartTime.isValid() &amp;&amp; (presentationTimestamp - nearestToPresentationStartTime).isBetween(MediaTime::zeroTime(), MediaSource::currentTimeFudgeFactor()))
1837             presentationTimestamp = nearestToPresentationStartTime;
1838 
1839         auto nearestToPresentationEndTime = trackBuffer.buffered.nearest(presentationEndTime);
1840         if (nearestToPresentationEndTime.isValid() &amp;&amp; (nearestToPresentationEndTime - presentationEndTime).isBetween(MediaTime::zeroTime(), MediaSource::currentTimeFudgeFactor()))
1841             presentationEndTime = nearestToPresentationEndTime;
1842 
1843         trackBuffer.buffered.add(presentationTimestamp, presentationEndTime);
1844         m_bufferedSinceLastMonitor += frameDuration.toDouble();
1845         setBufferedDirty(true);
1846 
1847         break;
1848     } while (1);
1849 
1850     // Steps 2-4 will be handled by MediaSource::monitorSourceBuffers()
1851 
1852     // 5. If the media segment contains data beyond the current duration, then run the duration change algorithm with new
1853     // duration set to the maximum of the current duration and the group end timestamp.
1854     if (m_groupEndTimestamp &gt; m_source-&gt;duration())
1855         m_source-&gt;setDurationInternal(m_groupEndTimestamp);
1856 }
1857 
1858 bool SourceBuffer::hasAudio() const
1859 {
1860     return m_audioTracks &amp;&amp; m_audioTracks-&gt;length();
1861 }
1862 
1863 bool SourceBuffer::hasVideo() const
1864 {
1865     return m_videoTracks &amp;&amp; m_videoTracks-&gt;length();
1866 }
1867 
1868 bool SourceBuffer::sourceBufferPrivateHasAudio() const
1869 {
1870     return hasAudio();
1871 }
1872 
1873 bool SourceBuffer::sourceBufferPrivateHasVideo() const
1874 {
1875     return hasVideo();
1876 }
1877 
1878 void SourceBuffer::videoTrackSelectedChanged(VideoTrack&amp; track)
1879 {
1880     // 2.4.5 Changes to selected/enabled track state
1881     // If the selected video track changes, then run the following steps:
1882     // 1. If the SourceBuffer associated with the previously selected video track is not associated with
1883     // any other enabled tracks, run the following steps:
1884     if (!track.selected()
1885         &amp;&amp; (!m_videoTracks || !m_videoTracks-&gt;isAnyTrackEnabled())
1886         &amp;&amp; (!m_audioTracks || !m_audioTracks-&gt;isAnyTrackEnabled())
1887         &amp;&amp; (!m_textTracks || !m_textTracks-&gt;isAnyTrackEnabled())) {
1888         // 1.1 Remove the SourceBuffer from activeSourceBuffers.
1889         // 1.2 Queue a task to fire a simple event named removesourcebuffer at activeSourceBuffers
1890         setActive(false);
1891     } else if (track.selected()) {
1892         // 2. If the SourceBuffer associated with the newly selected video track is not already in activeSourceBuffers,
1893         // run the following steps:
1894         // 2.1 Add the SourceBuffer to activeSourceBuffers.
1895         // 2.2 Queue a task to fire a simple event named addsourcebuffer at activeSourceBuffers
1896         setActive(true);
1897     }
1898 
1899     if (m_videoTracks &amp;&amp; m_videoTracks-&gt;contains(track))
1900         m_videoTracks-&gt;scheduleChangeEvent();
1901 
1902     if (!isRemoved())
1903         m_source-&gt;mediaElement()-&gt;videoTrackSelectedChanged(track);
1904 }
1905 
1906 void SourceBuffer::audioTrackEnabledChanged(AudioTrack&amp; track)
1907 {
1908     // 2.4.5 Changes to selected/enabled track state
1909     // If an audio track becomes disabled and the SourceBuffer associated with this track is not
1910     // associated with any other enabled or selected track, then run the following steps:
1911     if (!track.enabled()
1912         &amp;&amp; (!m_videoTracks || !m_videoTracks-&gt;isAnyTrackEnabled())
1913         &amp;&amp; (!m_audioTracks || !m_audioTracks-&gt;isAnyTrackEnabled())
1914         &amp;&amp; (!m_textTracks || !m_textTracks-&gt;isAnyTrackEnabled())) {
1915         // 1. Remove the SourceBuffer associated with the audio track from activeSourceBuffers
1916         // 2. Queue a task to fire a simple event named removesourcebuffer at activeSourceBuffers
1917         setActive(false);
1918     } else if (track.enabled()) {
1919         // If an audio track becomes enabled and the SourceBuffer associated with this track is
1920         // not already in activeSourceBuffers, then run the following steps:
1921         // 1. Add the SourceBuffer associated with the audio track to activeSourceBuffers
1922         // 2. Queue a task to fire a simple event named addsourcebuffer at activeSourceBuffers
1923         setActive(true);
1924     }
1925 
1926     if (m_audioTracks &amp;&amp; m_audioTracks-&gt;contains(track))
1927         m_audioTracks-&gt;scheduleChangeEvent();
1928 
1929     if (!isRemoved())
1930         m_source-&gt;mediaElement()-&gt;audioTrackEnabledChanged(track);
1931 }
1932 
1933 void SourceBuffer::textTrackModeChanged(TextTrack&amp; track)
1934 {
1935     // 2.4.5 Changes to selected/enabled track state
1936     // If a text track mode becomes &quot;disabled&quot; and the SourceBuffer associated with this track is not
1937     // associated with any other enabled or selected track, then run the following steps:
1938     if (track.mode() == TextTrack::Mode::Disabled
1939         &amp;&amp; (!m_videoTracks || !m_videoTracks-&gt;isAnyTrackEnabled())
1940         &amp;&amp; (!m_audioTracks || !m_audioTracks-&gt;isAnyTrackEnabled())
1941         &amp;&amp; (!m_textTracks || !m_textTracks-&gt;isAnyTrackEnabled())) {
1942         // 1. Remove the SourceBuffer associated with the audio track from activeSourceBuffers
1943         // 2. Queue a task to fire a simple event named removesourcebuffer at activeSourceBuffers
1944         setActive(false);
1945     } else {
1946         // If a text track mode becomes &quot;showing&quot; or &quot;hidden&quot; and the SourceBuffer associated with this
1947         // track is not already in activeSourceBuffers, then run the following steps:
1948         // 1. Add the SourceBuffer associated with the text track to activeSourceBuffers
1949         // 2. Queue a task to fire a simple event named addsourcebuffer at activeSourceBuffers
1950         setActive(true);
1951     }
1952 
1953     if (m_textTracks &amp;&amp; m_textTracks-&gt;contains(track))
1954         m_textTracks-&gt;scheduleChangeEvent();
1955 
1956     if (!isRemoved())
1957         m_source-&gt;mediaElement()-&gt;textTrackModeChanged(track);
1958 }
1959 
1960 void SourceBuffer::textTrackAddCue(TextTrack&amp; track, TextTrackCue&amp; cue)
1961 {
1962     if (!isRemoved())
1963         m_source-&gt;mediaElement()-&gt;textTrackAddCue(track, cue);
1964 }
1965 
1966 void SourceBuffer::textTrackAddCues(TextTrack&amp; track, const TextTrackCueList&amp; cueList)
1967 {
1968     if (!isRemoved())
1969         m_source-&gt;mediaElement()-&gt;textTrackAddCues(track, cueList);
1970 }
1971 
1972 void SourceBuffer::textTrackRemoveCue(TextTrack&amp; track, TextTrackCue&amp; cue)
1973 {
1974     if (!isRemoved())
1975         m_source-&gt;mediaElement()-&gt;textTrackRemoveCue(track, cue);
1976 }
1977 
1978 void SourceBuffer::textTrackRemoveCues(TextTrack&amp; track, const TextTrackCueList&amp; cueList)
1979 {
1980     if (!isRemoved())
1981         m_source-&gt;mediaElement()-&gt;textTrackRemoveCues(track, cueList);
1982 }
1983 
1984 void SourceBuffer::textTrackKindChanged(TextTrack&amp; track)
1985 {
1986     if (!isRemoved())
1987         m_source-&gt;mediaElement()-&gt;textTrackKindChanged(track);
1988 }
1989 
1990 void SourceBuffer::sourceBufferPrivateReenqueSamples(const AtomString&amp; trackID)
1991 {
1992     if (isRemoved())
1993         return;
1994 
1995     DEBUG_LOG(LOGIDENTIFIER);
1996     auto it = m_trackBufferMap.find(trackID);
1997     if (it == m_trackBufferMap.end())
1998         return;
1999 
2000     auto&amp; trackBuffer = it-&gt;value;
2001     trackBuffer.needsReenqueueing = true;
2002     reenqueueMediaForTime(trackBuffer, trackID, m_source-&gt;currentTime());
2003 }
2004 
2005 void SourceBuffer::sourceBufferPrivateDidBecomeReadyForMoreSamples(const AtomString&amp; trackID)
2006 {
2007     if (isRemoved())
2008         return;
2009 
2010     DEBUG_LOG(LOGIDENTIFIER);
2011     auto it = m_trackBufferMap.find(trackID);
2012     if (it == m_trackBufferMap.end())
2013         return;
2014 
2015     auto&amp; trackBuffer = it-&gt;value;
2016     if (!trackBuffer.needsReenqueueing &amp;&amp; !m_source-&gt;isSeeking())
2017         provideMediaData(trackBuffer, trackID);
2018 }
2019 
2020 void SourceBuffer::provideMediaData(TrackBuffer&amp; trackBuffer, const AtomString&amp; trackID)
2021 {
2022     if (m_source-&gt;isSeeking())
2023         return;
2024 
2025 #if !RELEASE_LOG_DISABLED
2026     unsigned enqueuedSamples = 0;
2027 #endif
2028 
2029     if (trackBuffer.needsMinimumUpcomingPresentationTimeUpdating)
2030         resetMinimumUpcomingPresentationTime(trackBuffer, trackID);
2031 
2032     while (!trackBuffer.decodeQueue.empty()) {
2033         if (!m_private-&gt;isReadyForMoreSamples(trackID)) {
2034             DEBUG_LOG(LOGIDENTIFIER, &quot;bailing early, track id &quot;, trackID, &quot; is not ready for more data&quot;);
2035             m_private-&gt;notifyClientWhenReadyForMoreSamples(trackID);
2036             break;
2037         }
2038 
2039         // FIXME(rdar://problem/20635969): Remove this re-entrancy protection when the aforementioned radar is resolved; protecting
2040         // against re-entrancy introduces a small inefficency when removing appended samples from the decode queue one at a time
2041         // rather than when all samples have been enqueued.
2042         auto sample = trackBuffer.decodeQueue.begin()-&gt;second;
2043 
2044         // Do not enqueue samples spanning a significant unbuffered gap.
2045         // NOTE: one second is somewhat arbitrary. MediaSource::monitorSourceBuffers() is run
2046         // on the playbackTimer, which is effectively every 350ms. Allowing &gt; 350ms gap between
2047         // enqueued samples allows for situations where we overrun the end of a buffered range
2048         // but don&#39;t notice for 350s of playback time, and the client can enqueue data for the
2049         // new current time without triggering this early return.
2050         // FIXME(135867): Make this gap detection logic less arbitrary.
2051         MediaTime oneSecond(1, 1);
2052         if (trackBuffer.lastEnqueuedDecodeKey.first.isValid()
2053             &amp;&amp; trackBuffer.lastEnqueuedDecodeDuration.isValid()
2054             &amp;&amp; sample-&gt;decodeTime() - trackBuffer.lastEnqueuedDecodeKey.first &gt; oneSecond + trackBuffer.lastEnqueuedDecodeDuration) {
2055 
2056         DEBUG_LOG(LOGIDENTIFIER, &quot;bailing early because of unbuffered gap, new sample: &quot;, sample-&gt;decodeTime(), &quot;, last enqueued sample ends: &quot;, trackBuffer.lastEnqueuedDecodeKey.first + trackBuffer.lastEnqueuedDecodeDuration);
2057             break;
2058         }
2059 
2060         // Remove the sample from the decode queue now.
2061         trackBuffer.decodeQueue.erase(trackBuffer.decodeQueue.begin());
2062 
2063         trackBuffer.lastEnqueuedPresentationTime = sample-&gt;presentationTime();
2064         trackBuffer.lastEnqueuedDecodeKey = {sample-&gt;decodeTime(), sample-&gt;presentationTime()};
2065         trackBuffer.lastEnqueuedDecodeDuration = sample-&gt;duration();
2066         m_private-&gt;enqueueSample(sample.releaseNonNull(), trackID);
2067 #if !RELEASE_LOG_DISABLED
2068         ++enqueuedSamples;
2069 #endif
2070     }
2071 
2072     updateMinimumUpcomingPresentationTime(trackBuffer, trackID);
2073 
2074 #if !RELEASE_LOG_DISABLED
2075     DEBUG_LOG(LOGIDENTIFIER, &quot;enqueued &quot;, enqueuedSamples, &quot; samples, &quot;, static_cast&lt;size_t&gt;(trackBuffer.decodeQueue.size()), &quot; remaining&quot;);
2076 #endif
2077 
2078     trySignalAllSamplesInTrackEnqueued(trackID);
2079 }
2080 
2081 void SourceBuffer::updateMinimumUpcomingPresentationTime(TrackBuffer&amp; trackBuffer, const AtomString&amp; trackID)
2082 {
2083     if (!m_private-&gt;canSetMinimumUpcomingPresentationTime(trackID))
2084         return;
2085 
2086     if (trackBuffer.decodeQueue.empty()) {
2087         trackBuffer.minimumEnqueuedPresentationTime = MediaTime::invalidTime();
2088         m_private-&gt;clearMinimumUpcomingPresentationTime(trackID);
2089         return;
2090     }
2091 
2092     auto minPts = std::min_element(trackBuffer.decodeQueue.begin(), trackBuffer.decodeQueue.end(), [](auto&amp; left, auto&amp; right) -&gt; bool {
2093         return left.second-&gt;outputPresentationTime() &lt; right.second-&gt;outputPresentationTime();
2094     });
2095 
2096     if (minPts == trackBuffer.decodeQueue.end()) {
2097         trackBuffer.minimumEnqueuedPresentationTime = MediaTime::invalidTime();
2098         m_private-&gt;clearMinimumUpcomingPresentationTime(trackID);
2099         return;
2100     }
2101 
2102     trackBuffer.minimumEnqueuedPresentationTime = minPts-&gt;second-&gt;outputPresentationTime();
2103     m_private-&gt;setMinimumUpcomingPresentationTime(trackID, trackBuffer.minimumEnqueuedPresentationTime);
2104 }
2105 
2106 
2107 void SourceBuffer::resetMinimumUpcomingPresentationTime(TrackBuffer&amp; trackBuffer, const AtomString&amp; trackID)
2108 {
2109     if (!m_private-&gt;canSetMinimumUpcomingPresentationTime(trackID))
2110         return;
2111 
2112     trackBuffer.minimumEnqueuedPresentationTime = MediaTime::invalidTime();
2113     m_private-&gt;clearMinimumUpcomingPresentationTime(trackID);
2114 }
2115 
2116 void SourceBuffer::trySignalAllSamplesInTrackEnqueued(const AtomString&amp; trackID)
2117 {
2118     if (m_source-&gt;isEnded() &amp;&amp; m_trackBufferMap.get(trackID).decodeQueue.empty()) {
2119         DEBUG_LOG(LOGIDENTIFIER, &quot;enqueued all samples from track &quot;, trackID);
2120         m_private-&gt;allSamplesInTrackEnqueued(trackID);
2121     }
2122 }
2123 
2124 void SourceBuffer::trySignalAllSamplesEnqueued()
2125 {
2126     for (const AtomString&amp; trackID : m_trackBufferMap.keys())
2127         trySignalAllSamplesInTrackEnqueued(trackID);
2128 }
2129 
2130 void SourceBuffer::reenqueueMediaForTime(TrackBuffer&amp; trackBuffer, const AtomString&amp; trackID, const MediaTime&amp; time)
2131 {
2132     m_private-&gt;flush(trackID);
2133     trackBuffer.decodeQueue.clear();
2134 
2135     // Find the sample which contains the current presentation time.
2136     auto currentSamplePTSIterator = trackBuffer.samples.presentationOrder().findSampleContainingPresentationTime(time);
2137 
2138     if (currentSamplePTSIterator == trackBuffer.samples.presentationOrder().end())
2139         currentSamplePTSIterator = trackBuffer.samples.presentationOrder().findSampleStartingOnOrAfterPresentationTime(time);
2140 
2141     if (currentSamplePTSIterator == trackBuffer.samples.presentationOrder().end()
2142         || (currentSamplePTSIterator-&gt;first - time) &gt; MediaSource::currentTimeFudgeFactor())
2143         return;
2144 
2145     // Seach backward for the previous sync sample.
2146     DecodeOrderSampleMap::KeyType decodeKey(currentSamplePTSIterator-&gt;second-&gt;decodeTime(), currentSamplePTSIterator-&gt;second-&gt;presentationTime());
2147     auto currentSampleDTSIterator = trackBuffer.samples.decodeOrder().findSampleWithDecodeKey(decodeKey);
2148     ASSERT(currentSampleDTSIterator != trackBuffer.samples.decodeOrder().end());
2149 
2150     auto reverseCurrentSampleIter = --DecodeOrderSampleMap::reverse_iterator(currentSampleDTSIterator);
2151     auto reverseLastSyncSampleIter = trackBuffer.samples.decodeOrder().findSyncSamplePriorToDecodeIterator(reverseCurrentSampleIter);
2152     if (reverseLastSyncSampleIter == trackBuffer.samples.decodeOrder().rend())
2153         return;
2154 
2155     // Fill the decode queue with the non-displaying samples.
2156     for (auto iter = reverseLastSyncSampleIter; iter != reverseCurrentSampleIter; --iter) {
2157         auto copy = iter-&gt;second-&gt;createNonDisplayingCopy();
2158         DecodeOrderSampleMap::KeyType decodeKey(copy-&gt;decodeTime(), copy-&gt;presentationTime());
2159         trackBuffer.decodeQueue.insert(DecodeOrderSampleMap::MapType::value_type(decodeKey, WTFMove(copy)));
2160     }
2161 
2162     if (!trackBuffer.decodeQueue.empty()) {
2163         auto lastSampleIter = trackBuffer.decodeQueue.rbegin();
2164         auto lastSampleDecodeKey = lastSampleIter-&gt;first;
2165         auto lastSampleDuration = lastSampleIter-&gt;second-&gt;duration();
2166         trackBuffer.lastEnqueuedPresentationTime = lastSampleDecodeKey.second;
2167         trackBuffer.lastEnqueuedDecodeKey = lastSampleDecodeKey;
2168         trackBuffer.lastEnqueuedDecodeDuration = lastSampleDuration;
2169     } else {
2170         trackBuffer.lastEnqueuedPresentationTime = MediaTime::invalidTime();
2171         trackBuffer.lastEnqueuedDecodeKey = {MediaTime::invalidTime(), MediaTime::invalidTime()};
2172         trackBuffer.lastEnqueuedDecodeDuration = MediaTime::invalidTime();
2173     }
2174 
2175     // Fill the decode queue with the remaining samples.
2176     for (auto iter = currentSampleDTSIterator; iter != trackBuffer.samples.decodeOrder().end(); ++iter)
2177         trackBuffer.decodeQueue.insert(*iter);
2178     provideMediaData(trackBuffer, trackID);
2179 
2180     trackBuffer.needsReenqueueing = false;
2181 }
2182 
2183 
2184 void SourceBuffer::didDropSample()
2185 {
2186     if (!isRemoved())
2187         m_source-&gt;mediaElement()-&gt;incrementDroppedFrameCount();
2188 }
2189 
2190 void SourceBuffer::monitorBufferingRate()
2191 {
2192     MonotonicTime now = MonotonicTime::now();
2193     Seconds interval = now - m_timeOfBufferingMonitor;
2194     double rateSinceLastMonitor = m_bufferedSinceLastMonitor / interval.seconds();
2195 
2196     m_timeOfBufferingMonitor = now;
2197     m_bufferedSinceLastMonitor = 0;
2198 
2199     m_averageBufferRate += (interval.seconds() * ExponentialMovingAverageCoefficient) * (rateSinceLastMonitor - m_averageBufferRate);
2200 
2201     DEBUG_LOG(LOGIDENTIFIER, m_averageBufferRate);
2202 }
2203 
2204 void SourceBuffer::updateBufferedFromTrackBuffers()
2205 {
2206     // 3.1 Attributes, buffered
2207     // https://rawgit.com/w3c/media-source/45627646344eea0170dd1cbc5a3d508ca751abb8/media-source-respec.html#dom-sourcebuffer-buffered
2208 
2209     // 2. Let highest end time be the largest track buffer ranges end time across all the track buffers managed by this SourceBuffer object.
2210     MediaTime highestEndTime = MediaTime::negativeInfiniteTime();
2211     for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
2212         if (!trackBuffer.buffered.length())
2213             continue;
2214         highestEndTime = std::max(highestEndTime, trackBuffer.buffered.maximumBufferedTime());
2215     }
2216 
2217     // NOTE: Short circuit the following if none of the TrackBuffers have buffered ranges to avoid generating
2218     // a single range of {0, 0}.
2219     if (highestEndTime.isNegativeInfinite()) {
2220         m_buffered-&gt;ranges() = PlatformTimeRanges();
2221         return;
2222     }
2223 
2224     // 3. Let intersection ranges equal a TimeRange object containing a single range from 0 to highest end time.
2225     PlatformTimeRanges intersectionRanges { MediaTime::zeroTime(), highestEndTime };
2226 
2227     // 4. For each audio and video track buffer managed by this SourceBuffer, run the following steps:
2228     for (auto&amp; trackBuffer : m_trackBufferMap.values()) {
2229         // 4.1 Let track ranges equal the track buffer ranges for the current track buffer.
2230         PlatformTimeRanges trackRanges = trackBuffer.buffered;
2231         if (!trackRanges.length())
2232             continue;
2233 
2234         // 4.2 If readyState is &quot;ended&quot;, then set the end time on the last range in track ranges to highest end time.
2235         if (m_source-&gt;isEnded())
2236             trackRanges.add(trackRanges.maximumBufferedTime(), highestEndTime);
2237 
2238         // 4.3 Let new intersection ranges equal the intersection between the intersection ranges and the track ranges.
2239         // 4.4 Replace the ranges in intersection ranges with the new intersection ranges.
2240         intersectionRanges.intersectWith(trackRanges);
2241     }
2242 
2243     // 5. If intersection ranges does not contain the exact same range information as the current value of this attribute,
2244     //    then update the current value of this attribute to intersection ranges.
2245     m_buffered-&gt;ranges() = intersectionRanges;
2246     setBufferedDirty(true);
2247 }
2248 
2249 bool SourceBuffer::canPlayThroughRange(PlatformTimeRanges&amp; ranges)
2250 {
2251     if (isRemoved())
2252         return false;
2253 
2254     monitorBufferingRate();
2255 
2256     // Assuming no fluctuations in the buffering rate, loading 1 second per second or greater
2257     // means indefinite playback. This could be improved by taking jitter into account.
2258     if (m_averageBufferRate &gt; 1)
2259         return true;
2260 
2261     // Add up all the time yet to be buffered.
2262     MediaTime currentTime = m_source-&gt;currentTime();
2263     MediaTime duration = m_source-&gt;duration();
2264 
2265     PlatformTimeRanges unbufferedRanges = ranges;
2266     unbufferedRanges.invert();
2267     unbufferedRanges.intersectWith(PlatformTimeRanges(currentTime, std::max(currentTime, duration)));
2268     MediaTime unbufferedTime = unbufferedRanges.totalDuration();
2269     if (!unbufferedTime.isValid())
2270         return true;
2271 
2272     MediaTime timeRemaining = duration - currentTime;
2273     return unbufferedTime.toDouble() / m_averageBufferRate &lt; timeRemaining.toDouble();
2274 }
2275 
2276 size_t SourceBuffer::extraMemoryCost() const
2277 {
2278     size_t extraMemoryCost = m_pendingAppendData.capacity();
2279     for (auto&amp; trackBuffer : m_trackBufferMap.values())
2280         extraMemoryCost += trackBuffer.samples.sizeInBytes();
2281 
2282     return extraMemoryCost;
2283 }
2284 
2285 void SourceBuffer::reportExtraMemoryAllocated()
2286 {
2287     size_t extraMemoryCost = this-&gt;extraMemoryCost();
2288     if (extraMemoryCost &lt;= m_reportedExtraMemoryCost)
2289         return;
2290 
2291     size_t extraMemoryCostDelta = extraMemoryCost - m_reportedExtraMemoryCost;
2292     m_reportedExtraMemoryCost = extraMemoryCost;
2293 
2294     JSC::JSLockHolder lock(scriptExecutionContext()-&gt;vm());
2295     // FIXME: Adopt reportExtraMemoryVisited, and switch to reportExtraMemoryAllocated.
2296     // https://bugs.webkit.org/show_bug.cgi?id=142595
2297     scriptExecutionContext()-&gt;vm().heap.deprecatedReportExtraMemory(extraMemoryCostDelta);
2298 }
2299 
2300 Vector&lt;String&gt; SourceBuffer::bufferedSamplesForTrackID(const AtomString&amp; trackID)
2301 {
2302     auto it = m_trackBufferMap.find(trackID);
2303     if (it == m_trackBufferMap.end())
2304         return Vector&lt;String&gt;();
2305 
2306     TrackBuffer&amp; trackBuffer = it-&gt;value;
2307     Vector&lt;String&gt; sampleDescriptions;
2308     for (auto&amp; pair : trackBuffer.samples.decodeOrder())
2309         sampleDescriptions.append(toString(*pair.second));
2310 
2311     return sampleDescriptions;
2312 }
2313 
2314 Vector&lt;String&gt; SourceBuffer::enqueuedSamplesForTrackID(const AtomString&amp; trackID)
2315 {
2316     return m_private-&gt;enqueuedSamplesForTrackID(trackID);
2317 }
2318 
2319 MediaTime SourceBuffer::minimumUpcomingPresentationTimeForTrackID(const AtomString&amp; trackID)
2320 {
2321     return m_private-&gt;minimumUpcomingPresentationTimeForTrackID(trackID);
2322 }
2323 
2324 void SourceBuffer::setMaximumQueueDepthForTrackID(const AtomString&amp; trackID, size_t maxQueueDepth)
2325 {
2326     m_private-&gt;setMaximumQueueDepthForTrackID(trackID, maxQueueDepth);
2327 }
2328 
2329 Document&amp; SourceBuffer::document() const
2330 {
2331     ASSERT(scriptExecutionContext());
2332     return downcast&lt;Document&gt;(*scriptExecutionContext());
2333 }
2334 
2335 ExceptionOr&lt;void&gt; SourceBuffer::setMode(AppendMode newMode)
2336 {
2337     // 3.1 Attributes - mode
2338     // http://www.w3.org/TR/media-source/#widl-SourceBuffer-mode
2339 
2340     // On setting, run the following steps:
2341 
2342     // 1. Let new mode equal the new value being assigned to this attribute.
2343     // 2. If generate timestamps flag equals true and new mode equals &quot;segments&quot;, then throw an InvalidAccessError exception and abort these steps.
2344     if (m_shouldGenerateTimestamps &amp;&amp; newMode == AppendMode::Segments)
2345         return Exception { InvalidAccessError };
2346 
2347     // 3. If this object has been removed from the sourceBuffers attribute of the parent media source, then throw an InvalidStateError exception and abort these steps.
2348     // 4. If the updating attribute equals true, then throw an InvalidStateError exception and abort these steps.
2349     if (isRemoved() || m_updating)
2350         return Exception { InvalidStateError };
2351 
2352     // 5. If the readyState attribute of the parent media source is in the &quot;ended&quot; state then run the following steps:
2353     if (m_source-&gt;isEnded()) {
2354         // 5.1. Set the readyState attribute of the parent media source to &quot;open&quot;
2355         // 5.2. Queue a task to fire a simple event named sourceopen at the parent media source.
2356         m_source-&gt;openIfInEndedState();
2357     }
2358 
2359     // 6. If the append state equals PARSING_MEDIA_SEGMENT, then throw an InvalidStateError and abort these steps.
2360     if (m_appendState == ParsingMediaSegment)
2361         return Exception { InvalidStateError };
2362 
2363     // 7. If the new mode equals &quot;sequence&quot;, then set the group start timestamp to the group end timestamp.
2364     if (newMode == AppendMode::Sequence)
2365         m_groupStartTimestamp = m_groupEndTimestamp;
2366 
2367     // 8. Update the attribute to new mode.
2368     m_mode = newMode;
2369 
2370     return { };
2371 }
2372 
2373 #if !RELEASE_LOG_DISABLED
2374 WTFLogChannel&amp; SourceBuffer::logChannel() const
2375 {
2376     return LogMediaSource;
2377 }
2378 #endif
2379 
2380 } // namespace WebCore
2381 
2382 #endif
    </pre>
  </body>
</html>