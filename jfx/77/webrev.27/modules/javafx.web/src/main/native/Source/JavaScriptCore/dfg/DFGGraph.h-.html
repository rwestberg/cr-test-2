<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGGraph.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2011-2018 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #pragma once
  27 
  28 #if ENABLE(DFG_JIT)
  29 
  30 #include &quot;AssemblyHelpers.h&quot;
  31 #include &quot;BytecodeLivenessAnalysisInlines.h&quot;
  32 #include &quot;CodeBlock.h&quot;
  33 #include &quot;DFGArgumentPosition.h&quot;
  34 #include &quot;DFGBasicBlock.h&quot;
  35 #include &quot;DFGFrozenValue.h&quot;
  36 #include &quot;DFGNode.h&quot;
  37 #include &quot;DFGPlan.h&quot;
  38 #include &quot;DFGPropertyTypeKey.h&quot;
  39 #include &quot;DFGScannable.h&quot;
  40 #include &quot;FullBytecodeLiveness.h&quot;
  41 #include &quot;MethodOfGettingAValueProfile.h&quot;
  42 #include &lt;wtf/BitVector.h&gt;
  43 #include &lt;wtf/HashMap.h&gt;
  44 #include &lt;wtf/Vector.h&gt;
  45 #include &lt;wtf/StdLibExtras.h&gt;
  46 #include &lt;wtf/StdUnorderedMap.h&gt;
  47 
  48 namespace WTF {
  49 template &lt;typename T&gt; class SingleRootGraph;
  50 }
  51 
  52 namespace JSC {
  53 
  54 class CodeBlock;
  55 class ExecState;
  56 
  57 namespace DFG {
  58 
  59 class BackwardsCFG;
  60 class BackwardsDominators;
  61 class CFG;
  62 class CPSCFG;
  63 class ControlEquivalenceAnalysis;
  64 template &lt;typename T&gt; class Dominators;
  65 template &lt;typename T&gt; class NaturalLoops;
  66 class FlowIndexing;
  67 template&lt;typename&gt; class FlowMap;
  68 
  69 using ArgumentsVector = Vector&lt;Node*, 8&gt;;
  70 
  71 using SSACFG = CFG;
  72 using CPSDominators = Dominators&lt;CPSCFG&gt;;
  73 using SSADominators = Dominators&lt;SSACFG&gt;;
  74 using CPSNaturalLoops = NaturalLoops&lt;CPSCFG&gt;;
  75 using SSANaturalLoops = NaturalLoops&lt;SSACFG&gt;;
  76 
  77 #define DFG_NODE_DO_TO_CHILDREN(graph, node, thingToDo) do {            \
  78         Node* _node = (node);                                           \
  79         if (_node-&gt;flags() &amp; NodeHasVarArgs) {                          \
  80             for (unsigned _childIdx = _node-&gt;firstChild();              \
  81                 _childIdx &lt; _node-&gt;firstChild() + _node-&gt;numChildren(); \
  82                 _childIdx++) {                                          \
  83                 if (!!(graph).m_varArgChildren[_childIdx])              \
  84                     thingToDo(_node, (graph).m_varArgChildren[_childIdx]); \
  85             }                                                           \
  86         } else {                                                        \
  87             for (unsigned _edgeIndex = 0; _edgeIndex &lt; AdjacencyList::Size; _edgeIndex++) { \
  88                 Edge&amp; _edge = _node-&gt;children.child(_edgeIndex);        \
  89                 if (!_edge)                                             \
  90                     break;                                              \
  91                 thingToDo(_node, _edge);                                \
  92             }                                                           \
  93         }                                                               \
  94     } while (false)
  95 
  96 #define DFG_ASSERT(graph, node, assertion, ...) do {                    \
  97         if (!!(assertion))                                              \
  98             break;                                                      \
  99         (graph).logAssertionFailure(                                    \
 100             (node), __FILE__, __LINE__, WTF_PRETTY_FUNCTION, #assertion); \
 101         CRASH_WITH_SECURITY_IMPLICATION_AND_INFO(__VA_ARGS__);          \
 102     } while (false)
 103 
 104 #define DFG_CRASH(graph, node, reason, ...) do {                        \
 105         (graph).logAssertionFailure(                                    \
 106             (node), __FILE__, __LINE__, WTF_PRETTY_FUNCTION, (reason)); \
 107         CRASH_WITH_SECURITY_IMPLICATION_AND_INFO(__VA_ARGS__);          \
 108     } while (false)
 109 
 110 struct InlineVariableData {
 111     InlineCallFrame* inlineCallFrame;
 112     unsigned argumentPositionStart;
 113     VariableAccessData* calleeVariable;
 114 };
 115 
 116 enum AddSpeculationMode {
 117     DontSpeculateInt32,
 118     SpeculateInt32AndTruncateConstants,
 119     SpeculateInt32
 120 };
 121 
 122 //
 123 // === Graph ===
 124 //
 125 // The order may be significant for nodes with side-effects (property accesses, value conversions).
 126 // Nodes that are &#39;dead&#39; remain in the vector with refCount 0.
 127 class Graph : public virtual Scannable {
 128 public:
 129     Graph(VM&amp;, Plan&amp;);
 130     ~Graph();
 131 
 132     void changeChild(Edge&amp; edge, Node* newNode)
 133     {
 134         edge.setNode(newNode);
 135     }
 136 
 137     void changeEdge(Edge&amp; edge, Edge newEdge)
 138     {
 139         edge = newEdge;
 140     }
 141 
 142     void compareAndSwap(Edge&amp; edge, Node* oldNode, Node* newNode)
 143     {
 144         if (edge.node() != oldNode)
 145             return;
 146         changeChild(edge, newNode);
 147     }
 148 
 149     void compareAndSwap(Edge&amp; edge, Edge oldEdge, Edge newEdge)
 150     {
 151         if (edge != oldEdge)
 152             return;
 153         changeEdge(edge, newEdge);
 154     }
 155 
 156     void performSubstitution(Node* node)
 157     {
 158         if (node-&gt;flags() &amp; NodeHasVarArgs) {
 159             for (unsigned childIdx = node-&gt;firstChild(); childIdx &lt; node-&gt;firstChild() + node-&gt;numChildren(); childIdx++)
 160                 performSubstitutionForEdge(m_varArgChildren[childIdx]);
 161         } else {
 162             performSubstitutionForEdge(node-&gt;child1());
 163             performSubstitutionForEdge(node-&gt;child2());
 164             performSubstitutionForEdge(node-&gt;child3());
 165         }
 166     }
 167 
 168     void performSubstitutionForEdge(Edge&amp; child)
 169     {
 170         // Check if this operand is actually unused.
 171         if (!child)
 172             return;
 173 
 174         // Check if there is any replacement.
 175         Node* replacement = child-&gt;replacement();
 176         if (!replacement)
 177             return;
 178 
 179         child.setNode(replacement);
 180 
 181         // There is definitely a replacement. Assert that the replacement does not
 182         // have a replacement.
 183         ASSERT(!child-&gt;replacement());
 184     }
 185 
 186     template&lt;typename... Params&gt;
 187     Node* addNode(Params... params)
 188     {
 189         return m_nodes.addNew(params...);
 190     }
 191 
 192     template&lt;typename... Params&gt;
 193     Node* addNode(SpeculatedType type, Params... params)
 194     {
 195         Node* node = m_nodes.addNew(params...);
 196         node-&gt;predict(type);
 197         return node;
 198     }
 199 
 200     void deleteNode(Node*);
 201     unsigned maxNodeCount() const { return m_nodes.size(); }
 202     Node* nodeAt(unsigned index) const { return m_nodes[index]; }
 203     void packNodeIndices();
 204 
 205     void dethread();
 206 
 207     FrozenValue* freeze(JSValue); // We use weak freezing by default.
 208     FrozenValue* freezeStrong(JSValue); // Shorthand for freeze(value)-&gt;strengthenTo(StrongValue).
 209 
 210     void convertToConstant(Node* node, FrozenValue* value);
 211     void convertToConstant(Node* node, JSValue value);
 212     void convertToStrongConstant(Node* node, JSValue value);
 213 
 214     RegisteredStructure registerStructure(Structure* structure)
 215     {
 216         StructureRegistrationResult ignored;
 217         return registerStructure(structure, ignored);
 218     }
 219     RegisteredStructure registerStructure(Structure*, StructureRegistrationResult&amp;);
 220     void registerAndWatchStructureTransition(Structure*);
 221     void assertIsRegistered(Structure* structure);
 222 
 223     // CodeBlock is optional, but may allow additional information to be dumped (e.g. Identifier names).
 224     void dump(PrintStream&amp; = WTF::dataFile(), DumpContext* = 0);
 225 
 226     bool terminalsAreValid();
 227 
 228     enum PhiNodeDumpMode { DumpLivePhisOnly, DumpAllPhis };
 229     void dumpBlockHeader(PrintStream&amp;, const char* prefix, BasicBlock*, PhiNodeDumpMode, DumpContext*);
 230     void dump(PrintStream&amp;, Edge);
 231     void dump(PrintStream&amp;, const char* prefix, Node*, DumpContext* = 0);
 232     static int amountOfNodeWhiteSpace(Node*);
 233     static void printNodeWhiteSpace(PrintStream&amp;, Node*);
 234 
 235     // Dump the code origin of the given node as a diff from the code origin of the
 236     // preceding node. Returns true if anything was printed.
 237     bool dumpCodeOrigin(PrintStream&amp;, const char* prefix, Node*&amp; previousNode, Node* currentNode, DumpContext*);
 238 
 239     AddSpeculationMode addSpeculationMode(Node* add, bool leftShouldSpeculateInt32, bool rightShouldSpeculateInt32, PredictionPass pass)
 240     {
 241         ASSERT(add-&gt;op() == ValueAdd || add-&gt;op() == ValueSub || add-&gt;op() == ArithAdd || add-&gt;op() == ArithSub);
 242 
 243         RareCaseProfilingSource source = add-&gt;sourceFor(pass);
 244 
 245         Node* left = add-&gt;child1().node();
 246         Node* right = add-&gt;child2().node();
 247 
 248         if (left-&gt;hasConstant())
 249             return addImmediateShouldSpeculateInt32(add, rightShouldSpeculateInt32, right, left, source);
 250         if (right-&gt;hasConstant())
 251             return addImmediateShouldSpeculateInt32(add, leftShouldSpeculateInt32, left, right, source);
 252 
 253         return (leftShouldSpeculateInt32 &amp;&amp; rightShouldSpeculateInt32 &amp;&amp; add-&gt;canSpeculateInt32(source)) ? SpeculateInt32 : DontSpeculateInt32;
 254     }
 255 
 256     AddSpeculationMode valueAddSpeculationMode(Node* add, PredictionPass pass)
 257     {
 258         return addSpeculationMode(
 259             add,
 260             add-&gt;child1()-&gt;shouldSpeculateInt32OrBooleanExpectingDefined(),
 261             add-&gt;child2()-&gt;shouldSpeculateInt32OrBooleanExpectingDefined(),
 262             pass);
 263     }
 264 
 265     AddSpeculationMode arithAddSpeculationMode(Node* add, PredictionPass pass)
 266     {
 267         return addSpeculationMode(
 268             add,
 269             add-&gt;child1()-&gt;shouldSpeculateInt32OrBooleanForArithmetic(),
 270             add-&gt;child2()-&gt;shouldSpeculateInt32OrBooleanForArithmetic(),
 271             pass);
 272     }
 273 
 274     AddSpeculationMode addSpeculationMode(Node* add, PredictionPass pass)
 275     {
 276         if (add-&gt;op() == ValueAdd)
 277             return valueAddSpeculationMode(add, pass);
 278 
 279         return arithAddSpeculationMode(add, pass);
 280     }
 281 
 282     bool addShouldSpeculateInt32(Node* add, PredictionPass pass)
 283     {
 284         return addSpeculationMode(add, pass) != DontSpeculateInt32;
 285     }
 286 
 287     bool addShouldSpeculateAnyInt(Node* add)
 288     {
 289         if (!enableInt52())
 290             return false;
 291 
 292         Node* left = add-&gt;child1().node();
 293         Node* right = add-&gt;child2().node();
 294 
 295         if (hasExitSite(add, Int52Overflow))
 296             return false;
 297 
 298         if (Node::shouldSpeculateAnyInt(left, right))
 299             return true;
 300 
 301         auto shouldSpeculateAnyIntForAdd = [](Node* node) {
 302             auto isAnyIntSpeculationForAdd = [](SpeculatedType value) {
 303                 return !!value &amp;&amp; (value &amp; (SpecAnyInt | SpecAnyIntAsDouble)) == value;
 304             };
 305 
 306             // When DoubleConstant node appears, it means that users explicitly write a constant in their code with double form instead of integer form (1.0 instead of 1).
 307             // In that case, we should honor this decision: using it as integer is not appropriate.
 308             if (node-&gt;op() == DoubleConstant)
 309                 return false;
 310             return isAnyIntSpeculationForAdd(node-&gt;prediction());
 311         };
 312 
 313         // Allow AnyInt ArithAdd only when the one side of the binary operation should be speculated AnyInt. It is a bit conservative
 314         // decision. This is because Double to Int52 conversion is not so cheap. Frequent back-and-forth conversions between Double and Int52
 315         // rather hurt the performance. If the one side of the operation is already Int52, the cost for constructing ArithAdd becomes
 316         // cheap since only one Double to Int52 conversion could be required.
 317         // This recovers some regression in assorted tests while keeping kraken crypto improvements.
 318         if (!left-&gt;shouldSpeculateAnyInt() &amp;&amp; !right-&gt;shouldSpeculateAnyInt())
 319             return false;
 320 
 321         auto usesAsNumbers = [](Node* node) {
 322             NodeFlags flags = node-&gt;flags() &amp; NodeBytecodeBackPropMask;
 323             if (!flags)
 324                 return false;
 325             return (flags &amp; (NodeBytecodeUsesAsNumber | NodeBytecodeNeedsNegZero | NodeBytecodeUsesAsInt | NodeBytecodeUsesAsArrayIndex)) == flags;
 326         };
 327 
 328         // Wrapping Int52 to Value is also not so cheap. Thus, we allow Int52 addition only when the node is used as number.
 329         if (!usesAsNumbers(add))
 330             return false;
 331 
 332         return shouldSpeculateAnyIntForAdd(left) &amp;&amp; shouldSpeculateAnyIntForAdd(right);
 333     }
 334 
 335     bool binaryArithShouldSpeculateInt32(Node* node, PredictionPass pass)
 336     {
 337         Node* left = node-&gt;child1().node();
 338         Node* right = node-&gt;child2().node();
 339 
 340         return Node::shouldSpeculateInt32OrBooleanForArithmetic(left, right)
 341             &amp;&amp; node-&gt;canSpeculateInt32(node-&gt;sourceFor(pass));
 342     }
 343 
 344     bool binaryArithShouldSpeculateAnyInt(Node* node, PredictionPass pass)
 345     {
 346         if (!enableInt52())
 347             return false;
 348 
 349         Node* left = node-&gt;child1().node();
 350         Node* right = node-&gt;child2().node();
 351 
 352         return Node::shouldSpeculateAnyInt(left, right)
 353             &amp;&amp; node-&gt;canSpeculateInt52(pass)
 354             &amp;&amp; !hasExitSite(node, Int52Overflow);
 355     }
 356 
 357     bool unaryArithShouldSpeculateInt32(Node* node, PredictionPass pass)
 358     {
 359         return node-&gt;child1()-&gt;shouldSpeculateInt32OrBooleanForArithmetic()
 360             &amp;&amp; node-&gt;canSpeculateInt32(pass);
 361     }
 362 
 363     bool unaryArithShouldSpeculateAnyInt(Node* node, PredictionPass pass)
 364     {
 365         if (!enableInt52())
 366             return false;
 367         return node-&gt;child1()-&gt;shouldSpeculateAnyInt()
 368             &amp;&amp; node-&gt;canSpeculateInt52(pass)
 369             &amp;&amp; !hasExitSite(node, Int52Overflow);
 370     }
 371 
 372     bool canOptimizeStringObjectAccess(const CodeOrigin&amp;);
 373 
 374     bool getRegExpPrototypeProperty(JSObject* regExpPrototype, Structure* regExpPrototypeStructure, UniquedStringImpl* uid, JSValue&amp; returnJSValue);
 375 
 376     bool roundShouldSpeculateInt32(Node* arithRound, PredictionPass pass)
 377     {
 378         ASSERT(arithRound-&gt;op() == ArithRound || arithRound-&gt;op() == ArithFloor || arithRound-&gt;op() == ArithCeil || arithRound-&gt;op() == ArithTrunc);
 379         return arithRound-&gt;canSpeculateInt32(pass) &amp;&amp; !hasExitSite(arithRound-&gt;origin.semantic, Overflow) &amp;&amp; !hasExitSite(arithRound-&gt;origin.semantic, NegativeZero);
 380     }
 381 
 382     static const char *opName(NodeType);
 383 
 384     RegisteredStructureSet* addStructureSet(const StructureSet&amp; structureSet)
 385     {
 386         m_structureSets.append();
 387         RegisteredStructureSet* result = &amp;m_structureSets.last();
 388 
 389         for (Structure* structure : structureSet)
 390             result-&gt;add(registerStructure(structure));
 391 
 392         return result;
 393     }
 394 
 395     RegisteredStructureSet* addStructureSet(const RegisteredStructureSet&amp; structureSet)
 396     {
 397         m_structureSets.append();
 398         RegisteredStructureSet* result = &amp;m_structureSets.last();
 399 
 400         for (RegisteredStructure structure : structureSet)
 401             result-&gt;add(structure);
 402 
 403         return result;
 404     }
 405 
 406     JSGlobalObject* globalObjectFor(CodeOrigin codeOrigin)
 407     {
 408         return m_codeBlock-&gt;globalObjectFor(codeOrigin);
 409     }
 410 
 411     JSObject* globalThisObjectFor(CodeOrigin codeOrigin)
 412     {
 413         JSGlobalObject* object = globalObjectFor(codeOrigin);
 414         return jsCast&lt;JSObject*&gt;(object-&gt;methodTable(m_vm)-&gt;toThis(object, object-&gt;globalExec(), NotStrictMode));
 415     }
 416 
 417     ScriptExecutable* executableFor(InlineCallFrame* inlineCallFrame)
 418     {
 419         if (!inlineCallFrame)
 420             return m_codeBlock-&gt;ownerExecutable();
 421 
 422         return inlineCallFrame-&gt;baselineCodeBlock-&gt;ownerExecutable();
 423     }
 424 
 425     ScriptExecutable* executableFor(const CodeOrigin&amp; codeOrigin)
 426     {
 427         return executableFor(codeOrigin.inlineCallFrame);
 428     }
 429 
 430     CodeBlock* baselineCodeBlockFor(InlineCallFrame* inlineCallFrame)
 431     {
 432         if (!inlineCallFrame)
 433             return m_profiledBlock;
 434         return baselineCodeBlockForInlineCallFrame(inlineCallFrame);
 435     }
 436 
 437     CodeBlock* baselineCodeBlockFor(const CodeOrigin&amp; codeOrigin)
 438     {
 439         return baselineCodeBlockForOriginAndBaselineCodeBlock(codeOrigin, m_profiledBlock);
 440     }
 441 
 442     bool isStrictModeFor(CodeOrigin codeOrigin)
 443     {
 444         if (!codeOrigin.inlineCallFrame)
 445             return m_codeBlock-&gt;isStrictMode();
 446         return codeOrigin.inlineCallFrame-&gt;isStrictMode();
 447     }
 448 
 449     ECMAMode ecmaModeFor(CodeOrigin codeOrigin)
 450     {
 451         return isStrictModeFor(codeOrigin) ? StrictMode : NotStrictMode;
 452     }
 453 
 454     bool masqueradesAsUndefinedWatchpointIsStillValid(const CodeOrigin&amp; codeOrigin)
 455     {
 456         return globalObjectFor(codeOrigin)-&gt;masqueradesAsUndefinedWatchpoint()-&gt;isStillValid();
 457     }
 458 
 459     bool hasGlobalExitSite(const CodeOrigin&amp; codeOrigin, ExitKind exitKind)
 460     {
 461         return baselineCodeBlockFor(codeOrigin)-&gt;unlinkedCodeBlock()-&gt;hasExitSite(FrequentExitSite(exitKind));
 462     }
 463 
 464     bool hasExitSite(const CodeOrigin&amp; codeOrigin, ExitKind exitKind)
 465     {
 466         return baselineCodeBlockFor(codeOrigin)-&gt;unlinkedCodeBlock()-&gt;hasExitSite(FrequentExitSite(codeOrigin.bytecodeIndex, exitKind));
 467     }
 468 
 469     bool hasExitSite(Node* node, ExitKind exitKind)
 470     {
 471         return hasExitSite(node-&gt;origin.semantic, exitKind);
 472     }
 473 
 474     MethodOfGettingAValueProfile methodOfGettingAValueProfileFor(Node* currentNode, Node* operandNode);
 475 
 476     BlockIndex numBlocks() const { return m_blocks.size(); }
 477     BasicBlock* block(BlockIndex blockIndex) const { return m_blocks[blockIndex].get(); }
 478     BasicBlock* lastBlock() const { return block(numBlocks() - 1); }
 479 
 480     void appendBlock(Ref&lt;BasicBlock&gt;&amp;&amp; basicBlock)
 481     {
 482         basicBlock-&gt;index = m_blocks.size();
 483         m_blocks.append(WTFMove(basicBlock));
 484     }
 485 
 486     void killBlock(BlockIndex blockIndex)
 487     {
 488         m_blocks[blockIndex] = nullptr;
 489     }
 490 
 491     void killBlock(BasicBlock* basicBlock)
 492     {
 493         killBlock(basicBlock-&gt;index);
 494     }
 495 
 496     void killBlockAndItsContents(BasicBlock*);
 497 
 498     void killUnreachableBlocks();
 499 
 500     void determineReachability();
 501     void resetReachability();
 502 
 503     void computeRefCounts();
 504 
 505     unsigned varArgNumChildren(Node* node)
 506     {
 507         ASSERT(node-&gt;flags() &amp; NodeHasVarArgs);
 508         return node-&gt;numChildren();
 509     }
 510 
 511     unsigned numChildren(Node* node)
 512     {
 513         if (node-&gt;flags() &amp; NodeHasVarArgs)
 514             return varArgNumChildren(node);
 515         return AdjacencyList::Size;
 516     }
 517 
 518     template &lt;typename Function = bool(*)(Edge)&gt;
 519     AdjacencyList copyVarargChildren(Node* node, Function filter = [] (Edge) { return true; })
 520     {
 521         ASSERT(node-&gt;flags() &amp; NodeHasVarArgs);
 522         unsigned firstChild = m_varArgChildren.size();
 523         unsigned numChildren = 0;
 524         doToChildren(node, [&amp;] (Edge edge) {
 525             if (filter(edge)) {
 526                 ++numChildren;
 527                 m_varArgChildren.append(edge);
 528             }
 529         });
 530 
 531         return AdjacencyList(AdjacencyList::Variable, firstChild, numChildren);
 532     }
 533 
 534     Edge&amp; varArgChild(Node* node, unsigned index)
 535     {
 536         ASSERT(node-&gt;flags() &amp; NodeHasVarArgs);
 537         return m_varArgChildren[node-&gt;firstChild() + index];
 538     }
 539 
 540     Edge&amp; child(Node* node, unsigned index)
 541     {
 542         if (node-&gt;flags() &amp; NodeHasVarArgs)
 543             return varArgChild(node, index);
 544         return node-&gt;children.child(index);
 545     }
 546 
 547     void voteNode(Node* node, unsigned ballot, float weight = 1)
 548     {
 549         switch (node-&gt;op()) {
 550         case ValueToInt32:
 551         case UInt32ToNumber:
 552             node = node-&gt;child1().node();
 553             break;
 554         default:
 555             break;
 556         }
 557 
 558         if (node-&gt;op() == GetLocal)
 559             node-&gt;variableAccessData()-&gt;vote(ballot, weight);
 560     }
 561 
 562     void voteNode(Edge edge, unsigned ballot, float weight = 1)
 563     {
 564         voteNode(edge.node(), ballot, weight);
 565     }
 566 
 567     void voteChildren(Node* node, unsigned ballot, float weight = 1)
 568     {
 569         if (node-&gt;flags() &amp; NodeHasVarArgs) {
 570             for (unsigned childIdx = node-&gt;firstChild();
 571                 childIdx &lt; node-&gt;firstChild() + node-&gt;numChildren();
 572                 childIdx++) {
 573                 if (!!m_varArgChildren[childIdx])
 574                     voteNode(m_varArgChildren[childIdx], ballot, weight);
 575             }
 576             return;
 577         }
 578 
 579         if (!node-&gt;child1())
 580             return;
 581         voteNode(node-&gt;child1(), ballot, weight);
 582         if (!node-&gt;child2())
 583             return;
 584         voteNode(node-&gt;child2(), ballot, weight);
 585         if (!node-&gt;child3())
 586             return;
 587         voteNode(node-&gt;child3(), ballot, weight);
 588     }
 589 
 590     template&lt;typename T&gt; // T = Node* or Edge
 591     void substitute(BasicBlock&amp; block, unsigned startIndexInBlock, T oldThing, T newThing)
 592     {
 593         for (unsigned indexInBlock = startIndexInBlock; indexInBlock &lt; block.size(); ++indexInBlock) {
 594             Node* node = block[indexInBlock];
 595             if (node-&gt;flags() &amp; NodeHasVarArgs) {
 596                 for (unsigned childIdx = node-&gt;firstChild(); childIdx &lt; node-&gt;firstChild() + node-&gt;numChildren(); ++childIdx) {
 597                     if (!!m_varArgChildren[childIdx])
 598                         compareAndSwap(m_varArgChildren[childIdx], oldThing, newThing);
 599                 }
 600                 continue;
 601             }
 602             if (!node-&gt;child1())
 603                 continue;
 604             compareAndSwap(node-&gt;children.child1(), oldThing, newThing);
 605             if (!node-&gt;child2())
 606                 continue;
 607             compareAndSwap(node-&gt;children.child2(), oldThing, newThing);
 608             if (!node-&gt;child3())
 609                 continue;
 610             compareAndSwap(node-&gt;children.child3(), oldThing, newThing);
 611         }
 612     }
 613 
 614     // Use this if you introduce a new GetLocal and you know that you introduced it *before*
 615     // any GetLocals in the basic block.
 616     // FIXME: it may be appropriate, in the future, to generalize this to handle GetLocals
 617     // introduced anywhere in the basic block.
 618     void substituteGetLocal(BasicBlock&amp; block, unsigned startIndexInBlock, VariableAccessData* variableAccessData, Node* newGetLocal);
 619 
 620     void invalidateCFG();
 621     void invalidateNodeLiveness();
 622 
 623     void clearFlagsOnAllNodes(NodeFlags);
 624 
 625     void clearReplacements();
 626     void clearEpochs();
 627     void initializeNodeOwners();
 628 
 629     BlockList blocksInPreOrder();
 630     BlockList blocksInPostOrder(bool isSafeToValidate = true);
 631 
 632     class NaturalBlockIterable {
 633     public:
 634         NaturalBlockIterable()
 635             : m_graph(nullptr)
 636         {
 637         }
 638 
 639         NaturalBlockIterable(Graph&amp; graph)
 640             : m_graph(&amp;graph)
 641         {
 642         }
 643 
 644         class iterator {
 645         public:
 646             iterator()
 647                 : m_graph(nullptr)
 648                 , m_index(0)
 649             {
 650             }
 651 
 652             iterator(Graph&amp; graph, BlockIndex index)
 653                 : m_graph(&amp;graph)
 654                 , m_index(findNext(index))
 655             {
 656             }
 657 
 658             BasicBlock *operator*()
 659             {
 660                 return m_graph-&gt;block(m_index);
 661             }
 662 
 663             iterator&amp; operator++()
 664             {
 665                 m_index = findNext(m_index + 1);
 666                 return *this;
 667             }
 668 
 669             bool operator==(const iterator&amp; other) const
 670             {
 671                 return m_index == other.m_index;
 672             }
 673 
 674             bool operator!=(const iterator&amp; other) const
 675             {
 676                 return !(*this == other);
 677             }
 678 
 679         private:
 680             BlockIndex findNext(BlockIndex index)
 681             {
 682                 while (index &lt; m_graph-&gt;numBlocks() &amp;&amp; !m_graph-&gt;block(index))
 683                     index++;
 684                 return index;
 685             }
 686 
 687             Graph* m_graph;
 688             BlockIndex m_index;
 689         };
 690 
 691         iterator begin()
 692         {
 693             return iterator(*m_graph, 0);
 694         }
 695 
 696         iterator end()
 697         {
 698             return iterator(*m_graph, m_graph-&gt;numBlocks());
 699         }
 700 
 701     private:
 702         Graph* m_graph;
 703     };
 704 
 705     NaturalBlockIterable blocksInNaturalOrder()
 706     {
 707         return NaturalBlockIterable(*this);
 708     }
 709 
 710     template&lt;typename ChildFunctor&gt;
 711     ALWAYS_INLINE void doToChildrenWithNode(Node* node, const ChildFunctor&amp; functor)
 712     {
 713         DFG_NODE_DO_TO_CHILDREN(*this, node, functor);
 714     }
 715 
 716     template&lt;typename ChildFunctor&gt;
 717     ALWAYS_INLINE void doToChildren(Node* node, const ChildFunctor&amp; functor)
 718     {
 719         class ForwardingFunc {
 720         public:
 721             ForwardingFunc(const ChildFunctor&amp; functor)
 722                 : m_functor(functor)
 723             {
 724             }
 725 
 726             // This is a manually written func because we want ALWAYS_INLINE.
 727             ALWAYS_INLINE void operator()(Node*, Edge&amp; edge) const
 728             {
 729                 m_functor(edge);
 730             }
 731 
 732         private:
 733             const ChildFunctor&amp; m_functor;
 734         };
 735 
 736         doToChildrenWithNode(node, ForwardingFunc(functor));
 737     }
 738 
 739     bool uses(Node* node, Node* child)
 740     {
 741         bool result = false;
 742         doToChildren(node, [&amp;] (Edge edge) { result |= edge == child; });
 743         return result;
 744     }
 745 
 746     bool isWatchingHavingABadTimeWatchpoint(Node* node)
 747     {
 748         JSGlobalObject* globalObject = globalObjectFor(node-&gt;origin.semantic);
 749         return watchpoints().isWatched(globalObject-&gt;havingABadTimeWatchpoint());
 750     }
 751 
 752     bool isWatchingGlobalObjectWatchpoint(JSGlobalObject* globalObject, InlineWatchpointSet&amp; set)
 753     {
 754         if (watchpoints().isWatched(set))
 755             return true;
 756 
 757         if (set.isStillValid()) {
 758             // Since the global object owns this watchpoint, we make ourselves have a weak pointer to it.
 759             // If the global object got deallocated, it wouldn&#39;t fire the watchpoint. It&#39;s unlikely the
 760             // global object would get deallocated without this code ever getting thrown away, however,
 761             // it&#39;s more sound logically to depend on the global object lifetime weakly.
 762             freeze(globalObject);
 763             watchpoints().addLazily(set);
 764             return true;
 765         }
 766 
 767         return false;
 768     }
 769 
 770     bool isWatchingArrayIteratorProtocolWatchpoint(Node* node)
 771     {
 772         JSGlobalObject* globalObject = globalObjectFor(node-&gt;origin.semantic);
 773         InlineWatchpointSet&amp; set = globalObject-&gt;arrayIteratorProtocolWatchpoint();
 774         return isWatchingGlobalObjectWatchpoint(globalObject, set);
 775     }
 776 
 777     bool isWatchingNumberToStringWatchpoint(Node* node)
 778     {
 779         JSGlobalObject* globalObject = globalObjectFor(node-&gt;origin.semantic);
 780         InlineWatchpointSet&amp; set = globalObject-&gt;numberToStringWatchpoint();
 781         return isWatchingGlobalObjectWatchpoint(globalObject, set);
 782     }
 783 
 784     Profiler::Compilation* compilation() { return m_plan.compilation(); }
 785 
 786     DesiredIdentifiers&amp; identifiers() { return m_plan.identifiers(); }
 787     DesiredWatchpoints&amp; watchpoints() { return m_plan.watchpoints(); }
 788     DesiredGlobalProperties&amp; globalProperties() { return m_plan.globalProperties(); }
 789 
 790     // Returns false if the key is already invalid or unwatchable. If this is a Presence condition,
 791     // this also makes it cheap to query if the condition holds. Also makes sure that the GC knows
 792     // what&#39;s going on.
 793     bool watchCondition(const ObjectPropertyCondition&amp;);
 794     bool watchConditions(const ObjectPropertyConditionSet&amp;);
 795 
 796     bool watchGlobalProperty(JSGlobalObject*, unsigned identifierNumber);
 797 
 798     // Checks if it&#39;s known that loading from the given object at the given offset is fine. This is
 799     // computed by tracking which conditions we track with watchCondition().
 800     bool isSafeToLoad(JSObject* base, PropertyOffset);
 801 
 802     // This uses either constant property inference or property type inference to derive a good abstract
 803     // value for some property accessed with the given abstract value base.
 804     AbstractValue inferredValueForProperty(
 805         const AbstractValue&amp; base, PropertyOffset, StructureClobberState);
 806 
 807     FullBytecodeLiveness&amp; livenessFor(CodeBlock*);
 808     FullBytecodeLiveness&amp; livenessFor(InlineCallFrame*);
 809 
 810     // Quickly query if a single local is live at the given point. This is faster than calling
 811     // forAllLiveInBytecode() if you will only query one local. But, if you want to know all of the
 812     // locals live, then calling this for each local is much slower than forAllLiveInBytecode().
 813     bool isLiveInBytecode(VirtualRegister, CodeOrigin);
 814 
 815     // Quickly get all of the non-argument locals live at the given point. This doesn&#39;t give you
 816     // any arguments because those are all presumed live. You can call forAllLiveInBytecode() to
 817     // also get the arguments. This is much faster than calling isLiveInBytecode() for each local.
 818     template&lt;typename Functor&gt;
 819     void forAllLocalsLiveInBytecode(CodeOrigin codeOrigin, const Functor&amp; functor)
 820     {
 821         // Support for not redundantly reporting arguments. Necessary because in case of a varargs
 822         // call, only the callee knows that arguments are live while in the case of a non-varargs
 823         // call, both callee and caller will see the variables live.
 824         VirtualRegister exclusionStart;
 825         VirtualRegister exclusionEnd;
 826 
 827         CodeOrigin* codeOriginPtr = &amp;codeOrigin;
 828 
 829         for (;;) {
 830             InlineCallFrame* inlineCallFrame = codeOriginPtr-&gt;inlineCallFrame;
 831             VirtualRegister stackOffset(inlineCallFrame ? inlineCallFrame-&gt;stackOffset : 0);
 832 
 833             if (inlineCallFrame) {
 834                 if (inlineCallFrame-&gt;isClosureCall)
 835                     functor(stackOffset + CallFrameSlot::callee);
 836                 if (inlineCallFrame-&gt;isVarargs())
 837                     functor(stackOffset + CallFrameSlot::argumentCount);
 838             }
 839 
 840             CodeBlock* codeBlock = baselineCodeBlockFor(inlineCallFrame);
 841             FullBytecodeLiveness&amp; fullLiveness = livenessFor(codeBlock);
 842             const FastBitVector&amp; liveness = fullLiveness.getLiveness(codeOriginPtr-&gt;bytecodeIndex);
 843             for (unsigned relativeLocal = codeBlock-&gt;numCalleeLocals(); relativeLocal--;) {
 844                 VirtualRegister reg = stackOffset + virtualRegisterForLocal(relativeLocal);
 845 
 846                 // Don&#39;t report if our callee already reported.
 847                 if (reg &gt;= exclusionStart &amp;&amp; reg &lt; exclusionEnd)
 848                     continue;
 849 
 850                 if (liveness[relativeLocal])
 851                     functor(reg);
 852             }
 853 
 854             if (!inlineCallFrame)
 855                 break;
 856 
 857             // Arguments are always live. This would be redundant if it wasn&#39;t for our
 858             // op_call_varargs inlining. See the comment above.
 859             exclusionStart = stackOffset + CallFrame::argumentOffsetIncludingThis(0);
 860             exclusionEnd = stackOffset + CallFrame::argumentOffsetIncludingThis(inlineCallFrame-&gt;argumentsWithFixup.size());
 861 
 862             // We will always have a &quot;this&quot; argument and exclusionStart should be a smaller stack
 863             // offset than exclusionEnd.
 864             ASSERT(exclusionStart &lt; exclusionEnd);
 865 
 866             for (VirtualRegister reg = exclusionStart; reg &lt; exclusionEnd; reg += 1)
 867                 functor(reg);
 868 
 869             codeOriginPtr = inlineCallFrame-&gt;getCallerSkippingTailCalls();
 870 
 871             // The first inline call frame could be an inline tail call
 872             if (!codeOriginPtr)
 873                 break;
 874         }
 875     }
 876 
 877     // Get a BitVector of all of the non-argument locals live right now. This is mostly useful if
 878     // you want to compare two sets of live locals from two different CodeOrigins.
 879     BitVector localsLiveInBytecode(CodeOrigin);
 880 
 881     // Tells you all of the arguments and locals live at the given CodeOrigin. This is a small
 882     // extension to forAllLocalsLiveInBytecode(), since all arguments are always presumed live.
 883     template&lt;typename Functor&gt;
 884     void forAllLiveInBytecode(CodeOrigin codeOrigin, const Functor&amp; functor)
 885     {
 886         forAllLocalsLiveInBytecode(codeOrigin, functor);
 887 
 888         // Report all arguments as being live.
 889         for (unsigned argument = block(0)-&gt;variablesAtHead.numberOfArguments(); argument--;)
 890             functor(virtualRegisterForArgument(argument));
 891     }
 892 
 893     BytecodeKills&amp; killsFor(CodeBlock*);
 894     BytecodeKills&amp; killsFor(InlineCallFrame*);
 895 
 896     static unsigned parameterSlotsForArgCount(unsigned);
 897 
 898     unsigned frameRegisterCount();
 899     unsigned stackPointerOffset();
 900     unsigned requiredRegisterCountForExit();
 901     unsigned requiredRegisterCountForExecutionAndExit();
 902 
 903     JSValue tryGetConstantProperty(JSValue base, const RegisteredStructureSet&amp;, PropertyOffset);
 904     JSValue tryGetConstantProperty(JSValue base, Structure*, PropertyOffset);
 905     JSValue tryGetConstantProperty(JSValue base, const StructureAbstractValue&amp;, PropertyOffset);
 906     JSValue tryGetConstantProperty(const AbstractValue&amp;, PropertyOffset);
 907 
 908     JSValue tryGetConstantClosureVar(JSValue base, ScopeOffset);
 909     JSValue tryGetConstantClosureVar(const AbstractValue&amp;, ScopeOffset);
 910     JSValue tryGetConstantClosureVar(Node*, ScopeOffset);
 911 
 912     JSArrayBufferView* tryGetFoldableView(JSValue);
 913     JSArrayBufferView* tryGetFoldableView(JSValue, ArrayMode arrayMode);
 914 
 915     bool canDoFastSpread(Node*, const AbstractValue&amp;);
 916 
 917     void registerFrozenValues();
 918 
 919     void visitChildren(SlotVisitor&amp;) override;
 920 
 921     void logAssertionFailure(
 922         std::nullptr_t, const char* file, int line, const char* function,
 923         const char* assertion);
 924     void logAssertionFailure(
 925         Node*, const char* file, int line, const char* function,
 926         const char* assertion);
 927     void logAssertionFailure(
 928         BasicBlock*, const char* file, int line, const char* function,
 929         const char* assertion);
 930 
 931     bool hasDebuggerEnabled() const { return m_hasDebuggerEnabled; }
 932 
 933     CPSDominators&amp; ensureCPSDominators();
 934     SSADominators&amp; ensureSSADominators();
 935     CPSNaturalLoops&amp; ensureCPSNaturalLoops();
 936     SSANaturalLoops&amp; ensureSSANaturalLoops();
 937     BackwardsCFG&amp; ensureBackwardsCFG();
 938     BackwardsDominators&amp; ensureBackwardsDominators();
 939     ControlEquivalenceAnalysis&amp; ensureControlEquivalenceAnalysis();
 940     CPSCFG&amp; ensureCPSCFG();
 941 
 942     // These functions only makes sense to call after bytecode parsing
 943     // because it queries the m_hasExceptionHandlers boolean whose value
 944     // is only fully determined after bytcode parsing.
 945     bool willCatchExceptionInMachineFrame(CodeOrigin codeOrigin)
 946     {
 947         CodeOrigin ignored;
 948         HandlerInfo* ignored2;
 949         return willCatchExceptionInMachineFrame(codeOrigin, ignored, ignored2);
 950     }
 951     bool willCatchExceptionInMachineFrame(CodeOrigin, CodeOrigin&amp; opCatchOriginOut, HandlerInfo*&amp; catchHandlerOut);
 952 
 953     bool needsScopeRegister() const { return m_hasDebuggerEnabled || m_codeBlock-&gt;usesEval(); }
 954     bool needsFlushedThis() const { return m_codeBlock-&gt;usesEval(); }
 955 
 956     void clearCPSCFGData();
 957 
 958     bool isRoot(BasicBlock* block) const
 959     {
 960         ASSERT_WITH_MESSAGE(!m_isInSSAConversion, &quot;This is not written to work during SSA conversion.&quot;);
 961 
 962         if (m_form == SSA) {
 963             ASSERT(m_roots.size() == 1);
 964             ASSERT(m_roots.contains(this-&gt;block(0)));
 965             return block == this-&gt;block(0);
 966         }
 967 
 968         if (m_roots.size() &lt;= 4) {
 969             bool result = m_roots.contains(block);
 970             ASSERT(result == m_rootToArguments.contains(block));
 971             return result;
 972         }
 973         bool result = m_rootToArguments.contains(block);
 974         ASSERT(result == m_roots.contains(block));
 975         return result;
 976     }
 977 
 978     VM&amp; m_vm;
 979     Plan&amp; m_plan;
 980     CodeBlock* m_codeBlock;
 981     CodeBlock* m_profiledBlock;
 982 
 983     Vector&lt;RefPtr&lt;BasicBlock&gt;, 8&gt; m_blocks;
 984     Vector&lt;BasicBlock*, 1&gt; m_roots;
 985     Vector&lt;Edge, 16&gt; m_varArgChildren;
 986 
 987     HashMap&lt;EncodedJSValue, FrozenValue*, EncodedJSValueHash, EncodedJSValueHashTraits&gt; m_frozenValueMap;
 988     Bag&lt;FrozenValue&gt; m_frozenValues;
 989 
 990     Vector&lt;uint32_t&gt; m_uint32ValuesInUse;
 991 
 992     Bag&lt;StorageAccessData&gt; m_storageAccessData;
 993 
 994     // In CPS, this is all of the SetArgument nodes for the arguments in the machine code block
 995     // that survived DCE. All of them except maybe &quot;this&quot; will survive DCE, because of the Flush
 996     // nodes. In SSA, this has no meaning. It&#39;s empty.
 997     HashMap&lt;BasicBlock*, ArgumentsVector&gt; m_rootToArguments;
 998 
 999     // In SSA, this is the argument speculation that we&#39;ve locked in for an entrypoint block.
1000     //
1001     // We must speculate on the argument types at each entrypoint even if operations involving
1002     // arguments get killed. For example:
1003     //
1004     //     function foo(x) {
1005     //        var tmp = x + 1;
1006     //     }
1007     //
1008     // Assume that x is always int during profiling. The ArithAdd for &quot;x + 1&quot; will be dead and will
1009     // have a proven check for the edge to &quot;x&quot;. So, we will not insert a Check node and we will
1010     // kill the GetStack for &quot;x&quot;. But, we must do the int check in the progolue, because that&#39;s the
1011     // thing we used to allow DCE of ArithAdd. Otherwise the add could be impure:
1012     //
1013     //     var o = {
1014     //         valueOf: function() { do side effects; }
1015     //     };
1016     //     foo(o);
1017     //
1018     // If we DCE the ArithAdd and we remove the int check on x, then this won&#39;t do the side
1019     // effects.
1020     //
1021     // By convention, entrypoint index 0 is used for the CodeBlock&#39;s op_enter entrypoint.
1022     // So argumentFormats[0] are the argument formats for the normal call entrypoint.
1023     Vector&lt;Vector&lt;FlushFormat&gt;&gt; m_argumentFormats;
1024 
1025     // This maps an entrypoint index to a particular op_catch bytecode offset. By convention,
1026     // it&#39;ll never have zero as a key because we use zero to mean the op_enter entrypoint.
1027     HashMap&lt;unsigned, unsigned&gt; m_entrypointIndexToCatchBytecodeOffset;
1028 
1029     // This is the number of logical entrypoints that we&#39;re compiling. This is only used
1030     // in SSA. Each EntrySwitch node must have m_numberOfEntrypoints cases. Note, this is
1031     // not the same as m_roots.size(). m_roots.size() represents the number of roots in
1032     // the CFG. In SSA, m_roots.size() == 1 even if we&#39;re compiling more than one entrypoint.
1033     unsigned m_numberOfEntrypoints { UINT_MAX };
1034 
1035     SegmentedVector&lt;VariableAccessData, 16&gt; m_variableAccessData;
1036     SegmentedVector&lt;ArgumentPosition, 8&gt; m_argumentPositions;
1037     Bag&lt;Transition&gt; m_transitions;
1038     Bag&lt;BranchData&gt; m_branchData;
1039     Bag&lt;SwitchData&gt; m_switchData;
1040     Bag&lt;MultiGetByOffsetData&gt; m_multiGetByOffsetData;
1041     Bag&lt;MultiPutByOffsetData&gt; m_multiPutByOffsetData;
1042     Bag&lt;MatchStructureData&gt; m_matchStructureData;
1043     Bag&lt;ObjectMaterializationData&gt; m_objectMaterializationData;
1044     Bag&lt;CallVarargsData&gt; m_callVarargsData;
1045     Bag&lt;LoadVarargsData&gt; m_loadVarargsData;
1046     Bag&lt;StackAccessData&gt; m_stackAccessData;
1047     Bag&lt;LazyJSValue&gt; m_lazyJSValues;
1048     Bag&lt;CallDOMGetterData&gt; m_callDOMGetterData;
1049     Bag&lt;BitVector&gt; m_bitVectors;
1050     Vector&lt;InlineVariableData, 4&gt; m_inlineVariableData;
1051     HashMap&lt;CodeBlock*, std::unique_ptr&lt;FullBytecodeLiveness&gt;&gt; m_bytecodeLiveness;
1052     HashMap&lt;CodeBlock*, std::unique_ptr&lt;BytecodeKills&gt;&gt; m_bytecodeKills;
1053     HashSet&lt;std::pair&lt;JSObject*, PropertyOffset&gt;&gt; m_safeToLoad;
1054     Vector&lt;Ref&lt;Snippet&gt;&gt; m_domJITSnippets;
1055     std::unique_ptr&lt;CPSDominators&gt; m_cpsDominators;
1056     std::unique_ptr&lt;SSADominators&gt; m_ssaDominators;
1057     std::unique_ptr&lt;CPSNaturalLoops&gt; m_cpsNaturalLoops;
1058     std::unique_ptr&lt;SSANaturalLoops&gt; m_ssaNaturalLoops;
1059     std::unique_ptr&lt;SSACFG&gt; m_ssaCFG;
1060     std::unique_ptr&lt;CPSCFG&gt; m_cpsCFG;
1061     std::unique_ptr&lt;BackwardsCFG&gt; m_backwardsCFG;
1062     std::unique_ptr&lt;BackwardsDominators&gt; m_backwardsDominators;
1063     std::unique_ptr&lt;ControlEquivalenceAnalysis&gt; m_controlEquivalenceAnalysis;
1064     unsigned m_localVars;
1065     unsigned m_nextMachineLocal;
1066     unsigned m_parameterSlots;
1067 
1068     HashSet&lt;String&gt; m_localStrings;
1069     HashMap&lt;const StringImpl*, String&gt; m_copiedStrings;
1070 
1071 #if USE(JSVALUE32_64)
1072     StdUnorderedMap&lt;int64_t, double*&gt; m_doubleConstantsMap;
1073     std::unique_ptr&lt;Bag&lt;double&gt;&gt; m_doubleConstants;
1074 #endif
1075 
1076     OptimizationFixpointState m_fixpointState;
1077     StructureRegistrationState m_structureRegistrationState;
1078     GraphForm m_form;
1079     UnificationState m_unificationState;
1080     PlanStage m_planStage { PlanStage::Initial };
1081     RefCountState m_refCountState;
1082     bool m_hasDebuggerEnabled;
1083     bool m_hasExceptionHandlers { false };
1084     bool m_isInSSAConversion { false };
1085     Optional&lt;uint32_t&gt; m_maxLocalsForCatchOSREntry;
1086     std::unique_ptr&lt;FlowIndexing&gt; m_indexingCache;
1087     std::unique_ptr&lt;FlowMap&lt;AbstractValue&gt;&gt; m_abstractValuesCache;
1088     Bag&lt;EntrySwitchData&gt; m_entrySwitchData;
1089 
1090     RegisteredStructure stringStructure;
1091     RegisteredStructure symbolStructure;
1092 
1093 private:
1094     bool isStringPrototypeMethodSane(JSGlobalObject*, UniquedStringImpl*);
1095 
1096     void handleSuccessor(Vector&lt;BasicBlock*, 16&gt;&amp; worklist, BasicBlock*, BasicBlock* successor);
1097 
1098     AddSpeculationMode addImmediateShouldSpeculateInt32(Node* add, bool variableShouldSpeculateInt32, Node* operand, Node*immediate, RareCaseProfilingSource source)
1099     {
1100         ASSERT(immediate-&gt;hasConstant());
1101 
1102         JSValue immediateValue = immediate-&gt;asJSValue();
1103         if (!immediateValue.isNumber() &amp;&amp; !immediateValue.isBoolean())
1104             return DontSpeculateInt32;
1105 
1106         if (!variableShouldSpeculateInt32)
1107             return DontSpeculateInt32;
1108 
1109         // Integer constants can be typed Double if they are written like a double in the source code (e.g. 42.0).
1110         // In that case, we stay conservative unless the other operand was explicitly typed as integer.
1111         NodeFlags operandResultType = operand-&gt;result();
1112         if (operandResultType != NodeResultInt32 &amp;&amp; immediateValue.isDouble())
1113             return DontSpeculateInt32;
1114 
1115         if (immediateValue.isBoolean() || jsNumber(immediateValue.asNumber()).isInt32())
1116             return add-&gt;canSpeculateInt32(source) ? SpeculateInt32 : DontSpeculateInt32;
1117 
1118         double doubleImmediate = immediateValue.asDouble();
1119         const double twoToThe48 = 281474976710656.0;
1120         if (doubleImmediate &lt; -twoToThe48 || doubleImmediate &gt; twoToThe48)
1121             return DontSpeculateInt32;
1122 
1123         return bytecodeCanTruncateInteger(add-&gt;arithNodeFlags()) ? SpeculateInt32AndTruncateConstants : DontSpeculateInt32;
1124     }
1125 
1126     B3::SparseCollection&lt;Node&gt; m_nodes;
1127     SegmentedVector&lt;RegisteredStructureSet, 16&gt; m_structureSets;
1128 };
1129 
1130 } } // namespace JSC::DFG
1131 
1132 #endif
    </pre>
  </body>
</html>