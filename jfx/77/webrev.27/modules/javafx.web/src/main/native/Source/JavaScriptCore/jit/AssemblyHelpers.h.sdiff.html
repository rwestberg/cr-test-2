<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/AssemblyHelpers.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="AssemblyHelpers.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="BinarySwitch.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/AssemblyHelpers.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  40 #include &quot;RegisterAtOffsetList.h&quot;
  41 #include &quot;RegisterSet.h&quot;
  42 #include &quot;StackAlignment.h&quot;
  43 #include &quot;TagRegistersMode.h&quot;
  44 #include &quot;TypeofType.h&quot;
  45 #include &quot;VM.h&quot;
  46 
  47 namespace JSC {
  48 
  49 typedef void (*V_DebugOperation_EPP)(ExecState*, void*, void*);
  50 
  51 class AssemblyHelpers : public MacroAssembler {
  52 public:
  53     AssemblyHelpers(CodeBlock* codeBlock)
  54         : m_codeBlock(codeBlock)
  55         , m_baselineCodeBlock(codeBlock ? codeBlock-&gt;baselineAlternative() : 0)
  56     {
  57         if (m_codeBlock) {
  58             ASSERT(m_baselineCodeBlock);
  59             ASSERT(!m_baselineCodeBlock-&gt;alternative());
<span class="line-modified">  60             ASSERT(m_baselineCodeBlock-&gt;jitType() == JITCode::None || JITCode::isBaselineCode(m_baselineCodeBlock-&gt;jitType()));</span>
  61         }
  62     }
  63 
  64     CodeBlock* codeBlock() { return m_codeBlock; }
<span class="line-modified">  65     VM&amp; vm() { return *m_codeBlock-&gt;vm(); }</span>
  66     AssemblerType_T&amp; assembler() { return m_assembler; }
  67 
  68     void checkStackPointerAlignment()
  69     {
  70         // This check is both unneeded and harder to write correctly for ARM64
  71 #if !defined(NDEBUG) &amp;&amp; !CPU(ARM64)
  72         Jump stackPointerAligned = branchTestPtr(Zero, stackPointerRegister, TrustedImm32(0xf));
  73         abortWithReason(AHStackPointerMisaligned);
  74         stackPointerAligned.link(this);
  75 #endif
  76     }
  77 
  78     template&lt;typename T&gt;
  79     void storeCell(T cell, Address address)
  80     {
  81 #if USE(JSVALUE64)
  82         store64(cell, address);
  83 #else
  84         store32(cell, address.withOffset(PayloadOffset));
  85         store32(TrustedImm32(JSValue::CellTag), address.withOffset(TagOffset));
</pre>
<hr />
<pre>
 465                 storePtr(TrustedImm32(0), Address(currentTop, -8 - offset));
 466         } else {
 467             constexpr unsigned storeBytesPerIteration = stackAlignmentBytes();
 468             constexpr unsigned storesPerIteration = storeBytesPerIteration / sizeof(CPURegister);
 469 
 470             move(currentTop, temp);
 471             Label zeroLoop = label();
 472             subPtr(TrustedImm32(storeBytesPerIteration), temp);
 473 #if CPU(ARM64)
 474             static_assert(storesPerIteration == 2, &quot;clearStackFrame() for ARM64 assumes stack is 16 byte aligned&quot;);
 475             storePair64(ARM64Registers::zr, ARM64Registers::zr, temp);
 476 #else
 477             for (unsigned i = storesPerIteration; i-- != 0;)
 478                 storePtr(TrustedImm32(0), Address(temp, sizeof(CPURegister) * i));
 479 #endif
 480             branchPtr(NotEqual, temp, newTop).linkTo(zeroLoop, this);
 481         }
 482     }
 483 
 484 #if CPU(X86_64) || CPU(X86)
<span class="line-modified"> 485     static size_t prologueStackPointerDelta()</span>
 486     {
 487         // Prologue only saves the framePointerRegister
 488         return sizeof(void*);
 489     }
 490 
 491     void emitFunctionPrologue()
 492     {
 493         push(framePointerRegister);
 494         move(stackPointerRegister, framePointerRegister);
 495     }
 496 
 497     void emitFunctionEpilogueWithEmptyFrame()
 498     {
 499         pop(framePointerRegister);
 500     }
 501 
 502     void emitFunctionEpilogue()
 503     {
 504         move(framePointerRegister, stackPointerRegister);
 505         pop(framePointerRegister);
 506     }
 507 
 508     void preserveReturnAddressAfterCall(GPRReg reg)
 509     {
 510         pop(reg);
 511     }
 512 
 513     void restoreReturnAddressBeforeReturn(GPRReg reg)
 514     {
 515         push(reg);
 516     }
 517 
 518     void restoreReturnAddressBeforeReturn(Address address)
 519     {
 520         push(address);
 521     }
 522 #endif // CPU(X86_64) || CPU(X86)
 523 
 524 #if CPU(ARM_THUMB2) || CPU(ARM64)
<span class="line-modified"> 525     static size_t prologueStackPointerDelta()</span>
 526     {
 527         // Prologue saves the framePointerRegister and linkRegister
 528         return 2 * sizeof(void*);
 529     }
 530 
 531     void emitFunctionPrologue()
 532     {
 533         tagReturnAddress();
 534         pushPair(framePointerRegister, linkRegister);
 535         move(stackPointerRegister, framePointerRegister);
 536     }
 537 
 538     void emitFunctionEpilogueWithEmptyFrame()
 539     {
 540         popPair(framePointerRegister, linkRegister);
 541     }
 542 
 543     void emitFunctionEpilogue()
 544     {
 545         move(framePointerRegister, stackPointerRegister);
 546         emitFunctionEpilogueWithEmptyFrame();
 547     }
 548 
 549     ALWAYS_INLINE void preserveReturnAddressAfterCall(RegisterID reg)
 550     {
 551         move(linkRegister, reg);
 552     }
 553 
 554     ALWAYS_INLINE void restoreReturnAddressBeforeReturn(RegisterID reg)
 555     {
 556         move(reg, linkRegister);
 557     }
 558 
 559     ALWAYS_INLINE void restoreReturnAddressBeforeReturn(Address address)
 560     {
 561         loadPtr(address, linkRegister);
 562     }
 563 #endif
 564 
 565 #if CPU(MIPS)
<span class="line-modified"> 566     static size_t prologueStackPointerDelta()</span>
 567     {
 568         // Prologue saves the framePointerRegister and returnAddressRegister
 569         return 2 * sizeof(void*);
 570     }
 571 
 572     void emitFunctionPrologue()
 573     {
 574         pushPair(framePointerRegister, returnAddressRegister);
 575         move(stackPointerRegister, framePointerRegister);
 576     }
 577 
 578     void emitFunctionEpilogueWithEmptyFrame()
 579     {
 580         popPair(framePointerRegister, returnAddressRegister);
 581     }
 582 
 583     void emitFunctionEpilogue()
 584     {
 585         move(framePointerRegister, stackPointerRegister);
 586         emitFunctionEpilogueWithEmptyFrame();
</pre>
<hr />
<pre>
1415         jit.add64(TrustedImm32(increment), AbsoluteAddress(counter.addressOfCounter()));
1416     }
1417     void emitCount(AbstractSamplingCounter&amp; counter, int32_t increment = 1)
1418     {
1419         add64(TrustedImm32(increment), AbsoluteAddress(counter.addressOfCounter()));
1420     }
1421 #endif
1422 
1423 #if ENABLE(SAMPLING_FLAGS)
1424     void setSamplingFlag(int32_t);
1425     void clearSamplingFlag(int32_t flag);
1426 #endif
1427 
1428     JSGlobalObject* globalObjectFor(CodeOrigin codeOrigin)
1429     {
1430         return codeBlock()-&gt;globalObjectFor(codeOrigin);
1431     }
1432 
1433     bool isStrictModeFor(CodeOrigin codeOrigin)
1434     {
<span class="line-modified">1435         if (!codeOrigin.inlineCallFrame)</span>

1436             return codeBlock()-&gt;isStrictMode();
<span class="line-modified">1437         return codeOrigin.inlineCallFrame-&gt;isStrictMode();</span>
1438     }
1439 
1440     ECMAMode ecmaModeFor(CodeOrigin codeOrigin)
1441     {
1442         return isStrictModeFor(codeOrigin) ? StrictMode : NotStrictMode;
1443     }
1444 
1445     ExecutableBase* executableFor(const CodeOrigin&amp; codeOrigin);
1446 
1447     CodeBlock* baselineCodeBlockFor(const CodeOrigin&amp; codeOrigin)
1448     {
1449         return baselineCodeBlockForOriginAndBaselineCodeBlock(codeOrigin, baselineCodeBlock());
1450     }
1451 
1452     CodeBlock* baselineCodeBlockFor(InlineCallFrame* inlineCallFrame)
1453     {
1454         if (!inlineCallFrame)
1455             return baselineCodeBlock();
1456         return baselineCodeBlockForInlineCallFrame(inlineCallFrame);
1457     }
1458 
1459     CodeBlock* baselineCodeBlock()
1460     {
1461         return m_baselineCodeBlock;
1462     }
1463 
1464     static VirtualRegister argumentsStart(InlineCallFrame* inlineCallFrame)
1465     {
1466         if (!inlineCallFrame)
1467             return VirtualRegister(CallFrame::argumentOffset(0));
1468         if (inlineCallFrame-&gt;argumentsWithFixup.size() &lt;= 1)
1469             return virtualRegisterForLocal(0);
1470         ValueRecovery recovery = inlineCallFrame-&gt;argumentsWithFixup[1];
1471         RELEASE_ASSERT(recovery.technique() == DisplacedInJSStack);
1472         return recovery.virtualRegister();
1473     }
1474 
1475     static VirtualRegister argumentsStart(const CodeOrigin&amp; codeOrigin)
1476     {
<span class="line-modified">1477         return argumentsStart(codeOrigin.inlineCallFrame);</span>
1478     }
1479 
1480     static VirtualRegister argumentCount(InlineCallFrame* inlineCallFrame)
1481     {
1482         ASSERT(!inlineCallFrame || inlineCallFrame-&gt;isVarargs());
1483         if (!inlineCallFrame)
1484             return VirtualRegister(CallFrameSlot::argumentCount);
1485         return inlineCallFrame-&gt;argumentCountRegister;
1486     }
1487 
1488     static VirtualRegister argumentCount(const CodeOrigin&amp; codeOrigin)
1489     {
<span class="line-modified">1490         return argumentCount(codeOrigin.inlineCallFrame);</span>
1491     }
1492 
1493     void emitLoadStructure(VM&amp;, RegisterID source, RegisterID dest, RegisterID scratch);
1494 
1495     void emitStoreStructureWithTypeInfo(TrustedImmPtr structure, RegisterID dest, RegisterID)
1496     {
1497         emitStoreStructureWithTypeInfo(*this, structure, dest);
1498     }
1499 
1500     void emitStoreStructureWithTypeInfo(RegisterID structure, RegisterID dest, RegisterID scratch)
1501     {
1502 #if USE(JSVALUE64)
1503         load64(MacroAssembler::Address(structure, Structure::structureIDOffset()), scratch);
1504         store64(scratch, MacroAssembler::Address(dest, JSCell::structureIDOffset()));
1505 #else
1506         // Store all the info flags using a single 32-bit wide load and store.
1507         load32(MacroAssembler::Address(structure, Structure::indexingModeIncludingHistoryOffset()), scratch);
1508         store32(scratch, MacroAssembler::Address(dest, JSCell::indexingTypeAndMiscOffset()));
1509 
1510         // Store the StructureID
</pre>
<hr />
<pre>
1523     {
1524         uint8_t* address = reinterpret_cast&lt;uint8_t*&gt;(cell) + JSCell::cellStateOffset();
1525         return branch8(Above, AbsoluteAddress(address), TrustedImm32(blackThreshold));
1526     }
1527 
1528     Jump barrierBranch(VM&amp; vm, GPRReg cell, GPRReg scratchGPR)
1529     {
1530         load8(Address(cell, JSCell::cellStateOffset()), scratchGPR);
1531         return branch32(Above, scratchGPR, AbsoluteAddress(vm.heap.addressOfBarrierThreshold()));
1532     }
1533 
1534     Jump barrierBranch(VM&amp; vm, JSCell* cell, GPRReg scratchGPR)
1535     {
1536         uint8_t* address = reinterpret_cast&lt;uint8_t*&gt;(cell) + JSCell::cellStateOffset();
1537         load8(address, scratchGPR);
1538         return branch32(Above, scratchGPR, AbsoluteAddress(vm.heap.addressOfBarrierThreshold()));
1539     }
1540 
1541     void barrierStoreLoadFence(VM&amp; vm)
1542     {
<span class="line-removed">1543         if (!Options::useConcurrentBarriers())</span>
<span class="line-removed">1544             return;</span>
1545         Jump ok = jumpIfMutatorFenceNotNeeded(vm);
1546         memoryFence();
1547         ok.link(this);
1548     }
1549 
1550     void mutatorFence(VM&amp; vm)
1551     {
1552         if (isX86())
1553             return;
1554         Jump ok = jumpIfMutatorFenceNotNeeded(vm);
1555         storeFence();
1556         ok.link(this);
1557     }
1558 
<span class="line-modified">1559     void cage(Gigacage::Kind kind, GPRReg storage)</span>
1560     {
1561 #if GIGACAGE_ENABLED
1562         if (!Gigacage::isEnabled(kind))
1563             return;
1564 









1565         andPtr(TrustedImmPtr(Gigacage::mask(kind)), storage);
1566         addPtr(TrustedImmPtr(Gigacage::basePtr(kind)), storage);





1567 #else
1568         UNUSED_PARAM(kind);
1569         UNUSED_PARAM(storage);
1570 #endif
1571     }
1572 
<span class="line-modified">1573     void cageConditionally(Gigacage::Kind kind, GPRReg storage, GPRReg scratch)</span>

1574     {
1575 #if GIGACAGE_ENABLED
<span class="line-modified">1576         if (!Gigacage::isEnabled(kind))</span>
<span class="line-modified">1577             return;</span>
<span class="line-modified">1578 </span>
<span class="line-modified">1579         if (kind != Gigacage::Primitive || Gigacage::isDisablingPrimitiveGigacageDisabled())</span>
<span class="line-modified">1580             return cage(kind, storage);</span>
<span class="line-modified">1581 </span>
<span class="line-modified">1582         loadPtr(&amp;Gigacage::basePtr(kind), scratch);</span>
<span class="line-modified">1583         Jump done = branchTestPtr(Zero, scratch);</span>
<span class="line-modified">1584         andPtr(TrustedImmPtr(Gigacage::mask(kind)), storage);</span>
<span class="line-modified">1585         addPtr(scratch, storage);</span>
<span class="line-modified">1586         done.link(this);</span>






1587 #else












1588         UNUSED_PARAM(kind);
1589         UNUSED_PARAM(storage);

1590         UNUSED_PARAM(scratch);
<span class="line-removed">1591 #endif</span>
1592     }
1593 
1594     void emitComputeButterflyIndexingMask(GPRReg vectorLengthGPR, GPRReg scratchGPR, GPRReg resultGPR)
1595     {
1596         ASSERT(scratchGPR != resultGPR);
1597         Jump done;
1598         if (isX86() &amp;&amp; !isX86_64()) {
1599             Jump nonZero = branchTest32(NonZero, vectorLengthGPR);
1600             move(TrustedImm32(0), resultGPR);
1601             done = jump();
1602             nonZero.link(this);
1603         }
1604         // If vectorLength == 0 then clz will return 32 on both ARM and x86. On 64-bit systems, we can then do a 64-bit right shift on a 32-bit -1 to get a 0 mask for zero vectorLength. On 32-bit ARM, shift masks with 0xff, which means it will still create a 0 mask.
1605         countLeadingZeros32(vectorLengthGPR, scratchGPR);
1606         move(TrustedImm32(-1), resultGPR);
1607         urshiftPtr(scratchGPR, resultGPR);
1608         if (done.isSet())
1609             done.link(this);
1610     }
1611 
</pre>
</td>
<td>
<hr />
<pre>
  40 #include &quot;RegisterAtOffsetList.h&quot;
  41 #include &quot;RegisterSet.h&quot;
  42 #include &quot;StackAlignment.h&quot;
  43 #include &quot;TagRegistersMode.h&quot;
  44 #include &quot;TypeofType.h&quot;
  45 #include &quot;VM.h&quot;
  46 
  47 namespace JSC {
  48 
  49 typedef void (*V_DebugOperation_EPP)(ExecState*, void*, void*);
  50 
  51 class AssemblyHelpers : public MacroAssembler {
  52 public:
  53     AssemblyHelpers(CodeBlock* codeBlock)
  54         : m_codeBlock(codeBlock)
  55         , m_baselineCodeBlock(codeBlock ? codeBlock-&gt;baselineAlternative() : 0)
  56     {
  57         if (m_codeBlock) {
  58             ASSERT(m_baselineCodeBlock);
  59             ASSERT(!m_baselineCodeBlock-&gt;alternative());
<span class="line-modified">  60             ASSERT(m_baselineCodeBlock-&gt;jitType() == JITType::None || JITCode::isBaselineCode(m_baselineCodeBlock-&gt;jitType()));</span>
  61         }
  62     }
  63 
  64     CodeBlock* codeBlock() { return m_codeBlock; }
<span class="line-modified">  65     VM&amp; vm() { return m_codeBlock-&gt;vm(); }</span>
  66     AssemblerType_T&amp; assembler() { return m_assembler; }
  67 
  68     void checkStackPointerAlignment()
  69     {
  70         // This check is both unneeded and harder to write correctly for ARM64
  71 #if !defined(NDEBUG) &amp;&amp; !CPU(ARM64)
  72         Jump stackPointerAligned = branchTestPtr(Zero, stackPointerRegister, TrustedImm32(0xf));
  73         abortWithReason(AHStackPointerMisaligned);
  74         stackPointerAligned.link(this);
  75 #endif
  76     }
  77 
  78     template&lt;typename T&gt;
  79     void storeCell(T cell, Address address)
  80     {
  81 #if USE(JSVALUE64)
  82         store64(cell, address);
  83 #else
  84         store32(cell, address.withOffset(PayloadOffset));
  85         store32(TrustedImm32(JSValue::CellTag), address.withOffset(TagOffset));
</pre>
<hr />
<pre>
 465                 storePtr(TrustedImm32(0), Address(currentTop, -8 - offset));
 466         } else {
 467             constexpr unsigned storeBytesPerIteration = stackAlignmentBytes();
 468             constexpr unsigned storesPerIteration = storeBytesPerIteration / sizeof(CPURegister);
 469 
 470             move(currentTop, temp);
 471             Label zeroLoop = label();
 472             subPtr(TrustedImm32(storeBytesPerIteration), temp);
 473 #if CPU(ARM64)
 474             static_assert(storesPerIteration == 2, &quot;clearStackFrame() for ARM64 assumes stack is 16 byte aligned&quot;);
 475             storePair64(ARM64Registers::zr, ARM64Registers::zr, temp);
 476 #else
 477             for (unsigned i = storesPerIteration; i-- != 0;)
 478                 storePtr(TrustedImm32(0), Address(temp, sizeof(CPURegister) * i));
 479 #endif
 480             branchPtr(NotEqual, temp, newTop).linkTo(zeroLoop, this);
 481         }
 482     }
 483 
 484 #if CPU(X86_64) || CPU(X86)
<span class="line-modified"> 485     static constexpr size_t prologueStackPointerDelta()</span>
 486     {
 487         // Prologue only saves the framePointerRegister
 488         return sizeof(void*);
 489     }
 490 
 491     void emitFunctionPrologue()
 492     {
 493         push(framePointerRegister);
 494         move(stackPointerRegister, framePointerRegister);
 495     }
 496 
 497     void emitFunctionEpilogueWithEmptyFrame()
 498     {
 499         pop(framePointerRegister);
 500     }
 501 
 502     void emitFunctionEpilogue()
 503     {
 504         move(framePointerRegister, stackPointerRegister);
 505         pop(framePointerRegister);
 506     }
 507 
 508     void preserveReturnAddressAfterCall(GPRReg reg)
 509     {
 510         pop(reg);
 511     }
 512 
 513     void restoreReturnAddressBeforeReturn(GPRReg reg)
 514     {
 515         push(reg);
 516     }
 517 
 518     void restoreReturnAddressBeforeReturn(Address address)
 519     {
 520         push(address);
 521     }
 522 #endif // CPU(X86_64) || CPU(X86)
 523 
 524 #if CPU(ARM_THUMB2) || CPU(ARM64)
<span class="line-modified"> 525     static constexpr size_t prologueStackPointerDelta()</span>
 526     {
 527         // Prologue saves the framePointerRegister and linkRegister
 528         return 2 * sizeof(void*);
 529     }
 530 
 531     void emitFunctionPrologue()
 532     {
 533         tagReturnAddress();
 534         pushPair(framePointerRegister, linkRegister);
 535         move(stackPointerRegister, framePointerRegister);
 536     }
 537 
 538     void emitFunctionEpilogueWithEmptyFrame()
 539     {
 540         popPair(framePointerRegister, linkRegister);
 541     }
 542 
 543     void emitFunctionEpilogue()
 544     {
 545         move(framePointerRegister, stackPointerRegister);
 546         emitFunctionEpilogueWithEmptyFrame();
 547     }
 548 
 549     ALWAYS_INLINE void preserveReturnAddressAfterCall(RegisterID reg)
 550     {
 551         move(linkRegister, reg);
 552     }
 553 
 554     ALWAYS_INLINE void restoreReturnAddressBeforeReturn(RegisterID reg)
 555     {
 556         move(reg, linkRegister);
 557     }
 558 
 559     ALWAYS_INLINE void restoreReturnAddressBeforeReturn(Address address)
 560     {
 561         loadPtr(address, linkRegister);
 562     }
 563 #endif
 564 
 565 #if CPU(MIPS)
<span class="line-modified"> 566     static constexpr size_t prologueStackPointerDelta()</span>
 567     {
 568         // Prologue saves the framePointerRegister and returnAddressRegister
 569         return 2 * sizeof(void*);
 570     }
 571 
 572     void emitFunctionPrologue()
 573     {
 574         pushPair(framePointerRegister, returnAddressRegister);
 575         move(stackPointerRegister, framePointerRegister);
 576     }
 577 
 578     void emitFunctionEpilogueWithEmptyFrame()
 579     {
 580         popPair(framePointerRegister, returnAddressRegister);
 581     }
 582 
 583     void emitFunctionEpilogue()
 584     {
 585         move(framePointerRegister, stackPointerRegister);
 586         emitFunctionEpilogueWithEmptyFrame();
</pre>
<hr />
<pre>
1415         jit.add64(TrustedImm32(increment), AbsoluteAddress(counter.addressOfCounter()));
1416     }
1417     void emitCount(AbstractSamplingCounter&amp; counter, int32_t increment = 1)
1418     {
1419         add64(TrustedImm32(increment), AbsoluteAddress(counter.addressOfCounter()));
1420     }
1421 #endif
1422 
1423 #if ENABLE(SAMPLING_FLAGS)
1424     void setSamplingFlag(int32_t);
1425     void clearSamplingFlag(int32_t flag);
1426 #endif
1427 
1428     JSGlobalObject* globalObjectFor(CodeOrigin codeOrigin)
1429     {
1430         return codeBlock()-&gt;globalObjectFor(codeOrigin);
1431     }
1432 
1433     bool isStrictModeFor(CodeOrigin codeOrigin)
1434     {
<span class="line-modified">1435         auto* inlineCallFrame = codeOrigin.inlineCallFrame();</span>
<span class="line-added">1436         if (!inlineCallFrame)</span>
1437             return codeBlock()-&gt;isStrictMode();
<span class="line-modified">1438         return inlineCallFrame-&gt;isStrictMode();</span>
1439     }
1440 
1441     ECMAMode ecmaModeFor(CodeOrigin codeOrigin)
1442     {
1443         return isStrictModeFor(codeOrigin) ? StrictMode : NotStrictMode;
1444     }
1445 
1446     ExecutableBase* executableFor(const CodeOrigin&amp; codeOrigin);
1447 
1448     CodeBlock* baselineCodeBlockFor(const CodeOrigin&amp; codeOrigin)
1449     {
1450         return baselineCodeBlockForOriginAndBaselineCodeBlock(codeOrigin, baselineCodeBlock());
1451     }
1452 
1453     CodeBlock* baselineCodeBlockFor(InlineCallFrame* inlineCallFrame)
1454     {
1455         if (!inlineCallFrame)
1456             return baselineCodeBlock();
1457         return baselineCodeBlockForInlineCallFrame(inlineCallFrame);
1458     }
1459 
1460     CodeBlock* baselineCodeBlock()
1461     {
1462         return m_baselineCodeBlock;
1463     }
1464 
1465     static VirtualRegister argumentsStart(InlineCallFrame* inlineCallFrame)
1466     {
1467         if (!inlineCallFrame)
1468             return VirtualRegister(CallFrame::argumentOffset(0));
1469         if (inlineCallFrame-&gt;argumentsWithFixup.size() &lt;= 1)
1470             return virtualRegisterForLocal(0);
1471         ValueRecovery recovery = inlineCallFrame-&gt;argumentsWithFixup[1];
1472         RELEASE_ASSERT(recovery.technique() == DisplacedInJSStack);
1473         return recovery.virtualRegister();
1474     }
1475 
1476     static VirtualRegister argumentsStart(const CodeOrigin&amp; codeOrigin)
1477     {
<span class="line-modified">1478         return argumentsStart(codeOrigin.inlineCallFrame());</span>
1479     }
1480 
1481     static VirtualRegister argumentCount(InlineCallFrame* inlineCallFrame)
1482     {
1483         ASSERT(!inlineCallFrame || inlineCallFrame-&gt;isVarargs());
1484         if (!inlineCallFrame)
1485             return VirtualRegister(CallFrameSlot::argumentCount);
1486         return inlineCallFrame-&gt;argumentCountRegister;
1487     }
1488 
1489     static VirtualRegister argumentCount(const CodeOrigin&amp; codeOrigin)
1490     {
<span class="line-modified">1491         return argumentCount(codeOrigin.inlineCallFrame());</span>
1492     }
1493 
1494     void emitLoadStructure(VM&amp;, RegisterID source, RegisterID dest, RegisterID scratch);
1495 
1496     void emitStoreStructureWithTypeInfo(TrustedImmPtr structure, RegisterID dest, RegisterID)
1497     {
1498         emitStoreStructureWithTypeInfo(*this, structure, dest);
1499     }
1500 
1501     void emitStoreStructureWithTypeInfo(RegisterID structure, RegisterID dest, RegisterID scratch)
1502     {
1503 #if USE(JSVALUE64)
1504         load64(MacroAssembler::Address(structure, Structure::structureIDOffset()), scratch);
1505         store64(scratch, MacroAssembler::Address(dest, JSCell::structureIDOffset()));
1506 #else
1507         // Store all the info flags using a single 32-bit wide load and store.
1508         load32(MacroAssembler::Address(structure, Structure::indexingModeIncludingHistoryOffset()), scratch);
1509         store32(scratch, MacroAssembler::Address(dest, JSCell::indexingTypeAndMiscOffset()));
1510 
1511         // Store the StructureID
</pre>
<hr />
<pre>
1524     {
1525         uint8_t* address = reinterpret_cast&lt;uint8_t*&gt;(cell) + JSCell::cellStateOffset();
1526         return branch8(Above, AbsoluteAddress(address), TrustedImm32(blackThreshold));
1527     }
1528 
1529     Jump barrierBranch(VM&amp; vm, GPRReg cell, GPRReg scratchGPR)
1530     {
1531         load8(Address(cell, JSCell::cellStateOffset()), scratchGPR);
1532         return branch32(Above, scratchGPR, AbsoluteAddress(vm.heap.addressOfBarrierThreshold()));
1533     }
1534 
1535     Jump barrierBranch(VM&amp; vm, JSCell* cell, GPRReg scratchGPR)
1536     {
1537         uint8_t* address = reinterpret_cast&lt;uint8_t*&gt;(cell) + JSCell::cellStateOffset();
1538         load8(address, scratchGPR);
1539         return branch32(Above, scratchGPR, AbsoluteAddress(vm.heap.addressOfBarrierThreshold()));
1540     }
1541 
1542     void barrierStoreLoadFence(VM&amp; vm)
1543     {


1544         Jump ok = jumpIfMutatorFenceNotNeeded(vm);
1545         memoryFence();
1546         ok.link(this);
1547     }
1548 
1549     void mutatorFence(VM&amp; vm)
1550     {
1551         if (isX86())
1552             return;
1553         Jump ok = jumpIfMutatorFenceNotNeeded(vm);
1554         storeFence();
1555         ok.link(this);
1556     }
1557 
<span class="line-modified">1558     void cageWithoutUntagging(Gigacage::Kind kind, GPRReg storage)</span>
1559     {
1560 #if GIGACAGE_ENABLED
1561         if (!Gigacage::isEnabled(kind))
1562             return;
1563 
<span class="line-added">1564 #if CPU(ARM64E)</span>
<span class="line-added">1565         RegisterID tempReg = InvalidGPRReg;</span>
<span class="line-added">1566         if (kind == Gigacage::Primitive) {</span>
<span class="line-added">1567             tempReg = getCachedMemoryTempRegisterIDAndInvalidate();</span>
<span class="line-added">1568             move(storage, tempReg);</span>
<span class="line-added">1569             // Flip the registers since bitFieldInsert only inserts into the low bits.</span>
<span class="line-added">1570             std::swap(storage, tempReg);</span>
<span class="line-added">1571         }</span>
<span class="line-added">1572 #endif</span>
1573         andPtr(TrustedImmPtr(Gigacage::mask(kind)), storage);
1574         addPtr(TrustedImmPtr(Gigacage::basePtr(kind)), storage);
<span class="line-added">1575 #if CPU(ARM64E)</span>
<span class="line-added">1576         if (kind == Gigacage::Primitive)</span>
<span class="line-added">1577             bitFieldInsert64(storage, 0, 64 - numberOfPACBits, tempReg);</span>
<span class="line-added">1578 #endif</span>
<span class="line-added">1579 </span>
1580 #else
1581         UNUSED_PARAM(kind);
1582         UNUSED_PARAM(storage);
1583 #endif
1584     }
1585 
<span class="line-modified">1586     // length may be the same register as scratch.</span>
<span class="line-added">1587     void cageConditionally(Gigacage::Kind kind, GPRReg storage, GPRReg length, GPRReg scratch)</span>
1588     {
1589 #if GIGACAGE_ENABLED
<span class="line-modified">1590         if (Gigacage::isEnabled(kind)) {</span>
<span class="line-modified">1591             if (kind != Gigacage::Primitive || Gigacage::isDisablingPrimitiveGigacageDisabled())</span>
<span class="line-modified">1592                 cageWithoutUntagging(kind, storage);</span>
<span class="line-modified">1593             else {</span>
<span class="line-modified">1594 #if CPU(ARM64E)</span>
<span class="line-modified">1595                 if (length == scratch)</span>
<span class="line-modified">1596                     scratch = getCachedMemoryTempRegisterIDAndInvalidate();</span>
<span class="line-modified">1597 #endif</span>
<span class="line-modified">1598                 loadPtr(&amp;Gigacage::basePtr(kind), scratch);</span>
<span class="line-modified">1599                 Jump done = branchTest64(Zero, scratch);</span>
<span class="line-modified">1600 #if CPU(ARM64E)</span>
<span class="line-added">1601                 GPRReg tempReg = getCachedDataTempRegisterIDAndInvalidate();</span>
<span class="line-added">1602                 move(storage, tempReg);</span>
<span class="line-added">1603                 ASSERT(LogicalImmediate::create64(Gigacage::mask(kind)).isValid());</span>
<span class="line-added">1604                 andPtr(TrustedImmPtr(Gigacage::mask(kind)), tempReg);</span>
<span class="line-added">1605                 addPtr(scratch, tempReg);</span>
<span class="line-added">1606                 bitFieldInsert64(tempReg, 0, 64 - numberOfPACBits, storage);</span>
1607 #else
<span class="line-added">1608                 andPtr(TrustedImmPtr(Gigacage::mask(kind)), storage);</span>
<span class="line-added">1609                 addPtr(scratch, storage);</span>
<span class="line-added">1610 #endif // CPU(ARM64E)</span>
<span class="line-added">1611                 done.link(this);</span>
<span class="line-added">1612             }</span>
<span class="line-added">1613         }</span>
<span class="line-added">1614 #endif</span>
<span class="line-added">1615 </span>
<span class="line-added">1616 #if CPU(ARM64E)</span>
<span class="line-added">1617         if (kind == Gigacage::Primitive)</span>
<span class="line-added">1618             untagArrayPtr(length, storage);</span>
<span class="line-added">1619 #endif</span>
1620         UNUSED_PARAM(kind);
1621         UNUSED_PARAM(storage);
<span class="line-added">1622         UNUSED_PARAM(length);</span>
1623         UNUSED_PARAM(scratch);

1624     }
1625 
1626     void emitComputeButterflyIndexingMask(GPRReg vectorLengthGPR, GPRReg scratchGPR, GPRReg resultGPR)
1627     {
1628         ASSERT(scratchGPR != resultGPR);
1629         Jump done;
1630         if (isX86() &amp;&amp; !isX86_64()) {
1631             Jump nonZero = branchTest32(NonZero, vectorLengthGPR);
1632             move(TrustedImm32(0), resultGPR);
1633             done = jump();
1634             nonZero.link(this);
1635         }
1636         // If vectorLength == 0 then clz will return 32 on both ARM and x86. On 64-bit systems, we can then do a 64-bit right shift on a 32-bit -1 to get a 0 mask for zero vectorLength. On 32-bit ARM, shift masks with 0xff, which means it will still create a 0 mask.
1637         countLeadingZeros32(vectorLengthGPR, scratchGPR);
1638         move(TrustedImm32(-1), resultGPR);
1639         urshiftPtr(scratchGPR, resultGPR);
1640         if (done.isSet())
1641             done.link(this);
1642     }
1643 
</pre>
</td>
</tr>
</table>
<center><a href="AssemblyHelpers.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="BinarySwitch.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>