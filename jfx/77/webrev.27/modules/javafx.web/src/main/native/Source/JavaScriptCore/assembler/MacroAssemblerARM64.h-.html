<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/assembler/MacroAssemblerARM64.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2012-2018 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #pragma once
  27 
  28 #if ENABLE(ASSEMBLER)
  29 
  30 #include &quot;ARM64Assembler.h&quot;
  31 #include &quot;AbstractMacroAssembler.h&quot;
  32 #include &lt;wtf/MathExtras.h&gt;
  33 #include &lt;wtf/Optional.h&gt;
  34 
  35 namespace JSC {
  36 
  37 using Assembler = TARGET_ASSEMBLER;
  38 
  39 class MacroAssemblerARM64 : public AbstractMacroAssembler&lt;Assembler&gt; {
  40 public:
  41     static const unsigned numGPRs = 32;
  42     static const unsigned numFPRs = 32;
  43 
  44     static constexpr RegisterID dataTempRegister = ARM64Registers::ip0;
  45     static constexpr RegisterID memoryTempRegister = ARM64Registers::ip1;
  46 
  47     RegisterID scratchRegister()
  48     {
  49         RELEASE_ASSERT(m_allowScratchRegister);
  50         return getCachedDataTempRegisterIDAndInvalidate();
  51     }
  52 
  53 protected:
  54     static const ARM64Registers::FPRegisterID fpTempRegister = ARM64Registers::q31;
  55     static const Assembler::SetFlags S = Assembler::S;
  56     static const int64_t maskHalfWord0 = 0xffffl;
  57     static const int64_t maskHalfWord1 = 0xffff0000l;
  58     static const int64_t maskUpperWord = 0xffffffff00000000l;
  59 
  60     static constexpr size_t INSTRUCTION_SIZE = 4;
  61 
  62     // N instructions to load the pointer + 1 call instruction.
  63     static constexpr ptrdiff_t REPATCH_OFFSET_CALL_TO_POINTER = -((Assembler::MAX_POINTER_BITS / 16 + 1) * INSTRUCTION_SIZE);
  64 
  65 public:
  66     MacroAssemblerARM64()
  67         : m_dataMemoryTempRegister(this, dataTempRegister)
  68         , m_cachedMemoryTempRegister(this, memoryTempRegister)
  69         , m_makeJumpPatchable(false)
  70     {
  71     }
  72 
  73     typedef Assembler::LinkRecord LinkRecord;
  74     typedef Assembler::JumpType JumpType;
  75     typedef Assembler::JumpLinkType JumpLinkType;
  76     typedef Assembler::Condition Condition;
  77 
  78     static const Assembler::Condition DefaultCondition = Assembler::ConditionInvalid;
  79     static const Assembler::JumpType DefaultJump = Assembler::JumpNoConditionFixedSize;
  80 
  81     Vector&lt;LinkRecord, 0, UnsafeVectorOverflow&gt;&amp; jumpsToLink() { return m_assembler.jumpsToLink(); }
  82     static bool canCompact(JumpType jumpType) { return Assembler::canCompact(jumpType); }
  83     static JumpLinkType computeJumpType(JumpType jumpType, const uint8_t* from, const uint8_t* to) { return Assembler::computeJumpType(jumpType, from, to); }
  84     static JumpLinkType computeJumpType(LinkRecord&amp; record, const uint8_t* from, const uint8_t* to) { return Assembler::computeJumpType(record, from, to); }
  85     static int jumpSizeDelta(JumpType jumpType, JumpLinkType jumpLinkType) { return Assembler::jumpSizeDelta(jumpType, jumpLinkType); }
  86     template &lt;typename CopyFunction&gt;
  87     static void link(LinkRecord&amp; record, uint8_t* from, const uint8_t* fromInstruction, uint8_t* to, CopyFunction copy) { return Assembler::link(record, from, fromInstruction, to, copy); }
  88 
  89     static const Scale ScalePtr = TimesEight;
  90 
  91     static bool isCompactPtrAlignedAddressOffset(ptrdiff_t value)
  92     {
  93         // This is the largest 32-bit access allowed, aligned to 64-bit boundary.
  94         return !(value &amp; ~0x3ff8);
  95     }
  96 
  97     enum RelationalCondition {
  98         Equal = Assembler::ConditionEQ,
  99         NotEqual = Assembler::ConditionNE,
 100         Above = Assembler::ConditionHI,
 101         AboveOrEqual = Assembler::ConditionHS,
 102         Below = Assembler::ConditionLO,
 103         BelowOrEqual = Assembler::ConditionLS,
 104         GreaterThan = Assembler::ConditionGT,
 105         GreaterThanOrEqual = Assembler::ConditionGE,
 106         LessThan = Assembler::ConditionLT,
 107         LessThanOrEqual = Assembler::ConditionLE
 108     };
 109 
 110     enum ResultCondition {
 111         Overflow = Assembler::ConditionVS,
 112         Signed = Assembler::ConditionMI,
 113         PositiveOrZero = Assembler::ConditionPL,
 114         Zero = Assembler::ConditionEQ,
 115         NonZero = Assembler::ConditionNE
 116     };
 117 
 118     enum ZeroCondition {
 119         IsZero = Assembler::ConditionEQ,
 120         IsNonZero = Assembler::ConditionNE
 121     };
 122 
 123     enum DoubleCondition {
 124         // These conditions will only evaluate to true if the comparison is ordered - i.e. neither operand is NaN.
 125         DoubleEqual = Assembler::ConditionEQ,
 126         DoubleNotEqual = Assembler::ConditionVC, // Not the right flag! check for this &amp; handle differently.
 127         DoubleGreaterThan = Assembler::ConditionGT,
 128         DoubleGreaterThanOrEqual = Assembler::ConditionGE,
 129         DoubleLessThan = Assembler::ConditionLO,
 130         DoubleLessThanOrEqual = Assembler::ConditionLS,
 131         // If either operand is NaN, these conditions always evaluate to true.
 132         DoubleEqualOrUnordered = Assembler::ConditionVS, // Not the right flag! check for this &amp; handle differently.
 133         DoubleNotEqualOrUnordered = Assembler::ConditionNE,
 134         DoubleGreaterThanOrUnordered = Assembler::ConditionHI,
 135         DoubleGreaterThanOrEqualOrUnordered = Assembler::ConditionHS,
 136         DoubleLessThanOrUnordered = Assembler::ConditionLT,
 137         DoubleLessThanOrEqualOrUnordered = Assembler::ConditionLE,
 138     };
 139 
 140     static const RegisterID stackPointerRegister = ARM64Registers::sp;
 141     static const RegisterID framePointerRegister = ARM64Registers::fp;
 142     static const RegisterID linkRegister = ARM64Registers::lr;
 143 
 144     // FIXME: Get reasonable implementations for these
 145     static bool shouldBlindForSpecificArch(uint32_t value) { return value &gt;= 0x00ffffff; }
 146     static bool shouldBlindForSpecificArch(uint64_t value) { return value &gt;= 0x00ffffff; }
 147 
 148     // Integer operations:
 149 
 150     void add32(RegisterID a, RegisterID b, RegisterID dest)
 151     {
 152         ASSERT(a != ARM64Registers::sp &amp;&amp; b != ARM64Registers::sp);
 153         m_assembler.add&lt;32&gt;(dest, a, b);
 154     }
 155 
 156     void add32(RegisterID src, RegisterID dest)
 157     {
 158         m_assembler.add&lt;32&gt;(dest, dest, src);
 159     }
 160 
 161     void add32(TrustedImm32 imm, RegisterID dest)
 162     {
 163         add32(imm, dest, dest);
 164     }
 165 
 166     void add32(TrustedImm32 imm, RegisterID src, RegisterID dest)
 167     {
 168         if (isUInt12(imm.m_value))
 169             m_assembler.add&lt;32&gt;(dest, src, UInt12(imm.m_value));
 170         else if (isUInt12(-imm.m_value))
 171             m_assembler.sub&lt;32&gt;(dest, src, UInt12(-imm.m_value));
 172         else if (src != dest) {
 173             move(imm, dest);
 174             add32(src, dest);
 175         } else {
 176             move(imm, getCachedDataTempRegisterIDAndInvalidate());
 177             m_assembler.add&lt;32&gt;(dest, src, dataTempRegister);
 178         }
 179     }
 180 
 181     void add32(TrustedImm32 imm, Address address)
 182     {
 183         load32(address, getCachedDataTempRegisterIDAndInvalidate());
 184 
 185         if (isUInt12(imm.m_value))
 186             m_assembler.add&lt;32&gt;(dataTempRegister, dataTempRegister, UInt12(imm.m_value));
 187         else if (isUInt12(-imm.m_value))
 188             m_assembler.sub&lt;32&gt;(dataTempRegister, dataTempRegister, UInt12(-imm.m_value));
 189         else {
 190             move(imm, getCachedMemoryTempRegisterIDAndInvalidate());
 191             m_assembler.add&lt;32&gt;(dataTempRegister, dataTempRegister, memoryTempRegister);
 192         }
 193 
 194         store32(dataTempRegister, address);
 195     }
 196 
 197     void add32(TrustedImm32 imm, AbsoluteAddress address)
 198     {
 199         load32(address.m_ptr, getCachedDataTempRegisterIDAndInvalidate());
 200 
 201         if (isUInt12(imm.m_value)) {
 202             m_assembler.add&lt;32&gt;(dataTempRegister, dataTempRegister, UInt12(imm.m_value));
 203             store32(dataTempRegister, address.m_ptr);
 204             return;
 205         }
 206 
 207         if (isUInt12(-imm.m_value)) {
 208             m_assembler.sub&lt;32&gt;(dataTempRegister, dataTempRegister, UInt12(-imm.m_value));
 209             store32(dataTempRegister, address.m_ptr);
 210             return;
 211         }
 212 
 213         move(imm, getCachedMemoryTempRegisterIDAndInvalidate());
 214         m_assembler.add&lt;32&gt;(dataTempRegister, dataTempRegister, memoryTempRegister);
 215         store32(dataTempRegister, address.m_ptr);
 216     }
 217 
 218     void add32(Address src, RegisterID dest)
 219     {
 220         load32(src, getCachedDataTempRegisterIDAndInvalidate());
 221         add32(dataTempRegister, dest);
 222     }
 223 
 224     void add64(RegisterID a, RegisterID b, RegisterID dest)
 225     {
 226         ASSERT(a != ARM64Registers::sp || b != ARM64Registers::sp);
 227         if (b == ARM64Registers::sp)
 228             std::swap(a, b);
 229         m_assembler.add&lt;64&gt;(dest, a, b);
 230     }
 231 
 232     void add64(RegisterID src, RegisterID dest)
 233     {
 234         if (src == ARM64Registers::sp)
 235             m_assembler.add&lt;64&gt;(dest, src, dest);
 236         else
 237             m_assembler.add&lt;64&gt;(dest, dest, src);
 238     }
 239 
 240     void add64(TrustedImm32 imm, RegisterID dest)
 241     {
 242         if (isUInt12(imm.m_value)) {
 243             m_assembler.add&lt;64&gt;(dest, dest, UInt12(imm.m_value));
 244             return;
 245         }
 246         if (isUInt12(-imm.m_value)) {
 247             m_assembler.sub&lt;64&gt;(dest, dest, UInt12(-imm.m_value));
 248             return;
 249         }
 250 
 251         signExtend32ToPtr(imm, getCachedDataTempRegisterIDAndInvalidate());
 252         m_assembler.add&lt;64&gt;(dest, dest, dataTempRegister);
 253     }
 254 
 255     void add64(TrustedImm64 imm, RegisterID dest)
 256     {
 257         intptr_t immediate = imm.m_value;
 258 
 259         if (isUInt12(immediate)) {
 260             m_assembler.add&lt;64&gt;(dest, dest, UInt12(static_cast&lt;int32_t&gt;(immediate)));
 261             return;
 262         }
 263         if (isUInt12(-immediate)) {
 264             m_assembler.sub&lt;64&gt;(dest, dest, UInt12(static_cast&lt;int32_t&gt;(-immediate)));
 265             return;
 266         }
 267 
 268         move(imm, getCachedDataTempRegisterIDAndInvalidate());
 269         m_assembler.add&lt;64&gt;(dest, dest, dataTempRegister);
 270     }
 271 
 272     void add64(TrustedImm32 imm, RegisterID src, RegisterID dest)
 273     {
 274         if (isUInt12(imm.m_value)) {
 275             m_assembler.add&lt;64&gt;(dest, src, UInt12(imm.m_value));
 276             return;
 277         }
 278         if (isUInt12(-imm.m_value)) {
 279             m_assembler.sub&lt;64&gt;(dest, src, UInt12(-imm.m_value));
 280             return;
 281         }
 282 
 283         signExtend32ToPtr(imm, getCachedDataTempRegisterIDAndInvalidate());
 284         m_assembler.add&lt;64&gt;(dest, src, dataTempRegister);
 285     }
 286 
 287     void add64(TrustedImm32 imm, Address address)
 288     {
 289         load64(address, getCachedDataTempRegisterIDAndInvalidate());
 290 
 291         if (isUInt12(imm.m_value))
 292             m_assembler.add&lt;64&gt;(dataTempRegister, dataTempRegister, UInt12(imm.m_value));
 293         else if (isUInt12(-imm.m_value))
 294             m_assembler.sub&lt;64&gt;(dataTempRegister, dataTempRegister, UInt12(-imm.m_value));
 295         else {
 296             signExtend32ToPtr(imm, getCachedMemoryTempRegisterIDAndInvalidate());
 297             m_assembler.add&lt;64&gt;(dataTempRegister, dataTempRegister, memoryTempRegister);
 298         }
 299 
 300         store64(dataTempRegister, address);
 301     }
 302 
 303     void add64(TrustedImm32 imm, AbsoluteAddress address)
 304     {
 305         load64(address.m_ptr, getCachedDataTempRegisterIDAndInvalidate());
 306 
 307         if (isUInt12(imm.m_value)) {
 308             m_assembler.add&lt;64&gt;(dataTempRegister, dataTempRegister, UInt12(imm.m_value));
 309             store64(dataTempRegister, address.m_ptr);
 310             return;
 311         }
 312 
 313         if (isUInt12(-imm.m_value)) {
 314             m_assembler.sub&lt;64&gt;(dataTempRegister, dataTempRegister, UInt12(-imm.m_value));
 315             store64(dataTempRegister, address.m_ptr);
 316             return;
 317         }
 318 
 319         signExtend32ToPtr(imm, getCachedMemoryTempRegisterIDAndInvalidate());
 320         m_assembler.add&lt;64&gt;(dataTempRegister, dataTempRegister, memoryTempRegister);
 321         store64(dataTempRegister, address.m_ptr);
 322     }
 323 
 324     void addPtrNoFlags(TrustedImm32 imm, RegisterID srcDest)
 325     {
 326         add64(imm, srcDest);
 327     }
 328 
 329     void add64(Address src, RegisterID dest)
 330     {
 331         load64(src, getCachedDataTempRegisterIDAndInvalidate());
 332         m_assembler.add&lt;64&gt;(dest, dest, dataTempRegister);
 333     }
 334 
 335     void add64(AbsoluteAddress src, RegisterID dest)
 336     {
 337         load64(src.m_ptr, getCachedDataTempRegisterIDAndInvalidate());
 338         m_assembler.add&lt;64&gt;(dest, dest, dataTempRegister);
 339     }
 340 
 341     void and32(RegisterID src, RegisterID dest)
 342     {
 343         and32(dest, src, dest);
 344     }
 345 
 346     void and32(RegisterID op1, RegisterID op2, RegisterID dest)
 347     {
 348         m_assembler.and_&lt;32&gt;(dest, op1, op2);
 349     }
 350 
 351     void and32(TrustedImm32 imm, RegisterID dest)
 352     {
 353         and32(imm, dest, dest);
 354     }
 355 
 356     void and32(TrustedImm32 imm, RegisterID src, RegisterID dest)
 357     {
 358         LogicalImmediate logicalImm = LogicalImmediate::create32(imm.m_value);
 359 
 360         if (logicalImm.isValid()) {
 361             m_assembler.and_&lt;32&gt;(dest, src, logicalImm);
 362             return;
 363         }
 364 
 365         move(imm, getCachedDataTempRegisterIDAndInvalidate());
 366         m_assembler.and_&lt;32&gt;(dest, src, dataTempRegister);
 367     }
 368 
 369     void and32(Address src, RegisterID dest)
 370     {
 371         load32(src, getCachedDataTempRegisterIDAndInvalidate());
 372         and32(dataTempRegister, dest);
 373     }
 374 
 375     void and16(Address src, RegisterID dest)
 376     {
 377         load16(src, getCachedDataTempRegisterIDAndInvalidate());
 378         and32(dataTempRegister, dest);
 379     }
 380 
 381     void and64(RegisterID src1, RegisterID src2, RegisterID dest)
 382     {
 383         m_assembler.and_&lt;64&gt;(dest, src1, src2);
 384     }
 385 
 386     void and64(TrustedImm64 imm, RegisterID src, RegisterID dest)
 387     {
 388         LogicalImmediate logicalImm = LogicalImmediate::create64(imm.m_value);
 389 
 390         if (logicalImm.isValid()) {
 391             m_assembler.and_&lt;64&gt;(dest, src, logicalImm);
 392             return;
 393         }
 394 
 395         move(imm, getCachedDataTempRegisterIDAndInvalidate());
 396         m_assembler.and_&lt;64&gt;(dest, src, dataTempRegister);
 397     }
 398 
 399     void and64(RegisterID src, RegisterID dest)
 400     {
 401         m_assembler.and_&lt;64&gt;(dest, dest, src);
 402     }
 403 
 404     void and64(TrustedImm32 imm, RegisterID dest)
 405     {
 406         LogicalImmediate logicalImm = LogicalImmediate::create64(static_cast&lt;intptr_t&gt;(static_cast&lt;int64_t&gt;(imm.m_value)));
 407 
 408         if (logicalImm.isValid()) {
 409             m_assembler.and_&lt;64&gt;(dest, dest, logicalImm);
 410             return;
 411         }
 412 
 413         signExtend32ToPtr(imm, getCachedDataTempRegisterIDAndInvalidate());
 414         m_assembler.and_&lt;64&gt;(dest, dest, dataTempRegister);
 415     }
 416 
 417     void and64(TrustedImmPtr imm, RegisterID dest)
 418     {
 419         LogicalImmediate logicalImm = LogicalImmediate::create64(reinterpret_cast&lt;uint64_t&gt;(imm.m_value));
 420 
 421         if (logicalImm.isValid()) {
 422             m_assembler.and_&lt;64&gt;(dest, dest, logicalImm);
 423             return;
 424         }
 425 
 426         move(imm, getCachedDataTempRegisterIDAndInvalidate());
 427         m_assembler.and_&lt;64&gt;(dest, dest, dataTempRegister);
 428     }
 429 
 430     void countLeadingZeros32(RegisterID src, RegisterID dest)
 431     {
 432         m_assembler.clz&lt;32&gt;(dest, src);
 433     }
 434 
 435     void countLeadingZeros64(RegisterID src, RegisterID dest)
 436     {
 437         m_assembler.clz&lt;64&gt;(dest, src);
 438     }
 439 
 440     void countTrailingZeros32(RegisterID src, RegisterID dest)
 441     {
 442         // Arm does not have a count trailing zeros only a count leading zeros.
 443         m_assembler.rbit&lt;32&gt;(dest, src);
 444         m_assembler.clz&lt;32&gt;(dest, dest);
 445     }
 446 
 447     void countTrailingZeros64(RegisterID src, RegisterID dest)
 448     {
 449         // Arm does not have a count trailing zeros only a count leading zeros.
 450         m_assembler.rbit&lt;64&gt;(dest, src);
 451         m_assembler.clz&lt;64&gt;(dest, dest);
 452     }
 453 
 454     void byteSwap16(RegisterID dst)
 455     {
 456         m_assembler.rev16&lt;32&gt;(dst, dst);
 457         zeroExtend16To32(dst, dst);
 458     }
 459 
 460     void byteSwap32(RegisterID dst)
 461     {
 462         m_assembler.rev&lt;32&gt;(dst, dst);
 463     }
 464 
 465     void byteSwap64(RegisterID dst)
 466     {
 467         m_assembler.rev&lt;64&gt;(dst, dst);
 468     }
 469 
 470     // Only used for testing purposes.
 471     void illegalInstruction()
 472     {
 473         m_assembler.illegalInstruction();
 474     }
 475 
 476     void lshift32(RegisterID src, RegisterID shiftAmount, RegisterID dest)
 477     {
 478         m_assembler.lsl&lt;32&gt;(dest, src, shiftAmount);
 479     }
 480 
 481     void lshift32(RegisterID src, TrustedImm32 imm, RegisterID dest)
 482     {
 483         m_assembler.lsl&lt;32&gt;(dest, src, imm.m_value &amp; 0x1f);
 484     }
 485 
 486     void lshift32(RegisterID shiftAmount, RegisterID dest)
 487     {
 488         lshift32(dest, shiftAmount, dest);
 489     }
 490 
 491     void lshift32(TrustedImm32 imm, RegisterID dest)
 492     {
 493         lshift32(dest, imm, dest);
 494     }
 495 
 496     void lshift64(RegisterID src, RegisterID shiftAmount, RegisterID dest)
 497     {
 498         m_assembler.lsl&lt;64&gt;(dest, src, shiftAmount);
 499     }
 500 
 501     void lshift64(RegisterID src, TrustedImm32 imm, RegisterID dest)
 502     {
 503         m_assembler.lsl&lt;64&gt;(dest, src, imm.m_value &amp; 0x3f);
 504     }
 505 
 506     void lshift64(RegisterID shiftAmount, RegisterID dest)
 507     {
 508         lshift64(dest, shiftAmount, dest);
 509     }
 510 
 511     void lshift64(TrustedImm32 imm, RegisterID dest)
 512     {
 513         lshift64(dest, imm, dest);
 514     }
 515 
 516     void mul32(RegisterID left, RegisterID right, RegisterID dest)
 517     {
 518         m_assembler.mul&lt;32&gt;(dest, left, right);
 519     }
 520 
 521     void mul32(RegisterID src, RegisterID dest)
 522     {
 523         m_assembler.mul&lt;32&gt;(dest, dest, src);
 524     }
 525 
 526     void mul32(TrustedImm32 imm, RegisterID src, RegisterID dest)
 527     {
 528         move(imm, getCachedDataTempRegisterIDAndInvalidate());
 529         m_assembler.mul&lt;32&gt;(dest, src, dataTempRegister);
 530     }
 531 
 532     void mul64(RegisterID src, RegisterID dest)
 533     {
 534         m_assembler.mul&lt;64&gt;(dest, dest, src);
 535     }
 536 
 537     void mul64(RegisterID left, RegisterID right, RegisterID dest)
 538     {
 539         m_assembler.mul&lt;64&gt;(dest, left, right);
 540     }
 541 
 542     void multiplyAdd32(RegisterID mulLeft, RegisterID mulRight, RegisterID summand, RegisterID dest)
 543     {
 544         m_assembler.madd&lt;32&gt;(dest, mulLeft, mulRight, summand);
 545     }
 546 
 547     void multiplySub32(RegisterID mulLeft, RegisterID mulRight, RegisterID minuend, RegisterID dest)
 548     {
 549         m_assembler.msub&lt;32&gt;(dest, mulLeft, mulRight, minuend);
 550     }
 551 
 552     void multiplyNeg32(RegisterID mulLeft, RegisterID mulRight, RegisterID dest)
 553     {
 554         m_assembler.msub&lt;32&gt;(dest, mulLeft, mulRight, ARM64Registers::zr);
 555     }
 556 
 557     void multiplyAdd64(RegisterID mulLeft, RegisterID mulRight, RegisterID summand, RegisterID dest)
 558     {
 559         m_assembler.madd&lt;64&gt;(dest, mulLeft, mulRight, summand);
 560     }
 561 
 562     void multiplySub64(RegisterID mulLeft, RegisterID mulRight, RegisterID minuend, RegisterID dest)
 563     {
 564         m_assembler.msub&lt;64&gt;(dest, mulLeft, mulRight, minuend);
 565     }
 566 
 567     void multiplyNeg64(RegisterID mulLeft, RegisterID mulRight, RegisterID dest)
 568     {
 569         m_assembler.msub&lt;64&gt;(dest, mulLeft, mulRight, ARM64Registers::zr);
 570     }
 571 
 572     void div32(RegisterID dividend, RegisterID divisor, RegisterID dest)
 573     {
 574         m_assembler.sdiv&lt;32&gt;(dest, dividend, divisor);
 575     }
 576 
 577     void div64(RegisterID dividend, RegisterID divisor, RegisterID dest)
 578     {
 579         m_assembler.sdiv&lt;64&gt;(dest, dividend, divisor);
 580     }
 581 
 582     void uDiv32(RegisterID dividend, RegisterID divisor, RegisterID dest)
 583     {
 584         m_assembler.udiv&lt;32&gt;(dest, dividend, divisor);
 585     }
 586 
 587     void uDiv64(RegisterID dividend, RegisterID divisor, RegisterID dest)
 588     {
 589         m_assembler.udiv&lt;64&gt;(dest, dividend, divisor);
 590     }
 591 
 592     void neg32(RegisterID dest)
 593     {
 594         m_assembler.neg&lt;32&gt;(dest, dest);
 595     }
 596 
 597     void neg32(RegisterID src, RegisterID dest)
 598     {
 599         m_assembler.neg&lt;32&gt;(dest, src);
 600     }
 601 
 602     void neg64(RegisterID dest)
 603     {
 604         m_assembler.neg&lt;64&gt;(dest, dest);
 605     }
 606 
 607     void neg64(RegisterID src, RegisterID dest)
 608     {
 609         m_assembler.neg&lt;64&gt;(dest, src);
 610     }
 611 
 612     void or32(RegisterID src, RegisterID dest)
 613     {
 614         or32(dest, src, dest);
 615     }
 616 
 617     void or32(RegisterID op1, RegisterID op2, RegisterID dest)
 618     {
 619         m_assembler.orr&lt;32&gt;(dest, op1, op2);
 620     }
 621 
 622     void or32(TrustedImm32 imm, RegisterID dest)
 623     {
 624         or32(imm, dest, dest);
 625     }
 626 
 627     void or32(TrustedImm32 imm, RegisterID src, RegisterID dest)
 628     {
 629         LogicalImmediate logicalImm = LogicalImmediate::create32(imm.m_value);
 630 
 631         if (logicalImm.isValid()) {
 632             m_assembler.orr&lt;32&gt;(dest, src, logicalImm);
 633             return;
 634         }
 635 
 636         ASSERT(src != dataTempRegister);
 637         move(imm, getCachedDataTempRegisterIDAndInvalidate());
 638         m_assembler.orr&lt;32&gt;(dest, src, dataTempRegister);
 639     }
 640 
 641     void or32(RegisterID src, AbsoluteAddress address)
 642     {
 643         load32(address.m_ptr, getCachedDataTempRegisterIDAndInvalidate());
 644         m_assembler.orr&lt;32&gt;(dataTempRegister, dataTempRegister, src);
 645         store32(dataTempRegister, address.m_ptr);
 646     }
 647 
 648     void or32(TrustedImm32 imm, AbsoluteAddress address)
 649     {
 650         LogicalImmediate logicalImm = LogicalImmediate::create32(imm.m_value);
 651         if (logicalImm.isValid()) {
 652             load32(address.m_ptr, getCachedDataTempRegisterIDAndInvalidate());
 653             m_assembler.orr&lt;32&gt;(dataTempRegister, dataTempRegister, logicalImm);
 654             store32(dataTempRegister, address.m_ptr);
 655         } else {
 656             load32(address.m_ptr, getCachedMemoryTempRegisterIDAndInvalidate());
 657             or32(imm, memoryTempRegister, getCachedDataTempRegisterIDAndInvalidate());
 658             store32(dataTempRegister, address.m_ptr);
 659         }
 660     }
 661 
 662     void or32(TrustedImm32 imm, Address address)
 663     {
 664         load32(address, getCachedDataTempRegisterIDAndInvalidate());
 665         or32(imm, dataTempRegister, dataTempRegister);
 666         store32(dataTempRegister, address);
 667     }
 668 
 669     void or64(RegisterID src, RegisterID dest)
 670     {
 671         or64(dest, src, dest);
 672     }
 673 
 674     void or64(RegisterID op1, RegisterID op2, RegisterID dest)
 675     {
 676         m_assembler.orr&lt;64&gt;(dest, op1, op2);
 677     }
 678 
 679     void or64(TrustedImm32 imm, RegisterID dest)
 680     {
 681         or64(imm, dest, dest);
 682     }
 683 
 684     void or64(TrustedImm32 imm, RegisterID src, RegisterID dest)
 685     {
 686         LogicalImmediate logicalImm = LogicalImmediate::create64(static_cast&lt;intptr_t&gt;(static_cast&lt;int64_t&gt;(imm.m_value)));
 687 
 688         if (logicalImm.isValid()) {
 689             m_assembler.orr&lt;64&gt;(dest, src, logicalImm);
 690             return;
 691         }
 692 
 693         signExtend32ToPtr(imm, getCachedDataTempRegisterIDAndInvalidate());
 694         m_assembler.orr&lt;64&gt;(dest, src, dataTempRegister);
 695     }
 696 
 697     void or64(TrustedImm64 imm, RegisterID src, RegisterID dest)
 698     {
 699         LogicalImmediate logicalImm = LogicalImmediate::create64(imm.m_value);
 700 
 701         if (logicalImm.isValid()) {
 702             m_assembler.orr&lt;64&gt;(dest, src, logicalImm);
 703             return;
 704         }
 705 
 706         move(imm, getCachedDataTempRegisterIDAndInvalidate());
 707         m_assembler.orr&lt;64&gt;(dest, src, dataTempRegister);
 708     }
 709 
 710     void or64(TrustedImm64 imm, RegisterID dest)
 711     {
 712         LogicalImmediate logicalImm = LogicalImmediate::create64(static_cast&lt;intptr_t&gt;(static_cast&lt;int64_t&gt;(imm.m_value)));
 713 
 714         if (logicalImm.isValid()) {
 715             m_assembler.orr&lt;64&gt;(dest, dest, logicalImm);
 716             return;
 717         }
 718 
 719         move(imm, getCachedDataTempRegisterIDAndInvalidate());
 720         m_assembler.orr&lt;64&gt;(dest, dest, dataTempRegister);
 721     }
 722 
 723     void rotateRight32(RegisterID src, TrustedImm32 imm, RegisterID dest)
 724     {
 725         m_assembler.ror&lt;32&gt;(dest, src, imm.m_value &amp; 31);
 726     }
 727 
 728     void rotateRight32(TrustedImm32 imm, RegisterID srcDst)
 729     {
 730         rotateRight32(srcDst, imm, srcDst);
 731     }
 732 
 733     void rotateRight32(RegisterID src, RegisterID shiftAmmount, RegisterID dest)
 734     {
 735         m_assembler.ror&lt;32&gt;(dest, src, shiftAmmount);
 736     }
 737 
 738     void rotateRight64(RegisterID src, TrustedImm32 imm, RegisterID dest)
 739     {
 740         m_assembler.ror&lt;64&gt;(dest, src, imm.m_value &amp; 63);
 741     }
 742 
 743     void rotateRight64(TrustedImm32 imm, RegisterID srcDst)
 744     {
 745         rotateRight64(srcDst, imm, srcDst);
 746     }
 747 
 748     void rotateRight64(RegisterID src, RegisterID shiftAmmount, RegisterID dest)
 749     {
 750         m_assembler.ror&lt;64&gt;(dest, src, shiftAmmount);
 751     }
 752 
 753     void rshift32(RegisterID src, RegisterID shiftAmount, RegisterID dest)
 754     {
 755         m_assembler.asr&lt;32&gt;(dest, src, shiftAmount);
 756     }
 757 
 758     void rshift32(RegisterID src, TrustedImm32 imm, RegisterID dest)
 759     {
 760         m_assembler.asr&lt;32&gt;(dest, src, imm.m_value &amp; 0x1f);
 761     }
 762 
 763     void rshift32(RegisterID shiftAmount, RegisterID dest)
 764     {
 765         rshift32(dest, shiftAmount, dest);
 766     }
 767 
 768     void rshift32(TrustedImm32 imm, RegisterID dest)
 769     {
 770         rshift32(dest, imm, dest);
 771     }
 772 
 773     void rshift64(RegisterID src, RegisterID shiftAmount, RegisterID dest)
 774     {
 775         m_assembler.asr&lt;64&gt;(dest, src, shiftAmount);
 776     }
 777 
 778     void rshift64(RegisterID src, TrustedImm32 imm, RegisterID dest)
 779     {
 780         m_assembler.asr&lt;64&gt;(dest, src, imm.m_value &amp; 0x3f);
 781     }
 782 
 783     void rshift64(RegisterID shiftAmount, RegisterID dest)
 784     {
 785         rshift64(dest, shiftAmount, dest);
 786     }
 787 
 788     void rshift64(TrustedImm32 imm, RegisterID dest)
 789     {
 790         rshift64(dest, imm, dest);
 791     }
 792 
 793     void sub32(RegisterID src, RegisterID dest)
 794     {
 795         m_assembler.sub&lt;32&gt;(dest, dest, src);
 796     }
 797 
 798     void sub32(RegisterID left, RegisterID right, RegisterID dest)
 799     {
 800         m_assembler.sub&lt;32&gt;(dest, left, right);
 801     }
 802 
 803     void sub32(TrustedImm32 imm, RegisterID dest)
 804     {
 805         if (isUInt12(imm.m_value)) {
 806             m_assembler.sub&lt;32&gt;(dest, dest, UInt12(imm.m_value));
 807             return;
 808         }
 809         if (isUInt12(-imm.m_value)) {
 810             m_assembler.add&lt;32&gt;(dest, dest, UInt12(-imm.m_value));
 811             return;
 812         }
 813 
 814         move(imm, getCachedDataTempRegisterIDAndInvalidate());
 815         m_assembler.sub&lt;32&gt;(dest, dest, dataTempRegister);
 816     }
 817 
 818     void sub32(TrustedImm32 imm, Address address)
 819     {
 820         load32(address, getCachedDataTempRegisterIDAndInvalidate());
 821 
 822         if (isUInt12(imm.m_value))
 823             m_assembler.sub&lt;32&gt;(dataTempRegister, dataTempRegister, UInt12(imm.m_value));
 824         else if (isUInt12(-imm.m_value))
 825             m_assembler.add&lt;32&gt;(dataTempRegister, dataTempRegister, UInt12(-imm.m_value));
 826         else {
 827             move(imm, getCachedMemoryTempRegisterIDAndInvalidate());
 828             m_assembler.sub&lt;32&gt;(dataTempRegister, dataTempRegister, memoryTempRegister);
 829         }
 830 
 831         store32(dataTempRegister, address);
 832     }
 833 
 834     void sub32(TrustedImm32 imm, AbsoluteAddress address)
 835     {
 836         load32(address.m_ptr, getCachedDataTempRegisterIDAndInvalidate());
 837 
 838         if (isUInt12(imm.m_value)) {
 839             m_assembler.sub&lt;32&gt;(dataTempRegister, dataTempRegister, UInt12(imm.m_value));
 840             store32(dataTempRegister, address.m_ptr);
 841             return;
 842         }
 843 
 844         if (isUInt12(-imm.m_value)) {
 845             m_assembler.add&lt;32&gt;(dataTempRegister, dataTempRegister, UInt12(-imm.m_value));
 846             store32(dataTempRegister, address.m_ptr);
 847             return;
 848         }
 849 
 850         move(imm, getCachedMemoryTempRegisterIDAndInvalidate());
 851         m_assembler.sub&lt;32&gt;(dataTempRegister, dataTempRegister, memoryTempRegister);
 852         store32(dataTempRegister, address.m_ptr);
 853     }
 854 
 855     void sub32(Address src, RegisterID dest)
 856     {
 857         load32(src, getCachedDataTempRegisterIDAndInvalidate());
 858         sub32(dataTempRegister, dest);
 859     }
 860 
 861     void sub64(RegisterID src, RegisterID dest)
 862     {
 863         m_assembler.sub&lt;64&gt;(dest, dest, src);
 864     }
 865 
 866     void sub64(RegisterID a, RegisterID b, RegisterID dest)
 867     {
 868         m_assembler.sub&lt;64&gt;(dest, a, b);
 869     }
 870 
 871     void sub64(TrustedImm32 imm, RegisterID dest)
 872     {
 873         if (isUInt12(imm.m_value)) {
 874             m_assembler.sub&lt;64&gt;(dest, dest, UInt12(imm.m_value));
 875             return;
 876         }
 877         if (isUInt12(-imm.m_value)) {
 878             m_assembler.add&lt;64&gt;(dest, dest, UInt12(-imm.m_value));
 879             return;
 880         }
 881 
 882         signExtend32ToPtr(imm, getCachedDataTempRegisterIDAndInvalidate());
 883         m_assembler.sub&lt;64&gt;(dest, dest, dataTempRegister);
 884     }
 885 
 886     void sub64(TrustedImm64 imm, RegisterID dest)
 887     {
 888         intptr_t immediate = imm.m_value;
 889 
 890         if (isUInt12(immediate)) {
 891             m_assembler.sub&lt;64&gt;(dest, dest, UInt12(static_cast&lt;int32_t&gt;(immediate)));
 892             return;
 893         }
 894         if (isUInt12(-immediate)) {
 895             m_assembler.add&lt;64&gt;(dest, dest, UInt12(static_cast&lt;int32_t&gt;(-immediate)));
 896             return;
 897         }
 898 
 899         move(imm, getCachedDataTempRegisterIDAndInvalidate());
 900         m_assembler.sub&lt;64&gt;(dest, dest, dataTempRegister);
 901     }
 902 
 903     void urshift32(RegisterID src, RegisterID shiftAmount, RegisterID dest)
 904     {
 905         m_assembler.lsr&lt;32&gt;(dest, src, shiftAmount);
 906     }
 907 
 908     void urshift32(RegisterID src, TrustedImm32 imm, RegisterID dest)
 909     {
 910         m_assembler.lsr&lt;32&gt;(dest, src, imm.m_value &amp; 0x1f);
 911     }
 912 
 913     void urshift32(RegisterID shiftAmount, RegisterID dest)
 914     {
 915         urshift32(dest, shiftAmount, dest);
 916     }
 917 
 918     void urshift32(TrustedImm32 imm, RegisterID dest)
 919     {
 920         urshift32(dest, imm, dest);
 921     }
 922 
 923     void urshift64(RegisterID src, RegisterID shiftAmount, RegisterID dest)
 924     {
 925         m_assembler.lsr&lt;64&gt;(dest, src, shiftAmount);
 926     }
 927 
 928     void urshift64(RegisterID src, TrustedImm32 imm, RegisterID dest)
 929     {
 930         m_assembler.lsr&lt;64&gt;(dest, src, imm.m_value &amp; 0x3f);
 931     }
 932 
 933     void urshift64(RegisterID shiftAmount, RegisterID dest)
 934     {
 935         urshift64(dest, shiftAmount, dest);
 936     }
 937 
 938     void urshift64(TrustedImm32 imm, RegisterID dest)
 939     {
 940         urshift64(dest, imm, dest);
 941     }
 942 
 943     void xor32(RegisterID src, RegisterID dest)
 944     {
 945         xor32(dest, src, dest);
 946     }
 947 
 948     void xor32(Address src, RegisterID dest)
 949     {
 950         load32(src, getCachedDataTempRegisterIDAndInvalidate());
 951         xor32(dataTempRegister, dest);
 952     }
 953 
 954     void xor32(RegisterID op1, RegisterID op2, RegisterID dest)
 955     {
 956         m_assembler.eor&lt;32&gt;(dest, op1, op2);
 957     }
 958 
 959     void xor32(TrustedImm32 imm, RegisterID dest)
 960     {
 961         xor32(imm, dest, dest);
 962     }
 963 
 964     void xor32(TrustedImm32 imm, RegisterID src, RegisterID dest)
 965     {
 966         if (imm.m_value == -1)
 967             m_assembler.mvn&lt;32&gt;(dest, src);
 968         else {
 969             LogicalImmediate logicalImm = LogicalImmediate::create32(imm.m_value);
 970 
 971             if (logicalImm.isValid()) {
 972                 m_assembler.eor&lt;32&gt;(dest, src, logicalImm);
 973                 return;
 974             }
 975 
 976             move(imm, getCachedDataTempRegisterIDAndInvalidate());
 977             m_assembler.eor&lt;32&gt;(dest, src, dataTempRegister);
 978         }
 979     }
 980 
 981     void xor64(RegisterID src, Address address)
 982     {
 983         load64(address, getCachedDataTempRegisterIDAndInvalidate());
 984         m_assembler.eor&lt;64&gt;(dataTempRegister, dataTempRegister, src);
 985         store64(dataTempRegister, address);
 986     }
 987 
 988     void xor64(RegisterID src, RegisterID dest)
 989     {
 990         xor64(dest, src, dest);
 991     }
 992 
 993     void xor64(RegisterID op1, RegisterID op2, RegisterID dest)
 994     {
 995         m_assembler.eor&lt;64&gt;(dest, op1, op2);
 996     }
 997 
 998     void xor64(TrustedImm32 imm, RegisterID dest)
 999     {
1000         xor64(imm, dest, dest);
1001     }
1002 
1003     void xor64(TrustedImm64 imm, RegisterID src, RegisterID dest)
1004     {
1005         if (imm.m_value == -1)
1006             m_assembler.mvn&lt;64&gt;(dest, src);
1007         else {
1008             LogicalImmediate logicalImm = LogicalImmediate::create64(imm.m_value);
1009 
1010             if (logicalImm.isValid()) {
1011                 m_assembler.eor&lt;64&gt;(dest, src, logicalImm);
1012                 return;
1013             }
1014 
1015             move(imm, getCachedDataTempRegisterIDAndInvalidate());
1016             m_assembler.eor&lt;64&gt;(dest, src, dataTempRegister);
1017         }
1018     }
1019 
1020     void xor64(TrustedImm64 imm, RegisterID srcDest)
1021     {
1022         xor64(imm, srcDest, srcDest);
1023     }
1024 
1025     void xor64(TrustedImm32 imm, RegisterID src, RegisterID dest)
1026     {
1027         if (imm.m_value == -1)
1028             m_assembler.mvn&lt;64&gt;(dest, src);
1029         else {
1030             LogicalImmediate logicalImm = LogicalImmediate::create64(static_cast&lt;intptr_t&gt;(static_cast&lt;int64_t&gt;(imm.m_value)));
1031 
1032             if (logicalImm.isValid()) {
1033                 m_assembler.eor&lt;64&gt;(dest, src, logicalImm);
1034                 return;
1035             }
1036 
1037             signExtend32ToPtr(imm, getCachedDataTempRegisterIDAndInvalidate());
1038             m_assembler.eor&lt;64&gt;(dest, src, dataTempRegister);
1039         }
1040     }
1041 
1042     void xor64(Address src, RegisterID dest)
1043     {
1044         load64(src, getCachedDataTempRegisterIDAndInvalidate());
1045         xor64(dataTempRegister, dest);
1046     }
1047 
1048     void not32(RegisterID srcDest)
1049     {
1050         m_assembler.mvn&lt;32&gt;(srcDest, srcDest);
1051     }
1052 
1053     void not32(RegisterID src, RegisterID dest)
1054     {
1055         m_assembler.mvn&lt;32&gt;(dest, src);
1056     }
1057 
1058     void not64(RegisterID src, RegisterID dest)
1059     {
1060         m_assembler.mvn&lt;64&gt;(dest, src);
1061     }
1062 
1063     void not64(RegisterID srcDst)
1064     {
1065         m_assembler.mvn&lt;64&gt;(srcDst, srcDst);
1066     }
1067 
1068     // Memory access operations:
1069 
1070     void load64(ImplicitAddress address, RegisterID dest)
1071     {
1072         if (tryLoadWithOffset&lt;64&gt;(dest, address.base, address.offset))
1073             return;
1074 
1075         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1076         m_assembler.ldr&lt;64&gt;(dest, address.base, memoryTempRegister);
1077     }
1078 
1079     void load64(BaseIndex address, RegisterID dest)
1080     {
1081         if (!address.offset &amp;&amp; (!address.scale || address.scale == 3)) {
1082             m_assembler.ldr&lt;64&gt;(dest, address.base, address.index, Assembler::UXTX, address.scale);
1083             return;
1084         }
1085 
1086         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1087         m_assembler.add&lt;64&gt;(memoryTempRegister, memoryTempRegister, address.index, Assembler::UXTX, address.scale);
1088         m_assembler.ldr&lt;64&gt;(dest, address.base, memoryTempRegister);
1089     }
1090 
1091     void load64(const void* address, RegisterID dest)
1092     {
1093         load&lt;64&gt;(address, dest);
1094     }
1095 
1096     void load64(RegisterID src, PostIndex simm, RegisterID dest)
1097     {
1098         m_assembler.ldr&lt;64&gt;(dest, src, simm);
1099     }
1100 
1101     DataLabel32 load64WithAddressOffsetPatch(Address address, RegisterID dest)
1102     {
1103         DataLabel32 label(this);
1104         signExtend32ToPtrWithFixedWidth(address.offset, getCachedMemoryTempRegisterIDAndInvalidate());
1105         m_assembler.ldr&lt;64&gt;(dest, address.base, memoryTempRegister, Assembler::SXTW, 0);
1106         return label;
1107     }
1108 
1109     DataLabelCompact load64WithCompactAddressOffsetPatch(Address address, RegisterID dest)
1110     {
1111         ASSERT(isCompactPtrAlignedAddressOffset(address.offset));
1112         DataLabelCompact label(this);
1113         m_assembler.ldr&lt;64&gt;(dest, address.base, address.offset);
1114         return label;
1115     }
1116 
1117     void loadPair64(RegisterID src, RegisterID dest1, RegisterID dest2)
1118     {
1119         loadPair64(src, TrustedImm32(0), dest1, dest2);
1120     }
1121 
1122     void loadPair64(RegisterID src, TrustedImm32 offset, RegisterID dest1, RegisterID dest2)
1123     {
1124         m_assembler.ldp&lt;64&gt;(dest1, dest2, src, offset.m_value);
1125     }
1126 
1127     void loadPair64WithNonTemporalAccess(RegisterID src, RegisterID dest1, RegisterID dest2)
1128     {
1129         loadPair64WithNonTemporalAccess(src, TrustedImm32(0), dest1, dest2);
1130     }
1131 
1132     void loadPair64WithNonTemporalAccess(RegisterID src, TrustedImm32 offset, RegisterID dest1, RegisterID dest2)
1133     {
1134         m_assembler.ldnp&lt;64&gt;(dest1, dest2, src, offset.m_value);
1135     }
1136 
1137     void abortWithReason(AbortReason reason)
1138     {
1139         // It is safe to use dataTempRegister directly since this is a crashing JIT Assert.
1140         move(TrustedImm32(reason), dataTempRegister);
1141         breakpoint();
1142     }
1143 
1144     void abortWithReason(AbortReason reason, intptr_t misc)
1145     {
1146         // It is safe to use memoryTempRegister directly since this is a crashing JIT Assert.
1147         move(TrustedImm64(misc), memoryTempRegister);
1148         abortWithReason(reason);
1149     }
1150 
1151     ConvertibleLoadLabel convertibleLoadPtr(Address address, RegisterID dest)
1152     {
1153         ConvertibleLoadLabel result(this);
1154         ASSERT(!(address.offset &amp; ~0xff8));
1155         m_assembler.ldr&lt;64&gt;(dest, address.base, address.offset);
1156         return result;
1157     }
1158 
1159     void load32(ImplicitAddress address, RegisterID dest)
1160     {
1161         if (tryLoadWithOffset&lt;32&gt;(dest, address.base, address.offset))
1162             return;
1163 
1164         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1165         m_assembler.ldr&lt;32&gt;(dest, address.base, memoryTempRegister);
1166     }
1167 
1168     void load32(BaseIndex address, RegisterID dest)
1169     {
1170         if (!address.offset &amp;&amp; (!address.scale || address.scale == 2)) {
1171             m_assembler.ldr&lt;32&gt;(dest, address.base, address.index, Assembler::UXTX, address.scale);
1172             return;
1173         }
1174 
1175         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1176         m_assembler.add&lt;64&gt;(memoryTempRegister, memoryTempRegister, address.index, Assembler::UXTX, address.scale);
1177         m_assembler.ldr&lt;32&gt;(dest, address.base, memoryTempRegister);
1178     }
1179 
1180     void load32(const void* address, RegisterID dest)
1181     {
1182         load&lt;32&gt;(address, dest);
1183     }
1184 
1185     DataLabel32 load32WithAddressOffsetPatch(Address address, RegisterID dest)
1186     {
1187         DataLabel32 label(this);
1188         signExtend32ToPtrWithFixedWidth(address.offset, getCachedMemoryTempRegisterIDAndInvalidate());
1189         m_assembler.ldr&lt;32&gt;(dest, address.base, memoryTempRegister, Assembler::SXTW, 0);
1190         return label;
1191     }
1192 
1193     DataLabelCompact load32WithCompactAddressOffsetPatch(Address address, RegisterID dest)
1194     {
1195         ASSERT(isCompactPtrAlignedAddressOffset(address.offset));
1196         DataLabelCompact label(this);
1197         m_assembler.ldr&lt;32&gt;(dest, address.base, address.offset);
1198         return label;
1199     }
1200 
1201     void load32WithUnalignedHalfWords(BaseIndex address, RegisterID dest)
1202     {
1203         load32(address, dest);
1204     }
1205 
1206     void load16(ImplicitAddress address, RegisterID dest)
1207     {
1208         if (tryLoadWithOffset&lt;16&gt;(dest, address.base, address.offset))
1209             return;
1210 
1211         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1212         m_assembler.ldrh(dest, address.base, memoryTempRegister);
1213     }
1214 
1215     void load16(BaseIndex address, RegisterID dest)
1216     {
1217         if (!address.offset &amp;&amp; (!address.scale || address.scale == 1)) {
1218             m_assembler.ldrh(dest, address.base, address.index, Assembler::UXTX, address.scale);
1219             return;
1220         }
1221 
1222         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1223         m_assembler.add&lt;64&gt;(memoryTempRegister, memoryTempRegister, address.index, Assembler::UXTX, address.scale);
1224         m_assembler.ldrh(dest, address.base, memoryTempRegister);
1225     }
1226 
1227     void load16(ExtendedAddress address, RegisterID dest)
1228     {
1229         moveToCachedReg(TrustedImmPtr(reinterpret_cast&lt;void*&gt;(address.offset)), cachedMemoryTempRegister());
1230         m_assembler.ldrh(dest, memoryTempRegister, address.base, Assembler::UXTX, 1);
1231         if (dest == memoryTempRegister)
1232             cachedMemoryTempRegister().invalidate();
1233     }
1234 
1235     void load16Unaligned(ImplicitAddress address, RegisterID dest)
1236     {
1237         load16(address, dest);
1238     }
1239 
1240     void load16Unaligned(BaseIndex address, RegisterID dest)
1241     {
1242         load16(address, dest);
1243     }
1244 
1245     void load16SignedExtendTo32(ImplicitAddress address, RegisterID dest)
1246     {
1247         if (tryLoadSignedWithOffset&lt;16&gt;(dest, address.base, address.offset))
1248             return;
1249 
1250         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1251         m_assembler.ldrsh&lt;32&gt;(dest, address.base, memoryTempRegister);
1252     }
1253 
1254     void load16SignedExtendTo32(BaseIndex address, RegisterID dest)
1255     {
1256         if (!address.offset &amp;&amp; (!address.scale || address.scale == 1)) {
1257             m_assembler.ldrsh&lt;32&gt;(dest, address.base, address.index, Assembler::UXTX, address.scale);
1258             return;
1259         }
1260 
1261         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1262         m_assembler.add&lt;64&gt;(memoryTempRegister, memoryTempRegister, address.index, Assembler::UXTX, address.scale);
1263         m_assembler.ldrsh&lt;32&gt;(dest, address.base, memoryTempRegister);
1264     }
1265 
1266     void zeroExtend16To32(RegisterID src, RegisterID dest)
1267     {
1268         m_assembler.uxth&lt;32&gt;(dest, src);
1269     }
1270 
1271     void signExtend16To32(RegisterID src, RegisterID dest)
1272     {
1273         m_assembler.sxth&lt;32&gt;(dest, src);
1274     }
1275 
1276     void load8(ImplicitAddress address, RegisterID dest)
1277     {
1278         if (tryLoadWithOffset&lt;8&gt;(dest, address.base, address.offset))
1279             return;
1280 
1281         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1282         m_assembler.ldrb(dest, address.base, memoryTempRegister);
1283     }
1284 
1285     void load8(BaseIndex address, RegisterID dest)
1286     {
1287         if (!address.offset &amp;&amp; !address.scale) {
1288             m_assembler.ldrb(dest, address.base, address.index, Assembler::UXTX, address.scale);
1289             return;
1290         }
1291 
1292         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1293         m_assembler.add&lt;64&gt;(memoryTempRegister, memoryTempRegister, address.index, Assembler::UXTX, address.scale);
1294         m_assembler.ldrb(dest, address.base, memoryTempRegister);
1295     }
1296 
1297     void load8(const void* address, RegisterID dest)
1298     {
1299         moveToCachedReg(TrustedImmPtr(address), cachedMemoryTempRegister());
1300         m_assembler.ldrb(dest, memoryTempRegister, ARM64Registers::zr);
1301         if (dest == memoryTempRegister)
1302             cachedMemoryTempRegister().invalidate();
1303     }
1304 
1305     void load8(RegisterID src, PostIndex simm, RegisterID dest)
1306     {
1307         m_assembler.ldrb(dest, src, simm);
1308     }
1309 
1310     void load8SignedExtendTo32(ImplicitAddress address, RegisterID dest)
1311     {
1312         if (tryLoadSignedWithOffset&lt;8&gt;(dest, address.base, address.offset))
1313             return;
1314 
1315         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1316         m_assembler.ldrsb&lt;32&gt;(dest, address.base, memoryTempRegister);
1317     }
1318 
1319     void load8SignedExtendTo32(BaseIndex address, RegisterID dest)
1320     {
1321         if (!address.offset &amp;&amp; !address.scale) {
1322             m_assembler.ldrsb&lt;32&gt;(dest, address.base, address.index, Assembler::UXTX, address.scale);
1323             return;
1324         }
1325 
1326         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1327         m_assembler.add&lt;64&gt;(memoryTempRegister, memoryTempRegister, address.index, Assembler::UXTX, address.scale);
1328         m_assembler.ldrsb&lt;32&gt;(dest, address.base, memoryTempRegister);
1329     }
1330 
1331     void load8SignedExtendTo32(const void* address, RegisterID dest)
1332     {
1333         moveToCachedReg(TrustedImmPtr(address), cachedMemoryTempRegister());
1334         m_assembler.ldrsb&lt;32&gt;(dest, memoryTempRegister, ARM64Registers::zr);
1335         if (dest == memoryTempRegister)
1336             cachedMemoryTempRegister().invalidate();
1337     }
1338 
1339     void zeroExtend8To32(RegisterID src, RegisterID dest)
1340     {
1341         m_assembler.uxtb&lt;32&gt;(dest, src);
1342     }
1343 
1344     void signExtend8To32(RegisterID src, RegisterID dest)
1345     {
1346         m_assembler.sxtb&lt;32&gt;(dest, src);
1347     }
1348 
1349     void store64(RegisterID src, ImplicitAddress address)
1350     {
1351         if (tryStoreWithOffset&lt;64&gt;(src, address.base, address.offset))
1352             return;
1353 
1354         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1355         m_assembler.str&lt;64&gt;(src, address.base, memoryTempRegister);
1356     }
1357 
1358     void store64(RegisterID src, BaseIndex address)
1359     {
1360         if (!address.offset &amp;&amp; (!address.scale || address.scale == 3)) {
1361             m_assembler.str&lt;64&gt;(src, address.base, address.index, Assembler::UXTX, address.scale);
1362             return;
1363         }
1364 
1365         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1366         m_assembler.add&lt;64&gt;(memoryTempRegister, memoryTempRegister, address.index, Assembler::UXTX, address.scale);
1367         m_assembler.str&lt;64&gt;(src, address.base, memoryTempRegister);
1368     }
1369 
1370     void store64(RegisterID src, const void* address)
1371     {
1372         store&lt;64&gt;(src, address);
1373     }
1374 
1375     void store64(TrustedImm32 imm, ImplicitAddress address)
1376     {
1377         store64(TrustedImm64(imm.m_value), address);
1378     }
1379 
1380     void store64(TrustedImm64 imm, ImplicitAddress address)
1381     {
1382         if (!imm.m_value) {
1383             store64(ARM64Registers::zr, address);
1384             return;
1385         }
1386 
1387         moveToCachedReg(imm, dataMemoryTempRegister());
1388         store64(dataTempRegister, address);
1389     }
1390 
1391     void store64(TrustedImm64 imm, BaseIndex address)
1392     {
1393         if (!imm.m_value) {
1394             store64(ARM64Registers::zr, address);
1395             return;
1396         }
1397 
1398         moveToCachedReg(imm, dataMemoryTempRegister());
1399         store64(dataTempRegister, address);
1400     }
1401 
1402     void store64(RegisterID src, RegisterID dest, PostIndex simm)
1403     {
1404         m_assembler.str&lt;64&gt;(src, dest, simm);
1405     }
1406 
1407     void storeZero64(ImplicitAddress address)
1408     {
1409         store64(ARM64Registers::zr, address);
1410     }
1411 
1412     void storeZero64(BaseIndex address)
1413     {
1414         store64(ARM64Registers::zr, address);
1415     }
1416 
1417     DataLabel32 store64WithAddressOffsetPatch(RegisterID src, Address address)
1418     {
1419         DataLabel32 label(this);
1420         signExtend32ToPtrWithFixedWidth(address.offset, getCachedMemoryTempRegisterIDAndInvalidate());
1421         m_assembler.str&lt;64&gt;(src, address.base, memoryTempRegister, Assembler::SXTW, 0);
1422         return label;
1423     }
1424 
1425     void storePair64(RegisterID src1, RegisterID src2, RegisterID dest)
1426     {
1427         storePair64(src1, src2, dest, TrustedImm32(0));
1428     }
1429 
1430     void storePair64(RegisterID src1, RegisterID src2, RegisterID dest, TrustedImm32 offset)
1431     {
1432         m_assembler.stp&lt;64&gt;(src1, src2, dest, offset.m_value);
1433     }
1434 
1435     void storePair64WithNonTemporalAccess(RegisterID src1, RegisterID src2, RegisterID dest)
1436     {
1437         storePair64WithNonTemporalAccess(src1, src2, dest, TrustedImm32(0));
1438     }
1439 
1440     void storePair64WithNonTemporalAccess(RegisterID src1, RegisterID src2, RegisterID dest, TrustedImm32 offset)
1441     {
1442         m_assembler.stnp&lt;64&gt;(src1, src2, dest, offset.m_value);
1443     }
1444 
1445     void store32(RegisterID src, ImplicitAddress address)
1446     {
1447         if (tryStoreWithOffset&lt;32&gt;(src, address.base, address.offset))
1448             return;
1449 
1450         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1451         m_assembler.str&lt;32&gt;(src, address.base, memoryTempRegister);
1452     }
1453 
1454     void store32(RegisterID src, BaseIndex address)
1455     {
1456         if (!address.offset &amp;&amp; (!address.scale || address.scale == 2)) {
1457             m_assembler.str&lt;32&gt;(src, address.base, address.index, Assembler::UXTX, address.scale);
1458             return;
1459         }
1460 
1461         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1462         m_assembler.add&lt;64&gt;(memoryTempRegister, memoryTempRegister, address.index, Assembler::UXTX, address.scale);
1463         m_assembler.str&lt;32&gt;(src, address.base, memoryTempRegister);
1464     }
1465 
1466     void store32(RegisterID src, const void* address)
1467     {
1468         store&lt;32&gt;(src, address);
1469     }
1470 
1471     void store32(TrustedImm32 imm, ImplicitAddress address)
1472     {
1473         if (!imm.m_value) {
1474             store32(ARM64Registers::zr, address);
1475             return;
1476         }
1477 
1478         moveToCachedReg(imm, dataMemoryTempRegister());
1479         store32(dataTempRegister, address);
1480     }
1481 
1482     void store32(TrustedImm32 imm, BaseIndex address)
1483     {
1484         if (!imm.m_value) {
1485             store32(ARM64Registers::zr, address);
1486             return;
1487         }
1488 
1489         moveToCachedReg(imm, dataMemoryTempRegister());
1490         store32(dataTempRegister, address);
1491     }
1492 
1493     void store32(TrustedImm32 imm, const void* address)
1494     {
1495         if (!imm.m_value) {
1496             store32(ARM64Registers::zr, address);
1497             return;
1498         }
1499 
1500         moveToCachedReg(imm, dataMemoryTempRegister());
1501         store32(dataTempRegister, address);
1502     }
1503 
1504     void storeZero32(ImplicitAddress address)
1505     {
1506         store32(ARM64Registers::zr, address);
1507     }
1508 
1509     void storeZero32(BaseIndex address)
1510     {
1511         store32(ARM64Registers::zr, address);
1512     }
1513 
1514     DataLabel32 store32WithAddressOffsetPatch(RegisterID src, Address address)
1515     {
1516         DataLabel32 label(this);
1517         signExtend32ToPtrWithFixedWidth(address.offset, getCachedMemoryTempRegisterIDAndInvalidate());
1518         m_assembler.str&lt;32&gt;(src, address.base, memoryTempRegister, Assembler::SXTW, 0);
1519         return label;
1520     }
1521 
1522     void store16(RegisterID src, ImplicitAddress address)
1523     {
1524         if (tryStoreWithOffset&lt;16&gt;(src, address.base, address.offset))
1525             return;
1526 
1527         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1528         m_assembler.strh(src, address.base, memoryTempRegister);
1529     }
1530 
1531     void store16(RegisterID src, BaseIndex address)
1532     {
1533         if (!address.offset &amp;&amp; (!address.scale || address.scale == 1)) {
1534             m_assembler.strh(src, address.base, address.index, Assembler::UXTX, address.scale);
1535             return;
1536         }
1537 
1538         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1539         m_assembler.add&lt;64&gt;(memoryTempRegister, memoryTempRegister, address.index, Assembler::UXTX, address.scale);
1540         m_assembler.strh(src, address.base, memoryTempRegister);
1541     }
1542 
1543     void storeZero16(ImplicitAddress address)
1544     {
1545         store16(ARM64Registers::zr, address);
1546     }
1547 
1548     void storeZero16(BaseIndex address)
1549     {
1550         store16(ARM64Registers::zr, address);
1551     }
1552 
1553     void store8(RegisterID src, BaseIndex address)
1554     {
1555         if (!address.offset &amp;&amp; !address.scale) {
1556             m_assembler.strb(src, address.base, address.index, Assembler::UXTX, address.scale);
1557             return;
1558         }
1559 
1560         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1561         m_assembler.add&lt;64&gt;(memoryTempRegister, memoryTempRegister, address.index, Assembler::UXTX, address.scale);
1562         m_assembler.strb(src, address.base, memoryTempRegister);
1563     }
1564 
1565     void store8(RegisterID src, void* address)
1566     {
1567         move(TrustedImmPtr(address), getCachedMemoryTempRegisterIDAndInvalidate());
1568         m_assembler.strb(src, memoryTempRegister, 0);
1569     }
1570 
1571     void store8(RegisterID src, ImplicitAddress address)
1572     {
1573         if (tryStoreWithOffset&lt;8&gt;(src, address.base, address.offset))
1574             return;
1575 
1576         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1577         m_assembler.strb(src, address.base, memoryTempRegister);
1578     }
1579 
1580     void store8(TrustedImm32 imm, void* address)
1581     {
1582         TrustedImm32 imm8(static_cast&lt;int8_t&gt;(imm.m_value));
1583         if (!imm8.m_value) {
1584             store8(ARM64Registers::zr, address);
1585             return;
1586         }
1587 
1588         move(imm8, getCachedDataTempRegisterIDAndInvalidate());
1589         store8(dataTempRegister, address);
1590     }
1591 
1592     void store8(TrustedImm32 imm, ImplicitAddress address)
1593     {
1594         TrustedImm32 imm8(static_cast&lt;int8_t&gt;(imm.m_value));
1595         if (!imm8.m_value) {
1596             store8(ARM64Registers::zr, address);
1597             return;
1598         }
1599 
1600         move(imm8, getCachedDataTempRegisterIDAndInvalidate());
1601         store8(dataTempRegister, address);
1602     }
1603 
1604     void store8(RegisterID src, RegisterID dest, PostIndex simm)
1605     {
1606         m_assembler.strb(src, dest, simm);
1607     }
1608 
1609     void getEffectiveAddress(BaseIndex address, RegisterID dest)
1610     {
1611         m_assembler.add&lt;64&gt;(dest, address.base, address.index, Assembler::LSL, address.scale);
1612         if (address.offset)
1613             add64(TrustedImm32(address.offset), dest);
1614     }
1615 
1616     // Floating-point operations:
1617 
1618     static bool supportsFloatingPoint() { return true; }
1619     static bool supportsFloatingPointTruncate() { return true; }
1620     static bool supportsFloatingPointSqrt() { return true; }
1621     static bool supportsFloatingPointAbs() { return true; }
1622     static bool supportsFloatingPointRounding() { return true; }
1623     static bool supportsCountPopulation() { return false; }
1624 
1625     enum BranchTruncateType { BranchIfTruncateFailed, BranchIfTruncateSuccessful };
1626 
1627     void absDouble(FPRegisterID src, FPRegisterID dest)
1628     {
1629         m_assembler.fabs&lt;64&gt;(dest, src);
1630     }
1631 
1632     void absFloat(FPRegisterID src, FPRegisterID dest)
1633     {
1634         m_assembler.fabs&lt;32&gt;(dest, src);
1635     }
1636 
1637     void addDouble(FPRegisterID src, FPRegisterID dest)
1638     {
1639         addDouble(dest, src, dest);
1640     }
1641 
1642     void addDouble(FPRegisterID op1, FPRegisterID op2, FPRegisterID dest)
1643     {
1644         m_assembler.fadd&lt;64&gt;(dest, op1, op2);
1645     }
1646 
1647     void addDouble(Address src, FPRegisterID dest)
1648     {
1649         loadDouble(src, fpTempRegister);
1650         addDouble(fpTempRegister, dest);
1651     }
1652 
1653     void addDouble(AbsoluteAddress address, FPRegisterID dest)
1654     {
1655         loadDouble(TrustedImmPtr(address.m_ptr), fpTempRegister);
1656         addDouble(fpTempRegister, dest);
1657     }
1658 
1659     void addFloat(FPRegisterID op1, FPRegisterID op2, FPRegisterID dest)
1660     {
1661         m_assembler.fadd&lt;32&gt;(dest, op1, op2);
1662     }
1663 
1664     void ceilDouble(FPRegisterID src, FPRegisterID dest)
1665     {
1666         m_assembler.frintp&lt;64&gt;(dest, src);
1667     }
1668 
1669     void ceilFloat(FPRegisterID src, FPRegisterID dest)
1670     {
1671         m_assembler.frintp&lt;32&gt;(dest, src);
1672     }
1673 
1674     void floorDouble(FPRegisterID src, FPRegisterID dest)
1675     {
1676         m_assembler.frintm&lt;64&gt;(dest, src);
1677     }
1678 
1679     void floorFloat(FPRegisterID src, FPRegisterID dest)
1680     {
1681         m_assembler.frintm&lt;32&gt;(dest, src);
1682     }
1683 
1684     void roundTowardNearestIntDouble(FPRegisterID src, FPRegisterID dest)
1685     {
1686         m_assembler.frintn&lt;64&gt;(dest, src);
1687     }
1688 
1689     void roundTowardNearestIntFloat(FPRegisterID src, FPRegisterID dest)
1690     {
1691         m_assembler.frintn&lt;32&gt;(dest, src);
1692     }
1693 
1694     void roundTowardZeroDouble(FPRegisterID src, FPRegisterID dest)
1695     {
1696         m_assembler.frintz&lt;64&gt;(dest, src);
1697     }
1698 
1699     void roundTowardZeroFloat(FPRegisterID src, FPRegisterID dest)
1700     {
1701         m_assembler.frintz&lt;32&gt;(dest, src);
1702     }
1703 
1704 
1705     // Convert &#39;src&#39; to an integer, and places the resulting &#39;dest&#39;.
1706     // If the result is not representable as a 32 bit value, branch.
1707     // May also branch for some values that are representable in 32 bits
1708     // (specifically, in this case, 0).
1709     void branchConvertDoubleToInt32(FPRegisterID src, RegisterID dest, JumpList&amp; failureCases, FPRegisterID, bool negZeroCheck = true)
1710     {
1711         m_assembler.fcvtns&lt;32, 64&gt;(dest, src);
1712 
1713         // Convert the integer result back to float &amp; compare to the original value - if not equal or unordered (NaN) then jump.
1714         m_assembler.scvtf&lt;64, 32&gt;(fpTempRegister, dest);
1715         failureCases.append(branchDouble(DoubleNotEqualOrUnordered, src, fpTempRegister));
1716 
1717         // Test for negative zero.
1718         if (negZeroCheck) {
1719             Jump valueIsNonZero = branchTest32(NonZero, dest);
1720             RegisterID scratch = getCachedMemoryTempRegisterIDAndInvalidate();
1721             m_assembler.fmov&lt;64&gt;(scratch, src);
1722             failureCases.append(makeTestBitAndBranch(scratch, 63, IsNonZero));
1723             valueIsNonZero.link(this);
1724         }
1725     }
1726 
1727     Jump branchDouble(DoubleCondition cond, FPRegisterID left, FPRegisterID right)
1728     {
1729         m_assembler.fcmp&lt;64&gt;(left, right);
1730         return jumpAfterFloatingPointCompare(cond);
1731     }
1732 
1733     Jump branchFloat(DoubleCondition cond, FPRegisterID left, FPRegisterID right)
1734     {
1735         m_assembler.fcmp&lt;32&gt;(left, right);
1736         return jumpAfterFloatingPointCompare(cond);
1737     }
1738 
1739     void compareDouble(DoubleCondition cond, FPRegisterID left, FPRegisterID right, RegisterID dest)
1740     {
1741         floatingPointCompare(cond, left, right, dest, [this] (FPRegisterID arg1, FPRegisterID arg2) {
1742             m_assembler.fcmp&lt;64&gt;(arg1, arg2);
1743         });
1744     }
1745 
1746     void compareFloat(DoubleCondition cond, FPRegisterID left, FPRegisterID right, RegisterID dest)
1747     {
1748         floatingPointCompare(cond, left, right, dest, [this] (FPRegisterID arg1, FPRegisterID arg2) {
1749             m_assembler.fcmp&lt;32&gt;(arg1, arg2);
1750         });
1751     }
1752 
1753     Jump branchDoubleNonZero(FPRegisterID reg, FPRegisterID)
1754     {
1755         m_assembler.fcmp_0&lt;64&gt;(reg);
1756         Jump unordered = makeBranch(Assembler::ConditionVS);
1757         Jump result = makeBranch(Assembler::ConditionNE);
1758         unordered.link(this);
1759         return result;
1760     }
1761 
1762     Jump branchDoubleZeroOrNaN(FPRegisterID reg, FPRegisterID)
1763     {
1764         m_assembler.fcmp_0&lt;64&gt;(reg);
1765         Jump unordered = makeBranch(Assembler::ConditionVS);
1766         Jump notEqual = makeBranch(Assembler::ConditionNE);
1767         unordered.link(this);
1768         // We get here if either unordered or equal.
1769         Jump result = jump();
1770         notEqual.link(this);
1771         return result;
1772     }
1773 
1774     Jump branchTruncateDoubleToInt32(FPRegisterID src, RegisterID dest, BranchTruncateType branchType = BranchIfTruncateFailed)
1775     {
1776         // Truncate to a 64-bit integer in dataTempRegister, copy the low 32-bit to dest.
1777         m_assembler.fcvtzs&lt;64, 64&gt;(getCachedDataTempRegisterIDAndInvalidate(), src);
1778         zeroExtend32ToPtr(dataTempRegister, dest);
1779         // Check the low 32-bits sign extend to be equal to the full value.
1780         m_assembler.cmp&lt;64&gt;(dataTempRegister, dataTempRegister, Assembler::SXTW, 0);
1781         return Jump(makeBranch(branchType == BranchIfTruncateSuccessful ? Equal : NotEqual));
1782     }
1783 
1784     void convertDoubleToFloat(FPRegisterID src, FPRegisterID dest)
1785     {
1786         m_assembler.fcvt&lt;32, 64&gt;(dest, src);
1787     }
1788 
1789     void convertFloatToDouble(FPRegisterID src, FPRegisterID dest)
1790     {
1791         m_assembler.fcvt&lt;64, 32&gt;(dest, src);
1792     }
1793 
1794     void convertInt32ToDouble(TrustedImm32 imm, FPRegisterID dest)
1795     {
1796         move(imm, getCachedDataTempRegisterIDAndInvalidate());
1797         convertInt32ToDouble(dataTempRegister, dest);
1798     }
1799 
1800     void convertInt32ToDouble(RegisterID src, FPRegisterID dest)
1801     {
1802         m_assembler.scvtf&lt;64, 32&gt;(dest, src);
1803     }
1804 
1805     void convertInt32ToDouble(Address address, FPRegisterID dest)
1806     {
1807         load32(address, getCachedDataTempRegisterIDAndInvalidate());
1808         convertInt32ToDouble(dataTempRegister, dest);
1809     }
1810 
1811     void convertInt32ToDouble(AbsoluteAddress address, FPRegisterID dest)
1812     {
1813         load32(address.m_ptr, getCachedDataTempRegisterIDAndInvalidate());
1814         convertInt32ToDouble(dataTempRegister, dest);
1815     }
1816 
1817     void convertInt32ToFloat(RegisterID src, FPRegisterID dest)
1818     {
1819         m_assembler.scvtf&lt;32, 32&gt;(dest, src);
1820     }
1821 
1822     void convertInt64ToDouble(RegisterID src, FPRegisterID dest)
1823     {
1824         m_assembler.scvtf&lt;64, 64&gt;(dest, src);
1825     }
1826 
1827     void convertInt64ToFloat(RegisterID src, FPRegisterID dest)
1828     {
1829         m_assembler.scvtf&lt;32, 64&gt;(dest, src);
1830     }
1831 
1832     void convertUInt64ToDouble(RegisterID src, FPRegisterID dest)
1833     {
1834         m_assembler.ucvtf&lt;64, 64&gt;(dest, src);
1835     }
1836 
1837     void convertUInt64ToFloat(RegisterID src, FPRegisterID dest)
1838     {
1839         m_assembler.ucvtf&lt;32, 64&gt;(dest, src);
1840     }
1841 
1842     void divDouble(FPRegisterID src, FPRegisterID dest)
1843     {
1844         divDouble(dest, src, dest);
1845     }
1846 
1847     void divDouble(FPRegisterID op1, FPRegisterID op2, FPRegisterID dest)
1848     {
1849         m_assembler.fdiv&lt;64&gt;(dest, op1, op2);
1850     }
1851 
1852     void divFloat(FPRegisterID op1, FPRegisterID op2, FPRegisterID dest)
1853     {
1854         m_assembler.fdiv&lt;32&gt;(dest, op1, op2);
1855     }
1856 
1857     void loadDouble(ImplicitAddress address, FPRegisterID dest)
1858     {
1859         if (tryLoadWithOffset&lt;64&gt;(dest, address.base, address.offset))
1860             return;
1861 
1862         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1863         m_assembler.ldr&lt;64&gt;(dest, address.base, memoryTempRegister);
1864     }
1865 
1866     void loadDouble(BaseIndex address, FPRegisterID dest)
1867     {
1868         if (!address.offset &amp;&amp; (!address.scale || address.scale == 3)) {
1869             m_assembler.ldr&lt;64&gt;(dest, address.base, address.index, Assembler::UXTX, address.scale);
1870             return;
1871         }
1872 
1873         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1874         m_assembler.add&lt;64&gt;(memoryTempRegister, memoryTempRegister, address.index, Assembler::UXTX, address.scale);
1875         m_assembler.ldr&lt;64&gt;(dest, address.base, memoryTempRegister);
1876     }
1877 
1878     void loadDouble(TrustedImmPtr address, FPRegisterID dest)
1879     {
1880         moveToCachedReg(address, cachedMemoryTempRegister());
1881         m_assembler.ldr&lt;64&gt;(dest, memoryTempRegister, ARM64Registers::zr);
1882     }
1883 
1884     void loadFloat(ImplicitAddress address, FPRegisterID dest)
1885     {
1886         if (tryLoadWithOffset&lt;32&gt;(dest, address.base, address.offset))
1887             return;
1888 
1889         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1890         m_assembler.ldr&lt;32&gt;(dest, address.base, memoryTempRegister);
1891     }
1892 
1893     void loadFloat(BaseIndex address, FPRegisterID dest)
1894     {
1895         if (!address.offset &amp;&amp; (!address.scale || address.scale == 2)) {
1896             m_assembler.ldr&lt;32&gt;(dest, address.base, address.index, Assembler::UXTX, address.scale);
1897             return;
1898         }
1899 
1900         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
1901         m_assembler.add&lt;64&gt;(memoryTempRegister, memoryTempRegister, address.index, Assembler::UXTX, address.scale);
1902         m_assembler.ldr&lt;32&gt;(dest, address.base, memoryTempRegister);
1903     }
1904 
1905     void loadFloat(TrustedImmPtr address, FPRegisterID dest)
1906     {
1907         moveToCachedReg(address, cachedMemoryTempRegister());
1908         m_assembler.ldr&lt;32&gt;(dest, memoryTempRegister, ARM64Registers::zr);
1909     }
1910 
1911     void moveDouble(FPRegisterID src, FPRegisterID dest)
1912     {
1913         m_assembler.fmov&lt;64&gt;(dest, src);
1914     }
1915 
1916     void moveZeroToDouble(FPRegisterID reg)
1917     {
1918         m_assembler.fmov&lt;64&gt;(reg, ARM64Registers::zr);
1919     }
1920 
1921     void moveDoubleTo64(FPRegisterID src, RegisterID dest)
1922     {
1923         m_assembler.fmov&lt;64&gt;(dest, src);
1924     }
1925 
1926     void moveFloatTo32(FPRegisterID src, RegisterID dest)
1927     {
1928         m_assembler.fmov&lt;32&gt;(dest, src);
1929     }
1930 
1931     void move64ToDouble(RegisterID src, FPRegisterID dest)
1932     {
1933         m_assembler.fmov&lt;64&gt;(dest, src);
1934     }
1935 
1936     void move32ToFloat(RegisterID src, FPRegisterID dest)
1937     {
1938         m_assembler.fmov&lt;32&gt;(dest, src);
1939     }
1940 
1941     void moveConditionallyDouble(DoubleCondition cond, FPRegisterID left, FPRegisterID right, RegisterID src, RegisterID dest)
1942     {
1943         m_assembler.fcmp&lt;64&gt;(left, right);
1944         moveConditionallyAfterFloatingPointCompare&lt;64&gt;(cond, src, dest);
1945     }
1946 
1947     void moveConditionallyDouble(DoubleCondition cond, FPRegisterID left, FPRegisterID right, RegisterID thenCase, RegisterID elseCase, RegisterID dest)
1948     {
1949         m_assembler.fcmp&lt;64&gt;(left, right);
1950         moveConditionallyAfterFloatingPointCompare&lt;64&gt;(cond, thenCase, elseCase, dest);
1951     }
1952 
1953     void moveConditionallyFloat(DoubleCondition cond, FPRegisterID left, FPRegisterID right, RegisterID src, RegisterID dest)
1954     {
1955         m_assembler.fcmp&lt;32&gt;(left, right);
1956         moveConditionallyAfterFloatingPointCompare&lt;64&gt;(cond, src, dest);
1957     }
1958 
1959     void moveConditionallyFloat(DoubleCondition cond, FPRegisterID left, FPRegisterID right, RegisterID thenCase, RegisterID elseCase, RegisterID dest)
1960     {
1961         m_assembler.fcmp&lt;32&gt;(left, right);
1962         moveConditionallyAfterFloatingPointCompare&lt;64&gt;(cond, thenCase, elseCase, dest);
1963     }
1964 
1965     template&lt;int datasize&gt;
1966     void moveConditionallyAfterFloatingPointCompare(DoubleCondition cond, RegisterID src, RegisterID dest)
1967     {
1968         if (cond == DoubleNotEqual) {
1969             Jump unordered = makeBranch(Assembler::ConditionVS);
1970             m_assembler.csel&lt;datasize&gt;(dest, src, dest, Assembler::ConditionNE);
1971             unordered.link(this);
1972             return;
1973         }
1974         if (cond == DoubleEqualOrUnordered) {
1975             // If the compare is unordered, src is copied to dest and the
1976             // next csel has all arguments equal to src.
1977             // If the compare is ordered, dest is unchanged and EQ decides
1978             // what value to set.
1979             m_assembler.csel&lt;datasize&gt;(dest, src, dest, Assembler::ConditionVS);
1980             m_assembler.csel&lt;datasize&gt;(dest, src, dest, Assembler::ConditionEQ);
1981             return;
1982         }
1983         m_assembler.csel&lt;datasize&gt;(dest, src, dest, ARM64Condition(cond));
1984     }
1985 
1986     template&lt;int datasize&gt;
1987     void moveConditionallyAfterFloatingPointCompare(DoubleCondition cond, RegisterID thenCase, RegisterID elseCase, RegisterID dest)
1988     {
1989         if (cond == DoubleNotEqual) {
1990             Jump unordered = makeBranch(Assembler::ConditionVS);
1991             m_assembler.csel&lt;datasize&gt;(dest, thenCase, elseCase, Assembler::ConditionNE);
1992             unordered.link(this);
1993             return;
1994         }
1995         if (cond == DoubleEqualOrUnordered) {
1996             // If the compare is unordered, thenCase is copied to elseCase and the
1997             // next csel has all arguments equal to thenCase.
1998             // If the compare is ordered, dest is unchanged and EQ decides
1999             // what value to set.
2000             m_assembler.csel&lt;datasize&gt;(elseCase, thenCase, elseCase, Assembler::ConditionVS);
2001             m_assembler.csel&lt;datasize&gt;(dest, thenCase, elseCase, Assembler::ConditionEQ);
2002             return;
2003         }
2004         m_assembler.csel&lt;datasize&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2005     }
2006 
2007     template&lt;int datasize&gt;
2008     void moveDoubleConditionallyAfterFloatingPointCompare(DoubleCondition cond, FPRegisterID thenCase, FPRegisterID elseCase, FPRegisterID dest)
2009     {
2010         if (cond == DoubleNotEqual) {
2011             Jump unordered = makeBranch(Assembler::ConditionVS);
2012             m_assembler.fcsel&lt;datasize&gt;(dest, thenCase, elseCase, Assembler::ConditionNE);
2013             unordered.link(this);
2014             return;
2015         }
2016         if (cond == DoubleEqualOrUnordered) {
2017             // If the compare is unordered, thenCase is copied to elseCase and the
2018             // next csel has all arguments equal to thenCase.
2019             // If the compare is ordered, dest is unchanged and EQ decides
2020             // what value to set.
2021             m_assembler.fcsel&lt;datasize&gt;(elseCase, thenCase, elseCase, Assembler::ConditionVS);
2022             m_assembler.fcsel&lt;datasize&gt;(dest, thenCase, elseCase, Assembler::ConditionEQ);
2023             return;
2024         }
2025         m_assembler.fcsel&lt;datasize&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2026     }
2027 
2028     void moveDoubleConditionallyDouble(DoubleCondition cond, FPRegisterID left, FPRegisterID right, FPRegisterID thenCase, FPRegisterID elseCase, FPRegisterID dest)
2029     {
2030         m_assembler.fcmp&lt;64&gt;(left, right);
2031         moveDoubleConditionallyAfterFloatingPointCompare&lt;64&gt;(cond, thenCase, elseCase, dest);
2032     }
2033 
2034     void moveDoubleConditionallyFloat(DoubleCondition cond, FPRegisterID left, FPRegisterID right, FPRegisterID thenCase, FPRegisterID elseCase, FPRegisterID dest)
2035     {
2036         m_assembler.fcmp&lt;32&gt;(left, right);
2037         moveDoubleConditionallyAfterFloatingPointCompare&lt;64&gt;(cond, thenCase, elseCase, dest);
2038     }
2039 
2040     void mulDouble(FPRegisterID src, FPRegisterID dest)
2041     {
2042         mulDouble(dest, src, dest);
2043     }
2044 
2045     void mulDouble(FPRegisterID op1, FPRegisterID op2, FPRegisterID dest)
2046     {
2047         m_assembler.fmul&lt;64&gt;(dest, op1, op2);
2048     }
2049 
2050     void mulDouble(Address src, FPRegisterID dest)
2051     {
2052         loadDouble(src, fpTempRegister);
2053         mulDouble(fpTempRegister, dest);
2054     }
2055 
2056     void mulFloat(FPRegisterID op1, FPRegisterID op2, FPRegisterID dest)
2057     {
2058         m_assembler.fmul&lt;32&gt;(dest, op1, op2);
2059     }
2060 
2061     void andDouble(FPRegisterID op1, FPRegisterID op2, FPRegisterID dest)
2062     {
2063         m_assembler.vand&lt;64&gt;(dest, op1, op2);
2064     }
2065 
2066     void andFloat(FPRegisterID op1, FPRegisterID op2, FPRegisterID dest)
2067     {
2068         andDouble(op1, op2, dest);
2069     }
2070 
2071     void orDouble(FPRegisterID op1, FPRegisterID op2, FPRegisterID dest)
2072     {
2073         m_assembler.vorr&lt;64&gt;(dest, op1, op2);
2074     }
2075 
2076     void orFloat(FPRegisterID op1, FPRegisterID op2, FPRegisterID dest)
2077     {
2078         orDouble(op1, op2, dest);
2079     }
2080 
2081     void negateDouble(FPRegisterID src, FPRegisterID dest)
2082     {
2083         m_assembler.fneg&lt;64&gt;(dest, src);
2084     }
2085 
2086     void negateFloat(FPRegisterID src, FPRegisterID dest)
2087     {
2088         m_assembler.fneg&lt;32&gt;(dest, src);
2089     }
2090 
2091     void sqrtDouble(FPRegisterID src, FPRegisterID dest)
2092     {
2093         m_assembler.fsqrt&lt;64&gt;(dest, src);
2094     }
2095 
2096     void sqrtFloat(FPRegisterID src, FPRegisterID dest)
2097     {
2098         m_assembler.fsqrt&lt;32&gt;(dest, src);
2099     }
2100 
2101     void storeDouble(FPRegisterID src, ImplicitAddress address)
2102     {
2103         if (tryStoreWithOffset&lt;64&gt;(src, address.base, address.offset))
2104             return;
2105 
2106         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
2107         m_assembler.str&lt;64&gt;(src, address.base, memoryTempRegister);
2108     }
2109 
2110     void storeDouble(FPRegisterID src, TrustedImmPtr address)
2111     {
2112         moveToCachedReg(address, cachedMemoryTempRegister());
2113         m_assembler.str&lt;64&gt;(src, memoryTempRegister, ARM64Registers::zr);
2114     }
2115 
2116     void storeDouble(FPRegisterID src, BaseIndex address)
2117     {
2118         if (!address.offset &amp;&amp; (!address.scale || address.scale == 3)) {
2119             m_assembler.str&lt;64&gt;(src, address.base, address.index, Assembler::UXTX, address.scale);
2120             return;
2121         }
2122 
2123         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
2124         m_assembler.add&lt;64&gt;(memoryTempRegister, memoryTempRegister, address.index, Assembler::UXTX, address.scale);
2125         m_assembler.str&lt;64&gt;(src, address.base, memoryTempRegister);
2126     }
2127 
2128     void storeFloat(FPRegisterID src, ImplicitAddress address)
2129     {
2130         if (tryStoreWithOffset&lt;32&gt;(src, address.base, address.offset))
2131             return;
2132 
2133         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
2134         m_assembler.str&lt;32&gt;(src, address.base, memoryTempRegister);
2135     }
2136 
2137     void storeFloat(FPRegisterID src, BaseIndex address)
2138     {
2139         if (!address.offset &amp;&amp; (!address.scale || address.scale == 2)) {
2140             m_assembler.str&lt;32&gt;(src, address.base, address.index, Assembler::UXTX, address.scale);
2141             return;
2142         }
2143 
2144         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
2145         m_assembler.add&lt;64&gt;(memoryTempRegister, memoryTempRegister, address.index, Assembler::UXTX, address.scale);
2146         m_assembler.str&lt;32&gt;(src, address.base, memoryTempRegister);
2147     }
2148 
2149     void subDouble(FPRegisterID src, FPRegisterID dest)
2150     {
2151         subDouble(dest, src, dest);
2152     }
2153 
2154     void subDouble(FPRegisterID op1, FPRegisterID op2, FPRegisterID dest)
2155     {
2156         m_assembler.fsub&lt;64&gt;(dest, op1, op2);
2157     }
2158 
2159     void subDouble(Address src, FPRegisterID dest)
2160     {
2161         loadDouble(src, fpTempRegister);
2162         subDouble(fpTempRegister, dest);
2163     }
2164 
2165     void subFloat(FPRegisterID op1, FPRegisterID op2, FPRegisterID dest)
2166     {
2167         m_assembler.fsub&lt;32&gt;(dest, op1, op2);
2168     }
2169 
2170     // Result is undefined if the value is outside of the integer range.
2171     void truncateDoubleToInt32(FPRegisterID src, RegisterID dest)
2172     {
2173         m_assembler.fcvtzs&lt;32, 64&gt;(dest, src);
2174     }
2175 
2176     void truncateDoubleToUint32(FPRegisterID src, RegisterID dest)
2177     {
2178         m_assembler.fcvtzu&lt;32, 64&gt;(dest, src);
2179     }
2180 
2181     void truncateDoubleToInt64(FPRegisterID src, RegisterID dest)
2182     {
2183         m_assembler.fcvtzs&lt;64, 64&gt;(dest, src);
2184     }
2185 
2186     void truncateDoubleToUint64(FPRegisterID src, RegisterID dest, FPRegisterID, FPRegisterID)
2187     {
2188         truncateDoubleToUint64(src, dest);
2189     }
2190 
2191     void truncateDoubleToUint64(FPRegisterID src, RegisterID dest)
2192     {
2193         m_assembler.fcvtzu&lt;64, 64&gt;(dest, src);
2194     }
2195 
2196     void truncateFloatToInt32(FPRegisterID src, RegisterID dest)
2197     {
2198         m_assembler.fcvtzs&lt;32, 32&gt;(dest, src);
2199     }
2200 
2201     void truncateFloatToUint32(FPRegisterID src, RegisterID dest)
2202     {
2203         m_assembler.fcvtzu&lt;32, 32&gt;(dest, src);
2204     }
2205 
2206     void truncateFloatToInt64(FPRegisterID src, RegisterID dest)
2207     {
2208         m_assembler.fcvtzs&lt;64, 32&gt;(dest, src);
2209     }
2210 
2211     void truncateFloatToUint64(FPRegisterID src, RegisterID dest, FPRegisterID, FPRegisterID)
2212     {
2213         truncateFloatToUint64(src, dest);
2214     }
2215 
2216     void truncateFloatToUint64(FPRegisterID src, RegisterID dest)
2217     {
2218         m_assembler.fcvtzu&lt;64, 32&gt;(dest, src);
2219     }
2220 
2221     // Stack manipulation operations:
2222     //
2223     // The ABI is assumed to provide a stack abstraction to memory,
2224     // containing machine word sized units of data. Push and pop
2225     // operations add and remove a single register sized unit of data
2226     // to or from the stack. These operations are not supported on
2227     // ARM64. Peek and poke operations read or write values on the
2228     // stack, without moving the current stack position. Additionally,
2229     // there are popToRestore and pushToSave operations, which are
2230     // designed just for quick-and-dirty saving and restoring of
2231     // temporary values. These operations don&#39;t claim to have any
2232     // ABI compatibility.
2233 
2234     void pop(RegisterID) NO_RETURN_DUE_TO_CRASH
2235     {
2236         CRASH();
2237     }
2238 
2239     void push(RegisterID) NO_RETURN_DUE_TO_CRASH
2240     {
2241         CRASH();
2242     }
2243 
2244     void push(Address) NO_RETURN_DUE_TO_CRASH
2245     {
2246         CRASH();
2247     }
2248 
2249     void push(TrustedImm32) NO_RETURN_DUE_TO_CRASH
2250     {
2251         CRASH();
2252     }
2253 
2254     void popPair(RegisterID dest1, RegisterID dest2)
2255     {
2256         m_assembler.ldp&lt;64&gt;(dest1, dest2, ARM64Registers::sp, PairPostIndex(16));
2257     }
2258 
2259     void pushPair(RegisterID src1, RegisterID src2)
2260     {
2261         m_assembler.stp&lt;64&gt;(src1, src2, ARM64Registers::sp, PairPreIndex(-16));
2262     }
2263 
2264     void popToRestore(RegisterID dest)
2265     {
2266         m_assembler.ldr&lt;64&gt;(dest, ARM64Registers::sp, PostIndex(16));
2267     }
2268 
2269     void pushToSave(RegisterID src)
2270     {
2271         m_assembler.str&lt;64&gt;(src, ARM64Registers::sp, PreIndex(-16));
2272     }
2273 
2274     void pushToSaveImmediateWithoutTouchingRegisters(TrustedImm32 imm)
2275     {
2276         // We can use any non-hardware reserved register here since we restore its value.
2277         // We pick dataTempRegister arbitrarily. We don&#39;t need to invalidate it here since
2278         // we restore its original value.
2279         RegisterID reg = dataTempRegister;
2280 
2281         pushPair(reg, reg);
2282         move(imm, reg);
2283         store64(reg, stackPointerRegister);
2284         load64(Address(stackPointerRegister, 8), reg);
2285     }
2286 
2287     void pushToSave(Address address)
2288     {
2289         load32(address, getCachedDataTempRegisterIDAndInvalidate());
2290         pushToSave(dataTempRegister);
2291     }
2292 
2293     void pushToSave(TrustedImm32 imm)
2294     {
2295         move(imm, getCachedDataTempRegisterIDAndInvalidate());
2296         pushToSave(dataTempRegister);
2297     }
2298 
2299     void popToRestore(FPRegisterID dest)
2300     {
2301         loadDouble(stackPointerRegister, dest);
2302         add64(TrustedImm32(16), stackPointerRegister);
2303     }
2304 
2305     void pushToSave(FPRegisterID src)
2306     {
2307         sub64(TrustedImm32(16), stackPointerRegister);
2308         storeDouble(src, stackPointerRegister);
2309     }
2310 
2311     static ptrdiff_t pushToSaveByteOffset() { return 16; }
2312 
2313     // Register move operations:
2314 
2315     void move(RegisterID src, RegisterID dest)
2316     {
2317         if (src != dest)
2318             m_assembler.mov&lt;64&gt;(dest, src);
2319     }
2320 
2321     void move(TrustedImm32 imm, RegisterID dest)
2322     {
2323         moveInternal&lt;TrustedImm32, int32_t&gt;(imm, dest);
2324     }
2325 
2326     void move(TrustedImmPtr imm, RegisterID dest)
2327     {
2328         moveInternal&lt;TrustedImmPtr, intptr_t&gt;(imm, dest);
2329     }
2330 
2331     void move(TrustedImm64 imm, RegisterID dest)
2332     {
2333         moveInternal&lt;TrustedImm64, int64_t&gt;(imm, dest);
2334     }
2335 
2336     void swap(RegisterID reg1, RegisterID reg2)
2337     {
2338         move(reg1, getCachedDataTempRegisterIDAndInvalidate());
2339         move(reg2, reg1);
2340         move(dataTempRegister, reg2);
2341     }
2342 
2343     void swap(FPRegisterID reg1, FPRegisterID reg2)
2344     {
2345         moveDouble(reg1, fpTempRegister);
2346         moveDouble(reg2, reg1);
2347         moveDouble(fpTempRegister, reg2);
2348     }
2349 
2350     void signExtend32ToPtr(TrustedImm32 imm, RegisterID dest)
2351     {
2352         move(TrustedImmPtr(reinterpret_cast&lt;void*&gt;(static_cast&lt;intptr_t&gt;(imm.m_value))), dest);
2353     }
2354 
2355     void signExtend32ToPtr(RegisterID src, RegisterID dest)
2356     {
2357         m_assembler.sxtw(dest, src);
2358     }
2359 
2360     void zeroExtend32ToPtr(RegisterID src, RegisterID dest)
2361     {
2362         m_assembler.uxtw(dest, src);
2363     }
2364 
2365     void moveConditionally32(RelationalCondition cond, RegisterID left, RegisterID right, RegisterID src, RegisterID dest)
2366     {
2367         m_assembler.cmp&lt;32&gt;(left, right);
2368         m_assembler.csel&lt;64&gt;(dest, src, dest, ARM64Condition(cond));
2369     }
2370 
2371     void moveConditionally32(RelationalCondition cond, RegisterID left, RegisterID right, RegisterID thenCase, RegisterID elseCase, RegisterID dest)
2372     {
2373         m_assembler.cmp&lt;32&gt;(left, right);
2374         m_assembler.csel&lt;64&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2375     }
2376 
2377     void moveConditionally32(RelationalCondition cond, RegisterID left, TrustedImm32 right, RegisterID thenCase, RegisterID elseCase, RegisterID dest)
2378     {
2379         if (!right.m_value) {
2380             if (auto resultCondition = commuteCompareToZeroIntoTest(cond)) {
2381                 moveConditionallyTest32(*resultCondition, left, left, thenCase, elseCase, dest);
2382                 return;
2383             }
2384         }
2385 
2386         if (isUInt12(right.m_value))
2387             m_assembler.cmp&lt;32&gt;(left, UInt12(right.m_value));
2388         else if (isUInt12(-right.m_value))
2389             m_assembler.cmn&lt;32&gt;(left, UInt12(-right.m_value));
2390         else {
2391             moveToCachedReg(right, dataMemoryTempRegister());
2392             m_assembler.cmp&lt;32&gt;(left, dataTempRegister);
2393         }
2394         m_assembler.csel&lt;64&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2395     }
2396 
2397     void moveConditionally64(RelationalCondition cond, RegisterID left, RegisterID right, RegisterID src, RegisterID dest)
2398     {
2399         m_assembler.cmp&lt;64&gt;(left, right);
2400         m_assembler.csel&lt;64&gt;(dest, src, dest, ARM64Condition(cond));
2401     }
2402 
2403     void moveConditionally64(RelationalCondition cond, RegisterID left, RegisterID right, RegisterID thenCase, RegisterID elseCase, RegisterID dest)
2404     {
2405         m_assembler.cmp&lt;64&gt;(left, right);
2406         m_assembler.csel&lt;64&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2407     }
2408 
2409     void moveConditionally64(RelationalCondition cond, RegisterID left, TrustedImm32 right, RegisterID thenCase, RegisterID elseCase, RegisterID dest)
2410     {
2411         if (!right.m_value) {
2412             if (auto resultCondition = commuteCompareToZeroIntoTest(cond)) {
2413                 moveConditionallyTest64(*resultCondition, left, left, thenCase, elseCase, dest);
2414                 return;
2415             }
2416         }
2417 
2418         if (isUInt12(right.m_value))
2419             m_assembler.cmp&lt;64&gt;(left, UInt12(right.m_value));
2420         else if (isUInt12(-right.m_value))
2421             m_assembler.cmn&lt;64&gt;(left, UInt12(-right.m_value));
2422         else {
2423             moveToCachedReg(right, dataMemoryTempRegister());
2424             m_assembler.cmp&lt;64&gt;(left, dataTempRegister);
2425         }
2426         m_assembler.csel&lt;64&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2427     }
2428 
2429     void moveConditionallyTest32(ResultCondition cond, RegisterID testReg, RegisterID mask, RegisterID src, RegisterID dest)
2430     {
2431         m_assembler.tst&lt;32&gt;(testReg, mask);
2432         m_assembler.csel&lt;64&gt;(dest, src, dest, ARM64Condition(cond));
2433     }
2434 
2435     void moveConditionallyTest32(ResultCondition cond, RegisterID left, RegisterID right, RegisterID thenCase, RegisterID elseCase, RegisterID dest)
2436     {
2437         m_assembler.tst&lt;32&gt;(left, right);
2438         m_assembler.csel&lt;64&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2439     }
2440 
2441     void moveConditionallyTest32(ResultCondition cond, RegisterID left, TrustedImm32 right, RegisterID thenCase, RegisterID elseCase, RegisterID dest)
2442     {
2443         test32(left, right);
2444         m_assembler.csel&lt;64&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2445     }
2446 
2447     void moveConditionallyTest64(ResultCondition cond, RegisterID testReg, RegisterID mask, RegisterID src, RegisterID dest)
2448     {
2449         m_assembler.tst&lt;64&gt;(testReg, mask);
2450         m_assembler.csel&lt;64&gt;(dest, src, dest, ARM64Condition(cond));
2451     }
2452 
2453     void moveConditionallyTest64(ResultCondition cond, RegisterID left, RegisterID right, RegisterID thenCase, RegisterID elseCase, RegisterID dest)
2454     {
2455         m_assembler.tst&lt;64&gt;(left, right);
2456         m_assembler.csel&lt;64&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2457     }
2458 
2459     void moveDoubleConditionally32(RelationalCondition cond, RegisterID left, RegisterID right, FPRegisterID thenCase, FPRegisterID elseCase, FPRegisterID dest)
2460     {
2461         m_assembler.cmp&lt;32&gt;(left, right);
2462         m_assembler.fcsel&lt;32&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2463     }
2464 
2465     void moveDoubleConditionally32(RelationalCondition cond, RegisterID left, TrustedImm32 right, FPRegisterID thenCase, FPRegisterID elseCase, FPRegisterID dest)
2466     {
2467         if (!right.m_value) {
2468             if (auto resultCondition = commuteCompareToZeroIntoTest(cond)) {
2469                 moveDoubleConditionallyTest32(*resultCondition, left, left, thenCase, elseCase, dest);
2470                 return;
2471             }
2472         }
2473 
2474         if (isUInt12(right.m_value))
2475             m_assembler.cmp&lt;32&gt;(left, UInt12(right.m_value));
2476         else if (isUInt12(-right.m_value))
2477             m_assembler.cmn&lt;32&gt;(left, UInt12(-right.m_value));
2478         else {
2479             moveToCachedReg(right, dataMemoryTempRegister());
2480             m_assembler.cmp&lt;32&gt;(left, dataTempRegister);
2481         }
2482         m_assembler.fcsel&lt;64&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2483     }
2484 
2485     void moveDoubleConditionally64(RelationalCondition cond, RegisterID left, RegisterID right, FPRegisterID thenCase, FPRegisterID elseCase, FPRegisterID dest)
2486     {
2487         m_assembler.cmp&lt;64&gt;(left, right);
2488         m_assembler.fcsel&lt;64&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2489     }
2490 
2491     void moveDoubleConditionally64(RelationalCondition cond, RegisterID left, TrustedImm32 right, FPRegisterID thenCase, FPRegisterID elseCase, FPRegisterID dest)
2492     {
2493         if (!right.m_value) {
2494             if (auto resultCondition = commuteCompareToZeroIntoTest(cond)) {
2495                 moveDoubleConditionallyTest64(*resultCondition, left, left, thenCase, elseCase, dest);
2496                 return;
2497             }
2498         }
2499 
2500         if (isUInt12(right.m_value))
2501             m_assembler.cmp&lt;64&gt;(left, UInt12(right.m_value));
2502         else if (isUInt12(-right.m_value))
2503             m_assembler.cmn&lt;64&gt;(left, UInt12(-right.m_value));
2504         else {
2505             moveToCachedReg(right, dataMemoryTempRegister());
2506             m_assembler.cmp&lt;64&gt;(left, dataTempRegister);
2507         }
2508         m_assembler.fcsel&lt;64&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2509     }
2510 
2511     void moveDoubleConditionallyTest32(ResultCondition cond, RegisterID left, RegisterID right, FPRegisterID thenCase, FPRegisterID elseCase, FPRegisterID dest)
2512     {
2513         m_assembler.tst&lt;32&gt;(left, right);
2514         m_assembler.fcsel&lt;64&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2515     }
2516 
2517     void moveDoubleConditionallyTest32(ResultCondition cond, RegisterID left, TrustedImm32 right, FPRegisterID thenCase, FPRegisterID elseCase, FPRegisterID dest)
2518     {
2519         test32(left, right);
2520         m_assembler.fcsel&lt;64&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2521     }
2522 
2523     void moveDoubleConditionallyTest64(ResultCondition cond, RegisterID left, RegisterID right, FPRegisterID thenCase, FPRegisterID elseCase, FPRegisterID dest)
2524     {
2525         m_assembler.tst&lt;64&gt;(left, right);
2526         m_assembler.fcsel&lt;64&gt;(dest, thenCase, elseCase, ARM64Condition(cond));
2527     }
2528 
2529     // Forwards / external control flow operations:
2530     //
2531     // This set of jump and conditional branch operations return a Jump
2532     // object which may linked at a later point, allow forwards jump,
2533     // or jumps that will require external linkage (after the code has been
2534     // relocated).
2535     //
2536     // For branches, signed &lt;, &gt;, &lt;= and &gt;= are denoted as l, g, le, and ge
2537     // respecitvely, for unsigned comparisons the names b, a, be, and ae are
2538     // used (representing the names &#39;below&#39; and &#39;above&#39;).
2539     //
2540     // Operands to the comparision are provided in the expected order, e.g.
2541     // jle32(reg1, TrustedImm32(5)) will branch if the value held in reg1, when
2542     // treated as a signed 32bit value, is less than or equal to 5.
2543     //
2544     // jz and jnz test whether the first operand is equal to zero, and take
2545     // an optional second operand of a mask under which to perform the test.
2546 
2547     Jump branch32(RelationalCondition cond, RegisterID left, RegisterID right)
2548     {
2549         m_assembler.cmp&lt;32&gt;(left, right);
2550         return Jump(makeBranch(cond));
2551     }
2552 
2553     Jump branch32(RelationalCondition cond, RegisterID left, TrustedImm32 right)
2554     {
2555         if (!right.m_value) {
2556             if (auto resultCondition = commuteCompareToZeroIntoTest(cond))
2557                 return branchTest32(*resultCondition, left, left);
2558         }
2559 
2560         if (isUInt12(right.m_value))
2561             m_assembler.cmp&lt;32&gt;(left, UInt12(right.m_value));
2562         else if (isUInt12(-right.m_value))
2563             m_assembler.cmn&lt;32&gt;(left, UInt12(-right.m_value));
2564         else {
2565             moveToCachedReg(right, dataMemoryTempRegister());
2566             m_assembler.cmp&lt;32&gt;(left, dataTempRegister);
2567         }
2568         return Jump(makeBranch(cond));
2569     }
2570 
2571     Jump branch32(RelationalCondition cond, RegisterID left, Address right)
2572     {
2573         load32(right, getCachedMemoryTempRegisterIDAndInvalidate());
2574         return branch32(cond, left, memoryTempRegister);
2575     }
2576 
2577     Jump branch32(RelationalCondition cond, Address left, RegisterID right)
2578     {
2579         load32(left, getCachedMemoryTempRegisterIDAndInvalidate());
2580         return branch32(cond, memoryTempRegister, right);
2581     }
2582 
2583     Jump branch32(RelationalCondition cond, Address left, TrustedImm32 right)
2584     {
2585         load32(left, getCachedMemoryTempRegisterIDAndInvalidate());
2586         return branch32(cond, memoryTempRegister, right);
2587     }
2588 
2589     Jump branch32(RelationalCondition cond, BaseIndex left, TrustedImm32 right)
2590     {
2591         load32(left, getCachedMemoryTempRegisterIDAndInvalidate());
2592         return branch32(cond, memoryTempRegister, right);
2593     }
2594 
2595     Jump branch32(RelationalCondition cond, AbsoluteAddress left, RegisterID right)
2596     {
2597         load32(left.m_ptr, getCachedDataTempRegisterIDAndInvalidate());
2598         return branch32(cond, dataTempRegister, right);
2599     }
2600 
2601     Jump branch32(RelationalCondition cond, AbsoluteAddress left, TrustedImm32 right)
2602     {
2603         load32(left.m_ptr, getCachedMemoryTempRegisterIDAndInvalidate());
2604         return branch32(cond, memoryTempRegister, right);
2605     }
2606 
2607     Jump branch64(RelationalCondition cond, RegisterID left, RegisterID right)
2608     {
2609         if (right == ARM64Registers::sp) {
2610             if (cond == Equal &amp;&amp; left != ARM64Registers::sp) {
2611                 // CMP can only use SP for the left argument, since we are testing for equality, the order
2612                 // does not matter here.
2613                 std::swap(left, right);
2614             } else {
2615                 move(right, getCachedDataTempRegisterIDAndInvalidate());
2616                 right = dataTempRegister;
2617             }
2618         }
2619         m_assembler.cmp&lt;64&gt;(left, right);
2620         return Jump(makeBranch(cond));
2621     }
2622 
2623     Jump branch64(RelationalCondition cond, RegisterID left, TrustedImm32 right)
2624     {
2625         if (!right.m_value) {
2626             if (auto resultCondition = commuteCompareToZeroIntoTest(cond))
2627                 return branchTest64(*resultCondition, left, left);
2628         }
2629 
2630         if (isUInt12(right.m_value))
2631             m_assembler.cmp&lt;64&gt;(left, UInt12(right.m_value));
2632         else if (isUInt12(-right.m_value))
2633             m_assembler.cmn&lt;64&gt;(left, UInt12(-right.m_value));
2634         else {
2635             moveToCachedReg(right, dataMemoryTempRegister());
2636             m_assembler.cmp&lt;64&gt;(left, dataTempRegister);
2637         }
2638         return Jump(makeBranch(cond));
2639     }
2640 
2641     Jump branch64(RelationalCondition cond, RegisterID left, TrustedImm64 right)
2642     {
2643         intptr_t immediate = right.m_value;
2644         if (!immediate) {
2645             if (auto resultCondition = commuteCompareToZeroIntoTest(cond))
2646                 return branchTest64(*resultCondition, left, left);
2647         }
2648 
2649         if (isUInt12(immediate))
2650             m_assembler.cmp&lt;64&gt;(left, UInt12(static_cast&lt;int32_t&gt;(immediate)));
2651         else if (isUInt12(-immediate))
2652             m_assembler.cmn&lt;64&gt;(left, UInt12(static_cast&lt;int32_t&gt;(-immediate)));
2653         else {
2654             moveToCachedReg(right, dataMemoryTempRegister());
2655             m_assembler.cmp&lt;64&gt;(left, dataTempRegister);
2656         }
2657         return Jump(makeBranch(cond));
2658     }
2659 
2660     Jump branch64(RelationalCondition cond, RegisterID left, Address right)
2661     {
2662         load64(right, getCachedMemoryTempRegisterIDAndInvalidate());
2663         return branch64(cond, left, memoryTempRegister);
2664     }
2665 
2666     Jump branch64(RelationalCondition cond, AbsoluteAddress left, RegisterID right)
2667     {
2668         load64(left.m_ptr, getCachedDataTempRegisterIDAndInvalidate());
2669         return branch64(cond, dataTempRegister, right);
2670     }
2671 
2672     Jump branch64(RelationalCondition cond, Address left, RegisterID right)
2673     {
2674         load64(left, getCachedMemoryTempRegisterIDAndInvalidate());
2675         return branch64(cond, memoryTempRegister, right);
2676     }
2677 
2678     Jump branch64(RelationalCondition cond, Address left, TrustedImm64 right)
2679     {
2680         load64(left, getCachedMemoryTempRegisterIDAndInvalidate());
2681         return branch64(cond, memoryTempRegister, right);
2682     }
2683 
2684     Jump branch64(RelationalCondition cond, BaseIndex left, RegisterID right)
2685     {
2686         load64(left, getCachedMemoryTempRegisterIDAndInvalidate());
2687         return branch64(cond, memoryTempRegister, right);
2688     }
2689 
2690     Jump branchPtr(RelationalCondition cond, BaseIndex left, RegisterID right)
2691     {
2692         return branch64(cond, left, right);
2693     }
2694 
2695     Jump branch8(RelationalCondition cond, Address left, TrustedImm32 right)
2696     {
2697         TrustedImm32 right8 = MacroAssemblerHelpers::mask8OnCondition(*this, cond, right);
2698         MacroAssemblerHelpers::load8OnCondition(*this, cond, left, getCachedMemoryTempRegisterIDAndInvalidate());
2699         return branch32(cond, memoryTempRegister, right8);
2700     }
2701 
2702     Jump branch8(RelationalCondition cond, BaseIndex left, TrustedImm32 right)
2703     {
2704         TrustedImm32 right8 = MacroAssemblerHelpers::mask8OnCondition(*this, cond, right);
2705         MacroAssemblerHelpers::load8OnCondition(*this, cond, left, getCachedMemoryTempRegisterIDAndInvalidate());
2706         return branch32(cond, memoryTempRegister, right8);
2707     }
2708 
2709     Jump branch8(RelationalCondition cond, AbsoluteAddress left, TrustedImm32 right)
2710     {
2711         TrustedImm32 right8 = MacroAssemblerHelpers::mask8OnCondition(*this, cond, right);
2712         MacroAssemblerHelpers::load8OnCondition(*this, cond, left.m_ptr, getCachedMemoryTempRegisterIDAndInvalidate());
2713         return branch32(cond, memoryTempRegister, right8);
2714     }
2715 
2716     Jump branchTest32(ResultCondition cond, RegisterID reg, RegisterID mask)
2717     {
2718         if (reg == mask &amp;&amp; (cond == Zero || cond == NonZero))
2719             return Jump(makeCompareAndBranch&lt;32&gt;(static_cast&lt;ZeroCondition&gt;(cond), reg));
2720         m_assembler.tst&lt;32&gt;(reg, mask);
2721         return Jump(makeBranch(cond));
2722     }
2723 
2724     void test32(RegisterID reg, TrustedImm32 mask = TrustedImm32(-1))
2725     {
2726         if (mask.m_value == -1)
2727             m_assembler.tst&lt;32&gt;(reg, reg);
2728         else {
2729             LogicalImmediate logicalImm = LogicalImmediate::create32(mask.m_value);
2730 
2731             if (logicalImm.isValid())
2732                 m_assembler.tst&lt;32&gt;(reg, logicalImm);
2733             else {
2734                 move(mask, getCachedDataTempRegisterIDAndInvalidate());
2735                 m_assembler.tst&lt;32&gt;(reg, dataTempRegister);
2736             }
2737         }
2738     }
2739 
2740     Jump branch(ResultCondition cond)
2741     {
2742         return Jump(makeBranch(cond));
2743     }
2744 
2745     Jump branchTest32(ResultCondition cond, RegisterID reg, TrustedImm32 mask = TrustedImm32(-1))
2746     {
2747         if (mask.m_value == -1) {
2748             if ((cond == Zero) || (cond == NonZero))
2749                 return Jump(makeCompareAndBranch&lt;32&gt;(static_cast&lt;ZeroCondition&gt;(cond), reg));
2750             m_assembler.tst&lt;32&gt;(reg, reg);
2751         } else if (hasOneBitSet(mask.m_value) &amp;&amp; ((cond == Zero) || (cond == NonZero)))
2752             return Jump(makeTestBitAndBranch(reg, getLSBSet(mask.m_value), static_cast&lt;ZeroCondition&gt;(cond)));
2753         else {
2754             LogicalImmediate logicalImm = LogicalImmediate::create32(mask.m_value);
2755             if (logicalImm.isValid()) {
2756                 m_assembler.tst&lt;32&gt;(reg, logicalImm);
2757                 return Jump(makeBranch(cond));
2758             }
2759 
2760             move(mask, getCachedDataTempRegisterIDAndInvalidate());
2761             m_assembler.tst&lt;32&gt;(reg, dataTempRegister);
2762         }
2763         return Jump(makeBranch(cond));
2764     }
2765 
2766     Jump branchTest32(ResultCondition cond, Address address, TrustedImm32 mask = TrustedImm32(-1))
2767     {
2768         load32(address, getCachedMemoryTempRegisterIDAndInvalidate());
2769         return branchTest32(cond, memoryTempRegister, mask);
2770     }
2771 
2772     Jump branchTest32(ResultCondition cond, BaseIndex address, TrustedImm32 mask = TrustedImm32(-1))
2773     {
2774         load32(address, getCachedMemoryTempRegisterIDAndInvalidate());
2775         return branchTest32(cond, memoryTempRegister, mask);
2776     }
2777 
2778     Jump branchTest64(ResultCondition cond, RegisterID reg, RegisterID mask)
2779     {
2780         if (reg == mask &amp;&amp; (cond == Zero || cond == NonZero))
2781             return Jump(makeCompareAndBranch&lt;64&gt;(static_cast&lt;ZeroCondition&gt;(cond), reg));
2782         m_assembler.tst&lt;64&gt;(reg, mask);
2783         return Jump(makeBranch(cond));
2784     }
2785 
2786     Jump branchTest64(ResultCondition cond, RegisterID reg, TrustedImm32 mask = TrustedImm32(-1))
2787     {
2788         if (mask.m_value == -1) {
2789             if ((cond == Zero) || (cond == NonZero))
2790                 return Jump(makeCompareAndBranch&lt;64&gt;(static_cast&lt;ZeroCondition&gt;(cond), reg));
2791             m_assembler.tst&lt;64&gt;(reg, reg);
2792         } else if (hasOneBitSet(mask.m_value) &amp;&amp; ((cond == Zero) || (cond == NonZero)))
2793             return Jump(makeTestBitAndBranch(reg, getLSBSet(mask.m_value), static_cast&lt;ZeroCondition&gt;(cond)));
2794         else {
2795             LogicalImmediate logicalImm = LogicalImmediate::create64(mask.m_value);
2796 
2797             if (logicalImm.isValid()) {
2798                 m_assembler.tst&lt;64&gt;(reg, logicalImm);
2799                 return Jump(makeBranch(cond));
2800             }
2801 
2802             signExtend32ToPtr(mask, getCachedDataTempRegisterIDAndInvalidate());
2803             m_assembler.tst&lt;64&gt;(reg, dataTempRegister);
2804         }
2805         return Jump(makeBranch(cond));
2806     }
2807 
2808     Jump branchTest64(ResultCondition cond, RegisterID reg, TrustedImm64 mask)
2809     {
2810         if (mask.m_value == -1) {
2811             if ((cond == Zero) || (cond == NonZero))
2812                 return Jump(makeCompareAndBranch&lt;64&gt;(static_cast&lt;ZeroCondition&gt;(cond), reg));
2813             m_assembler.tst&lt;64&gt;(reg, reg);
2814         } else if (hasOneBitSet(mask.m_value) &amp;&amp; ((cond == Zero) || (cond == NonZero)))
2815             return Jump(makeTestBitAndBranch(reg, getLSBSet(mask.m_value), static_cast&lt;ZeroCondition&gt;(cond)));
2816         else {
2817             LogicalImmediate logicalImm = LogicalImmediate::create64(mask.m_value);
2818 
2819             if (logicalImm.isValid()) {
2820                 m_assembler.tst&lt;64&gt;(reg, logicalImm);
2821                 return Jump(makeBranch(cond));
2822             }
2823 
2824             move(mask, getCachedDataTempRegisterIDAndInvalidate());
2825             m_assembler.tst&lt;64&gt;(reg, dataTempRegister);
2826         }
2827         return Jump(makeBranch(cond));
2828     }
2829 
2830     Jump branchTest64(ResultCondition cond, Address address, RegisterID mask)
2831     {
2832         load64(address, getCachedDataTempRegisterIDAndInvalidate());
2833         return branchTest64(cond, dataTempRegister, mask);
2834     }
2835 
2836     Jump branchTest64(ResultCondition cond, Address address, TrustedImm32 mask = TrustedImm32(-1))
2837     {
2838         load64(address, getCachedDataTempRegisterIDAndInvalidate());
2839         return branchTest64(cond, dataTempRegister, mask);
2840     }
2841 
2842     Jump branchTest64(ResultCondition cond, BaseIndex address, TrustedImm32 mask = TrustedImm32(-1))
2843     {
2844         load64(address, getCachedDataTempRegisterIDAndInvalidate());
2845         return branchTest64(cond, dataTempRegister, mask);
2846     }
2847 
2848     Jump branchTest64(ResultCondition cond, AbsoluteAddress address, TrustedImm32 mask = TrustedImm32(-1))
2849     {
2850         load64(address.m_ptr, getCachedDataTempRegisterIDAndInvalidate());
2851         return branchTest64(cond, dataTempRegister, mask);
2852     }
2853 
2854     Jump branchTest8(ResultCondition cond, Address address, TrustedImm32 mask = TrustedImm32(-1))
2855     {
2856         TrustedImm32 mask8 = MacroAssemblerHelpers::mask8OnCondition(*this, cond, mask);
2857         MacroAssemblerHelpers::load8OnCondition(*this, cond, address, getCachedDataTempRegisterIDAndInvalidate());
2858         return branchTest32(cond, dataTempRegister, mask8);
2859     }
2860 
2861     Jump branchTest8(ResultCondition cond, AbsoluteAddress address, TrustedImm32 mask = TrustedImm32(-1))
2862     {
2863         TrustedImm32 mask8 = MacroAssemblerHelpers::mask8OnCondition(*this, cond, mask);
2864         MacroAssemblerHelpers::load8OnCondition(*this, cond, address.m_ptr, getCachedDataTempRegisterIDAndInvalidate());
2865         return branchTest32(cond, dataTempRegister, mask8);
2866     }
2867 
2868     Jump branchTest8(ResultCondition cond, ExtendedAddress address, TrustedImm32 mask = TrustedImm32(-1))
2869     {
2870         TrustedImm32 mask8 = MacroAssemblerHelpers::mask8OnCondition(*this, cond, mask);
2871         move(TrustedImmPtr(reinterpret_cast&lt;void*&gt;(address.offset)), getCachedDataTempRegisterIDAndInvalidate());
2872 
2873         if (MacroAssemblerHelpers::isUnsigned&lt;MacroAssemblerARM64&gt;(cond))
2874             m_assembler.ldrb(dataTempRegister, address.base, dataTempRegister);
2875         else
2876             m_assembler.ldrsb&lt;32&gt;(dataTempRegister, address.base, dataTempRegister);
2877 
2878         return branchTest32(cond, dataTempRegister, mask8);
2879     }
2880 
2881     Jump branchTest8(ResultCondition cond, BaseIndex address, TrustedImm32 mask = TrustedImm32(-1))
2882     {
2883         TrustedImm32 mask8 = MacroAssemblerHelpers::mask8OnCondition(*this, cond, mask);
2884         MacroAssemblerHelpers::load8OnCondition(*this, cond, address, getCachedDataTempRegisterIDAndInvalidate());
2885         return branchTest32(cond, dataTempRegister, mask8);
2886     }
2887 
2888     Jump branch32WithUnalignedHalfWords(RelationalCondition cond, BaseIndex left, TrustedImm32 right)
2889     {
2890         return branch32(cond, left, right);
2891     }
2892 
2893 
2894     // Arithmetic control flow operations:
2895     //
2896     // This set of conditional branch operations branch based
2897     // on the result of an arithmetic operation. The operation
2898     // is performed as normal, storing the result.
2899     //
2900     // * jz operations branch if the result is zero.
2901     // * jo operations branch if the (signed) arithmetic
2902     //   operation caused an overflow to occur.
2903 
2904     Jump branchAdd32(ResultCondition cond, RegisterID op1, RegisterID op2, RegisterID dest)
2905     {
2906         m_assembler.add&lt;32, S&gt;(dest, op1, op2);
2907         return Jump(makeBranch(cond));
2908     }
2909 
2910     Jump branchAdd32(ResultCondition cond, RegisterID op1, TrustedImm32 imm, RegisterID dest)
2911     {
2912         if (isUInt12(imm.m_value)) {
2913             m_assembler.add&lt;32, S&gt;(dest, op1, UInt12(imm.m_value));
2914             return Jump(makeBranch(cond));
2915         }
2916         if (isUInt12(-imm.m_value)) {
2917             m_assembler.sub&lt;32, S&gt;(dest, op1, UInt12(-imm.m_value));
2918             return Jump(makeBranch(cond));
2919         }
2920 
2921         signExtend32ToPtr(imm, getCachedDataTempRegisterIDAndInvalidate());
2922         return branchAdd32(cond, op1, dataTempRegister, dest);
2923     }
2924 
2925     Jump branchAdd32(ResultCondition cond, Address src, RegisterID dest)
2926     {
2927         load32(src, getCachedDataTempRegisterIDAndInvalidate());
2928         return branchAdd32(cond, dest, dataTempRegister, dest);
2929     }
2930 
2931     Jump branchAdd32(ResultCondition cond, RegisterID src, RegisterID dest)
2932     {
2933         return branchAdd32(cond, dest, src, dest);
2934     }
2935 
2936     Jump branchAdd32(ResultCondition cond, TrustedImm32 imm, RegisterID dest)
2937     {
2938         return branchAdd32(cond, dest, imm, dest);
2939     }
2940 
2941     Jump branchAdd32(ResultCondition cond, TrustedImm32 imm, AbsoluteAddress address)
2942     {
2943         load32(address.m_ptr, getCachedDataTempRegisterIDAndInvalidate());
2944 
2945         if (isUInt12(imm.m_value)) {
2946             m_assembler.add&lt;32, S&gt;(dataTempRegister, dataTempRegister, UInt12(imm.m_value));
2947             store32(dataTempRegister, address.m_ptr);
2948         } else if (isUInt12(-imm.m_value)) {
2949             m_assembler.sub&lt;32, S&gt;(dataTempRegister, dataTempRegister, UInt12(-imm.m_value));
2950             store32(dataTempRegister, address.m_ptr);
2951         } else {
2952             move(imm, getCachedMemoryTempRegisterIDAndInvalidate());
2953             m_assembler.add&lt;32, S&gt;(dataTempRegister, dataTempRegister, memoryTempRegister);
2954             store32(dataTempRegister, address.m_ptr);
2955         }
2956 
2957         return Jump(makeBranch(cond));
2958     }
2959 
2960     Jump branchAdd64(ResultCondition cond, RegisterID op1, RegisterID op2, RegisterID dest)
2961     {
2962         m_assembler.add&lt;64, S&gt;(dest, op1, op2);
2963         return Jump(makeBranch(cond));
2964     }
2965 
2966     Jump branchAdd64(ResultCondition cond, RegisterID op1, TrustedImm32 imm, RegisterID dest)
2967     {
2968         if (isUInt12(imm.m_value)) {
2969             m_assembler.add&lt;64, S&gt;(dest, op1, UInt12(imm.m_value));
2970             return Jump(makeBranch(cond));
2971         }
2972         if (isUInt12(-imm.m_value)) {
2973             m_assembler.sub&lt;64, S&gt;(dest, op1, UInt12(-imm.m_value));
2974             return Jump(makeBranch(cond));
2975         }
2976 
2977         move(imm, getCachedDataTempRegisterIDAndInvalidate());
2978         return branchAdd64(cond, op1, dataTempRegister, dest);
2979     }
2980 
2981     Jump branchAdd64(ResultCondition cond, RegisterID src, RegisterID dest)
2982     {
2983         return branchAdd64(cond, dest, src, dest);
2984     }
2985 
2986     Jump branchAdd64(ResultCondition cond, TrustedImm32 imm, RegisterID dest)
2987     {
2988         return branchAdd64(cond, dest, imm, dest);
2989     }
2990 
2991     Jump branchAdd64(RelationalCondition cond, TrustedImm32 imm, RegisterID dest)
2992     {
2993         ASSERT(isUInt12(imm.m_value));
2994         m_assembler.add&lt;64, S&gt;(dest, dest, UInt12(imm.m_value));
2995         return Jump(makeBranch(cond));
2996     }
2997 
2998     Jump branchMul32(ResultCondition cond, RegisterID src1, RegisterID src2, RegisterID scratch1, RegisterID scratch2, RegisterID dest)
2999     {
3000         ASSERT(cond != Signed);
3001 
3002         if (cond != Overflow) {
3003             m_assembler.mul&lt;32&gt;(dest, src1, src2);
3004             return branchTest32(cond, dest);
3005         }
3006 
3007         // This is a signed multiple of two 32-bit values, producing a 64-bit result.
3008         m_assembler.smull(dest, src1, src2);
3009         // Copy bits 63..32 of the result to bits 31..0 of scratch1.
3010         m_assembler.asr&lt;64&gt;(scratch1, dest, 32);
3011         // Splat bit 31 of the result to bits 31..0 of scratch2.
3012         m_assembler.asr&lt;32&gt;(scratch2, dest, 31);
3013         // After a mul32 the top 32 bits of the register should be clear.
3014         zeroExtend32ToPtr(dest, dest);
3015         // Check that bits 31..63 of the original result were all equal.
3016         return branch32(NotEqual, scratch2, scratch1);
3017     }
3018 
3019     Jump branchMul32(ResultCondition cond, RegisterID src1, RegisterID src2, RegisterID dest)
3020     {
3021         return branchMul32(cond, src1, src2, getCachedDataTempRegisterIDAndInvalidate(), getCachedMemoryTempRegisterIDAndInvalidate(), dest);
3022     }
3023 
3024     Jump branchMul32(ResultCondition cond, RegisterID src, RegisterID dest)
3025     {
3026         return branchMul32(cond, dest, src, dest);
3027     }
3028 
3029     Jump branchMul32(ResultCondition cond, RegisterID src, TrustedImm32 imm, RegisterID dest)
3030     {
3031         move(imm, getCachedDataTempRegisterIDAndInvalidate());
3032         return branchMul32(cond, dataTempRegister, src, dest);
3033     }
3034 
3035     Jump branchMul64(ResultCondition cond, RegisterID src1, RegisterID src2, RegisterID scratch1, RegisterID scratch2, RegisterID dest)
3036     {
3037         ASSERT(cond != Signed);
3038 
3039         // This is a signed multiple of two 64-bit values, producing a 64-bit result.
3040         m_assembler.mul&lt;64&gt;(dest, src1, src2);
3041 
3042         if (cond != Overflow)
3043             return branchTest64(cond, dest);
3044 
3045         // Compute bits 127..64 of the result into scratch1.
3046         m_assembler.smulh(scratch1, src1, src2);
3047         // Splat bit 63 of the result to bits 63..0 of scratch2.
3048         m_assembler.asr&lt;64&gt;(scratch2, dest, 63);
3049         // Check that bits 31..63 of the original result were all equal.
3050         return branch64(NotEqual, scratch2, scratch1);
3051     }
3052 
3053     Jump branchMul64(ResultCondition cond, RegisterID src1, RegisterID src2, RegisterID dest)
3054     {
3055         return branchMul64(cond, src1, src2, getCachedDataTempRegisterIDAndInvalidate(), getCachedMemoryTempRegisterIDAndInvalidate(), dest);
3056     }
3057 
3058     Jump branchMul64(ResultCondition cond, RegisterID src, RegisterID dest)
3059     {
3060         return branchMul64(cond, dest, src, dest);
3061     }
3062 
3063     Jump branchNeg32(ResultCondition cond, RegisterID dest)
3064     {
3065         m_assembler.neg&lt;32, S&gt;(dest, dest);
3066         return Jump(makeBranch(cond));
3067     }
3068 
3069     Jump branchNeg64(ResultCondition cond, RegisterID srcDest)
3070     {
3071         m_assembler.neg&lt;64, S&gt;(srcDest, srcDest);
3072         return Jump(makeBranch(cond));
3073     }
3074 
3075     Jump branchSub32(ResultCondition cond, RegisterID dest)
3076     {
3077         m_assembler.neg&lt;32, S&gt;(dest, dest);
3078         return Jump(makeBranch(cond));
3079     }
3080 
3081     Jump branchSub32(ResultCondition cond, RegisterID op1, RegisterID op2, RegisterID dest)
3082     {
3083         m_assembler.sub&lt;32, S&gt;(dest, op1, op2);
3084         return Jump(makeBranch(cond));
3085     }
3086 
3087     Jump branchSub32(ResultCondition cond, RegisterID op1, TrustedImm32 imm, RegisterID dest)
3088     {
3089         if (isUInt12(imm.m_value)) {
3090             m_assembler.sub&lt;32, S&gt;(dest, op1, UInt12(imm.m_value));
3091             return Jump(makeBranch(cond));
3092         }
3093         if (isUInt12(-imm.m_value)) {
3094             m_assembler.add&lt;32, S&gt;(dest, op1, UInt12(-imm.m_value));
3095             return Jump(makeBranch(cond));
3096         }
3097 
3098         signExtend32ToPtr(imm, getCachedDataTempRegisterIDAndInvalidate());
3099         return branchSub32(cond, op1, dataTempRegister, dest);
3100     }
3101 
3102     Jump branchSub32(ResultCondition cond, RegisterID src, RegisterID dest)
3103     {
3104         return branchSub32(cond, dest, src, dest);
3105     }
3106 
3107     Jump branchSub32(ResultCondition cond, TrustedImm32 imm, RegisterID dest)
3108     {
3109         return branchSub32(cond, dest, imm, dest);
3110     }
3111 
3112     Jump branchSub64(ResultCondition cond, RegisterID op1, RegisterID op2, RegisterID dest)
3113     {
3114         m_assembler.sub&lt;64, S&gt;(dest, op1, op2);
3115         return Jump(makeBranch(cond));
3116     }
3117 
3118     Jump branchSub64(ResultCondition cond, RegisterID op1, TrustedImm32 imm, RegisterID dest)
3119     {
3120         if (isUInt12(imm.m_value)) {
3121             m_assembler.sub&lt;64, S&gt;(dest, op1, UInt12(imm.m_value));
3122             return Jump(makeBranch(cond));
3123         }
3124         if (isUInt12(-imm.m_value)) {
3125             m_assembler.add&lt;64, S&gt;(dest, op1, UInt12(-imm.m_value));
3126             return Jump(makeBranch(cond));
3127         }
3128 
3129         move(imm, getCachedDataTempRegisterIDAndInvalidate());
3130         return branchSub64(cond, op1, dataTempRegister, dest);
3131     }
3132 
3133     Jump branchSub64(ResultCondition cond, RegisterID src, RegisterID dest)
3134     {
3135         return branchSub64(cond, dest, src, dest);
3136     }
3137 
3138     Jump branchSub64(ResultCondition cond, TrustedImm32 imm, RegisterID dest)
3139     {
3140         return branchSub64(cond, dest, imm, dest);
3141     }
3142 
3143     Jump branchSub64(RelationalCondition cond, TrustedImm32 imm, RegisterID dest)
3144     {
3145         ASSERT(isUInt12(imm.m_value));
3146         m_assembler.sub&lt;64, S&gt;(dest, dest, UInt12(imm.m_value));
3147         return Jump(makeBranch(cond));
3148     }
3149 
3150 
3151     // Jumps, calls, returns
3152 
3153     ALWAYS_INLINE Call call(PtrTag)
3154     {
3155         AssemblerLabel pointerLabel = m_assembler.label();
3156         moveWithFixedWidth(TrustedImmPtr(nullptr), getCachedDataTempRegisterIDAndInvalidate());
3157         invalidateAllTempRegisters();
3158         m_assembler.blr(dataTempRegister);
3159         AssemblerLabel callLabel = m_assembler.label();
3160         ASSERT_UNUSED(pointerLabel, Assembler::getDifferenceBetweenLabels(callLabel, pointerLabel) == REPATCH_OFFSET_CALL_TO_POINTER);
3161         return Call(callLabel, Call::Linkable);
3162     }
3163 
3164     ALWAYS_INLINE Call call(RegisterID target, PtrTag)
3165     {
3166         invalidateAllTempRegisters();
3167         m_assembler.blr(target);
3168         return Call(m_assembler.label(), Call::None);
3169     }
3170 
3171     ALWAYS_INLINE Call call(Address address, PtrTag tag)
3172     {
3173         load64(address, getCachedDataTempRegisterIDAndInvalidate());
3174         return call(dataTempRegister, tag);
3175     }
3176 
3177     ALWAYS_INLINE Call call(RegisterID callTag) { return UNUSED_PARAM(callTag), call(NoPtrTag); }
3178     ALWAYS_INLINE Call call(RegisterID target, RegisterID callTag) { return UNUSED_PARAM(callTag), call(target, NoPtrTag); }
3179     ALWAYS_INLINE Call call(Address address, RegisterID callTag) { return UNUSED_PARAM(callTag), call(address, NoPtrTag); }
3180 
3181     ALWAYS_INLINE Jump jump()
3182     {
3183         AssemblerLabel label = m_assembler.label();
3184         m_assembler.b();
3185         return Jump(label, m_makeJumpPatchable ? Assembler::JumpNoConditionFixedSize : Assembler::JumpNoCondition);
3186     }
3187 
3188     void jump(RegisterID target, PtrTag)
3189     {
3190         m_assembler.br(target);
3191     }
3192 
3193     void jump(Address address, PtrTag)
3194     {
3195         load64(address, getCachedDataTempRegisterIDAndInvalidate());
3196         m_assembler.br(dataTempRegister);
3197     }
3198 
3199     void jump(BaseIndex address, PtrTag)
3200     {
3201         load64(address, getCachedDataTempRegisterIDAndInvalidate());
3202         m_assembler.br(dataTempRegister);
3203     }
3204 
3205     void jump(AbsoluteAddress address, PtrTag)
3206     {
3207         move(TrustedImmPtr(address.m_ptr), getCachedDataTempRegisterIDAndInvalidate());
3208         load64(Address(dataTempRegister), dataTempRegister);
3209         m_assembler.br(dataTempRegister);
3210     }
3211 
3212     ALWAYS_INLINE void jump(RegisterID target, RegisterID jumpTag) { UNUSED_PARAM(jumpTag), jump(target, NoPtrTag); }
3213     ALWAYS_INLINE void jump(Address address, RegisterID jumpTag) { UNUSED_PARAM(jumpTag), jump(address, NoPtrTag); }
3214     ALWAYS_INLINE void jump(BaseIndex address, RegisterID jumpTag) { UNUSED_PARAM(jumpTag), jump(address, NoPtrTag); }
3215     ALWAYS_INLINE void jump(AbsoluteAddress address, RegisterID jumpTag) { UNUSED_PARAM(jumpTag), jump(address, NoPtrTag); }
3216 
3217     ALWAYS_INLINE Call makeTailRecursiveCall(Jump oldJump)
3218     {
3219         oldJump.link(this);
3220         return tailRecursiveCall();
3221     }
3222 
3223     ALWAYS_INLINE Call nearCall()
3224     {
3225         m_assembler.bl();
3226         return Call(m_assembler.label(), Call::LinkableNear);
3227     }
3228 
3229     ALWAYS_INLINE Call nearTailCall()
3230     {
3231         AssemblerLabel label = m_assembler.label();
3232         m_assembler.b();
3233         return Call(label, Call::LinkableNearTail);
3234     }
3235 
3236     ALWAYS_INLINE Call threadSafePatchableNearCall()
3237     {
3238         m_assembler.bl();
3239         return Call(m_assembler.label(), Call::LinkableNear);
3240     }
3241 
3242     ALWAYS_INLINE void ret()
3243     {
3244         m_assembler.ret();
3245     }
3246 
3247     ALWAYS_INLINE Call tailRecursiveCall()
3248     {
3249         // Like a normal call, but don&#39;t link.
3250         AssemblerLabel pointerLabel = m_assembler.label();
3251         moveWithFixedWidth(TrustedImmPtr(nullptr), getCachedDataTempRegisterIDAndInvalidate());
3252         m_assembler.br(dataTempRegister);
3253         AssemblerLabel callLabel = m_assembler.label();
3254         ASSERT_UNUSED(pointerLabel, Assembler::getDifferenceBetweenLabels(callLabel, pointerLabel) == REPATCH_OFFSET_CALL_TO_POINTER);
3255         return Call(callLabel, Call::Linkable);
3256     }
3257 
3258 
3259     // Comparisons operations
3260 
3261     void compare32(RelationalCondition cond, RegisterID left, RegisterID right, RegisterID dest)
3262     {
3263         m_assembler.cmp&lt;32&gt;(left, right);
3264         m_assembler.cset&lt;32&gt;(dest, ARM64Condition(cond));
3265     }
3266 
3267     void compare32(RelationalCondition cond, Address left, RegisterID right, RegisterID dest)
3268     {
3269         load32(left, getCachedDataTempRegisterIDAndInvalidate());
3270         m_assembler.cmp&lt;32&gt;(dataTempRegister, right);
3271         m_assembler.cset&lt;32&gt;(dest, ARM64Condition(cond));
3272     }
3273 
3274     void compare32(RelationalCondition cond, RegisterID left, TrustedImm32 right, RegisterID dest)
3275     {
3276         if (!right.m_value) {
3277             if (auto resultCondition = commuteCompareToZeroIntoTest(cond)) {
3278                 test32(*resultCondition, left, left, dest);
3279                 return;
3280             }
3281         }
3282 
3283         if (isUInt12(right.m_value))
3284             m_assembler.cmp&lt;32&gt;(left, UInt12(right.m_value));
3285         else if (isUInt12(-right.m_value))
3286             m_assembler.cmn&lt;32&gt;(left, UInt12(-right.m_value));
3287         else {
3288             move(right, getCachedDataTempRegisterIDAndInvalidate());
3289             m_assembler.cmp&lt;32&gt;(left, dataTempRegister);
3290         }
3291         m_assembler.cset&lt;32&gt;(dest, ARM64Condition(cond));
3292     }
3293 
3294     void compare64(RelationalCondition cond, RegisterID left, RegisterID right, RegisterID dest)
3295     {
3296         m_assembler.cmp&lt;64&gt;(left, right);
3297         m_assembler.cset&lt;32&gt;(dest, ARM64Condition(cond));
3298     }
3299 
3300     void compare64(RelationalCondition cond, RegisterID left, TrustedImm32 right, RegisterID dest)
3301     {
3302         if (!right.m_value) {
3303             if (auto resultCondition = commuteCompareToZeroIntoTest(cond)) {
3304                 test64(*resultCondition, left, left, dest);
3305                 return;
3306             }
3307         }
3308 
3309         signExtend32ToPtr(right, getCachedDataTempRegisterIDAndInvalidate());
3310         m_assembler.cmp&lt;64&gt;(left, dataTempRegister);
3311         m_assembler.cset&lt;32&gt;(dest, ARM64Condition(cond));
3312     }
3313 
3314     void compare8(RelationalCondition cond, Address left, TrustedImm32 right, RegisterID dest)
3315     {
3316         TrustedImm32 right8 = MacroAssemblerHelpers::mask8OnCondition(*this, cond, right);
3317         MacroAssemblerHelpers::load8OnCondition(*this, cond, left, getCachedMemoryTempRegisterIDAndInvalidate());
3318         move(right8, getCachedDataTempRegisterIDAndInvalidate());
3319         compare32(cond, memoryTempRegister, dataTempRegister, dest);
3320     }
3321 
3322     void test32(ResultCondition cond, RegisterID src, RegisterID mask, RegisterID dest)
3323     {
3324         m_assembler.tst&lt;32&gt;(src, mask);
3325         m_assembler.cset&lt;32&gt;(dest, ARM64Condition(cond));
3326     }
3327 
3328     void test32(ResultCondition cond, RegisterID src, TrustedImm32 mask, RegisterID dest)
3329     {
3330         test32(src, mask);
3331         m_assembler.cset&lt;32&gt;(dest, ARM64Condition(cond));
3332     }
3333 
3334     void test32(ResultCondition cond, Address address, TrustedImm32 mask, RegisterID dest)
3335     {
3336         load32(address, getCachedMemoryTempRegisterIDAndInvalidate());
3337         test32(cond, memoryTempRegister, mask, dest);
3338     }
3339 
3340     void test8(ResultCondition cond, Address address, TrustedImm32 mask, RegisterID dest)
3341     {
3342         TrustedImm32 mask8 = MacroAssemblerHelpers::mask8OnCondition(*this, cond, mask);
3343         MacroAssemblerHelpers::load8OnCondition(*this, cond, address, getCachedMemoryTempRegisterIDAndInvalidate());
3344         test32(cond, memoryTempRegister, mask8, dest);
3345     }
3346 
3347     void test64(ResultCondition cond, RegisterID op1, RegisterID op2, RegisterID dest)
3348     {
3349         m_assembler.tst&lt;64&gt;(op1, op2);
3350         m_assembler.cset&lt;32&gt;(dest, ARM64Condition(cond));
3351     }
3352 
3353     void test64(ResultCondition cond, RegisterID src, TrustedImm32 mask, RegisterID dest)
3354     {
3355         if (mask.m_value == -1)
3356             m_assembler.tst&lt;64&gt;(src, src);
3357         else {
3358             signExtend32ToPtr(mask, getCachedDataTempRegisterIDAndInvalidate());
3359             m_assembler.tst&lt;64&gt;(src, dataTempRegister);
3360         }
3361         m_assembler.cset&lt;32&gt;(dest, ARM64Condition(cond));
3362     }
3363 
3364     void setCarry(RegisterID dest)
3365     {
3366         m_assembler.cset&lt;32&gt;(dest, Assembler::ConditionCS);
3367     }
3368 
3369     // Patchable operations
3370 
3371     ALWAYS_INLINE DataLabel32 moveWithPatch(TrustedImm32 imm, RegisterID dest)
3372     {
3373         DataLabel32 label(this);
3374         moveWithFixedWidth(imm, dest);
3375         return label;
3376     }
3377 
3378     ALWAYS_INLINE DataLabelPtr moveWithPatch(TrustedImmPtr imm, RegisterID dest)
3379     {
3380         DataLabelPtr label(this);
3381         moveWithFixedWidth(imm, dest);
3382         return label;
3383     }
3384 
3385     ALWAYS_INLINE Jump branchPtrWithPatch(RelationalCondition cond, RegisterID left, DataLabelPtr&amp; dataLabel, TrustedImmPtr initialRightValue = TrustedImmPtr(nullptr))
3386     {
3387         dataLabel = DataLabelPtr(this);
3388         moveWithPatch(initialRightValue, getCachedDataTempRegisterIDAndInvalidate());
3389         return branch64(cond, left, dataTempRegister);
3390     }
3391 
3392     ALWAYS_INLINE Jump branchPtrWithPatch(RelationalCondition cond, Address left, DataLabelPtr&amp; dataLabel, TrustedImmPtr initialRightValue = TrustedImmPtr(nullptr))
3393     {
3394         dataLabel = DataLabelPtr(this);
3395         moveWithPatch(initialRightValue, getCachedDataTempRegisterIDAndInvalidate());
3396         return branch64(cond, left, dataTempRegister);
3397     }
3398 
3399     ALWAYS_INLINE Jump branch32WithPatch(RelationalCondition cond, Address left, DataLabel32&amp; dataLabel, TrustedImm32 initialRightValue = TrustedImm32(0))
3400     {
3401         dataLabel = DataLabel32(this);
3402         moveWithPatch(initialRightValue, getCachedDataTempRegisterIDAndInvalidate());
3403         return branch32(cond, left, dataTempRegister);
3404     }
3405 
3406     PatchableJump patchableBranchPtr(RelationalCondition cond, Address left, TrustedImmPtr right)
3407     {
3408         m_makeJumpPatchable = true;
3409         Jump result = branch64(cond, left, TrustedImm64(right));
3410         m_makeJumpPatchable = false;
3411         return PatchableJump(result);
3412     }
3413 
3414     PatchableJump patchableBranch8(RelationalCondition cond, Address left, TrustedImm32 imm)
3415     {
3416         m_makeJumpPatchable = true;
3417         Jump result = branch8(cond, left, imm);
3418         m_makeJumpPatchable = false;
3419         return PatchableJump(result);
3420     }
3421 
3422     PatchableJump patchableBranchTest32(ResultCondition cond, RegisterID reg, TrustedImm32 mask = TrustedImm32(-1))
3423     {
3424         m_makeJumpPatchable = true;
3425         Jump result = branchTest32(cond, reg, mask);
3426         m_makeJumpPatchable = false;
3427         return PatchableJump(result);
3428     }
3429 
3430     PatchableJump patchableBranch32(RelationalCondition cond, RegisterID reg, TrustedImm32 imm)
3431     {
3432         m_makeJumpPatchable = true;
3433         Jump result = branch32(cond, reg, imm);
3434         m_makeJumpPatchable = false;
3435         return PatchableJump(result);
3436     }
3437 
3438     PatchableJump patchableBranch32(RelationalCondition cond, Address left, TrustedImm32 imm)
3439     {
3440         m_makeJumpPatchable = true;
3441         Jump result = branch32(cond, left, imm);
3442         m_makeJumpPatchable = false;
3443         return PatchableJump(result);
3444     }
3445 
3446     PatchableJump patchableBranch64(RelationalCondition cond, RegisterID reg, TrustedImm64 imm)
3447     {
3448         m_makeJumpPatchable = true;
3449         Jump result = branch64(cond, reg, imm);
3450         m_makeJumpPatchable = false;
3451         return PatchableJump(result);
3452     }
3453 
3454     PatchableJump patchableBranch64(RelationalCondition cond, RegisterID left, RegisterID right)
3455     {
3456         m_makeJumpPatchable = true;
3457         Jump result = branch64(cond, left, right);
3458         m_makeJumpPatchable = false;
3459         return PatchableJump(result);
3460     }
3461 
3462     PatchableJump patchableBranchPtrWithPatch(RelationalCondition cond, Address left, DataLabelPtr&amp; dataLabel, TrustedImmPtr initialRightValue = TrustedImmPtr(nullptr))
3463     {
3464         m_makeJumpPatchable = true;
3465         Jump result = branchPtrWithPatch(cond, left, dataLabel, initialRightValue);
3466         m_makeJumpPatchable = false;
3467         return PatchableJump(result);
3468     }
3469 
3470     PatchableJump patchableBranch32WithPatch(RelationalCondition cond, Address left, DataLabel32&amp; dataLabel, TrustedImm32 initialRightValue = TrustedImm32(0))
3471     {
3472         m_makeJumpPatchable = true;
3473         Jump result = branch32WithPatch(cond, left, dataLabel, initialRightValue);
3474         m_makeJumpPatchable = false;
3475         return PatchableJump(result);
3476     }
3477 
3478     PatchableJump patchableJump()
3479     {
3480         m_makeJumpPatchable = true;
3481         Jump result = jump();
3482         m_makeJumpPatchable = false;
3483         return PatchableJump(result);
3484     }
3485 
3486     ALWAYS_INLINE DataLabelPtr storePtrWithPatch(TrustedImmPtr initialValue, ImplicitAddress address)
3487     {
3488         DataLabelPtr label(this);
3489         moveWithFixedWidth(initialValue, getCachedDataTempRegisterIDAndInvalidate());
3490         store64(dataTempRegister, address);
3491         return label;
3492     }
3493 
3494     ALWAYS_INLINE DataLabelPtr storePtrWithPatch(ImplicitAddress address)
3495     {
3496         return storePtrWithPatch(TrustedImmPtr(nullptr), address);
3497     }
3498 
3499     static void reemitInitialMoveWithPatch(void* address, void* value)
3500     {
3501         Assembler::setPointer(static_cast&lt;int*&gt;(address), value, dataTempRegister, true);
3502     }
3503 
3504     // Miscellaneous operations:
3505 
3506     void breakpoint(uint16_t imm = 0)
3507     {
3508         m_assembler.brk(imm);
3509     }
3510 
3511     static bool isBreakpoint(void* address) { return Assembler::isBrk(address); }
3512 
3513     void nop()
3514     {
3515         m_assembler.nop();
3516     }
3517 
3518     // We take memoryFence to mean acqrel. This has acqrel semantics on ARM64.
3519     void memoryFence()
3520     {
3521         m_assembler.dmbISH();
3522     }
3523 
3524     // We take this to mean that it prevents motion of normal stores. That&#39;s a store fence on ARM64 (hence the &quot;ST&quot;).
3525     void storeFence()
3526     {
3527         m_assembler.dmbISHST();
3528     }
3529 
3530     // We take this to mean that it prevents motion of normal loads. Ideally we&#39;d have expressed this
3531     // using dependencies or half fences, but there are cases where this is as good as it gets. The only
3532     // way to get a standalone load fence instruction on ARM is to use the ISH fence, which is just like
3533     // the memoryFence().
3534     void loadFence()
3535     {
3536         m_assembler.dmbISH();
3537     }
3538 
3539     void loadAcq8SignedExtendTo32(ImplicitAddress address, RegisterID dest)
3540     {
3541         m_assembler.ldar&lt;8&gt;(dest, extractSimpleAddress(address));
3542     }
3543 
3544     void loadAcq8(ImplicitAddress address, RegisterID dest)
3545     {
3546         loadAcq8SignedExtendTo32(address, dest);
3547         and32(TrustedImm32(0xff), dest);
3548     }
3549 
3550     void storeRel8(RegisterID src, ImplicitAddress address)
3551     {
3552         m_assembler.stlr&lt;8&gt;(src, extractSimpleAddress(address));
3553     }
3554 
3555     void loadAcq16SignedExtendTo32(ImplicitAddress address, RegisterID dest)
3556     {
3557         m_assembler.ldar&lt;16&gt;(dest, extractSimpleAddress(address));
3558     }
3559 
3560     void loadAcq16(ImplicitAddress address, RegisterID dest)
3561     {
3562         loadAcq16SignedExtendTo32(address, dest);
3563         and32(TrustedImm32(0xffff), dest);
3564     }
3565 
3566     void storeRel16(RegisterID src, ImplicitAddress address)
3567     {
3568         m_assembler.stlr&lt;16&gt;(src, extractSimpleAddress(address));
3569     }
3570 
3571     void loadAcq32(ImplicitAddress address, RegisterID dest)
3572     {
3573         m_assembler.ldar&lt;32&gt;(dest, extractSimpleAddress(address));
3574     }
3575 
3576     void loadAcq64(ImplicitAddress address, RegisterID dest)
3577     {
3578         m_assembler.ldar&lt;64&gt;(dest, extractSimpleAddress(address));
3579     }
3580 
3581     void storeRel32(RegisterID dest, ImplicitAddress address)
3582     {
3583         m_assembler.stlr&lt;32&gt;(dest, extractSimpleAddress(address));
3584     }
3585 
3586     void storeRel64(RegisterID dest, ImplicitAddress address)
3587     {
3588         m_assembler.stlr&lt;64&gt;(dest, extractSimpleAddress(address));
3589     }
3590 
3591     void loadLink8(ImplicitAddress address, RegisterID dest)
3592     {
3593         m_assembler.ldxr&lt;8&gt;(dest, extractSimpleAddress(address));
3594     }
3595 
3596     void loadLinkAcq8(ImplicitAddress address, RegisterID dest)
3597     {
3598         m_assembler.ldaxr&lt;8&gt;(dest, extractSimpleAddress(address));
3599     }
3600 
3601     void storeCond8(RegisterID src, ImplicitAddress address, RegisterID result)
3602     {
3603         m_assembler.stxr&lt;8&gt;(result, src, extractSimpleAddress(address));
3604     }
3605 
3606     void storeCondRel8(RegisterID src, ImplicitAddress address, RegisterID result)
3607     {
3608         m_assembler.stlxr&lt;8&gt;(result, src, extractSimpleAddress(address));
3609     }
3610 
3611     void loadLink16(ImplicitAddress address, RegisterID dest)
3612     {
3613         m_assembler.ldxr&lt;16&gt;(dest, extractSimpleAddress(address));
3614     }
3615 
3616     void loadLinkAcq16(ImplicitAddress address, RegisterID dest)
3617     {
3618         m_assembler.ldaxr&lt;16&gt;(dest, extractSimpleAddress(address));
3619     }
3620 
3621     void storeCond16(RegisterID src, ImplicitAddress address, RegisterID result)
3622     {
3623         m_assembler.stxr&lt;16&gt;(result, src, extractSimpleAddress(address));
3624     }
3625 
3626     void storeCondRel16(RegisterID src, ImplicitAddress address, RegisterID result)
3627     {
3628         m_assembler.stlxr&lt;16&gt;(result, src, extractSimpleAddress(address));
3629     }
3630 
3631     void loadLink32(ImplicitAddress address, RegisterID dest)
3632     {
3633         m_assembler.ldxr&lt;32&gt;(dest, extractSimpleAddress(address));
3634     }
3635 
3636     void loadLinkAcq32(ImplicitAddress address, RegisterID dest)
3637     {
3638         m_assembler.ldaxr&lt;32&gt;(dest, extractSimpleAddress(address));
3639     }
3640 
3641     void storeCond32(RegisterID src, ImplicitAddress address, RegisterID result)
3642     {
3643         m_assembler.stxr&lt;32&gt;(result, src, extractSimpleAddress(address));
3644     }
3645 
3646     void storeCondRel32(RegisterID src, ImplicitAddress address, RegisterID result)
3647     {
3648         m_assembler.stlxr&lt;32&gt;(result, src, extractSimpleAddress(address));
3649     }
3650 
3651     void loadLink64(ImplicitAddress address, RegisterID dest)
3652     {
3653         m_assembler.ldxr&lt;64&gt;(dest, extractSimpleAddress(address));
3654     }
3655 
3656     void loadLinkAcq64(ImplicitAddress address, RegisterID dest)
3657     {
3658         m_assembler.ldaxr&lt;64&gt;(dest, extractSimpleAddress(address));
3659     }
3660 
3661     void storeCond64(RegisterID src, ImplicitAddress address, RegisterID result)
3662     {
3663         m_assembler.stxr&lt;64&gt;(result, src, extractSimpleAddress(address));
3664     }
3665 
3666     void storeCondRel64(RegisterID src, ImplicitAddress address, RegisterID result)
3667     {
3668         m_assembler.stlxr&lt;64&gt;(result, src, extractSimpleAddress(address));
3669     }
3670 
3671     template&lt;typename AddressType&gt;
3672     void atomicStrongCAS8(StatusCondition cond, RegisterID expectedAndResult, RegisterID newValue, AddressType address, RegisterID result)
3673     {
3674         atomicStrongCAS&lt;8&gt;(cond, expectedAndResult, newValue, address, result);
3675     }
3676 
3677     template&lt;typename AddressType&gt;
3678     void atomicStrongCAS16(StatusCondition cond, RegisterID expectedAndResult, RegisterID newValue, AddressType address, RegisterID result)
3679     {
3680         atomicStrongCAS&lt;16&gt;(cond, expectedAndResult, newValue, address, result);
3681     }
3682 
3683     template&lt;typename AddressType&gt;
3684     void atomicStrongCAS32(StatusCondition cond, RegisterID expectedAndResult, RegisterID newValue, AddressType address, RegisterID result)
3685     {
3686         atomicStrongCAS&lt;32&gt;(cond, expectedAndResult, newValue, address, result);
3687     }
3688 
3689     template&lt;typename AddressType&gt;
3690     void atomicStrongCAS64(StatusCondition cond, RegisterID expectedAndResult, RegisterID newValue, AddressType address, RegisterID result)
3691     {
3692         atomicStrongCAS&lt;64&gt;(cond, expectedAndResult, newValue, address, result);
3693     }
3694 
3695     template&lt;typename AddressType&gt;
3696     void atomicRelaxedStrongCAS8(StatusCondition cond, RegisterID expectedAndResult, RegisterID newValue, AddressType address, RegisterID result)
3697     {
3698         atomicRelaxedStrongCAS&lt;8&gt;(cond, expectedAndResult, newValue, address, result);
3699     }
3700 
3701     template&lt;typename AddressType&gt;
3702     void atomicRelaxedStrongCAS16(StatusCondition cond, RegisterID expectedAndResult, RegisterID newValue, AddressType address, RegisterID result)
3703     {
3704         atomicRelaxedStrongCAS&lt;16&gt;(cond, expectedAndResult, newValue, address, result);
3705     }
3706 
3707     template&lt;typename AddressType&gt;
3708     void atomicRelaxedStrongCAS32(StatusCondition cond, RegisterID expectedAndResult, RegisterID newValue, AddressType address, RegisterID result)
3709     {
3710         atomicRelaxedStrongCAS&lt;32&gt;(cond, expectedAndResult, newValue, address, result);
3711     }
3712 
3713     template&lt;typename AddressType&gt;
3714     void atomicRelaxedStrongCAS64(StatusCondition cond, RegisterID expectedAndResult, RegisterID newValue, AddressType address, RegisterID result)
3715     {
3716         atomicRelaxedStrongCAS&lt;64&gt;(cond, expectedAndResult, newValue, address, result);
3717     }
3718 
3719     template&lt;typename AddressType&gt;
3720     JumpList branchAtomicWeakCAS8(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, AddressType address)
3721     {
3722         return branchAtomicWeakCAS&lt;8&gt;(cond, expectedAndClobbered, newValue, address);
3723     }
3724 
3725     template&lt;typename AddressType&gt;
3726     JumpList branchAtomicWeakCAS16(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, AddressType address)
3727     {
3728         return branchAtomicWeakCAS&lt;16&gt;(cond, expectedAndClobbered, newValue, address);
3729     }
3730 
3731     template&lt;typename AddressType&gt;
3732     JumpList branchAtomicWeakCAS32(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, AddressType address)
3733     {
3734         return branchAtomicWeakCAS&lt;32&gt;(cond, expectedAndClobbered, newValue, address);
3735     }
3736 
3737     template&lt;typename AddressType&gt;
3738     JumpList branchAtomicWeakCAS64(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, AddressType address)
3739     {
3740         return branchAtomicWeakCAS&lt;64&gt;(cond, expectedAndClobbered, newValue, address);
3741     }
3742 
3743     template&lt;typename AddressType&gt;
3744     JumpList branchAtomicRelaxedWeakCAS8(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, AddressType address)
3745     {
3746         return branchAtomicRelaxedWeakCAS&lt;8&gt;(cond, expectedAndClobbered, newValue, address);
3747     }
3748 
3749     template&lt;typename AddressType&gt;
3750     JumpList branchAtomicRelaxedWeakCAS16(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, AddressType address)
3751     {
3752         return branchAtomicRelaxedWeakCAS&lt;16&gt;(cond, expectedAndClobbered, newValue, address);
3753     }
3754 
3755     template&lt;typename AddressType&gt;
3756     JumpList branchAtomicRelaxedWeakCAS32(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, AddressType address)
3757     {
3758         return branchAtomicRelaxedWeakCAS&lt;32&gt;(cond, expectedAndClobbered, newValue, address);
3759     }
3760 
3761     template&lt;typename AddressType&gt;
3762     JumpList branchAtomicRelaxedWeakCAS64(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, AddressType address)
3763     {
3764         return branchAtomicRelaxedWeakCAS&lt;64&gt;(cond, expectedAndClobbered, newValue, address);
3765     }
3766 
3767     void depend32(RegisterID src, RegisterID dest)
3768     {
3769         m_assembler.eor&lt;32&gt;(dest, src, src);
3770     }
3771 
3772     void depend64(RegisterID src, RegisterID dest)
3773     {
3774         m_assembler.eor&lt;64&gt;(dest, src, src);
3775     }
3776 
3777     ALWAYS_INLINE static bool supportsDoubleToInt32ConversionUsingJavaScriptSemantics()
3778     {
3779 #if HAVE(FJCVTZS_INSTRUCTION)
3780         return true;
3781 #else
3782         if (s_jscvtCheckState == CPUIDCheckState::NotChecked)
3783             collectCPUFeatures();
3784 
3785         return s_jscvtCheckState == CPUIDCheckState::Set;
3786 #endif
3787     }
3788 
3789     void convertDoubleToInt32UsingJavaScriptSemantics(FPRegisterID src, RegisterID dest)
3790     {
3791         m_assembler.fjcvtzs(dest, src); // This zero extends.
3792     }
3793 
3794 #if ENABLE(FAST_TLS_JIT)
3795     // This will use scratch registers if the offset is not legal.
3796 
3797     void loadFromTLS32(uint32_t offset, RegisterID dst)
3798     {
3799         m_assembler.mrs_TPIDRRO_EL0(dst);
3800         and64(TrustedImm32(~7), dst);
3801         load32(Address(dst, offset), dst);
3802     }
3803 
3804     void loadFromTLS64(uint32_t offset, RegisterID dst)
3805     {
3806         m_assembler.mrs_TPIDRRO_EL0(dst);
3807         and64(TrustedImm32(~7), dst);
3808         load64(Address(dst, offset), dst);
3809     }
3810 
3811     static bool loadFromTLSPtrNeedsMacroScratchRegister()
3812     {
3813         return true;
3814     }
3815 
3816     void storeToTLS32(RegisterID src, uint32_t offset)
3817     {
3818         RegisterID tmp = getCachedDataTempRegisterIDAndInvalidate();
3819         ASSERT(src != tmp);
3820         m_assembler.mrs_TPIDRRO_EL0(tmp);
3821         and64(TrustedImm32(~7), tmp);
3822         store32(src, Address(tmp, offset));
3823     }
3824 
3825     void storeToTLS64(RegisterID src, uint32_t offset)
3826     {
3827         RegisterID tmp = getCachedDataTempRegisterIDAndInvalidate();
3828         ASSERT(src != tmp);
3829         m_assembler.mrs_TPIDRRO_EL0(tmp);
3830         and64(TrustedImm32(~7), tmp);
3831         store64(src, Address(tmp, offset));
3832     }
3833 
3834     static bool storeToTLSPtrNeedsMacroScratchRegister()
3835     {
3836         return true;
3837     }
3838 #endif // ENABLE(FAST_TLS_JIT)
3839 
3840     // Misc helper functions.
3841 
3842     // Invert a relational condition, e.g. == becomes !=, &lt; becomes &gt;=, etc.
3843     static RelationalCondition invert(RelationalCondition cond)
3844     {
3845         return static_cast&lt;RelationalCondition&gt;(Assembler::invert(static_cast&lt;Assembler::Condition&gt;(cond)));
3846     }
3847 
3848     static Optional&lt;ResultCondition&gt; commuteCompareToZeroIntoTest(RelationalCondition cond)
3849     {
3850         switch (cond) {
3851         case Equal:
3852             return Zero;
3853         case NotEqual:
3854             return NonZero;
3855         case LessThan:
3856             return Signed;
3857         case GreaterThanOrEqual:
3858             return PositiveOrZero;
3859             break;
3860         default:
3861             return WTF::nullopt;
3862         }
3863     }
3864 
3865     template&lt;PtrTag resultTag, PtrTag locationTag&gt;
3866     static FunctionPtr&lt;resultTag&gt; readCallTarget(CodeLocationCall&lt;locationTag&gt; call)
3867     {
3868         return FunctionPtr&lt;resultTag&gt;(MacroAssemblerCodePtr&lt;resultTag&gt;(Assembler::readCallTarget(call.dataLocation())));
3869     }
3870 
3871     template&lt;PtrTag tag&gt;
3872     static void replaceWithVMHalt(CodeLocationLabel&lt;tag&gt; instructionStart)
3873     {
3874         Assembler::replaceWithVMHalt(instructionStart.dataLocation());
3875     }
3876 
3877     template&lt;PtrTag startTag, PtrTag destTag&gt;
3878     static void replaceWithJump(CodeLocationLabel&lt;startTag&gt; instructionStart, CodeLocationLabel&lt;destTag&gt; destination)
3879     {
3880         Assembler::replaceWithJump(instructionStart.dataLocation(), destination.dataLocation());
3881     }
3882 
3883     static ptrdiff_t maxJumpReplacementSize()
3884     {
3885         return Assembler::maxJumpReplacementSize();
3886     }
3887 
3888     static ptrdiff_t patchableJumpSize()
3889     {
3890         return Assembler::patchableJumpSize();
3891     }
3892 
3893     RegisterID scratchRegisterForBlinding()
3894     {
3895         // We *do not* have a scratch register for blinding.
3896         RELEASE_ASSERT_NOT_REACHED();
3897         return getCachedDataTempRegisterIDAndInvalidate();
3898     }
3899 
3900     static bool canJumpReplacePatchableBranchPtrWithPatch() { return false; }
3901     static bool canJumpReplacePatchableBranch32WithPatch() { return false; }
3902 
3903     template&lt;PtrTag tag&gt;
3904     static CodeLocationLabel&lt;tag&gt; startOfBranchPtrWithPatchOnRegister(CodeLocationDataLabelPtr&lt;tag&gt; label)
3905     {
3906         return label.labelAtOffset(0);
3907     }
3908 
3909     template&lt;PtrTag tag&gt;
3910     static CodeLocationLabel&lt;tag&gt; startOfPatchableBranchPtrWithPatchOnAddress(CodeLocationDataLabelPtr&lt;tag&gt;)
3911     {
3912         UNREACHABLE_FOR_PLATFORM();
3913         return CodeLocationLabel&lt;tag&gt;();
3914     }
3915 
3916     template&lt;PtrTag tag&gt;
3917     static CodeLocationLabel&lt;tag&gt; startOfPatchableBranch32WithPatchOnAddress(CodeLocationDataLabel32&lt;tag&gt;)
3918     {
3919         UNREACHABLE_FOR_PLATFORM();
3920         return CodeLocationLabel&lt;tag&gt;();
3921     }
3922 
3923     template&lt;PtrTag tag&gt;
3924     static void revertJumpReplacementToBranchPtrWithPatch(CodeLocationLabel&lt;tag&gt; instructionStart, RegisterID, void* initialValue)
3925     {
3926         reemitInitialMoveWithPatch(instructionStart.dataLocation(), initialValue);
3927     }
3928 
3929     template&lt;PtrTag tag&gt;
3930     static void revertJumpReplacementToPatchableBranchPtrWithPatch(CodeLocationLabel&lt;tag&gt;, Address, void*)
3931     {
3932         UNREACHABLE_FOR_PLATFORM();
3933     }
3934 
3935     template&lt;PtrTag tag&gt;
3936     static void revertJumpReplacementToPatchableBranch32WithPatch(CodeLocationLabel&lt;tag&gt;, Address, int32_t)
3937     {
3938         UNREACHABLE_FOR_PLATFORM();
3939     }
3940 
3941     template&lt;PtrTag callTag, PtrTag destTag&gt;
3942     static void repatchCall(CodeLocationCall&lt;callTag&gt; call, CodeLocationLabel&lt;destTag&gt; destination)
3943     {
3944         Assembler::repatchPointer(call.dataLabelPtrAtOffset(REPATCH_OFFSET_CALL_TO_POINTER).dataLocation(), destination.executableAddress());
3945     }
3946 
3947     template&lt;PtrTag callTag, PtrTag destTag&gt;
3948     static void repatchCall(CodeLocationCall&lt;callTag&gt; call, FunctionPtr&lt;destTag&gt; destination)
3949     {
3950         Assembler::repatchPointer(call.dataLabelPtrAtOffset(REPATCH_OFFSET_CALL_TO_POINTER).dataLocation(), destination.executableAddress());
3951     }
3952 
3953 protected:
3954     ALWAYS_INLINE Jump makeBranch(Assembler::Condition cond)
3955     {
3956         m_assembler.b_cond(cond);
3957         AssemblerLabel label = m_assembler.label();
3958         m_assembler.nop();
3959         return Jump(label, m_makeJumpPatchable ? Assembler::JumpConditionFixedSize : Assembler::JumpCondition, cond);
3960     }
3961     ALWAYS_INLINE Jump makeBranch(RelationalCondition cond) { return makeBranch(ARM64Condition(cond)); }
3962     ALWAYS_INLINE Jump makeBranch(ResultCondition cond) { return makeBranch(ARM64Condition(cond)); }
3963     ALWAYS_INLINE Jump makeBranch(DoubleCondition cond) { return makeBranch(ARM64Condition(cond)); }
3964 
3965     template &lt;int dataSize&gt;
3966     ALWAYS_INLINE Jump makeCompareAndBranch(ZeroCondition cond, RegisterID reg)
3967     {
3968         if (cond == IsZero)
3969             m_assembler.cbz&lt;dataSize&gt;(reg);
3970         else
3971             m_assembler.cbnz&lt;dataSize&gt;(reg);
3972         AssemblerLabel label = m_assembler.label();
3973         m_assembler.nop();
3974         return Jump(label, m_makeJumpPatchable ? Assembler::JumpCompareAndBranchFixedSize : Assembler::JumpCompareAndBranch, static_cast&lt;Assembler::Condition&gt;(cond), dataSize == 64, reg);
3975     }
3976 
3977     ALWAYS_INLINE Jump makeTestBitAndBranch(RegisterID reg, unsigned bit, ZeroCondition cond)
3978     {
3979         ASSERT(bit &lt; 64);
3980         bit &amp;= 0x3f;
3981         if (cond == IsZero)
3982             m_assembler.tbz(reg, bit);
3983         else
3984             m_assembler.tbnz(reg, bit);
3985         AssemblerLabel label = m_assembler.label();
3986         m_assembler.nop();
3987         return Jump(label, m_makeJumpPatchable ? Assembler::JumpTestBitFixedSize : Assembler::JumpTestBit, static_cast&lt;Assembler::Condition&gt;(cond), bit, reg);
3988     }
3989 
3990     Assembler::Condition ARM64Condition(RelationalCondition cond)
3991     {
3992         return static_cast&lt;Assembler::Condition&gt;(cond);
3993     }
3994 
3995     Assembler::Condition ARM64Condition(ResultCondition cond)
3996     {
3997         return static_cast&lt;Assembler::Condition&gt;(cond);
3998     }
3999 
4000     Assembler::Condition ARM64Condition(DoubleCondition cond)
4001     {
4002         return static_cast&lt;Assembler::Condition&gt;(cond);
4003     }
4004 
4005 protected:
4006     ALWAYS_INLINE RegisterID getCachedDataTempRegisterIDAndInvalidate()
4007     {
4008         RELEASE_ASSERT(m_allowScratchRegister);
4009         return dataMemoryTempRegister().registerIDInvalidate();
4010     }
4011     ALWAYS_INLINE RegisterID getCachedMemoryTempRegisterIDAndInvalidate()
4012     {
4013         RELEASE_ASSERT(m_allowScratchRegister);
4014         return cachedMemoryTempRegister().registerIDInvalidate();
4015     }
4016     ALWAYS_INLINE CachedTempRegister&amp; dataMemoryTempRegister()
4017     {
4018         RELEASE_ASSERT(m_allowScratchRegister);
4019         return m_dataMemoryTempRegister;
4020     }
4021     ALWAYS_INLINE CachedTempRegister&amp; cachedMemoryTempRegister()
4022     {
4023         RELEASE_ASSERT(m_allowScratchRegister);
4024         return m_cachedMemoryTempRegister;
4025     }
4026 
4027     template&lt;typename ImmediateType, typename rawType&gt;
4028     void moveInternal(ImmediateType imm, RegisterID dest)
4029     {
4030         const int dataSize = sizeof(rawType) * 8;
4031         const int numberHalfWords = dataSize / 16;
4032         rawType value = bitwise_cast&lt;rawType&gt;(imm.m_value);
4033         uint16_t halfword[numberHalfWords];
4034 
4035         // Handle 0 and ~0 here to simplify code below
4036         if (!value) {
4037             m_assembler.movz&lt;dataSize&gt;(dest, 0);
4038             return;
4039         }
4040         if (!~value) {
4041             m_assembler.movn&lt;dataSize&gt;(dest, 0);
4042             return;
4043         }
4044 
4045         LogicalImmediate logicalImm = dataSize == 64 ? LogicalImmediate::create64(static_cast&lt;uint64_t&gt;(value)) : LogicalImmediate::create32(static_cast&lt;uint32_t&gt;(value));
4046 
4047         if (logicalImm.isValid()) {
4048             m_assembler.movi&lt;dataSize&gt;(dest, logicalImm);
4049             return;
4050         }
4051 
4052         // Figure out how many halfwords are 0 or FFFF, then choose movz or movn accordingly.
4053         int zeroOrNegateVote = 0;
4054         for (int i = 0; i &lt; numberHalfWords; ++i) {
4055             halfword[i] = getHalfword(value, i);
4056             if (!halfword[i])
4057                 zeroOrNegateVote++;
4058             else if (halfword[i] == 0xffff)
4059                 zeroOrNegateVote--;
4060         }
4061 
4062         bool needToClearRegister = true;
4063         if (zeroOrNegateVote &gt;= 0) {
4064             for (int i = 0; i &lt; numberHalfWords; i++) {
4065                 if (halfword[i]) {
4066                     if (needToClearRegister) {
4067                         m_assembler.movz&lt;dataSize&gt;(dest, halfword[i], 16*i);
4068                         needToClearRegister = false;
4069                     } else
4070                         m_assembler.movk&lt;dataSize&gt;(dest, halfword[i], 16*i);
4071                 }
4072             }
4073         } else {
4074             for (int i = 0; i &lt; numberHalfWords; i++) {
4075                 if (halfword[i] != 0xffff) {
4076                     if (needToClearRegister) {
4077                         m_assembler.movn&lt;dataSize&gt;(dest, ~halfword[i], 16*i);
4078                         needToClearRegister = false;
4079                     } else
4080                         m_assembler.movk&lt;dataSize&gt;(dest, halfword[i], 16*i);
4081                 }
4082             }
4083         }
4084     }
4085 
4086     template&lt;int datasize&gt;
4087     ALWAYS_INLINE void loadUnsignedImmediate(RegisterID rt, RegisterID rn, unsigned pimm)
4088     {
4089         m_assembler.ldr&lt;datasize&gt;(rt, rn, pimm);
4090     }
4091 
4092     template&lt;int datasize&gt;
4093     ALWAYS_INLINE void loadUnscaledImmediate(RegisterID rt, RegisterID rn, int simm)
4094     {
4095         m_assembler.ldur&lt;datasize&gt;(rt, rn, simm);
4096     }
4097 
4098     template&lt;int datasize&gt;
4099     ALWAYS_INLINE void loadSignedAddressedByUnsignedImmediate(RegisterID rt, RegisterID rn, unsigned pimm)
4100     {
4101         loadUnsignedImmediate&lt;datasize&gt;(rt, rn, pimm);
4102     }
4103 
4104     template&lt;int datasize&gt;
4105     ALWAYS_INLINE void loadSignedAddressedByUnscaledImmediate(RegisterID rt, RegisterID rn, int simm)
4106     {
4107         loadUnscaledImmediate&lt;datasize&gt;(rt, rn, simm);
4108     }
4109 
4110     template&lt;int datasize&gt;
4111     ALWAYS_INLINE void storeUnsignedImmediate(RegisterID rt, RegisterID rn, unsigned pimm)
4112     {
4113         m_assembler.str&lt;datasize&gt;(rt, rn, pimm);
4114     }
4115 
4116     template&lt;int datasize&gt;
4117     ALWAYS_INLINE void storeUnscaledImmediate(RegisterID rt, RegisterID rn, int simm)
4118     {
4119         m_assembler.stur&lt;datasize&gt;(rt, rn, simm);
4120     }
4121 
4122     void moveWithFixedWidth(TrustedImm32 imm, RegisterID dest)
4123     {
4124         int32_t value = imm.m_value;
4125         m_assembler.movz&lt;32&gt;(dest, getHalfword(value, 0));
4126         m_assembler.movk&lt;32&gt;(dest, getHalfword(value, 1), 16);
4127     }
4128 
4129     void moveWithFixedWidth(TrustedImmPtr imm, RegisterID dest)
4130     {
4131         intptr_t value = reinterpret_cast&lt;intptr_t&gt;(imm.m_value);
4132         m_assembler.movz&lt;64&gt;(dest, getHalfword(value, 0));
4133         m_assembler.movk&lt;64&gt;(dest, getHalfword(value, 1), 16);
4134         m_assembler.movk&lt;64&gt;(dest, getHalfword(value, 2), 32);
4135         if (Assembler::MAX_POINTER_BITS &gt; 48)
4136             m_assembler.movk&lt;64&gt;(dest, getHalfword(value, 3), 48);
4137     }
4138 
4139     void signExtend32ToPtrWithFixedWidth(int32_t value, RegisterID dest)
4140     {
4141         if (value &gt;= 0) {
4142             m_assembler.movz&lt;32&gt;(dest, getHalfword(value, 0));
4143             m_assembler.movk&lt;32&gt;(dest, getHalfword(value, 1), 16);
4144         } else {
4145             m_assembler.movn&lt;32&gt;(dest, ~getHalfword(value, 0));
4146             m_assembler.movk&lt;32&gt;(dest, getHalfword(value, 1), 16);
4147         }
4148     }
4149 
4150     template&lt;int datasize&gt;
4151     ALWAYS_INLINE void load(const void* address, RegisterID dest)
4152     {
4153         intptr_t currentRegisterContents;
4154         if (cachedMemoryTempRegister().value(currentRegisterContents)) {
4155             intptr_t addressAsInt = reinterpret_cast&lt;intptr_t&gt;(address);
4156             intptr_t addressDelta = addressAsInt - currentRegisterContents;
4157 
4158             if (dest == memoryTempRegister)
4159                 cachedMemoryTempRegister().invalidate();
4160 
4161             if (isInt&lt;32&gt;(addressDelta)) {
4162                 if (Assembler::canEncodeSImmOffset(addressDelta)) {
4163                     m_assembler.ldur&lt;datasize&gt;(dest,  memoryTempRegister, addressDelta);
4164                     return;
4165                 }
4166 
4167                 if (Assembler::canEncodePImmOffset&lt;datasize&gt;(addressDelta)) {
4168                     m_assembler.ldr&lt;datasize&gt;(dest,  memoryTempRegister, addressDelta);
4169                     return;
4170                 }
4171             }
4172 
4173             if ((addressAsInt &amp; (~maskHalfWord0)) == (currentRegisterContents &amp; (~maskHalfWord0))) {
4174                 m_assembler.movk&lt;64&gt;(memoryTempRegister, addressAsInt &amp; maskHalfWord0, 0);
4175                 cachedMemoryTempRegister().setValue(reinterpret_cast&lt;intptr_t&gt;(address));
4176                 m_assembler.ldr&lt;datasize&gt;(dest, memoryTempRegister, ARM64Registers::zr);
4177                 return;
4178             }
4179         }
4180 
4181         move(TrustedImmPtr(address), memoryTempRegister);
4182         if (dest == memoryTempRegister)
4183             cachedMemoryTempRegister().invalidate();
4184         else
4185             cachedMemoryTempRegister().setValue(reinterpret_cast&lt;intptr_t&gt;(address));
4186         m_assembler.ldr&lt;datasize&gt;(dest, memoryTempRegister, ARM64Registers::zr);
4187     }
4188 
4189     template&lt;int datasize&gt;
4190     ALWAYS_INLINE void store(RegisterID src, const void* address)
4191     {
4192         ASSERT(src != memoryTempRegister);
4193         intptr_t currentRegisterContents;
4194         if (cachedMemoryTempRegister().value(currentRegisterContents)) {
4195             intptr_t addressAsInt = reinterpret_cast&lt;intptr_t&gt;(address);
4196             intptr_t addressDelta = addressAsInt - currentRegisterContents;
4197 
4198             if (isInt&lt;32&gt;(addressDelta)) {
4199                 if (Assembler::canEncodeSImmOffset(addressDelta)) {
4200                     m_assembler.stur&lt;datasize&gt;(src, memoryTempRegister, addressDelta);
4201                     return;
4202                 }
4203 
4204                 if (Assembler::canEncodePImmOffset&lt;datasize&gt;(addressDelta)) {
4205                     m_assembler.str&lt;datasize&gt;(src, memoryTempRegister, addressDelta);
4206                     return;
4207                 }
4208             }
4209 
4210             if ((addressAsInt &amp; (~maskHalfWord0)) == (currentRegisterContents &amp; (~maskHalfWord0))) {
4211                 m_assembler.movk&lt;64&gt;(memoryTempRegister, addressAsInt &amp; maskHalfWord0, 0);
4212                 cachedMemoryTempRegister().setValue(reinterpret_cast&lt;intptr_t&gt;(address));
4213                 m_assembler.str&lt;datasize&gt;(src, memoryTempRegister, ARM64Registers::zr);
4214                 return;
4215             }
4216         }
4217 
4218         move(TrustedImmPtr(address), memoryTempRegister);
4219         cachedMemoryTempRegister().setValue(reinterpret_cast&lt;intptr_t&gt;(address));
4220         m_assembler.str&lt;datasize&gt;(src, memoryTempRegister, ARM64Registers::zr);
4221     }
4222 
4223     template &lt;int dataSize&gt;
4224     ALWAYS_INLINE bool tryMoveUsingCacheRegisterContents(intptr_t immediate, CachedTempRegister&amp; dest)
4225     {
4226         intptr_t currentRegisterContents;
4227         if (dest.value(currentRegisterContents)) {
4228             if (currentRegisterContents == immediate)
4229                 return true;
4230 
4231             LogicalImmediate logicalImm = dataSize == 64 ? LogicalImmediate::create64(static_cast&lt;uint64_t&gt;(immediate)) : LogicalImmediate::create32(static_cast&lt;uint32_t&gt;(immediate));
4232 
4233             if (logicalImm.isValid()) {
4234                 m_assembler.movi&lt;dataSize&gt;(dest.registerIDNoInvalidate(), logicalImm);
4235                 dest.setValue(immediate);
4236                 return true;
4237             }
4238 
4239             if ((immediate &amp; maskUpperWord) == (currentRegisterContents &amp; maskUpperWord)) {
4240                 if ((immediate &amp; maskHalfWord1) != (currentRegisterContents &amp; maskHalfWord1))
4241                     m_assembler.movk&lt;dataSize&gt;(dest.registerIDNoInvalidate(), (immediate &amp; maskHalfWord1) &gt;&gt; 16, 16);
4242 
4243                 if ((immediate &amp; maskHalfWord0) != (currentRegisterContents &amp; maskHalfWord0))
4244                     m_assembler.movk&lt;dataSize&gt;(dest.registerIDNoInvalidate(), immediate &amp; maskHalfWord0, 0);
4245 
4246                 dest.setValue(immediate);
4247                 return true;
4248             }
4249         }
4250 
4251         return false;
4252     }
4253 
4254     void moveToCachedReg(TrustedImm32 imm, CachedTempRegister&amp; dest)
4255     {
4256         if (tryMoveUsingCacheRegisterContents&lt;32&gt;(static_cast&lt;intptr_t&gt;(imm.m_value), dest))
4257             return;
4258 
4259         moveInternal&lt;TrustedImm32, int32_t&gt;(imm, dest.registerIDNoInvalidate());
4260         dest.setValue(imm.m_value);
4261     }
4262 
4263     void moveToCachedReg(TrustedImmPtr imm, CachedTempRegister&amp; dest)
4264     {
4265         if (tryMoveUsingCacheRegisterContents&lt;64&gt;(imm.asIntptr(), dest))
4266             return;
4267 
4268         moveInternal&lt;TrustedImmPtr, intptr_t&gt;(imm, dest.registerIDNoInvalidate());
4269         dest.setValue(imm.asIntptr());
4270     }
4271 
4272     void moveToCachedReg(TrustedImm64 imm, CachedTempRegister&amp; dest)
4273     {
4274         if (tryMoveUsingCacheRegisterContents&lt;64&gt;(static_cast&lt;intptr_t&gt;(imm.m_value), dest))
4275             return;
4276 
4277         moveInternal&lt;TrustedImm64, int64_t&gt;(imm, dest.registerIDNoInvalidate());
4278         dest.setValue(imm.m_value);
4279     }
4280 
4281     template&lt;int datasize&gt;
4282     ALWAYS_INLINE bool tryLoadWithOffset(RegisterID rt, RegisterID rn, int32_t offset)
4283     {
4284         if (Assembler::canEncodeSImmOffset(offset)) {
4285             loadUnscaledImmediate&lt;datasize&gt;(rt, rn, offset);
4286             return true;
4287         }
4288         if (Assembler::canEncodePImmOffset&lt;datasize&gt;(offset)) {
4289             loadUnsignedImmediate&lt;datasize&gt;(rt, rn, static_cast&lt;unsigned&gt;(offset));
4290             return true;
4291         }
4292         return false;
4293     }
4294 
4295     template&lt;int datasize&gt;
4296     ALWAYS_INLINE bool tryLoadSignedWithOffset(RegisterID rt, RegisterID rn, int32_t offset)
4297     {
4298         if (Assembler::canEncodeSImmOffset(offset)) {
4299             loadSignedAddressedByUnscaledImmediate&lt;datasize&gt;(rt, rn, offset);
4300             return true;
4301         }
4302         if (Assembler::canEncodePImmOffset&lt;datasize&gt;(offset)) {
4303             loadSignedAddressedByUnsignedImmediate&lt;datasize&gt;(rt, rn, static_cast&lt;unsigned&gt;(offset));
4304             return true;
4305         }
4306         return false;
4307     }
4308 
4309     template&lt;int datasize&gt;
4310     ALWAYS_INLINE bool tryLoadWithOffset(FPRegisterID rt, RegisterID rn, int32_t offset)
4311     {
4312         if (Assembler::canEncodeSImmOffset(offset)) {
4313             m_assembler.ldur&lt;datasize&gt;(rt, rn, offset);
4314             return true;
4315         }
4316         if (Assembler::canEncodePImmOffset&lt;datasize&gt;(offset)) {
4317             m_assembler.ldr&lt;datasize&gt;(rt, rn, static_cast&lt;unsigned&gt;(offset));
4318             return true;
4319         }
4320         return false;
4321     }
4322 
4323     template&lt;int datasize&gt;
4324     ALWAYS_INLINE bool tryStoreWithOffset(RegisterID rt, RegisterID rn, int32_t offset)
4325     {
4326         if (Assembler::canEncodeSImmOffset(offset)) {
4327             storeUnscaledImmediate&lt;datasize&gt;(rt, rn, offset);
4328             return true;
4329         }
4330         if (Assembler::canEncodePImmOffset&lt;datasize&gt;(offset)) {
4331             storeUnsignedImmediate&lt;datasize&gt;(rt, rn, static_cast&lt;unsigned&gt;(offset));
4332             return true;
4333         }
4334         return false;
4335     }
4336 
4337     template&lt;int datasize&gt;
4338     ALWAYS_INLINE bool tryStoreWithOffset(FPRegisterID rt, RegisterID rn, int32_t offset)
4339     {
4340         if (Assembler::canEncodeSImmOffset(offset)) {
4341             m_assembler.stur&lt;datasize&gt;(rt, rn, offset);
4342             return true;
4343         }
4344         if (Assembler::canEncodePImmOffset&lt;datasize&gt;(offset)) {
4345             m_assembler.str&lt;datasize&gt;(rt, rn, static_cast&lt;unsigned&gt;(offset));
4346             return true;
4347         }
4348         return false;
4349     }
4350 
4351     template&lt;int datasize&gt;
4352     void loadLink(RegisterID src, RegisterID dest)
4353     {
4354         m_assembler.ldxr&lt;datasize&gt;(dest, src);
4355     }
4356 
4357     template&lt;int datasize&gt;
4358     void loadLinkAcq(RegisterID src, RegisterID dest)
4359     {
4360         m_assembler.ldaxr&lt;datasize&gt;(dest, src);
4361     }
4362 
4363     template&lt;int datasize&gt;
4364     void storeCond(RegisterID src, RegisterID dest, RegisterID result)
4365     {
4366         m_assembler.stxr&lt;datasize&gt;(src, dest, result);
4367     }
4368 
4369     template&lt;int datasize&gt;
4370     void storeCondRel(RegisterID src, RegisterID dest, RegisterID result)
4371     {
4372         m_assembler.stlxr&lt;datasize&gt;(result, src, dest);
4373     }
4374 
4375     template&lt;int datasize&gt;
4376     void signExtend(RegisterID src, RegisterID dest)
4377     {
4378         move(src, dest);
4379     }
4380 
4381     template&lt;int datasize&gt;
4382     Jump branch(RelationalCondition cond, RegisterID left, RegisterID right)
4383     {
4384         return branch32(cond, left, right);
4385     }
4386 
4387     template&lt;int datasize&gt;
4388     void atomicStrongCAS(StatusCondition cond, RegisterID expectedAndResult, RegisterID newValue, Address address, RegisterID result)
4389     {
4390         signExtend&lt;datasize&gt;(expectedAndResult, expectedAndResult);
4391 
4392         RegisterID simpleAddress = extractSimpleAddress(address);
4393         RegisterID tmp = getCachedDataTempRegisterIDAndInvalidate();
4394 
4395         Label reloop = label();
4396         loadLinkAcq&lt;datasize&gt;(simpleAddress, tmp);
4397         Jump failure = branch&lt;datasize&gt;(NotEqual, expectedAndResult, tmp);
4398 
4399         storeCondRel&lt;datasize&gt;(newValue, simpleAddress, result);
4400         branchTest32(NonZero, result).linkTo(reloop, this);
4401         move(TrustedImm32(cond == Success), result);
4402         Jump done = jump();
4403 
4404         failure.link(this);
4405         move(tmp, expectedAndResult);
4406         storeCondRel&lt;datasize&gt;(tmp, simpleAddress, result);
4407         branchTest32(NonZero, result).linkTo(reloop, this);
4408         move(TrustedImm32(cond == Failure), result);
4409 
4410         done.link(this);
4411     }
4412 
4413     template&lt;int datasize, typename AddressType&gt;
4414     void atomicRelaxedStrongCAS(StatusCondition cond, RegisterID expectedAndResult, RegisterID newValue, AddressType address, RegisterID result)
4415     {
4416         signExtend&lt;datasize&gt;(expectedAndResult, expectedAndResult);
4417 
4418         RegisterID simpleAddress = extractSimpleAddress(address);
4419         RegisterID tmp = getCachedDataTempRegisterIDAndInvalidate();
4420 
4421         Label reloop = label();
4422         loadLink&lt;datasize&gt;(simpleAddress, tmp);
4423         Jump failure = branch&lt;datasize&gt;(NotEqual, expectedAndResult, tmp);
4424 
4425         storeCond&lt;datasize&gt;(newValue, simpleAddress, result);
4426         branchTest32(NonZero, result).linkTo(reloop, this);
4427         move(TrustedImm32(cond == Success), result);
4428         Jump done = jump();
4429 
4430         failure.link(this);
4431         move(tmp, expectedAndResult);
4432         move(TrustedImm32(cond == Failure), result);
4433 
4434         done.link(this);
4435     }
4436 
4437     template&lt;int datasize, typename AddressType&gt;
4438     JumpList branchAtomicWeakCAS(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, AddressType address)
4439     {
4440         signExtend&lt;datasize&gt;(expectedAndClobbered, expectedAndClobbered);
4441 
4442         RegisterID simpleAddress = extractSimpleAddress(address);
4443         RegisterID tmp = getCachedDataTempRegisterIDAndInvalidate();
4444 
4445         JumpList success;
4446         JumpList failure;
4447 
4448         loadLinkAcq&lt;datasize&gt;(simpleAddress, tmp);
4449         failure.append(branch&lt;datasize&gt;(NotEqual, expectedAndClobbered, tmp));
4450         storeCondRel&lt;datasize&gt;(newValue, simpleAddress, expectedAndClobbered);
4451 
4452         switch (cond) {
4453         case Success:
4454             success.append(branchTest32(Zero, expectedAndClobbered));
4455             failure.link(this);
4456             return success;
4457         case Failure:
4458             failure.append(branchTest32(NonZero, expectedAndClobbered));
4459             return failure;
4460         }
4461 
4462         RELEASE_ASSERT_NOT_REACHED();
4463     }
4464 
4465     template&lt;int datasize, typename AddressType&gt;
4466     JumpList branchAtomicRelaxedWeakCAS(StatusCondition cond, RegisterID expectedAndClobbered, RegisterID newValue, AddressType address)
4467     {
4468         signExtend&lt;datasize&gt;(expectedAndClobbered, expectedAndClobbered);
4469 
4470         RegisterID simpleAddress = extractSimpleAddress(address);
4471         RegisterID tmp = getCachedDataTempRegisterIDAndInvalidate();
4472 
4473         JumpList success;
4474         JumpList failure;
4475 
4476         loadLink&lt;datasize&gt;(simpleAddress, tmp);
4477         failure.append(branch&lt;datasize&gt;(NotEqual, expectedAndClobbered, tmp));
4478         storeCond&lt;datasize&gt;(newValue, simpleAddress, expectedAndClobbered);
4479 
4480         switch (cond) {
4481         case Success:
4482             success.append(branchTest32(Zero, expectedAndClobbered));
4483             failure.link(this);
4484             return success;
4485         case Failure:
4486             failure.append(branchTest32(NonZero, expectedAndClobbered));
4487             return failure;
4488         }
4489 
4490         RELEASE_ASSERT_NOT_REACHED();
4491     }
4492 
4493     RegisterID extractSimpleAddress(ImplicitAddress address)
4494     {
4495         if (!address.offset)
4496             return address.base;
4497 
4498         signExtend32ToPtr(TrustedImm32(address.offset), getCachedMemoryTempRegisterIDAndInvalidate());
4499         add64(address.base, memoryTempRegister);
4500         return memoryTempRegister;
4501     }
4502 
4503     // This uses both the memory and data temp, but only returns the memorty temp. So you can use the
4504     // data temp after this finishes.
4505     RegisterID extractSimpleAddress(BaseIndex address)
4506     {
4507         RegisterID result = getCachedMemoryTempRegisterIDAndInvalidate();
4508         lshift64(address.index, TrustedImm32(address.scale), result);
4509         add64(address.base, result);
4510         add64(TrustedImm32(address.offset), result);
4511         return result;
4512     }
4513 
4514     Jump jumpAfterFloatingPointCompare(DoubleCondition cond)
4515     {
4516         if (cond == DoubleNotEqual) {
4517             // ConditionNE jumps if NotEqual *or* unordered - force the unordered cases not to jump.
4518             Jump unordered = makeBranch(Assembler::ConditionVS);
4519             Jump result = makeBranch(Assembler::ConditionNE);
4520             unordered.link(this);
4521             return result;
4522         }
4523         if (cond == DoubleEqualOrUnordered) {
4524             Jump unordered = makeBranch(Assembler::ConditionVS);
4525             Jump notEqual = makeBranch(Assembler::ConditionNE);
4526             unordered.link(this);
4527             // We get here if either unordered or equal.
4528             Jump result = jump();
4529             notEqual.link(this);
4530             return result;
4531         }
4532         return makeBranch(cond);
4533     }
4534 
4535     template&lt;typename Function&gt;
4536     void floatingPointCompare(DoubleCondition cond, FPRegisterID left, FPRegisterID right, RegisterID dest, Function compare)
4537     {
4538         if (cond == DoubleNotEqual) {
4539             // ConditionNE sets 1 if NotEqual *or* unordered - force the unordered cases not to set 1.
4540             move(TrustedImm32(0), dest);
4541             compare(left, right);
4542             Jump unordered = makeBranch(Assembler::ConditionVS);
4543             m_assembler.cset&lt;32&gt;(dest, Assembler::ConditionNE);
4544             unordered.link(this);
4545             return;
4546         }
4547         if (cond == DoubleEqualOrUnordered) {
4548             // ConditionEQ sets 1 only if Equal - force the unordered cases to set 1 too.
4549             move(TrustedImm32(1), dest);
4550             compare(left, right);
4551             Jump unordered = makeBranch(Assembler::ConditionVS);
4552             m_assembler.cset&lt;32&gt;(dest, Assembler::ConditionEQ);
4553             unordered.link(this);
4554             return;
4555         }
4556         compare(left, right);
4557         m_assembler.cset&lt;32&gt;(dest, ARM64Condition(cond));
4558     }
4559 
4560     friend class LinkBuffer;
4561 
4562     template&lt;PtrTag tag&gt;
4563     static void linkCall(void* code, Call call, FunctionPtr&lt;tag&gt; function)
4564     {
4565         if (!call.isFlagSet(Call::Near))
4566             Assembler::linkPointer(code, call.m_label.labelAtOffset(REPATCH_OFFSET_CALL_TO_POINTER), function.executableAddress());
4567         else if (call.isFlagSet(Call::Tail))
4568             Assembler::linkJump(code, call.m_label, function.template retaggedExecutableAddress&lt;NoPtrTag&gt;());
4569         else
4570             Assembler::linkCall(code, call.m_label, function.template retaggedExecutableAddress&lt;NoPtrTag&gt;());
4571     }
4572 
4573     JS_EXPORT_PRIVATE static void collectCPUFeatures();
4574 
4575     JS_EXPORT_PRIVATE static CPUIDCheckState s_jscvtCheckState;
4576 
4577     CachedTempRegister m_dataMemoryTempRegister;
4578     CachedTempRegister m_cachedMemoryTempRegister;
4579     bool m_makeJumpPatchable;
4580 };
4581 
4582 // Extend the {load,store}{Unsigned,Unscaled}Immediate templated general register methods to cover all load/store sizes
4583 template&lt;&gt;
4584 ALWAYS_INLINE void MacroAssemblerARM64::loadUnsignedImmediate&lt;8&gt;(RegisterID rt, RegisterID rn, unsigned pimm)
4585 {
4586     m_assembler.ldrb(rt, rn, pimm);
4587 }
4588 
4589 template&lt;&gt;
4590 ALWAYS_INLINE void MacroAssemblerARM64::loadUnsignedImmediate&lt;16&gt;(RegisterID rt, RegisterID rn, unsigned pimm)
4591 {
4592     m_assembler.ldrh(rt, rn, pimm);
4593 }
4594 
4595 template&lt;&gt;
4596 ALWAYS_INLINE void MacroAssemblerARM64::loadSignedAddressedByUnsignedImmediate&lt;8&gt;(RegisterID rt, RegisterID rn, unsigned pimm)
4597 {
4598     m_assembler.ldrsb&lt;64&gt;(rt, rn, pimm);
4599 }
4600 
4601 template&lt;&gt;
4602 ALWAYS_INLINE void MacroAssemblerARM64::loadSignedAddressedByUnsignedImmediate&lt;16&gt;(RegisterID rt, RegisterID rn, unsigned pimm)
4603 {
4604     m_assembler.ldrsh&lt;64&gt;(rt, rn, pimm);
4605 }
4606 
4607 template&lt;&gt;
4608 ALWAYS_INLINE void MacroAssemblerARM64::loadUnscaledImmediate&lt;8&gt;(RegisterID rt, RegisterID rn, int simm)
4609 {
4610     m_assembler.ldurb(rt, rn, simm);
4611 }
4612 
4613 template&lt;&gt;
4614 ALWAYS_INLINE void MacroAssemblerARM64::loadUnscaledImmediate&lt;16&gt;(RegisterID rt, RegisterID rn, int simm)
4615 {
4616     m_assembler.ldurh(rt, rn, simm);
4617 }
4618 
4619 template&lt;&gt;
4620 ALWAYS_INLINE void MacroAssemblerARM64::loadSignedAddressedByUnscaledImmediate&lt;8&gt;(RegisterID rt, RegisterID rn, int simm)
4621 {
4622     m_assembler.ldursb&lt;64&gt;(rt, rn, simm);
4623 }
4624 
4625 template&lt;&gt;
4626 ALWAYS_INLINE void MacroAssemblerARM64::loadSignedAddressedByUnscaledImmediate&lt;16&gt;(RegisterID rt, RegisterID rn, int simm)
4627 {
4628     m_assembler.ldursh&lt;64&gt;(rt, rn, simm);
4629 }
4630 
4631 template&lt;&gt;
4632 ALWAYS_INLINE void MacroAssemblerARM64::storeUnsignedImmediate&lt;8&gt;(RegisterID rt, RegisterID rn, unsigned pimm)
4633 {
4634     m_assembler.strb(rt, rn, pimm);
4635 }
4636 
4637 template&lt;&gt;
4638 ALWAYS_INLINE void MacroAssemblerARM64::storeUnsignedImmediate&lt;16&gt;(RegisterID rt, RegisterID rn, unsigned pimm)
4639 {
4640     m_assembler.strh(rt, rn, pimm);
4641 }
4642 
4643 template&lt;&gt;
4644 ALWAYS_INLINE void MacroAssemblerARM64::storeUnscaledImmediate&lt;8&gt;(RegisterID rt, RegisterID rn, int simm)
4645 {
4646     m_assembler.sturb(rt, rn, simm);
4647 }
4648 
4649 template&lt;&gt;
4650 ALWAYS_INLINE void MacroAssemblerARM64::storeUnscaledImmediate&lt;16&gt;(RegisterID rt, RegisterID rn, int simm)
4651 {
4652     m_assembler.sturh(rt, rn, simm);
4653 }
4654 
4655 template&lt;&gt;
4656 inline void MacroAssemblerARM64::signExtend&lt;8&gt;(RegisterID src, RegisterID dest)
4657 {
4658     signExtend8To32(src, dest);
4659 }
4660 
4661 template&lt;&gt;
4662 inline void MacroAssemblerARM64::signExtend&lt;16&gt;(RegisterID src, RegisterID dest)
4663 {
4664     signExtend16To32(src, dest);
4665 }
4666 
4667 template&lt;&gt;
4668 inline MacroAssemblerARM64::Jump MacroAssemblerARM64::branch&lt;64&gt;(RelationalCondition cond, RegisterID left, RegisterID right)
4669 {
4670     return branch64(cond, left, right);
4671 }
4672 
4673 } // namespace JSC
4674 
4675 #endif // ENABLE(ASSEMBLER)
    </pre>
  </body>
</html>