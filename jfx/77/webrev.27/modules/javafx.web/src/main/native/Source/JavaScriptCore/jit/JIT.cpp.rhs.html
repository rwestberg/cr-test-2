<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
<a name="1" id="anc1"></a><span class="line-modified">   2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.</span>
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 
  28 #if ENABLE(JIT)
  29 
  30 #include &quot;JIT.h&quot;
  31 
  32 #include &quot;BytecodeGraph.h&quot;
  33 #include &quot;BytecodeLivenessAnalysis.h&quot;
  34 #include &quot;CodeBlock.h&quot;
  35 #include &quot;CodeBlockWithJITType.h&quot;
  36 #include &quot;DFGCapabilities.h&quot;
  37 #include &quot;InterpreterInlines.h&quot;
  38 #include &quot;JITInlines.h&quot;
  39 #include &quot;JITOperations.h&quot;
  40 #include &quot;JSArray.h&quot;
  41 #include &quot;JSCInlines.h&quot;
  42 #include &quot;JSFunction.h&quot;
  43 #include &quot;LinkBuffer.h&quot;
  44 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;
  45 #include &quot;ModuleProgramCodeBlock.h&quot;
  46 #include &quot;PCToCodeOriginMap.h&quot;
  47 #include &quot;ProbeContext.h&quot;
  48 #include &quot;ProfilerDatabase.h&quot;
  49 #include &quot;ProgramCodeBlock.h&quot;
  50 #include &quot;ResultType.h&quot;
  51 #include &quot;SlowPathCall.h&quot;
  52 #include &quot;StackAlignment.h&quot;
  53 #include &quot;ThunkGenerators.h&quot;
  54 #include &quot;TypeProfilerLog.h&quot;
  55 #include &lt;wtf/CryptographicallyRandomNumber.h&gt;
  56 #include &lt;wtf/GraphNodeWorklist.h&gt;
  57 #include &lt;wtf/SimpleStats.h&gt;
  58 
  59 namespace JSC {
  60 namespace JITInternal {
  61 static constexpr const bool verbose = false;
  62 }
  63 
  64 Seconds totalBaselineCompileTime;
  65 Seconds totalDFGCompileTime;
  66 Seconds totalFTLCompileTime;
  67 Seconds totalFTLDFGCompileTime;
  68 Seconds totalFTLB3CompileTime;
  69 
  70 void ctiPatchCallByReturnAddress(ReturnAddressPtr returnAddress, FunctionPtr&lt;CFunctionPtrTag&gt; newCalleeFunction)
  71 {
  72     MacroAssembler::repatchCall(
  73         CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)),
  74         newCalleeFunction.retagged&lt;OperationPtrTag&gt;());
  75 }
  76 
<a name="2" id="anc2"></a><span class="line-modified">  77 JIT::JIT(VM&amp; vm, CodeBlock* codeBlock, unsigned loopOSREntryBytecodeOffset)</span>
<span class="line-modified">  78     : JSInterfaceJIT(&amp;vm, codeBlock)</span>
<span class="line-modified">  79     , m_interpreter(vm.interpreter)</span>
  80     , m_labels(codeBlock ? codeBlock-&gt;instructions().size() : 0)
  81     , m_bytecodeOffset(std::numeric_limits&lt;unsigned&gt;::max())
<a name="3" id="anc3"></a><span class="line-modified">  82     , m_pcToCodeOriginMapBuilder(vm)</span>
  83     , m_canBeOptimized(false)
  84     , m_shouldEmitProfiling(false)
  85     , m_shouldUseIndexMasking(Options::enableSpectreMitigations())
  86     , m_loopOSREntryBytecodeOffset(loopOSREntryBytecodeOffset)
  87 {
  88 }
  89 
  90 JIT::~JIT()
  91 {
  92 }
  93 
<a name="4" id="anc4"></a>



















  94 void JIT::emitNotifyWrite(WatchpointSet* set)
  95 {
  96     if (!set || set-&gt;state() == IsInvalidated) {
  97         addSlowCase(Jump());
  98         return;
  99     }
 100 
 101     addSlowCase(branch8(NotEqual, AbsoluteAddress(set-&gt;addressOfState()), TrustedImm32(IsInvalidated)));
 102 }
 103 
 104 void JIT::emitNotifyWrite(GPRReg pointerToSet)
 105 {
 106     addSlowCase(branch8(NotEqual, Address(pointerToSet, WatchpointSet::offsetOfState()), TrustedImm32(IsInvalidated)));
 107 }
 108 
 109 void JIT::assertStackPointerOffset()
 110 {
 111     if (ASSERT_DISABLED)
 112         return;
 113 
 114     addPtr(TrustedImm32(stackPointerOffsetFor(m_codeBlock) * sizeof(Register)), callFrameRegister, regT0);
 115     Jump ok = branchPtr(Equal, regT0, stackPointerRegister);
 116     breakpoint();
 117     ok.link(this);
 118 }
 119 
 120 #define NEXT_OPCODE(name) \
 121     m_bytecodeOffset += currentInstruction-&gt;size(); \
 122     break;
 123 
 124 #define DEFINE_SLOW_OP(name) \
 125     case op_##name: { \
 126         if (m_bytecodeOffset &gt;= startBytecodeOffset) { \
 127             JITSlowPathCall slowPathCall(this, currentInstruction, slow_path_##name); \
 128             slowPathCall.call(); \
 129         } \
 130         NEXT_OPCODE(op_##name); \
 131     }
 132 
 133 #define DEFINE_OP(name) \
 134     case name: { \
 135         if (m_bytecodeOffset &gt;= startBytecodeOffset) { \
 136             emit_##name(currentInstruction); \
 137         } \
 138         NEXT_OPCODE(name); \
 139     }
 140 
 141 #define DEFINE_SLOWCASE_OP(name) \
 142     case name: { \
 143         emitSlow_##name(currentInstruction, iter); \
 144         NEXT_OPCODE(name); \
 145     }
 146 
 147 #define DEFINE_SLOWCASE_SLOW_OP(name) \
 148     case op_##name: { \
 149         emitSlowCaseCall(currentInstruction, iter, slow_path_##name); \
 150         NEXT_OPCODE(op_##name); \
 151     }
 152 
 153 void JIT::emitSlowCaseCall(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter, SlowPathFunction stub)
 154 {
 155     linkAllSlowCases(iter);
 156 
 157     JITSlowPathCall slowPathCall(this, currentInstruction, stub);
 158     slowPathCall.call();
 159 }
 160 
 161 void JIT::privateCompileMainPass()
 162 {
 163     if (JITInternal::verbose)
 164         dataLog(&quot;Compiling &quot;, *m_codeBlock, &quot;\n&quot;);
 165 
 166     jitAssertTagsInPlace();
 167     jitAssertArgumentCountSane();
 168 
 169     auto&amp; instructions = m_codeBlock-&gt;instructions();
 170     unsigned instructionCount = m_codeBlock-&gt;instructions().size();
 171 
 172     m_callLinkInfoIndex = 0;
 173 
<a name="5" id="anc5"></a><span class="line-modified"> 174     VM&amp; vm = m_codeBlock-&gt;vm();</span>
 175     unsigned startBytecodeOffset = 0;
 176     if (m_loopOSREntryBytecodeOffset &amp;&amp; (m_codeBlock-&gt;inherits&lt;ProgramCodeBlock&gt;(vm) || m_codeBlock-&gt;inherits&lt;ModuleProgramCodeBlock&gt;(vm))) {
 177         // We can only do this optimization because we execute ProgramCodeBlock&#39;s exactly once.
 178         // This optimization would be invalid otherwise. When the LLInt determines it wants to
 179         // do OSR entry into the baseline JIT in a loop, it will pass in the bytecode offset it
 180         // was executing at when it kicked off our compilation. We only need to compile code for
 181         // anything reachable from that bytecode offset.
 182 
 183         // We only bother building the bytecode graph if it could save time and executable
 184         // memory. We pick an arbitrary offset where we deem this is profitable.
 185         if (m_loopOSREntryBytecodeOffset &gt;= 200) {
 186             // As a simplification, we don&#39;t find all bytecode ranges that are unreachable.
 187             // Instead, we just find the minimum bytecode offset that is reachable, and
 188             // compile code from that bytecode offset onwards.
 189 
 190             BytecodeGraph graph(m_codeBlock, m_codeBlock-&gt;instructions());
 191             BytecodeBasicBlock* block = graph.findBasicBlockForBytecodeOffset(m_loopOSREntryBytecodeOffset);
 192             RELEASE_ASSERT(block);
 193 
 194             GraphNodeWorklist&lt;BytecodeBasicBlock*&gt; worklist;
 195             startBytecodeOffset = UINT_MAX;
 196             worklist.push(block);
 197 
 198             while (BytecodeBasicBlock* block = worklist.pop()) {
 199                 startBytecodeOffset = std::min(startBytecodeOffset, block-&gt;leaderOffset());
 200                 worklist.pushAll(block-&gt;successors());
 201 
 202                 // Also add catch blocks for bytecodes that throw.
 203                 if (m_codeBlock-&gt;numberOfExceptionHandlers()) {
 204                     for (unsigned bytecodeOffset = block-&gt;leaderOffset(); bytecodeOffset &lt; block-&gt;leaderOffset() + block-&gt;totalLength();) {
 205                         auto instruction = instructions.at(bytecodeOffset);
 206                         if (auto* handler = m_codeBlock-&gt;handlerForBytecodeOffset(bytecodeOffset))
 207                             worklist.push(graph.findBasicBlockWithLeaderOffset(handler-&gt;target));
 208 
 209                         bytecodeOffset += instruction-&gt;size();
 210                     }
 211                 }
 212             }
 213         }
 214     }
 215 
 216     for (m_bytecodeOffset = 0; m_bytecodeOffset &lt; instructionCount; ) {
 217         if (m_bytecodeOffset == startBytecodeOffset &amp;&amp; startBytecodeOffset &gt; 0) {
 218             // We&#39;ve proven all bytecode instructions up until here are unreachable.
 219             // Let&#39;s ensure that by crashing if it&#39;s ever hit.
 220             breakpoint();
 221         }
 222 
 223         if (m_disassembler)
 224             m_disassembler-&gt;setForBytecodeMainPath(m_bytecodeOffset, label());
 225         const Instruction* currentInstruction = instructions.at(m_bytecodeOffset).ptr();
 226         ASSERT_WITH_MESSAGE(currentInstruction-&gt;size(), &quot;privateCompileMainPass gone bad @ %d&quot;, m_bytecodeOffset);
 227 
 228         m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeOffset));
 229 
 230 #if ENABLE(OPCODE_SAMPLING)
 231         if (m_bytecodeOffset &gt; 0) // Avoid the overhead of sampling op_enter twice.
 232             sampleInstruction(currentInstruction);
 233 #endif
 234 
 235         m_labels[m_bytecodeOffset] = label();
 236 
 237         if (JITInternal::verbose)
 238             dataLogF(&quot;Old JIT emitting code for bc#%u at offset 0x%lx.\n&quot;, m_bytecodeOffset, (long)debugOffset());
 239 
 240         OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
 241 
 242         if (UNLIKELY(m_compilation)) {
 243             add64(
 244                 TrustedImm32(1),
 245                 AbsoluteAddress(m_compilation-&gt;executionCounterFor(Profiler::OriginStack(Profiler::Origin(
 246                     m_compilation-&gt;bytecodes(), m_bytecodeOffset)))-&gt;address()));
 247         }
 248 
 249         if (Options::eagerlyUpdateTopCallFrame())
 250             updateTopCallFrame();
 251 
 252         unsigned bytecodeOffset = m_bytecodeOffset;
 253 #if ENABLE(MASM_PROBE)
 254         if (UNLIKELY(Options::traceBaselineJITExecution())) {
 255             CodeBlock* codeBlock = m_codeBlock;
 256             probe([=] (Probe::Context&amp; ctx) {
 257                 dataLogLn(&quot;JIT [&quot;, bytecodeOffset, &quot;] &quot;, opcodeNames[opcodeID], &quot; cfr &quot;, RawPointer(ctx.fp()), &quot; @ &quot;, codeBlock);
 258             });
 259         }
 260 #endif
 261 
 262         switch (opcodeID) {
 263         DEFINE_SLOW_OP(in_by_val)
 264         DEFINE_SLOW_OP(less)
 265         DEFINE_SLOW_OP(lesseq)
 266         DEFINE_SLOW_OP(greater)
 267         DEFINE_SLOW_OP(greatereq)
 268         DEFINE_SLOW_OP(is_function)
 269         DEFINE_SLOW_OP(is_object_or_null)
 270         DEFINE_SLOW_OP(typeof)
 271         DEFINE_SLOW_OP(strcat)
 272         DEFINE_SLOW_OP(push_with_scope)
 273         DEFINE_SLOW_OP(create_lexical_environment)
 274         DEFINE_SLOW_OP(get_by_val_with_this)
 275         DEFINE_SLOW_OP(put_by_id_with_this)
 276         DEFINE_SLOW_OP(put_by_val_with_this)
 277         DEFINE_SLOW_OP(resolve_scope_for_hoisting_func_decl_in_eval)
 278         DEFINE_SLOW_OP(define_data_property)
 279         DEFINE_SLOW_OP(define_accessor_property)
 280         DEFINE_SLOW_OP(unreachable)
 281         DEFINE_SLOW_OP(throw_static_error)
 282         DEFINE_SLOW_OP(new_array_with_spread)
 283         DEFINE_SLOW_OP(new_array_buffer)
 284         DEFINE_SLOW_OP(spread)
 285         DEFINE_SLOW_OP(get_enumerable_length)
 286         DEFINE_SLOW_OP(has_generic_property)
 287         DEFINE_SLOW_OP(get_property_enumerator)
 288         DEFINE_SLOW_OP(to_index_string)
 289         DEFINE_SLOW_OP(create_direct_arguments)
 290         DEFINE_SLOW_OP(create_scoped_arguments)
 291         DEFINE_SLOW_OP(create_cloned_arguments)
 292         DEFINE_SLOW_OP(create_rest)
 293         DEFINE_SLOW_OP(pow)
 294 
 295         DEFINE_OP(op_add)
 296         DEFINE_OP(op_bitnot)
 297         DEFINE_OP(op_bitand)
 298         DEFINE_OP(op_bitor)
 299         DEFINE_OP(op_bitxor)
 300         DEFINE_OP(op_call)
 301         DEFINE_OP(op_tail_call)
 302         DEFINE_OP(op_call_eval)
 303         DEFINE_OP(op_call_varargs)
 304         DEFINE_OP(op_tail_call_varargs)
 305         DEFINE_OP(op_tail_call_forward_arguments)
 306         DEFINE_OP(op_construct_varargs)
 307         DEFINE_OP(op_catch)
 308         DEFINE_OP(op_construct)
 309         DEFINE_OP(op_create_this)
 310         DEFINE_OP(op_to_this)
 311         DEFINE_OP(op_get_argument)
 312         DEFINE_OP(op_argument_count)
 313         DEFINE_OP(op_get_rest_length)
 314         DEFINE_OP(op_check_tdz)
 315         DEFINE_OP(op_identity_with_profile)
 316         DEFINE_OP(op_debug)
 317         DEFINE_OP(op_del_by_id)
 318         DEFINE_OP(op_del_by_val)
 319         DEFINE_OP(op_div)
 320         DEFINE_OP(op_end)
 321         DEFINE_OP(op_enter)
 322         DEFINE_OP(op_get_scope)
 323         DEFINE_OP(op_eq)
 324         DEFINE_OP(op_eq_null)
 325         DEFINE_OP(op_below)
 326         DEFINE_OP(op_beloweq)
 327         DEFINE_OP(op_try_get_by_id)
 328         DEFINE_OP(op_in_by_id)
 329         DEFINE_OP(op_get_by_id)
 330         DEFINE_OP(op_get_by_id_with_this)
 331         DEFINE_OP(op_get_by_id_direct)
 332         DEFINE_OP(op_get_by_val)
 333         DEFINE_OP(op_overrides_has_instance)
 334         DEFINE_OP(op_instanceof)
 335         DEFINE_OP(op_instanceof_custom)
 336         DEFINE_OP(op_is_empty)
 337         DEFINE_OP(op_is_undefined)
 338         DEFINE_OP(op_is_undefined_or_null)
 339         DEFINE_OP(op_is_boolean)
 340         DEFINE_OP(op_is_number)
 341         DEFINE_OP(op_is_object)
 342         DEFINE_OP(op_is_cell_with_type)
 343         DEFINE_OP(op_jeq_null)
 344         DEFINE_OP(op_jfalse)
 345         DEFINE_OP(op_jmp)
 346         DEFINE_OP(op_jneq_null)
<a name="6" id="anc6"></a><span class="line-added"> 347         DEFINE_OP(op_jundefined_or_null)</span>
<span class="line-added"> 348         DEFINE_OP(op_jnundefined_or_null)</span>
 349         DEFINE_OP(op_jneq_ptr)
 350         DEFINE_OP(op_jless)
 351         DEFINE_OP(op_jlesseq)
 352         DEFINE_OP(op_jgreater)
 353         DEFINE_OP(op_jgreatereq)
 354         DEFINE_OP(op_jnless)
 355         DEFINE_OP(op_jnlesseq)
 356         DEFINE_OP(op_jngreater)
 357         DEFINE_OP(op_jngreatereq)
 358         DEFINE_OP(op_jeq)
 359         DEFINE_OP(op_jneq)
 360         DEFINE_OP(op_jstricteq)
 361         DEFINE_OP(op_jnstricteq)
 362         DEFINE_OP(op_jbelow)
 363         DEFINE_OP(op_jbeloweq)
 364         DEFINE_OP(op_jtrue)
 365         DEFINE_OP(op_loop_hint)
<a name="7" id="anc7"></a>
 366         DEFINE_OP(op_nop)
 367         DEFINE_OP(op_super_sampler_begin)
 368         DEFINE_OP(op_super_sampler_end)
 369         DEFINE_OP(op_lshift)
 370         DEFINE_OP(op_mod)
 371         DEFINE_OP(op_mov)
 372         DEFINE_OP(op_mul)
 373         DEFINE_OP(op_negate)
 374         DEFINE_OP(op_neq)
 375         DEFINE_OP(op_neq_null)
 376         DEFINE_OP(op_new_array)
 377         DEFINE_OP(op_new_array_with_size)
 378         DEFINE_OP(op_new_func)
 379         DEFINE_OP(op_new_func_exp)
 380         DEFINE_OP(op_new_generator_func)
 381         DEFINE_OP(op_new_generator_func_exp)
 382         DEFINE_OP(op_new_async_func)
 383         DEFINE_OP(op_new_async_func_exp)
 384         DEFINE_OP(op_new_async_generator_func)
 385         DEFINE_OP(op_new_async_generator_func_exp)
 386         DEFINE_OP(op_new_object)
 387         DEFINE_OP(op_new_regexp)
 388         DEFINE_OP(op_not)
 389         DEFINE_OP(op_nstricteq)
 390         DEFINE_OP(op_dec)
 391         DEFINE_OP(op_inc)
 392         DEFINE_OP(op_profile_type)
 393         DEFINE_OP(op_profile_control_flow)
 394         DEFINE_OP(op_get_parent_scope)
 395         DEFINE_OP(op_put_by_id)
 396         DEFINE_OP(op_put_by_val_direct)
 397         DEFINE_OP(op_put_by_val)
 398         DEFINE_OP(op_put_getter_by_id)
 399         DEFINE_OP(op_put_setter_by_id)
 400         DEFINE_OP(op_put_getter_setter_by_id)
 401         DEFINE_OP(op_put_getter_by_val)
 402         DEFINE_OP(op_put_setter_by_val)
 403 
 404         DEFINE_OP(op_ret)
 405         DEFINE_OP(op_rshift)
 406         DEFINE_OP(op_unsigned)
 407         DEFINE_OP(op_urshift)
 408         DEFINE_OP(op_set_function_name)
 409         DEFINE_OP(op_stricteq)
 410         DEFINE_OP(op_sub)
 411         DEFINE_OP(op_switch_char)
 412         DEFINE_OP(op_switch_imm)
 413         DEFINE_OP(op_switch_string)
 414         DEFINE_OP(op_throw)
 415         DEFINE_OP(op_to_number)
 416         DEFINE_OP(op_to_string)
 417         DEFINE_OP(op_to_object)
 418         DEFINE_OP(op_to_primitive)
 419 
 420         DEFINE_OP(op_resolve_scope)
 421         DEFINE_OP(op_get_from_scope)
 422         DEFINE_OP(op_put_to_scope)
 423         DEFINE_OP(op_get_from_arguments)
 424         DEFINE_OP(op_put_to_arguments)
 425 
 426         DEFINE_OP(op_has_structure_property)
 427         DEFINE_OP(op_has_indexed_property)
 428         DEFINE_OP(op_get_direct_pname)
 429         DEFINE_OP(op_enumerator_structure_pname)
 430         DEFINE_OP(op_enumerator_generic_pname)
 431 
 432         DEFINE_OP(op_log_shadow_chicken_prologue)
 433         DEFINE_OP(op_log_shadow_chicken_tail)
 434         default:
 435             RELEASE_ASSERT_NOT_REACHED();
 436         }
 437 
 438         if (JITInternal::verbose)
 439             dataLog(&quot;At &quot;, bytecodeOffset, &quot;: &quot;, m_slowCases.size(), &quot;\n&quot;);
 440     }
 441 
 442     RELEASE_ASSERT(m_callLinkInfoIndex == m_callCompilationInfo.size());
 443 
 444 #ifndef NDEBUG
 445     // Reset this, in order to guard its use with ASSERTs.
 446     m_bytecodeOffset = std::numeric_limits&lt;unsigned&gt;::max();
 447 #endif
 448 }
 449 
 450 void JIT::privateCompileLinkPass()
 451 {
 452     unsigned jmpTableCount = m_jmpTable.size();
 453     for (unsigned i = 0; i &lt; jmpTableCount; ++i)
 454         m_jmpTable[i].from.linkTo(m_labels[m_jmpTable[i].toBytecodeOffset], this);
 455     m_jmpTable.clear();
 456 }
 457 
 458 void JIT::privateCompileSlowCases()
 459 {
 460     m_getByIdIndex = 0;
 461     m_getByIdWithThisIndex = 0;
 462     m_putByIdIndex = 0;
 463     m_inByIdIndex = 0;
 464     m_instanceOfIndex = 0;
 465     m_byValInstructionIndex = 0;
 466     m_callLinkInfoIndex = 0;
 467 
 468     for (Vector&lt;SlowCaseEntry&gt;::iterator iter = m_slowCases.begin(); iter != m_slowCases.end();) {
 469         m_bytecodeOffset = iter-&gt;to;
 470 
 471         m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(m_bytecodeOffset));
 472 
 473         unsigned firstTo = m_bytecodeOffset;
 474 
 475         const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(m_bytecodeOffset).ptr();
 476 
 477         RareCaseProfile* rareCaseProfile = 0;
 478         if (shouldEmitProfiling())
 479             rareCaseProfile = m_codeBlock-&gt;addRareCaseProfile(m_bytecodeOffset);
 480 
 481         if (JITInternal::verbose)
 482             dataLogF(&quot;Old JIT emitting slow code for bc#%u at offset 0x%lx.\n&quot;, m_bytecodeOffset, (long)debugOffset());
 483 
 484         if (m_disassembler)
 485             m_disassembler-&gt;setForBytecodeSlowPath(m_bytecodeOffset, label());
 486 
 487 #if ENABLE(MASM_PROBE)
 488         if (UNLIKELY(Options::traceBaselineJITExecution())) {
 489             OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
 490             unsigned bytecodeOffset = m_bytecodeOffset;
 491             CodeBlock* codeBlock = m_codeBlock;
 492             probe([=] (Probe::Context&amp; ctx) {
 493                 dataLogLn(&quot;JIT [&quot;, bytecodeOffset, &quot;] SLOW &quot;, opcodeNames[opcodeID], &quot; cfr &quot;, RawPointer(ctx.fp()), &quot; @ &quot;, codeBlock);
 494             });
 495         }
 496 #endif
 497 
 498         switch (currentInstruction-&gt;opcodeID()) {
 499         DEFINE_SLOWCASE_OP(op_add)
 500         DEFINE_SLOWCASE_OP(op_call)
 501         DEFINE_SLOWCASE_OP(op_tail_call)
 502         DEFINE_SLOWCASE_OP(op_call_eval)
 503         DEFINE_SLOWCASE_OP(op_call_varargs)
 504         DEFINE_SLOWCASE_OP(op_tail_call_varargs)
 505         DEFINE_SLOWCASE_OP(op_tail_call_forward_arguments)
 506         DEFINE_SLOWCASE_OP(op_construct_varargs)
 507         DEFINE_SLOWCASE_OP(op_construct)
 508         DEFINE_SLOWCASE_OP(op_eq)
 509         DEFINE_SLOWCASE_OP(op_try_get_by_id)
 510         DEFINE_SLOWCASE_OP(op_in_by_id)
 511         DEFINE_SLOWCASE_OP(op_get_by_id)
 512         DEFINE_SLOWCASE_OP(op_get_by_id_with_this)
 513         DEFINE_SLOWCASE_OP(op_get_by_id_direct)
 514         DEFINE_SLOWCASE_OP(op_get_by_val)
 515         DEFINE_SLOWCASE_OP(op_instanceof)
 516         DEFINE_SLOWCASE_OP(op_instanceof_custom)
 517         DEFINE_SLOWCASE_OP(op_jless)
 518         DEFINE_SLOWCASE_OP(op_jlesseq)
 519         DEFINE_SLOWCASE_OP(op_jgreater)
 520         DEFINE_SLOWCASE_OP(op_jgreatereq)
 521         DEFINE_SLOWCASE_OP(op_jnless)
 522         DEFINE_SLOWCASE_OP(op_jnlesseq)
 523         DEFINE_SLOWCASE_OP(op_jngreater)
 524         DEFINE_SLOWCASE_OP(op_jngreatereq)
 525         DEFINE_SLOWCASE_OP(op_jeq)
 526         DEFINE_SLOWCASE_OP(op_jneq)
 527         DEFINE_SLOWCASE_OP(op_jstricteq)
 528         DEFINE_SLOWCASE_OP(op_jnstricteq)
 529         DEFINE_SLOWCASE_OP(op_loop_hint)
<a name="8" id="anc8"></a><span class="line-modified"> 530         DEFINE_SLOWCASE_OP(op_enter)</span>
 531         DEFINE_SLOWCASE_OP(op_mod)
 532         DEFINE_SLOWCASE_OP(op_mul)
 533         DEFINE_SLOWCASE_OP(op_negate)
 534         DEFINE_SLOWCASE_OP(op_neq)
 535         DEFINE_SLOWCASE_OP(op_new_object)
 536         DEFINE_SLOWCASE_OP(op_put_by_id)
 537         case op_put_by_val_direct:
 538         DEFINE_SLOWCASE_OP(op_put_by_val)
 539         DEFINE_SLOWCASE_OP(op_sub)
 540         DEFINE_SLOWCASE_OP(op_has_indexed_property)
 541         DEFINE_SLOWCASE_OP(op_get_from_scope)
 542         DEFINE_SLOWCASE_OP(op_put_to_scope)
 543 
 544         DEFINE_SLOWCASE_SLOW_OP(unsigned)
 545         DEFINE_SLOWCASE_SLOW_OP(inc)
 546         DEFINE_SLOWCASE_SLOW_OP(dec)
 547         DEFINE_SLOWCASE_SLOW_OP(bitnot)
 548         DEFINE_SLOWCASE_SLOW_OP(bitand)
 549         DEFINE_SLOWCASE_SLOW_OP(bitor)
 550         DEFINE_SLOWCASE_SLOW_OP(bitxor)
 551         DEFINE_SLOWCASE_SLOW_OP(lshift)
 552         DEFINE_SLOWCASE_SLOW_OP(rshift)
 553         DEFINE_SLOWCASE_SLOW_OP(urshift)
 554         DEFINE_SLOWCASE_SLOW_OP(div)
 555         DEFINE_SLOWCASE_SLOW_OP(create_this)
 556         DEFINE_SLOWCASE_SLOW_OP(to_this)
 557         DEFINE_SLOWCASE_SLOW_OP(to_primitive)
 558         DEFINE_SLOWCASE_SLOW_OP(to_number)
 559         DEFINE_SLOWCASE_SLOW_OP(to_string)
 560         DEFINE_SLOWCASE_SLOW_OP(to_object)
 561         DEFINE_SLOWCASE_SLOW_OP(not)
 562         DEFINE_SLOWCASE_SLOW_OP(stricteq)
 563         DEFINE_SLOWCASE_SLOW_OP(nstricteq)
 564         DEFINE_SLOWCASE_SLOW_OP(get_direct_pname)
 565         DEFINE_SLOWCASE_SLOW_OP(has_structure_property)
 566         DEFINE_SLOWCASE_SLOW_OP(resolve_scope)
 567         DEFINE_SLOWCASE_SLOW_OP(check_tdz)
 568 
 569         default:
 570             RELEASE_ASSERT_NOT_REACHED();
 571         }
 572 
 573         if (JITInternal::verbose)
 574             dataLog(&quot;At &quot;, firstTo, &quot; slow: &quot;, iter - m_slowCases.begin(), &quot;\n&quot;);
 575 
 576         RELEASE_ASSERT_WITH_MESSAGE(iter == m_slowCases.end() || firstTo != iter-&gt;to, &quot;Not enough jumps linked in slow case codegen.&quot;);
 577         RELEASE_ASSERT_WITH_MESSAGE(firstTo == (iter - 1)-&gt;to, &quot;Too many jumps linked in slow case codegen.&quot;);
 578 
 579         if (shouldEmitProfiling())
 580             add32(TrustedImm32(1), AbsoluteAddress(&amp;rareCaseProfile-&gt;m_counter));
 581 
 582         emitJumpSlowToHot(jump(), 0);
 583     }
 584 
 585     RELEASE_ASSERT(m_getByIdIndex == m_getByIds.size());
 586     RELEASE_ASSERT(m_getByIdWithThisIndex == m_getByIdsWithThis.size());
 587     RELEASE_ASSERT(m_putByIdIndex == m_putByIds.size());
 588     RELEASE_ASSERT(m_inByIdIndex == m_inByIds.size());
 589     RELEASE_ASSERT(m_instanceOfIndex == m_instanceOfs.size());
 590     RELEASE_ASSERT(m_callLinkInfoIndex == m_callCompilationInfo.size());
 591 
 592 #ifndef NDEBUG
 593     // Reset this, in order to guard its use with ASSERTs.
 594     m_bytecodeOffset = std::numeric_limits&lt;unsigned&gt;::max();
 595 #endif
 596 }
 597 
 598 void JIT::compileWithoutLinking(JITCompilationEffort effort)
 599 {
 600     MonotonicTime before { };
 601     if (UNLIKELY(computeCompileTimes()))
 602         before = MonotonicTime::now();
 603 
 604     DFG::CapabilityLevel level = m_codeBlock-&gt;capabilityLevel();
 605     switch (level) {
 606     case DFG::CannotCompile:
 607         m_canBeOptimized = false;
 608         m_canBeOptimizedOrInlined = false;
 609         m_shouldEmitProfiling = false;
 610         break;
 611     case DFG::CanCompile:
 612     case DFG::CanCompileAndInline:
 613         m_canBeOptimized = true;
 614         m_canBeOptimizedOrInlined = true;
 615         m_shouldEmitProfiling = true;
 616         break;
 617     default:
 618         RELEASE_ASSERT_NOT_REACHED();
 619         break;
 620     }
 621 
 622     switch (m_codeBlock-&gt;codeType()) {
 623     case GlobalCode:
 624     case ModuleCode:
 625     case EvalCode:
 626         m_codeBlock-&gt;m_shouldAlwaysBeInlined = false;
 627         break;
 628     case FunctionCode:
 629         // We could have already set it to false because we detected an uninlineable call.
 630         // Don&#39;t override that observation.
 631         m_codeBlock-&gt;m_shouldAlwaysBeInlined &amp;= canInline(level) &amp;&amp; DFG::mightInlineFunction(m_codeBlock);
 632         break;
 633     }
 634 
 635     if (UNLIKELY(Options::dumpDisassembly() || (m_vm-&gt;m_perBytecodeProfiler &amp;&amp; Options::disassembleBaselineForProfiler())))
<a name="9" id="anc9"></a><span class="line-modified"> 636         m_disassembler = makeUnique&lt;JITDisassembler&gt;(m_codeBlock);</span>
 637     if (UNLIKELY(m_vm-&gt;m_perBytecodeProfiler)) {
 638         m_compilation = adoptRef(
 639             new Profiler::Compilation(
 640                 m_vm-&gt;m_perBytecodeProfiler-&gt;ensureBytecodesFor(m_codeBlock),
 641                 Profiler::Baseline));
 642         m_compilation-&gt;addProfiledBytecodes(*m_vm-&gt;m_perBytecodeProfiler, m_codeBlock);
 643     }
 644 
 645     m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(0, nullptr));
 646 
 647     Label entryLabel(this);
 648     if (m_disassembler)
 649         m_disassembler-&gt;setStartOfCode(entryLabel);
 650 
 651     // Just add a little bit of randomness to the codegen
 652     if (random() &amp; 1)
 653         nop();
 654 
 655     emitFunctionPrologue();
 656     emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
 657 
 658     Label beginLabel(this);
 659 
 660     sampleCodeBlock(m_codeBlock);
 661 #if ENABLE(OPCODE_SAMPLING)
 662     sampleInstruction(m_codeBlock-&gt;instructions().begin());
 663 #endif
 664 
 665     int frameTopOffset = stackPointerOffsetFor(m_codeBlock) * sizeof(Register);
 666     unsigned maxFrameSize = -frameTopOffset;
 667     addPtr(TrustedImm32(frameTopOffset), callFrameRegister, regT1);
 668     JumpList stackOverflow;
 669     if (UNLIKELY(maxFrameSize &gt; Options::reservedZoneSize()))
 670         stackOverflow.append(branchPtr(Above, regT1, callFrameRegister));
 671     stackOverflow.append(branchPtr(Above, AbsoluteAddress(m_vm-&gt;addressOfSoftStackLimit()), regT1));
 672 
 673     move(regT1, stackPointerRegister);
 674     checkStackPointerAlignment();
 675     if (Options::zeroStackFrame())
 676         clearStackFrame(callFrameRegister, stackPointerRegister, regT0, maxFrameSize);
 677 
 678     emitSaveCalleeSaves();
 679     emitMaterializeTagCheckRegisters();
 680 
 681     if (m_codeBlock-&gt;codeType() == FunctionCode) {
 682         ASSERT(m_bytecodeOffset == std::numeric_limits&lt;unsigned&gt;::max());
 683         if (shouldEmitProfiling()) {
 684             for (int argument = 0; argument &lt; m_codeBlock-&gt;numParameters(); ++argument) {
 685                 // If this is a constructor, then we want to put in a dummy profiling site (to
 686                 // keep things consistent) but we don&#39;t actually want to record the dummy value.
 687                 if (m_codeBlock-&gt;isConstructor() &amp;&amp; !argument)
 688                     continue;
 689                 int offset = CallFrame::argumentOffsetIncludingThis(argument) * static_cast&lt;int&gt;(sizeof(Register));
 690 #if USE(JSVALUE64)
 691                 load64(Address(callFrameRegister, offset), regT0);
 692 #elif USE(JSVALUE32_64)
 693                 load32(Address(callFrameRegister, offset + OBJECT_OFFSETOF(JSValue, u.asBits.payload)), regT0);
 694                 load32(Address(callFrameRegister, offset + OBJECT_OFFSETOF(JSValue, u.asBits.tag)), regT1);
 695 #endif
 696                 emitValueProfilingSite(m_codeBlock-&gt;valueProfileForArgument(argument));
 697             }
 698         }
 699     }
 700 
 701     RELEASE_ASSERT(!JITCode::isJIT(m_codeBlock-&gt;jitType()));
 702 
 703     privateCompileMainPass();
 704     privateCompileLinkPass();
 705     privateCompileSlowCases();
 706 
 707     if (m_disassembler)
 708         m_disassembler-&gt;setEndOfSlowPath(label());
 709     m_pcToCodeOriginMapBuilder.appendItem(label(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
 710 
 711     stackOverflow.link(this);
 712     m_bytecodeOffset = 0;
 713     if (maxFrameExtentForSlowPathCall)
 714         addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
 715     callOperationWithCallFrameRollbackOnException(operationThrowStackOverflowError, m_codeBlock);
 716 
 717     // If the number of parameters is 1, we never require arity fixup.
 718     bool requiresArityFixup = m_codeBlock-&gt;m_numParameters != 1;
 719     if (m_codeBlock-&gt;codeType() == FunctionCode &amp;&amp; requiresArityFixup) {
 720         m_arityCheck = label();
 721         store8(TrustedImm32(0), &amp;m_codeBlock-&gt;m_shouldAlwaysBeInlined);
 722         emitFunctionPrologue();
 723         emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
 724 
 725         load32(payloadFor(CallFrameSlot::argumentCount), regT1);
 726         branch32(AboveOrEqual, regT1, TrustedImm32(m_codeBlock-&gt;m_numParameters)).linkTo(beginLabel, this);
 727 
 728         m_bytecodeOffset = 0;
 729 
 730         if (maxFrameExtentForSlowPathCall)
 731             addPtr(TrustedImm32(-static_cast&lt;int32_t&gt;(maxFrameExtentForSlowPathCall)), stackPointerRegister);
 732         callOperationWithCallFrameRollbackOnException(m_codeBlock-&gt;isConstructor() ? operationConstructArityCheck : operationCallArityCheck);
 733         if (maxFrameExtentForSlowPathCall)
 734             addPtr(TrustedImm32(maxFrameExtentForSlowPathCall), stackPointerRegister);
 735         branchTest32(Zero, returnValueGPR).linkTo(beginLabel, this);
 736         move(returnValueGPR, GPRInfo::argumentGPR0);
 737         emitNakedCall(m_vm-&gt;getCTIStub(arityFixupGenerator).retaggedCode&lt;NoPtrTag&gt;());
 738 
 739 #if !ASSERT_DISABLED
 740         m_bytecodeOffset = std::numeric_limits&lt;unsigned&gt;::max(); // Reset this, in order to guard its use with ASSERTs.
 741 #endif
 742 
 743         jump(beginLabel);
 744     } else
 745         m_arityCheck = entryLabel; // Never require arity fixup.
 746 
 747     ASSERT(m_jmpTable.isEmpty());
 748 
 749     privateCompileExceptionHandlers();
 750 
 751     if (m_disassembler)
 752         m_disassembler-&gt;setEndOfCode(label());
 753     m_pcToCodeOriginMapBuilder.appendItem(label(), PCToCodeOriginMapBuilder::defaultCodeOrigin());
 754 
 755     m_linkBuffer = std::unique_ptr&lt;LinkBuffer&gt;(new LinkBuffer(*this, m_codeBlock, effort));
 756 
 757     MonotonicTime after { };
 758     if (UNLIKELY(computeCompileTimes())) {
 759         after = MonotonicTime::now();
 760 
 761         if (Options::reportTotalCompileTimes())
 762             totalBaselineCompileTime += after - before;
 763     }
 764     if (UNLIKELY(reportCompileTimes())) {
 765         CString codeBlockName = toCString(*m_codeBlock);
 766 
 767         dataLog(&quot;Optimized &quot;, codeBlockName, &quot; with Baseline JIT into &quot;, m_linkBuffer-&gt;size(), &quot; bytes in &quot;, (after - before).milliseconds(), &quot; ms.\n&quot;);
 768     }
 769 }
 770 
 771 CompilationResult JIT::link()
 772 {
 773     LinkBuffer&amp; patchBuffer = *m_linkBuffer;
 774 
 775     if (patchBuffer.didFailToAllocate())
 776         return CompilationFailed;
 777 
 778     // Translate vPC offsets into addresses in JIT generated code, for switch tables.
 779     for (auto&amp; record : m_switches) {
 780         unsigned bytecodeOffset = record.bytecodeOffset;
 781 
 782         if (record.type != SwitchRecord::String) {
 783             ASSERT(record.type == SwitchRecord::Immediate || record.type == SwitchRecord::Character);
 784             ASSERT(record.jumpTable.simpleJumpTable-&gt;branchOffsets.size() == record.jumpTable.simpleJumpTable-&gt;ctiOffsets.size());
 785 
 786             auto* simpleJumpTable = record.jumpTable.simpleJumpTable;
 787             simpleJumpTable-&gt;ctiDefault = patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + record.defaultOffset]);
 788 
 789             for (unsigned j = 0; j &lt; record.jumpTable.simpleJumpTable-&gt;branchOffsets.size(); ++j) {
 790                 unsigned offset = record.jumpTable.simpleJumpTable-&gt;branchOffsets[j];
 791                 simpleJumpTable-&gt;ctiOffsets[j] = offset
 792                     ? patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + offset])
 793                     : simpleJumpTable-&gt;ctiDefault;
 794             }
 795         } else {
 796             ASSERT(record.type == SwitchRecord::String);
 797 
 798             auto* stringJumpTable = record.jumpTable.stringJumpTable;
 799             stringJumpTable-&gt;ctiDefault =
 800                 patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + record.defaultOffset]);
 801 
 802             for (auto&amp; location : stringJumpTable-&gt;offsetTable.values()) {
 803                 unsigned offset = location.branchOffset;
 804                 location.ctiOffset = offset
 805                     ? patchBuffer.locationOf&lt;JSSwitchPtrTag&gt;(m_labels[bytecodeOffset + offset])
 806                     : stringJumpTable-&gt;ctiDefault;
 807             }
 808         }
 809     }
 810 
 811     for (size_t i = 0; i &lt; m_codeBlock-&gt;numberOfExceptionHandlers(); ++i) {
 812         HandlerInfo&amp; handler = m_codeBlock-&gt;exceptionHandler(i);
 813         // FIXME: &lt;rdar://problem/39433318&gt;.
 814         handler.nativeCode = patchBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(m_labels[handler.target]);
 815     }
 816 
 817     for (auto&amp; record : m_calls) {
 818         if (record.callee)
 819             patchBuffer.link(record.from, record.callee);
 820     }
 821 
 822     finalizeInlineCaches(m_getByIds, patchBuffer);
 823     finalizeInlineCaches(m_getByIdsWithThis, patchBuffer);
 824     finalizeInlineCaches(m_putByIds, patchBuffer);
 825     finalizeInlineCaches(m_inByIds, patchBuffer);
 826     finalizeInlineCaches(m_instanceOfs, patchBuffer);
 827 
 828     if (m_byValCompilationInfo.size()) {
 829         CodeLocationLabel&lt;ExceptionHandlerPtrTag&gt; exceptionHandler = patchBuffer.locationOf&lt;ExceptionHandlerPtrTag&gt;(m_exceptionHandler);
 830 
 831         for (const auto&amp; byValCompilationInfo : m_byValCompilationInfo) {
 832             PatchableJump patchableNotIndexJump = byValCompilationInfo.notIndexJump;
 833             auto notIndexJump = CodeLocationJump&lt;JSInternalPtrTag&gt;();
 834             if (Jump(patchableNotIndexJump).isSet())
 835                 notIndexJump = CodeLocationJump&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(patchableNotIndexJump));
 836             auto badTypeJump = CodeLocationJump&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.badTypeJump));
 837             auto doneTarget = CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.doneTarget));
 838             auto nextHotPathTarget = CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.nextHotPathTarget));
 839             auto slowPathTarget = CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(byValCompilationInfo.slowPathTarget));
 840 
 841             *byValCompilationInfo.byValInfo = ByValInfo(
 842                 byValCompilationInfo.bytecodeIndex,
 843                 notIndexJump,
 844                 badTypeJump,
 845                 exceptionHandler,
 846                 byValCompilationInfo.arrayMode,
 847                 byValCompilationInfo.arrayProfile,
 848                 doneTarget,
 849                 nextHotPathTarget,
 850                 slowPathTarget);
 851         }
 852     }
 853 
 854     for (auto&amp; compilationInfo : m_callCompilationInfo) {
 855         CallLinkInfo&amp; info = *compilationInfo.callLinkInfo;
 856         info.setCallLocations(
 857             CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(compilationInfo.callReturnLocation)),
 858             CodeLocationLabel&lt;JSInternalPtrTag&gt;(patchBuffer.locationOf&lt;JSInternalPtrTag&gt;(compilationInfo.hotPathBegin)),
 859             patchBuffer.locationOfNearCall&lt;JSInternalPtrTag&gt;(compilationInfo.hotPathOther));
 860     }
 861 
 862     JITCodeMap jitCodeMap;
 863     for (unsigned bytecodeOffset = 0; bytecodeOffset &lt; m_labels.size(); ++bytecodeOffset) {
 864         if (m_labels[bytecodeOffset].isSet())
 865             jitCodeMap.append(bytecodeOffset, patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_labels[bytecodeOffset]));
 866     }
 867     jitCodeMap.finish();
 868     m_codeBlock-&gt;setJITCodeMap(WTFMove(jitCodeMap));
 869 
 870     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; withArityCheck = patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_arityCheck);
 871 
 872     if (Options::dumpDisassembly()) {
 873         m_disassembler-&gt;dump(patchBuffer);
 874         patchBuffer.didAlreadyDisassemble();
 875     }
 876     if (UNLIKELY(m_compilation)) {
 877         if (Options::disassembleBaselineForProfiler())
 878             m_disassembler-&gt;reportToProfiler(m_compilation.get(), patchBuffer);
 879         m_vm-&gt;m_perBytecodeProfiler-&gt;addCompilation(m_codeBlock, *m_compilation);
 880     }
 881 
 882     if (m_pcToCodeOriginMapBuilder.didBuildMapping())
<a name="10" id="anc10"></a><span class="line-modified"> 883         m_codeBlock-&gt;setPCToCodeOriginMap(makeUnique&lt;PCToCodeOriginMap&gt;(WTFMove(m_pcToCodeOriginMapBuilder), patchBuffer));</span>
 884 
 885     CodeRef&lt;JSEntryPtrTag&gt; result = FINALIZE_CODE(
 886         patchBuffer, JSEntryPtrTag,
<a name="11" id="anc11"></a><span class="line-modified"> 887         &quot;Baseline JIT code for %s&quot;, toCString(CodeBlockWithJITType(m_codeBlock, JITType::BaselineJIT)).data());</span>
 888 
 889     m_vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;add(
 890         static_cast&lt;double&gt;(result.size()) /
<a name="12" id="anc12"></a><span class="line-modified"> 891         static_cast&lt;double&gt;(m_codeBlock-&gt;instructionsSize()));</span>
 892 
 893     m_codeBlock-&gt;shrinkToFit(CodeBlock::LateShrink);
 894     m_codeBlock-&gt;setJITCode(
<a name="13" id="anc13"></a><span class="line-modified"> 895         adoptRef(*new DirectJITCode(result, withArityCheck, JITType::BaselineJIT)));</span>
 896 
 897     if (JITInternal::verbose)
 898         dataLogF(&quot;JIT generated code for %p at [%p, %p).\n&quot;, m_codeBlock, result.executableMemory()-&gt;start().untaggedPtr(), result.executableMemory()-&gt;end().untaggedPtr());
 899 
 900     return CompilationSuccessful;
 901 }
 902 
 903 CompilationResult JIT::privateCompile(JITCompilationEffort effort)
 904 {
 905     doMainThreadPreparationBeforeCompile();
 906     compileWithoutLinking(effort);
 907     return link();
 908 }
 909 
 910 void JIT::privateCompileExceptionHandlers()
 911 {
 912     if (!m_exceptionChecksWithCallFrameRollback.empty()) {
 913         m_exceptionChecksWithCallFrameRollback.link(this);
 914 
<a name="14" id="anc14"></a><span class="line-modified"> 915         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
 916 
 917         // lookupExceptionHandlerFromCallerFrame is passed two arguments, the VM and the exec (the CallFrame*).
 918 
<a name="15" id="anc15"></a><span class="line-modified"> 919         move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);</span>
 920         move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
 921 
 922 #if CPU(X86)
 923         // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!
 924         poke(GPRInfo::argumentGPR0);
 925         poke(GPRInfo::argumentGPR1, 1);
 926 #endif
 927         m_calls.append(CallRecord(call(OperationPtrTag), std::numeric_limits&lt;unsigned&gt;::max(), FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandlerFromCallerFrame)));
<a name="16" id="anc16"></a><span class="line-modified"> 928         jumpToExceptionHandler(vm());</span>
 929     }
 930 
 931     if (!m_exceptionChecks.empty() || m_byValCompilationInfo.size()) {
 932         m_exceptionHandler = label();
 933         m_exceptionChecks.link(this);
 934 
<a name="17" id="anc17"></a><span class="line-modified"> 935         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
 936 
 937         // lookupExceptionHandler is passed two arguments, the VM and the exec (the CallFrame*).
<a name="18" id="anc18"></a><span class="line-modified"> 938         move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);</span>
 939         move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
 940 
 941 #if CPU(X86)
 942         // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!
 943         poke(GPRInfo::argumentGPR0);
 944         poke(GPRInfo::argumentGPR1, 1);
 945 #endif
 946         m_calls.append(CallRecord(call(OperationPtrTag), std::numeric_limits&lt;unsigned&gt;::max(), FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandler)));
<a name="19" id="anc19"></a><span class="line-modified"> 947         jumpToExceptionHandler(vm());</span>
 948     }
 949 }
 950 
 951 void JIT::doMainThreadPreparationBeforeCompile()
 952 {
 953     // This ensures that we have the most up to date type information when performing typecheck optimizations for op_profile_type.
 954     if (m_vm-&gt;typeProfiler())
 955         m_vm-&gt;typeProfilerLog()-&gt;processLogEntries(*m_vm, &quot;Preparing for JIT compilation.&quot;_s);
 956 }
 957 
 958 unsigned JIT::frameRegisterCountFor(CodeBlock* codeBlock)
 959 {
 960     ASSERT(static_cast&lt;unsigned&gt;(codeBlock-&gt;numCalleeLocals()) == WTF::roundUpToMultipleOf(stackAlignmentRegisters(), static_cast&lt;unsigned&gt;(codeBlock-&gt;numCalleeLocals())));
 961 
 962     return roundLocalRegisterCountForFramePointerOffset(codeBlock-&gt;numCalleeLocals() + maxFrameExtentForSlowPathCallInRegisters);
 963 }
 964 
 965 int JIT::stackPointerOffsetFor(CodeBlock* codeBlock)
 966 {
 967     return virtualRegisterForLocal(frameRegisterCountFor(codeBlock) - 1).offset();
 968 }
 969 
 970 bool JIT::reportCompileTimes()
 971 {
 972     return Options::reportCompileTimes() || Options::reportBaselineCompileTimes();
 973 }
 974 
 975 bool JIT::computeCompileTimes()
 976 {
 977     return reportCompileTimes() || Options::reportTotalCompileTimes();
 978 }
 979 
 980 HashMap&lt;CString, Seconds&gt; JIT::compileTimeStats()
 981 {
 982     HashMap&lt;CString, Seconds&gt; result;
 983     if (Options::reportTotalCompileTimes()) {
 984         result.add(&quot;Total Compile Time&quot;, totalBaselineCompileTime + totalDFGCompileTime + totalFTLCompileTime);
 985         result.add(&quot;Baseline Compile Time&quot;, totalBaselineCompileTime);
 986 #if ENABLE(DFG_JIT)
 987         result.add(&quot;DFG Compile Time&quot;, totalDFGCompileTime);
 988 #if ENABLE(FTL_JIT)
 989         result.add(&quot;FTL Compile Time&quot;, totalFTLCompileTime);
 990         result.add(&quot;FTL (DFG) Compile Time&quot;, totalFTLDFGCompileTime);
 991         result.add(&quot;FTL (B3) Compile Time&quot;, totalFTLB3CompileTime);
 992 #endif // ENABLE(FTL_JIT)
 993 #endif // ENABLE(DFG_JIT)
 994     }
 995     return result;
 996 }
 997 
 998 Seconds JIT::totalCompileTime()
 999 {
1000     return totalBaselineCompileTime + totalDFGCompileTime + totalFTLCompileTime;
1001 }
1002 
1003 } // namespace JSC
1004 
1005 #endif // ENABLE(JIT)
<a name="20" id="anc20"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="20" type="hidden" />
</body>
</html>