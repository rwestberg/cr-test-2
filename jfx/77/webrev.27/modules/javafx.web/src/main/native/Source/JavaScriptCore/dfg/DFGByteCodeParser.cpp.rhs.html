<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGByteCodeParser.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (C) 2011-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;DFGByteCodeParser.h&quot;
  28 
  29 #if ENABLE(DFG_JIT)
  30 
  31 #include &quot;ArithProfile.h&quot;
  32 #include &quot;ArrayConstructor.h&quot;
  33 #include &quot;BasicBlockLocation.h&quot;
  34 #include &quot;BuiltinNames.h&quot;
  35 #include &quot;BytecodeStructs.h&quot;
  36 #include &quot;CallLinkStatus.h&quot;
  37 #include &quot;CodeBlock.h&quot;
  38 #include &quot;CodeBlockWithJITType.h&quot;
  39 #include &quot;CommonSlowPaths.h&quot;
  40 #include &quot;DFGAbstractHeap.h&quot;
  41 #include &quot;DFGArrayMode.h&quot;
  42 #include &quot;DFGCFG.h&quot;
  43 #include &quot;DFGCapabilities.h&quot;
  44 #include &quot;DFGClobberize.h&quot;
  45 #include &quot;DFGClobbersExitState.h&quot;
  46 #include &quot;DFGGraph.h&quot;
  47 #include &quot;DFGJITCode.h&quot;
  48 #include &quot;FunctionCodeBlock.h&quot;
  49 #include &quot;GetByIdStatus.h&quot;
  50 #include &quot;Heap.h&quot;
  51 #include &quot;InByIdStatus.h&quot;
  52 #include &quot;InstanceOfStatus.h&quot;
  53 #include &quot;JSCInlines.h&quot;
  54 #include &quot;JSFixedArray.h&quot;
  55 #include &quot;JSImmutableButterfly.h&quot;
  56 #include &quot;JSModuleEnvironment.h&quot;
  57 #include &quot;JSModuleNamespaceObject.h&quot;
  58 #include &quot;NumberConstructor.h&quot;
  59 #include &quot;ObjectConstructor.h&quot;
  60 #include &quot;OpcodeInlines.h&quot;
  61 #include &quot;PreciseJumpTargets.h&quot;
  62 #include &quot;PutByIdFlags.h&quot;
  63 #include &quot;PutByIdStatus.h&quot;
  64 #include &quot;RegExpPrototype.h&quot;
  65 #include &quot;StackAlignment.h&quot;
  66 #include &quot;StringConstructor.h&quot;
  67 #include &quot;StructureStubInfo.h&quot;
  68 #include &quot;SymbolConstructor.h&quot;
  69 #include &quot;Watchdog.h&quot;
  70 #include &lt;wtf/CommaPrinter.h&gt;
  71 #include &lt;wtf/HashMap.h&gt;
  72 #include &lt;wtf/MathExtras.h&gt;
  73 #include &lt;wtf/SetForScope.h&gt;
  74 #include &lt;wtf/StdLibExtras.h&gt;
  75 
  76 namespace JSC { namespace DFG {
  77 
  78 namespace DFGByteCodeParserInternal {
  79 #ifdef NDEBUG
  80 static const bool verbose = false;
  81 #else
  82 static const bool verbose = true;
  83 #endif
  84 } // namespace DFGByteCodeParserInternal
  85 
  86 #define VERBOSE_LOG(...) do { \
  87 if (DFGByteCodeParserInternal::verbose &amp;&amp; Options::verboseDFGBytecodeParsing()) \
  88 dataLog(__VA_ARGS__); \
  89 } while (false)
  90 
  91 // === ByteCodeParser ===
  92 //
  93 // This class is used to compile the dataflow graph from a CodeBlock.
  94 class ByteCodeParser {
  95 public:
  96     ByteCodeParser(Graph&amp; graph)
  97         : m_vm(&amp;graph.m_vm)
  98         , m_codeBlock(graph.m_codeBlock)
  99         , m_profiledBlock(graph.m_profiledBlock)
 100         , m_graph(graph)
 101         , m_currentBlock(0)
 102         , m_currentIndex(0)
 103         , m_constantUndefined(graph.freeze(jsUndefined()))
 104         , m_constantNull(graph.freeze(jsNull()))
 105         , m_constantNaN(graph.freeze(jsNumber(PNaN)))
 106         , m_constantOne(graph.freeze(jsNumber(1)))
 107         , m_numArguments(m_codeBlock-&gt;numParameters())
 108         , m_numLocals(m_codeBlock-&gt;numCalleeLocals())
 109         , m_parameterSlots(0)
 110         , m_numPassedVarArgs(0)
 111         , m_inlineStackTop(0)
 112         , m_currentInstruction(0)
 113         , m_hasDebuggerEnabled(graph.hasDebuggerEnabled())
 114     {
 115         ASSERT(m_profiledBlock);
 116     }
 117 
 118     // Parse a full CodeBlock of bytecode.
 119     void parse();
 120 
 121 private:
 122     struct InlineStackEntry;
 123 
 124     // Just parse from m_currentIndex to the end of the current CodeBlock.
 125     void parseCodeBlock();
 126 
 127     void ensureLocals(unsigned newNumLocals)
 128     {
 129         VERBOSE_LOG(&quot;   ensureLocals: trying to raise m_numLocals from &quot;, m_numLocals, &quot; to &quot;, newNumLocals, &quot;\n&quot;);
 130         if (newNumLocals &lt;= m_numLocals)
 131             return;
 132         m_numLocals = newNumLocals;
 133         for (size_t i = 0; i &lt; m_graph.numBlocks(); ++i)
 134             m_graph.block(i)-&gt;ensureLocals(newNumLocals);
 135     }
 136 
 137     // Helper for min and max.
 138     template&lt;typename ChecksFunctor&gt;
 139     bool handleMinMax(VirtualRegister result, NodeType op, int registerOffset, int argumentCountIncludingThis, const ChecksFunctor&amp; insertChecks);
 140 
 141     void refineStatically(CallLinkStatus&amp;, Node* callTarget);
 142     // Blocks can either be targetable (i.e. in the m_blockLinkingTargets of one InlineStackEntry) with a well-defined bytecodeBegin,
 143     // or they can be untargetable, with bytecodeBegin==UINT_MAX, to be managed manually and not by the linkBlock machinery.
 144     // This is used most notably when doing polyvariant inlining (it requires a fair bit of control-flow with no bytecode analog).
 145     // It is also used when doing an early return from an inlined callee: it is easier to fix the bytecode index later on if needed
 146     // than to move the right index all the way to the treatment of op_ret.
 147     BasicBlock* allocateTargetableBlock(unsigned bytecodeIndex);
 148     BasicBlock* allocateUntargetableBlock();
 149     // An untargetable block can be given a bytecodeIndex to be later managed by linkBlock, but only once, and it can never go in the other direction
 150     void makeBlockTargetable(BasicBlock*, unsigned bytecodeIndex);
 151     void addJumpTo(BasicBlock*);
 152     void addJumpTo(unsigned bytecodeIndex);
 153     // Handle calls. This resolves issues surrounding inlining and intrinsics.
 154     enum Terminality { Terminal, NonTerminal };
 155     Terminality handleCall(
 156         VirtualRegister result, NodeType op, InlineCallFrame::Kind, unsigned instructionSize,
 157         Node* callTarget, int argumentCountIncludingThis, int registerOffset, CallLinkStatus,
 158         SpeculatedType prediction);
 159     template&lt;typename CallOp&gt;
 160     Terminality handleCall(const Instruction* pc, NodeType op, CallMode);
 161     template&lt;typename CallOp&gt;
 162     Terminality handleVarargsCall(const Instruction* pc, NodeType op, CallMode);
 163     void emitFunctionChecks(CallVariant, Node* callTarget, VirtualRegister thisArgumnt);
 164     void emitArgumentPhantoms(int registerOffset, int argumentCountIncludingThis);
 165     Node* getArgumentCount();
 166     template&lt;typename ChecksFunctor&gt;
 167     bool handleRecursiveTailCall(Node* callTargetNode, CallVariant, int registerOffset, int argumentCountIncludingThis, const ChecksFunctor&amp; emitFunctionCheckIfNeeded);
 168     unsigned inliningCost(CallVariant, int argumentCountIncludingThis, InlineCallFrame::Kind); // Return UINT_MAX if it&#39;s not an inlining candidate. By convention, intrinsics have a cost of 1.
 169     // Handle inlining. Return true if it succeeded, false if we need to plant a call.
 170     bool handleVarargsInlining(Node* callTargetNode, VirtualRegister result, const CallLinkStatus&amp;, int registerOffset, VirtualRegister thisArgument, VirtualRegister argumentsArgument, unsigned argumentsOffset, NodeType callOp, InlineCallFrame::Kind);
 171     unsigned getInliningBalance(const CallLinkStatus&amp;, CodeSpecializationKind);
 172     enum class CallOptimizationResult { OptimizedToJump, Inlined, DidNothing };
 173     CallOptimizationResult handleCallVariant(Node* callTargetNode, VirtualRegister result, CallVariant, int registerOffset, VirtualRegister thisArgument, int argumentCountIncludingThis, unsigned nextOffset, InlineCallFrame::Kind, SpeculatedType prediction, unsigned&amp; inliningBalance, BasicBlock* continuationBlock, bool needsToCheckCallee);
 174     CallOptimizationResult handleInlining(Node* callTargetNode, VirtualRegister result, const CallLinkStatus&amp;, int registerOffset, VirtualRegister thisArgument, int argumentCountIncludingThis, unsigned nextOffset, NodeType callOp, InlineCallFrame::Kind, SpeculatedType prediction);
 175     template&lt;typename ChecksFunctor&gt;
 176     void inlineCall(Node* callTargetNode, VirtualRegister result, CallVariant, int registerOffset, int argumentCountIncludingThis, InlineCallFrame::Kind, BasicBlock* continuationBlock, const ChecksFunctor&amp; insertChecks);
 177     // Handle intrinsic functions. Return true if it succeeded, false if we need to plant a call.
 178     template&lt;typename ChecksFunctor&gt;
 179     bool handleIntrinsicCall(Node* callee, VirtualRegister result, Intrinsic, int registerOffset, int argumentCountIncludingThis, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks);
 180     template&lt;typename ChecksFunctor&gt;
 181     bool handleDOMJITCall(Node* callee, VirtualRegister result, const DOMJIT::Signature*, int registerOffset, int argumentCountIncludingThis, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks);
 182     template&lt;typename ChecksFunctor&gt;
 183     bool handleIntrinsicGetter(VirtualRegister result, SpeculatedType prediction, const GetByIdVariant&amp; intrinsicVariant, Node* thisNode, const ChecksFunctor&amp; insertChecks);
 184     template&lt;typename ChecksFunctor&gt;
 185     bool handleTypedArrayConstructor(VirtualRegister result, InternalFunction*, int registerOffset, int argumentCountIncludingThis, TypedArrayType, const ChecksFunctor&amp; insertChecks);
 186     template&lt;typename ChecksFunctor&gt;
 187     bool handleConstantInternalFunction(Node* callTargetNode, VirtualRegister result, InternalFunction*, int registerOffset, int argumentCountIncludingThis, CodeSpecializationKind, SpeculatedType, const ChecksFunctor&amp; insertChecks);
 188     Node* handlePutByOffset(Node* base, unsigned identifier, PropertyOffset, Node* value);
 189     Node* handleGetByOffset(SpeculatedType, Node* base, unsigned identifierNumber, PropertyOffset, NodeType = GetByOffset);
 190     bool handleDOMJITGetter(VirtualRegister result, const GetByIdVariant&amp;, Node* thisNode, unsigned identifierNumber, SpeculatedType prediction);
 191     bool handleModuleNamespaceLoad(VirtualRegister result, SpeculatedType, Node* base, GetByIdStatus);
 192 
 193     template&lt;typename Bytecode&gt;
 194     void handlePutByVal(Bytecode, unsigned instructionSize);
 195     template &lt;typename Bytecode&gt;
 196     void handlePutAccessorById(NodeType, Bytecode);
 197     template &lt;typename Bytecode&gt;
 198     void handlePutAccessorByVal(NodeType, Bytecode);
 199     template &lt;typename Bytecode&gt;
 200     void handleNewFunc(NodeType, Bytecode);
 201     template &lt;typename Bytecode&gt;
 202     void handleNewFuncExp(NodeType, Bytecode);
 203 
 204     // Create a presence ObjectPropertyCondition based on some known offset and structure set. Does not
 205     // check the validity of the condition, but it may return a null one if it encounters a contradiction.
 206     ObjectPropertyCondition presenceLike(
 207         JSObject* knownBase, UniquedStringImpl*, PropertyOffset, const StructureSet&amp;);
 208 
 209     // Attempt to watch the presence of a property. It will watch that the property is present in the same
 210     // way as in all of the structures in the set. It may emit code instead of just setting a watchpoint.
 211     // Returns true if this all works out.
 212     bool checkPresenceLike(JSObject* knownBase, UniquedStringImpl*, PropertyOffset, const StructureSet&amp;);
 213     void checkPresenceLike(Node* base, UniquedStringImpl*, PropertyOffset, const StructureSet&amp;);
 214 
 215     // Works with both GetByIdVariant and the setter form of PutByIdVariant.
 216     template&lt;typename VariantType&gt;
 217     Node* load(SpeculatedType, Node* base, unsigned identifierNumber, const VariantType&amp;);
 218 
 219     Node* store(Node* base, unsigned identifier, const PutByIdVariant&amp;, Node* value);
 220 
 221     template&lt;typename Op&gt;
 222     void parseGetById(const Instruction*);
 223     void handleGetById(
 224         VirtualRegister destination, SpeculatedType, Node* base, unsigned identifierNumber, GetByIdStatus, AccessType, unsigned instructionSize);
 225     void emitPutById(
 226         Node* base, unsigned identifierNumber, Node* value,  const PutByIdStatus&amp;, bool isDirect);
 227     void handlePutById(
 228         Node* base, unsigned identifierNumber, Node* value, const PutByIdStatus&amp;,
 229         bool isDirect, unsigned intructionSize);
 230 
 231     // Either register a watchpoint or emit a check for this condition. Returns false if the
 232     // condition no longer holds, and therefore no reasonable check can be emitted.
 233     bool check(const ObjectPropertyCondition&amp;);
 234 
 235     GetByOffsetMethod promoteToConstant(GetByOffsetMethod);
 236 
 237     // Either register a watchpoint or emit a check for this condition. It must be a Presence
 238     // condition. It will attempt to promote a Presence condition to an Equivalence condition.
 239     // Emits code for the loaded value that the condition guards, and returns a node containing
 240     // the loaded value. Returns null if the condition no longer holds.
 241     GetByOffsetMethod planLoad(const ObjectPropertyCondition&amp;);
 242     Node* load(SpeculatedType, unsigned identifierNumber, const GetByOffsetMethod&amp;, NodeType = GetByOffset);
 243     Node* load(SpeculatedType, const ObjectPropertyCondition&amp;, NodeType = GetByOffset);
 244 
 245     // Calls check() for each condition in the set: that is, it either emits checks or registers
 246     // watchpoints (or a combination of the two) to make the conditions hold. If any of those
 247     // conditions are no longer checkable, returns false.
 248     bool check(const ObjectPropertyConditionSet&amp;);
 249 
 250     // Calls check() for those conditions that aren&#39;t the slot base, and calls load() for the slot
 251     // base. Does a combination of watchpoint registration and check emission to guard the
 252     // conditions, and emits code to load the value from the slot base. Returns a node containing
 253     // the loaded value. Returns null if any of the conditions were no longer checkable.
 254     GetByOffsetMethod planLoad(const ObjectPropertyConditionSet&amp;);
 255     Node* load(SpeculatedType, const ObjectPropertyConditionSet&amp;, NodeType = GetByOffset);
 256 
 257     void prepareToParseBlock();
 258     void clearCaches();
 259 
 260     // Parse a single basic block of bytecode instructions.
 261     void parseBlock(unsigned limit);
 262     // Link block successors.
 263     void linkBlock(BasicBlock*, Vector&lt;BasicBlock*&gt;&amp; possibleTargets);
 264     void linkBlocks(Vector&lt;BasicBlock*&gt;&amp; unlinkedBlocks, Vector&lt;BasicBlock*&gt;&amp; possibleTargets);
 265 
 266     VariableAccessData* newVariableAccessData(VirtualRegister operand)
 267     {
 268         ASSERT(!operand.isConstant());
 269 
<a name="1" id="anc1"></a><span class="line-modified"> 270         m_graph.m_variableAccessData.append(operand);</span>
 271         return &amp;m_graph.m_variableAccessData.last();
 272     }
 273 
 274     // Get/Set the operands/result of a bytecode instruction.
 275     Node* getDirect(VirtualRegister operand)
 276     {
 277         ASSERT(!operand.isConstant());
 278 
 279         // Is this an argument?
 280         if (operand.isArgument())
 281             return getArgument(operand);
 282 
 283         // Must be a local.
 284         return getLocal(operand);
 285     }
 286 
 287     Node* get(VirtualRegister operand)
 288     {
 289         if (operand.isConstant()) {
 290             unsigned constantIndex = operand.toConstantIndex();
 291             unsigned oldSize = m_constants.size();
 292             if (constantIndex &gt;= oldSize || !m_constants[constantIndex]) {
 293                 const CodeBlock&amp; codeBlock = *m_inlineStackTop-&gt;m_codeBlock;
 294                 JSValue value = codeBlock.getConstant(operand.offset());
 295                 SourceCodeRepresentation sourceCodeRepresentation = codeBlock.constantSourceCodeRepresentation(operand.offset());
 296                 if (constantIndex &gt;= oldSize) {
 297                     m_constants.grow(constantIndex + 1);
 298                     for (unsigned i = oldSize; i &lt; m_constants.size(); ++i)
 299                         m_constants[i] = nullptr;
 300                 }
 301 
 302                 Node* constantNode = nullptr;
 303                 if (sourceCodeRepresentation == SourceCodeRepresentation::Double)
 304                     constantNode = addToGraph(DoubleConstant, OpInfo(m_graph.freezeStrong(jsDoubleNumber(value.asNumber()))));
 305                 else
 306                     constantNode = addToGraph(JSConstant, OpInfo(m_graph.freezeStrong(value)));
 307                 m_constants[constantIndex] = constantNode;
 308             }
 309             ASSERT(m_constants[constantIndex]);
 310             return m_constants[constantIndex];
 311         }
 312 
 313         if (inlineCallFrame()) {
 314             if (!inlineCallFrame()-&gt;isClosureCall) {
 315                 JSFunction* callee = inlineCallFrame()-&gt;calleeConstant();
 316                 if (operand.offset() == CallFrameSlot::callee)
 317                     return weakJSConstant(callee);
 318             }
 319         } else if (operand.offset() == CallFrameSlot::callee) {
 320             // We have to do some constant-folding here because this enables CreateThis folding. Note
 321             // that we don&#39;t have such watchpoint-based folding for inlined uses of Callee, since in that
 322             // case if the function is a singleton then we already know it.
 323             if (FunctionExecutable* executable = jsDynamicCast&lt;FunctionExecutable*&gt;(*m_vm, m_codeBlock-&gt;ownerExecutable())) {
<a name="2" id="anc2"></a><span class="line-modified"> 324                 if (JSFunction* function = executable-&gt;singleton().inferredValue()) {</span>
<span class="line-modified"> 325                     m_graph.watchpoints().addLazily(executable);</span>


 326                     return weakJSConstant(function);
 327                 }
 328             }
 329             return addToGraph(GetCallee);
 330         }
 331 
 332         return getDirect(m_inlineStackTop-&gt;remapOperand(operand));
 333     }
 334 
 335     enum SetMode {
 336         // A normal set which follows a two-phase commit that spans code origins. During
 337         // the current code origin it issues a MovHint, and at the start of the next
 338         // code origin there will be a SetLocal. If the local needs flushing, the second
 339         // SetLocal will be preceded with a Flush.
 340         NormalSet,
 341 
 342         // A set where the SetLocal happens immediately and there is still a Flush. This
 343         // is relevant when assigning to a local in tricky situations for the delayed
 344         // SetLocal logic but where we know that we have not performed any side effects
 345         // within this code origin. This is a safe replacement for NormalSet anytime we
 346         // know that we have not yet performed side effects in this code origin.
 347         ImmediateSetWithFlush,
 348 
 349         // A set where the SetLocal happens immediately and we do not Flush it even if
 350         // this is a local that is marked as needing it. This is relevant when
 351         // initializing locals at the top of a function.
 352         ImmediateNakedSet
 353     };
 354     Node* setDirect(VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 355     {
 356         addToGraph(MovHint, OpInfo(operand.offset()), value);
 357 
 358         // We can&#39;t exit anymore because our OSR exit state has changed.
 359         m_exitOK = false;
 360 
 361         DelayedSetLocal delayed(currentCodeOrigin(), operand, value, setMode);
 362 
 363         if (setMode == NormalSet) {
 364             m_setLocalQueue.append(delayed);
 365             return nullptr;
 366         }
 367 
 368         return delayed.execute(this);
 369     }
 370 
 371     void processSetLocalQueue()
 372     {
 373         for (unsigned i = 0; i &lt; m_setLocalQueue.size(); ++i)
 374             m_setLocalQueue[i].execute(this);
 375         m_setLocalQueue.shrink(0);
 376     }
 377 
 378     Node* set(VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 379     {
 380         return setDirect(m_inlineStackTop-&gt;remapOperand(operand), value, setMode);
 381     }
 382 
 383     Node* injectLazyOperandSpeculation(Node* node)
 384     {
 385         ASSERT(node-&gt;op() == GetLocal);
<a name="3" id="anc3"></a><span class="line-modified"> 386         ASSERT(node-&gt;origin.semantic.bytecodeIndex() == m_currentIndex);</span>
 387         ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
 388         LazyOperandValueProfileKey key(m_currentIndex, node-&gt;local());
 389         SpeculatedType prediction = m_inlineStackTop-&gt;m_lazyOperands.prediction(locker, key);
 390         node-&gt;variableAccessData()-&gt;predict(prediction);
 391         return node;
 392     }
 393 
 394     // Used in implementing get/set, above, where the operand is a local variable.
 395     Node* getLocal(VirtualRegister operand)
 396     {
 397         unsigned local = operand.toLocal();
 398 
 399         Node* node = m_currentBlock-&gt;variablesAtTail.local(local);
 400 
 401         // This has two goals: 1) link together variable access datas, and 2)
 402         // try to avoid creating redundant GetLocals. (1) is required for
 403         // correctness - no other phase will ensure that block-local variable
 404         // access data unification is done correctly. (2) is purely opportunistic
 405         // and is meant as an compile-time optimization only.
 406 
 407         VariableAccessData* variable;
 408 
 409         if (node) {
 410             variable = node-&gt;variableAccessData();
 411 
 412             switch (node-&gt;op()) {
 413             case GetLocal:
 414                 return node;
 415             case SetLocal:
 416                 return node-&gt;child1().node();
 417             default:
 418                 break;
 419             }
 420         } else
 421             variable = newVariableAccessData(operand);
 422 
 423         node = injectLazyOperandSpeculation(addToGraph(GetLocal, OpInfo(variable)));
 424         m_currentBlock-&gt;variablesAtTail.local(local) = node;
 425         return node;
 426     }
 427     Node* setLocal(const CodeOrigin&amp; semanticOrigin, VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 428     {
 429         SetForScope&lt;CodeOrigin&gt; originChange(m_currentSemanticOrigin, semanticOrigin);
 430 
 431         unsigned local = operand.toLocal();
 432 
 433         if (setMode != ImmediateNakedSet) {
 434             ArgumentPosition* argumentPosition = findArgumentPositionForLocal(operand);
 435             if (argumentPosition)
 436                 flushDirect(operand, argumentPosition);
 437             else if (m_graph.needsScopeRegister() &amp;&amp; operand == m_codeBlock-&gt;scopeRegister())
 438                 flush(operand);
 439         }
 440 
 441         VariableAccessData* variableAccessData = newVariableAccessData(operand);
 442         variableAccessData-&gt;mergeStructureCheckHoistingFailed(
<a name="4" id="anc4"></a><span class="line-modified"> 443             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex(), BadCache));</span>
 444         variableAccessData-&gt;mergeCheckArrayHoistingFailed(
<a name="5" id="anc5"></a><span class="line-modified"> 445             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex(), BadIndexingType));</span>
 446         Node* node = addToGraph(SetLocal, OpInfo(variableAccessData), value);
 447         m_currentBlock-&gt;variablesAtTail.local(local) = node;
 448         return node;
 449     }
 450 
 451     // Used in implementing get/set, above, where the operand is an argument.
 452     Node* getArgument(VirtualRegister operand)
 453     {
 454         unsigned argument = operand.toArgument();
 455         ASSERT(argument &lt; m_numArguments);
 456 
 457         Node* node = m_currentBlock-&gt;variablesAtTail.argument(argument);
 458 
 459         VariableAccessData* variable;
 460 
 461         if (node) {
 462             variable = node-&gt;variableAccessData();
 463 
 464             switch (node-&gt;op()) {
 465             case GetLocal:
 466                 return node;
 467             case SetLocal:
 468                 return node-&gt;child1().node();
 469             default:
 470                 break;
 471             }
 472         } else
 473             variable = newVariableAccessData(operand);
 474 
 475         node = injectLazyOperandSpeculation(addToGraph(GetLocal, OpInfo(variable)));
 476         m_currentBlock-&gt;variablesAtTail.argument(argument) = node;
 477         return node;
 478     }
 479     Node* setArgument(const CodeOrigin&amp; semanticOrigin, VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 480     {
 481         SetForScope&lt;CodeOrigin&gt; originChange(m_currentSemanticOrigin, semanticOrigin);
 482 
 483         unsigned argument = operand.toArgument();
 484         ASSERT(argument &lt; m_numArguments);
 485 
 486         VariableAccessData* variableAccessData = newVariableAccessData(operand);
 487 
 488         // Always flush arguments, except for &#39;this&#39;. If &#39;this&#39; is created by us,
 489         // then make sure that it&#39;s never unboxed.
 490         if (argument || m_graph.needsFlushedThis()) {
 491             if (setMode != ImmediateNakedSet)
 492                 flushDirect(operand);
 493         }
 494 
 495         if (!argument &amp;&amp; m_codeBlock-&gt;specializationKind() == CodeForConstruct)
 496             variableAccessData-&gt;mergeShouldNeverUnbox(true);
 497 
 498         variableAccessData-&gt;mergeStructureCheckHoistingFailed(
<a name="6" id="anc6"></a><span class="line-modified"> 499             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex(), BadCache));</span>
 500         variableAccessData-&gt;mergeCheckArrayHoistingFailed(
<a name="7" id="anc7"></a><span class="line-modified"> 501             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex(), BadIndexingType));</span>
 502         Node* node = addToGraph(SetLocal, OpInfo(variableAccessData), value);
 503         m_currentBlock-&gt;variablesAtTail.argument(argument) = node;
 504         return node;
 505     }
 506 
 507     ArgumentPosition* findArgumentPositionForArgument(int argument)
 508     {
 509         InlineStackEntry* stack = m_inlineStackTop;
 510         while (stack-&gt;m_inlineCallFrame)
 511             stack = stack-&gt;m_caller;
 512         return stack-&gt;m_argumentPositions[argument];
 513     }
 514 
 515     ArgumentPosition* findArgumentPositionForLocal(VirtualRegister operand)
 516     {
 517         for (InlineStackEntry* stack = m_inlineStackTop; ; stack = stack-&gt;m_caller) {
 518             InlineCallFrame* inlineCallFrame = stack-&gt;m_inlineCallFrame;
 519             if (!inlineCallFrame)
 520                 break;
 521             if (operand.offset() &lt; static_cast&lt;int&gt;(inlineCallFrame-&gt;stackOffset + CallFrame::headerSizeInRegisters))
 522                 continue;
 523             if (operand.offset() &gt;= static_cast&lt;int&gt;(inlineCallFrame-&gt;stackOffset + CallFrame::thisArgumentOffset() + inlineCallFrame-&gt;argumentsWithFixup.size()))
 524                 continue;
 525             int argument = VirtualRegister(operand.offset() - inlineCallFrame-&gt;stackOffset).toArgument();
 526             return stack-&gt;m_argumentPositions[argument];
 527         }
 528         return 0;
 529     }
 530 
 531     ArgumentPosition* findArgumentPosition(VirtualRegister operand)
 532     {
 533         if (operand.isArgument())
 534             return findArgumentPositionForArgument(operand.toArgument());
 535         return findArgumentPositionForLocal(operand);
 536     }
 537 
 538     template&lt;typename AddFlushDirectFunc&gt;
 539     void flushImpl(InlineCallFrame* inlineCallFrame, const AddFlushDirectFunc&amp; addFlushDirect)
 540     {
 541         int numArguments;
 542         if (inlineCallFrame) {
 543             ASSERT(!m_graph.hasDebuggerEnabled());
 544             numArguments = inlineCallFrame-&gt;argumentsWithFixup.size();
 545             if (inlineCallFrame-&gt;isClosureCall)
 546                 addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, VirtualRegister(CallFrameSlot::callee)));
 547             if (inlineCallFrame-&gt;isVarargs())
 548                 addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, VirtualRegister(CallFrameSlot::argumentCount)));
 549         } else
 550             numArguments = m_graph.baselineCodeBlockFor(inlineCallFrame)-&gt;numParameters();
 551 
 552         for (unsigned argument = numArguments; argument--;)
 553             addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, virtualRegisterForArgument(argument)));
 554 
 555         if (m_graph.needsScopeRegister())
 556             addFlushDirect(nullptr, m_graph.m_codeBlock-&gt;scopeRegister());
 557     }
 558 
 559     template&lt;typename AddFlushDirectFunc, typename AddPhantomLocalDirectFunc&gt;
 560     void flushForTerminalImpl(CodeOrigin origin, const AddFlushDirectFunc&amp; addFlushDirect, const AddPhantomLocalDirectFunc&amp; addPhantomLocalDirect)
 561     {
 562         origin.walkUpInlineStack(
 563             [&amp;] (CodeOrigin origin) {
<a name="8" id="anc8"></a><span class="line-modified"> 564                 unsigned bytecodeIndex = origin.bytecodeIndex();</span>
<span class="line-modified"> 565                 InlineCallFrame* inlineCallFrame = origin.inlineCallFrame();</span>
 566                 flushImpl(inlineCallFrame, addFlushDirect);
 567 
 568                 CodeBlock* codeBlock = m_graph.baselineCodeBlockFor(inlineCallFrame);
 569                 FullBytecodeLiveness&amp; fullLiveness = m_graph.livenessFor(codeBlock);
 570                 const FastBitVector&amp; livenessAtBytecode = fullLiveness.getLiveness(bytecodeIndex);
 571 
 572                 for (unsigned local = codeBlock-&gt;numCalleeLocals(); local--;) {
 573                     if (livenessAtBytecode[local])
 574                         addPhantomLocalDirect(inlineCallFrame, remapOperand(inlineCallFrame, virtualRegisterForLocal(local)));
 575                 }
 576             });
 577     }
 578 
 579     void flush(VirtualRegister operand)
 580     {
 581         flushDirect(m_inlineStackTop-&gt;remapOperand(operand));
 582     }
 583 
 584     void flushDirect(VirtualRegister operand)
 585     {
 586         flushDirect(operand, findArgumentPosition(operand));
 587     }
 588 
 589     void flushDirect(VirtualRegister operand, ArgumentPosition* argumentPosition)
 590     {
 591         addFlushOrPhantomLocal&lt;Flush&gt;(operand, argumentPosition);
 592     }
 593 
 594     template&lt;NodeType nodeType&gt;
 595     void addFlushOrPhantomLocal(VirtualRegister operand, ArgumentPosition* argumentPosition)
 596     {
 597         ASSERT(!operand.isConstant());
 598 
 599         Node* node = m_currentBlock-&gt;variablesAtTail.operand(operand);
 600 
 601         VariableAccessData* variable;
 602 
 603         if (node)
 604             variable = node-&gt;variableAccessData();
 605         else
 606             variable = newVariableAccessData(operand);
 607 
 608         node = addToGraph(nodeType, OpInfo(variable));
 609         m_currentBlock-&gt;variablesAtTail.operand(operand) = node;
 610         if (argumentPosition)
 611             argumentPosition-&gt;addVariable(variable);
 612     }
 613 
 614     void phantomLocalDirect(VirtualRegister operand)
 615     {
 616         addFlushOrPhantomLocal&lt;PhantomLocal&gt;(operand, findArgumentPosition(operand));
 617     }
 618 
 619     void flush(InlineStackEntry* inlineStackEntry)
 620     {
 621         auto addFlushDirect = [&amp;] (InlineCallFrame*, VirtualRegister reg) { flushDirect(reg); };
 622         flushImpl(inlineStackEntry-&gt;m_inlineCallFrame, addFlushDirect);
 623     }
 624 
 625     void flushForTerminal()
 626     {
 627         auto addFlushDirect = [&amp;] (InlineCallFrame*, VirtualRegister reg) { flushDirect(reg); };
 628         auto addPhantomLocalDirect = [&amp;] (InlineCallFrame*, VirtualRegister reg) { phantomLocalDirect(reg); };
 629         flushForTerminalImpl(currentCodeOrigin(), addFlushDirect, addPhantomLocalDirect);
 630     }
 631 
 632     void flushForReturn()
 633     {
 634         flush(m_inlineStackTop);
 635     }
 636 
 637     void flushIfTerminal(SwitchData&amp; data)
 638     {
 639         if (data.fallThrough.bytecodeIndex() &gt; m_currentIndex)
 640             return;
 641 
 642         for (unsigned i = data.cases.size(); i--;) {
 643             if (data.cases[i].target.bytecodeIndex() &gt; m_currentIndex)
 644                 return;
 645         }
 646 
 647         flushForTerminal();
 648     }
 649 
 650     // Assumes that the constant should be strongly marked.
 651     Node* jsConstant(JSValue constantValue)
 652     {
 653         return addToGraph(JSConstant, OpInfo(m_graph.freezeStrong(constantValue)));
 654     }
 655 
 656     Node* weakJSConstant(JSValue constantValue)
 657     {
 658         return addToGraph(JSConstant, OpInfo(m_graph.freeze(constantValue)));
 659     }
 660 
 661     // Helper functions to get/set the this value.
 662     Node* getThis()
 663     {
 664         return get(m_inlineStackTop-&gt;m_codeBlock-&gt;thisRegister());
 665     }
 666 
 667     void setThis(Node* value)
 668     {
 669         set(m_inlineStackTop-&gt;m_codeBlock-&gt;thisRegister(), value);
 670     }
 671 
 672     InlineCallFrame* inlineCallFrame()
 673     {
 674         return m_inlineStackTop-&gt;m_inlineCallFrame;
 675     }
 676 
 677     bool allInlineFramesAreTailCalls()
 678     {
 679         return !inlineCallFrame() || !inlineCallFrame()-&gt;getCallerSkippingTailCalls();
 680     }
 681 
 682     CodeOrigin currentCodeOrigin()
 683     {
 684         return CodeOrigin(m_currentIndex, inlineCallFrame());
 685     }
 686 
 687     NodeOrigin currentNodeOrigin()
 688     {
 689         CodeOrigin semantic;
 690         CodeOrigin forExit;
 691 
 692         if (m_currentSemanticOrigin.isSet())
 693             semantic = m_currentSemanticOrigin;
 694         else
 695             semantic = currentCodeOrigin();
 696 
 697         forExit = currentCodeOrigin();
 698 
 699         return NodeOrigin(semantic, forExit, m_exitOK);
 700     }
 701 
 702     BranchData* branchData(unsigned taken, unsigned notTaken)
 703     {
 704         // We assume that branches originating from bytecode always have a fall-through. We
 705         // use this assumption to avoid checking for the creation of terminal blocks.
 706         ASSERT((taken &gt; m_currentIndex) || (notTaken &gt; m_currentIndex));
 707         BranchData* data = m_graph.m_branchData.add();
 708         *data = BranchData::withBytecodeIndices(taken, notTaken);
 709         return data;
 710     }
 711 
 712     Node* addToGraph(Node* node)
 713     {
 714         VERBOSE_LOG(&quot;        appended &quot;, node, &quot; &quot;, Graph::opName(node-&gt;op()), &quot;\n&quot;);
 715 
 716         m_hasAnyForceOSRExits |= (node-&gt;op() == ForceOSRExit);
 717 
 718         m_currentBlock-&gt;append(node);
 719         if (clobbersExitState(m_graph, node))
 720             m_exitOK = false;
 721         return node;
 722     }
 723 
 724     Node* addToGraph(NodeType op, Node* child1 = 0, Node* child2 = 0, Node* child3 = 0)
 725     {
 726         Node* result = m_graph.addNode(
 727             op, currentNodeOrigin(), Edge(child1), Edge(child2),
 728             Edge(child3));
 729         return addToGraph(result);
 730     }
 731     Node* addToGraph(NodeType op, Edge child1, Edge child2 = Edge(), Edge child3 = Edge())
 732     {
 733         Node* result = m_graph.addNode(
 734             op, currentNodeOrigin(), child1, child2, child3);
 735         return addToGraph(result);
 736     }
 737     Node* addToGraph(NodeType op, OpInfo info, Node* child1 = 0, Node* child2 = 0, Node* child3 = 0)
 738     {
 739         Node* result = m_graph.addNode(
 740             op, currentNodeOrigin(), info, Edge(child1), Edge(child2),
 741             Edge(child3));
 742         return addToGraph(result);
 743     }
 744     Node* addToGraph(NodeType op, OpInfo info, Edge child1, Edge child2 = Edge(), Edge child3 = Edge())
 745     {
 746         Node* result = m_graph.addNode(op, currentNodeOrigin(), info, child1, child2, child3);
 747         return addToGraph(result);
 748     }
 749     Node* addToGraph(NodeType op, OpInfo info1, OpInfo info2, Node* child1 = 0, Node* child2 = 0, Node* child3 = 0)
 750     {
 751         Node* result = m_graph.addNode(
 752             op, currentNodeOrigin(), info1, info2,
 753             Edge(child1), Edge(child2), Edge(child3));
 754         return addToGraph(result);
 755     }
 756     Node* addToGraph(NodeType op, OpInfo info1, OpInfo info2, Edge child1, Edge child2 = Edge(), Edge child3 = Edge())
 757     {
 758         Node* result = m_graph.addNode(
 759             op, currentNodeOrigin(), info1, info2, child1, child2, child3);
 760         return addToGraph(result);
 761     }
 762 
 763     Node* addToGraph(Node::VarArgTag, NodeType op, OpInfo info1, OpInfo info2 = OpInfo())
 764     {
 765         Node* result = m_graph.addNode(
 766             Node::VarArg, op, currentNodeOrigin(), info1, info2,
 767             m_graph.m_varArgChildren.size() - m_numPassedVarArgs, m_numPassedVarArgs);
 768         addToGraph(result);
 769 
 770         m_numPassedVarArgs = 0;
 771 
 772         return result;
 773     }
 774 
 775     void addVarArgChild(Node* child)
 776     {
 777         m_graph.m_varArgChildren.append(Edge(child));
 778         m_numPassedVarArgs++;
 779     }
 780 
 781     void addVarArgChild(Edge child)
 782     {
 783         m_graph.m_varArgChildren.append(child);
 784         m_numPassedVarArgs++;
 785     }
 786 
 787     Node* addCallWithoutSettingResult(
 788         NodeType op, OpInfo opInfo, Node* callee, int argCount, int registerOffset,
 789         OpInfo prediction)
 790     {
 791         addVarArgChild(callee);
 792         size_t parameterSlots = Graph::parameterSlotsForArgCount(argCount);
 793 
 794         if (parameterSlots &gt; m_parameterSlots)
 795             m_parameterSlots = parameterSlots;
 796 
 797         for (int i = 0; i &lt; argCount; ++i)
 798             addVarArgChild(get(virtualRegisterForArgument(i, registerOffset)));
 799 
 800         return addToGraph(Node::VarArg, op, opInfo, prediction);
 801     }
 802 
 803     Node* addCall(
 804         VirtualRegister result, NodeType op, const DOMJIT::Signature* signature, Node* callee, int argCount, int registerOffset,
 805         SpeculatedType prediction)
 806     {
 807         if (op == TailCall) {
 808             if (allInlineFramesAreTailCalls())
 809                 return addCallWithoutSettingResult(op, OpInfo(signature), callee, argCount, registerOffset, OpInfo());
 810             op = TailCallInlinedCaller;
 811         }
 812 
 813 
 814         Node* call = addCallWithoutSettingResult(
 815             op, OpInfo(signature), callee, argCount, registerOffset, OpInfo(prediction));
 816         if (result.isValid())
 817             set(result, call);
 818         return call;
 819     }
 820 
 821     Node* cellConstantWithStructureCheck(JSCell* object, Structure* structure)
 822     {
 823         // FIXME: This should route to emitPropertyCheck, not the other way around. But currently,
 824         // this gets no profit from using emitPropertyCheck() since we&#39;ll non-adaptively watch the
 825         // object&#39;s structure as soon as we make it a weakJSCosntant.
 826         Node* objectNode = weakJSConstant(object);
 827         addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(structure)), objectNode);
 828         return objectNode;
 829     }
 830 
 831     SpeculatedType getPredictionWithoutOSRExit(unsigned bytecodeIndex)
 832     {
<a name="9" id="anc9"></a><span class="line-modified"> 833         auto getValueProfilePredictionFromForCodeBlockAndBytecodeOffset = [&amp;] (CodeBlock* codeBlock, const CodeOrigin&amp; codeOrigin)</span>
 834         {
<a name="10" id="anc10"></a><span class="line-modified"> 835             SpeculatedType prediction;</span>
<span class="line-modified"> 836             {</span>
<span class="line-modified"> 837                 ConcurrentJSLocker locker(codeBlock-&gt;m_lock);</span>
<span class="line-added"> 838                 prediction = codeBlock-&gt;valueProfilePredictionForBytecodeOffset(locker, codeOrigin.bytecodeIndex());</span>
<span class="line-added"> 839             }</span>
<span class="line-added"> 840             auto* fuzzerAgent = m_vm-&gt;fuzzerAgent();</span>
<span class="line-added"> 841             if (UNLIKELY(fuzzerAgent))</span>
<span class="line-added"> 842                 return fuzzerAgent-&gt;getPrediction(codeBlock, codeOrigin, prediction) &amp; SpecBytecodeTop;</span>
<span class="line-added"> 843             return prediction;</span>
<span class="line-added"> 844         };</span>
 845 
<a name="11" id="anc11"></a><span class="line-added"> 846         SpeculatedType prediction = getValueProfilePredictionFromForCodeBlockAndBytecodeOffset(m_inlineStackTop-&gt;m_profiledBlock, CodeOrigin(bytecodeIndex, inlineCallFrame()));</span>
 847         if (prediction != SpecNone)
 848             return prediction;
 849 
 850         // If we have no information about the values this
 851         // node generates, we check if by any chance it is
 852         // a tail call opcode. In that case, we walk up the
 853         // inline frames to find a call higher in the call
 854         // chain and use its prediction. If we only have
 855         // inlined tail call frames, we use SpecFullTop
 856         // to avoid a spurious OSR exit.
 857         auto instruction = m_inlineStackTop-&gt;m_profiledBlock-&gt;instructions().at(bytecodeIndex);
 858         OpcodeID opcodeID = instruction-&gt;opcodeID();
 859 
 860         switch (opcodeID) {
 861         case op_tail_call:
 862         case op_tail_call_varargs:
 863         case op_tail_call_forward_arguments: {
 864             // Things should be more permissive to us returning BOTTOM instead of TOP here.
 865             // Currently, this will cause us to Force OSR exit. This is bad because returning
 866             // TOP will cause anything that transitively touches this speculated type to
 867             // also become TOP during prediction propagation.
 868             // https://bugs.webkit.org/show_bug.cgi?id=164337
 869             if (!inlineCallFrame())
 870                 return SpecFullTop;
 871 
 872             CodeOrigin* codeOrigin = inlineCallFrame()-&gt;getCallerSkippingTailCalls();
 873             if (!codeOrigin)
 874                 return SpecFullTop;
 875 
 876             InlineStackEntry* stack = m_inlineStackTop;
<a name="12" id="anc12"></a><span class="line-modified"> 877             while (stack-&gt;m_inlineCallFrame != codeOrigin-&gt;inlineCallFrame())</span>
 878                 stack = stack-&gt;m_caller;
 879 
<a name="13" id="anc13"></a><span class="line-modified"> 880             return getValueProfilePredictionFromForCodeBlockAndBytecodeOffset(stack-&gt;m_profiledBlock, *codeOrigin);</span>



 881         }
 882 
 883         default:
 884             return SpecNone;
 885         }
 886 
 887         RELEASE_ASSERT_NOT_REACHED();
 888         return SpecNone;
 889     }
 890 
 891     SpeculatedType getPrediction(unsigned bytecodeIndex)
 892     {
 893         SpeculatedType prediction = getPredictionWithoutOSRExit(bytecodeIndex);
 894 
 895         if (prediction == SpecNone) {
 896             // We have no information about what values this node generates. Give up
 897             // on executing this code, since we&#39;re likely to do more damage than good.
 898             addToGraph(ForceOSRExit);
 899         }
 900 
 901         return prediction;
 902     }
 903 
 904     SpeculatedType getPredictionWithoutOSRExit()
 905     {
 906         return getPredictionWithoutOSRExit(m_currentIndex);
 907     }
 908 
 909     SpeculatedType getPrediction()
 910     {
 911         return getPrediction(m_currentIndex);
 912     }
 913 
 914     ArrayMode getArrayMode(Array::Action action)
 915     {
 916         CodeBlock* codeBlock = m_inlineStackTop-&gt;m_profiledBlock;
 917         ArrayProfile* profile = codeBlock-&gt;getArrayProfile(codeBlock-&gt;bytecodeOffset(m_currentInstruction));
 918         return getArrayMode(*profile, action);
 919     }
 920 
 921     ArrayMode getArrayMode(ArrayProfile&amp; profile, Array::Action action)
 922     {
 923         ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
 924         profile.computeUpdatedPrediction(locker, m_inlineStackTop-&gt;m_profiledBlock);
 925         bool makeSafe = profile.outOfBounds(locker);
 926         return ArrayMode::fromObserved(locker, &amp;profile, action, makeSafe);
 927     }
 928 
 929     Node* makeSafe(Node* node)
 930     {
 931         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
 932             node-&gt;mergeFlags(NodeMayOverflowInt32InDFG);
 933         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
 934             node-&gt;mergeFlags(NodeMayNegZeroInDFG);
 935 
<a name="14" id="anc14"></a><span class="line-modified"> 936         if (!isX86() &amp;&amp; (node-&gt;op() == ArithMod || node-&gt;op() == ValueMod))</span>
 937             return node;
 938 
 939         {
 940             ArithProfile* arithProfile = m_inlineStackTop-&gt;m_profiledBlock-&gt;arithProfileForBytecodeOffset(m_currentIndex);
 941             if (arithProfile) {
 942                 switch (node-&gt;op()) {
 943                 case ArithAdd:
 944                 case ArithSub:
 945                 case ValueAdd:
 946                     if (arithProfile-&gt;didObserveDouble())
 947                         node-&gt;mergeFlags(NodeMayHaveDoubleResult);
 948                     if (arithProfile-&gt;didObserveNonNumeric())
 949                         node-&gt;mergeFlags(NodeMayHaveNonNumericResult);
 950                     if (arithProfile-&gt;didObserveBigInt())
 951                         node-&gt;mergeFlags(NodeMayHaveBigIntResult);
 952                     break;
 953 
 954                 case ValueMul:
 955                 case ArithMul: {
 956                     if (arithProfile-&gt;didObserveInt52Overflow())
 957                         node-&gt;mergeFlags(NodeMayOverflowInt52);
 958                     if (arithProfile-&gt;didObserveInt32Overflow() || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
 959                         node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline);
 960                     if (arithProfile-&gt;didObserveNegZeroDouble() || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
 961                         node-&gt;mergeFlags(NodeMayNegZeroInBaseline);
 962                     if (arithProfile-&gt;didObserveDouble())
 963                         node-&gt;mergeFlags(NodeMayHaveDoubleResult);
 964                     if (arithProfile-&gt;didObserveNonNumeric())
 965                         node-&gt;mergeFlags(NodeMayHaveNonNumericResult);
 966                     if (arithProfile-&gt;didObserveBigInt())
 967                         node-&gt;mergeFlags(NodeMayHaveBigIntResult);
 968                     break;
 969                 }
 970                 case ValueNegate:
 971                 case ArithNegate: {
 972                     if (arithProfile-&gt;lhsObservedType().sawNumber() || arithProfile-&gt;didObserveDouble())
 973                         node-&gt;mergeFlags(NodeMayHaveDoubleResult);
 974                     if (arithProfile-&gt;didObserveNegZeroDouble() || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
 975                         node-&gt;mergeFlags(NodeMayNegZeroInBaseline);
 976                     if (arithProfile-&gt;didObserveInt32Overflow() || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
 977                         node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline);
 978                     if (arithProfile-&gt;didObserveNonNumeric())
 979                         node-&gt;mergeFlags(NodeMayHaveNonNumericResult);
 980                     if (arithProfile-&gt;didObserveBigInt())
 981                         node-&gt;mergeFlags(NodeMayHaveBigIntResult);
 982                     break;
 983                 }
 984 
 985                 default:
 986                     break;
 987                 }
 988             }
 989         }
 990 
 991         if (m_inlineStackTop-&gt;m_profiledBlock-&gt;likelyToTakeSlowCase(m_currentIndex)) {
 992             switch (node-&gt;op()) {
 993             case UInt32ToNumber:
 994             case ArithAdd:
 995             case ArithSub:
 996             case ValueAdd:
<a name="15" id="anc15"></a><span class="line-added"> 997             case ValueMod:</span>
 998             case ArithMod: // for ArithMod &quot;MayOverflow&quot; means we tried to divide by zero, or we saw double.
 999                 node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline);
1000                 break;
1001 
1002             default:
1003                 break;
1004             }
1005         }
1006 
1007         return node;
1008     }
1009 
1010     Node* makeDivSafe(Node* node)
1011     {
1012         ASSERT(node-&gt;op() == ArithDiv || node-&gt;op() == ValueDiv);
1013 
1014         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
1015             node-&gt;mergeFlags(NodeMayOverflowInt32InDFG);
1016         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
1017             node-&gt;mergeFlags(NodeMayNegZeroInDFG);
1018 
1019         // The main slow case counter for op_div in the old JIT counts only when
1020         // the operands are not numbers. We don&#39;t care about that since we already
1021         // have speculations in place that take care of that separately. We only
1022         // care about when the outcome of the division is not an integer, which
1023         // is what the special fast case counter tells us.
1024 
1025         if (!m_inlineStackTop-&gt;m_profiledBlock-&gt;couldTakeSpecialFastCase(m_currentIndex))
1026             return node;
1027 
1028         // FIXME: It might be possible to make this more granular.
1029         node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline | NodeMayNegZeroInBaseline);
1030 
1031         ArithProfile* arithProfile = m_inlineStackTop-&gt;m_profiledBlock-&gt;arithProfileForBytecodeOffset(m_currentIndex);
1032         if (arithProfile-&gt;didObserveBigInt())
1033             node-&gt;mergeFlags(NodeMayHaveBigIntResult);
1034 
1035         return node;
1036     }
1037 
1038     void noticeArgumentsUse()
1039     {
1040         // All of the arguments in this function need to be formatted as JSValues because we will
1041         // load from them in a random-access fashion and we don&#39;t want to have to switch on
1042         // format.
1043 
1044         for (ArgumentPosition* argument : m_inlineStackTop-&gt;m_argumentPositions)
1045             argument-&gt;mergeShouldNeverUnbox(true);
1046     }
1047 
1048     bool needsDynamicLookup(ResolveType, OpcodeID);
1049 
1050     VM* m_vm;
1051     CodeBlock* m_codeBlock;
1052     CodeBlock* m_profiledBlock;
1053     Graph&amp; m_graph;
1054 
1055     // The current block being generated.
1056     BasicBlock* m_currentBlock;
1057     // The bytecode index of the current instruction being generated.
1058     unsigned m_currentIndex;
1059     // The semantic origin of the current node if different from the current Index.
1060     CodeOrigin m_currentSemanticOrigin;
1061     // True if it&#39;s OK to OSR exit right now.
1062     bool m_exitOK { false };
1063 
1064     FrozenValue* m_constantUndefined;
1065     FrozenValue* m_constantNull;
1066     FrozenValue* m_constantNaN;
1067     FrozenValue* m_constantOne;
1068     Vector&lt;Node*, 16&gt; m_constants;
1069 
1070     HashMap&lt;InlineCallFrame*, Vector&lt;ArgumentPosition*&gt;, WTF::DefaultHash&lt;InlineCallFrame*&gt;::Hash, WTF::NullableHashTraits&lt;InlineCallFrame*&gt;&gt; m_inlineCallFrameToArgumentPositions;
1071 
1072     // The number of arguments passed to the function.
1073     unsigned m_numArguments;
1074     // The number of locals (vars + temporaries) used in the function.
1075     unsigned m_numLocals;
1076     // The number of slots (in units of sizeof(Register)) that we need to
1077     // preallocate for arguments to outgoing calls from this frame. This
1078     // number includes the CallFrame slots that we initialize for the callee
1079     // (but not the callee-initialized CallerFrame and ReturnPC slots).
1080     // This number is 0 if and only if this function is a leaf.
1081     unsigned m_parameterSlots;
1082     // The number of var args passed to the next var arg node.
1083     unsigned m_numPassedVarArgs;
1084 
1085     struct InlineStackEntry {
1086         ByteCodeParser* m_byteCodeParser;
1087 
1088         CodeBlock* m_codeBlock;
1089         CodeBlock* m_profiledBlock;
1090         InlineCallFrame* m_inlineCallFrame;
1091 
1092         ScriptExecutable* executable() { return m_codeBlock-&gt;ownerExecutable(); }
1093 
1094         QueryableExitProfile m_exitProfile;
1095 
1096         // Remapping of identifier and constant numbers from the code block being
1097         // inlined (inline callee) to the code block that we&#39;re inlining into
1098         // (the machine code block, which is the transitive, though not necessarily
1099         // direct, caller).
1100         Vector&lt;unsigned&gt; m_identifierRemap;
1101         Vector&lt;unsigned&gt; m_switchRemap;
1102 
1103         // These are blocks whose terminal is a Jump, Branch or Switch, and whose target has not yet been linked.
1104         // Their terminal instead refers to a bytecode index, and the right BB can be found in m_blockLinkingTargets.
1105         Vector&lt;BasicBlock*&gt; m_unlinkedBlocks;
1106 
1107         // Potential block linking targets. Must be sorted by bytecodeBegin, and
1108         // cannot have two blocks that have the same bytecodeBegin.
1109         Vector&lt;BasicBlock*&gt; m_blockLinkingTargets;
1110 
1111         // Optional: a continuation block for returns to jump to. It is set by early returns if it does not exist.
1112         BasicBlock* m_continuationBlock;
1113 
1114         VirtualRegister m_returnValue;
1115 
1116         // Speculations about variable types collected from the profiled code block,
1117         // which are based on OSR exit profiles that past DFG compilations of this
1118         // code block had gathered.
1119         LazyOperandValueProfileParser m_lazyOperands;
1120 
1121         ICStatusMap m_baselineMap;
1122         ICStatusContext m_optimizedContext;
1123 
1124         // Pointers to the argument position trackers for this slice of code.
1125         Vector&lt;ArgumentPosition*&gt; m_argumentPositions;
1126 
1127         InlineStackEntry* m_caller;
1128 
1129         InlineStackEntry(
1130             ByteCodeParser*,
1131             CodeBlock*,
1132             CodeBlock* profiledBlock,
1133             JSFunction* callee, // Null if this is a closure call.
1134             VirtualRegister returnValueVR,
1135             VirtualRegister inlineCallFrameStart,
1136             int argumentCountIncludingThis,
1137             InlineCallFrame::Kind,
1138             BasicBlock* continuationBlock);
1139 
1140         ~InlineStackEntry();
1141 
1142         VirtualRegister remapOperand(VirtualRegister operand) const
1143         {
1144             if (!m_inlineCallFrame)
1145                 return operand;
1146 
1147             ASSERT(!operand.isConstant());
1148 
1149             return VirtualRegister(operand.offset() + m_inlineCallFrame-&gt;stackOffset);
1150         }
1151     };
1152 
1153     InlineStackEntry* m_inlineStackTop;
1154 
1155     ICStatusContextStack m_icContextStack;
1156 
1157     struct DelayedSetLocal {
1158         CodeOrigin m_origin;
1159         VirtualRegister m_operand;
1160         Node* m_value;
1161         SetMode m_setMode;
1162 
1163         DelayedSetLocal() { }
1164         DelayedSetLocal(const CodeOrigin&amp; origin, VirtualRegister operand, Node* value, SetMode setMode)
1165             : m_origin(origin)
1166             , m_operand(operand)
1167             , m_value(value)
1168             , m_setMode(setMode)
1169         {
1170             RELEASE_ASSERT(operand.isValid());
1171         }
1172 
1173         Node* execute(ByteCodeParser* parser)
1174         {
1175             if (m_operand.isArgument())
1176                 return parser-&gt;setArgument(m_origin, m_operand, m_value, m_setMode);
1177             return parser-&gt;setLocal(m_origin, m_operand, m_value, m_setMode);
1178         }
1179     };
1180 
1181     Vector&lt;DelayedSetLocal, 2&gt; m_setLocalQueue;
1182 
1183     const Instruction* m_currentInstruction;
1184     bool m_hasDebuggerEnabled;
1185     bool m_hasAnyForceOSRExits { false };
1186 };
1187 
1188 BasicBlock* ByteCodeParser::allocateTargetableBlock(unsigned bytecodeIndex)
1189 {
1190     ASSERT(bytecodeIndex != UINT_MAX);
1191     Ref&lt;BasicBlock&gt; block = adoptRef(*new BasicBlock(bytecodeIndex, m_numArguments, m_numLocals, 1));
1192     BasicBlock* blockPtr = block.ptr();
1193     // m_blockLinkingTargets must always be sorted in increasing order of bytecodeBegin
1194     if (m_inlineStackTop-&gt;m_blockLinkingTargets.size())
1195         ASSERT(m_inlineStackTop-&gt;m_blockLinkingTargets.last()-&gt;bytecodeBegin &lt; bytecodeIndex);
1196     m_inlineStackTop-&gt;m_blockLinkingTargets.append(blockPtr);
1197     m_graph.appendBlock(WTFMove(block));
1198     return blockPtr;
1199 }
1200 
1201 BasicBlock* ByteCodeParser::allocateUntargetableBlock()
1202 {
1203     Ref&lt;BasicBlock&gt; block = adoptRef(*new BasicBlock(UINT_MAX, m_numArguments, m_numLocals, 1));
1204     BasicBlock* blockPtr = block.ptr();
1205     m_graph.appendBlock(WTFMove(block));
1206     return blockPtr;
1207 }
1208 
1209 void ByteCodeParser::makeBlockTargetable(BasicBlock* block, unsigned bytecodeIndex)
1210 {
1211     RELEASE_ASSERT(block-&gt;bytecodeBegin == UINT_MAX);
1212     block-&gt;bytecodeBegin = bytecodeIndex;
1213     // m_blockLinkingTargets must always be sorted in increasing order of bytecodeBegin
1214     if (m_inlineStackTop-&gt;m_blockLinkingTargets.size())
1215         ASSERT(m_inlineStackTop-&gt;m_blockLinkingTargets.last()-&gt;bytecodeBegin &lt; bytecodeIndex);
1216     m_inlineStackTop-&gt;m_blockLinkingTargets.append(block);
1217 }
1218 
1219 void ByteCodeParser::addJumpTo(BasicBlock* block)
1220 {
1221     ASSERT(!m_currentBlock-&gt;terminal());
1222     Node* jumpNode = addToGraph(Jump);
1223     jumpNode-&gt;targetBlock() = block;
1224     m_currentBlock-&gt;didLink();
1225 }
1226 
1227 void ByteCodeParser::addJumpTo(unsigned bytecodeIndex)
1228 {
1229     ASSERT(!m_currentBlock-&gt;terminal());
1230     addToGraph(Jump, OpInfo(bytecodeIndex));
1231     m_inlineStackTop-&gt;m_unlinkedBlocks.append(m_currentBlock);
1232 }
1233 
1234 template&lt;typename CallOp&gt;
1235 ByteCodeParser::Terminality ByteCodeParser::handleCall(const Instruction* pc, NodeType op, CallMode callMode)
1236 {
1237     auto bytecode = pc-&gt;as&lt;CallOp&gt;();
1238     Node* callTarget = get(bytecode.m_callee);
1239     int registerOffset = -static_cast&lt;int&gt;(bytecode.m_argv);
1240 
1241     CallLinkStatus callLinkStatus = CallLinkStatus::computeFor(
1242         m_inlineStackTop-&gt;m_profiledBlock, currentCodeOrigin(),
1243         m_inlineStackTop-&gt;m_baselineMap, m_icContextStack);
1244 
1245     InlineCallFrame::Kind kind = InlineCallFrame::kindFor(callMode);
1246 
1247     return handleCall(bytecode.m_dst, op, kind, pc-&gt;size(), callTarget,
1248         bytecode.m_argc, registerOffset, callLinkStatus, getPrediction());
1249 }
1250 
1251 void ByteCodeParser::refineStatically(CallLinkStatus&amp; callLinkStatus, Node* callTarget)
1252 {
1253     if (callTarget-&gt;isCellConstant())
1254         callLinkStatus.setProvenConstantCallee(CallVariant(callTarget-&gt;asCell()));
1255 }
1256 
1257 ByteCodeParser::Terminality ByteCodeParser::handleCall(
1258     VirtualRegister result, NodeType op, InlineCallFrame::Kind kind, unsigned instructionSize,
1259     Node* callTarget, int argumentCountIncludingThis, int registerOffset,
1260     CallLinkStatus callLinkStatus, SpeculatedType prediction)
1261 {
1262     ASSERT(registerOffset &lt;= 0);
1263 
1264     refineStatically(callLinkStatus, callTarget);
1265 
1266     VERBOSE_LOG(&quot;    Handling call at &quot;, currentCodeOrigin(), &quot;: &quot;, callLinkStatus, &quot;\n&quot;);
1267 
1268     // If we have profiling information about this call, and it did not behave too polymorphically,
1269     // we may be able to inline it, or in the case of recursive tail calls turn it into a jump.
1270     if (callLinkStatus.canOptimize()) {
1271         addToGraph(FilterCallLinkStatus, OpInfo(m_graph.m_plan.recordedStatuses().addCallLinkStatus(currentCodeOrigin(), callLinkStatus)), callTarget);
1272 
1273         VirtualRegister thisArgument = virtualRegisterForArgument(0, registerOffset);
1274         auto optimizationResult = handleInlining(callTarget, result, callLinkStatus, registerOffset, thisArgument,
1275             argumentCountIncludingThis, m_currentIndex + instructionSize, op, kind, prediction);
1276         if (optimizationResult == CallOptimizationResult::OptimizedToJump)
1277             return Terminal;
1278         if (optimizationResult == CallOptimizationResult::Inlined) {
1279             if (UNLIKELY(m_graph.compilation()))
1280                 m_graph.compilation()-&gt;noticeInlinedCall();
1281             return NonTerminal;
1282         }
1283     }
1284 
1285     Node* callNode = addCall(result, op, nullptr, callTarget, argumentCountIncludingThis, registerOffset, prediction);
1286     ASSERT(callNode-&gt;op() != TailCallVarargs &amp;&amp; callNode-&gt;op() != TailCallForwardVarargs);
1287     return callNode-&gt;op() == TailCall ? Terminal : NonTerminal;
1288 }
1289 
1290 template&lt;typename CallOp&gt;
1291 ByteCodeParser::Terminality ByteCodeParser::handleVarargsCall(const Instruction* pc, NodeType op, CallMode callMode)
1292 {
1293     auto bytecode = pc-&gt;as&lt;CallOp&gt;();
1294     int firstFreeReg = bytecode.m_firstFree.offset();
1295     int firstVarArgOffset = bytecode.m_firstVarArg;
1296 
1297     SpeculatedType prediction = getPrediction();
1298 
1299     Node* callTarget = get(bytecode.m_callee);
1300 
1301     CallLinkStatus callLinkStatus = CallLinkStatus::computeFor(
1302         m_inlineStackTop-&gt;m_profiledBlock, currentCodeOrigin(),
1303         m_inlineStackTop-&gt;m_baselineMap, m_icContextStack);
1304     refineStatically(callLinkStatus, callTarget);
1305 
1306     VERBOSE_LOG(&quot;    Varargs call link status at &quot;, currentCodeOrigin(), &quot;: &quot;, callLinkStatus, &quot;\n&quot;);
1307 
1308     if (callLinkStatus.canOptimize()) {
1309         addToGraph(FilterCallLinkStatus, OpInfo(m_graph.m_plan.recordedStatuses().addCallLinkStatus(currentCodeOrigin(), callLinkStatus)), callTarget);
1310 
1311         if (handleVarargsInlining(callTarget, bytecode.m_dst,
1312             callLinkStatus, firstFreeReg, bytecode.m_thisValue, bytecode.m_arguments,
1313             firstVarArgOffset, op,
1314             InlineCallFrame::varargsKindFor(callMode))) {
1315             if (UNLIKELY(m_graph.compilation()))
1316                 m_graph.compilation()-&gt;noticeInlinedCall();
1317             return NonTerminal;
1318         }
1319     }
1320 
1321     CallVarargsData* data = m_graph.m_callVarargsData.add();
1322     data-&gt;firstVarArgOffset = firstVarArgOffset;
1323 
1324     Node* thisChild = get(bytecode.m_thisValue);
1325     Node* argumentsChild = nullptr;
1326     if (op != TailCallForwardVarargs)
1327         argumentsChild = get(bytecode.m_arguments);
1328 
1329     if (op == TailCallVarargs || op == TailCallForwardVarargs) {
1330         if (allInlineFramesAreTailCalls()) {
1331             addToGraph(op, OpInfo(data), OpInfo(), callTarget, thisChild, argumentsChild);
1332             return Terminal;
1333         }
1334         op = op == TailCallVarargs ? TailCallVarargsInlinedCaller : TailCallForwardVarargsInlinedCaller;
1335     }
1336 
1337     Node* call = addToGraph(op, OpInfo(data), OpInfo(prediction), callTarget, thisChild, argumentsChild);
1338     if (bytecode.m_dst.isValid())
1339         set(bytecode.m_dst, call);
1340     return NonTerminal;
1341 }
1342 
1343 void ByteCodeParser::emitFunctionChecks(CallVariant callee, Node* callTarget, VirtualRegister thisArgumentReg)
1344 {
1345     Node* thisArgument;
1346     if (thisArgumentReg.isValid())
1347         thisArgument = get(thisArgumentReg);
1348     else
1349         thisArgument = nullptr;
1350 
1351     JSCell* calleeCell;
1352     Node* callTargetForCheck;
1353     if (callee.isClosureCall()) {
1354         calleeCell = callee.executable();
1355         callTargetForCheck = addToGraph(GetExecutable, callTarget);
1356     } else {
1357         calleeCell = callee.nonExecutableCallee();
1358         callTargetForCheck = callTarget;
1359     }
1360 
1361     ASSERT(calleeCell);
1362     addToGraph(CheckCell, OpInfo(m_graph.freeze(calleeCell)), callTargetForCheck);
1363     if (thisArgument)
1364         addToGraph(Phantom, thisArgument);
1365 }
1366 
1367 Node* ByteCodeParser::getArgumentCount()
1368 {
1369     Node* argumentCount;
1370     if (m_inlineStackTop-&gt;m_inlineCallFrame &amp;&amp; !m_inlineStackTop-&gt;m_inlineCallFrame-&gt;isVarargs())
1371         argumentCount = jsConstant(m_graph.freeze(jsNumber(m_inlineStackTop-&gt;m_inlineCallFrame-&gt;argumentCountIncludingThis))-&gt;value());
1372     else
1373         argumentCount = addToGraph(GetArgumentCountIncludingThis, OpInfo(m_inlineStackTop-&gt;m_inlineCallFrame), OpInfo(SpecInt32Only));
1374     return argumentCount;
1375 }
1376 
1377 void ByteCodeParser::emitArgumentPhantoms(int registerOffset, int argumentCountIncludingThis)
1378 {
1379     for (int i = 0; i &lt; argumentCountIncludingThis; ++i)
1380         addToGraph(Phantom, get(virtualRegisterForArgument(i, registerOffset)));
1381 }
1382 
1383 template&lt;typename ChecksFunctor&gt;
1384 bool ByteCodeParser::handleRecursiveTailCall(Node* callTargetNode, CallVariant callVariant, int registerOffset, int argumentCountIncludingThis, const ChecksFunctor&amp; emitFunctionCheckIfNeeded)
1385 {
1386     if (UNLIKELY(!Options::optimizeRecursiveTailCalls()))
1387         return false;
1388 
1389     auto targetExecutable = callVariant.executable();
1390     InlineStackEntry* stackEntry = m_inlineStackTop;
1391     do {
1392         if (targetExecutable != stackEntry-&gt;executable())
1393             continue;
1394         VERBOSE_LOG(&quot;   We found a recursive tail call, trying to optimize it into a jump.\n&quot;);
1395 
1396         if (auto* callFrame = stackEntry-&gt;m_inlineCallFrame) {
1397             // Some code may statically use the argument count from the InlineCallFrame, so it would be invalid to loop back if it does not match.
1398             // We &quot;continue&quot; instead of returning false in case another stack entry further on the stack has the right number of arguments.
1399             if (argumentCountIncludingThis != static_cast&lt;int&gt;(callFrame-&gt;argumentCountIncludingThis))
1400                 continue;
1401         } else {
1402             // We are in the machine code entry (i.e. the original caller).
1403             // If we have more arguments than the number of parameters to the function, it is not clear where we could put them on the stack.
1404             if (argumentCountIncludingThis &gt; m_codeBlock-&gt;numParameters())
1405                 return false;
1406         }
1407 
1408         // If an InlineCallFrame is not a closure, it was optimized using a constant callee.
1409         // Check if this is the same callee that we try to inline here.
1410         if (stackEntry-&gt;m_inlineCallFrame &amp;&amp; !stackEntry-&gt;m_inlineCallFrame-&gt;isClosureCall) {
1411             if (stackEntry-&gt;m_inlineCallFrame-&gt;calleeConstant() != callVariant.function())
1412                 continue;
1413         }
1414 
1415         // We must add some check that the profiling information was correct and the target of this call is what we thought.
1416         emitFunctionCheckIfNeeded();
1417         // We flush everything, as if we were in the backedge of a loop (see treatment of op_jmp in parseBlock).
1418         flushForTerminal();
1419 
1420         // We must set the callee to the right value
1421         if (stackEntry-&gt;m_inlineCallFrame) {
1422             if (stackEntry-&gt;m_inlineCallFrame-&gt;isClosureCall)
1423                 setDirect(stackEntry-&gt;remapOperand(VirtualRegister(CallFrameSlot::callee)), callTargetNode, NormalSet);
1424         } else
1425             addToGraph(SetCallee, callTargetNode);
1426 
1427         // We must set the arguments to the right values
1428         if (!stackEntry-&gt;m_inlineCallFrame)
1429             addToGraph(SetArgumentCountIncludingThis, OpInfo(argumentCountIncludingThis));
1430         int argIndex = 0;
1431         for (; argIndex &lt; argumentCountIncludingThis; ++argIndex) {
1432             Node* value = get(virtualRegisterForArgument(argIndex, registerOffset));
1433             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForArgument(argIndex)), value, NormalSet);
1434         }
1435         Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
1436         for (; argIndex &lt; stackEntry-&gt;m_codeBlock-&gt;numParameters(); ++argIndex)
1437             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForArgument(argIndex)), undefined, NormalSet);
1438 
1439         // We must repeat the work of op_enter here as we will jump right after it.
1440         // We jump right after it and not before it, because of some invariant saying that a CFG root cannot have predecessors in the IR.
1441         for (int i = 0; i &lt; stackEntry-&gt;m_codeBlock-&gt;numVars(); ++i)
1442             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForLocal(i)), undefined, NormalSet);
1443 
<a name="16" id="anc16"></a>
1444         unsigned oldIndex = m_currentIndex;
1445         auto oldStackTop = m_inlineStackTop;
<a name="17" id="anc17"></a><span class="line-added">1446 </span>
<span class="line-added">1447         // First, we emit check-traps operation pointing to bc#0 as exit.</span>
1448         m_inlineStackTop = stackEntry;
<a name="18" id="anc18"></a><span class="line-added">1449         m_currentIndex = 0;</span>
<span class="line-added">1450         m_exitOK = true;</span>
<span class="line-added">1451         addToGraph(Options::usePollingTraps() ? CheckTraps : InvalidationPoint);</span>
<span class="line-added">1452 </span>
<span class="line-added">1453         // Then, we want to emit the SetLocals with an exit origin that points to the place we are jumping to.</span>
1454         m_currentIndex = opcodeLengths[op_enter];
1455         m_exitOK = true;
1456         processSetLocalQueue();
1457         m_currentIndex = oldIndex;
1458         m_inlineStackTop = oldStackTop;
1459         m_exitOK = false;
1460 
1461         BasicBlock** entryBlockPtr = tryBinarySearch&lt;BasicBlock*, unsigned&gt;(stackEntry-&gt;m_blockLinkingTargets, stackEntry-&gt;m_blockLinkingTargets.size(), opcodeLengths[op_enter], getBytecodeBeginForBlock);
1462         RELEASE_ASSERT(entryBlockPtr);
1463         addJumpTo(*entryBlockPtr);
1464         return true;
1465         // It would be unsound to jump over a non-tail call: the &quot;tail&quot; call is not really a tail call in that case.
1466     } while (stackEntry-&gt;m_inlineCallFrame &amp;&amp; stackEntry-&gt;m_inlineCallFrame-&gt;kind == InlineCallFrame::TailCall &amp;&amp; (stackEntry = stackEntry-&gt;m_caller));
1467 
1468     // The tail call was not recursive
1469     return false;
1470 }
1471 
1472 unsigned ByteCodeParser::inliningCost(CallVariant callee, int argumentCountIncludingThis, InlineCallFrame::Kind kind)
1473 {
1474     CallMode callMode = InlineCallFrame::callModeFor(kind);
1475     CodeSpecializationKind specializationKind = specializationKindFor(callMode);
1476     VERBOSE_LOG(&quot;Considering inlining &quot;, callee, &quot; into &quot;, currentCodeOrigin(), &quot;\n&quot;);
1477 
1478     if (m_hasDebuggerEnabled) {
1479         VERBOSE_LOG(&quot;    Failing because the debugger is in use.\n&quot;);
1480         return UINT_MAX;
1481     }
1482 
1483     FunctionExecutable* executable = callee.functionExecutable();
1484     if (!executable) {
1485         VERBOSE_LOG(&quot;    Failing because there is no function executable.\n&quot;);
1486         return UINT_MAX;
1487     }
1488 
1489     // Do we have a code block, and does the code block&#39;s size match the heuristics/requirements for
1490     // being an inline candidate? We might not have a code block (1) if code was thrown away,
1491     // (2) if we simply hadn&#39;t actually made this call yet or (3) code is a builtin function and
1492     // specialization kind is construct. In the former 2 cases, we could still theoretically attempt
1493     // to inline it if we had a static proof of what was being called; this might happen for example
1494     // if you call a global function, where watchpointing gives us static information. Overall,
1495     // it&#39;s a rare case because we expect that any hot callees would have already been compiled.
1496     CodeBlock* codeBlock = executable-&gt;baselineCodeBlockFor(specializationKind);
1497     if (!codeBlock) {
1498         VERBOSE_LOG(&quot;    Failing because no code block available.\n&quot;);
1499         return UINT_MAX;
1500     }
1501 
1502     if (!Options::useArityFixupInlining()) {
1503         if (codeBlock-&gt;numParameters() &gt; argumentCountIncludingThis) {
1504             VERBOSE_LOG(&quot;    Failing because of arity mismatch.\n&quot;);
1505             return UINT_MAX;
1506         }
1507     }
1508 
1509     CapabilityLevel capabilityLevel = inlineFunctionForCapabilityLevel(
1510         codeBlock, specializationKind, callee.isClosureCall());
1511     VERBOSE_LOG(&quot;    Call mode: &quot;, callMode, &quot;\n&quot;);
1512     VERBOSE_LOG(&quot;    Is closure call: &quot;, callee.isClosureCall(), &quot;\n&quot;);
1513     VERBOSE_LOG(&quot;    Capability level: &quot;, capabilityLevel, &quot;\n&quot;);
1514     VERBOSE_LOG(&quot;    Might inline function: &quot;, mightInlineFunctionFor(codeBlock, specializationKind), &quot;\n&quot;);
1515     VERBOSE_LOG(&quot;    Might compile function: &quot;, mightCompileFunctionFor(codeBlock, specializationKind), &quot;\n&quot;);
1516     VERBOSE_LOG(&quot;    Is supported for inlining: &quot;, isSupportedForInlining(codeBlock), &quot;\n&quot;);
1517     VERBOSE_LOG(&quot;    Is inlining candidate: &quot;, codeBlock-&gt;ownerExecutable()-&gt;isInliningCandidate(), &quot;\n&quot;);
1518     if (!canInline(capabilityLevel)) {
1519         VERBOSE_LOG(&quot;    Failing because the function is not inlineable.\n&quot;);
1520         return UINT_MAX;
1521     }
1522 
1523     // Check if the caller is already too large. We do this check here because that&#39;s just
1524     // where we happen to also have the callee&#39;s code block, and we want that for the
1525     // purpose of unsetting SABI.
1526     if (!isSmallEnoughToInlineCodeInto(m_codeBlock)) {
1527         codeBlock-&gt;m_shouldAlwaysBeInlined = false;
1528         VERBOSE_LOG(&quot;    Failing because the caller is too large.\n&quot;);
1529         return UINT_MAX;
1530     }
1531 
1532     // FIXME: this should be better at predicting how much bloat we will introduce by inlining
1533     // this function.
1534     // https://bugs.webkit.org/show_bug.cgi?id=127627
1535 
1536     // FIXME: We currently inline functions that have run in LLInt but not in Baseline. These
1537     // functions have very low fidelity profiling, and presumably they weren&#39;t very hot if they
1538     // haven&#39;t gotten to Baseline yet. Consider not inlining these functions.
1539     // https://bugs.webkit.org/show_bug.cgi?id=145503
1540 
1541     // Have we exceeded inline stack depth, or are we trying to inline a recursive call to
1542     // too many levels? If either of these are detected, then don&#39;t inline. We adjust our
1543     // heuristics if we are dealing with a function that cannot otherwise be compiled.
1544 
1545     unsigned depth = 0;
1546     unsigned recursion = 0;
1547 
1548     for (InlineStackEntry* entry = m_inlineStackTop; entry; entry = entry-&gt;m_caller) {
1549         ++depth;
1550         if (depth &gt;= Options::maximumInliningDepth()) {
1551             VERBOSE_LOG(&quot;    Failing because depth exceeded.\n&quot;);
1552             return UINT_MAX;
1553         }
1554 
1555         if (entry-&gt;executable() == executable) {
1556             ++recursion;
1557             if (recursion &gt;= Options::maximumInliningRecursion()) {
1558                 VERBOSE_LOG(&quot;    Failing because recursion detected.\n&quot;);
1559                 return UINT_MAX;
1560             }
1561         }
1562     }
1563 
1564     VERBOSE_LOG(&quot;    Inlining should be possible.\n&quot;);
1565 
1566     // It might be possible to inline.
<a name="19" id="anc19"></a><span class="line-modified">1567     return codeBlock-&gt;bytecodeCost();</span>
1568 }
1569 
1570 template&lt;typename ChecksFunctor&gt;
1571 void ByteCodeParser::inlineCall(Node* callTargetNode, VirtualRegister result, CallVariant callee, int registerOffset, int argumentCountIncludingThis, InlineCallFrame::Kind kind, BasicBlock* continuationBlock, const ChecksFunctor&amp; insertChecks)
1572 {
1573     const Instruction* savedCurrentInstruction = m_currentInstruction;
1574     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1575 
<a name="20" id="anc20"></a>

1576     CodeBlock* codeBlock = callee.functionExecutable()-&gt;baselineCodeBlockFor(specializationKind);
1577     insertChecks(codeBlock);
1578 
1579     // FIXME: Don&#39;t flush constants!
1580 
1581     // arityFixupCount and numberOfStackPaddingSlots are different. While arityFixupCount does not consider about stack alignment,
1582     // numberOfStackPaddingSlots consider alignment. Consider the following case,
1583     //
1584     // before: [ ... ][arg0][header]
1585     // after:  [ ... ][ext ][arg1][arg0][header]
1586     //
1587     // In the above case, arityFixupCount is 1. But numberOfStackPaddingSlots is 2 because the stack needs to be aligned.
1588     // We insert extra slots to align stack.
1589     int arityFixupCount = std::max&lt;int&gt;(codeBlock-&gt;numParameters() - argumentCountIncludingThis, 0);
1590     int numberOfStackPaddingSlots = CommonSlowPaths::numberOfStackPaddingSlots(codeBlock, argumentCountIncludingThis);
1591     ASSERT(!(numberOfStackPaddingSlots % stackAlignmentRegisters()));
1592     int registerOffsetAfterFixup = registerOffset - numberOfStackPaddingSlots;
1593 
1594     int inlineCallFrameStart = m_inlineStackTop-&gt;remapOperand(VirtualRegister(registerOffsetAfterFixup)).offset() + CallFrame::headerSizeInRegisters;
1595 
1596     ensureLocals(
1597         VirtualRegister(inlineCallFrameStart).toLocal() + 1 +
1598         CallFrame::headerSizeInRegisters + codeBlock-&gt;numCalleeLocals());
1599 
1600     size_t argumentPositionStart = m_graph.m_argumentPositions.size();
1601 
1602     if (result.isValid())
1603         result = m_inlineStackTop-&gt;remapOperand(result);
1604 
1605     VariableAccessData* calleeVariable = nullptr;
1606     if (callee.isClosureCall()) {
1607         Node* calleeSet = set(
1608             VirtualRegister(registerOffsetAfterFixup + CallFrameSlot::callee), callTargetNode, ImmediateNakedSet);
1609 
1610         calleeVariable = calleeSet-&gt;variableAccessData();
1611         calleeVariable-&gt;mergeShouldNeverUnbox(true);
1612     }
1613 
<a name="21" id="anc21"></a><span class="line-added">1614     InlineStackEntry* callerStackTop = m_inlineStackTop;</span>
<span class="line-added">1615     InlineStackEntry inlineStackEntry(this, codeBlock, codeBlock, callee.function(), result,</span>
<span class="line-added">1616         (VirtualRegister)inlineCallFrameStart, argumentCountIncludingThis, kind, continuationBlock);</span>
<span class="line-added">1617 </span>
<span class="line-added">1618     // This is where the actual inlining really happens.</span>
<span class="line-added">1619     unsigned oldIndex = m_currentIndex;</span>
<span class="line-added">1620     m_currentIndex = 0;</span>
<span class="line-added">1621 </span>
<span class="line-added">1622     switch (kind) {</span>
<span class="line-added">1623     case InlineCallFrame::GetterCall:</span>
<span class="line-added">1624     case InlineCallFrame::SetterCall: {</span>
<span class="line-added">1625         // When inlining getter and setter calls, we setup a stack frame which does not appear in the bytecode.</span>
<span class="line-added">1626         // Because Inlining can switch on executable, we could have a graph like this.</span>
<span class="line-added">1627         //</span>
<span class="line-added">1628         // BB#0</span>
<span class="line-added">1629         //     ...</span>
<span class="line-added">1630         //     30: GetSetter</span>
<span class="line-added">1631         //     31: MovHint(loc10)</span>
<span class="line-added">1632         //     32: SetLocal(loc10)</span>
<span class="line-added">1633         //     33: MovHint(loc9)</span>
<span class="line-added">1634         //     34: SetLocal(loc9)</span>
<span class="line-added">1635         //     ...</span>
<span class="line-added">1636         //     37: GetExecutable(@30)</span>
<span class="line-added">1637         //     ...</span>
<span class="line-added">1638         //     41: Switch(@37)</span>
<span class="line-added">1639         //</span>
<span class="line-added">1640         // BB#2</span>
<span class="line-added">1641         //     42: GetLocal(loc12, bc#7 of caller)</span>
<span class="line-added">1642         //     ...</span>
<span class="line-added">1643         //     --&gt; callee: loc9 and loc10 are arguments of callee.</span>
<span class="line-added">1644         //       ...</span>
<span class="line-added">1645         //       &lt;HERE, exit to callee, loc9 and loc10 are required in the bytecode&gt;</span>
<span class="line-added">1646         //</span>
<span class="line-added">1647         // When we prune OSR availability at the beginning of BB#2 (bc#7 in the caller), we prune loc9 and loc10&#39;s liveness because the caller does not actually have loc9 and loc10.</span>
<span class="line-added">1648         // However, when we begin executing the callee, we need OSR exit to be aware of where it can recover the arguments to the setter, loc9 and loc10. The MovHints in the inlined</span>
<span class="line-added">1649         // callee make it so that if we exit at &lt;HERE&gt;, we can recover loc9 and loc10.</span>
<span class="line-added">1650         for (int index = 0; index &lt; argumentCountIncludingThis; ++index) {</span>
<span class="line-added">1651             VirtualRegister argumentToGet = callerStackTop-&gt;remapOperand(virtualRegisterForArgument(index, registerOffset));</span>
<span class="line-added">1652             Node* value = getDirect(argumentToGet);</span>
<span class="line-added">1653             addToGraph(MovHint, OpInfo(argumentToGet.offset()), value);</span>
<span class="line-added">1654             m_setLocalQueue.append(DelayedSetLocal { currentCodeOrigin(), argumentToGet, value, ImmediateNakedSet });</span>
<span class="line-added">1655         }</span>
<span class="line-added">1656         break;</span>
<span class="line-added">1657     }</span>
<span class="line-added">1658     default:</span>
<span class="line-added">1659         break;</span>
<span class="line-added">1660     }</span>
<span class="line-added">1661 </span>
1662     if (arityFixupCount) {
1663         // Note: we do arity fixup in two phases:
1664         // 1. We get all the values we need and MovHint them to the expected locals.
<a name="22" id="anc22"></a><span class="line-modified">1665         // 2. We SetLocal them after that. This way, if we exit, the callee&#39;s</span>
1666         //    frame is already set up. If any SetLocal exits, we have a valid exit state.
1667         //    This is required because if we didn&#39;t do this in two phases, we may exit in
<a name="23" id="anc23"></a><span class="line-modified">1668         //    the middle of arity fixup from the callee&#39;s CodeOrigin. This is unsound because exited</span>
<span class="line-modified">1669         //    code does not have arity fixup so that remaining necessary fixups are not executed.</span>
<span class="line-modified">1670         //    For example, consider if we need to pad two args:</span>
1671         //    [arg3][arg2][arg1][arg0]
1672         //    [fix ][fix ][arg3][arg2][arg1][arg0]
1673         //    We memcpy starting from arg0 in the direction of arg3. If we were to exit at a type check
<a name="24" id="anc24"></a><span class="line-modified">1674         //    for arg3&#39;s SetLocal in the callee&#39;s CodeOrigin, we&#39;d exit with a frame like so:</span>
1675         //    [arg3][arg2][arg1][arg2][arg1][arg0]
<a name="25" id="anc25"></a><span class="line-modified">1676         //    Since we do not perform arity fixup in the callee, this is the frame used by the callee.</span>
<span class="line-modified">1677         //    And the callee would then just end up thinking its argument are:</span>
<span class="line-added">1678         //    [fix ][fix ][arg3][arg2][arg1][arg0]</span>
1679         //    which is incorrect.
1680 
1681         Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
1682         // The stack needs to be aligned due to the JS calling convention. Thus, we have a hole if the count of arguments is not aligned.
1683         // We call this hole &quot;extra slot&quot;. Consider the following case, the number of arguments is 2. If this argument
1684         // count does not fulfill the stack alignment requirement, we already inserted extra slots.
1685         //
1686         // before: [ ... ][ext ][arg1][arg0][header]
1687         //
1688         // In the above case, one extra slot is inserted. If the code&#39;s parameter count is 3, we will fixup arguments.
1689         // At that time, we can simply use this extra slots. So the fixuped stack is the following.
1690         //
1691         // before: [ ... ][ext ][arg1][arg0][header]
1692         // after:  [ ... ][arg2][arg1][arg0][header]
1693         //
1694         // In such cases, we do not need to move frames.
1695         if (registerOffsetAfterFixup != registerOffset) {
1696             for (int index = 0; index &lt; argumentCountIncludingThis; ++index) {
<a name="26" id="anc26"></a><span class="line-modified">1697                 VirtualRegister argumentToGet = callerStackTop-&gt;remapOperand(virtualRegisterForArgument(index, registerOffset));</span>
<span class="line-modified">1698                 Node* value = getDirect(argumentToGet);</span>
<span class="line-added">1699                 VirtualRegister argumentToSet = m_inlineStackTop-&gt;remapOperand(virtualRegisterForArgument(index));</span>
1700                 addToGraph(MovHint, OpInfo(argumentToSet.offset()), value);
1701                 m_setLocalQueue.append(DelayedSetLocal { currentCodeOrigin(), argumentToSet, value, ImmediateNakedSet });
1702             }
1703         }
1704         for (int index = 0; index &lt; arityFixupCount; ++index) {
<a name="27" id="anc27"></a><span class="line-modified">1705             VirtualRegister argumentToSet = m_inlineStackTop-&gt;remapOperand(virtualRegisterForArgument(argumentCountIncludingThis + index));</span>
1706             addToGraph(MovHint, OpInfo(argumentToSet.offset()), undefined);
1707             m_setLocalQueue.append(DelayedSetLocal { currentCodeOrigin(), argumentToSet, undefined, ImmediateNakedSet });
1708         }
1709 
1710         // At this point, it&#39;s OK to OSR exit because we finished setting up
<a name="28" id="anc28"></a><span class="line-modified">1711         // our callee&#39;s frame. We emit an ExitOK below.</span>
1712     }
1713 
<a name="29" id="anc29"></a>






1714     // At this point, it&#39;s again OK to OSR exit.
1715     m_exitOK = true;
1716     addToGraph(ExitOK);
1717 
1718     processSetLocalQueue();
1719 
1720     InlineVariableData inlineVariableData;
1721     inlineVariableData.inlineCallFrame = m_inlineStackTop-&gt;m_inlineCallFrame;
1722     inlineVariableData.argumentPositionStart = argumentPositionStart;
1723     inlineVariableData.calleeVariable = 0;
1724 
1725     RELEASE_ASSERT(
1726         m_inlineStackTop-&gt;m_inlineCallFrame-&gt;isClosureCall
1727         == callee.isClosureCall());
1728     if (callee.isClosureCall()) {
1729         RELEASE_ASSERT(calleeVariable);
1730         inlineVariableData.calleeVariable = calleeVariable;
1731     }
1732 
1733     m_graph.m_inlineVariableData.append(inlineVariableData);
1734 
1735     parseCodeBlock();
1736     clearCaches(); // Reset our state now that we&#39;re back to the outer code.
1737 
1738     m_currentIndex = oldIndex;
1739     m_exitOK = false;
1740 
1741     linkBlocks(inlineStackEntry.m_unlinkedBlocks, inlineStackEntry.m_blockLinkingTargets);
1742 
1743     // Most functions have at least one op_ret and thus set up the continuation block.
1744     // In some rare cases, a function ends in op_unreachable, forcing us to allocate a new continuationBlock here.
1745     if (inlineStackEntry.m_continuationBlock)
1746         m_currentBlock = inlineStackEntry.m_continuationBlock;
1747     else
1748         m_currentBlock = allocateUntargetableBlock();
1749     ASSERT(!m_currentBlock-&gt;terminal());
1750 
1751     prepareToParseBlock();
1752     m_currentInstruction = savedCurrentInstruction;
1753 }
1754 
1755 ByteCodeParser::CallOptimizationResult ByteCodeParser::handleCallVariant(Node* callTargetNode, VirtualRegister result, CallVariant callee, int registerOffset, VirtualRegister thisArgument, int argumentCountIncludingThis, unsigned nextOffset, InlineCallFrame::Kind kind, SpeculatedType prediction, unsigned&amp; inliningBalance, BasicBlock* continuationBlock, bool needsToCheckCallee)
1756 {
1757     VERBOSE_LOG(&quot;    Considering callee &quot;, callee, &quot;\n&quot;);
1758 
1759     bool didInsertChecks = false;
1760     auto insertChecksWithAccounting = [&amp;] () {
1761         if (needsToCheckCallee)
1762             emitFunctionChecks(callee, callTargetNode, thisArgument);
1763         didInsertChecks = true;
1764     };
1765 
1766     if (kind == InlineCallFrame::TailCall &amp;&amp; ByteCodeParser::handleRecursiveTailCall(callTargetNode, callee, registerOffset, argumentCountIncludingThis, insertChecksWithAccounting)) {
1767         RELEASE_ASSERT(didInsertChecks);
1768         return CallOptimizationResult::OptimizedToJump;
1769     }
1770     RELEASE_ASSERT(!didInsertChecks);
1771 
1772     if (!inliningBalance)
1773         return CallOptimizationResult::DidNothing;
1774 
1775     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1776 
1777     auto endSpecialCase = [&amp;] () {
1778         RELEASE_ASSERT(didInsertChecks);
1779         addToGraph(Phantom, callTargetNode);
1780         emitArgumentPhantoms(registerOffset, argumentCountIncludingThis);
1781         inliningBalance--;
1782         if (continuationBlock) {
1783             m_currentIndex = nextOffset;
1784             m_exitOK = true;
1785             processSetLocalQueue();
1786             addJumpTo(continuationBlock);
1787         }
1788     };
1789 
1790     if (InternalFunction* function = callee.internalFunction()) {
1791         if (handleConstantInternalFunction(callTargetNode, result, function, registerOffset, argumentCountIncludingThis, specializationKind, prediction, insertChecksWithAccounting)) {
1792             endSpecialCase();
1793             return CallOptimizationResult::Inlined;
1794         }
1795         RELEASE_ASSERT(!didInsertChecks);
1796         return CallOptimizationResult::DidNothing;
1797     }
1798 
1799     Intrinsic intrinsic = callee.intrinsicFor(specializationKind);
1800     if (intrinsic != NoIntrinsic) {
1801         if (handleIntrinsicCall(callTargetNode, result, intrinsic, registerOffset, argumentCountIncludingThis, prediction, insertChecksWithAccounting)) {
1802             endSpecialCase();
1803             return CallOptimizationResult::Inlined;
1804         }
1805         RELEASE_ASSERT(!didInsertChecks);
1806         // We might still try to inline the Intrinsic because it might be a builtin JS function.
1807     }
1808 
1809     if (Options::useDOMJIT()) {
1810         if (const DOMJIT::Signature* signature = callee.signatureFor(specializationKind)) {
1811             if (handleDOMJITCall(callTargetNode, result, signature, registerOffset, argumentCountIncludingThis, prediction, insertChecksWithAccounting)) {
1812                 endSpecialCase();
1813                 return CallOptimizationResult::Inlined;
1814             }
1815             RELEASE_ASSERT(!didInsertChecks);
1816         }
1817     }
1818 
1819     unsigned myInliningCost = inliningCost(callee, argumentCountIncludingThis, kind);
1820     if (myInliningCost &gt; inliningBalance)
1821         return CallOptimizationResult::DidNothing;
1822 
1823     auto insertCheck = [&amp;] (CodeBlock*) {
1824         if (needsToCheckCallee)
1825             emitFunctionChecks(callee, callTargetNode, thisArgument);
1826     };
1827     inlineCall(callTargetNode, result, callee, registerOffset, argumentCountIncludingThis, kind, continuationBlock, insertCheck);
1828     inliningBalance -= myInliningCost;
1829     return CallOptimizationResult::Inlined;
1830 }
1831 
1832 bool ByteCodeParser::handleVarargsInlining(Node* callTargetNode, VirtualRegister result,
1833     const CallLinkStatus&amp; callLinkStatus, int firstFreeReg, VirtualRegister thisArgument,
1834     VirtualRegister argumentsArgument, unsigned argumentsOffset,
1835     NodeType callOp, InlineCallFrame::Kind kind)
1836 {
1837     VERBOSE_LOG(&quot;Handling inlining (Varargs)...\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1838     if (callLinkStatus.maxNumArguments() &gt; Options::maximumVarargsForInlining()) {
1839         VERBOSE_LOG(&quot;Bailing inlining: too many arguments for varargs inlining.\n&quot;);
1840         return false;
1841     }
1842     if (callLinkStatus.couldTakeSlowPath() || callLinkStatus.size() != 1) {
1843         VERBOSE_LOG(&quot;Bailing inlining: polymorphic inlining is not yet supported for varargs.\n&quot;);
1844         return false;
1845     }
1846 
1847     CallVariant callVariant = callLinkStatus[0];
1848 
1849     unsigned mandatoryMinimum;
1850     if (FunctionExecutable* functionExecutable = callVariant.functionExecutable())
1851         mandatoryMinimum = functionExecutable-&gt;parameterCount();
1852     else
1853         mandatoryMinimum = 0;
1854 
1855     // includes &quot;this&quot;
1856     unsigned maxNumArguments = std::max(callLinkStatus.maxNumArguments(), mandatoryMinimum + 1);
1857 
1858     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1859     if (inliningCost(callVariant, maxNumArguments, kind) &gt; getInliningBalance(callLinkStatus, specializationKind)) {
1860         VERBOSE_LOG(&quot;Bailing inlining: inlining cost too high.\n&quot;);
1861         return false;
1862     }
1863 
1864     int registerOffset = firstFreeReg + 1;
1865     registerOffset -= maxNumArguments; // includes &quot;this&quot;
1866     registerOffset -= CallFrame::headerSizeInRegisters;
1867     registerOffset = -WTF::roundUpToMultipleOf(stackAlignmentRegisters(), -registerOffset);
1868 
1869     auto insertChecks = [&amp;] (CodeBlock* codeBlock) {
1870         emitFunctionChecks(callVariant, callTargetNode, thisArgument);
1871 
1872         int remappedRegisterOffset =
1873         m_inlineStackTop-&gt;remapOperand(VirtualRegister(registerOffset)).offset();
1874 
1875         ensureLocals(VirtualRegister(remappedRegisterOffset).toLocal());
1876 
1877         int argumentStart = registerOffset + CallFrame::headerSizeInRegisters;
<a name="30" id="anc30"></a><span class="line-modified">1878         int remappedArgumentStart = m_inlineStackTop-&gt;remapOperand(VirtualRegister(argumentStart)).offset();</span>

1879 
1880         LoadVarargsData* data = m_graph.m_loadVarargsData.add();
1881         data-&gt;start = VirtualRegister(remappedArgumentStart + 1);
1882         data-&gt;count = VirtualRegister(remappedRegisterOffset + CallFrameSlot::argumentCount);
1883         data-&gt;offset = argumentsOffset;
1884         data-&gt;limit = maxNumArguments;
1885         data-&gt;mandatoryMinimum = mandatoryMinimum;
1886 
1887         if (callOp == TailCallForwardVarargs)
1888             addToGraph(ForwardVarargs, OpInfo(data));
1889         else
1890             addToGraph(LoadVarargs, OpInfo(data), get(argumentsArgument));
1891 
1892         // LoadVarargs may OSR exit. Hence, we need to keep alive callTargetNode, thisArgument
1893         // and argumentsArgument for the baseline JIT. However, we only need a Phantom for
1894         // callTargetNode because the other 2 are still in use and alive at this point.
1895         addToGraph(Phantom, callTargetNode);
1896 
1897         // In DFG IR before SSA, we cannot insert control flow between after the
<a name="31" id="anc31"></a><span class="line-modified">1898         // LoadVarargs and the last SetArgumentDefinitely. This isn&#39;t a problem once we get to DFG</span>
1899         // SSA. Fortunately, we also have other reasons for not inserting control flow
1900         // before SSA.
1901 
1902         VariableAccessData* countVariable = newVariableAccessData(VirtualRegister(remappedRegisterOffset + CallFrameSlot::argumentCount));
1903         // This is pretty lame, but it will force the count to be flushed as an int. This doesn&#39;t
<a name="32" id="anc32"></a><span class="line-modified">1904         // matter very much, since our use of a SetArgumentDefinitely and Flushes for this local slot is</span>
1905         // mostly just a formality.
1906         countVariable-&gt;predict(SpecInt32Only);
1907         countVariable-&gt;mergeIsProfitableToUnbox(true);
<a name="33" id="anc33"></a><span class="line-modified">1908         Node* setArgumentCount = addToGraph(SetArgumentDefinitely, OpInfo(countVariable));</span>
1909         m_currentBlock-&gt;variablesAtTail.setOperand(countVariable-&gt;local(), setArgumentCount);
1910 
1911         set(VirtualRegister(argumentStart), get(thisArgument), ImmediateNakedSet);
<a name="34" id="anc34"></a><span class="line-added">1912         unsigned numSetArguments = 0;</span>
1913         for (unsigned argument = 1; argument &lt; maxNumArguments; ++argument) {
1914             VariableAccessData* variable = newVariableAccessData(VirtualRegister(remappedArgumentStart + argument));
1915             variable-&gt;mergeShouldNeverUnbox(true); // We currently have nowhere to put the type check on the LoadVarargs. LoadVarargs is effectful, so after it finishes, we cannot exit.
1916 
1917             // For a while it had been my intention to do things like this inside the
1918             // prediction injection phase. But in this case it&#39;s really best to do it here,
1919             // because it&#39;s here that we have access to the variable access datas for the
1920             // inlining we&#39;re about to do.
1921             //
1922             // Something else that&#39;s interesting here is that we&#39;d really love to get
1923             // predictions from the arguments loaded at the callsite, rather than the
1924             // arguments received inside the callee. But that probably won&#39;t matter for most
1925             // calls.
1926             if (codeBlock &amp;&amp; argument &lt; static_cast&lt;unsigned&gt;(codeBlock-&gt;numParameters())) {
1927                 ConcurrentJSLocker locker(codeBlock-&gt;m_lock);
1928                 ValueProfile&amp; profile = codeBlock-&gt;valueProfileForArgument(argument);
1929                 variable-&gt;predict(profile.computeUpdatedPrediction(locker));
1930             }
1931 
<a name="35" id="anc35"></a><span class="line-modified">1932             Node* setArgument = addToGraph(numSetArguments &gt;= mandatoryMinimum ? SetArgumentMaybe : SetArgumentDefinitely, OpInfo(variable));</span>
1933             m_currentBlock-&gt;variablesAtTail.setOperand(variable-&gt;local(), setArgument);
<a name="36" id="anc36"></a><span class="line-added">1934             ++numSetArguments;</span>
1935         }
1936     };
1937 
1938     // Intrinsics and internal functions can only be inlined if we&#39;re not doing varargs. This is because
1939     // we currently don&#39;t have any way of getting profiling information for arguments to non-JS varargs
1940     // calls. The prediction propagator won&#39;t be of any help because LoadVarargs obscures the data flow,
1941     // and there are no callsite value profiles and native function won&#39;t have callee value profiles for
1942     // those arguments. Even worse, if the intrinsic decides to exit, it won&#39;t really have anywhere to
1943     // exit to: LoadVarargs is effectful and it&#39;s part of the op_call_varargs, so we can&#39;t exit without
1944     // calling LoadVarargs twice.
1945     inlineCall(callTargetNode, result, callVariant, registerOffset, maxNumArguments, kind, nullptr, insertChecks);
1946 
<a name="37" id="anc37"></a><span class="line-added">1947 </span>
1948     VERBOSE_LOG(&quot;Successful inlining (varargs, monomorphic).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1949     return true;
1950 }
1951 
1952 unsigned ByteCodeParser::getInliningBalance(const CallLinkStatus&amp; callLinkStatus, CodeSpecializationKind specializationKind)
1953 {
<a name="38" id="anc38"></a><span class="line-modified">1954     unsigned inliningBalance = Options::maximumFunctionForCallInlineCandidateBytecodeCost();</span>
1955     if (specializationKind == CodeForConstruct)
<a name="39" id="anc39"></a><span class="line-modified">1956         inliningBalance = std::min(inliningBalance, Options::maximumFunctionForConstructInlineCandidateBytecoodeCost());</span>
1957     if (callLinkStatus.isClosureCall())
<a name="40" id="anc40"></a><span class="line-modified">1958         inliningBalance = std::min(inliningBalance, Options::maximumFunctionForClosureCallInlineCandidateBytecodeCost());</span>
1959     return inliningBalance;
1960 }
1961 
1962 ByteCodeParser::CallOptimizationResult ByteCodeParser::handleInlining(
1963     Node* callTargetNode, VirtualRegister result, const CallLinkStatus&amp; callLinkStatus,
1964     int registerOffset, VirtualRegister thisArgument,
1965     int argumentCountIncludingThis,
1966     unsigned nextOffset, NodeType callOp, InlineCallFrame::Kind kind, SpeculatedType prediction)
1967 {
1968     VERBOSE_LOG(&quot;Handling inlining...\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1969 
1970     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1971     unsigned inliningBalance = getInliningBalance(callLinkStatus, specializationKind);
1972 
1973     // First check if we can avoid creating control flow. Our inliner does some CFG
1974     // simplification on the fly and this helps reduce compile times, but we can only leverage
1975     // this in cases where we don&#39;t need control flow diamonds to check the callee.
1976     if (!callLinkStatus.couldTakeSlowPath() &amp;&amp; callLinkStatus.size() == 1) {
1977         return handleCallVariant(
1978             callTargetNode, result, callLinkStatus[0], registerOffset, thisArgument,
1979             argumentCountIncludingThis, nextOffset, kind, prediction, inliningBalance, nullptr, true);
1980     }
1981 
1982     // We need to create some kind of switch over callee. For now we only do this if we believe that
1983     // we&#39;re in the top tier. We have two reasons for this: first, it provides us an opportunity to
1984     // do more detailed polyvariant/polymorphic profiling; and second, it reduces compile times in
1985     // the DFG. And by polyvariant profiling we mean polyvariant profiling of *this* call. Note that
1986     // we could improve that aspect of this by doing polymorphic inlining but having the profiling
1987     // also.
1988     if (!m_graph.m_plan.isFTL() || !Options::usePolymorphicCallInlining()) {
1989         VERBOSE_LOG(&quot;Bailing inlining (hard).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1990         return CallOptimizationResult::DidNothing;
1991     }
1992 
1993     // If the claim is that this did not originate from a stub, then we don&#39;t want to emit a switch
1994     // statement. Whenever the non-stub profiling says that it could take slow path, it really means that
1995     // it has no idea.
1996     if (!Options::usePolymorphicCallInliningForNonStubStatus()
1997         &amp;&amp; !callLinkStatus.isBasedOnStub()) {
1998         VERBOSE_LOG(&quot;Bailing inlining (non-stub polymorphism).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1999         return CallOptimizationResult::DidNothing;
2000     }
2001 
2002     bool allAreClosureCalls = true;
2003     bool allAreDirectCalls = true;
2004     for (unsigned i = callLinkStatus.size(); i--;) {
2005         if (callLinkStatus[i].isClosureCall())
2006             allAreDirectCalls = false;
2007         else
2008             allAreClosureCalls = false;
2009     }
2010 
2011     Node* thingToSwitchOn;
2012     if (allAreDirectCalls)
2013         thingToSwitchOn = callTargetNode;
2014     else if (allAreClosureCalls)
2015         thingToSwitchOn = addToGraph(GetExecutable, callTargetNode);
2016     else {
2017         // FIXME: We should be able to handle this case, but it&#39;s tricky and we don&#39;t know of cases
2018         // where it would be beneficial. It might be best to handle these cases as if all calls were
2019         // closure calls.
2020         // https://bugs.webkit.org/show_bug.cgi?id=136020
2021         VERBOSE_LOG(&quot;Bailing inlining (mix).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
2022         return CallOptimizationResult::DidNothing;
2023     }
2024 
2025     VERBOSE_LOG(&quot;Doing hard inlining...\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
2026 
2027     // This makes me wish that we were in SSA all the time. We need to pick a variable into which to
2028     // store the callee so that it will be accessible to all of the blocks we&#39;re about to create. We
2029     // get away with doing an immediate-set here because we wouldn&#39;t have performed any side effects
2030     // yet.
2031     VERBOSE_LOG(&quot;Register offset: &quot;, registerOffset);
2032     VirtualRegister calleeReg(registerOffset + CallFrameSlot::callee);
2033     calleeReg = m_inlineStackTop-&gt;remapOperand(calleeReg);
2034     VERBOSE_LOG(&quot;Callee is going to be &quot;, calleeReg, &quot;\n&quot;);
2035     setDirect(calleeReg, callTargetNode, ImmediateSetWithFlush);
2036 
2037     // It&#39;s OK to exit right now, even though we set some locals. That&#39;s because those locals are not
2038     // user-visible.
2039     m_exitOK = true;
2040     addToGraph(ExitOK);
2041 
2042     SwitchData&amp; data = *m_graph.m_switchData.add();
2043     data.kind = SwitchCell;
2044     addToGraph(Switch, OpInfo(&amp;data), thingToSwitchOn);
2045     m_currentBlock-&gt;didLink();
2046 
2047     BasicBlock* continuationBlock = allocateUntargetableBlock();
2048     VERBOSE_LOG(&quot;Adding untargetable block &quot;, RawPointer(continuationBlock), &quot; (continuation)\n&quot;);
2049 
2050     // We may force this true if we give up on inlining any of the edges.
2051     bool couldTakeSlowPath = callLinkStatus.couldTakeSlowPath();
2052 
2053     VERBOSE_LOG(&quot;About to loop over functions at &quot;, currentCodeOrigin(), &quot;.\n&quot;);
2054 
2055     unsigned oldOffset = m_currentIndex;
2056     for (unsigned i = 0; i &lt; callLinkStatus.size(); ++i) {
2057         m_currentIndex = oldOffset;
2058         BasicBlock* calleeEntryBlock = allocateUntargetableBlock();
2059         m_currentBlock = calleeEntryBlock;
2060         prepareToParseBlock();
2061 
2062         // At the top of each switch case, we can exit.
2063         m_exitOK = true;
2064 
2065         Node* myCallTargetNode = getDirect(calleeReg);
2066 
2067         auto inliningResult = handleCallVariant(
2068             myCallTargetNode, result, callLinkStatus[i], registerOffset,
2069             thisArgument, argumentCountIncludingThis, nextOffset, kind, prediction,
2070             inliningBalance, continuationBlock, false);
2071 
2072         if (inliningResult == CallOptimizationResult::DidNothing) {
2073             // That failed so we let the block die. Nothing interesting should have been added to
2074             // the block. We also give up on inlining any of the (less frequent) callees.
2075             ASSERT(m_graph.m_blocks.last() == m_currentBlock);
2076             m_graph.killBlockAndItsContents(m_currentBlock);
2077             m_graph.m_blocks.removeLast();
2078             VERBOSE_LOG(&quot;Inlining of a poly call failed, we will have to go through a slow path\n&quot;);
2079 
2080             // The fact that inlining failed means we need a slow path.
2081             couldTakeSlowPath = true;
2082             break;
2083         }
2084 
2085         JSCell* thingToCaseOn;
2086         if (allAreDirectCalls)
2087             thingToCaseOn = callLinkStatus[i].nonExecutableCallee();
2088         else {
2089             ASSERT(allAreClosureCalls);
2090             thingToCaseOn = callLinkStatus[i].executable();
2091         }
2092         data.cases.append(SwitchCase(m_graph.freeze(thingToCaseOn), calleeEntryBlock));
2093         VERBOSE_LOG(&quot;Finished optimizing &quot;, callLinkStatus[i], &quot; at &quot;, currentCodeOrigin(), &quot;.\n&quot;);
2094     }
2095 
2096     // Slow path block
2097     m_currentBlock = allocateUntargetableBlock();
2098     m_currentIndex = oldOffset;
2099     m_exitOK = true;
2100     data.fallThrough = BranchTarget(m_currentBlock);
2101     prepareToParseBlock();
2102     Node* myCallTargetNode = getDirect(calleeReg);
2103     if (couldTakeSlowPath) {
2104         addCall(
2105             result, callOp, nullptr, myCallTargetNode, argumentCountIncludingThis,
2106             registerOffset, prediction);
2107         VERBOSE_LOG(&quot;We added a call in the slow path\n&quot;);
2108     } else {
2109         addToGraph(CheckBadCell);
2110         addToGraph(Phantom, myCallTargetNode);
2111         emitArgumentPhantoms(registerOffset, argumentCountIncludingThis);
2112 
<a name="41" id="anc41"></a><span class="line-modified">2113         if (result.isValid())</span>
<span class="line-added">2114             set(result, addToGraph(BottomValue));</span>
2115         VERBOSE_LOG(&quot;couldTakeSlowPath was false\n&quot;);
2116     }
2117 
2118     m_currentIndex = nextOffset;
2119     m_exitOK = true; // Origin changed, so it&#39;s fine to exit again.
2120     processSetLocalQueue();
2121 
2122     if (Node* terminal = m_currentBlock-&gt;terminal())
2123         ASSERT_UNUSED(terminal, terminal-&gt;op() == TailCall || terminal-&gt;op() == TailCallVarargs || terminal-&gt;op() == TailCallForwardVarargs);
2124     else {
2125         addJumpTo(continuationBlock);
2126     }
2127 
2128     prepareToParseBlock();
2129 
2130     m_currentIndex = oldOffset;
2131     m_currentBlock = continuationBlock;
2132     m_exitOK = true;
2133 
2134     VERBOSE_LOG(&quot;Done inlining (hard).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
2135     return CallOptimizationResult::Inlined;
2136 }
2137 
2138 template&lt;typename ChecksFunctor&gt;
2139 bool ByteCodeParser::handleMinMax(VirtualRegister result, NodeType op, int registerOffset, int argumentCountIncludingThis, const ChecksFunctor&amp; insertChecks)
2140 {
2141     ASSERT(op == ArithMin || op == ArithMax);
2142 
2143     if (argumentCountIncludingThis == 1) {
2144         insertChecks();
2145         double limit = op == ArithMax ? -std::numeric_limits&lt;double&gt;::infinity() : +std::numeric_limits&lt;double&gt;::infinity();
2146         set(result, addToGraph(JSConstant, OpInfo(m_graph.freeze(jsDoubleNumber(limit)))));
2147         return true;
2148     }
2149 
2150     if (argumentCountIncludingThis == 2) {
2151         insertChecks();
2152         Node* resultNode = get(VirtualRegister(virtualRegisterForArgument(1, registerOffset)));
2153         addToGraph(Phantom, Edge(resultNode, NumberUse));
2154         set(result, resultNode);
2155         return true;
2156     }
2157 
2158     if (argumentCountIncludingThis == 3) {
2159         insertChecks();
2160         set(result, addToGraph(op, get(virtualRegisterForArgument(1, registerOffset)), get(virtualRegisterForArgument(2, registerOffset))));
2161         return true;
2162     }
2163 
2164     // Don&#39;t handle &gt;=3 arguments for now.
2165     return false;
2166 }
2167 
2168 template&lt;typename ChecksFunctor&gt;
2169 bool ByteCodeParser::handleIntrinsicCall(Node* callee, VirtualRegister result, Intrinsic intrinsic, int registerOffset, int argumentCountIncludingThis, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks)
2170 {
2171     VERBOSE_LOG(&quot;       The intrinsic is &quot;, intrinsic, &quot;\n&quot;);
2172 
2173     if (!isOpcodeShape&lt;OpCallShape&gt;(m_currentInstruction))
2174         return false;
2175 
2176     // It so happens that the code below doesn&#39;t handle the invalid result case. We could fix that, but
2177     // it would only benefit intrinsics called as setters, like if you do:
2178     //
2179     //     o.__defineSetter__(&quot;foo&quot;, Math.pow)
2180     //
2181     // Which is extremely amusing, but probably not worth optimizing.
2182     if (!result.isValid())
2183         return false;
2184 
2185     bool didSetResult = false;
2186     auto setResult = [&amp;] (Node* node) {
2187         RELEASE_ASSERT(!didSetResult);
2188         set(result, node);
2189         didSetResult = true;
2190     };
2191 
2192     auto inlineIntrinsic = [&amp;] {
2193         switch (intrinsic) {
2194 
2195         // Intrinsic Functions:
2196 
2197         case AbsIntrinsic: {
2198             if (argumentCountIncludingThis == 1) { // Math.abs()
2199                 insertChecks();
2200                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
2201                 return true;
2202             }
2203 
2204             if (!MacroAssembler::supportsFloatingPointAbs())
2205                 return false;
2206 
2207             insertChecks();
2208             Node* node = addToGraph(ArithAbs, get(virtualRegisterForArgument(1, registerOffset)));
2209             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
2210                 node-&gt;mergeFlags(NodeMayOverflowInt32InDFG);
2211             setResult(node);
2212             return true;
2213         }
2214 
2215         case MinIntrinsic:
2216         case MaxIntrinsic:
2217             if (handleMinMax(result, intrinsic == MinIntrinsic ? ArithMin : ArithMax, registerOffset, argumentCountIncludingThis, insertChecks)) {
2218                 didSetResult = true;
2219                 return true;
2220             }
2221             return false;
2222 
2223 #define DFG_ARITH_UNARY(capitalizedName, lowerName) \
2224         case capitalizedName##Intrinsic:
2225         FOR_EACH_DFG_ARITH_UNARY_OP(DFG_ARITH_UNARY)
2226 #undef DFG_ARITH_UNARY
2227         {
2228             if (argumentCountIncludingThis == 1) {
2229                 insertChecks();
2230                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
2231                 return true;
2232             }
2233             Arith::UnaryType type = Arith::UnaryType::Sin;
2234             switch (intrinsic) {
2235 #define DFG_ARITH_UNARY(capitalizedName, lowerName) \
2236             case capitalizedName##Intrinsic: \
2237                 type = Arith::UnaryType::capitalizedName; \
2238                 break;
2239         FOR_EACH_DFG_ARITH_UNARY_OP(DFG_ARITH_UNARY)
2240 #undef DFG_ARITH_UNARY
2241             default:
2242                 RELEASE_ASSERT_NOT_REACHED();
2243             }
2244             insertChecks();
2245             setResult(addToGraph(ArithUnary, OpInfo(static_cast&lt;std::underlying_type&lt;Arith::UnaryType&gt;::type&gt;(type)), get(virtualRegisterForArgument(1, registerOffset))));
2246             return true;
2247         }
2248 
2249         case FRoundIntrinsic:
2250         case SqrtIntrinsic: {
2251             if (argumentCountIncludingThis == 1) {
2252                 insertChecks();
2253                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
2254                 return true;
2255             }
2256 
2257             NodeType nodeType = Unreachable;
2258             switch (intrinsic) {
2259             case FRoundIntrinsic:
2260                 nodeType = ArithFRound;
2261                 break;
2262             case SqrtIntrinsic:
2263                 nodeType = ArithSqrt;
2264                 break;
2265             default:
2266                 RELEASE_ASSERT_NOT_REACHED();
2267             }
2268             insertChecks();
2269             setResult(addToGraph(nodeType, get(virtualRegisterForArgument(1, registerOffset))));
2270             return true;
2271         }
2272 
2273         case PowIntrinsic: {
2274             if (argumentCountIncludingThis &lt; 3) {
2275                 // Math.pow() and Math.pow(x) return NaN.
2276                 insertChecks();
2277                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
2278                 return true;
2279             }
2280             insertChecks();
2281             VirtualRegister xOperand = virtualRegisterForArgument(1, registerOffset);
2282             VirtualRegister yOperand = virtualRegisterForArgument(2, registerOffset);
2283             setResult(addToGraph(ArithPow, get(xOperand), get(yOperand)));
2284             return true;
2285         }
2286 
2287         case ArrayPushIntrinsic: {
2288 #if USE(JSVALUE32_64)
2289             if (isX86()) {
2290                 if (argumentCountIncludingThis &gt; 2)
2291                     return false;
2292             }
2293 #endif
2294 
2295             if (static_cast&lt;unsigned&gt;(argumentCountIncludingThis) &gt;= MIN_SPARSE_ARRAY_INDEX)
2296                 return false;
2297 
2298             ArrayMode arrayMode = getArrayMode(Array::Write);
2299             if (!arrayMode.isJSArray())
2300                 return false;
2301             switch (arrayMode.type()) {
2302             case Array::Int32:
2303             case Array::Double:
2304             case Array::Contiguous:
2305             case Array::ArrayStorage: {
2306                 insertChecks();
2307 
2308                 addVarArgChild(nullptr); // For storage.
2309                 for (int i = 0; i &lt; argumentCountIncludingThis; ++i)
2310                     addVarArgChild(get(virtualRegisterForArgument(i, registerOffset)));
2311                 Node* arrayPush = addToGraph(Node::VarArg, ArrayPush, OpInfo(arrayMode.asWord()), OpInfo(prediction));
2312                 setResult(arrayPush);
2313                 return true;
2314             }
2315 
2316             default:
2317                 return false;
2318             }
2319         }
2320 
2321         case ArraySliceIntrinsic: {
2322 #if USE(JSVALUE32_64)
2323             if (isX86()) {
2324                 // There aren&#39;t enough registers for this to be done easily.
2325                 return false;
2326             }
2327 #endif
2328             if (argumentCountIncludingThis &lt; 1)
2329                 return false;
2330 
2331             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadConstantCache)
2332                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache))
2333                 return false;
2334 
2335             ArrayMode arrayMode = getArrayMode(Array::Read);
2336             if (!arrayMode.isJSArray())
2337                 return false;
2338 
2339             if (!arrayMode.isJSArrayWithOriginalStructure())
2340                 return false;
2341 
2342             switch (arrayMode.type()) {
2343             case Array::Double:
2344             case Array::Int32:
2345             case Array::Contiguous: {
2346                 JSGlobalObject* globalObject = m_graph.globalObjectFor(currentNodeOrigin().semantic);
2347 
2348                 Structure* arrayPrototypeStructure = globalObject-&gt;arrayPrototype()-&gt;structure(*m_vm);
2349                 Structure* objectPrototypeStructure = globalObject-&gt;objectPrototype()-&gt;structure(*m_vm);
2350 
2351                 // FIXME: We could easily relax the Array/Object.prototype transition as long as we OSR exitted if we saw a hole.
2352                 // https://bugs.webkit.org/show_bug.cgi?id=173171
<a name="42" id="anc42"></a><span class="line-modified">2353                 if (globalObject-&gt;arraySpeciesWatchpointSet().state() == IsWatched</span>
2354                     &amp;&amp; globalObject-&gt;havingABadTimeWatchpoint()-&gt;isStillValid()
2355                     &amp;&amp; arrayPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2356                     &amp;&amp; objectPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2357                     &amp;&amp; globalObject-&gt;arrayPrototypeChainIsSane()) {
2358 
<a name="43" id="anc43"></a><span class="line-modified">2359                     m_graph.watchpoints().addLazily(globalObject-&gt;arraySpeciesWatchpointSet());</span>
2360                     m_graph.watchpoints().addLazily(globalObject-&gt;havingABadTimeWatchpoint());
2361                     m_graph.registerAndWatchStructureTransition(arrayPrototypeStructure);
2362                     m_graph.registerAndWatchStructureTransition(objectPrototypeStructure);
2363 
2364                     insertChecks();
2365 
2366                     Node* array = get(virtualRegisterForArgument(0, registerOffset));
2367                     // We do a few things here to prove that we aren&#39;t skipping doing side-effects in an observable way:
2368                     // 1. We ensure that the &quot;constructor&quot; property hasn&#39;t been changed (because the observable
2369                     // effects of slice require that we perform a Get(array, &quot;constructor&quot;) and we can skip
2370                     // that if we&#39;re an original array structure. (We can relax this in the future by using
2371                     // TryGetById and CheckCell).
2372                     //
2373                     // 2. We check that the array we&#39;re calling slice on has the same global object as the lexical
2374                     // global object that this code is running in. This requirement is necessary because we setup the
2375                     // watchpoints above on the lexical global object. This means that code that calls slice on
2376                     // arrays produced by other global objects won&#39;t get this optimization. We could relax this
2377                     // requirement in the future by checking that the watchpoint hasn&#39;t fired at runtime in the code
2378                     // we generate instead of registering it as a watchpoint that would invalidate the compilation.
2379                     //
2380                     // 3. By proving we&#39;re an original array structure, we guarantee that the incoming array
2381                     // isn&#39;t a subclass of Array.
2382 
2383                     StructureSet structureSet;
2384                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(ArrayWithInt32));
2385                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(ArrayWithContiguous));
2386                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(ArrayWithDouble));
2387                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(CopyOnWriteArrayWithInt32));
2388                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(CopyOnWriteArrayWithContiguous));
2389                     structureSet.add(globalObject-&gt;originalArrayStructureForIndexingType(CopyOnWriteArrayWithDouble));
2390                     addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(structureSet)), array);
2391 
2392                     addVarArgChild(array);
2393                     if (argumentCountIncludingThis &gt;= 2)
2394                         addVarArgChild(get(virtualRegisterForArgument(1, registerOffset))); // Start index.
2395                     if (argumentCountIncludingThis &gt;= 3)
2396                         addVarArgChild(get(virtualRegisterForArgument(2, registerOffset))); // End index.
2397                     addVarArgChild(addToGraph(GetButterfly, array));
2398 
2399                     Node* arraySlice = addToGraph(Node::VarArg, ArraySlice, OpInfo(), OpInfo());
2400                     setResult(arraySlice);
2401                     return true;
2402                 }
2403 
2404                 return false;
2405             }
2406             default:
2407                 return false;
2408             }
2409 
2410             RELEASE_ASSERT_NOT_REACHED();
2411             return false;
2412         }
2413 
2414         case ArrayIndexOfIntrinsic: {
2415             if (argumentCountIncludingThis &lt; 2)
2416                 return false;
2417 
2418             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIndexingType)
2419                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadConstantCache)
2420                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache)
2421                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
2422                 return false;
2423 
2424             ArrayMode arrayMode = getArrayMode(Array::Read);
2425             if (!arrayMode.isJSArray())
2426                 return false;
2427 
2428             if (!arrayMode.isJSArrayWithOriginalStructure())
2429                 return false;
2430 
2431             // We do not want to convert arrays into one type just to perform indexOf.
2432             if (arrayMode.doesConversion())
2433                 return false;
2434 
2435             switch (arrayMode.type()) {
2436             case Array::Double:
2437             case Array::Int32:
2438             case Array::Contiguous: {
2439                 JSGlobalObject* globalObject = m_graph.globalObjectFor(currentNodeOrigin().semantic);
2440 
2441                 Structure* arrayPrototypeStructure = globalObject-&gt;arrayPrototype()-&gt;structure(*m_vm);
2442                 Structure* objectPrototypeStructure = globalObject-&gt;objectPrototype()-&gt;structure(*m_vm);
2443 
2444                 // FIXME: We could easily relax the Array/Object.prototype transition as long as we OSR exitted if we saw a hole.
2445                 // https://bugs.webkit.org/show_bug.cgi?id=173171
2446                 if (arrayPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2447                     &amp;&amp; objectPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2448                     &amp;&amp; globalObject-&gt;arrayPrototypeChainIsSane()) {
2449 
2450                     m_graph.registerAndWatchStructureTransition(arrayPrototypeStructure);
2451                     m_graph.registerAndWatchStructureTransition(objectPrototypeStructure);
2452 
2453                     insertChecks();
2454 
2455                     Node* array = get(virtualRegisterForArgument(0, registerOffset));
2456                     addVarArgChild(array);
2457                     addVarArgChild(get(virtualRegisterForArgument(1, registerOffset))); // Search element.
2458                     if (argumentCountIncludingThis &gt;= 3)
2459                         addVarArgChild(get(virtualRegisterForArgument(2, registerOffset))); // Start index.
2460                     addVarArgChild(nullptr);
2461 
2462                     Node* node = addToGraph(Node::VarArg, ArrayIndexOf, OpInfo(arrayMode.asWord()), OpInfo());
2463                     setResult(node);
2464                     return true;
2465                 }
2466 
2467                 return false;
2468             }
2469             default:
2470                 return false;
2471             }
2472 
2473             RELEASE_ASSERT_NOT_REACHED();
2474             return false;
2475 
2476         }
2477 
2478         case ArrayPopIntrinsic: {
2479             if (argumentCountIncludingThis != 1)
2480                 return false;
2481 
2482             ArrayMode arrayMode = getArrayMode(Array::Write);
2483             if (!arrayMode.isJSArray())
2484                 return false;
2485             switch (arrayMode.type()) {
2486             case Array::Int32:
2487             case Array::Double:
2488             case Array::Contiguous:
2489             case Array::ArrayStorage: {
2490                 insertChecks();
2491                 Node* arrayPop = addToGraph(ArrayPop, OpInfo(arrayMode.asWord()), OpInfo(prediction), get(virtualRegisterForArgument(0, registerOffset)));
2492                 setResult(arrayPop);
2493                 return true;
2494             }
2495 
2496             default:
2497                 return false;
2498             }
2499         }
2500 
2501         case AtomicsAddIntrinsic:
2502         case AtomicsAndIntrinsic:
2503         case AtomicsCompareExchangeIntrinsic:
2504         case AtomicsExchangeIntrinsic:
2505         case AtomicsIsLockFreeIntrinsic:
2506         case AtomicsLoadIntrinsic:
2507         case AtomicsOrIntrinsic:
2508         case AtomicsStoreIntrinsic:
2509         case AtomicsSubIntrinsic:
2510         case AtomicsXorIntrinsic: {
2511             if (!is64Bit())
2512                 return false;
2513 
2514             NodeType op = LastNodeType;
2515             Array::Action action = Array::Write;
2516             unsigned numArgs = 0; // Number of actual args; we add one for the backing store pointer.
2517             switch (intrinsic) {
2518             case AtomicsAddIntrinsic:
2519                 op = AtomicsAdd;
2520                 numArgs = 3;
2521                 break;
2522             case AtomicsAndIntrinsic:
2523                 op = AtomicsAnd;
2524                 numArgs = 3;
2525                 break;
2526             case AtomicsCompareExchangeIntrinsic:
2527                 op = AtomicsCompareExchange;
2528                 numArgs = 4;
2529                 break;
2530             case AtomicsExchangeIntrinsic:
2531                 op = AtomicsExchange;
2532                 numArgs = 3;
2533                 break;
2534             case AtomicsIsLockFreeIntrinsic:
2535                 // This gets no backing store, but we need no special logic for this since this also does
2536                 // not need varargs.
2537                 op = AtomicsIsLockFree;
2538                 numArgs = 1;
2539                 break;
2540             case AtomicsLoadIntrinsic:
2541                 op = AtomicsLoad;
2542                 numArgs = 2;
2543                 action = Array::Read;
2544                 break;
2545             case AtomicsOrIntrinsic:
2546                 op = AtomicsOr;
2547                 numArgs = 3;
2548                 break;
2549             case AtomicsStoreIntrinsic:
2550                 op = AtomicsStore;
2551                 numArgs = 3;
2552                 break;
2553             case AtomicsSubIntrinsic:
2554                 op = AtomicsSub;
2555                 numArgs = 3;
2556                 break;
2557             case AtomicsXorIntrinsic:
2558                 op = AtomicsXor;
2559                 numArgs = 3;
2560                 break;
2561             default:
2562                 RELEASE_ASSERT_NOT_REACHED();
2563                 break;
2564             }
2565 
2566             if (static_cast&lt;unsigned&gt;(argumentCountIncludingThis) &lt; 1 + numArgs)
2567                 return false;
2568 
2569             insertChecks();
2570 
2571             Vector&lt;Node*, 3&gt; args;
2572             for (unsigned i = 0; i &lt; numArgs; ++i)
2573                 args.append(get(virtualRegisterForArgument(1 + i, registerOffset)));
2574 
2575             Node* resultNode;
2576             if (numArgs + 1 &lt;= 3) {
2577                 while (args.size() &lt; 3)
2578                     args.append(nullptr);
2579                 resultNode = addToGraph(op, OpInfo(ArrayMode(Array::SelectUsingPredictions, action).asWord()), OpInfo(prediction), args[0], args[1], args[2]);
2580             } else {
2581                 for (Node* node : args)
2582                     addVarArgChild(node);
2583                 addVarArgChild(nullptr);
2584                 resultNode = addToGraph(Node::VarArg, op, OpInfo(ArrayMode(Array::SelectUsingPredictions, action).asWord()), OpInfo(prediction));
2585             }
2586 
2587             setResult(resultNode);
2588             return true;
2589         }
2590 
2591         case ParseIntIntrinsic: {
2592             if (argumentCountIncludingThis &lt; 2)
2593                 return false;
2594 
2595             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell) || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
2596                 return false;
2597 
2598             insertChecks();
2599             VirtualRegister valueOperand = virtualRegisterForArgument(1, registerOffset);
2600             Node* parseInt;
2601             if (argumentCountIncludingThis == 2)
2602                 parseInt = addToGraph(ParseInt, OpInfo(), OpInfo(prediction), get(valueOperand));
2603             else {
2604                 ASSERT(argumentCountIncludingThis &gt; 2);
2605                 VirtualRegister radixOperand = virtualRegisterForArgument(2, registerOffset);
2606                 parseInt = addToGraph(ParseInt, OpInfo(), OpInfo(prediction), get(valueOperand), get(radixOperand));
2607             }
2608             setResult(parseInt);
2609             return true;
2610         }
2611 
2612         case CharCodeAtIntrinsic: {
2613             if (argumentCountIncludingThis != 2)
2614                 return false;
2615 
2616             insertChecks();
2617             VirtualRegister thisOperand = virtualRegisterForArgument(0, registerOffset);
2618             VirtualRegister indexOperand = virtualRegisterForArgument(1, registerOffset);
2619             Node* charCode = addToGraph(StringCharCodeAt, OpInfo(ArrayMode(Array::String, Array::Read).asWord()), get(thisOperand), get(indexOperand));
2620 
2621             setResult(charCode);
2622             return true;
2623         }
2624 
2625         case CharAtIntrinsic: {
2626             if (argumentCountIncludingThis != 2)
2627                 return false;
2628 
2629             insertChecks();
2630             VirtualRegister thisOperand = virtualRegisterForArgument(0, registerOffset);
2631             VirtualRegister indexOperand = virtualRegisterForArgument(1, registerOffset);
2632             Node* charCode = addToGraph(StringCharAt, OpInfo(ArrayMode(Array::String, Array::Read).asWord()), get(thisOperand), get(indexOperand));
2633 
2634             setResult(charCode);
2635             return true;
2636         }
2637         case Clz32Intrinsic: {
2638             insertChecks();
2639             if (argumentCountIncludingThis == 1)
2640                 setResult(addToGraph(JSConstant, OpInfo(m_graph.freeze(jsNumber(32)))));
2641             else {
2642                 Node* operand = get(virtualRegisterForArgument(1, registerOffset));
2643                 setResult(addToGraph(ArithClz32, operand));
2644             }
2645             return true;
2646         }
2647         case FromCharCodeIntrinsic: {
2648             if (argumentCountIncludingThis != 2)
2649                 return false;
2650 
2651             insertChecks();
2652             VirtualRegister indexOperand = virtualRegisterForArgument(1, registerOffset);
2653             Node* charCode = addToGraph(StringFromCharCode, get(indexOperand));
2654 
2655             setResult(charCode);
2656 
2657             return true;
2658         }
2659 
2660         case RegExpExecIntrinsic: {
2661             if (argumentCountIncludingThis != 2)
2662                 return false;
2663 
2664             insertChecks();
2665             Node* regExpExec = addToGraph(RegExpExec, OpInfo(0), OpInfo(prediction), addToGraph(GetGlobalObject, callee), get(virtualRegisterForArgument(0, registerOffset)), get(virtualRegisterForArgument(1, registerOffset)));
2666             setResult(regExpExec);
2667 
2668             return true;
2669         }
2670 
2671         case RegExpTestIntrinsic:
2672         case RegExpTestFastIntrinsic: {
2673             if (argumentCountIncludingThis != 2)
2674                 return false;
2675 
2676             if (intrinsic == RegExpTestIntrinsic) {
2677                 // Don&#39;t inline intrinsic if we exited due to one of the primordial RegExp checks failing.
2678                 if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell))
2679                     return false;
2680 
2681                 JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
2682                 Structure* regExpStructure = globalObject-&gt;regExpStructure();
2683                 m_graph.registerStructure(regExpStructure);
2684                 ASSERT(regExpStructure-&gt;storedPrototype().isObject());
2685                 ASSERT(regExpStructure-&gt;storedPrototype().asCell()-&gt;classInfo(*m_vm) == RegExpPrototype::info());
2686 
2687                 FrozenValue* regExpPrototypeObjectValue = m_graph.freeze(regExpStructure-&gt;storedPrototype());
2688                 Structure* regExpPrototypeStructure = regExpPrototypeObjectValue-&gt;structure();
2689 
2690                 auto isRegExpPropertySame = [&amp;] (JSValue primordialProperty, UniquedStringImpl* propertyUID) {
2691                     JSValue currentProperty;
2692                     if (!m_graph.getRegExpPrototypeProperty(regExpStructure-&gt;storedPrototypeObject(), regExpPrototypeStructure, propertyUID, currentProperty))
2693                         return false;
2694 
2695                     return currentProperty == primordialProperty;
2696                 };
2697 
2698                 // Check that RegExp.exec is still the primordial RegExp.prototype.exec
2699                 if (!isRegExpPropertySame(globalObject-&gt;regExpProtoExecFunction(), m_vm-&gt;propertyNames-&gt;exec.impl()))
2700                     return false;
2701 
2702                 // Check that regExpObject is actually a RegExp object.
2703                 Node* regExpObject = get(virtualRegisterForArgument(0, registerOffset));
2704                 addToGraph(Check, Edge(regExpObject, RegExpObjectUse));
2705 
2706                 // Check that regExpObject&#39;s exec is actually the primodial RegExp.prototype.exec.
2707                 UniquedStringImpl* execPropertyID = m_vm-&gt;propertyNames-&gt;exec.impl();
2708                 unsigned execIndex = m_graph.identifiers().ensure(execPropertyID);
2709                 Node* actualProperty = addToGraph(TryGetById, OpInfo(execIndex), OpInfo(SpecFunction), Edge(regExpObject, CellUse));
2710                 FrozenValue* regExpPrototypeExec = m_graph.freeze(globalObject-&gt;regExpProtoExecFunction());
2711                 addToGraph(CheckCell, OpInfo(regExpPrototypeExec), Edge(actualProperty, CellUse));
2712             }
2713 
2714             insertChecks();
2715             Node* regExpObject = get(virtualRegisterForArgument(0, registerOffset));
2716             Node* regExpExec = addToGraph(RegExpTest, OpInfo(0), OpInfo(prediction), addToGraph(GetGlobalObject, callee), regExpObject, get(virtualRegisterForArgument(1, registerOffset)));
2717             setResult(regExpExec);
2718 
2719             return true;
2720         }
2721 
2722         case RegExpMatchFastIntrinsic: {
2723             RELEASE_ASSERT(argumentCountIncludingThis == 2);
2724 
2725             insertChecks();
2726             Node* regExpMatch = addToGraph(RegExpMatchFast, OpInfo(0), OpInfo(prediction), addToGraph(GetGlobalObject, callee), get(virtualRegisterForArgument(0, registerOffset)), get(virtualRegisterForArgument(1, registerOffset)));
2727             setResult(regExpMatch);
2728             return true;
2729         }
2730 
2731         case ObjectCreateIntrinsic: {
2732             if (argumentCountIncludingThis != 2)
2733                 return false;
2734 
2735             insertChecks();
2736             setResult(addToGraph(ObjectCreate, get(virtualRegisterForArgument(1, registerOffset))));
2737             return true;
2738         }
2739 
2740         case ObjectGetPrototypeOfIntrinsic: {
2741             if (argumentCountIncludingThis != 2)
2742                 return false;
2743 
2744             insertChecks();
2745             setResult(addToGraph(GetPrototypeOf, OpInfo(0), OpInfo(prediction), get(virtualRegisterForArgument(1, registerOffset))));
2746             return true;
2747         }
2748 
2749         case ObjectIsIntrinsic: {
2750             if (argumentCountIncludingThis &lt; 3)
2751                 return false;
2752 
2753             insertChecks();
2754             setResult(addToGraph(SameValue, get(virtualRegisterForArgument(1, registerOffset)), get(virtualRegisterForArgument(2, registerOffset))));
2755             return true;
2756         }
2757 
2758         case ObjectKeysIntrinsic: {
2759             if (argumentCountIncludingThis &lt; 2)
2760                 return false;
2761 
2762             insertChecks();
2763             setResult(addToGraph(ObjectKeys, get(virtualRegisterForArgument(1, registerOffset))));
2764             return true;
2765         }
2766 
2767         case ReflectGetPrototypeOfIntrinsic: {
2768             if (argumentCountIncludingThis != 2)
2769                 return false;
2770 
2771             insertChecks();
2772             setResult(addToGraph(GetPrototypeOf, OpInfo(0), OpInfo(prediction), Edge(get(virtualRegisterForArgument(1, registerOffset)), ObjectUse)));
2773             return true;
2774         }
2775 
2776         case IsTypedArrayViewIntrinsic: {
2777             ASSERT(argumentCountIncludingThis == 2);
2778 
2779             insertChecks();
2780             setResult(addToGraph(IsTypedArrayView, OpInfo(prediction), get(virtualRegisterForArgument(1, registerOffset))));
2781             return true;
2782         }
2783 
2784         case StringPrototypeValueOfIntrinsic: {
2785             insertChecks();
2786             Node* value = get(virtualRegisterForArgument(0, registerOffset));
2787             setResult(addToGraph(StringValueOf, value));
2788             return true;
2789         }
2790 
2791         case StringPrototypeReplaceIntrinsic: {
2792             if (argumentCountIncludingThis != 3)
2793                 return false;
2794 
2795             // Don&#39;t inline intrinsic if we exited due to &quot;search&quot; not being a RegExp or String object.
2796             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
2797                 return false;
2798 
2799             // Don&#39;t inline intrinsic if we exited due to one of the primordial RegExp checks failing.
2800             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell))
2801                 return false;
2802 
2803             JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
2804             Structure* regExpStructure = globalObject-&gt;regExpStructure();
2805             m_graph.registerStructure(regExpStructure);
2806             ASSERT(regExpStructure-&gt;storedPrototype().isObject());
2807             ASSERT(regExpStructure-&gt;storedPrototype().asCell()-&gt;classInfo(*m_vm) == RegExpPrototype::info());
2808 
2809             FrozenValue* regExpPrototypeObjectValue = m_graph.freeze(regExpStructure-&gt;storedPrototype());
2810             Structure* regExpPrototypeStructure = regExpPrototypeObjectValue-&gt;structure();
2811 
2812             auto isRegExpPropertySame = [&amp;] (JSValue primordialProperty, UniquedStringImpl* propertyUID) {
2813                 JSValue currentProperty;
2814                 if (!m_graph.getRegExpPrototypeProperty(regExpStructure-&gt;storedPrototypeObject(), regExpPrototypeStructure, propertyUID, currentProperty))
2815                     return false;
2816 
2817                 return currentProperty == primordialProperty;
2818             };
2819 
2820             // Check that searchRegExp.exec is still the primordial RegExp.prototype.exec
2821             if (!isRegExpPropertySame(globalObject-&gt;regExpProtoExecFunction(), m_vm-&gt;propertyNames-&gt;exec.impl()))
2822                 return false;
2823 
2824             // Check that searchRegExp.global is still the primordial RegExp.prototype.global
2825             if (!isRegExpPropertySame(globalObject-&gt;regExpProtoGlobalGetter(), m_vm-&gt;propertyNames-&gt;global.impl()))
2826                 return false;
2827 
2828             // Check that searchRegExp.unicode is still the primordial RegExp.prototype.unicode
2829             if (!isRegExpPropertySame(globalObject-&gt;regExpProtoUnicodeGetter(), m_vm-&gt;propertyNames-&gt;unicode.impl()))
2830                 return false;
2831 
2832             // Check that searchRegExp[Symbol.match] is still the primordial RegExp.prototype[Symbol.replace]
2833             if (!isRegExpPropertySame(globalObject-&gt;regExpProtoSymbolReplaceFunction(), m_vm-&gt;propertyNames-&gt;replaceSymbol.impl()))
2834                 return false;
2835 
2836             insertChecks();
2837 
2838             Node* resultNode = addToGraph(StringReplace, OpInfo(0), OpInfo(prediction), get(virtualRegisterForArgument(0, registerOffset)), get(virtualRegisterForArgument(1, registerOffset)), get(virtualRegisterForArgument(2, registerOffset)));
2839             setResult(resultNode);
2840             return true;
2841         }
2842 
2843         case StringPrototypeReplaceRegExpIntrinsic: {
2844             if (argumentCountIncludingThis != 3)
2845                 return false;
2846 
2847             insertChecks();
2848             Node* resultNode = addToGraph(StringReplaceRegExp, OpInfo(0), OpInfo(prediction), get(virtualRegisterForArgument(0, registerOffset)), get(virtualRegisterForArgument(1, registerOffset)), get(virtualRegisterForArgument(2, registerOffset)));
2849             setResult(resultNode);
2850             return true;
2851         }
2852 
2853         case RoundIntrinsic:
2854         case FloorIntrinsic:
2855         case CeilIntrinsic:
2856         case TruncIntrinsic: {
2857             if (argumentCountIncludingThis == 1) {
2858                 insertChecks();
2859                 setResult(addToGraph(JSConstant, OpInfo(m_constantNaN)));
2860                 return true;
2861             }
2862             insertChecks();
2863             Node* operand = get(virtualRegisterForArgument(1, registerOffset));
2864             NodeType op;
2865             if (intrinsic == RoundIntrinsic)
2866                 op = ArithRound;
2867             else if (intrinsic == FloorIntrinsic)
2868                 op = ArithFloor;
2869             else if (intrinsic == CeilIntrinsic)
2870                 op = ArithCeil;
2871             else {
2872                 ASSERT(intrinsic == TruncIntrinsic);
2873                 op = ArithTrunc;
2874             }
2875             Node* roundNode = addToGraph(op, OpInfo(0), OpInfo(prediction), operand);
2876             setResult(roundNode);
2877             return true;
2878         }
2879         case IMulIntrinsic: {
2880             if (argumentCountIncludingThis != 3)
2881                 return false;
2882             insertChecks();
2883             VirtualRegister leftOperand = virtualRegisterForArgument(1, registerOffset);
2884             VirtualRegister rightOperand = virtualRegisterForArgument(2, registerOffset);
2885             Node* left = get(leftOperand);
2886             Node* right = get(rightOperand);
2887             setResult(addToGraph(ArithIMul, left, right));
2888             return true;
2889         }
2890 
2891         case RandomIntrinsic: {
2892             if (argumentCountIncludingThis != 1)
2893                 return false;
2894             insertChecks();
2895             setResult(addToGraph(ArithRandom));
2896             return true;
2897         }
2898 
2899         case DFGTrueIntrinsic: {
2900             insertChecks();
2901             setResult(jsConstant(jsBoolean(true)));
2902             return true;
2903         }
2904 
2905         case FTLTrueIntrinsic: {
2906             insertChecks();
2907             setResult(jsConstant(jsBoolean(m_graph.m_plan.isFTL())));
2908             return true;
2909         }
2910 
2911         case OSRExitIntrinsic: {
2912             insertChecks();
2913             addToGraph(ForceOSRExit);
2914             setResult(addToGraph(JSConstant, OpInfo(m_constantUndefined)));
2915             return true;
2916         }
2917 
2918         case IsFinalTierIntrinsic: {
2919             insertChecks();
2920             setResult(jsConstant(jsBoolean(Options::useFTLJIT() ? m_graph.m_plan.isFTL() : true)));
2921             return true;
2922         }
2923 
2924         case SetInt32HeapPredictionIntrinsic: {
2925             insertChecks();
2926             for (int i = 1; i &lt; argumentCountIncludingThis; ++i) {
2927                 Node* node = get(virtualRegisterForArgument(i, registerOffset));
2928                 if (node-&gt;hasHeapPrediction())
2929                     node-&gt;setHeapPrediction(SpecInt32Only);
2930             }
2931             setResult(addToGraph(JSConstant, OpInfo(m_constantUndefined)));
2932             return true;
2933         }
2934 
2935         case CheckInt32Intrinsic: {
2936             insertChecks();
2937             for (int i = 1; i &lt; argumentCountIncludingThis; ++i) {
2938                 Node* node = get(virtualRegisterForArgument(i, registerOffset));
2939                 addToGraph(Phantom, Edge(node, Int32Use));
2940             }
2941             setResult(jsConstant(jsBoolean(true)));
2942             return true;
2943         }
2944 
2945         case FiatInt52Intrinsic: {
2946             if (argumentCountIncludingThis != 2)
2947                 return false;
2948             insertChecks();
2949             VirtualRegister operand = virtualRegisterForArgument(1, registerOffset);
2950             if (enableInt52())
2951                 setResult(addToGraph(FiatInt52, get(operand)));
2952             else
2953                 setResult(get(operand));
2954             return true;
2955         }
2956 
2957         case JSMapGetIntrinsic: {
2958             if (argumentCountIncludingThis != 2)
2959                 return false;
2960 
2961             insertChecks();
2962             Node* map = get(virtualRegisterForArgument(0, registerOffset));
2963             Node* key = get(virtualRegisterForArgument(1, registerOffset));
2964             Node* normalizedKey = addToGraph(NormalizeMapKey, key);
2965             Node* hash = addToGraph(MapHash, normalizedKey);
2966             Node* bucket = addToGraph(GetMapBucket, Edge(map, MapObjectUse), Edge(normalizedKey), Edge(hash));
2967             Node* resultNode = addToGraph(LoadValueFromMapBucket, OpInfo(BucketOwnerType::Map), OpInfo(prediction), bucket);
2968             setResult(resultNode);
2969             return true;
2970         }
2971 
2972         case JSSetHasIntrinsic:
2973         case JSMapHasIntrinsic: {
2974             if (argumentCountIncludingThis != 2)
2975                 return false;
2976 
2977             insertChecks();
2978             Node* mapOrSet = get(virtualRegisterForArgument(0, registerOffset));
2979             Node* key = get(virtualRegisterForArgument(1, registerOffset));
2980             Node* normalizedKey = addToGraph(NormalizeMapKey, key);
2981             Node* hash = addToGraph(MapHash, normalizedKey);
2982             UseKind useKind = intrinsic == JSSetHasIntrinsic ? SetObjectUse : MapObjectUse;
2983             Node* bucket = addToGraph(GetMapBucket, OpInfo(0), Edge(mapOrSet, useKind), Edge(normalizedKey), Edge(hash));
2984             JSCell* sentinel = nullptr;
2985             if (intrinsic == JSMapHasIntrinsic)
2986                 sentinel = m_vm-&gt;sentinelMapBucket();
2987             else
2988                 sentinel = m_vm-&gt;sentinelSetBucket();
2989 
2990             FrozenValue* frozenPointer = m_graph.freeze(sentinel);
2991             Node* invertedResult = addToGraph(CompareEqPtr, OpInfo(frozenPointer), bucket);
2992             Node* resultNode = addToGraph(LogicalNot, invertedResult);
2993             setResult(resultNode);
2994             return true;
2995         }
2996 
2997         case JSSetAddIntrinsic: {
2998             if (argumentCountIncludingThis != 2)
2999                 return false;
3000 
3001             insertChecks();
3002             Node* base = get(virtualRegisterForArgument(0, registerOffset));
3003             Node* key = get(virtualRegisterForArgument(1, registerOffset));
3004             Node* normalizedKey = addToGraph(NormalizeMapKey, key);
3005             Node* hash = addToGraph(MapHash, normalizedKey);
3006             addToGraph(SetAdd, base, normalizedKey, hash);
3007             setResult(base);
3008             return true;
3009         }
3010 
3011         case JSMapSetIntrinsic: {
3012             if (argumentCountIncludingThis != 3)
3013                 return false;
3014 
3015             insertChecks();
3016             Node* base = get(virtualRegisterForArgument(0, registerOffset));
3017             Node* key = get(virtualRegisterForArgument(1, registerOffset));
3018             Node* value = get(virtualRegisterForArgument(2, registerOffset));
3019 
3020             Node* normalizedKey = addToGraph(NormalizeMapKey, key);
3021             Node* hash = addToGraph(MapHash, normalizedKey);
3022 
3023             addVarArgChild(base);
3024             addVarArgChild(normalizedKey);
3025             addVarArgChild(value);
3026             addVarArgChild(hash);
3027             addToGraph(Node::VarArg, MapSet, OpInfo(0), OpInfo(0));
3028             setResult(base);
3029             return true;
3030         }
3031 
3032         case JSSetBucketHeadIntrinsic:
3033         case JSMapBucketHeadIntrinsic: {
3034             ASSERT(argumentCountIncludingThis == 2);
3035 
3036             insertChecks();
3037             Node* map = get(virtualRegisterForArgument(1, registerOffset));
3038             UseKind useKind = intrinsic == JSSetBucketHeadIntrinsic ? SetObjectUse : MapObjectUse;
3039             Node* resultNode = addToGraph(GetMapBucketHead, Edge(map, useKind));
3040             setResult(resultNode);
3041             return true;
3042         }
3043 
3044         case JSSetBucketNextIntrinsic:
3045         case JSMapBucketNextIntrinsic: {
3046             ASSERT(argumentCountIncludingThis == 2);
3047 
3048             insertChecks();
3049             Node* bucket = get(virtualRegisterForArgument(1, registerOffset));
3050             BucketOwnerType type = intrinsic == JSSetBucketNextIntrinsic ? BucketOwnerType::Set : BucketOwnerType::Map;
3051             Node* resultNode = addToGraph(GetMapBucketNext, OpInfo(type), bucket);
3052             setResult(resultNode);
3053             return true;
3054         }
3055 
3056         case JSSetBucketKeyIntrinsic:
3057         case JSMapBucketKeyIntrinsic: {
3058             ASSERT(argumentCountIncludingThis == 2);
3059 
3060             insertChecks();
3061             Node* bucket = get(virtualRegisterForArgument(1, registerOffset));
3062             BucketOwnerType type = intrinsic == JSSetBucketKeyIntrinsic ? BucketOwnerType::Set : BucketOwnerType::Map;
3063             Node* resultNode = addToGraph(LoadKeyFromMapBucket, OpInfo(type), OpInfo(prediction), bucket);
3064             setResult(resultNode);
3065             return true;
3066         }
3067 
3068         case JSMapBucketValueIntrinsic: {
3069             ASSERT(argumentCountIncludingThis == 2);
3070 
3071             insertChecks();
3072             Node* bucket = get(virtualRegisterForArgument(1, registerOffset));
3073             Node* resultNode = addToGraph(LoadValueFromMapBucket, OpInfo(BucketOwnerType::Map), OpInfo(prediction), bucket);
3074             setResult(resultNode);
3075             return true;
3076         }
3077 
3078         case JSWeakMapGetIntrinsic: {
3079             if (argumentCountIncludingThis != 2)
3080                 return false;
3081 
3082             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3083                 return false;
3084 
3085             insertChecks();
3086             Node* map = get(virtualRegisterForArgument(0, registerOffset));
3087             Node* key = get(virtualRegisterForArgument(1, registerOffset));
3088             addToGraph(Check, Edge(key, ObjectUse));
3089             Node* hash = addToGraph(MapHash, key);
3090             Node* holder = addToGraph(WeakMapGet, Edge(map, WeakMapObjectUse), Edge(key, ObjectUse), Edge(hash, Int32Use));
3091             Node* resultNode = addToGraph(ExtractValueFromWeakMapGet, OpInfo(), OpInfo(prediction), holder);
3092 
3093             setResult(resultNode);
3094             return true;
3095         }
3096 
3097         case JSWeakMapHasIntrinsic: {
3098             if (argumentCountIncludingThis != 2)
3099                 return false;
3100 
3101             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3102                 return false;
3103 
3104             insertChecks();
3105             Node* map = get(virtualRegisterForArgument(0, registerOffset));
3106             Node* key = get(virtualRegisterForArgument(1, registerOffset));
3107             addToGraph(Check, Edge(key, ObjectUse));
3108             Node* hash = addToGraph(MapHash, key);
3109             Node* holder = addToGraph(WeakMapGet, Edge(map, WeakMapObjectUse), Edge(key, ObjectUse), Edge(hash, Int32Use));
3110             Node* invertedResult = addToGraph(IsEmpty, holder);
3111             Node* resultNode = addToGraph(LogicalNot, invertedResult);
3112 
3113             setResult(resultNode);
3114             return true;
3115         }
3116 
3117         case JSWeakSetHasIntrinsic: {
3118             if (argumentCountIncludingThis != 2)
3119                 return false;
3120 
3121             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3122                 return false;
3123 
3124             insertChecks();
3125             Node* map = get(virtualRegisterForArgument(0, registerOffset));
3126             Node* key = get(virtualRegisterForArgument(1, registerOffset));
3127             addToGraph(Check, Edge(key, ObjectUse));
3128             Node* hash = addToGraph(MapHash, key);
3129             Node* holder = addToGraph(WeakMapGet, Edge(map, WeakSetObjectUse), Edge(key, ObjectUse), Edge(hash, Int32Use));
3130             Node* invertedResult = addToGraph(IsEmpty, holder);
3131             Node* resultNode = addToGraph(LogicalNot, invertedResult);
3132 
3133             setResult(resultNode);
3134             return true;
3135         }
3136 
3137         case JSWeakSetAddIntrinsic: {
3138             if (argumentCountIncludingThis != 2)
3139                 return false;
3140 
3141             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3142                 return false;
3143 
3144             insertChecks();
3145             Node* base = get(virtualRegisterForArgument(0, registerOffset));
3146             Node* key = get(virtualRegisterForArgument(1, registerOffset));
3147             addToGraph(Check, Edge(key, ObjectUse));
3148             Node* hash = addToGraph(MapHash, key);
3149             addToGraph(WeakSetAdd, Edge(base, WeakSetObjectUse), Edge(key, ObjectUse), Edge(hash, Int32Use));
3150             setResult(base);
3151             return true;
3152         }
3153 
3154         case JSWeakMapSetIntrinsic: {
3155             if (argumentCountIncludingThis != 3)
3156                 return false;
3157 
3158             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3159                 return false;
3160 
3161             insertChecks();
3162             Node* base = get(virtualRegisterForArgument(0, registerOffset));
3163             Node* key = get(virtualRegisterForArgument(1, registerOffset));
3164             Node* value = get(virtualRegisterForArgument(2, registerOffset));
3165 
3166             addToGraph(Check, Edge(key, ObjectUse));
3167             Node* hash = addToGraph(MapHash, key);
3168 
3169             addVarArgChild(Edge(base, WeakMapObjectUse));
3170             addVarArgChild(Edge(key, ObjectUse));
3171             addVarArgChild(Edge(value));
3172             addVarArgChild(Edge(hash, Int32Use));
3173             addToGraph(Node::VarArg, WeakMapSet, OpInfo(0), OpInfo(0));
3174             setResult(base);
3175             return true;
3176         }
3177 
3178         case DataViewGetInt8:
3179         case DataViewGetUint8:
3180         case DataViewGetInt16:
3181         case DataViewGetUint16:
3182         case DataViewGetInt32:
3183         case DataViewGetUint32:
3184         case DataViewGetFloat32:
3185         case DataViewGetFloat64: {
3186             if (!is64Bit())
3187                 return false;
3188 
3189             // To inline data view accesses, we assume the architecture we&#39;re running on:
3190             // - Is little endian.
3191             // - Allows unaligned loads/stores without crashing.
3192 
3193             if (argumentCountIncludingThis &lt; 2)
3194                 return false;
3195             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3196                 return false;
3197 
3198             insertChecks();
3199 
3200             uint8_t byteSize;
3201             NodeType op = DataViewGetInt;
3202             bool isSigned = false;
3203             switch (intrinsic) {
3204             case DataViewGetInt8:
3205                 isSigned = true;
3206                 FALLTHROUGH;
3207             case DataViewGetUint8:
3208                 byteSize = 1;
3209                 break;
3210 
3211             case DataViewGetInt16:
3212                 isSigned = true;
3213                 FALLTHROUGH;
3214             case DataViewGetUint16:
3215                 byteSize = 2;
3216                 break;
3217 
3218             case DataViewGetInt32:
3219                 isSigned = true;
3220                 FALLTHROUGH;
3221             case DataViewGetUint32:
3222                 byteSize = 4;
3223                 break;
3224 
3225             case DataViewGetFloat32:
3226                 byteSize = 4;
3227                 op = DataViewGetFloat;
3228                 break;
3229             case DataViewGetFloat64:
3230                 byteSize = 8;
3231                 op = DataViewGetFloat;
3232                 break;
3233             default:
3234                 RELEASE_ASSERT_NOT_REACHED();
3235             }
3236 
3237             TriState isLittleEndian = MixedTriState;
3238             Node* littleEndianChild = nullptr;
3239             if (byteSize &gt; 1) {
3240                 if (argumentCountIncludingThis &lt; 3)
3241                     isLittleEndian = FalseTriState;
3242                 else {
3243                     littleEndianChild = get(virtualRegisterForArgument(2, registerOffset));
3244                     if (littleEndianChild-&gt;hasConstant()) {
3245                         JSValue constant = littleEndianChild-&gt;constant()-&gt;value();
<a name="44" id="anc44"></a><span class="line-modified">3246                         if (constant) {</span>
<span class="line-modified">3247                             isLittleEndian = constant.pureToBoolean();</span>
<span class="line-modified">3248                             if (isLittleEndian != MixedTriState)</span>
<span class="line-added">3249                                 littleEndianChild = nullptr;</span>
<span class="line-added">3250                         }</span>
3251                     } else
3252                         isLittleEndian = MixedTriState;
3253                 }
3254             }
3255 
3256             DataViewData data { };
3257             data.isLittleEndian = isLittleEndian;
3258             data.isSigned = isSigned;
3259             data.byteSize = byteSize;
3260 
3261             setResult(
3262                 addToGraph(op, OpInfo(data.asQuadWord), OpInfo(prediction), get(virtualRegisterForArgument(0, registerOffset)), get(virtualRegisterForArgument(1, registerOffset)), littleEndianChild));
3263             return true;
3264         }
3265 
3266         case DataViewSetInt8:
3267         case DataViewSetUint8:
3268         case DataViewSetInt16:
3269         case DataViewSetUint16:
3270         case DataViewSetInt32:
3271         case DataViewSetUint32:
3272         case DataViewSetFloat32:
3273         case DataViewSetFloat64: {
3274             if (!is64Bit())
3275                 return false;
3276 
3277             if (argumentCountIncludingThis &lt; 3)
3278                 return false;
3279 
3280             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3281                 return false;
3282 
3283             insertChecks();
3284 
3285             uint8_t byteSize;
3286             bool isFloatingPoint = false;
3287             bool isSigned = false;
3288             switch (intrinsic) {
3289             case DataViewSetInt8:
3290                 isSigned = true;
3291                 FALLTHROUGH;
3292             case DataViewSetUint8:
3293                 byteSize = 1;
3294                 break;
3295 
3296             case DataViewSetInt16:
3297                 isSigned = true;
3298                 FALLTHROUGH;
3299             case DataViewSetUint16:
3300                 byteSize = 2;
3301                 break;
3302 
3303             case DataViewSetInt32:
3304                 isSigned = true;
3305                 FALLTHROUGH;
3306             case DataViewSetUint32:
3307                 byteSize = 4;
3308                 break;
3309 
3310             case DataViewSetFloat32:
3311                 isFloatingPoint = true;
3312                 byteSize = 4;
3313                 break;
3314             case DataViewSetFloat64:
3315                 isFloatingPoint = true;
3316                 byteSize = 8;
3317                 break;
3318             default:
3319                 RELEASE_ASSERT_NOT_REACHED();
3320             }
3321 
3322             TriState isLittleEndian = MixedTriState;
3323             Node* littleEndianChild = nullptr;
3324             if (byteSize &gt; 1) {
3325                 if (argumentCountIncludingThis &lt; 4)
3326                     isLittleEndian = FalseTriState;
3327                 else {
3328                     littleEndianChild = get(virtualRegisterForArgument(3, registerOffset));
3329                     if (littleEndianChild-&gt;hasConstant()) {
3330                         JSValue constant = littleEndianChild-&gt;constant()-&gt;value();
<a name="45" id="anc45"></a><span class="line-modified">3331                         if (constant) {</span>
<span class="line-modified">3332                             isLittleEndian = constant.pureToBoolean();</span>
<span class="line-modified">3333                             if (isLittleEndian != MixedTriState)</span>
<span class="line-added">3334                                 littleEndianChild = nullptr;</span>
<span class="line-added">3335                         }</span>
3336                     } else
3337                         isLittleEndian = MixedTriState;
3338                 }
3339             }
3340 
3341             DataViewData data { };
3342             data.isLittleEndian = isLittleEndian;
3343             data.isSigned = isSigned;
3344             data.byteSize = byteSize;
3345             data.isFloatingPoint = isFloatingPoint;
3346 
3347             addVarArgChild(get(virtualRegisterForArgument(0, registerOffset)));
3348             addVarArgChild(get(virtualRegisterForArgument(1, registerOffset)));
3349             addVarArgChild(get(virtualRegisterForArgument(2, registerOffset)));
3350             addVarArgChild(littleEndianChild);
3351 
3352             addToGraph(Node::VarArg, DataViewSet, OpInfo(data.asQuadWord), OpInfo());
3353             setResult(addToGraph(JSConstant, OpInfo(m_constantUndefined)));
3354             return true;
3355         }
3356 
3357         case HasOwnPropertyIntrinsic: {
3358             if (argumentCountIncludingThis != 2)
3359                 return false;
3360 
3361             // This can be racy, that&#39;s fine. We know that once we observe that this is created,
3362             // that it will never be destroyed until the VM is destroyed. It&#39;s unlikely that
3363             // we&#39;d ever get to the point where we inline this as an intrinsic without the
3364             // cache being created, however, it&#39;s possible if we always throw exceptions inside
3365             // hasOwnProperty.
3366             if (!m_vm-&gt;hasOwnPropertyCache())
3367                 return false;
3368 
3369             insertChecks();
3370             Node* object = get(virtualRegisterForArgument(0, registerOffset));
3371             Node* key = get(virtualRegisterForArgument(1, registerOffset));
3372             Node* resultNode = addToGraph(HasOwnProperty, object, key);
3373             setResult(resultNode);
3374             return true;
3375         }
3376 
3377         case StringPrototypeSliceIntrinsic: {
3378             if (argumentCountIncludingThis &lt; 2)
3379                 return false;
3380 
3381             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3382                 return false;
3383 
3384             insertChecks();
3385             Node* thisString = get(virtualRegisterForArgument(0, registerOffset));
3386             Node* start = get(virtualRegisterForArgument(1, registerOffset));
3387             Node* end = nullptr;
3388             if (argumentCountIncludingThis &gt; 2)
3389                 end = get(virtualRegisterForArgument(2, registerOffset));
3390             Node* resultNode = addToGraph(StringSlice, thisString, start, end);
3391             setResult(resultNode);
3392             return true;
3393         }
3394 
3395         case StringPrototypeToLowerCaseIntrinsic: {
3396             if (argumentCountIncludingThis != 1)
3397                 return false;
3398 
3399             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3400                 return false;
3401 
3402             insertChecks();
3403             Node* thisString = get(virtualRegisterForArgument(0, registerOffset));
3404             Node* resultNode = addToGraph(ToLowerCase, thisString);
3405             setResult(resultNode);
3406             return true;
3407         }
3408 
3409         case NumberPrototypeToStringIntrinsic: {
3410             if (argumentCountIncludingThis != 1 &amp;&amp; argumentCountIncludingThis != 2)
3411                 return false;
3412 
3413             if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3414                 return false;
3415 
3416             insertChecks();
3417             Node* thisNumber = get(virtualRegisterForArgument(0, registerOffset));
3418             if (argumentCountIncludingThis == 1) {
3419                 Node* resultNode = addToGraph(ToString, thisNumber);
3420                 setResult(resultNode);
3421             } else {
3422                 Node* radix = get(virtualRegisterForArgument(1, registerOffset));
3423                 Node* resultNode = addToGraph(NumberToStringWithRadix, thisNumber, radix);
3424                 setResult(resultNode);
3425             }
3426             return true;
3427         }
3428 
3429         case NumberIsIntegerIntrinsic: {
3430             if (argumentCountIncludingThis &lt; 2)
3431                 return false;
3432 
3433             insertChecks();
3434             Node* input = get(virtualRegisterForArgument(1, registerOffset));
3435             Node* resultNode = addToGraph(NumberIsInteger, input);
3436             setResult(resultNode);
3437             return true;
3438         }
3439 
3440         case CPUMfenceIntrinsic:
3441         case CPURdtscIntrinsic:
3442         case CPUCpuidIntrinsic:
3443         case CPUPauseIntrinsic: {
3444 #if CPU(X86_64)
3445             if (!m_graph.m_plan.isFTL())
3446                 return false;
3447             insertChecks();
3448             setResult(addToGraph(CPUIntrinsic, OpInfo(intrinsic), OpInfo()));
3449             return true;
3450 #else
3451             return false;
3452 #endif
3453         }
3454 
3455         default:
3456             return false;
3457         }
3458     };
3459 
3460     if (inlineIntrinsic()) {
3461         RELEASE_ASSERT(didSetResult);
3462         return true;
3463     }
3464 
3465     return false;
3466 }
3467 
3468 template&lt;typename ChecksFunctor&gt;
3469 bool ByteCodeParser::handleDOMJITCall(Node* callTarget, VirtualRegister result, const DOMJIT::Signature* signature, int registerOffset, int argumentCountIncludingThis, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks)
3470 {
3471     if (argumentCountIncludingThis != static_cast&lt;int&gt;(1 + signature-&gt;argumentCount))
3472         return false;
3473     if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType))
3474         return false;
3475 
3476     // FIXME: Currently, we only support functions which arguments are up to 2.
3477     // Eventually, we should extend this. But possibly, 2 or 3 can cover typical use cases.
3478     // https://bugs.webkit.org/show_bug.cgi?id=164346
3479     ASSERT_WITH_MESSAGE(argumentCountIncludingThis &lt;= JSC_DOMJIT_SIGNATURE_MAX_ARGUMENTS_INCLUDING_THIS, &quot;Currently CallDOM does not support an arbitrary length arguments.&quot;);
3480 
3481     insertChecks();
3482     addCall(result, Call, signature, callTarget, argumentCountIncludingThis, registerOffset, prediction);
3483     return true;
3484 }
3485 
3486 
3487 template&lt;typename ChecksFunctor&gt;
3488 bool ByteCodeParser::handleIntrinsicGetter(VirtualRegister result, SpeculatedType prediction, const GetByIdVariant&amp; variant, Node* thisNode, const ChecksFunctor&amp; insertChecks)
3489 {
3490     switch (variant.intrinsic()) {
3491     case TypedArrayByteLengthIntrinsic: {
3492         insertChecks();
3493 
3494         TypedArrayType type = (*variant.structureSet().begin())-&gt;classInfo()-&gt;typedArrayStorageType;
3495         Array::Type arrayType = toArrayType(type);
3496         size_t logSize = logElementSize(type);
3497 
3498         variant.structureSet().forEach([&amp;] (Structure* structure) {
3499             TypedArrayType curType = structure-&gt;classInfo()-&gt;typedArrayStorageType;
3500             ASSERT(logSize == logElementSize(curType));
3501             arrayType = refineTypedArrayType(arrayType, curType);
3502             ASSERT(arrayType != Array::Generic);
3503         });
3504 
3505         Node* lengthNode = addToGraph(GetArrayLength, OpInfo(ArrayMode(arrayType, Array::Read).asWord()), thisNode);
3506 
3507         if (!logSize) {
3508             set(result, lengthNode);
3509             return true;
3510         }
3511 
3512         // We can use a BitLShift here because typed arrays will never have a byteLength
3513         // that overflows int32.
3514         Node* shiftNode = jsConstant(jsNumber(logSize));
<a name="46" id="anc46"></a><span class="line-modified">3515         set(result, addToGraph(ArithBitLShift, lengthNode, shiftNode));</span>
3516 
3517         return true;
3518     }
3519 
3520     case TypedArrayLengthIntrinsic: {
3521         insertChecks();
3522 
3523         TypedArrayType type = (*variant.structureSet().begin())-&gt;classInfo()-&gt;typedArrayStorageType;
3524         Array::Type arrayType = toArrayType(type);
3525 
3526         variant.structureSet().forEach([&amp;] (Structure* structure) {
3527             TypedArrayType curType = structure-&gt;classInfo()-&gt;typedArrayStorageType;
3528             arrayType = refineTypedArrayType(arrayType, curType);
3529             ASSERT(arrayType != Array::Generic);
3530         });
3531 
3532         set(result, addToGraph(GetArrayLength, OpInfo(ArrayMode(arrayType, Array::Read).asWord()), thisNode));
3533 
3534         return true;
3535 
3536     }
3537 
3538     case TypedArrayByteOffsetIntrinsic: {
3539         insertChecks();
3540 
3541         TypedArrayType type = (*variant.structureSet().begin())-&gt;classInfo()-&gt;typedArrayStorageType;
3542         Array::Type arrayType = toArrayType(type);
3543 
3544         variant.structureSet().forEach([&amp;] (Structure* structure) {
3545             TypedArrayType curType = structure-&gt;classInfo()-&gt;typedArrayStorageType;
3546             arrayType = refineTypedArrayType(arrayType, curType);
3547             ASSERT(arrayType != Array::Generic);
3548         });
3549 
3550         set(result, addToGraph(GetTypedArrayByteOffset, OpInfo(ArrayMode(arrayType, Array::Read).asWord()), thisNode));
3551 
3552         return true;
3553     }
3554 
3555     case UnderscoreProtoIntrinsic: {
3556         insertChecks();
3557 
3558         bool canFold = !variant.structureSet().isEmpty();
3559         JSValue prototype;
3560         variant.structureSet().forEach([&amp;] (Structure* structure) {
3561             auto getPrototypeMethod = structure-&gt;classInfo()-&gt;methodTable.getPrototype;
3562             MethodTable::GetPrototypeFunctionPtr defaultGetPrototype = JSObject::getPrototype;
3563             if (getPrototypeMethod != defaultGetPrototype) {
3564                 canFold = false;
3565                 return;
3566             }
3567 
3568             if (structure-&gt;hasPolyProto()) {
3569                 canFold = false;
3570                 return;
3571             }
3572             if (!prototype)
3573                 prototype = structure-&gt;storedPrototype();
3574             else if (prototype != structure-&gt;storedPrototype())
3575                 canFold = false;
3576         });
3577 
3578         // OK, only one prototype is found. We perform constant folding here.
3579         // This information is important for super&#39;s constructor call to get new.target constant.
3580         if (prototype &amp;&amp; canFold) {
3581             set(result, weakJSConstant(prototype));
3582             return true;
3583         }
3584 
3585         set(result, addToGraph(GetPrototypeOf, OpInfo(0), OpInfo(prediction), thisNode));
3586         return true;
3587     }
3588 
3589     default:
3590         return false;
3591     }
3592     RELEASE_ASSERT_NOT_REACHED();
3593 }
3594 
3595 static void blessCallDOMGetter(Node* node)
3596 {
3597     DOMJIT::CallDOMGetterSnippet* snippet = node-&gt;callDOMGetterData()-&gt;snippet;
3598     if (snippet &amp;&amp; !snippet-&gt;effect.mustGenerate())
3599         node-&gt;clearFlags(NodeMustGenerate);
3600 }
3601 
3602 bool ByteCodeParser::handleDOMJITGetter(VirtualRegister result, const GetByIdVariant&amp; variant, Node* thisNode, unsigned identifierNumber, SpeculatedType prediction)
3603 {
3604     if (!variant.domAttribute())
3605         return false;
3606 
3607     auto domAttribute = variant.domAttribute().value();
3608 
3609     // We do not need to actually look up CustomGetterSetter here. Checking Structures or registering watchpoints are enough,
3610     // since replacement of CustomGetterSetter always incurs Structure transition.
3611     if (!check(variant.conditionSet()))
3612         return false;
3613     addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(variant.structureSet())), thisNode);
3614 
3615     // We do not need to emit CheckCell thingy here. When the custom accessor is replaced to different one, Structure transition occurs.
3616     addToGraph(CheckSubClass, OpInfo(domAttribute.classInfo), thisNode);
3617 
3618     bool wasSeenInJIT = true;
3619     addToGraph(FilterGetByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addGetByIdStatus(currentCodeOrigin(), GetByIdStatus(GetByIdStatus::Custom, wasSeenInJIT, variant))), thisNode);
3620 
3621     CallDOMGetterData* callDOMGetterData = m_graph.m_callDOMGetterData.add();
3622     callDOMGetterData-&gt;customAccessorGetter = variant.customAccessorGetter();
3623     ASSERT(callDOMGetterData-&gt;customAccessorGetter);
3624 
3625     if (const auto* domJIT = domAttribute.domJIT) {
3626         callDOMGetterData-&gt;domJIT = domJIT;
3627         Ref&lt;DOMJIT::CallDOMGetterSnippet&gt; snippet = domJIT-&gt;compiler()();
3628         callDOMGetterData-&gt;snippet = snippet.ptr();
3629         m_graph.m_domJITSnippets.append(WTFMove(snippet));
3630     }
3631     DOMJIT::CallDOMGetterSnippet* callDOMGetterSnippet = callDOMGetterData-&gt;snippet;
3632     callDOMGetterData-&gt;identifierNumber = identifierNumber;
3633 
3634     Node* callDOMGetterNode = nullptr;
3635     // GlobalObject of thisNode is always used to create a DOMWrapper.
3636     if (callDOMGetterSnippet &amp;&amp; callDOMGetterSnippet-&gt;requireGlobalObject) {
3637         Node* globalObject = addToGraph(GetGlobalObject, thisNode);
3638         callDOMGetterNode = addToGraph(CallDOMGetter, OpInfo(callDOMGetterData), OpInfo(prediction), thisNode, globalObject);
3639     } else
3640         callDOMGetterNode = addToGraph(CallDOMGetter, OpInfo(callDOMGetterData), OpInfo(prediction), thisNode);
3641     blessCallDOMGetter(callDOMGetterNode);
3642     set(result, callDOMGetterNode);
3643     return true;
3644 }
3645 
3646 bool ByteCodeParser::handleModuleNamespaceLoad(VirtualRegister result, SpeculatedType prediction, Node* base, GetByIdStatus getById)
3647 {
3648     if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell))
3649         return false;
3650     addToGraph(CheckCell, OpInfo(m_graph.freeze(getById.moduleNamespaceObject())), Edge(base, CellUse));
3651 
3652     addToGraph(FilterGetByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addGetByIdStatus(currentCodeOrigin(), getById)), base);
3653 
3654     // Ideally we wouldn&#39;t have to do this Phantom. But:
3655     //
3656     // For the constant case: we must do it because otherwise we would have no way of knowing
3657     // that the scope is live at OSR here.
3658     //
3659     // For the non-constant case: GetClosureVar could be DCE&#39;d, but baseline&#39;s implementation
3660     // won&#39;t be able to handle an Undefined scope.
3661     addToGraph(Phantom, base);
3662 
3663     // Constant folding in the bytecode parser is important for performance. This may not
3664     // have executed yet. If it hasn&#39;t, then we won&#39;t have a prediction. Lacking a
3665     // prediction, we&#39;d otherwise think that it has to exit. Then when it did execute, we
3666     // would recompile. But if we can fold it here, we avoid the exit.
3667     m_graph.freeze(getById.moduleEnvironment());
3668     if (JSValue value = m_graph.tryGetConstantClosureVar(getById.moduleEnvironment(), getById.scopeOffset())) {
3669         set(result, weakJSConstant(value));
3670         return true;
3671     }
3672     set(result, addToGraph(GetClosureVar, OpInfo(getById.scopeOffset().offset()), OpInfo(prediction), weakJSConstant(getById.moduleEnvironment())));
3673     return true;
3674 }
3675 
3676 template&lt;typename ChecksFunctor&gt;
3677 bool ByteCodeParser::handleTypedArrayConstructor(
3678     VirtualRegister result, InternalFunction* function, int registerOffset,
3679     int argumentCountIncludingThis, TypedArrayType type, const ChecksFunctor&amp; insertChecks)
3680 {
3681     if (!isTypedView(type))
3682         return false;
3683 
3684     if (function-&gt;classInfo() != constructorClassInfoForType(type))
3685         return false;
3686 
3687     if (function-&gt;globalObject(*m_vm) != m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject())
3688         return false;
3689 
3690     // We only have an intrinsic for the case where you say:
3691     //
3692     // new FooArray(blah);
3693     //
3694     // Of course, &#39;blah&#39; could be any of the following:
3695     //
3696     // - Integer, indicating that you want to allocate an array of that length.
3697     //   This is the thing we&#39;re hoping for, and what we can actually do meaningful
3698     //   optimizations for.
3699     //
3700     // - Array buffer, indicating that you want to create a view onto that _entire_
3701     //   buffer.
3702     //
3703     // - Non-buffer object, indicating that you want to create a copy of that
3704     //   object by pretending that it quacks like an array.
3705     //
3706     // - Anything else, indicating that you want to have an exception thrown at
3707     //   you.
3708     //
3709     // The intrinsic, NewTypedArray, will behave as if it could do any of these
3710     // things up until we do Fixup. Thereafter, if child1 (i.e. &#39;blah&#39;) is
3711     // predicted Int32, then we lock it in as a normal typed array allocation.
3712     // Otherwise, NewTypedArray turns into a totally opaque function call that
3713     // may clobber the world - by virtue of it accessing properties on what could
3714     // be an object.
3715     //
3716     // Note that although the generic form of NewTypedArray sounds sort of awful,
3717     // it is actually quite likely to be more efficient than a fully generic
3718     // Construct. So, we might want to think about making NewTypedArray variadic,
3719     // or else making Construct not super slow.
3720 
3721     if (argumentCountIncludingThis != 2)
3722         return false;
3723 
3724     if (!function-&gt;globalObject(*m_vm)-&gt;typedArrayStructureConcurrently(type))
3725         return false;
3726 
3727     insertChecks();
3728     set(result,
3729         addToGraph(NewTypedArray, OpInfo(type), get(virtualRegisterForArgument(1, registerOffset))));
3730     return true;
3731 }
3732 
3733 template&lt;typename ChecksFunctor&gt;
3734 bool ByteCodeParser::handleConstantInternalFunction(
3735     Node* callTargetNode, VirtualRegister result, InternalFunction* function, int registerOffset,
3736     int argumentCountIncludingThis, CodeSpecializationKind kind, SpeculatedType prediction, const ChecksFunctor&amp; insertChecks)
3737 {
3738     VERBOSE_LOG(&quot;    Handling constant internal function &quot;, JSValue(function), &quot;\n&quot;);
3739 
3740     // It so happens that the code below assumes that the result operand is valid. It&#39;s extremely
3741     // unlikely that the result operand would be invalid - you&#39;d have to call this via a setter call.
3742     if (!result.isValid())
3743         return false;
3744 
3745     if (kind == CodeForConstruct) {
3746         Node* newTargetNode = get(virtualRegisterForArgument(0, registerOffset));
3747         // We cannot handle the case where new.target != callee (i.e. a construct from a super call) because we
3748         // don&#39;t know what the prototype of the constructed object will be.
3749         // FIXME: If we have inlined super calls up to the call site, however, we should be able to figure out the structure. https://bugs.webkit.org/show_bug.cgi?id=152700
3750         if (newTargetNode != callTargetNode)
3751             return false;
3752     }
3753 
3754     if (function-&gt;classInfo() == ArrayConstructor::info()) {
3755         if (function-&gt;globalObject(*m_vm) != m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject())
3756             return false;
3757 
3758         insertChecks();
3759         if (argumentCountIncludingThis == 2) {
3760             set(result,
3761                 addToGraph(NewArrayWithSize, OpInfo(ArrayWithUndecided), get(virtualRegisterForArgument(1, registerOffset))));
3762             return true;
3763         }
3764 
3765         for (int i = 1; i &lt; argumentCountIncludingThis; ++i)
3766             addVarArgChild(get(virtualRegisterForArgument(i, registerOffset)));
3767         set(result,
3768             addToGraph(Node::VarArg, NewArray, OpInfo(ArrayWithUndecided), OpInfo(argumentCountIncludingThis - 1)));
3769         return true;
3770     }
3771 
3772     if (function-&gt;classInfo() == NumberConstructor::info()) {
3773         if (kind == CodeForConstruct)
3774             return false;
3775 
3776         insertChecks();
3777         if (argumentCountIncludingThis &lt;= 1)
3778             set(result, jsConstant(jsNumber(0)));
3779         else
3780             set(result, addToGraph(ToNumber, OpInfo(0), OpInfo(prediction), get(virtualRegisterForArgument(1, registerOffset))));
3781 
3782         return true;
3783     }
3784 
3785     if (function-&gt;classInfo() == StringConstructor::info()) {
3786         insertChecks();
3787 
3788         Node* resultNode;
3789 
3790         if (argumentCountIncludingThis &lt;= 1)
3791             resultNode = jsConstant(m_vm-&gt;smallStrings.emptyString());
3792         else
3793             resultNode = addToGraph(CallStringConstructor, get(virtualRegisterForArgument(1, registerOffset)));
3794 
3795         if (kind == CodeForConstruct)
3796             resultNode = addToGraph(NewStringObject, OpInfo(m_graph.registerStructure(function-&gt;globalObject(*m_vm)-&gt;stringObjectStructure())), resultNode);
3797 
3798         set(result, resultNode);
3799         return true;
3800     }
3801 
3802     if (function-&gt;classInfo() == SymbolConstructor::info() &amp;&amp; kind == CodeForCall) {
3803         insertChecks();
3804 
3805         Node* resultNode;
3806 
3807         if (argumentCountIncludingThis &lt;= 1)
3808             resultNode = addToGraph(NewSymbol);
3809         else
3810             resultNode = addToGraph(NewSymbol, addToGraph(ToString, get(virtualRegisterForArgument(1, registerOffset))));
3811 
3812         set(result, resultNode);
3813         return true;
3814     }
3815 
3816     // FIXME: This should handle construction as well. https://bugs.webkit.org/show_bug.cgi?id=155591
3817     if (function-&gt;classInfo() == ObjectConstructor::info() &amp;&amp; kind == CodeForCall) {
3818         insertChecks();
3819 
3820         Node* resultNode;
3821         if (argumentCountIncludingThis &lt;= 1)
3822             resultNode = addToGraph(NewObject, OpInfo(m_graph.registerStructure(function-&gt;globalObject(*m_vm)-&gt;objectStructureForObjectConstructor())));
3823         else
3824             resultNode = addToGraph(CallObjectConstructor, OpInfo(m_graph.freeze(function-&gt;globalObject(*m_vm))), OpInfo(prediction), get(virtualRegisterForArgument(1, registerOffset)));
3825         set(result, resultNode);
3826         return true;
3827     }
3828 
3829     for (unsigned typeIndex = 0; typeIndex &lt; NumberOfTypedArrayTypes; ++typeIndex) {
3830         bool handled = handleTypedArrayConstructor(
3831             result, function, registerOffset, argumentCountIncludingThis,
3832             indexToTypedArrayType(typeIndex), insertChecks);
3833         if (handled)
3834             return true;
3835     }
3836 
3837     return false;
3838 }
3839 
3840 Node* ByteCodeParser::handleGetByOffset(
3841     SpeculatedType prediction, Node* base, unsigned identifierNumber, PropertyOffset offset, NodeType op)
3842 {
3843     Node* propertyStorage;
3844     if (isInlineOffset(offset))
3845         propertyStorage = base;
3846     else
3847         propertyStorage = addToGraph(GetButterfly, base);
3848 
3849     StorageAccessData* data = m_graph.m_storageAccessData.add();
3850     data-&gt;offset = offset;
3851     data-&gt;identifierNumber = identifierNumber;
3852 
3853     Node* getByOffset = addToGraph(op, OpInfo(data), OpInfo(prediction), propertyStorage, base);
3854 
3855     return getByOffset;
3856 }
3857 
3858 Node* ByteCodeParser::handlePutByOffset(
3859     Node* base, unsigned identifier, PropertyOffset offset,
3860     Node* value)
3861 {
3862     Node* propertyStorage;
3863     if (isInlineOffset(offset))
3864         propertyStorage = base;
3865     else
3866         propertyStorage = addToGraph(GetButterfly, base);
3867 
3868     StorageAccessData* data = m_graph.m_storageAccessData.add();
3869     data-&gt;offset = offset;
3870     data-&gt;identifierNumber = identifier;
3871 
3872     Node* result = addToGraph(PutByOffset, OpInfo(data), propertyStorage, base, value);
3873 
3874     return result;
3875 }
3876 
3877 bool ByteCodeParser::check(const ObjectPropertyCondition&amp; condition)
3878 {
3879     if (!condition)
3880         return false;
3881 
3882     if (m_graph.watchCondition(condition))
3883         return true;
3884 
3885     Structure* structure = condition.object()-&gt;structure(*m_vm);
3886     if (!condition.structureEnsuresValidity(structure))
3887         return false;
3888 
3889     addToGraph(
3890         CheckStructure,
3891         OpInfo(m_graph.addStructureSet(structure)),
3892         weakJSConstant(condition.object()));
3893     return true;
3894 }
3895 
3896 GetByOffsetMethod ByteCodeParser::promoteToConstant(GetByOffsetMethod method)
3897 {
3898     if (method.kind() == GetByOffsetMethod::LoadFromPrototype
3899         &amp;&amp; method.prototype()-&gt;structure()-&gt;dfgShouldWatch()) {
3900         if (JSValue constant = m_graph.tryGetConstantProperty(method.prototype()-&gt;value(), method.prototype()-&gt;structure(), method.offset()))
3901             return GetByOffsetMethod::constant(m_graph.freeze(constant));
3902     }
3903 
3904     return method;
3905 }
3906 
3907 bool ByteCodeParser::needsDynamicLookup(ResolveType type, OpcodeID opcode)
3908 {
3909     ASSERT(opcode == op_resolve_scope || opcode == op_get_from_scope || opcode == op_put_to_scope);
3910 
3911     JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
3912     if (needsVarInjectionChecks(type) &amp;&amp; globalObject-&gt;varInjectionWatchpoint()-&gt;hasBeenInvalidated())
3913         return true;
3914 
3915     switch (type) {
3916     case GlobalProperty:
3917     case GlobalVar:
3918     case GlobalLexicalVar:
3919     case ClosureVar:
3920     case LocalClosureVar:
3921     case ModuleVar:
3922         return false;
3923 
3924     case UnresolvedProperty:
3925     case UnresolvedPropertyWithVarInjectionChecks: {
3926         // The heuristic for UnresolvedProperty scope accesses is we will ForceOSRExit if we
3927         // haven&#39;t exited from from this access before to let the baseline JIT try to better
3928         // cache the access. If we&#39;ve already exited from this operation, it&#39;s unlikely that
3929         // the baseline will come up with a better ResolveType and instead we will compile
3930         // this as a dynamic scope access.
3931 
3932         // We only track our heuristic through resolve_scope since resolve_scope will
3933         // dominate unresolved gets/puts on that scope.
3934         if (opcode != op_resolve_scope)
3935             return true;
3936 
3937         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, InadequateCoverage)) {
3938             // We&#39;ve already exited so give up on getting better ResolveType information.
3939             return true;
3940         }
3941 
3942         // We have not exited yet, so let&#39;s have the baseline get better ResolveType information for us.
3943         // This type of code is often seen when we tier up in a loop but haven&#39;t executed the part
3944         // of a function that comes after the loop.
3945         return false;
3946     }
3947 
3948     case Dynamic:
3949         return true;
3950 
3951     case GlobalPropertyWithVarInjectionChecks:
3952     case GlobalVarWithVarInjectionChecks:
3953     case GlobalLexicalVarWithVarInjectionChecks:
3954     case ClosureVarWithVarInjectionChecks:
3955         return false;
3956     }
3957 
3958     ASSERT_NOT_REACHED();
3959     return false;
3960 }
3961 
3962 GetByOffsetMethod ByteCodeParser::planLoad(const ObjectPropertyCondition&amp; condition)
3963 {
3964     VERBOSE_LOG(&quot;Planning a load: &quot;, condition, &quot;\n&quot;);
3965 
3966     // We might promote this to Equivalence, and a later DFG pass might also do such promotion
3967     // even if we fail, but for simplicity this cannot be asked to load an equivalence condition.
3968     // None of the clients of this method will request a load of an Equivalence condition anyway,
3969     // and supporting it would complicate the heuristics below.
3970     RELEASE_ASSERT(condition.kind() == PropertyCondition::Presence);
3971 
3972     // Here&#39;s the ranking of how to handle this, from most preferred to least preferred:
3973     //
3974     // 1) Watchpoint on an equivalence condition and return a constant node for the loaded value.
3975     //    No other code is emitted, and the structure of the base object is never registered.
3976     //    Hence this results in zero code and we won&#39;t jettison this compilation if the object
3977     //    transitions, even if the structure is watchable right now.
3978     //
3979     // 2) Need to emit a load, and the current structure of the base is going to be watched by the
3980     //    DFG anyway (i.e. dfgShouldWatch). Watch the structure and emit the load. Don&#39;t watch the
3981     //    condition, since the act of turning the base into a constant in IR will cause the DFG to
3982     //    watch the structure anyway and doing so would subsume watching the condition.
3983     //
3984     // 3) Need to emit a load, and the current structure of the base is watchable but not by the
3985     //    DFG (i.e. transitionWatchpointSetIsStillValid() and !dfgShouldWatchIfPossible()). Watch
3986     //    the condition, and emit a load.
3987     //
3988     // 4) Need to emit a load, and the current structure of the base is not watchable. Emit a
3989     //    structure check, and emit a load.
3990     //
3991     // 5) The condition does not hold. Give up and return null.
3992 
3993     // First, try to promote Presence to Equivalence. We do this before doing anything else
3994     // because it&#39;s the most profitable. Also, there are cases where the presence is watchable but
3995     // we don&#39;t want to watch it unless it became an equivalence (see the relationship between
3996     // (1), (2), and (3) above).
3997     ObjectPropertyCondition equivalenceCondition = condition.attemptToMakeEquivalenceWithoutBarrier(*m_vm);
3998     if (m_graph.watchCondition(equivalenceCondition))
3999         return GetByOffsetMethod::constant(m_graph.freeze(equivalenceCondition.requiredValue()));
4000 
4001     // At this point, we&#39;ll have to materialize the condition&#39;s base as a constant in DFG IR. Once
4002     // we do this, the frozen value will have its own idea of what the structure is. Use that from
4003     // now on just because it&#39;s less confusing.
4004     FrozenValue* base = m_graph.freeze(condition.object());
4005     Structure* structure = base-&gt;structure();
4006 
4007     // Check if the structure that we&#39;ve registered makes the condition hold. If not, just give
4008     // up. This is case (5) above.
4009     if (!condition.structureEnsuresValidity(structure))
4010         return GetByOffsetMethod();
4011 
4012     // If the structure is watched by the DFG already, then just use this fact to emit the load.
4013     // This is case (2) above.
4014     if (structure-&gt;dfgShouldWatch())
4015         return promoteToConstant(GetByOffsetMethod::loadFromPrototype(base, condition.offset()));
4016 
4017     // If we can watch the condition right now, then we can emit the load after watching it. This
4018     // is case (3) above.
4019     if (m_graph.watchCondition(condition))
4020         return promoteToConstant(GetByOffsetMethod::loadFromPrototype(base, condition.offset()));
4021 
4022     // We can&#39;t watch anything but we know that the current structure satisfies the condition. So,
4023     // check for that structure and then emit the load.
4024     addToGraph(
4025         CheckStructure,
4026         OpInfo(m_graph.addStructureSet(structure)),
4027         addToGraph(JSConstant, OpInfo(base)));
4028     return promoteToConstant(GetByOffsetMethod::loadFromPrototype(base, condition.offset()));
4029 }
4030 
4031 Node* ByteCodeParser::load(
4032     SpeculatedType prediction, unsigned identifierNumber, const GetByOffsetMethod&amp; method,
4033     NodeType op)
4034 {
4035     switch (method.kind()) {
4036     case GetByOffsetMethod::Invalid:
4037         return nullptr;
4038     case GetByOffsetMethod::Constant:
4039         return addToGraph(JSConstant, OpInfo(method.constant()));
4040     case GetByOffsetMethod::LoadFromPrototype: {
4041         Node* baseNode = addToGraph(JSConstant, OpInfo(method.prototype()));
4042         return handleGetByOffset(
4043             prediction, baseNode, identifierNumber, method.offset(), op);
4044     }
4045     case GetByOffsetMethod::Load:
4046         // Will never see this from planLoad().
4047         RELEASE_ASSERT_NOT_REACHED();
4048         return nullptr;
4049     }
4050 
4051     RELEASE_ASSERT_NOT_REACHED();
4052     return nullptr;
4053 }
4054 
4055 Node* ByteCodeParser::load(
4056     SpeculatedType prediction, const ObjectPropertyCondition&amp; condition, NodeType op)
4057 {
4058     GetByOffsetMethod method = planLoad(condition);
4059     return load(prediction, m_graph.identifiers().ensure(condition.uid()), method, op);
4060 }
4061 
4062 bool ByteCodeParser::check(const ObjectPropertyConditionSet&amp; conditionSet)
4063 {
4064     for (const ObjectPropertyCondition&amp; condition : conditionSet) {
4065         if (!check(condition))
4066             return false;
4067     }
4068     return true;
4069 }
4070 
4071 GetByOffsetMethod ByteCodeParser::planLoad(const ObjectPropertyConditionSet&amp; conditionSet)
4072 {
4073     VERBOSE_LOG(&quot;conditionSet = &quot;, conditionSet, &quot;\n&quot;);
4074 
4075     GetByOffsetMethod result;
4076     for (const ObjectPropertyCondition&amp; condition : conditionSet) {
4077         switch (condition.kind()) {
4078         case PropertyCondition::Presence:
4079             RELEASE_ASSERT(!result); // Should only see exactly one of these.
4080             result = planLoad(condition);
4081             if (!result)
4082                 return GetByOffsetMethod();
4083             break;
4084         default:
4085             if (!check(condition))
4086                 return GetByOffsetMethod();
4087             break;
4088         }
4089     }
4090     if (!result) {
4091         // We have a unset property.
4092         ASSERT(!conditionSet.numberOfConditionsWithKind(PropertyCondition::Presence));
4093         return GetByOffsetMethod::constant(m_constantUndefined);
4094     }
4095     return result;
4096 }
4097 
4098 Node* ByteCodeParser::load(
4099     SpeculatedType prediction, const ObjectPropertyConditionSet&amp; conditionSet, NodeType op)
4100 {
4101     GetByOffsetMethod method = planLoad(conditionSet);
4102     return load(
4103         prediction,
4104         m_graph.identifiers().ensure(conditionSet.slotBaseCondition().uid()),
4105         method, op);
4106 }
4107 
4108 ObjectPropertyCondition ByteCodeParser::presenceLike(
4109     JSObject* knownBase, UniquedStringImpl* uid, PropertyOffset offset, const StructureSet&amp; set)
4110 {
4111     if (set.isEmpty())
4112         return ObjectPropertyCondition();
4113     unsigned attributes;
4114     PropertyOffset firstOffset = set[0]-&gt;getConcurrently(uid, attributes);
4115     if (firstOffset != offset)
4116         return ObjectPropertyCondition();
4117     for (unsigned i = 1; i &lt; set.size(); ++i) {
4118         unsigned otherAttributes;
4119         PropertyOffset otherOffset = set[i]-&gt;getConcurrently(uid, otherAttributes);
4120         if (otherOffset != offset || otherAttributes != attributes)
4121             return ObjectPropertyCondition();
4122     }
4123     return ObjectPropertyCondition::presenceWithoutBarrier(knownBase, uid, offset, attributes);
4124 }
4125 
4126 bool ByteCodeParser::checkPresenceLike(
4127     JSObject* knownBase, UniquedStringImpl* uid, PropertyOffset offset, const StructureSet&amp; set)
4128 {
4129     return check(presenceLike(knownBase, uid, offset, set));
4130 }
4131 
4132 void ByteCodeParser::checkPresenceLike(
4133     Node* base, UniquedStringImpl* uid, PropertyOffset offset, const StructureSet&amp; set)
4134 {
4135     if (JSObject* knownBase = base-&gt;dynamicCastConstant&lt;JSObject*&gt;(*m_vm)) {
4136         if (checkPresenceLike(knownBase, uid, offset, set))
4137             return;
4138     }
4139 
4140     addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(set)), base);
4141 }
4142 
4143 template&lt;typename VariantType&gt;
4144 Node* ByteCodeParser::load(
4145     SpeculatedType prediction, Node* base, unsigned identifierNumber, const VariantType&amp; variant)
4146 {
4147     // Make sure backwards propagation knows that we&#39;ve used base.
4148     addToGraph(Phantom, base);
4149 
4150     bool needStructureCheck = true;
4151 
4152     UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
4153 
4154     if (JSObject* knownBase = base-&gt;dynamicCastConstant&lt;JSObject*&gt;(*m_vm)) {
4155         // Try to optimize away the structure check. Note that it&#39;s not worth doing anything about this
4156         // if the base&#39;s structure is watched.
4157         Structure* structure = base-&gt;constant()-&gt;structure();
4158         if (!structure-&gt;dfgShouldWatch()) {
4159             if (!variant.conditionSet().isEmpty()) {
4160                 // This means that we&#39;re loading from a prototype or we have a property miss. We expect
4161                 // the base not to have the property. We can only use ObjectPropertyCondition if all of
4162                 // the structures in the variant.structureSet() agree on the prototype (it would be
4163                 // hilariously rare if they didn&#39;t). Note that we are relying on structureSet() having
4164                 // at least one element. That will always be true here because of how GetByIdStatus/PutByIdStatus work.
4165 
4166                 // FIXME: right now, if we have an OPCS, we have mono proto. However, this will
4167                 // need to be changed in the future once we have a hybrid data structure for
4168                 // poly proto:
4169                 // https://bugs.webkit.org/show_bug.cgi?id=177339
4170                 JSObject* prototype = variant.structureSet()[0]-&gt;storedPrototypeObject();
4171                 bool allAgree = true;
4172                 for (unsigned i = 1; i &lt; variant.structureSet().size(); ++i) {
4173                     if (variant.structureSet()[i]-&gt;storedPrototypeObject() != prototype) {
4174                         allAgree = false;
4175                         break;
4176                     }
4177                 }
4178                 if (allAgree) {
4179                     ObjectPropertyCondition condition = ObjectPropertyCondition::absenceWithoutBarrier(
4180                         knownBase, uid, prototype);
4181                     if (check(condition))
4182                         needStructureCheck = false;
4183                 }
4184             } else {
4185                 // This means we&#39;re loading directly from base. We can avoid all of the code that follows
4186                 // if we can prove that the property is a constant. Otherwise, we try to prove that the
4187                 // property is watchably present, in which case we get rid of the structure check.
4188 
4189                 ObjectPropertyCondition presenceCondition =
4190                     presenceLike(knownBase, uid, variant.offset(), variant.structureSet());
4191                 if (presenceCondition) {
4192                     ObjectPropertyCondition equivalenceCondition =
4193                         presenceCondition.attemptToMakeEquivalenceWithoutBarrier(*m_vm);
4194                     if (m_graph.watchCondition(equivalenceCondition))
4195                         return weakJSConstant(equivalenceCondition.requiredValue());
4196 
4197                     if (check(presenceCondition))
4198                         needStructureCheck = false;
4199                 }
4200             }
4201         }
4202     }
4203 
4204     if (needStructureCheck)
4205         addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(variant.structureSet())), base);
4206 
4207     if (variant.isPropertyUnset()) {
4208         if (m_graph.watchConditions(variant.conditionSet()))
4209             return jsConstant(jsUndefined());
4210         return nullptr;
4211     }
4212 
4213     SpeculatedType loadPrediction;
4214     NodeType loadOp;
4215     if (variant.callLinkStatus() || variant.intrinsic() != NoIntrinsic) {
4216         loadPrediction = SpecCellOther;
4217         loadOp = GetGetterSetterByOffset;
4218     } else {
4219         loadPrediction = prediction;
4220         loadOp = GetByOffset;
4221     }
4222 
4223     Node* loadedValue;
4224     if (!variant.conditionSet().isEmpty())
4225         loadedValue = load(loadPrediction, variant.conditionSet(), loadOp);
4226     else {
4227         if (needStructureCheck &amp;&amp; base-&gt;hasConstant()) {
4228             // We did emit a structure check. That means that we have an opportunity to do constant folding
4229             // here, since we didn&#39;t do it above.
4230             JSValue constant = m_graph.tryGetConstantProperty(
4231                 base-&gt;asJSValue(), *m_graph.addStructureSet(variant.structureSet()), variant.offset());
4232             if (constant)
4233                 return weakJSConstant(constant);
4234         }
4235 
4236         loadedValue = handleGetByOffset(
4237             loadPrediction, base, identifierNumber, variant.offset(), loadOp);
4238     }
4239 
4240     return loadedValue;
4241 }
4242 
4243 Node* ByteCodeParser::store(Node* base, unsigned identifier, const PutByIdVariant&amp; variant, Node* value)
4244 {
4245     RELEASE_ASSERT(variant.kind() == PutByIdVariant::Replace);
4246 
4247     checkPresenceLike(base, m_graph.identifiers()[identifier], variant.offset(), variant.structure());
4248     return handlePutByOffset(base, identifier, variant.offset(), value);
4249 }
4250 
4251 void ByteCodeParser::handleGetById(
4252     VirtualRegister destination, SpeculatedType prediction, Node* base, unsigned identifierNumber,
4253     GetByIdStatus getByIdStatus, AccessType type, unsigned instructionSize)
4254 {
4255     // Attempt to reduce the set of things in the GetByIdStatus.
4256     if (base-&gt;op() == NewObject) {
4257         bool ok = true;
4258         for (unsigned i = m_currentBlock-&gt;size(); i--;) {
4259             Node* node = m_currentBlock-&gt;at(i);
4260             if (node == base)
4261                 break;
4262             if (writesOverlap(m_graph, node, JSCell_structureID)) {
4263                 ok = false;
4264                 break;
4265             }
4266         }
4267         if (ok)
4268             getByIdStatus.filter(base-&gt;structure().get());
4269     }
4270 
4271     NodeType getById;
4272     if (type == AccessType::Get)
4273         getById = getByIdStatus.makesCalls() ? GetByIdFlush : GetById;
4274     else if (type == AccessType::TryGet)
4275         getById = TryGetById;
4276     else
4277         getById = getByIdStatus.makesCalls() ? GetByIdDirectFlush : GetByIdDirect;
4278 
4279     if (getById != TryGetById &amp;&amp; getByIdStatus.isModuleNamespace()) {
4280         if (handleModuleNamespaceLoad(destination, prediction, base, getByIdStatus)) {
4281             if (UNLIKELY(m_graph.compilation()))
4282                 m_graph.compilation()-&gt;noticeInlinedGetById();
4283             return;
4284         }
4285     }
4286 
4287     // Special path for custom accessors since custom&#39;s offset does not have any meanings.
4288     // So, this is completely different from Simple one. But we have a chance to optimize it when we use DOMJIT.
4289     if (Options::useDOMJIT() &amp;&amp; getByIdStatus.isCustom()) {
4290         ASSERT(getByIdStatus.numVariants() == 1);
4291         ASSERT(!getByIdStatus.makesCalls());
4292         GetByIdVariant variant = getByIdStatus[0];
4293         ASSERT(variant.domAttribute());
4294         if (handleDOMJITGetter(destination, variant, base, identifierNumber, prediction)) {
4295             if (UNLIKELY(m_graph.compilation()))
4296                 m_graph.compilation()-&gt;noticeInlinedGetById();
4297             return;
4298         }
4299     }
4300 
4301     ASSERT(type == AccessType::Get || type == AccessType::GetDirect ||  !getByIdStatus.makesCalls());
4302     if (!getByIdStatus.isSimple() || !getByIdStatus.numVariants() || !Options::useAccessInlining()) {
4303         set(destination,
4304             addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4305         return;
4306     }
4307 
4308     // FIXME: If we use the GetByIdStatus for anything then we should record it and insert a node
4309     // after everything else (like the GetByOffset or whatever) that will filter the recorded
4310     // GetByIdStatus. That means that the constant folder also needs to do the same!
4311 
4312     if (getByIdStatus.numVariants() &gt; 1) {
4313         if (getByIdStatus.makesCalls() || !m_graph.m_plan.isFTL()
4314             || !Options::usePolymorphicAccessInlining()
4315             || getByIdStatus.numVariants() &gt; Options::maxPolymorphicAccessInliningListSize()) {
4316             set(destination,
4317                 addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4318             return;
4319         }
4320 
4321         addToGraph(FilterGetByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addGetByIdStatus(currentCodeOrigin(), getByIdStatus)), base);
4322 
4323         Vector&lt;MultiGetByOffsetCase, 2&gt; cases;
4324 
4325         // 1) Emit prototype structure checks for all chains. This could sort of maybe not be
4326         //    optimal, if there is some rarely executed case in the chain that requires a lot
4327         //    of checks and those checks are not watchpointable.
4328         for (const GetByIdVariant&amp; variant : getByIdStatus.variants()) {
4329             if (variant.intrinsic() != NoIntrinsic) {
4330                 set(destination,
4331                     addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4332                 return;
4333             }
4334 
4335             if (variant.conditionSet().isEmpty()) {
4336                 cases.append(
4337                     MultiGetByOffsetCase(
4338                         *m_graph.addStructureSet(variant.structureSet()),
4339                         GetByOffsetMethod::load(variant.offset())));
4340                 continue;
4341             }
4342 
4343             GetByOffsetMethod method = planLoad(variant.conditionSet());
4344             if (!method) {
4345                 set(destination,
4346                     addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4347                 return;
4348             }
4349 
4350             cases.append(MultiGetByOffsetCase(*m_graph.addStructureSet(variant.structureSet()), method));
4351         }
4352 
4353         if (UNLIKELY(m_graph.compilation()))
4354             m_graph.compilation()-&gt;noticeInlinedGetById();
4355 
4356         // 2) Emit a MultiGetByOffset
4357         MultiGetByOffsetData* data = m_graph.m_multiGetByOffsetData.add();
4358         data-&gt;cases = cases;
4359         data-&gt;identifierNumber = identifierNumber;
4360         set(destination,
4361             addToGraph(MultiGetByOffset, OpInfo(data), OpInfo(prediction), base));
4362         return;
4363     }
4364 
4365     addToGraph(FilterGetByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addGetByIdStatus(currentCodeOrigin(), getByIdStatus)), base);
4366 
4367     ASSERT(getByIdStatus.numVariants() == 1);
4368     GetByIdVariant variant = getByIdStatus[0];
4369 
4370     Node* loadedValue = load(prediction, base, identifierNumber, variant);
4371     if (!loadedValue) {
4372         set(destination,
4373             addToGraph(getById, OpInfo(identifierNumber), OpInfo(prediction), base));
4374         return;
4375     }
4376 
4377     if (UNLIKELY(m_graph.compilation()))
4378         m_graph.compilation()-&gt;noticeInlinedGetById();
4379 
4380     ASSERT(type == AccessType::Get || type == AccessType::GetDirect || !variant.callLinkStatus());
4381     if (!variant.callLinkStatus() &amp;&amp; variant.intrinsic() == NoIntrinsic) {
4382         set(destination, loadedValue);
4383         return;
4384     }
4385 
4386     Node* getter = addToGraph(GetGetter, loadedValue);
4387 
4388     if (handleIntrinsicGetter(destination, prediction, variant, base,
4389             [&amp;] () {
4390                 addToGraph(CheckCell, OpInfo(m_graph.freeze(variant.intrinsicFunction())), getter);
4391             })) {
4392         addToGraph(Phantom, base);
4393         return;
4394     }
4395 
4396     ASSERT(variant.intrinsic() == NoIntrinsic);
4397 
4398     // Make a call. We don&#39;t try to get fancy with using the smallest operand number because
4399     // the stack layout phase should compress the stack anyway.
4400 
4401     unsigned numberOfParameters = 0;
4402     numberOfParameters++; // The &#39;this&#39; argument.
4403     numberOfParameters++; // True return PC.
4404 
4405     // Start with a register offset that corresponds to the last in-use register.
4406     int registerOffset = virtualRegisterForLocal(
4407         m_inlineStackTop-&gt;m_profiledBlock-&gt;numCalleeLocals() - 1).offset();
4408     registerOffset -= numberOfParameters;
4409     registerOffset -= CallFrame::headerSizeInRegisters;
4410 
4411     // Get the alignment right.
4412     registerOffset = -WTF::roundUpToMultipleOf(
4413         stackAlignmentRegisters(),
4414         -registerOffset);
4415 
4416     ensureLocals(
4417         m_inlineStackTop-&gt;remapOperand(
4418             VirtualRegister(registerOffset)).toLocal());
4419 
4420     // Issue SetLocals. This has two effects:
4421     // 1) That&#39;s how handleCall() sees the arguments.
4422     // 2) If we inline then this ensures that the arguments are flushed so that if you use
4423     //    the dreaded arguments object on the getter, the right things happen. Well, sort of -
4424     //    since we only really care about &#39;this&#39; in this case. But we&#39;re not going to take that
4425     //    shortcut.
<a name="47" id="anc47"></a><span class="line-modified">4426     set(virtualRegisterForArgument(0, registerOffset), base, ImmediateNakedSet);</span>

4427 
4428     // We&#39;ve set some locals, but they are not user-visible. It&#39;s still OK to exit from here.
4429     m_exitOK = true;
4430     addToGraph(ExitOK);
4431 
4432     handleCall(
4433         destination, Call, InlineCallFrame::GetterCall, instructionSize,
4434         getter, numberOfParameters - 1, registerOffset, *variant.callLinkStatus(), prediction);
4435 }
4436 
4437 void ByteCodeParser::emitPutById(
4438     Node* base, unsigned identifierNumber, Node* value, const PutByIdStatus&amp; putByIdStatus, bool isDirect)
4439 {
4440     if (isDirect)
4441         addToGraph(PutByIdDirect, OpInfo(identifierNumber), base, value);
4442     else
4443         addToGraph(putByIdStatus.makesCalls() ? PutByIdFlush : PutById, OpInfo(identifierNumber), base, value);
4444 }
4445 
4446 void ByteCodeParser::handlePutById(
4447     Node* base, unsigned identifierNumber, Node* value,
4448     const PutByIdStatus&amp; putByIdStatus, bool isDirect, unsigned instructionSize)
4449 {
4450     if (!putByIdStatus.isSimple() || !putByIdStatus.numVariants() || !Options::useAccessInlining()) {
4451         if (!putByIdStatus.isSet())
4452             addToGraph(ForceOSRExit);
4453         emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4454         return;
4455     }
4456 
4457     if (putByIdStatus.numVariants() &gt; 1) {
4458         if (!m_graph.m_plan.isFTL() || putByIdStatus.makesCalls()
4459             || !Options::usePolymorphicAccessInlining()
4460             || putByIdStatus.numVariants() &gt; Options::maxPolymorphicAccessInliningListSize()) {
4461             emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4462             return;
4463         }
4464 
4465         if (!isDirect) {
4466             for (unsigned variantIndex = putByIdStatus.numVariants(); variantIndex--;) {
4467                 if (putByIdStatus[variantIndex].kind() != PutByIdVariant::Transition)
4468                     continue;
4469                 if (!check(putByIdStatus[variantIndex].conditionSet())) {
4470                     emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4471                     return;
4472                 }
4473             }
4474         }
4475 
4476         if (UNLIKELY(m_graph.compilation()))
4477             m_graph.compilation()-&gt;noticeInlinedPutById();
4478 
4479         addToGraph(FilterPutByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addPutByIdStatus(currentCodeOrigin(), putByIdStatus)), base);
4480 
4481         for (const PutByIdVariant&amp; variant : putByIdStatus.variants()) {
4482             for (Structure* structure : variant.oldStructure())
4483                 m_graph.registerStructure(structure);
4484             if (variant.kind() == PutByIdVariant::Transition)
4485                 m_graph.registerStructure(variant.newStructure());
4486         }
4487 
4488         MultiPutByOffsetData* data = m_graph.m_multiPutByOffsetData.add();
4489         data-&gt;variants = putByIdStatus.variants();
4490         data-&gt;identifierNumber = identifierNumber;
4491         addToGraph(MultiPutByOffset, OpInfo(data), base, value);
4492         return;
4493     }
4494 
4495     ASSERT(putByIdStatus.numVariants() == 1);
4496     const PutByIdVariant&amp; variant = putByIdStatus[0];
4497 
4498     switch (variant.kind()) {
4499     case PutByIdVariant::Replace: {
4500         addToGraph(FilterPutByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addPutByIdStatus(currentCodeOrigin(), putByIdStatus)), base);
4501 
4502         store(base, identifierNumber, variant, value);
4503         if (UNLIKELY(m_graph.compilation()))
4504             m_graph.compilation()-&gt;noticeInlinedPutById();
4505         return;
4506     }
4507 
4508     case PutByIdVariant::Transition: {
4509         addToGraph(FilterPutByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addPutByIdStatus(currentCodeOrigin(), putByIdStatus)), base);
4510 
4511         addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(variant.oldStructure())), base);
4512         if (!check(variant.conditionSet())) {
4513             emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4514             return;
4515         }
4516 
4517         ASSERT(variant.oldStructureForTransition()-&gt;transitionWatchpointSetHasBeenInvalidated());
4518 
4519         Node* propertyStorage;
4520         Transition* transition = m_graph.m_transitions.add(
4521             m_graph.registerStructure(variant.oldStructureForTransition()), m_graph.registerStructure(variant.newStructure()));
4522 
4523         if (variant.reallocatesStorage()) {
4524 
4525             // If we&#39;re growing the property storage then it must be because we&#39;re
4526             // storing into the out-of-line storage.
4527             ASSERT(!isInlineOffset(variant.offset()));
4528 
4529             if (!variant.oldStructureForTransition()-&gt;outOfLineCapacity()) {
4530                 propertyStorage = addToGraph(
4531                     AllocatePropertyStorage, OpInfo(transition), base);
4532             } else {
4533                 propertyStorage = addToGraph(
4534                     ReallocatePropertyStorage, OpInfo(transition),
4535                     base, addToGraph(GetButterfly, base));
4536             }
4537         } else {
4538             if (isInlineOffset(variant.offset()))
4539                 propertyStorage = base;
4540             else
4541                 propertyStorage = addToGraph(GetButterfly, base);
4542         }
4543 
4544         StorageAccessData* data = m_graph.m_storageAccessData.add();
4545         data-&gt;offset = variant.offset();
4546         data-&gt;identifierNumber = identifierNumber;
4547 
4548         // NOTE: We could GC at this point because someone could insert an operation that GCs.
4549         // That&#39;s fine because:
4550         // - Things already in the structure will get scanned because we haven&#39;t messed with
4551         //   the object yet.
4552         // - The value we are fixing to put is going to be kept live by OSR exit handling. So
4553         //   if the GC does a conservative scan here it will see the new value.
4554 
4555         addToGraph(
4556             PutByOffset,
4557             OpInfo(data),
4558             propertyStorage,
4559             base,
4560             value);
4561 
4562         if (variant.reallocatesStorage())
4563             addToGraph(NukeStructureAndSetButterfly, base, propertyStorage);
4564 
4565         // FIXME: PutStructure goes last until we fix either
4566         // https://bugs.webkit.org/show_bug.cgi?id=142921 or
4567         // https://bugs.webkit.org/show_bug.cgi?id=142924.
4568         addToGraph(PutStructure, OpInfo(transition), base);
4569 
4570         if (UNLIKELY(m_graph.compilation()))
4571             m_graph.compilation()-&gt;noticeInlinedPutById();
4572         return;
4573     }
4574 
4575     case PutByIdVariant::Setter: {
4576         addToGraph(FilterPutByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addPutByIdStatus(currentCodeOrigin(), putByIdStatus)), base);
4577 
4578         Node* loadedValue = load(SpecCellOther, base, identifierNumber, variant);
4579         if (!loadedValue) {
4580             emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4581             return;
4582         }
4583 
4584         Node* setter = addToGraph(GetSetter, loadedValue);
4585 
4586         // Make a call. We don&#39;t try to get fancy with using the smallest operand number because
4587         // the stack layout phase should compress the stack anyway.
4588 
4589         unsigned numberOfParameters = 0;
4590         numberOfParameters++; // The &#39;this&#39; argument.
4591         numberOfParameters++; // The new value.
4592         numberOfParameters++; // True return PC.
4593 
4594         // Start with a register offset that corresponds to the last in-use register.
4595         int registerOffset = virtualRegisterForLocal(
4596             m_inlineStackTop-&gt;m_profiledBlock-&gt;numCalleeLocals() - 1).offset();
4597         registerOffset -= numberOfParameters;
4598         registerOffset -= CallFrame::headerSizeInRegisters;
4599 
4600         // Get the alignment right.
4601         registerOffset = -WTF::roundUpToMultipleOf(
4602             stackAlignmentRegisters(),
4603             -registerOffset);
4604 
4605         ensureLocals(
4606             m_inlineStackTop-&gt;remapOperand(
4607                 VirtualRegister(registerOffset)).toLocal());
4608 
<a name="48" id="anc48"></a><span class="line-modified">4609         set(virtualRegisterForArgument(0, registerOffset), base, ImmediateNakedSet);</span>
<span class="line-modified">4610         set(virtualRegisterForArgument(1, registerOffset), value, ImmediateNakedSet);</span>

4611 
4612         // We&#39;ve set some locals, but they are not user-visible. It&#39;s still OK to exit from here.
4613         m_exitOK = true;
4614         addToGraph(ExitOK);
4615 
4616         handleCall(
4617             VirtualRegister(), Call, InlineCallFrame::SetterCall,
4618             instructionSize, setter, numberOfParameters - 1, registerOffset,
4619             *variant.callLinkStatus(), SpecOther);
4620         return;
4621     }
4622 
4623     default: {
4624         emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4625         return;
4626     } }
4627 }
4628 
4629 void ByteCodeParser::prepareToParseBlock()
4630 {
4631     clearCaches();
4632     ASSERT(m_setLocalQueue.isEmpty());
4633 }
4634 
4635 void ByteCodeParser::clearCaches()
4636 {
4637     m_constants.shrink(0);
4638 }
4639 
4640 template&lt;typename Op&gt;
4641 void ByteCodeParser::parseGetById(const Instruction* currentInstruction)
4642 {
4643     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
4644     SpeculatedType prediction = getPrediction();
4645 
4646     Node* base = get(bytecode.m_base);
4647     unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
4648 
4649     UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
4650     GetByIdStatus getByIdStatus = GetByIdStatus::computeFor(
4651         m_inlineStackTop-&gt;m_profiledBlock,
4652         m_inlineStackTop-&gt;m_baselineMap, m_icContextStack,
4653         currentCodeOrigin(), uid);
4654 
4655     AccessType type = AccessType::Get;
4656     unsigned opcodeLength = currentInstruction-&gt;size();
4657     if (Op::opcodeID == op_try_get_by_id)
4658         type = AccessType::TryGet;
4659     else if (Op::opcodeID == op_get_by_id_direct)
4660         type = AccessType::GetDirect;
4661 
4662     handleGetById(
4663         bytecode.m_dst, prediction, base, identifierNumber, getByIdStatus, type, opcodeLength);
4664 
4665 }
4666 
4667 static uint64_t makeDynamicVarOpInfo(unsigned identifierNumber, unsigned getPutInfo)
4668 {
4669     static_assert(sizeof(identifierNumber) == 4,
4670         &quot;We cannot fit identifierNumber into the high bits of m_opInfo&quot;);
4671     return static_cast&lt;uint64_t&gt;(identifierNumber) | (static_cast&lt;uint64_t&gt;(getPutInfo) &lt;&lt; 32);
4672 }
4673 
4674 // The idiom:
4675 //     if (true) { ...; goto label; } else label: continue
4676 // Allows using NEXT_OPCODE as a statement, even in unbraced if+else, while containing a `continue`.
4677 // The more common idiom:
4678 //     do { ...; } while (false)
4679 // Doesn&#39;t allow using `continue`.
4680 #define NEXT_OPCODE(name) \
4681     if (true) { \
4682         m_currentIndex += currentInstruction-&gt;size(); \
4683         goto WTF_CONCAT(NEXT_OPCODE_, __LINE__); /* Need a unique label: usable more than once per function. */ \
4684     } else \
4685         WTF_CONCAT(NEXT_OPCODE_, __LINE__): \
4686     continue
4687 
4688 #define LAST_OPCODE_LINKED(name) do { \
4689         m_currentIndex += currentInstruction-&gt;size(); \
4690         m_exitOK = false; \
4691         return; \
4692     } while (false)
4693 
4694 #define LAST_OPCODE(name) \
4695     do { \
4696         if (m_currentBlock-&gt;terminal()) { \
4697             switch (m_currentBlock-&gt;terminal()-&gt;op()) { \
4698             case Jump: \
4699             case Branch: \
4700             case Switch: \
4701                 ASSERT(!m_currentBlock-&gt;isLinked); \
4702                 m_inlineStackTop-&gt;m_unlinkedBlocks.append(m_currentBlock); \
4703                 break;\
4704             default: break; \
4705             } \
4706         } \
4707         LAST_OPCODE_LINKED(name); \
4708     } while (false)
4709 
4710 void ByteCodeParser::parseBlock(unsigned limit)
4711 {
4712     auto&amp; instructions = m_inlineStackTop-&gt;m_codeBlock-&gt;instructions();
4713     unsigned blockBegin = m_currentIndex;
4714 
4715     // If we are the first basic block, introduce markers for arguments. This allows
4716     // us to track if a use of an argument may use the actual argument passed, as
4717     // opposed to using a value we set explicitly.
4718     if (m_currentBlock == m_graph.block(0) &amp;&amp; !inlineCallFrame()) {
4719         auto addResult = m_graph.m_rootToArguments.add(m_currentBlock, ArgumentsVector());
4720         RELEASE_ASSERT(addResult.isNewEntry);
4721         ArgumentsVector&amp; entrypointArguments = addResult.iterator-&gt;value;
4722         entrypointArguments.resize(m_numArguments);
4723 
<a name="49" id="anc49"></a><span class="line-modified">4724         // We will emit SetArgumentDefinitely nodes. They don&#39;t exit, but we&#39;re at the top of an op_enter so</span>
4725         // exitOK = true.
4726         m_exitOK = true;
4727         for (unsigned argument = 0; argument &lt; m_numArguments; ++argument) {
4728             VariableAccessData* variable = newVariableAccessData(
4729                 virtualRegisterForArgument(argument));
4730             variable-&gt;mergeStructureCheckHoistingFailed(
4731                 m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache));
4732             variable-&gt;mergeCheckArrayHoistingFailed(
4733                 m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIndexingType));
4734 
<a name="50" id="anc50"></a><span class="line-modified">4735             Node* setArgument = addToGraph(SetArgumentDefinitely, OpInfo(variable));</span>
4736             entrypointArguments[argument] = setArgument;
4737             m_currentBlock-&gt;variablesAtTail.setArgumentFirstTime(argument, setArgument);
4738         }
4739     }
4740 
4741     CodeBlock* codeBlock = m_inlineStackTop-&gt;m_codeBlock;
4742 
4743     auto jumpTarget = [&amp;](int target) {
4744         if (target)
4745             return target;
4746         return codeBlock-&gt;outOfLineJumpOffset(m_currentInstruction);
4747     };
4748 
4749     while (true) {
4750         // We&#39;re staring at a new bytecode instruction. So we once again have a place that we can exit
4751         // to.
4752         m_exitOK = true;
4753 
4754         processSetLocalQueue();
4755 
4756         // Don&#39;t extend over jump destinations.
4757         if (m_currentIndex == limit) {
4758             // Ordinarily we want to plant a jump. But refuse to do this if the block is
4759             // empty. This is a special case for inlining, which might otherwise create
4760             // some empty blocks in some cases. When parseBlock() returns with an empty
4761             // block, it will get repurposed instead of creating a new one. Note that this
4762             // logic relies on every bytecode resulting in one or more nodes, which would
4763             // be true anyway except for op_loop_hint, which emits a Phantom to force this
4764             // to be true.
4765 
4766             if (!m_currentBlock-&gt;isEmpty())
4767                 addJumpTo(m_currentIndex);
4768             return;
4769         }
4770 
4771         // Switch on the current bytecode opcode.
4772         const Instruction* currentInstruction = instructions.at(m_currentIndex).ptr();
4773         m_currentInstruction = currentInstruction; // Some methods want to use this, and we&#39;d rather not thread it through calls.
4774         OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
4775 
4776         VERBOSE_LOG(&quot;    parsing &quot;, currentCodeOrigin(), &quot;: &quot;, opcodeID, &quot;\n&quot;);
4777 
4778         if (UNLIKELY(m_graph.compilation())) {
4779             addToGraph(CountExecution, OpInfo(m_graph.compilation()-&gt;executionCounterFor(
4780                 Profiler::OriginStack(*m_vm-&gt;m_perBytecodeProfiler, m_codeBlock, currentCodeOrigin()))));
4781         }
4782 
4783         switch (opcodeID) {
4784 
4785         // === Function entry opcodes ===
4786 
4787         case op_enter: {
<a name="51" id="anc51"></a><span class="line-added">4788             addToGraph(Options::usePollingTraps() ? CheckTraps : InvalidationPoint);</span>
4789             Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
4790             // Initialize all locals to undefined.
4791             for (int i = 0; i &lt; m_inlineStackTop-&gt;m_codeBlock-&gt;numVars(); ++i)
4792                 set(virtualRegisterForLocal(i), undefined, ImmediateNakedSet);
<a name="52" id="anc52"></a>
4793             NEXT_OPCODE(op_enter);
4794         }
4795 
4796         case op_to_this: {
4797             Node* op1 = getThis();
4798             auto&amp; metadata = currentInstruction-&gt;as&lt;OpToThis&gt;().metadata(codeBlock);
<a name="53" id="anc53"></a><span class="line-modified">4799             StructureID cachedStructureID = metadata.m_cachedStructureID;</span>
<span class="line-added">4800             Structure* cachedStructure = nullptr;</span>
<span class="line-added">4801             if (cachedStructureID)</span>
<span class="line-added">4802                 cachedStructure = m_vm-&gt;heap.structureIDTable().get(cachedStructureID);</span>
4803             if (metadata.m_toThisStatus != ToThisOK
4804                 || !cachedStructure
4805                 || cachedStructure-&gt;classInfo()-&gt;methodTable.toThis != JSObject::info()-&gt;methodTable.toThis
4806                 || m_inlineStackTop-&gt;m_profiledBlock-&gt;couldTakeSlowCase(m_currentIndex)
4807                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache)
4808                 || (op1-&gt;op() == GetLocal &amp;&amp; op1-&gt;variableAccessData()-&gt;structureCheckHoistingFailed())) {
4809                 setThis(addToGraph(ToThis, OpInfo(), OpInfo(getPrediction()), op1));
4810             } else {
4811                 addToGraph(
4812                     CheckStructure,
4813                     OpInfo(m_graph.addStructureSet(cachedStructure)),
4814                     op1);
4815             }
4816             NEXT_OPCODE(op_to_this);
4817         }
4818 
4819         case op_create_this: {
4820             auto bytecode = currentInstruction-&gt;as&lt;OpCreateThis&gt;();
4821             Node* callee = get(VirtualRegister(bytecode.m_callee));
4822 
4823             JSFunction* function = callee-&gt;dynamicCastConstant&lt;JSFunction*&gt;(*m_vm);
4824             if (!function) {
4825                 JSCell* cachedFunction = bytecode.metadata(codeBlock).m_cachedCallee.unvalidatedGet();
4826                 if (cachedFunction
4827                     &amp;&amp; cachedFunction != JSCell::seenMultipleCalleeObjects()
4828                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
4829                     ASSERT(cachedFunction-&gt;inherits&lt;JSFunction&gt;(*m_vm));
4830 
4831                     FrozenValue* frozen = m_graph.freeze(cachedFunction);
4832                     addToGraph(CheckCell, OpInfo(frozen), callee);
4833 
4834                     function = static_cast&lt;JSFunction*&gt;(cachedFunction);
4835                 }
4836             }
4837 
4838             bool alreadyEmitted = false;
4839             if (function) {
4840                 if (FunctionRareData* rareData = function-&gt;rareData()) {
4841                     if (rareData-&gt;allocationProfileWatchpointSet().isStillValid()) {
4842                         Structure* structure = rareData-&gt;objectAllocationStructure();
4843                         JSObject* prototype = rareData-&gt;objectAllocationPrototype();
4844                         if (structure
4845                             &amp;&amp; (structure-&gt;hasMonoProto() || prototype)
4846                             &amp;&amp; rareData-&gt;allocationProfileWatchpointSet().isStillValid()) {
4847 
4848                             m_graph.freeze(rareData);
4849                             m_graph.watchpoints().addLazily(rareData-&gt;allocationProfileWatchpointSet());
4850 
4851                             // The callee is still live up to this point.
4852                             addToGraph(Phantom, callee);
4853                             Node* object = addToGraph(NewObject, OpInfo(m_graph.registerStructure(structure)));
4854                             if (structure-&gt;hasPolyProto()) {
4855                                 StorageAccessData* data = m_graph.m_storageAccessData.add();
4856                                 data-&gt;offset = knownPolyProtoOffset;
4857                                 data-&gt;identifierNumber = m_graph.identifiers().ensure(m_graph.m_vm.propertyNames-&gt;builtinNames().polyProtoName().impl());
4858                                 ASSERT(isInlineOffset(knownPolyProtoOffset));
4859                                 addToGraph(PutByOffset, OpInfo(data), object, object, weakJSConstant(prototype));
4860                             }
4861                             set(VirtualRegister(bytecode.m_dst), object);
4862                             alreadyEmitted = true;
4863                         }
4864                     }
4865                 }
4866             }
4867             if (!alreadyEmitted) {
4868                 set(VirtualRegister(bytecode.m_dst),
4869                     addToGraph(CreateThis, OpInfo(bytecode.m_inlineCapacity), callee));
4870             }
4871             NEXT_OPCODE(op_create_this);
4872         }
4873 
4874         case op_new_object: {
4875             auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
4876             set(bytecode.m_dst,
4877                 addToGraph(NewObject,
4878                     OpInfo(m_graph.registerStructure(bytecode.metadata(codeBlock).m_objectAllocationProfile.structure()))));
4879             NEXT_OPCODE(op_new_object);
4880         }
4881 
4882         case op_new_array: {
4883             auto bytecode = currentInstruction-&gt;as&lt;OpNewArray&gt;();
4884             int startOperand = bytecode.m_argv.offset();
4885             int numOperands = bytecode.m_argc;
4886             ArrayAllocationProfile&amp; profile = bytecode.metadata(codeBlock).m_arrayAllocationProfile;
4887             for (int operandIdx = startOperand; operandIdx &gt; startOperand - numOperands; --operandIdx)
4888                 addVarArgChild(get(VirtualRegister(operandIdx)));
<a name="54" id="anc54"></a><span class="line-modified">4889             unsigned vectorLengthHint = std::max&lt;unsigned&gt;(profile.vectorLengthHintConcurrently(), numOperands);</span>
<span class="line-modified">4890             set(bytecode.m_dst, addToGraph(Node::VarArg, NewArray, OpInfo(profile.selectIndexingTypeConcurrently()), OpInfo(vectorLengthHint)));</span>
4891             NEXT_OPCODE(op_new_array);
4892         }
4893 
4894         case op_new_array_with_spread: {
4895             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSpread&gt;();
4896             int startOperand = bytecode.m_argv.offset();
4897             int numOperands = bytecode.m_argc;
4898             const BitVector&amp; bitVector = m_inlineStackTop-&gt;m_profiledBlock-&gt;unlinkedCodeBlock()-&gt;bitVector(bytecode.m_bitVector);
4899             for (int operandIdx = startOperand; operandIdx &gt; startOperand - numOperands; --operandIdx)
4900                 addVarArgChild(get(VirtualRegister(operandIdx)));
4901 
4902             BitVector* copy = m_graph.m_bitVectors.add(bitVector);
4903             ASSERT(*copy == bitVector);
4904 
4905             set(bytecode.m_dst,
4906                 addToGraph(Node::VarArg, NewArrayWithSpread, OpInfo(copy)));
4907             NEXT_OPCODE(op_new_array_with_spread);
4908         }
4909 
4910         case op_spread: {
4911             auto bytecode = currentInstruction-&gt;as&lt;OpSpread&gt;();
4912             set(bytecode.m_dst,
4913                 addToGraph(Spread, get(bytecode.m_argument)));
4914             NEXT_OPCODE(op_spread);
4915         }
4916 
4917         case op_new_array_with_size: {
4918             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSize&gt;();
4919             ArrayAllocationProfile&amp; profile = bytecode.metadata(codeBlock).m_arrayAllocationProfile;
<a name="55" id="anc55"></a><span class="line-modified">4920             set(bytecode.m_dst, addToGraph(NewArrayWithSize, OpInfo(profile.selectIndexingTypeConcurrently()), get(bytecode.m_length)));</span>
4921             NEXT_OPCODE(op_new_array_with_size);
4922         }
4923 
4924         case op_new_array_buffer: {
4925             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayBuffer&gt;();
4926             // Unfortunately, we can&#39;t allocate a new JSImmutableButterfly if the profile tells us new information because we
4927             // cannot allocate from compilation threads.
4928             WTF::loadLoadFence();
4929             FrozenValue* frozen = get(VirtualRegister(bytecode.m_immutableButterfly))-&gt;constant();
4930             WTF::loadLoadFence();
4931             JSImmutableButterfly* immutableButterfly = frozen-&gt;cast&lt;JSImmutableButterfly*&gt;();
4932             NewArrayBufferData data { };
4933             data.indexingMode = immutableButterfly-&gt;indexingMode();
4934             data.vectorLengthHint = immutableButterfly-&gt;toButterfly()-&gt;vectorLength();
4935 
4936             set(VirtualRegister(bytecode.m_dst), addToGraph(NewArrayBuffer, OpInfo(frozen), OpInfo(data.asQuadWord)));
4937             NEXT_OPCODE(op_new_array_buffer);
4938         }
4939 
4940         case op_new_regexp: {
4941             auto bytecode = currentInstruction-&gt;as&lt;OpNewRegexp&gt;();
4942             ASSERT(bytecode.m_regexp.isConstant());
4943             FrozenValue* frozenRegExp = m_graph.freezeStrong(m_inlineStackTop-&gt;m_codeBlock-&gt;getConstant(bytecode.m_regexp.offset()));
4944             set(bytecode.m_dst, addToGraph(NewRegexp, OpInfo(frozenRegExp), jsConstant(jsNumber(0))));
4945             NEXT_OPCODE(op_new_regexp);
4946         }
4947 
4948         case op_get_rest_length: {
4949             auto bytecode = currentInstruction-&gt;as&lt;OpGetRestLength&gt;();
4950             InlineCallFrame* inlineCallFrame = this-&gt;inlineCallFrame();
4951             Node* length;
4952             if (inlineCallFrame &amp;&amp; !inlineCallFrame-&gt;isVarargs()) {
4953                 unsigned argumentsLength = inlineCallFrame-&gt;argumentCountIncludingThis - 1;
4954                 JSValue restLength;
4955                 if (argumentsLength &lt;= bytecode.m_numParametersToSkip)
4956                     restLength = jsNumber(0);
4957                 else
4958                     restLength = jsNumber(argumentsLength - bytecode.m_numParametersToSkip);
4959 
4960                 length = jsConstant(restLength);
4961             } else
4962                 length = addToGraph(GetRestLength, OpInfo(bytecode.m_numParametersToSkip));
4963             set(bytecode.m_dst, length);
4964             NEXT_OPCODE(op_get_rest_length);
4965         }
4966 
4967         case op_create_rest: {
4968             auto bytecode = currentInstruction-&gt;as&lt;OpCreateRest&gt;();
4969             noticeArgumentsUse();
4970             Node* arrayLength = get(bytecode.m_arraySize);
4971             set(bytecode.m_dst,
4972                 addToGraph(CreateRest, OpInfo(bytecode.m_numParametersToSkip), arrayLength));
4973             NEXT_OPCODE(op_create_rest);
4974         }
4975 
4976         // === Bitwise operations ===
4977 
4978         case op_bitnot: {
4979             auto bytecode = currentInstruction-&gt;as&lt;OpBitnot&gt;();
<a name="56" id="anc56"></a><span class="line-added">4980             SpeculatedType prediction = getPrediction();</span>
4981             Node* op1 = get(bytecode.m_operand);
<a name="57" id="anc57"></a><span class="line-modified">4982             if (op1-&gt;hasNumberOrAnyIntResult())</span>
<span class="line-added">4983                 set(bytecode.m_dst, addToGraph(ArithBitNot, op1));</span>
<span class="line-added">4984             else</span>
<span class="line-added">4985                 set(bytecode.m_dst, addToGraph(ValueBitNot, OpInfo(), OpInfo(prediction), op1));</span>
4986             NEXT_OPCODE(op_bitnot);
4987         }
4988 
4989         case op_bitand: {
4990             auto bytecode = currentInstruction-&gt;as&lt;OpBitand&gt;();
4991             SpeculatedType prediction = getPrediction();
4992             Node* op1 = get(bytecode.m_lhs);
4993             Node* op2 = get(bytecode.m_rhs);
4994             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
4995                 set(bytecode.m_dst, addToGraph(ArithBitAnd, op1, op2));
4996             else
4997                 set(bytecode.m_dst, addToGraph(ValueBitAnd, OpInfo(), OpInfo(prediction), op1, op2));
4998             NEXT_OPCODE(op_bitand);
4999         }
5000 
5001         case op_bitor: {
5002             auto bytecode = currentInstruction-&gt;as&lt;OpBitor&gt;();
5003             SpeculatedType prediction = getPrediction();
5004             Node* op1 = get(bytecode.m_lhs);
5005             Node* op2 = get(bytecode.m_rhs);
5006             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
5007                 set(bytecode.m_dst, addToGraph(ArithBitOr, op1, op2));
5008             else
5009                 set(bytecode.m_dst, addToGraph(ValueBitOr, OpInfo(), OpInfo(prediction), op1, op2));
5010             NEXT_OPCODE(op_bitor);
5011         }
5012 
5013         case op_bitxor: {
5014             auto bytecode = currentInstruction-&gt;as&lt;OpBitxor&gt;();
5015             SpeculatedType prediction = getPrediction();
5016             Node* op1 = get(bytecode.m_lhs);
5017             Node* op2 = get(bytecode.m_rhs);
5018             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
5019                 set(bytecode.m_dst, addToGraph(ArithBitXor, op1, op2));
5020             else
5021                 set(bytecode.m_dst, addToGraph(ValueBitXor, OpInfo(), OpInfo(prediction), op1, op2));
5022             NEXT_OPCODE(op_bitxor);
5023         }
5024 
5025         case op_rshift: {
5026             auto bytecode = currentInstruction-&gt;as&lt;OpRshift&gt;();
5027             Node* op1 = get(bytecode.m_lhs);
5028             Node* op2 = get(bytecode.m_rhs);
5029             set(bytecode.m_dst, addToGraph(BitRShift, op1, op2));
5030             NEXT_OPCODE(op_rshift);
5031         }
5032 
5033         case op_lshift: {
5034             auto bytecode = currentInstruction-&gt;as&lt;OpLshift&gt;();
5035             Node* op1 = get(bytecode.m_lhs);
5036             Node* op2 = get(bytecode.m_rhs);
<a name="58" id="anc58"></a><span class="line-modified">5037             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())</span>
<span class="line-added">5038                 set(bytecode.m_dst, addToGraph(ArithBitLShift, op1, op2));</span>
<span class="line-added">5039             else {</span>
<span class="line-added">5040                 SpeculatedType prediction = getPredictionWithoutOSRExit();</span>
<span class="line-added">5041                 set(bytecode.m_dst, addToGraph(ValueBitLShift, OpInfo(), OpInfo(prediction), op1, op2));</span>
<span class="line-added">5042             }</span>
5043             NEXT_OPCODE(op_lshift);
5044         }
5045 
5046         case op_urshift: {
5047             auto bytecode = currentInstruction-&gt;as&lt;OpUrshift&gt;();
5048             Node* op1 = get(bytecode.m_lhs);
5049             Node* op2 = get(bytecode.m_rhs);
5050             set(bytecode.m_dst, addToGraph(BitURShift, op1, op2));
5051             NEXT_OPCODE(op_urshift);
5052         }
5053 
5054         case op_unsigned: {
5055             auto bytecode = currentInstruction-&gt;as&lt;OpUnsigned&gt;();
5056             set(bytecode.m_dst, makeSafe(addToGraph(UInt32ToNumber, get(bytecode.m_operand))));
5057             NEXT_OPCODE(op_unsigned);
5058         }
5059 
5060         // === Increment/Decrement opcodes ===
5061 
5062         case op_inc: {
5063             auto bytecode = currentInstruction-&gt;as&lt;OpInc&gt;();
5064             Node* op = get(bytecode.m_srcDst);
5065             set(bytecode.m_srcDst, makeSafe(addToGraph(ArithAdd, op, addToGraph(JSConstant, OpInfo(m_constantOne)))));
5066             NEXT_OPCODE(op_inc);
5067         }
5068 
5069         case op_dec: {
5070             auto bytecode = currentInstruction-&gt;as&lt;OpDec&gt;();
5071             Node* op = get(bytecode.m_srcDst);
5072             set(bytecode.m_srcDst, makeSafe(addToGraph(ArithSub, op, addToGraph(JSConstant, OpInfo(m_constantOne)))));
5073             NEXT_OPCODE(op_dec);
5074         }
5075 
5076         // === Arithmetic operations ===
5077 
5078         case op_add: {
5079             auto bytecode = currentInstruction-&gt;as&lt;OpAdd&gt;();
5080             Node* op1 = get(bytecode.m_lhs);
5081             Node* op2 = get(bytecode.m_rhs);
5082             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5083                 set(bytecode.m_dst, makeSafe(addToGraph(ArithAdd, op1, op2)));
5084             else
5085                 set(bytecode.m_dst, makeSafe(addToGraph(ValueAdd, op1, op2)));
5086             NEXT_OPCODE(op_add);
5087         }
5088 
5089         case op_sub: {
5090             auto bytecode = currentInstruction-&gt;as&lt;OpSub&gt;();
5091             Node* op1 = get(bytecode.m_lhs);
5092             Node* op2 = get(bytecode.m_rhs);
5093             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5094                 set(bytecode.m_dst, makeSafe(addToGraph(ArithSub, op1, op2)));
5095             else
5096                 set(bytecode.m_dst, makeSafe(addToGraph(ValueSub, op1, op2)));
5097             NEXT_OPCODE(op_sub);
5098         }
5099 
5100         case op_negate: {
5101             auto bytecode = currentInstruction-&gt;as&lt;OpNegate&gt;();
5102             Node* op1 = get(bytecode.m_operand);
5103             if (op1-&gt;hasNumberResult())
5104                 set(bytecode.m_dst, makeSafe(addToGraph(ArithNegate, op1)));
5105             else
5106                 set(bytecode.m_dst, makeSafe(addToGraph(ValueNegate, op1)));
5107             NEXT_OPCODE(op_negate);
5108         }
5109 
5110         case op_mul: {
5111             // Multiply requires that the inputs are not truncated, unfortunately.
5112             auto bytecode = currentInstruction-&gt;as&lt;OpMul&gt;();
5113             Node* op1 = get(bytecode.m_lhs);
5114             Node* op2 = get(bytecode.m_rhs);
5115             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5116                 set(bytecode.m_dst, makeSafe(addToGraph(ArithMul, op1, op2)));
5117             else
5118                 set(bytecode.m_dst, makeSafe(addToGraph(ValueMul, op1, op2)));
5119             NEXT_OPCODE(op_mul);
5120         }
5121 
5122         case op_mod: {
5123             auto bytecode = currentInstruction-&gt;as&lt;OpMod&gt;();
5124             Node* op1 = get(bytecode.m_lhs);
5125             Node* op2 = get(bytecode.m_rhs);
<a name="59" id="anc59"></a><span class="line-modified">5126             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())</span>
<span class="line-added">5127                 set(bytecode.m_dst, makeSafe(addToGraph(ArithMod, op1, op2)));</span>
<span class="line-added">5128             else</span>
<span class="line-added">5129                 set(bytecode.m_dst, makeSafe(addToGraph(ValueMod, op1, op2)));</span>
5130             NEXT_OPCODE(op_mod);
5131         }
5132 
5133         case op_pow: {
<a name="60" id="anc60"></a>

5134             auto bytecode = currentInstruction-&gt;as&lt;OpPow&gt;();
5135             Node* op1 = get(bytecode.m_lhs);
5136             Node* op2 = get(bytecode.m_rhs);
<a name="61" id="anc61"></a><span class="line-modified">5137             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())</span>
<span class="line-added">5138                 set(bytecode.m_dst, addToGraph(ArithPow, op1, op2));</span>
<span class="line-added">5139             else</span>
<span class="line-added">5140                 set(bytecode.m_dst, addToGraph(ValuePow, op1, op2));</span>
5141             NEXT_OPCODE(op_pow);
5142         }
5143 
5144         case op_div: {
5145             auto bytecode = currentInstruction-&gt;as&lt;OpDiv&gt;();
5146             Node* op1 = get(bytecode.m_lhs);
5147             Node* op2 = get(bytecode.m_rhs);
5148             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5149                 set(bytecode.m_dst, makeDivSafe(addToGraph(ArithDiv, op1, op2)));
5150             else
5151                 set(bytecode.m_dst, makeDivSafe(addToGraph(ValueDiv, op1, op2)));
5152             NEXT_OPCODE(op_div);
5153         }
5154 
5155         // === Misc operations ===
5156 
5157         case op_debug: {
5158             // This is a nop in the DFG/FTL because when we set a breakpoint in the debugger,
5159             // we will jettison all optimized CodeBlocks that contains the breakpoint.
5160             addToGraph(Check); // We add a nop here so that basic block linking doesn&#39;t break.
5161             NEXT_OPCODE(op_debug);
5162         }
5163 
5164         case op_mov: {
5165             auto bytecode = currentInstruction-&gt;as&lt;OpMov&gt;();
5166             Node* op = get(bytecode.m_src);
5167             set(bytecode.m_dst, op);
5168             NEXT_OPCODE(op_mov);
5169         }
5170 
5171         case op_check_tdz: {
5172             auto bytecode = currentInstruction-&gt;as&lt;OpCheckTdz&gt;();
5173             addToGraph(CheckNotEmpty, get(bytecode.m_targetVirtualRegister));
5174             NEXT_OPCODE(op_check_tdz);
5175         }
5176 
5177         case op_overrides_has_instance: {
5178             auto bytecode = currentInstruction-&gt;as&lt;OpOverridesHasInstance&gt;();
5179             JSFunction* defaultHasInstanceSymbolFunction = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObjectFor(currentCodeOrigin())-&gt;functionProtoHasInstanceSymbolFunction();
5180 
5181             Node* constructor = get(VirtualRegister(bytecode.m_constructor));
5182             Node* hasInstanceValue = get(VirtualRegister(bytecode.m_hasInstanceValue));
5183 
5184             set(VirtualRegister(bytecode.m_dst), addToGraph(OverridesHasInstance, OpInfo(m_graph.freeze(defaultHasInstanceSymbolFunction)), constructor, hasInstanceValue));
5185             NEXT_OPCODE(op_overrides_has_instance);
5186         }
5187 
5188         case op_identity_with_profile: {
5189             auto bytecode = currentInstruction-&gt;as&lt;OpIdentityWithProfile&gt;();
5190             Node* srcDst = get(bytecode.m_srcDst);
5191             SpeculatedType speculation = static_cast&lt;SpeculatedType&gt;(bytecode.m_topProfile) &lt;&lt; 32 | static_cast&lt;SpeculatedType&gt;(bytecode.m_bottomProfile);
5192             set(bytecode.m_srcDst, addToGraph(IdentityWithProfile, OpInfo(speculation), srcDst));
5193             NEXT_OPCODE(op_identity_with_profile);
5194         }
5195 
5196         case op_instanceof: {
5197             auto bytecode = currentInstruction-&gt;as&lt;OpInstanceof&gt;();
5198 
5199             InstanceOfStatus status = InstanceOfStatus::computeFor(
5200                 m_inlineStackTop-&gt;m_profiledBlock, m_inlineStackTop-&gt;m_baselineMap,
5201                 m_currentIndex);
5202 
5203             Node* value = get(bytecode.m_value);
5204             Node* prototype = get(bytecode.m_prototype);
5205 
5206             // Only inline it if it&#39;s Simple with a commonPrototype; bottom/top or variable
5207             // prototypes both get handled by the IC. This makes sense for bottom (unprofiled)
5208             // instanceof ICs because the profit of this optimization is fairly low. So, in the
5209             // absence of any information, it&#39;s better to avoid making this be the cause of a
5210             // recompilation.
5211             if (JSObject* commonPrototype = status.commonPrototype()) {
5212                 addToGraph(CheckCell, OpInfo(m_graph.freeze(commonPrototype)), prototype);
5213 
5214                 bool allOK = true;
5215                 MatchStructureData* data = m_graph.m_matchStructureData.add();
5216                 for (const InstanceOfVariant&amp; variant : status.variants()) {
5217                     if (!check(variant.conditionSet())) {
5218                         allOK = false;
5219                         break;
5220                     }
5221                     for (Structure* structure : variant.structureSet()) {
5222                         MatchStructureVariant matchVariant;
5223                         matchVariant.structure = m_graph.registerStructure(structure);
5224                         matchVariant.result = variant.isHit();
5225 
5226                         data-&gt;variants.append(WTFMove(matchVariant));
5227                     }
5228                 }
5229 
5230                 if (allOK) {
5231                     Node* match = addToGraph(MatchStructure, OpInfo(data), value);
5232                     set(bytecode.m_dst, match);
5233                     NEXT_OPCODE(op_instanceof);
5234                 }
5235             }
5236 
5237             set(bytecode.m_dst, addToGraph(InstanceOf, value, prototype));
5238             NEXT_OPCODE(op_instanceof);
5239         }
5240 
5241         case op_instanceof_custom: {
5242             auto bytecode = currentInstruction-&gt;as&lt;OpInstanceofCustom&gt;();
5243             Node* value = get(bytecode.m_value);
5244             Node* constructor = get(bytecode.m_constructor);
5245             Node* hasInstanceValue = get(bytecode.m_hasInstanceValue);
5246             set(bytecode.m_dst, addToGraph(InstanceOfCustom, value, constructor, hasInstanceValue));
5247             NEXT_OPCODE(op_instanceof_custom);
5248         }
5249         case op_is_empty: {
5250             auto bytecode = currentInstruction-&gt;as&lt;OpIsEmpty&gt;();
5251             Node* value = get(bytecode.m_operand);
5252             set(bytecode.m_dst, addToGraph(IsEmpty, value));
5253             NEXT_OPCODE(op_is_empty);
5254         }
5255         case op_is_undefined: {
5256             auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefined&gt;();
5257             Node* value = get(bytecode.m_operand);
5258             set(bytecode.m_dst, addToGraph(IsUndefined, value));
5259             NEXT_OPCODE(op_is_undefined);
5260         }
5261         case op_is_undefined_or_null: {
5262             auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefinedOrNull&gt;();
5263             Node* value = get(bytecode.m_operand);
5264             set(bytecode.m_dst, addToGraph(IsUndefinedOrNull, value));
5265             NEXT_OPCODE(op_is_undefined_or_null);
5266         }
5267 
5268         case op_is_boolean: {
5269             auto bytecode = currentInstruction-&gt;as&lt;OpIsBoolean&gt;();
5270             Node* value = get(bytecode.m_operand);
5271             set(bytecode.m_dst, addToGraph(IsBoolean, value));
5272             NEXT_OPCODE(op_is_boolean);
5273         }
5274 
5275         case op_is_number: {
5276             auto bytecode = currentInstruction-&gt;as&lt;OpIsNumber&gt;();
5277             Node* value = get(bytecode.m_operand);
5278             set(bytecode.m_dst, addToGraph(IsNumber, value));
5279             NEXT_OPCODE(op_is_number);
5280         }
5281 
5282         case op_is_cell_with_type: {
5283             auto bytecode = currentInstruction-&gt;as&lt;OpIsCellWithType&gt;();
5284             Node* value = get(bytecode.m_operand);
5285             set(bytecode.m_dst, addToGraph(IsCellWithType, OpInfo(bytecode.m_type), value));
5286             NEXT_OPCODE(op_is_cell_with_type);
5287         }
5288 
5289         case op_is_object: {
5290             auto bytecode = currentInstruction-&gt;as&lt;OpIsObject&gt;();
5291             Node* value = get(bytecode.m_operand);
5292             set(bytecode.m_dst, addToGraph(IsObject, value));
5293             NEXT_OPCODE(op_is_object);
5294         }
5295 
5296         case op_is_object_or_null: {
5297             auto bytecode = currentInstruction-&gt;as&lt;OpIsObjectOrNull&gt;();
5298             Node* value = get(bytecode.m_operand);
5299             set(bytecode.m_dst, addToGraph(IsObjectOrNull, value));
5300             NEXT_OPCODE(op_is_object_or_null);
5301         }
5302 
5303         case op_is_function: {
5304             auto bytecode = currentInstruction-&gt;as&lt;OpIsFunction&gt;();
5305             Node* value = get(bytecode.m_operand);
5306             set(bytecode.m_dst, addToGraph(IsFunction, value));
5307             NEXT_OPCODE(op_is_function);
5308         }
5309 
5310         case op_not: {
5311             auto bytecode = currentInstruction-&gt;as&lt;OpNot&gt;();
5312             Node* value = get(bytecode.m_operand);
5313             set(bytecode.m_dst, addToGraph(LogicalNot, value));
5314             NEXT_OPCODE(op_not);
5315         }
5316 
5317         case op_to_primitive: {
5318             auto bytecode = currentInstruction-&gt;as&lt;OpToPrimitive&gt;();
5319             Node* value = get(bytecode.m_src);
5320             set(bytecode.m_dst, addToGraph(ToPrimitive, value));
5321             NEXT_OPCODE(op_to_primitive);
5322         }
5323 
5324         case op_strcat: {
5325             auto bytecode = currentInstruction-&gt;as&lt;OpStrcat&gt;();
5326             int startOperand = bytecode.m_src.offset();
5327             int numOperands = bytecode.m_count;
5328 #if CPU(X86)
5329             // X86 doesn&#39;t have enough registers to compile MakeRope with three arguments. The
5330             // StrCat we emit here may be turned into a MakeRope. Rather than try to be clever,
5331             // we just make StrCat dumber on this processor.
5332             const unsigned maxArguments = 2;
5333 #else
5334             const unsigned maxArguments = 3;
5335 #endif
5336             Node* operands[AdjacencyList::Size];
5337             unsigned indexInOperands = 0;
5338             for (unsigned i = 0; i &lt; AdjacencyList::Size; ++i)
5339                 operands[i] = 0;
5340             for (int operandIdx = 0; operandIdx &lt; numOperands; ++operandIdx) {
5341                 if (indexInOperands == maxArguments) {
5342                     operands[0] = addToGraph(StrCat, operands[0], operands[1], operands[2]);
5343                     for (unsigned i = 1; i &lt; AdjacencyList::Size; ++i)
5344                         operands[i] = 0;
5345                     indexInOperands = 1;
5346                 }
5347 
5348                 ASSERT(indexInOperands &lt; AdjacencyList::Size);
5349                 ASSERT(indexInOperands &lt; maxArguments);
5350                 operands[indexInOperands++] = get(VirtualRegister(startOperand - operandIdx));
5351             }
5352             set(bytecode.m_dst, addToGraph(StrCat, operands[0], operands[1], operands[2]));
5353             NEXT_OPCODE(op_strcat);
5354         }
5355 
5356         case op_less: {
5357             auto bytecode = currentInstruction-&gt;as&lt;OpLess&gt;();
5358             Node* op1 = get(bytecode.m_lhs);
5359             Node* op2 = get(bytecode.m_rhs);
5360             set(bytecode.m_dst, addToGraph(CompareLess, op1, op2));
5361             NEXT_OPCODE(op_less);
5362         }
5363 
5364         case op_lesseq: {
5365             auto bytecode = currentInstruction-&gt;as&lt;OpLesseq&gt;();
5366             Node* op1 = get(bytecode.m_lhs);
5367             Node* op2 = get(bytecode.m_rhs);
5368             set(bytecode.m_dst, addToGraph(CompareLessEq, op1, op2));
5369             NEXT_OPCODE(op_lesseq);
5370         }
5371 
5372         case op_greater: {
5373             auto bytecode = currentInstruction-&gt;as&lt;OpGreater&gt;();
5374             Node* op1 = get(bytecode.m_lhs);
5375             Node* op2 = get(bytecode.m_rhs);
5376             set(bytecode.m_dst, addToGraph(CompareGreater, op1, op2));
5377             NEXT_OPCODE(op_greater);
5378         }
5379 
5380         case op_greatereq: {
5381             auto bytecode = currentInstruction-&gt;as&lt;OpGreatereq&gt;();
5382             Node* op1 = get(bytecode.m_lhs);
5383             Node* op2 = get(bytecode.m_rhs);
5384             set(bytecode.m_dst, addToGraph(CompareGreaterEq, op1, op2));
5385             NEXT_OPCODE(op_greatereq);
5386         }
5387 
5388         case op_below: {
5389             auto bytecode = currentInstruction-&gt;as&lt;OpBelow&gt;();
5390             Node* op1 = get(bytecode.m_lhs);
5391             Node* op2 = get(bytecode.m_rhs);
5392             set(bytecode.m_dst, addToGraph(CompareBelow, op1, op2));
5393             NEXT_OPCODE(op_below);
5394         }
5395 
5396         case op_beloweq: {
5397             auto bytecode = currentInstruction-&gt;as&lt;OpBeloweq&gt;();
5398             Node* op1 = get(bytecode.m_lhs);
5399             Node* op2 = get(bytecode.m_rhs);
5400             set(bytecode.m_dst, addToGraph(CompareBelowEq, op1, op2));
5401             NEXT_OPCODE(op_beloweq);
5402         }
5403 
5404         case op_eq: {
5405             auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
5406             Node* op1 = get(bytecode.m_lhs);
5407             Node* op2 = get(bytecode.m_rhs);
5408             set(bytecode.m_dst, addToGraph(CompareEq, op1, op2));
5409             NEXT_OPCODE(op_eq);
5410         }
5411 
5412         case op_eq_null: {
5413             auto bytecode = currentInstruction-&gt;as&lt;OpEqNull&gt;();
5414             Node* value = get(bytecode.m_operand);
5415             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
5416             set(bytecode.m_dst, addToGraph(CompareEq, value, nullConstant));
5417             NEXT_OPCODE(op_eq_null);
5418         }
5419 
5420         case op_stricteq: {
5421             auto bytecode = currentInstruction-&gt;as&lt;OpStricteq&gt;();
5422             Node* op1 = get(bytecode.m_lhs);
5423             Node* op2 = get(bytecode.m_rhs);
5424             set(bytecode.m_dst, addToGraph(CompareStrictEq, op1, op2));
5425             NEXT_OPCODE(op_stricteq);
5426         }
5427 
5428         case op_neq: {
5429             auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
5430             Node* op1 = get(bytecode.m_lhs);
5431             Node* op2 = get(bytecode.m_rhs);
5432             set(bytecode.m_dst, addToGraph(LogicalNot, addToGraph(CompareEq, op1, op2)));
5433             NEXT_OPCODE(op_neq);
5434         }
5435 
5436         case op_neq_null: {
5437             auto bytecode = currentInstruction-&gt;as&lt;OpNeqNull&gt;();
5438             Node* value = get(bytecode.m_operand);
5439             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
5440             set(bytecode.m_dst, addToGraph(LogicalNot, addToGraph(CompareEq, value, nullConstant)));
5441             NEXT_OPCODE(op_neq_null);
5442         }
5443 
5444         case op_nstricteq: {
5445             auto bytecode = currentInstruction-&gt;as&lt;OpNstricteq&gt;();
5446             Node* op1 = get(bytecode.m_lhs);
5447             Node* op2 = get(bytecode.m_rhs);
5448             Node* invertedResult;
5449             invertedResult = addToGraph(CompareStrictEq, op1, op2);
5450             set(bytecode.m_dst, addToGraph(LogicalNot, invertedResult));
5451             NEXT_OPCODE(op_nstricteq);
5452         }
5453 
5454         // === Property access operations ===
5455 
5456         case op_get_by_val: {
5457             auto bytecode = currentInstruction-&gt;as&lt;OpGetByVal&gt;();
5458             SpeculatedType prediction = getPredictionWithoutOSRExit();
5459 
5460             Node* base = get(bytecode.m_base);
5461             Node* property = get(bytecode.m_property);
5462             bool compiledAsGetById = false;
5463             GetByIdStatus getByIdStatus;
5464             unsigned identifierNumber = 0;
5465             {
5466                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
<a name="62" id="anc62"></a><span class="line-modified">5467                 ByValInfo* byValInfo = m_inlineStackTop-&gt;m_baselineMap.get(CodeOrigin(currentCodeOrigin().bytecodeIndex())).byValInfo;</span>
5468                 // FIXME: When the bytecode is not compiled in the baseline JIT, byValInfo becomes null.
5469                 // At that time, there is no information.
5470                 if (byValInfo
5471                     &amp;&amp; byValInfo-&gt;stubInfo
5472                     &amp;&amp; !byValInfo-&gt;tookSlowPath
5473                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIdent)
5474                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType)
5475                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
5476                     compiledAsGetById = true;
5477                     identifierNumber = m_graph.identifiers().ensure(byValInfo-&gt;cachedId.impl());
5478                     UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
5479 
5480                     if (Symbol* symbol = byValInfo-&gt;cachedSymbol.get()) {
5481                         FrozenValue* frozen = m_graph.freezeStrong(symbol);
5482                         addToGraph(CheckCell, OpInfo(frozen), property);
5483                     } else {
5484                         ASSERT(!uid-&gt;isSymbol());
5485                         addToGraph(CheckStringIdent, OpInfo(uid), property);
5486                     }
5487 
5488                     getByIdStatus = GetByIdStatus::computeForStubInfo(
5489                         locker, m_inlineStackTop-&gt;m_profiledBlock,
5490                         byValInfo-&gt;stubInfo, currentCodeOrigin(), uid);
5491                 }
5492             }
5493 
5494             if (compiledAsGetById)
5495                 handleGetById(bytecode.m_dst, prediction, base, identifierNumber, getByIdStatus, AccessType::Get, currentInstruction-&gt;size());
5496             else {
5497                 ArrayMode arrayMode = getArrayMode(bytecode.metadata(codeBlock).m_arrayProfile, Array::Read);
5498                 // FIXME: We could consider making this not vararg, since it only uses three child
5499                 // slots.
5500                 // https://bugs.webkit.org/show_bug.cgi?id=184192
5501                 addVarArgChild(base);
5502                 addVarArgChild(property);
5503                 addVarArgChild(0); // Leave room for property storage.
5504                 Node* getByVal = addToGraph(Node::VarArg, GetByVal, OpInfo(arrayMode.asWord()), OpInfo(prediction));
5505                 m_exitOK = false; // GetByVal must be treated as if it clobbers exit state, since FixupPhase may make it generic.
5506                 set(bytecode.m_dst, getByVal);
5507             }
5508 
5509             NEXT_OPCODE(op_get_by_val);
5510         }
5511 
5512         case op_get_by_val_with_this: {
5513             auto bytecode = currentInstruction-&gt;as&lt;OpGetByValWithThis&gt;();
5514             SpeculatedType prediction = getPrediction();
5515 
5516             Node* base = get(bytecode.m_base);
5517             Node* thisValue = get(bytecode.m_thisValue);
5518             Node* property = get(bytecode.m_property);
5519             Node* getByValWithThis = addToGraph(GetByValWithThis, OpInfo(), OpInfo(prediction), base, thisValue, property);
5520             set(bytecode.m_dst, getByValWithThis);
5521 
5522             NEXT_OPCODE(op_get_by_val_with_this);
5523         }
5524 
5525         case op_put_by_val_direct:
5526             handlePutByVal(currentInstruction-&gt;as&lt;OpPutByValDirect&gt;(), currentInstruction-&gt;size());
5527             NEXT_OPCODE(op_put_by_val_direct);
5528 
5529         case op_put_by_val: {
5530             handlePutByVal(currentInstruction-&gt;as&lt;OpPutByVal&gt;(), currentInstruction-&gt;size());
5531             NEXT_OPCODE(op_put_by_val);
5532         }
5533 
5534         case op_put_by_val_with_this: {
5535             auto bytecode = currentInstruction-&gt;as&lt;OpPutByValWithThis&gt;();
5536             Node* base = get(bytecode.m_base);
5537             Node* thisValue = get(bytecode.m_thisValue);
5538             Node* property = get(bytecode.m_property);
5539             Node* value = get(bytecode.m_value);
5540 
5541             addVarArgChild(base);
5542             addVarArgChild(thisValue);
5543             addVarArgChild(property);
5544             addVarArgChild(value);
5545             addToGraph(Node::VarArg, PutByValWithThis, OpInfo(0), OpInfo(0));
5546 
5547             NEXT_OPCODE(op_put_by_val_with_this);
5548         }
5549 
5550         case op_define_data_property: {
5551             auto bytecode = currentInstruction-&gt;as&lt;OpDefineDataProperty&gt;();
5552             Node* base = get(bytecode.m_base);
5553             Node* property = get(bytecode.m_property);
5554             Node* value = get(bytecode.m_value);
5555             Node* attributes = get(bytecode.m_attributes);
5556 
5557             addVarArgChild(base);
5558             addVarArgChild(property);
5559             addVarArgChild(value);
5560             addVarArgChild(attributes);
5561             addToGraph(Node::VarArg, DefineDataProperty, OpInfo(0), OpInfo(0));
5562 
5563             NEXT_OPCODE(op_define_data_property);
5564         }
5565 
5566         case op_define_accessor_property: {
5567             auto bytecode = currentInstruction-&gt;as&lt;OpDefineAccessorProperty&gt;();
5568             Node* base = get(bytecode.m_base);
5569             Node* property = get(bytecode.m_property);
5570             Node* getter = get(bytecode.m_getter);
5571             Node* setter = get(bytecode.m_setter);
5572             Node* attributes = get(bytecode.m_attributes);
5573 
5574             addVarArgChild(base);
5575             addVarArgChild(property);
5576             addVarArgChild(getter);
5577             addVarArgChild(setter);
5578             addVarArgChild(attributes);
5579             addToGraph(Node::VarArg, DefineAccessorProperty, OpInfo(0), OpInfo(0));
5580 
5581             NEXT_OPCODE(op_define_accessor_property);
5582         }
5583 
5584         case op_get_by_id_direct: {
5585             parseGetById&lt;OpGetByIdDirect&gt;(currentInstruction);
5586             NEXT_OPCODE(op_get_by_id_direct);
5587         }
5588         case op_try_get_by_id: {
5589             parseGetById&lt;OpTryGetById&gt;(currentInstruction);
5590             NEXT_OPCODE(op_try_get_by_id);
5591         }
5592         case op_get_by_id: {
5593             parseGetById&lt;OpGetById&gt;(currentInstruction);
5594             NEXT_OPCODE(op_get_by_id);
5595         }
5596         case op_get_by_id_with_this: {
5597             SpeculatedType prediction = getPrediction();
5598 
5599             auto bytecode = currentInstruction-&gt;as&lt;OpGetByIdWithThis&gt;();
5600             Node* base = get(bytecode.m_base);
5601             Node* thisValue = get(bytecode.m_thisValue);
5602             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5603 
5604             set(bytecode.m_dst,
5605                 addToGraph(GetByIdWithThis, OpInfo(identifierNumber), OpInfo(prediction), base, thisValue));
5606 
5607             NEXT_OPCODE(op_get_by_id_with_this);
5608         }
5609         case op_put_by_id: {
5610             auto bytecode = currentInstruction-&gt;as&lt;OpPutById&gt;();
5611             Node* value = get(bytecode.m_value);
5612             Node* base = get(bytecode.m_base);
5613             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5614             bool direct = !!(bytecode.m_flags &amp; PutByIdIsDirect);
5615 
5616             PutByIdStatus putByIdStatus = PutByIdStatus::computeFor(
5617                 m_inlineStackTop-&gt;m_profiledBlock,
5618                 m_inlineStackTop-&gt;m_baselineMap, m_icContextStack,
5619                 currentCodeOrigin(), m_graph.identifiers()[identifierNumber]);
5620 
5621             handlePutById(base, identifierNumber, value, putByIdStatus, direct, currentInstruction-&gt;size());
5622             NEXT_OPCODE(op_put_by_id);
5623         }
5624 
5625         case op_put_by_id_with_this: {
5626             auto bytecode = currentInstruction-&gt;as&lt;OpPutByIdWithThis&gt;();
5627             Node* base = get(bytecode.m_base);
5628             Node* thisValue = get(bytecode.m_thisValue);
5629             Node* value = get(bytecode.m_value);
5630             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5631 
5632             addToGraph(PutByIdWithThis, OpInfo(identifierNumber), base, thisValue, value);
5633             NEXT_OPCODE(op_put_by_id_with_this);
5634         }
5635 
5636         case op_put_getter_by_id:
5637             handlePutAccessorById(PutGetterById, currentInstruction-&gt;as&lt;OpPutGetterById&gt;());
5638             NEXT_OPCODE(op_put_getter_by_id);
5639         case op_put_setter_by_id: {
5640             handlePutAccessorById(PutSetterById, currentInstruction-&gt;as&lt;OpPutSetterById&gt;());
5641             NEXT_OPCODE(op_put_setter_by_id);
5642         }
5643 
5644         case op_put_getter_setter_by_id: {
5645             auto bytecode = currentInstruction-&gt;as&lt;OpPutGetterSetterById&gt;();
5646             Node* base = get(bytecode.m_base);
5647             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5648             Node* getter = get(bytecode.m_getter);
5649             Node* setter = get(bytecode.m_setter);
5650             addToGraph(PutGetterSetterById, OpInfo(identifierNumber), OpInfo(bytecode.m_attributes), base, getter, setter);
5651             NEXT_OPCODE(op_put_getter_setter_by_id);
5652         }
5653 
5654         case op_put_getter_by_val:
5655             handlePutAccessorByVal(PutGetterByVal, currentInstruction-&gt;as&lt;OpPutGetterByVal&gt;());
5656             NEXT_OPCODE(op_put_getter_by_val);
5657         case op_put_setter_by_val: {
5658             handlePutAccessorByVal(PutSetterByVal, currentInstruction-&gt;as&lt;OpPutSetterByVal&gt;());
5659             NEXT_OPCODE(op_put_setter_by_val);
5660         }
5661 
5662         case op_del_by_id: {
5663             auto bytecode = currentInstruction-&gt;as&lt;OpDelById&gt;();
5664             Node* base = get(bytecode.m_base);
5665             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
5666             set(bytecode.m_dst, addToGraph(DeleteById, OpInfo(identifierNumber), base));
5667             NEXT_OPCODE(op_del_by_id);
5668         }
5669 
5670         case op_del_by_val: {
5671             auto bytecode = currentInstruction-&gt;as&lt;OpDelByVal&gt;();
5672             Node* base = get(bytecode.m_base);
5673             Node* key = get(bytecode.m_property);
5674             set(bytecode.m_dst, addToGraph(DeleteByVal, base, key));
5675             NEXT_OPCODE(op_del_by_val);
5676         }
5677 
5678         case op_profile_type: {
5679             auto bytecode = currentInstruction-&gt;as&lt;OpProfileType&gt;();
5680             auto&amp; metadata = bytecode.metadata(codeBlock);
5681             Node* valueToProfile = get(bytecode.m_targetVirtualRegister);
5682             addToGraph(ProfileType, OpInfo(metadata.m_typeLocation), valueToProfile);
5683             NEXT_OPCODE(op_profile_type);
5684         }
5685 
5686         case op_profile_control_flow: {
5687             auto bytecode = currentInstruction-&gt;as&lt;OpProfileControlFlow&gt;();
5688             BasicBlockLocation* basicBlockLocation = bytecode.metadata(codeBlock).m_basicBlockLocation;
5689             addToGraph(ProfileControlFlow, OpInfo(basicBlockLocation));
5690             NEXT_OPCODE(op_profile_control_flow);
5691         }
5692 
5693         // === Block terminators. ===
5694 
5695         case op_jmp: {
5696             ASSERT(!m_currentBlock-&gt;terminal());
5697             auto bytecode = currentInstruction-&gt;as&lt;OpJmp&gt;();
5698             int relativeOffset = jumpTarget(bytecode.m_targetLabel);
5699             addToGraph(Jump, OpInfo(m_currentIndex + relativeOffset));
5700             if (relativeOffset &lt;= 0)
5701                 flushForTerminal();
5702             LAST_OPCODE(op_jmp);
5703         }
5704 
5705         case op_jtrue: {
5706             auto bytecode = currentInstruction-&gt;as&lt;OpJtrue&gt;();
5707             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5708             Node* condition = get(bytecode.m_condition);
5709             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5710             LAST_OPCODE(op_jtrue);
5711         }
5712 
5713         case op_jfalse: {
5714             auto bytecode = currentInstruction-&gt;as&lt;OpJfalse&gt;();
5715             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5716             Node* condition = get(bytecode.m_condition);
5717             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5718             LAST_OPCODE(op_jfalse);
5719         }
5720 
5721         case op_jeq_null: {
5722             auto bytecode = currentInstruction-&gt;as&lt;OpJeqNull&gt;();
5723             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5724             Node* value = get(bytecode.m_value);
5725             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
5726             Node* condition = addToGraph(CompareEq, value, nullConstant);
5727             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5728             LAST_OPCODE(op_jeq_null);
5729         }
5730 
5731         case op_jneq_null: {
5732             auto bytecode = currentInstruction-&gt;as&lt;OpJneqNull&gt;();
5733             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5734             Node* value = get(bytecode.m_value);
5735             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
5736             Node* condition = addToGraph(CompareEq, value, nullConstant);
5737             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5738             LAST_OPCODE(op_jneq_null);
5739         }
5740 
<a name="63" id="anc63"></a><span class="line-added">5741         case op_jundefined_or_null: {</span>
<span class="line-added">5742             auto bytecode = currentInstruction-&gt;as&lt;OpJundefinedOrNull&gt;();</span>
<span class="line-added">5743             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);</span>
<span class="line-added">5744             Node* value = get(bytecode.m_value);</span>
<span class="line-added">5745             Node* condition = addToGraph(IsUndefinedOrNull, value);</span>
<span class="line-added">5746             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);</span>
<span class="line-added">5747             LAST_OPCODE(op_jundefined_or_null);</span>
<span class="line-added">5748         }</span>
<span class="line-added">5749 </span>
<span class="line-added">5750         case op_jnundefined_or_null: {</span>
<span class="line-added">5751             auto bytecode = currentInstruction-&gt;as&lt;OpJnundefinedOrNull&gt;();</span>
<span class="line-added">5752             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);</span>
<span class="line-added">5753             Node* value = get(bytecode.m_value);</span>
<span class="line-added">5754             Node* condition = addToGraph(IsUndefinedOrNull, value);</span>
<span class="line-added">5755             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);</span>
<span class="line-added">5756             LAST_OPCODE(op_jnundefined_or_null);</span>
<span class="line-added">5757         }</span>
<span class="line-added">5758 </span>
5759         case op_jless: {
5760             auto bytecode = currentInstruction-&gt;as&lt;OpJless&gt;();
5761             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5762             Node* op1 = get(bytecode.m_lhs);
5763             Node* op2 = get(bytecode.m_rhs);
5764             Node* condition = addToGraph(CompareLess, op1, op2);
5765             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5766             LAST_OPCODE(op_jless);
5767         }
5768 
5769         case op_jlesseq: {
5770             auto bytecode = currentInstruction-&gt;as&lt;OpJlesseq&gt;();
5771             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5772             Node* op1 = get(bytecode.m_lhs);
5773             Node* op2 = get(bytecode.m_rhs);
5774             Node* condition = addToGraph(CompareLessEq, op1, op2);
5775             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5776             LAST_OPCODE(op_jlesseq);
5777         }
5778 
5779         case op_jgreater: {
5780             auto bytecode = currentInstruction-&gt;as&lt;OpJgreater&gt;();
5781             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5782             Node* op1 = get(bytecode.m_lhs);
5783             Node* op2 = get(bytecode.m_rhs);
5784             Node* condition = addToGraph(CompareGreater, op1, op2);
5785             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5786             LAST_OPCODE(op_jgreater);
5787         }
5788 
5789         case op_jgreatereq: {
5790             auto bytecode = currentInstruction-&gt;as&lt;OpJgreatereq&gt;();
5791             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5792             Node* op1 = get(bytecode.m_lhs);
5793             Node* op2 = get(bytecode.m_rhs);
5794             Node* condition = addToGraph(CompareGreaterEq, op1, op2);
5795             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5796             LAST_OPCODE(op_jgreatereq);
5797         }
5798 
5799         case op_jeq: {
5800             auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
5801             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5802             Node* op1 = get(bytecode.m_lhs);
5803             Node* op2 = get(bytecode.m_rhs);
5804             Node* condition = addToGraph(CompareEq, op1, op2);
5805             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5806             LAST_OPCODE(op_jeq);
5807         }
5808 
5809         case op_jstricteq: {
5810             auto bytecode = currentInstruction-&gt;as&lt;OpJstricteq&gt;();
5811             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5812             Node* op1 = get(bytecode.m_lhs);
5813             Node* op2 = get(bytecode.m_rhs);
5814             Node* condition = addToGraph(CompareStrictEq, op1, op2);
5815             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5816             LAST_OPCODE(op_jstricteq);
5817         }
5818 
5819         case op_jnless: {
5820             auto bytecode = currentInstruction-&gt;as&lt;OpJnless&gt;();
5821             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5822             Node* op1 = get(bytecode.m_lhs);
5823             Node* op2 = get(bytecode.m_rhs);
5824             Node* condition = addToGraph(CompareLess, op1, op2);
5825             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5826             LAST_OPCODE(op_jnless);
5827         }
5828 
5829         case op_jnlesseq: {
5830             auto bytecode = currentInstruction-&gt;as&lt;OpJnlesseq&gt;();
5831             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5832             Node* op1 = get(bytecode.m_lhs);
5833             Node* op2 = get(bytecode.m_rhs);
5834             Node* condition = addToGraph(CompareLessEq, op1, op2);
5835             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5836             LAST_OPCODE(op_jnlesseq);
5837         }
5838 
5839         case op_jngreater: {
5840             auto bytecode = currentInstruction-&gt;as&lt;OpJngreater&gt;();
5841             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5842             Node* op1 = get(bytecode.m_lhs);
5843             Node* op2 = get(bytecode.m_rhs);
5844             Node* condition = addToGraph(CompareGreater, op1, op2);
5845             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5846             LAST_OPCODE(op_jngreater);
5847         }
5848 
5849         case op_jngreatereq: {
5850             auto bytecode = currentInstruction-&gt;as&lt;OpJngreatereq&gt;();
5851             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5852             Node* op1 = get(bytecode.m_lhs);
5853             Node* op2 = get(bytecode.m_rhs);
5854             Node* condition = addToGraph(CompareGreaterEq, op1, op2);
5855             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5856             LAST_OPCODE(op_jngreatereq);
5857         }
5858 
5859         case op_jneq: {
5860             auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
5861             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5862             Node* op1 = get(bytecode.m_lhs);
5863             Node* op2 = get(bytecode.m_rhs);
5864             Node* condition = addToGraph(CompareEq, op1, op2);
5865             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5866             LAST_OPCODE(op_jneq);
5867         }
5868 
5869         case op_jnstricteq: {
5870             auto bytecode = currentInstruction-&gt;as&lt;OpJnstricteq&gt;();
5871             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5872             Node* op1 = get(bytecode.m_lhs);
5873             Node* op2 = get(bytecode.m_rhs);
5874             Node* condition = addToGraph(CompareStrictEq, op1, op2);
5875             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5876             LAST_OPCODE(op_jnstricteq);
5877         }
5878 
5879         case op_jbelow: {
5880             auto bytecode = currentInstruction-&gt;as&lt;OpJbelow&gt;();
5881             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5882             Node* op1 = get(bytecode.m_lhs);
5883             Node* op2 = get(bytecode.m_rhs);
5884             Node* condition = addToGraph(CompareBelow, op1, op2);
5885             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5886             LAST_OPCODE(op_jbelow);
5887         }
5888 
5889         case op_jbeloweq: {
5890             auto bytecode = currentInstruction-&gt;as&lt;OpJbeloweq&gt;();
5891             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5892             Node* op1 = get(bytecode.m_lhs);
5893             Node* op2 = get(bytecode.m_rhs);
5894             Node* condition = addToGraph(CompareBelowEq, op1, op2);
5895             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5896             LAST_OPCODE(op_jbeloweq);
5897         }
5898 
5899         case op_switch_imm: {
5900             auto bytecode = currentInstruction-&gt;as&lt;OpSwitchImm&gt;();
5901             SwitchData&amp; data = *m_graph.m_switchData.add();
5902             data.kind = SwitchImm;
5903             data.switchTableIndex = m_inlineStackTop-&gt;m_switchRemap[bytecode.m_tableIndex];
5904             data.fallThrough.setBytecodeIndex(m_currentIndex + jumpTarget(bytecode.m_defaultOffset));
5905             SimpleJumpTable&amp; table = m_codeBlock-&gt;switchJumpTable(data.switchTableIndex);
5906             for (unsigned i = 0; i &lt; table.branchOffsets.size(); ++i) {
5907                 if (!table.branchOffsets[i])
5908                     continue;
5909                 unsigned target = m_currentIndex + table.branchOffsets[i];
5910                 if (target == data.fallThrough.bytecodeIndex())
5911                     continue;
5912                 data.cases.append(SwitchCase::withBytecodeIndex(m_graph.freeze(jsNumber(static_cast&lt;int32_t&gt;(table.min + i))), target));
5913             }
5914             addToGraph(Switch, OpInfo(&amp;data), get(bytecode.m_scrutinee));
5915             flushIfTerminal(data);
5916             LAST_OPCODE(op_switch_imm);
5917         }
5918 
5919         case op_switch_char: {
5920             auto bytecode = currentInstruction-&gt;as&lt;OpSwitchChar&gt;();
5921             SwitchData&amp; data = *m_graph.m_switchData.add();
5922             data.kind = SwitchChar;
5923             data.switchTableIndex = m_inlineStackTop-&gt;m_switchRemap[bytecode.m_tableIndex];
5924             data.fallThrough.setBytecodeIndex(m_currentIndex + jumpTarget(bytecode.m_defaultOffset));
5925             SimpleJumpTable&amp; table = m_codeBlock-&gt;switchJumpTable(data.switchTableIndex);
5926             for (unsigned i = 0; i &lt; table.branchOffsets.size(); ++i) {
5927                 if (!table.branchOffsets[i])
5928                     continue;
5929                 unsigned target = m_currentIndex + table.branchOffsets[i];
5930                 if (target == data.fallThrough.bytecodeIndex())
5931                     continue;
5932                 data.cases.append(
5933                     SwitchCase::withBytecodeIndex(LazyJSValue::singleCharacterString(table.min + i), target));
5934             }
5935             addToGraph(Switch, OpInfo(&amp;data), get(bytecode.m_scrutinee));
5936             flushIfTerminal(data);
5937             LAST_OPCODE(op_switch_char);
5938         }
5939 
5940         case op_switch_string: {
5941             auto bytecode = currentInstruction-&gt;as&lt;OpSwitchString&gt;();
5942             SwitchData&amp; data = *m_graph.m_switchData.add();
5943             data.kind = SwitchString;
5944             data.switchTableIndex = bytecode.m_tableIndex;
5945             data.fallThrough.setBytecodeIndex(m_currentIndex + jumpTarget(bytecode.m_defaultOffset));
5946             StringJumpTable&amp; table = m_codeBlock-&gt;stringSwitchJumpTable(data.switchTableIndex);
5947             StringJumpTable::StringOffsetTable::iterator iter;
5948             StringJumpTable::StringOffsetTable::iterator end = table.offsetTable.end();
5949             for (iter = table.offsetTable.begin(); iter != end; ++iter) {
5950                 unsigned target = m_currentIndex + iter-&gt;value.branchOffset;
5951                 if (target == data.fallThrough.bytecodeIndex())
5952                     continue;
5953                 data.cases.append(
5954                     SwitchCase::withBytecodeIndex(LazyJSValue::knownStringImpl(iter-&gt;key.get()), target));
5955             }
5956             addToGraph(Switch, OpInfo(&amp;data), get(bytecode.m_scrutinee));
5957             flushIfTerminal(data);
5958             LAST_OPCODE(op_switch_string);
5959         }
5960 
5961         case op_ret: {
5962             auto bytecode = currentInstruction-&gt;as&lt;OpRet&gt;();
5963             ASSERT(!m_currentBlock-&gt;terminal());
5964             if (!inlineCallFrame()) {
5965                 // Simple case: we are just producing a return
5966                 addToGraph(Return, get(bytecode.m_value));
5967                 flushForReturn();
5968                 LAST_OPCODE(op_ret);
5969             }
5970 
5971             flushForReturn();
5972             if (m_inlineStackTop-&gt;m_returnValue.isValid())
5973                 setDirect(m_inlineStackTop-&gt;m_returnValue, get(bytecode.m_value), ImmediateSetWithFlush);
5974 
5975             if (!m_inlineStackTop-&gt;m_continuationBlock &amp;&amp; m_currentIndex + currentInstruction-&gt;size() != m_inlineStackTop-&gt;m_codeBlock-&gt;instructions().size()) {
5976                 // This is an early return from an inlined function and we do not have a continuation block, so we must allocate one.
5977                 // It is untargetable, because we do not know the appropriate index.
5978                 // If this block turns out to be a jump target, parseCodeBlock will fix its bytecodeIndex before putting it in m_blockLinkingTargets
5979                 m_inlineStackTop-&gt;m_continuationBlock = allocateUntargetableBlock();
5980             }
5981 
5982             if (m_inlineStackTop-&gt;m_continuationBlock)
5983                 addJumpTo(m_inlineStackTop-&gt;m_continuationBlock);
5984             else {
5985                 // We are returning from an inlined function, and do not need to jump anywhere, so we just keep the current block
5986                 m_inlineStackTop-&gt;m_continuationBlock = m_currentBlock;
5987             }
5988             LAST_OPCODE_LINKED(op_ret);
5989         }
5990         case op_end:
5991             ASSERT(!inlineCallFrame());
5992             addToGraph(Return, get(currentInstruction-&gt;as&lt;OpEnd&gt;().m_value));
5993             flushForReturn();
5994             LAST_OPCODE(op_end);
5995 
5996         case op_throw:
5997             addToGraph(Throw, get(currentInstruction-&gt;as&lt;OpThrow&gt;().m_value));
5998             flushForTerminal();
5999             LAST_OPCODE(op_throw);
6000 
6001         case op_throw_static_error: {
6002             auto bytecode = currentInstruction-&gt;as&lt;OpThrowStaticError&gt;();
6003             addToGraph(ThrowStaticError, OpInfo(bytecode.m_errorType), get(bytecode.m_message));
6004             flushForTerminal();
6005             LAST_OPCODE(op_throw_static_error);
6006         }
6007 
6008         case op_catch: {
6009             auto bytecode = currentInstruction-&gt;as&lt;OpCatch&gt;();
6010             m_graph.m_hasExceptionHandlers = true;
6011 
6012             if (inlineCallFrame()) {
6013                 // We can&#39;t do OSR entry into an inlined frame.
6014                 NEXT_OPCODE(op_catch);
6015             }
6016 
6017             if (m_graph.m_plan.mode() == FTLForOSREntryMode) {
6018                 NEXT_OPCODE(op_catch);
6019             }
6020 
6021             RELEASE_ASSERT(!m_currentBlock-&gt;size() || (m_graph.compilation() &amp;&amp; m_currentBlock-&gt;size() == 1 &amp;&amp; m_currentBlock-&gt;at(0)-&gt;op() == CountExecution));
6022 
6023             ValueProfileAndOperandBuffer* buffer = bytecode.metadata(codeBlock).m_buffer;
6024 
6025             if (!buffer) {
6026                 NEXT_OPCODE(op_catch); // This catch has yet to execute. Note: this load can be racy with the main thread.
6027             }
6028 
6029             // We&#39;re now committed to compiling this as an entrypoint.
6030             m_currentBlock-&gt;isCatchEntrypoint = true;
6031             m_graph.m_roots.append(m_currentBlock);
6032 
6033             Vector&lt;SpeculatedType&gt; argumentPredictions(m_numArguments);
6034             Vector&lt;SpeculatedType&gt; localPredictions;
6035             HashSet&lt;unsigned, WTF::IntHash&lt;unsigned&gt;, WTF::UnsignedWithZeroKeyHashTraits&lt;unsigned&gt;&gt; seenArguments;
6036 
6037             {
6038                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
6039 
6040                 buffer-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {
6041                     VirtualRegister operand(profile.m_operand);
<a name="64" id="anc64"></a><span class="line-modified">6042                     SpeculatedType prediction = profile.computeUpdatedPrediction(locker);</span>
6043                     if (operand.isLocal())
6044                         localPredictions.append(prediction);
6045                     else {
6046                         RELEASE_ASSERT(operand.isArgument());
6047                         RELEASE_ASSERT(static_cast&lt;uint32_t&gt;(operand.toArgument()) &lt; argumentPredictions.size());
6048                         if (validationEnabled())
6049                             seenArguments.add(operand.toArgument());
6050                         argumentPredictions[operand.toArgument()] = prediction;
6051                     }
6052                 });
6053 
6054                 if (validationEnabled()) {
6055                     for (unsigned argument = 0; argument &lt; m_numArguments; ++argument)
6056                         RELEASE_ASSERT(seenArguments.contains(argument));
6057                 }
6058             }
6059 
6060             Vector&lt;std::pair&lt;VirtualRegister, Node*&gt;&gt; localsToSet;
6061             localsToSet.reserveInitialCapacity(buffer-&gt;m_size); // Note: This will reserve more than the number of locals we see below because the buffer includes arguments.
6062 
6063             // We&#39;re not allowed to exit here since we would not properly recover values.
6064             // We first need to bootstrap the catch entrypoint state.
6065             m_exitOK = false;
6066 
6067             unsigned numberOfLocals = 0;
6068             buffer-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {
6069                 VirtualRegister operand(profile.m_operand);
6070                 if (operand.isArgument())
6071                     return;
6072                 ASSERT(operand.isLocal());
6073                 Node* value = addToGraph(ExtractCatchLocal, OpInfo(numberOfLocals), OpInfo(localPredictions[numberOfLocals]));
6074                 ++numberOfLocals;
6075                 addToGraph(MovHint, OpInfo(profile.m_operand), value);
6076                 localsToSet.uncheckedAppend(std::make_pair(operand, value));
6077             });
6078             if (numberOfLocals)
6079                 addToGraph(ClearCatchLocals);
6080 
6081             if (!m_graph.m_maxLocalsForCatchOSREntry)
6082                 m_graph.m_maxLocalsForCatchOSREntry = 0;
6083             m_graph.m_maxLocalsForCatchOSREntry = std::max(numberOfLocals, *m_graph.m_maxLocalsForCatchOSREntry);
6084 
6085             // We could not exit before this point in the program because we would not know how to do value
6086             // recovery for live locals. The above IR sets up the necessary state so we can recover values
6087             // during OSR exit.
6088             //
6089             // The nodes that follow here all exit to the following bytecode instruction, not
6090             // the op_catch. Exiting to op_catch is reserved for when an exception is thrown.
<a name="65" id="anc65"></a><span class="line-modified">6091             // The SetArgumentDefinitely nodes that follow below may exit because we may hoist type checks</span>
6092             // to them. The SetLocal nodes that follow below may exit because we may choose
6093             // a flush format that speculates on the type of the local.
6094             m_exitOK = true;
6095             addToGraph(ExitOK);
6096 
6097             {
6098                 auto addResult = m_graph.m_rootToArguments.add(m_currentBlock, ArgumentsVector());
6099                 RELEASE_ASSERT(addResult.isNewEntry);
6100                 ArgumentsVector&amp; entrypointArguments = addResult.iterator-&gt;value;
6101                 entrypointArguments.resize(m_numArguments);
6102 
6103                 unsigned exitBytecodeIndex = m_currentIndex + currentInstruction-&gt;size();
6104 
6105                 for (unsigned argument = 0; argument &lt; argumentPredictions.size(); ++argument) {
6106                     VariableAccessData* variable = newVariableAccessData(virtualRegisterForArgument(argument));
6107                     variable-&gt;predict(argumentPredictions[argument]);
6108 
6109                     variable-&gt;mergeStructureCheckHoistingFailed(
6110                         m_inlineStackTop-&gt;m_exitProfile.hasExitSite(exitBytecodeIndex, BadCache));
6111                     variable-&gt;mergeCheckArrayHoistingFailed(
6112                         m_inlineStackTop-&gt;m_exitProfile.hasExitSite(exitBytecodeIndex, BadIndexingType));
6113 
<a name="66" id="anc66"></a><span class="line-modified">6114                     Node* setArgument = addToGraph(SetArgumentDefinitely, OpInfo(variable));</span>
<span class="line-modified">6115                     setArgument-&gt;origin.forExit = CodeOrigin(exitBytecodeIndex, setArgument-&gt;origin.forExit.inlineCallFrame());</span>
6116                     m_currentBlock-&gt;variablesAtTail.setArgumentFirstTime(argument, setArgument);
6117                     entrypointArguments[argument] = setArgument;
6118                 }
6119             }
6120 
6121             for (const std::pair&lt;VirtualRegister, Node*&gt;&amp; pair : localsToSet) {
6122                 DelayedSetLocal delayed { currentCodeOrigin(), pair.first, pair.second, ImmediateNakedSet };
6123                 m_setLocalQueue.append(delayed);
6124             }
6125 
6126             NEXT_OPCODE(op_catch);
6127         }
6128 
6129         case op_call:
6130             handleCall&lt;OpCall&gt;(currentInstruction, Call, CallMode::Regular);
6131             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6132             NEXT_OPCODE(op_call);
6133 
6134         case op_tail_call: {
6135             flushForReturn();
6136             Terminality terminality = handleCall&lt;OpTailCall&gt;(currentInstruction, TailCall, CallMode::Tail);
6137             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6138             // If the call is terminal then we should not parse any further bytecodes as the TailCall will exit the function.
6139             // If the call is not terminal, however, then we want the subsequent op_ret/op_jmp to update metadata and clean
6140             // things up.
6141             if (terminality == NonTerminal)
6142                 NEXT_OPCODE(op_tail_call);
6143             else
6144                 LAST_OPCODE_LINKED(op_tail_call);
6145             // We use LAST_OPCODE_LINKED instead of LAST_OPCODE because if the tail call was optimized, it may now be a jump to a bytecode index in a different InlineStackEntry.
6146         }
6147 
6148         case op_construct:
6149             handleCall&lt;OpConstruct&gt;(currentInstruction, Construct, CallMode::Construct);
6150             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6151             NEXT_OPCODE(op_construct);
6152 
6153         case op_call_varargs: {
6154             handleVarargsCall&lt;OpCallVarargs&gt;(currentInstruction, CallVarargs, CallMode::Regular);
6155             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleVarargsCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6156             NEXT_OPCODE(op_call_varargs);
6157         }
6158 
6159         case op_tail_call_varargs: {
6160             flushForReturn();
6161             Terminality terminality = handleVarargsCall&lt;OpTailCallVarargs&gt;(currentInstruction, TailCallVarargs, CallMode::Tail);
6162             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleVarargsCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6163             // If the call is terminal then we should not parse any further bytecodes as the TailCall will exit the function.
6164             // If the call is not terminal, however, then we want the subsequent op_ret/op_jmp to update metadata and clean
6165             // things up.
6166             if (terminality == NonTerminal)
6167                 NEXT_OPCODE(op_tail_call_varargs);
6168             else
6169                 LAST_OPCODE(op_tail_call_varargs);
6170         }
6171 
6172         case op_tail_call_forward_arguments: {
6173             // We need to make sure that we don&#39;t unbox our arguments here since that won&#39;t be
6174             // done by the arguments object creation node as that node may not exist.
6175             noticeArgumentsUse();
6176             flushForReturn();
6177             Terminality terminality = handleVarargsCall&lt;OpTailCallForwardArguments&gt;(currentInstruction, TailCallForwardVarargs, CallMode::Tail);
6178             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleVarargsCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6179             // If the call is terminal then we should not parse any further bytecodes as the TailCall will exit the function.
6180             // If the call is not terminal, however, then we want the subsequent op_ret/op_jmp to update metadata and clean
6181             // things up.
6182             if (terminality == NonTerminal)
6183                 NEXT_OPCODE(op_tail_call_forward_arguments);
6184             else
6185                 LAST_OPCODE(op_tail_call_forward_arguments);
6186         }
6187 
6188         case op_construct_varargs: {
6189             handleVarargsCall&lt;OpConstructVarargs&gt;(currentInstruction, ConstructVarargs, CallMode::Construct);
6190             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleVarargsCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6191             NEXT_OPCODE(op_construct_varargs);
6192         }
6193 
6194         case op_call_eval: {
6195             auto bytecode = currentInstruction-&gt;as&lt;OpCallEval&gt;();
6196             int registerOffset = -bytecode.m_argv;
6197             addCall(bytecode.m_dst, CallEval, nullptr, get(bytecode.m_callee), bytecode.m_argc, registerOffset, getPrediction());
6198             NEXT_OPCODE(op_call_eval);
6199         }
6200 
6201         case op_jneq_ptr: {
6202             auto bytecode = currentInstruction-&gt;as&lt;OpJneqPtr&gt;();
6203             Special::Pointer specialPointer = bytecode.m_specialPointer;
6204             ASSERT(pointerIsCell(specialPointer));
6205             JSCell* actualPointer = static_cast&lt;JSCell*&gt;(
6206                 actualPointerFor(m_inlineStackTop-&gt;m_codeBlock, specialPointer));
6207             FrozenValue* frozenPointer = m_graph.freeze(actualPointer);
6208             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
6209             Node* child = get(bytecode.m_value);
6210             if (bytecode.metadata(codeBlock).m_hasJumped) {
6211                 Node* condition = addToGraph(CompareEqPtr, OpInfo(frozenPointer), child);
6212                 addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
6213                 LAST_OPCODE(op_jneq_ptr);
6214             }
6215             addToGraph(CheckCell, OpInfo(frozenPointer), child);
6216             NEXT_OPCODE(op_jneq_ptr);
6217         }
6218 
6219         case op_resolve_scope: {
6220             auto bytecode = currentInstruction-&gt;as&lt;OpResolveScope&gt;();
6221             auto&amp; metadata = bytecode.metadata(codeBlock);
6222 
6223             ResolveType resolveType;
6224             unsigned depth;
6225             JSScope* constantScope = nullptr;
6226             JSCell* lexicalEnvironment = nullptr;
6227             SymbolTable* symbolTable = nullptr;
6228             {
6229                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
6230                 resolveType = metadata.m_resolveType;
6231                 depth = metadata.m_localScopeDepth;
6232                 switch (resolveType) {
6233                 case GlobalProperty:
6234                 case GlobalVar:
6235                 case GlobalPropertyWithVarInjectionChecks:
6236                 case GlobalVarWithVarInjectionChecks:
6237                 case GlobalLexicalVar:
6238                 case GlobalLexicalVarWithVarInjectionChecks:
6239                     constantScope = metadata.m_constantScope.get();
6240                     break;
6241                 case ModuleVar:
6242                     lexicalEnvironment = metadata.m_lexicalEnvironment.get();
6243                     break;
6244                 case LocalClosureVar:
6245                 case ClosureVar:
6246                 case ClosureVarWithVarInjectionChecks:
6247                     symbolTable = metadata.m_symbolTable.get();
6248                     break;
6249                 default:
6250                     break;
6251                 }
6252             }
6253 
6254             if (needsDynamicLookup(resolveType, op_resolve_scope)) {
6255                 unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_var];
6256                 set(bytecode.m_dst, addToGraph(ResolveScope, OpInfo(identifierNumber), get(bytecode.m_scope)));
6257                 NEXT_OPCODE(op_resolve_scope);
6258             }
6259 
6260             // get_from_scope and put_to_scope depend on this watchpoint forcing OSR exit, so they don&#39;t add their own watchpoints.
6261             if (needsVarInjectionChecks(resolveType))
6262                 m_graph.watchpoints().addLazily(m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject()-&gt;varInjectionWatchpoint());
6263 
6264             // FIXME: Currently, module code do not query to JSGlobalLexicalEnvironment. So this case should be removed once it is fixed.
6265             // https://bugs.webkit.org/show_bug.cgi?id=193347
6266             if (m_inlineStackTop-&gt;m_codeBlock-&gt;scriptMode() != JSParserScriptMode::Module) {
6267                 if (resolveType == GlobalProperty || resolveType == GlobalPropertyWithVarInjectionChecks) {
6268                     JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
6269                     unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_var];
6270                     if (!m_graph.watchGlobalProperty(globalObject, identifierNumber))
6271                         addToGraph(ForceOSRExit);
6272                 }
6273             }
6274 
6275             switch (resolveType) {
6276             case GlobalProperty:
6277             case GlobalVar:
6278             case GlobalPropertyWithVarInjectionChecks:
6279             case GlobalVarWithVarInjectionChecks:
6280             case GlobalLexicalVar:
6281             case GlobalLexicalVarWithVarInjectionChecks: {
6282                 RELEASE_ASSERT(constantScope);
6283                 RELEASE_ASSERT(constantScope == JSScope::constantScopeForCodeBlock(resolveType, m_inlineStackTop-&gt;m_codeBlock));
6284                 set(bytecode.m_dst, weakJSConstant(constantScope));
6285                 addToGraph(Phantom, get(bytecode.m_scope));
6286                 break;
6287             }
6288             case ModuleVar: {
6289                 // Since the value of the &quot;scope&quot; virtual register is not used in LLInt / baseline op_resolve_scope with ModuleVar,
6290                 // we need not to keep it alive by the Phantom node.
6291                 // Module environment is already strongly referenced by the CodeBlock.
6292                 set(bytecode.m_dst, weakJSConstant(lexicalEnvironment));
6293                 break;
6294             }
6295             case LocalClosureVar:
6296             case ClosureVar:
6297             case ClosureVarWithVarInjectionChecks: {
6298                 Node* localBase = get(bytecode.m_scope);
6299                 addToGraph(Phantom, localBase); // OSR exit cannot handle resolve_scope on a DCE&#39;d scope.
6300 
6301                 // We have various forms of constant folding here. This is necessary to avoid
6302                 // spurious recompiles in dead-but-foldable code.
<a name="67" id="anc67"></a><span class="line-added">6303 </span>
6304                 if (symbolTable) {
<a name="68" id="anc68"></a><span class="line-modified">6305                     if (JSScope* scope = symbolTable-&gt;singleton().inferredValue()) {</span>
<span class="line-modified">6306                         m_graph.watchpoints().addLazily(symbolTable);</span>
<span class="line-modified">6307                         set(bytecode.m_dst, weakJSConstant(scope));</span>

6308                         break;
6309                     }
6310                 }
6311                 if (JSScope* scope = localBase-&gt;dynamicCastConstant&lt;JSScope*&gt;(*m_vm)) {
6312                     for (unsigned n = depth; n--;)
6313                         scope = scope-&gt;next();
6314                     set(bytecode.m_dst, weakJSConstant(scope));
6315                     break;
6316                 }
6317                 for (unsigned n = depth; n--;)
6318                     localBase = addToGraph(SkipScope, localBase);
6319                 set(bytecode.m_dst, localBase);
6320                 break;
6321             }
6322             case UnresolvedProperty:
6323             case UnresolvedPropertyWithVarInjectionChecks: {
6324                 addToGraph(Phantom, get(bytecode.m_scope));
6325                 addToGraph(ForceOSRExit);
6326                 set(bytecode.m_dst, addToGraph(JSConstant, OpInfo(m_constantNull)));
6327                 break;
6328             }
6329             case Dynamic:
6330                 RELEASE_ASSERT_NOT_REACHED();
6331                 break;
6332             }
6333             NEXT_OPCODE(op_resolve_scope);
6334         }
6335         case op_resolve_scope_for_hoisting_func_decl_in_eval: {
6336             auto bytecode = currentInstruction-&gt;as&lt;OpResolveScopeForHoistingFuncDeclInEval&gt;();
6337             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
<a name="69" id="anc69"></a>
6338             set(bytecode.m_dst, addToGraph(ResolveScopeForHoistingFuncDeclInEval, OpInfo(identifierNumber), get(bytecode.m_scope)));
6339 
6340             NEXT_OPCODE(op_resolve_scope_for_hoisting_func_decl_in_eval);
6341         }
6342 
6343         case op_get_from_scope: {
6344             auto bytecode = currentInstruction-&gt;as&lt;OpGetFromScope&gt;();
6345             auto&amp; metadata = bytecode.metadata(codeBlock);
6346             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_var];
6347             UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
6348 
6349             ResolveType resolveType;
6350             GetPutInfo getPutInfo(0);
6351             Structure* structure = 0;
6352             WatchpointSet* watchpoints = 0;
6353             uintptr_t operand;
6354             {
6355                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
6356                 getPutInfo = metadata.m_getPutInfo;
6357                 resolveType = getPutInfo.resolveType();
6358                 if (resolveType == GlobalVar || resolveType == GlobalVarWithVarInjectionChecks || resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks)
6359                     watchpoints = metadata.m_watchpointSet;
6360                 else if (resolveType != UnresolvedProperty &amp;&amp; resolveType != UnresolvedPropertyWithVarInjectionChecks)
6361                     structure = metadata.m_structure.get();
6362                 operand = metadata.m_operand;
6363             }
6364 
6365             if (needsDynamicLookup(resolveType, op_get_from_scope)) {
6366                 uint64_t opInfo1 = makeDynamicVarOpInfo(identifierNumber, getPutInfo.operand());
6367                 SpeculatedType prediction = getPrediction();
6368                 set(bytecode.m_dst,
6369                     addToGraph(GetDynamicVar, OpInfo(opInfo1), OpInfo(prediction), get(bytecode.m_scope)));
6370                 NEXT_OPCODE(op_get_from_scope);
6371             }
6372 
6373             UNUSED_PARAM(watchpoints); // We will use this in the future. For now we set it as a way of documenting the fact that that&#39;s what index 5 is in GlobalVar mode.
6374 
6375             JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
6376 
6377             switch (resolveType) {
6378             case GlobalProperty:
6379             case GlobalPropertyWithVarInjectionChecks: {
6380                 // FIXME: Currently, module code do not query to JSGlobalLexicalEnvironment. So this case should be removed once it is fixed.
6381                 // https://bugs.webkit.org/show_bug.cgi?id=193347
6382                 if (m_inlineStackTop-&gt;m_codeBlock-&gt;scriptMode() != JSParserScriptMode::Module) {
6383                     if (!m_graph.watchGlobalProperty(globalObject, identifierNumber))
6384                         addToGraph(ForceOSRExit);
6385                 }
6386 
6387                 SpeculatedType prediction = getPrediction();
6388 
6389                 GetByIdStatus status = GetByIdStatus::computeFor(structure, uid);
6390                 if (status.state() != GetByIdStatus::Simple
6391                     || status.numVariants() != 1
6392                     || status[0].structureSet().size() != 1) {
6393                     set(bytecode.m_dst, addToGraph(GetByIdFlush, OpInfo(identifierNumber), OpInfo(prediction), get(bytecode.m_scope)));
6394                     break;
6395                 }
6396 
6397                 Node* base = weakJSConstant(globalObject);
6398                 Node* result = load(prediction, base, identifierNumber, status[0]);
6399                 addToGraph(Phantom, get(bytecode.m_scope));
6400                 set(bytecode.m_dst, result);
6401                 break;
6402             }
6403             case GlobalVar:
6404             case GlobalVarWithVarInjectionChecks:
6405             case GlobalLexicalVar:
6406             case GlobalLexicalVarWithVarInjectionChecks: {
6407                 addToGraph(Phantom, get(bytecode.m_scope));
6408                 WatchpointSet* watchpointSet;
6409                 ScopeOffset offset;
6410                 JSSegmentedVariableObject* scopeObject = jsCast&lt;JSSegmentedVariableObject*&gt;(JSScope::constantScopeForCodeBlock(resolveType, m_inlineStackTop-&gt;m_codeBlock));
6411                 {
6412                     ConcurrentJSLocker locker(scopeObject-&gt;symbolTable()-&gt;m_lock);
6413                     SymbolTableEntry entry = scopeObject-&gt;symbolTable()-&gt;get(locker, uid);
6414                     watchpointSet = entry.watchpointSet();
6415                     offset = entry.scopeOffset();
6416                 }
6417                 if (watchpointSet &amp;&amp; watchpointSet-&gt;state() == IsWatched) {
6418                     // This has a fun concurrency story. There is the possibility of a race in two
6419                     // directions:
6420                     //
6421                     // We see that the set IsWatched, but in the meantime it gets invalidated: this is
6422                     // fine because if we saw that it IsWatched then we add a watchpoint. If it gets
6423                     // invalidated, then this compilation is invalidated. Note that in the meantime we
6424                     // may load an absurd value from the global object. It&#39;s fine to load an absurd
6425                     // value if the compilation is invalidated anyway.
6426                     //
6427                     // We see that the set IsWatched, but the value isn&#39;t yet initialized: this isn&#39;t
6428                     // possible because of the ordering of operations.
6429                     //
6430                     // Here&#39;s how we order operations:
6431                     //
6432                     // Main thread stores to the global object: always store a value first, and only
6433                     // after that do we touch the watchpoint set. There is a fence in the touch, that
6434                     // ensures that the store to the global object always happens before the touch on the
6435                     // set.
6436                     //
6437                     // Compilation thread: always first load the state of the watchpoint set, and then
6438                     // load the value. The WatchpointSet::state() method does fences for us to ensure
6439                     // that the load of the state happens before our load of the value.
6440                     //
6441                     // Finalizing compilation: this happens on the main thread and synchronously checks
6442                     // validity of all watchpoint sets.
6443                     //
6444                     // We will only perform optimizations if the load of the state yields IsWatched. That
6445                     // means that at least one store would have happened to initialize the original value
6446                     // of the variable (that is, the value we&#39;d like to constant fold to). There may be
6447                     // other stores that happen after that, but those stores will invalidate the
6448                     // watchpoint set and also the compilation.
6449 
6450                     // Note that we need to use the operand, which is a direct pointer at the global,
6451                     // rather than looking up the global by doing variableAt(offset). That&#39;s because the
6452                     // internal data structures of JSSegmentedVariableObject are not thread-safe even
6453                     // though accessing the global itself is. The segmentation involves a vector spine
6454                     // that resizes with malloc/free, so if new globals unrelated to the one we are
6455                     // reading are added, we might access freed memory if we do variableAt().
6456                     WriteBarrier&lt;Unknown&gt;* pointer = bitwise_cast&lt;WriteBarrier&lt;Unknown&gt;*&gt;(operand);
6457 
6458                     ASSERT(scopeObject-&gt;findVariableIndex(pointer) == offset);
6459 
6460                     JSValue value = pointer-&gt;get();
6461                     if (value) {
6462                         m_graph.watchpoints().addLazily(watchpointSet);
6463                         set(bytecode.m_dst, weakJSConstant(value));
6464                         break;
6465                     }
6466                 }
6467 
6468                 SpeculatedType prediction = getPrediction();
6469                 NodeType nodeType;
6470                 if (resolveType == GlobalVar || resolveType == GlobalVarWithVarInjectionChecks)
6471                     nodeType = GetGlobalVar;
6472                 else
6473                     nodeType = GetGlobalLexicalVariable;
6474                 Node* value = addToGraph(nodeType, OpInfo(operand), OpInfo(prediction));
6475                 if (resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks)
6476                     addToGraph(CheckNotEmpty, value);
6477                 set(bytecode.m_dst, value);
6478                 break;
6479             }
6480             case LocalClosureVar:
6481             case ClosureVar:
6482             case ClosureVarWithVarInjectionChecks: {
6483                 Node* scopeNode = get(bytecode.m_scope);
6484 
6485                 // Ideally we wouldn&#39;t have to do this Phantom. But:
6486                 //
6487                 // For the constant case: we must do it because otherwise we would have no way of knowing
6488                 // that the scope is live at OSR here.
6489                 //
6490                 // For the non-constant case: GetClosureVar could be DCE&#39;d, but baseline&#39;s implementation
6491                 // won&#39;t be able to handle an Undefined scope.
6492                 addToGraph(Phantom, scopeNode);
6493 
6494                 // Constant folding in the bytecode parser is important for performance. This may not
6495                 // have executed yet. If it hasn&#39;t, then we won&#39;t have a prediction. Lacking a
6496                 // prediction, we&#39;d otherwise think that it has to exit. Then when it did execute, we
6497                 // would recompile. But if we can fold it here, we avoid the exit.
6498                 if (JSValue value = m_graph.tryGetConstantClosureVar(scopeNode, ScopeOffset(operand))) {
6499                     set(bytecode.m_dst, weakJSConstant(value));
6500                     break;
6501                 }
6502                 SpeculatedType prediction = getPrediction();
6503                 set(bytecode.m_dst,
6504                     addToGraph(GetClosureVar, OpInfo(operand), OpInfo(prediction), scopeNode));
6505                 break;
6506             }
6507             case UnresolvedProperty:
6508             case UnresolvedPropertyWithVarInjectionChecks:
6509             case ModuleVar:
6510             case Dynamic:
6511                 RELEASE_ASSERT_NOT_REACHED();
6512                 break;
6513             }
6514             NEXT_OPCODE(op_get_from_scope);
6515         }
6516 
6517         case op_put_to_scope: {
6518             auto bytecode = currentInstruction-&gt;as&lt;OpPutToScope&gt;();
6519             auto&amp; metadata = bytecode.metadata(codeBlock);
6520             unsigned identifierNumber = bytecode.m_var;
6521             if (identifierNumber != UINT_MAX)
6522                 identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[identifierNumber];
6523             UniquedStringImpl* uid;
6524             if (identifierNumber != UINT_MAX)
6525                 uid = m_graph.identifiers()[identifierNumber];
6526             else
6527                 uid = nullptr;
6528 
6529             ResolveType resolveType;
6530             GetPutInfo getPutInfo(0);
6531             Structure* structure = nullptr;
6532             WatchpointSet* watchpoints = nullptr;
6533             uintptr_t operand;
6534             {
6535                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
6536                 getPutInfo = metadata.m_getPutInfo;
6537                 resolveType = getPutInfo.resolveType();
6538                 if (resolveType == GlobalVar || resolveType == GlobalVarWithVarInjectionChecks || resolveType == LocalClosureVar || resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks)
6539                     watchpoints = metadata.m_watchpointSet;
6540                 else if (resolveType != UnresolvedProperty &amp;&amp; resolveType != UnresolvedPropertyWithVarInjectionChecks)
6541                     structure = metadata.m_structure.get();
6542                 operand = metadata.m_operand;
6543             }
6544 
6545             JSGlobalObject* globalObject = m_inlineStackTop-&gt;m_codeBlock-&gt;globalObject();
6546 
6547             if (needsDynamicLookup(resolveType, op_put_to_scope)) {
6548                 ASSERT(identifierNumber != UINT_MAX);
6549                 uint64_t opInfo1 = makeDynamicVarOpInfo(identifierNumber, getPutInfo.operand());
6550                 addToGraph(PutDynamicVar, OpInfo(opInfo1), OpInfo(), get(bytecode.m_scope), get(bytecode.m_value));
6551                 NEXT_OPCODE(op_put_to_scope);
6552             }
6553 
6554             switch (resolveType) {
6555             case GlobalProperty:
6556             case GlobalPropertyWithVarInjectionChecks: {
6557                 // FIXME: Currently, module code do not query to JSGlobalLexicalEnvironment. So this case should be removed once it is fixed.
6558                 // https://bugs.webkit.org/show_bug.cgi?id=193347
6559                 if (m_inlineStackTop-&gt;m_codeBlock-&gt;scriptMode() != JSParserScriptMode::Module) {
6560                     if (!m_graph.watchGlobalProperty(globalObject, identifierNumber))
6561                         addToGraph(ForceOSRExit);
6562                 }
6563 
6564                 PutByIdStatus status;
6565                 if (uid)
6566                     status = PutByIdStatus::computeFor(globalObject, structure, uid, false);
6567                 else
6568                     status = PutByIdStatus(PutByIdStatus::TakesSlowPath);
6569                 if (status.numVariants() != 1
6570                     || status[0].kind() != PutByIdVariant::Replace
6571                     || status[0].structure().size() != 1) {
6572                     addToGraph(PutById, OpInfo(identifierNumber), get(bytecode.m_scope), get(bytecode.m_value));
6573                     break;
6574                 }
6575                 Node* base = weakJSConstant(globalObject);
6576                 store(base, identifierNumber, status[0], get(bytecode.m_value));
6577                 // Keep scope alive until after put.
6578                 addToGraph(Phantom, get(bytecode.m_scope));
6579                 break;
6580             }
6581             case GlobalLexicalVar:
6582             case GlobalLexicalVarWithVarInjectionChecks:
6583             case GlobalVar:
6584             case GlobalVarWithVarInjectionChecks: {
6585                 if (!isInitialization(getPutInfo.initializationMode()) &amp;&amp; (resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks)) {
6586                     SpeculatedType prediction = SpecEmpty;
6587                     Node* value = addToGraph(GetGlobalLexicalVariable, OpInfo(operand), OpInfo(prediction));
6588                     addToGraph(CheckNotEmpty, value);
6589                 }
6590 
6591                 JSSegmentedVariableObject* scopeObject = jsCast&lt;JSSegmentedVariableObject*&gt;(JSScope::constantScopeForCodeBlock(resolveType, m_inlineStackTop-&gt;m_codeBlock));
6592                 if (watchpoints) {
6593                     SymbolTableEntry entry = scopeObject-&gt;symbolTable()-&gt;get(uid);
6594                     ASSERT_UNUSED(entry, watchpoints == entry.watchpointSet());
6595                 }
6596                 Node* valueNode = get(bytecode.m_value);
6597                 addToGraph(PutGlobalVariable, OpInfo(operand), weakJSConstant(scopeObject), valueNode);
6598                 if (watchpoints &amp;&amp; watchpoints-&gt;state() != IsInvalidated) {
6599                     // Must happen after the store. See comment for GetGlobalVar.
6600                     addToGraph(NotifyWrite, OpInfo(watchpoints));
6601                 }
6602                 // Keep scope alive until after put.
6603                 addToGraph(Phantom, get(bytecode.m_scope));
6604                 break;
6605             }
6606             case LocalClosureVar:
6607             case ClosureVar:
6608             case ClosureVarWithVarInjectionChecks: {
6609                 Node* scopeNode = get(bytecode.m_scope);
6610                 Node* valueNode = get(bytecode.m_value);
6611 
6612                 addToGraph(PutClosureVar, OpInfo(operand), scopeNode, valueNode);
6613 
6614                 if (watchpoints &amp;&amp; watchpoints-&gt;state() != IsInvalidated) {
6615                     // Must happen after the store. See comment for GetGlobalVar.
6616                     addToGraph(NotifyWrite, OpInfo(watchpoints));
6617                 }
6618                 break;
6619             }
6620 
6621             case ModuleVar:
6622                 // Need not to keep &quot;scope&quot; and &quot;value&quot; register values here by Phantom because
6623                 // they are not used in LLInt / baseline op_put_to_scope with ModuleVar.
6624                 addToGraph(ForceOSRExit);
6625                 break;
6626 
6627             case Dynamic:
6628             case UnresolvedProperty:
6629             case UnresolvedPropertyWithVarInjectionChecks:
6630                 RELEASE_ASSERT_NOT_REACHED();
6631                 break;
6632             }
6633             NEXT_OPCODE(op_put_to_scope);
6634         }
6635 
6636         case op_loop_hint: {
6637             // Baseline-&gt;DFG OSR jumps between loop hints. The DFG assumes that Baseline-&gt;DFG
6638             // OSR can only happen at basic block boundaries. Assert that these two statements
6639             // are compatible.
6640             RELEASE_ASSERT(m_currentIndex == blockBegin);
6641 
6642             // We never do OSR into an inlined code block. That could not happen, since OSR
6643             // looks up the code block that is the replacement for the baseline JIT code
6644             // block. Hence, machine code block = true code block = not inline code block.
6645             if (!m_inlineStackTop-&gt;m_caller)
6646                 m_currentBlock-&gt;isOSRTarget = true;
6647 
6648             addToGraph(LoopHint);
<a name="70" id="anc70"></a>



6649             addToGraph(Options::usePollingTraps() ? CheckTraps : InvalidationPoint);
<a name="71" id="anc71"></a><span class="line-modified">6650             NEXT_OPCODE(op_loop_hint);</span>
6651         }
6652 
6653         case op_nop: {
6654             addToGraph(Check); // We add a nop here so that basic block linking doesn&#39;t break.
6655             NEXT_OPCODE(op_nop);
6656         }
6657 
6658         case op_super_sampler_begin: {
6659             addToGraph(SuperSamplerBegin);
6660             NEXT_OPCODE(op_super_sampler_begin);
6661         }
6662 
6663         case op_super_sampler_end: {
6664             addToGraph(SuperSamplerEnd);
6665             NEXT_OPCODE(op_super_sampler_end);
6666         }
6667 
6668         case op_create_lexical_environment: {
6669             auto bytecode = currentInstruction-&gt;as&lt;OpCreateLexicalEnvironment&gt;();
6670             ASSERT(bytecode.m_symbolTable.isConstant() &amp;&amp; bytecode.m_initialValue.isConstant());
6671             FrozenValue* symbolTable = m_graph.freezeStrong(m_inlineStackTop-&gt;m_codeBlock-&gt;getConstant(bytecode.m_symbolTable.offset()));
6672             FrozenValue* initialValue = m_graph.freezeStrong(m_inlineStackTop-&gt;m_codeBlock-&gt;getConstant(bytecode.m_initialValue.offset()));
6673             Node* scope = get(bytecode.m_scope);
6674             Node* lexicalEnvironment = addToGraph(CreateActivation, OpInfo(symbolTable), OpInfo(initialValue), scope);
6675             set(bytecode.m_dst, lexicalEnvironment);
6676             NEXT_OPCODE(op_create_lexical_environment);
6677         }
6678 
6679         case op_push_with_scope: {
6680             auto bytecode = currentInstruction-&gt;as&lt;OpPushWithScope&gt;();
6681             Node* currentScope = get(bytecode.m_currentScope);
6682             Node* object = get(bytecode.m_newScope);
6683             set(bytecode.m_dst, addToGraph(PushWithScope, currentScope, object));
6684             NEXT_OPCODE(op_push_with_scope);
6685         }
6686 
6687         case op_get_parent_scope: {
6688             auto bytecode = currentInstruction-&gt;as&lt;OpGetParentScope&gt;();
6689             Node* currentScope = get(bytecode.m_scope);
6690             Node* newScope = addToGraph(SkipScope, currentScope);
6691             set(bytecode.m_dst, newScope);
6692             addToGraph(Phantom, currentScope);
6693             NEXT_OPCODE(op_get_parent_scope);
6694         }
6695 
6696         case op_get_scope: {
6697             // Help the later stages a bit by doing some small constant folding here. Note that this
6698             // only helps for the first basic block. It&#39;s extremely important not to constant fold
6699             // loads from the scope register later, as that would prevent the DFG from tracking the
6700             // bytecode-level liveness of the scope register.
6701             auto bytecode = currentInstruction-&gt;as&lt;OpGetScope&gt;();
6702             Node* callee = get(VirtualRegister(CallFrameSlot::callee));
6703             Node* result;
6704             if (JSFunction* function = callee-&gt;dynamicCastConstant&lt;JSFunction*&gt;(*m_vm))
6705                 result = weakJSConstant(function-&gt;scope());
6706             else
6707                 result = addToGraph(GetScope, callee);
6708             set(bytecode.m_dst, result);
6709             NEXT_OPCODE(op_get_scope);
6710         }
6711 
6712         case op_argument_count: {
6713             auto bytecode = currentInstruction-&gt;as&lt;OpArgumentCount&gt;();
6714             Node* sub = addToGraph(ArithSub, OpInfo(Arith::Unchecked), OpInfo(SpecInt32Only), getArgumentCount(), addToGraph(JSConstant, OpInfo(m_constantOne)));
6715             set(bytecode.m_dst, sub);
6716             NEXT_OPCODE(op_argument_count);
6717         }
6718 
6719         case op_create_direct_arguments: {
6720             auto bytecode = currentInstruction-&gt;as&lt;OpCreateDirectArguments&gt;();
6721             noticeArgumentsUse();
6722             Node* createArguments = addToGraph(CreateDirectArguments);
6723             set(bytecode.m_dst, createArguments);
6724             NEXT_OPCODE(op_create_direct_arguments);
6725         }
6726 
6727         case op_create_scoped_arguments: {
6728             auto bytecode = currentInstruction-&gt;as&lt;OpCreateScopedArguments&gt;();
6729             noticeArgumentsUse();
6730             Node* createArguments = addToGraph(CreateScopedArguments, get(bytecode.m_scope));
6731             set(bytecode.m_dst, createArguments);
6732             NEXT_OPCODE(op_create_scoped_arguments);
6733         }
6734 
6735         case op_create_cloned_arguments: {
6736             auto bytecode = currentInstruction-&gt;as&lt;OpCreateClonedArguments&gt;();
6737             noticeArgumentsUse();
6738             Node* createArguments = addToGraph(CreateClonedArguments);
6739             set(bytecode.m_dst, createArguments);
6740             NEXT_OPCODE(op_create_cloned_arguments);
6741         }
6742 
6743         case op_get_from_arguments: {
6744             auto bytecode = currentInstruction-&gt;as&lt;OpGetFromArguments&gt;();
6745             set(bytecode.m_dst,
6746                 addToGraph(
6747                     GetFromArguments,
6748                     OpInfo(bytecode.m_index),
6749                     OpInfo(getPrediction()),
6750                     get(bytecode.m_arguments)));
6751             NEXT_OPCODE(op_get_from_arguments);
6752         }
6753 
6754         case op_put_to_arguments: {
6755             auto bytecode = currentInstruction-&gt;as&lt;OpPutToArguments&gt;();
6756             addToGraph(
6757                 PutToArguments,
6758                 OpInfo(bytecode.m_index),
6759                 get(bytecode.m_arguments),
6760                 get(bytecode.m_value));
6761             NEXT_OPCODE(op_put_to_arguments);
6762         }
6763 
6764         case op_get_argument: {
6765             auto bytecode = currentInstruction-&gt;as&lt;OpGetArgument&gt;();
6766             InlineCallFrame* inlineCallFrame = this-&gt;inlineCallFrame();
6767             Node* argument;
6768             int32_t argumentIndexIncludingThis = bytecode.m_index;
6769             if (inlineCallFrame &amp;&amp; !inlineCallFrame-&gt;isVarargs()) {
6770                 int32_t argumentCountIncludingThisWithFixup = inlineCallFrame-&gt;argumentsWithFixup.size();
6771                 if (argumentIndexIncludingThis &lt; argumentCountIncludingThisWithFixup)
6772                     argument = get(virtualRegisterForArgument(argumentIndexIncludingThis));
6773                 else
6774                     argument = addToGraph(JSConstant, OpInfo(m_constantUndefined));
6775             } else
6776                 argument = addToGraph(GetArgument, OpInfo(argumentIndexIncludingThis), OpInfo(getPrediction()));
6777             set(bytecode.m_dst, argument);
6778             NEXT_OPCODE(op_get_argument);
6779         }
6780         case op_new_async_generator_func:
6781             handleNewFunc(NewAsyncGeneratorFunction, currentInstruction-&gt;as&lt;OpNewAsyncGeneratorFunc&gt;());
6782             NEXT_OPCODE(op_new_async_generator_func);
6783         case op_new_func:
6784             handleNewFunc(NewFunction, currentInstruction-&gt;as&lt;OpNewFunc&gt;());
6785             NEXT_OPCODE(op_new_func);
6786         case op_new_generator_func:
6787             handleNewFunc(NewGeneratorFunction, currentInstruction-&gt;as&lt;OpNewGeneratorFunc&gt;());
6788             NEXT_OPCODE(op_new_generator_func);
6789         case op_new_async_func:
6790             handleNewFunc(NewAsyncFunction, currentInstruction-&gt;as&lt;OpNewAsyncFunc&gt;());
6791             NEXT_OPCODE(op_new_async_func);
6792 
6793         case op_new_func_exp:
6794             handleNewFuncExp(NewFunction, currentInstruction-&gt;as&lt;OpNewFuncExp&gt;());
6795             NEXT_OPCODE(op_new_func_exp);
6796         case op_new_generator_func_exp:
6797             handleNewFuncExp(NewGeneratorFunction, currentInstruction-&gt;as&lt;OpNewGeneratorFuncExp&gt;());
6798             NEXT_OPCODE(op_new_generator_func_exp);
6799         case op_new_async_generator_func_exp:
6800             handleNewFuncExp(NewAsyncGeneratorFunction, currentInstruction-&gt;as&lt;OpNewAsyncGeneratorFuncExp&gt;());
6801             NEXT_OPCODE(op_new_async_generator_func_exp);
6802         case op_new_async_func_exp:
6803             handleNewFuncExp(NewAsyncFunction, currentInstruction-&gt;as&lt;OpNewAsyncFuncExp&gt;());
6804             NEXT_OPCODE(op_new_async_func_exp);
6805 
6806         case op_set_function_name: {
6807             auto bytecode = currentInstruction-&gt;as&lt;OpSetFunctionName&gt;();
6808             Node* func = get(bytecode.m_function);
6809             Node* name = get(bytecode.m_name);
6810             addToGraph(SetFunctionName, func, name);
6811             NEXT_OPCODE(op_set_function_name);
6812         }
6813 
6814         case op_typeof: {
6815             auto bytecode = currentInstruction-&gt;as&lt;OpTypeof&gt;();
6816             set(bytecode.m_dst, addToGraph(TypeOf, get(bytecode.m_value)));
6817             NEXT_OPCODE(op_typeof);
6818         }
6819 
6820         case op_to_number: {
6821             auto bytecode = currentInstruction-&gt;as&lt;OpToNumber&gt;();
6822             SpeculatedType prediction = getPrediction();
6823             Node* value = get(bytecode.m_operand);
6824             set(bytecode.m_dst, addToGraph(ToNumber, OpInfo(0), OpInfo(prediction), value));
6825             NEXT_OPCODE(op_to_number);
6826         }
6827 
6828         case op_to_string: {
6829             auto bytecode = currentInstruction-&gt;as&lt;OpToString&gt;();
6830             Node* value = get(bytecode.m_operand);
6831             set(bytecode.m_dst, addToGraph(ToString, value));
6832             NEXT_OPCODE(op_to_string);
6833         }
6834 
6835         case op_to_object: {
6836             auto bytecode = currentInstruction-&gt;as&lt;OpToObject&gt;();
6837             SpeculatedType prediction = getPrediction();
6838             Node* value = get(bytecode.m_operand);
6839             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_message];
6840             set(bytecode.m_dst, addToGraph(ToObject, OpInfo(identifierNumber), OpInfo(prediction), value));
6841             NEXT_OPCODE(op_to_object);
6842         }
6843 
6844         case op_in_by_val: {
6845             auto bytecode = currentInstruction-&gt;as&lt;OpInByVal&gt;();
6846             ArrayMode arrayMode = getArrayMode(bytecode.metadata(codeBlock).m_arrayProfile, Array::Read);
6847             set(bytecode.m_dst, addToGraph(InByVal, OpInfo(arrayMode.asWord()), get(bytecode.m_base), get(bytecode.m_property)));
6848             NEXT_OPCODE(op_in_by_val);
6849         }
6850 
6851         case op_in_by_id: {
6852             auto bytecode = currentInstruction-&gt;as&lt;OpInById&gt;();
6853             Node* base = get(bytecode.m_base);
6854             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
6855             UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
6856 
6857             InByIdStatus status = InByIdStatus::computeFor(
6858                 m_inlineStackTop-&gt;m_profiledBlock,
6859                 m_inlineStackTop-&gt;m_baselineMap, m_icContextStack,
6860                 currentCodeOrigin(), uid);
6861 
6862             if (status.isSimple()) {
6863                 bool allOK = true;
6864                 MatchStructureData* data = m_graph.m_matchStructureData.add();
6865                 for (const InByIdVariant&amp; variant : status.variants()) {
6866                     if (!check(variant.conditionSet())) {
6867                         allOK = false;
6868                         break;
6869                     }
6870                     for (Structure* structure : variant.structureSet()) {
6871                         MatchStructureVariant matchVariant;
6872                         matchVariant.structure = m_graph.registerStructure(structure);
6873                         matchVariant.result = variant.isHit();
6874 
6875                         data-&gt;variants.append(WTFMove(matchVariant));
6876                     }
6877                 }
6878 
6879                 if (allOK) {
6880                     addToGraph(FilterInByIdStatus, OpInfo(m_graph.m_plan.recordedStatuses().addInByIdStatus(currentCodeOrigin(), status)), base);
6881 
6882                     Node* match = addToGraph(MatchStructure, OpInfo(data), base);
6883                     set(bytecode.m_dst, match);
6884                     NEXT_OPCODE(op_in_by_id);
6885                 }
6886             }
6887 
6888             set(bytecode.m_dst, addToGraph(InById, OpInfo(identifierNumber), base));
6889             NEXT_OPCODE(op_in_by_id);
6890         }
6891 
6892         case op_get_enumerable_length: {
6893             auto bytecode = currentInstruction-&gt;as&lt;OpGetEnumerableLength&gt;();
6894             set(bytecode.m_dst, addToGraph(GetEnumerableLength, get(bytecode.m_base)));
6895             NEXT_OPCODE(op_get_enumerable_length);
6896         }
6897 
6898         case op_has_generic_property: {
6899             auto bytecode = currentInstruction-&gt;as&lt;OpHasGenericProperty&gt;();
6900             set(bytecode.m_dst, addToGraph(HasGenericProperty, get(bytecode.m_base), get(bytecode.m_property)));
6901             NEXT_OPCODE(op_has_generic_property);
6902         }
6903 
6904         case op_has_structure_property: {
6905             auto bytecode = currentInstruction-&gt;as&lt;OpHasStructureProperty&gt;();
6906             set(bytecode.m_dst, addToGraph(HasStructureProperty,
6907                 get(bytecode.m_base),
6908                 get(bytecode.m_property),
6909                 get(bytecode.m_enumerator)));
6910             NEXT_OPCODE(op_has_structure_property);
6911         }
6912 
6913         case op_has_indexed_property: {
6914             auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
6915             Node* base = get(bytecode.m_base);
6916             ArrayMode arrayMode = getArrayMode(bytecode.metadata(codeBlock).m_arrayProfile, Array::Read);
6917             Node* property = get(bytecode.m_property);
6918             addVarArgChild(base);
6919             addVarArgChild(property);
6920             addVarArgChild(nullptr);
6921             Node* hasIterableProperty = addToGraph(Node::VarArg, HasIndexedProperty, OpInfo(arrayMode.asWord()), OpInfo(static_cast&lt;uint32_t&gt;(PropertySlot::InternalMethodType::GetOwnProperty)));
6922             m_exitOK = false; // HasIndexedProperty must be treated as if it clobbers exit state, since FixupPhase may make it generic.
6923             set(bytecode.m_dst, hasIterableProperty);
6924             NEXT_OPCODE(op_has_indexed_property);
6925         }
6926 
6927         case op_get_direct_pname: {
6928             auto bytecode = currentInstruction-&gt;as&lt;OpGetDirectPname&gt;();
6929             SpeculatedType prediction = getPredictionWithoutOSRExit();
6930 
6931             Node* base = get(bytecode.m_base);
6932             Node* property = get(bytecode.m_property);
6933             Node* index = get(bytecode.m_index);
6934             Node* enumerator = get(bytecode.m_enumerator);
6935 
6936             addVarArgChild(base);
6937             addVarArgChild(property);
6938             addVarArgChild(index);
6939             addVarArgChild(enumerator);
6940             set(bytecode.m_dst, addToGraph(Node::VarArg, GetDirectPname, OpInfo(0), OpInfo(prediction)));
6941 
6942             NEXT_OPCODE(op_get_direct_pname);
6943         }
6944 
6945         case op_get_property_enumerator: {
6946             auto bytecode = currentInstruction-&gt;as&lt;OpGetPropertyEnumerator&gt;();
6947             set(bytecode.m_dst, addToGraph(GetPropertyEnumerator, get(bytecode.m_base)));
6948             NEXT_OPCODE(op_get_property_enumerator);
6949         }
6950 
6951         case op_enumerator_structure_pname: {
6952             auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorStructurePname&gt;();
6953             set(bytecode.m_dst, addToGraph(GetEnumeratorStructurePname,
6954                 get(bytecode.m_enumerator),
6955                 get(bytecode.m_index)));
6956             NEXT_OPCODE(op_enumerator_structure_pname);
6957         }
6958 
6959         case op_enumerator_generic_pname: {
6960             auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorGenericPname&gt;();
6961             set(bytecode.m_dst, addToGraph(GetEnumeratorGenericPname,
6962                 get(bytecode.m_enumerator),
6963                 get(bytecode.m_index)));
6964             NEXT_OPCODE(op_enumerator_generic_pname);
6965         }
6966 
6967         case op_to_index_string: {
6968             auto bytecode = currentInstruction-&gt;as&lt;OpToIndexString&gt;();
6969             set(bytecode.m_dst, addToGraph(ToIndexString, get(bytecode.m_index)));
6970             NEXT_OPCODE(op_to_index_string);
6971         }
6972 
6973         case op_log_shadow_chicken_prologue: {
6974             auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenPrologue&gt;();
6975             if (!m_inlineStackTop-&gt;m_inlineCallFrame)
6976                 addToGraph(LogShadowChickenPrologue, get(bytecode.m_scope));
6977             NEXT_OPCODE(op_log_shadow_chicken_prologue);
6978         }
6979 
6980         case op_log_shadow_chicken_tail: {
6981             auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenTail&gt;();
6982             if (!m_inlineStackTop-&gt;m_inlineCallFrame) {
6983                 // FIXME: The right solution for inlining is to elide these whenever the tail call
6984                 // ends up being inlined.
6985                 // https://bugs.webkit.org/show_bug.cgi?id=155686
6986                 addToGraph(LogShadowChickenTail, get(bytecode.m_thisValue), get(bytecode.m_scope));
6987             }
6988             NEXT_OPCODE(op_log_shadow_chicken_tail);
6989         }
6990 
6991         case op_unreachable: {
6992             flushForTerminal();
6993             addToGraph(Unreachable);
6994             LAST_OPCODE(op_unreachable);
6995         }
6996 
6997         default:
6998             // Parse failed! This should not happen because the capabilities checker
6999             // should have caught it.
7000             RELEASE_ASSERT_NOT_REACHED();
7001             return;
7002         }
7003     }
7004 }
7005 
7006 void ByteCodeParser::linkBlock(BasicBlock* block, Vector&lt;BasicBlock*&gt;&amp; possibleTargets)
7007 {
7008     ASSERT(!block-&gt;isLinked);
7009     ASSERT(!block-&gt;isEmpty());
7010     Node* node = block-&gt;terminal();
7011     ASSERT(node-&gt;isTerminal());
7012 
7013     switch (node-&gt;op()) {
7014     case Jump:
7015         node-&gt;targetBlock() = blockForBytecodeOffset(possibleTargets, node-&gt;targetBytecodeOffsetDuringParsing());
7016         break;
7017 
7018     case Branch: {
7019         BranchData* data = node-&gt;branchData();
7020         data-&gt;taken.block = blockForBytecodeOffset(possibleTargets, data-&gt;takenBytecodeIndex());
7021         data-&gt;notTaken.block = blockForBytecodeOffset(possibleTargets, data-&gt;notTakenBytecodeIndex());
7022         break;
7023     }
7024 
7025     case Switch: {
7026         SwitchData* data = node-&gt;switchData();
7027         for (unsigned i = node-&gt;switchData()-&gt;cases.size(); i--;)
7028             data-&gt;cases[i].target.block = blockForBytecodeOffset(possibleTargets, data-&gt;cases[i].target.bytecodeIndex());
7029         data-&gt;fallThrough.block = blockForBytecodeOffset(possibleTargets, data-&gt;fallThrough.bytecodeIndex());
7030         break;
7031     }
7032 
7033     default:
7034         RELEASE_ASSERT_NOT_REACHED();
7035     }
7036 
7037     VERBOSE_LOG(&quot;Marking &quot;, RawPointer(block), &quot; as linked (actually did linking)\n&quot;);
7038     block-&gt;didLink();
7039 }
7040 
7041 void ByteCodeParser::linkBlocks(Vector&lt;BasicBlock*&gt;&amp; unlinkedBlocks, Vector&lt;BasicBlock*&gt;&amp; possibleTargets)
7042 {
7043     for (size_t i = 0; i &lt; unlinkedBlocks.size(); ++i) {
7044         VERBOSE_LOG(&quot;Attempting to link &quot;, RawPointer(unlinkedBlocks[i]), &quot;\n&quot;);
7045         linkBlock(unlinkedBlocks[i], possibleTargets);
7046     }
7047 }
7048 
7049 ByteCodeParser::InlineStackEntry::InlineStackEntry(
7050     ByteCodeParser* byteCodeParser,
7051     CodeBlock* codeBlock,
7052     CodeBlock* profiledBlock,
7053     JSFunction* callee, // Null if this is a closure call.
7054     VirtualRegister returnValueVR,
7055     VirtualRegister inlineCallFrameStart,
7056     int argumentCountIncludingThis,
7057     InlineCallFrame::Kind kind,
7058     BasicBlock* continuationBlock)
7059     : m_byteCodeParser(byteCodeParser)
7060     , m_codeBlock(codeBlock)
7061     , m_profiledBlock(profiledBlock)
7062     , m_continuationBlock(continuationBlock)
7063     , m_returnValue(returnValueVR)
7064     , m_caller(byteCodeParser-&gt;m_inlineStackTop)
7065 {
7066     {
7067         m_exitProfile.initialize(m_profiledBlock-&gt;unlinkedCodeBlock());
7068 
7069         ConcurrentJSLocker locker(m_profiledBlock-&gt;m_lock);
7070         m_lazyOperands.initialize(locker, m_profiledBlock-&gt;lazyOperandValueProfiles(locker));
7071 
7072         // We do this while holding the lock because we want to encourage StructureStubInfo&#39;s
7073         // to be potentially added to operations and because the profiled block could be in the
7074         // middle of LLInt-&gt;JIT tier-up in which case we would be adding the info&#39;s right now.
7075         if (m_profiledBlock-&gt;hasBaselineJITProfiling())
7076             m_profiledBlock-&gt;getICStatusMap(locker, m_baselineMap);
7077     }
7078 
7079     CodeBlock* optimizedBlock = m_profiledBlock-&gt;replacement();
7080     m_optimizedContext.optimizedCodeBlock = optimizedBlock;
7081     if (Options::usePolyvariantDevirtualization() &amp;&amp; optimizedBlock) {
7082         ConcurrentJSLocker locker(optimizedBlock-&gt;m_lock);
7083         optimizedBlock-&gt;getICStatusMap(locker, m_optimizedContext.map);
7084     }
7085     byteCodeParser-&gt;m_icContextStack.append(&amp;m_optimizedContext);
7086 
7087     int argumentCountIncludingThisWithFixup = std::max&lt;int&gt;(argumentCountIncludingThis, codeBlock-&gt;numParameters());
7088 
7089     if (m_caller) {
7090         // Inline case.
7091         ASSERT(codeBlock != byteCodeParser-&gt;m_codeBlock);
7092         ASSERT(inlineCallFrameStart.isValid());
7093 
7094         m_inlineCallFrame = byteCodeParser-&gt;m_graph.m_plan.inlineCallFrames()-&gt;add();
7095         m_optimizedContext.inlineCallFrame = m_inlineCallFrame;
7096 
7097         // The owner is the machine code block, and we already have a barrier on that when the
7098         // plan finishes.
7099         m_inlineCallFrame-&gt;baselineCodeBlock.setWithoutWriteBarrier(codeBlock-&gt;baselineVersion());
7100         m_inlineCallFrame-&gt;setStackOffset(inlineCallFrameStart.offset() - CallFrame::headerSizeInRegisters);
7101         m_inlineCallFrame-&gt;argumentCountIncludingThis = argumentCountIncludingThis;
7102         if (callee) {
7103             m_inlineCallFrame-&gt;calleeRecovery = ValueRecovery::constant(callee);
7104             m_inlineCallFrame-&gt;isClosureCall = false;
7105         } else
7106             m_inlineCallFrame-&gt;isClosureCall = true;
7107         m_inlineCallFrame-&gt;directCaller = byteCodeParser-&gt;currentCodeOrigin();
7108         m_inlineCallFrame-&gt;argumentsWithFixup.resizeToFit(argumentCountIncludingThisWithFixup); // Set the number of arguments including this, but don&#39;t configure the value recoveries, yet.
7109         m_inlineCallFrame-&gt;kind = kind;
7110 
7111         m_identifierRemap.resize(codeBlock-&gt;numberOfIdentifiers());
7112         m_switchRemap.resize(codeBlock-&gt;numberOfSwitchJumpTables());
7113 
7114         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfIdentifiers(); ++i) {
7115             UniquedStringImpl* rep = codeBlock-&gt;identifier(i).impl();
7116             unsigned index = byteCodeParser-&gt;m_graph.identifiers().ensure(rep);
7117             m_identifierRemap[i] = index;
7118         }
7119         for (unsigned i = 0; i &lt; codeBlock-&gt;numberOfSwitchJumpTables(); ++i) {
7120             m_switchRemap[i] = byteCodeParser-&gt;m_codeBlock-&gt;numberOfSwitchJumpTables();
<a name="72" id="anc72"></a><span class="line-modified">7121             byteCodeParser-&gt;m_codeBlock-&gt;addSwitchJumpTableFromProfiledCodeBlock(codeBlock-&gt;switchJumpTable(i));</span>
7122         }
7123     } else {
7124         // Machine code block case.
7125         ASSERT(codeBlock == byteCodeParser-&gt;m_codeBlock);
7126         ASSERT(!callee);
7127         ASSERT(!returnValueVR.isValid());
7128         ASSERT(!inlineCallFrameStart.isValid());
7129 
7130         m_inlineCallFrame = 0;
7131 
7132         m_identifierRemap.resize(codeBlock-&gt;numberOfIdentifiers());
7133         m_switchRemap.resize(codeBlock-&gt;numberOfSwitchJumpTables());
7134         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfIdentifiers(); ++i)
7135             m_identifierRemap[i] = i;
7136         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfSwitchJumpTables(); ++i)
7137             m_switchRemap[i] = i;
7138     }
7139 
7140     m_argumentPositions.resize(argumentCountIncludingThisWithFixup);
7141     for (int i = 0; i &lt; argumentCountIncludingThisWithFixup; ++i) {
7142         byteCodeParser-&gt;m_graph.m_argumentPositions.append(ArgumentPosition());
7143         ArgumentPosition* argumentPosition = &amp;byteCodeParser-&gt;m_graph.m_argumentPositions.last();
7144         m_argumentPositions[i] = argumentPosition;
7145     }
7146     byteCodeParser-&gt;m_inlineCallFrameToArgumentPositions.add(m_inlineCallFrame, m_argumentPositions);
7147 
7148     byteCodeParser-&gt;m_inlineStackTop = this;
7149 }
7150 
7151 ByteCodeParser::InlineStackEntry::~InlineStackEntry()
7152 {
7153     m_byteCodeParser-&gt;m_inlineStackTop = m_caller;
7154     RELEASE_ASSERT(m_byteCodeParser-&gt;m_icContextStack.last() == &amp;m_optimizedContext);
7155     m_byteCodeParser-&gt;m_icContextStack.removeLast();
7156 }
7157 
7158 void ByteCodeParser::parseCodeBlock()
7159 {
7160     clearCaches();
7161 
7162     CodeBlock* codeBlock = m_inlineStackTop-&gt;m_codeBlock;
7163 
7164     if (UNLIKELY(m_graph.compilation())) {
7165         m_graph.compilation()-&gt;addProfiledBytecodes(
7166             *m_vm-&gt;m_perBytecodeProfiler, m_inlineStackTop-&gt;m_profiledBlock);
7167     }
7168 
7169     if (UNLIKELY(Options::dumpSourceAtDFGTime())) {
7170         Vector&lt;DeferredSourceDump&gt;&amp; deferredSourceDump = m_graph.m_plan.callback()-&gt;ensureDeferredSourceDump();
7171         if (inlineCallFrame()) {
<a name="73" id="anc73"></a><span class="line-modified">7172             DeferredSourceDump dump(codeBlock-&gt;baselineVersion(), m_codeBlock, JITType::DFGJIT, inlineCallFrame()-&gt;directCaller.bytecodeIndex());</span>
7173             deferredSourceDump.append(dump);
7174         } else
7175             deferredSourceDump.append(DeferredSourceDump(codeBlock-&gt;baselineVersion()));
7176     }
7177 
7178     if (Options::dumpBytecodeAtDFGTime()) {
7179         dataLog(&quot;Parsing &quot;, *codeBlock);
7180         if (inlineCallFrame()) {
7181             dataLog(
<a name="74" id="anc74"></a><span class="line-modified">7182                 &quot; for inlining at &quot;, CodeBlockWithJITType(m_codeBlock, JITType::DFGJIT),</span>
7183                 &quot; &quot;, inlineCallFrame()-&gt;directCaller);
7184         }
7185         dataLog(
7186             &quot;, isStrictMode = &quot;, codeBlock-&gt;ownerExecutable()-&gt;isStrictMode(), &quot;\n&quot;);
7187         codeBlock-&gt;baselineVersion()-&gt;dumpBytecode();
7188     }
7189 
7190     Vector&lt;InstructionStream::Offset, 32&gt; jumpTargets;
7191     computePreciseJumpTargets(codeBlock, jumpTargets);
7192     if (Options::dumpBytecodeAtDFGTime()) {
7193         dataLog(&quot;Jump targets: &quot;);
7194         CommaPrinter comma;
7195         for (unsigned i = 0; i &lt; jumpTargets.size(); ++i)
7196             dataLog(comma, jumpTargets[i]);
7197         dataLog(&quot;\n&quot;);
7198     }
7199 
7200     for (unsigned jumpTargetIndex = 0; jumpTargetIndex &lt;= jumpTargets.size(); ++jumpTargetIndex) {
7201         // The maximum bytecode offset to go into the current basicblock is either the next jump target, or the end of the instructions.
7202         unsigned limit = jumpTargetIndex &lt; jumpTargets.size() ? jumpTargets[jumpTargetIndex] : codeBlock-&gt;instructions().size();
7203         ASSERT(m_currentIndex &lt; limit);
7204 
7205         // Loop until we reach the current limit (i.e. next jump target).
7206         do {
7207             // There may already be a currentBlock in two cases:
7208             // - we may have just entered the loop for the first time
7209             // - we may have just returned from an inlined callee that had some early returns and
7210             //   so allocated a continuation block, and the instruction after the call is a jump target.
7211             // In both cases, we want to keep using it.
7212             if (!m_currentBlock) {
7213                 m_currentBlock = allocateTargetableBlock(m_currentIndex);
7214 
7215                 // The first block is definitely an OSR target.
7216                 if (m_graph.numBlocks() == 1) {
7217                     m_currentBlock-&gt;isOSRTarget = true;
7218                     m_graph.m_roots.append(m_currentBlock);
7219                 }
7220                 prepareToParseBlock();
7221             }
7222 
7223             parseBlock(limit);
7224 
7225             // We should not have gone beyond the limit.
7226             ASSERT(m_currentIndex &lt;= limit);
7227 
7228             if (m_currentBlock-&gt;isEmpty()) {
7229                 // This case only happens if the last instruction was an inlined call with early returns
7230                 // or polymorphic (creating an empty continuation block),
7231                 // and then we hit the limit before putting anything in the continuation block.
7232                 ASSERT(m_currentIndex == limit);
7233                 makeBlockTargetable(m_currentBlock, m_currentIndex);
7234             } else {
7235                 ASSERT(m_currentBlock-&gt;terminal() || (m_currentIndex == codeBlock-&gt;instructions().size() &amp;&amp; inlineCallFrame()));
7236                 m_currentBlock = nullptr;
7237             }
7238         } while (m_currentIndex &lt; limit);
7239     }
7240 
7241     // Should have reached the end of the instructions.
7242     ASSERT(m_currentIndex == codeBlock-&gt;instructions().size());
7243 
7244     VERBOSE_LOG(&quot;Done parsing &quot;, *codeBlock, &quot; (fell off end)\n&quot;);
7245 }
7246 
7247 template &lt;typename Bytecode&gt;
7248 void ByteCodeParser::handlePutByVal(Bytecode bytecode, unsigned instructionSize)
7249 {
7250     Node* base = get(bytecode.m_base);
7251     Node* property = get(bytecode.m_property);
7252     Node* value = get(bytecode.m_value);
7253     bool isDirect = Bytecode::opcodeID == op_put_by_val_direct;
7254     bool compiledAsPutById = false;
7255     {
7256         unsigned identifierNumber = std::numeric_limits&lt;unsigned&gt;::max();
7257         PutByIdStatus putByIdStatus;
7258         {
7259             ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
<a name="75" id="anc75"></a><span class="line-modified">7260             ByValInfo* byValInfo = m_inlineStackTop-&gt;m_baselineMap.get(CodeOrigin(currentCodeOrigin().bytecodeIndex())).byValInfo;</span>
7261             // FIXME: When the bytecode is not compiled in the baseline JIT, byValInfo becomes null.
7262             // At that time, there is no information.
7263             if (byValInfo
7264                 &amp;&amp; byValInfo-&gt;stubInfo
7265                 &amp;&amp; !byValInfo-&gt;tookSlowPath
7266                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIdent)
7267                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType)
7268                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
7269                 compiledAsPutById = true;
7270                 identifierNumber = m_graph.identifiers().ensure(byValInfo-&gt;cachedId.impl());
7271                 UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
7272 
7273                 if (Symbol* symbol = byValInfo-&gt;cachedSymbol.get()) {
7274                     FrozenValue* frozen = m_graph.freezeStrong(symbol);
7275                     addToGraph(CheckCell, OpInfo(frozen), property);
7276                 } else {
7277                     ASSERT(!uid-&gt;isSymbol());
7278                     addToGraph(CheckStringIdent, OpInfo(uid), property);
7279                 }
7280 
7281                 putByIdStatus = PutByIdStatus::computeForStubInfo(
7282                     locker, m_inlineStackTop-&gt;m_profiledBlock,
7283                     byValInfo-&gt;stubInfo, currentCodeOrigin(), uid);
7284 
7285             }
7286         }
7287 
7288         if (compiledAsPutById)
7289             handlePutById(base, identifierNumber, value, putByIdStatus, isDirect, instructionSize);
7290     }
7291 
7292     if (!compiledAsPutById) {
7293         ArrayMode arrayMode = getArrayMode(bytecode.metadata(m_inlineStackTop-&gt;m_codeBlock).m_arrayProfile, Array::Write);
7294 
7295         addVarArgChild(base);
7296         addVarArgChild(property);
7297         addVarArgChild(value);
7298         addVarArgChild(0); // Leave room for property storage.
7299         addVarArgChild(0); // Leave room for length.
7300         addToGraph(Node::VarArg, isDirect ? PutByValDirect : PutByVal, OpInfo(arrayMode.asWord()), OpInfo(0));
7301         m_exitOK = false; // PutByVal and PutByValDirect must be treated as if they clobber exit state, since FixupPhase may make them generic.
7302     }
7303 }
7304 
7305 template &lt;typename Bytecode&gt;
7306 void ByteCodeParser::handlePutAccessorById(NodeType op, Bytecode bytecode)
7307 {
7308     Node* base = get(bytecode.m_base);
7309     unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
7310     Node* accessor = get(bytecode.m_accessor);
7311     addToGraph(op, OpInfo(identifierNumber), OpInfo(bytecode.m_attributes), base, accessor);
7312 }
7313 
7314 template &lt;typename Bytecode&gt;
7315 void ByteCodeParser::handlePutAccessorByVal(NodeType op, Bytecode bytecode)
7316 {
7317     Node* base = get(bytecode.m_base);
7318     Node* subscript = get(bytecode.m_property);
7319     Node* accessor = get(bytecode.m_accessor);
7320     addToGraph(op, OpInfo(bytecode.m_attributes), base, subscript, accessor);
7321 }
7322 
7323 template &lt;typename Bytecode&gt;
7324 void ByteCodeParser::handleNewFunc(NodeType op, Bytecode bytecode)
7325 {
7326     FunctionExecutable* decl = m_inlineStackTop-&gt;m_profiledBlock-&gt;functionDecl(bytecode.m_functionDecl);
7327     FrozenValue* frozen = m_graph.freezeStrong(decl);
7328     Node* scope = get(bytecode.m_scope);
7329     set(bytecode.m_dst, addToGraph(op, OpInfo(frozen), scope));
7330     // Ideally we wouldn&#39;t have to do this Phantom. But:
7331     //
7332     // For the constant case: we must do it because otherwise we would have no way of knowing
7333     // that the scope is live at OSR here.
7334     //
7335     // For the non-constant case: NewFunction could be DCE&#39;d, but baseline&#39;s implementation
7336     // won&#39;t be able to handle an Undefined scope.
7337     addToGraph(Phantom, scope);
7338 }
7339 
7340 template &lt;typename Bytecode&gt;
7341 void ByteCodeParser::handleNewFuncExp(NodeType op, Bytecode bytecode)
7342 {
7343     FunctionExecutable* expr = m_inlineStackTop-&gt;m_profiledBlock-&gt;functionExpr(bytecode.m_functionDecl);
7344     FrozenValue* frozen = m_graph.freezeStrong(expr);
7345     Node* scope = get(bytecode.m_scope);
7346     set(bytecode.m_dst, addToGraph(op, OpInfo(frozen), scope));
7347     // Ideally we wouldn&#39;t have to do this Phantom. But:
7348     //
7349     // For the constant case: we must do it because otherwise we would have no way of knowing
7350     // that the scope is live at OSR here.
7351     //
7352     // For the non-constant case: NewFunction could be DCE&#39;d, but baseline&#39;s implementation
7353     // won&#39;t be able to handle an Undefined scope.
7354     addToGraph(Phantom, scope);
7355 }
7356 
7357 void ByteCodeParser::parse()
7358 {
7359     // Set during construction.
7360     ASSERT(!m_currentIndex);
7361 
7362     VERBOSE_LOG(&quot;Parsing &quot;, *m_codeBlock, &quot;\n&quot;);
7363 
7364     InlineStackEntry inlineStackEntry(
7365         this, m_codeBlock, m_profiledBlock, 0, VirtualRegister(), VirtualRegister(),
7366         m_codeBlock-&gt;numParameters(), InlineCallFrame::Call, nullptr);
7367 
7368     parseCodeBlock();
7369     linkBlocks(inlineStackEntry.m_unlinkedBlocks, inlineStackEntry.m_blockLinkingTargets);
7370 
7371     if (m_hasAnyForceOSRExits) {
7372         BlockSet blocksToIgnore;
7373         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
7374             if (block-&gt;isOSRTarget &amp;&amp; block-&gt;bytecodeBegin == m_graph.m_plan.osrEntryBytecodeIndex()) {
7375                 blocksToIgnore.add(block);
7376                 break;
7377             }
7378         }
7379 
7380         {
7381             bool isSafeToValidate = false;
7382             auto postOrder = m_graph.blocksInPostOrder(isSafeToValidate); // This algorithm doesn&#39;t rely on the predecessors list, which is not yet built.
7383             bool changed;
7384             do {
7385                 changed = false;
7386                 for (BasicBlock* block : postOrder) {
7387                     for (BasicBlock* successor : block-&gt;successors()) {
7388                         if (blocksToIgnore.contains(successor)) {
7389                             changed |= blocksToIgnore.add(block);
7390                             break;
7391                         }
7392                     }
7393                 }
7394             } while (changed);
7395         }
7396 
7397         InsertionSet insertionSet(m_graph);
7398         Operands&lt;VariableAccessData*&gt; mapping(OperandsLike, m_graph.block(0)-&gt;variablesAtHead);
7399 
7400         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
7401             if (blocksToIgnore.contains(block))
7402                 continue;
7403 
7404             mapping.fill(nullptr);
7405             if (validationEnabled()) {
7406                 // Verify that it&#39;s correct to fill mapping with nullptr.
7407                 for (unsigned i = 0; i &lt; block-&gt;variablesAtHead.size(); ++i) {
7408                     Node* node = block-&gt;variablesAtHead.at(i);
7409                     RELEASE_ASSERT(!node);
7410                 }
7411             }
7412 
7413             for (unsigned nodeIndex = 0; nodeIndex &lt; block-&gt;size(); ++nodeIndex) {
<a name="76" id="anc76"></a><span class="line-modified">7414                 {</span>
<span class="line-added">7415                     Node* node = block-&gt;at(nodeIndex);</span>
<span class="line-added">7416 </span>
<span class="line-added">7417                     if (node-&gt;hasVariableAccessData(m_graph))</span>
<span class="line-added">7418                         mapping.operand(node-&gt;local()) = node-&gt;variableAccessData();</span>
7419 
<a name="77" id="anc77"></a><span class="line-modified">7420                     if (node-&gt;op() != ForceOSRExit)</span>
<span class="line-modified">7421                         continue;</span>
<span class="line-added">7422                 }</span>
<span class="line-added">7423 </span>
<span class="line-added">7424                 NodeOrigin origin = block-&gt;at(nodeIndex)-&gt;origin;</span>
<span class="line-added">7425                 RELEASE_ASSERT(origin.exitOK);</span>
7426 
<a name="78" id="anc78"></a><span class="line-modified">7427                 ++nodeIndex;</span>

7428 
<a name="79" id="anc79"></a><span class="line-added">7429                 {</span>
7430                     if (validationEnabled()) {
7431                         // This verifies that we don&#39;t need to change any of the successors&#39;s predecessor
7432                         // list after planting the Unreachable below. At this point in the bytecode
7433                         // parser, we haven&#39;t linked up the predecessor lists yet.
7434                         for (BasicBlock* successor : block-&gt;successors())
7435                             RELEASE_ASSERT(successor-&gt;predecessors.isEmpty());
7436                     }
7437 
<a name="80" id="anc80"></a>



7438                     auto insertLivenessPreservingOp = [&amp;] (InlineCallFrame* inlineCallFrame, NodeType op, VirtualRegister operand) {
7439                         VariableAccessData* variable = mapping.operand(operand);
7440                         if (!variable) {
7441                             variable = newVariableAccessData(operand);
7442                             mapping.operand(operand) = variable;
7443                         }
7444 
7445                         VirtualRegister argument = operand - (inlineCallFrame ? inlineCallFrame-&gt;stackOffset : 0);
7446                         if (argument.isArgument() &amp;&amp; !argument.isHeader()) {
7447                             const Vector&lt;ArgumentPosition*&gt;&amp; arguments = m_inlineCallFrameToArgumentPositions.get(inlineCallFrame);
7448                             arguments[argument.toArgument()]-&gt;addVariable(variable);
<a name="81" id="anc81"></a><span class="line-modified">7449                         }</span>
<span class="line-added">7450                         insertionSet.insertNode(nodeIndex, SpecNone, op, origin, OpInfo(variable));</span>
7451                     };
7452                     auto addFlushDirect = [&amp;] (InlineCallFrame* inlineCallFrame, VirtualRegister operand) {
7453                         insertLivenessPreservingOp(inlineCallFrame, Flush, operand);
7454                     };
7455                     auto addPhantomLocalDirect = [&amp;] (InlineCallFrame* inlineCallFrame, VirtualRegister operand) {
7456                         insertLivenessPreservingOp(inlineCallFrame, PhantomLocal, operand);
7457                     };
<a name="82" id="anc82"></a><span class="line-modified">7458                     flushForTerminalImpl(origin.semantic, addFlushDirect, addPhantomLocalDirect);</span>
<span class="line-added">7459                 }</span>
7460 
<a name="83" id="anc83"></a><span class="line-modified">7461                 while (true) {</span>
<span class="line-modified">7462                     RELEASE_ASSERT(nodeIndex &lt; block-&gt;size());</span>
<span class="line-modified">7463 </span>
<span class="line-added">7464                     Node* node = block-&gt;at(nodeIndex);</span>
<span class="line-added">7465 </span>
<span class="line-added">7466                     node-&gt;origin = origin;</span>
<span class="line-added">7467                     m_graph.doToChildren(node, [&amp;] (Edge edge) {</span>
<span class="line-added">7468                         // We only need to keep data flow edges to nodes defined prior to the ForceOSRExit. The reason</span>
<span class="line-added">7469                         // for this is we rely on backwards propagation being able to see the &quot;full&quot; bytecode. To model</span>
<span class="line-added">7470                         // this, we preserve uses of a node in a generic way so that backwards propagation can reason</span>
<span class="line-added">7471                         // about them. Therefore, we can&#39;t remove uses of a node which is defined before the ForceOSRExit</span>
<span class="line-added">7472                         // even when we&#39;re at a point in the program after the ForceOSRExit, because that would break backwards</span>
<span class="line-added">7473                         // propagation&#39;s analysis over the uses of a node. However, we don&#39;t need this same preservation for</span>
<span class="line-added">7474                         // nodes defined after ForceOSRExit, as we&#39;ve already exitted before those defs.</span>
<span class="line-added">7475                         if (edge-&gt;hasResult())</span>
<span class="line-added">7476                             insertionSet.insertNode(nodeIndex, SpecNone, Phantom, origin, Edge(edge.node(), UntypedUse));</span>
<span class="line-added">7477                     });</span>
<span class="line-added">7478 </span>
<span class="line-added">7479                     bool isTerminal = node-&gt;isTerminal();</span>
<span class="line-added">7480 </span>
<span class="line-added">7481                     node-&gt;removeWithoutChecks();</span>
<span class="line-added">7482 </span>
<span class="line-added">7483                     if (isTerminal) {</span>
<span class="line-added">7484                         insertionSet.insertNode(nodeIndex, SpecNone, Unreachable, origin);</span>
<span class="line-added">7485                         break;</span>
<span class="line-added">7486                     }</span>
<span class="line-added">7487 </span>
<span class="line-added">7488                     ++nodeIndex;</span>
7489                 }
<a name="84" id="anc84"></a><span class="line-added">7490 </span>
<span class="line-added">7491                 insertionSet.execute(block);</span>
<span class="line-added">7492 </span>
<span class="line-added">7493                 auto nodeAndIndex = block-&gt;findTerminal();</span>
<span class="line-added">7494                 RELEASE_ASSERT(nodeAndIndex.node-&gt;op() == Unreachable);</span>
<span class="line-added">7495                 block-&gt;resize(nodeAndIndex.index + 1);</span>
<span class="line-added">7496                 break;</span>
7497             }
7498         }
7499     } else if (validationEnabled()) {
7500         // Ensure our bookkeeping for ForceOSRExit nodes is working.
7501         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
7502             for (Node* node : *block)
7503                 RELEASE_ASSERT(node-&gt;op() != ForceOSRExit);
7504         }
7505     }
7506 
7507     m_graph.determineReachability();
7508     m_graph.killUnreachableBlocks();
7509 
7510     for (BlockIndex blockIndex = m_graph.numBlocks(); blockIndex--;) {
7511         BasicBlock* block = m_graph.block(blockIndex);
7512         if (!block)
7513             continue;
7514         ASSERT(block-&gt;variablesAtHead.numberOfLocals() == m_graph.block(0)-&gt;variablesAtHead.numberOfLocals());
7515         ASSERT(block-&gt;variablesAtHead.numberOfArguments() == m_graph.block(0)-&gt;variablesAtHead.numberOfArguments());
7516         ASSERT(block-&gt;variablesAtTail.numberOfLocals() == m_graph.block(0)-&gt;variablesAtHead.numberOfLocals());
7517         ASSERT(block-&gt;variablesAtTail.numberOfArguments() == m_graph.block(0)-&gt;variablesAtHead.numberOfArguments());
7518     }
7519 
7520     m_graph.m_localVars = m_numLocals;
7521     m_graph.m_parameterSlots = m_parameterSlots;
7522 }
7523 
7524 void parse(Graph&amp; graph)
7525 {
7526     ByteCodeParser(graph).parse();
7527 }
7528 
7529 } } // namespace JSC::DFG
7530 
7531 #endif
<a name="85" id="anc85"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="85" type="hidden" />
</body>
</html>