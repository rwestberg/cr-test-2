<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  *  Copyright (C) 1999-2000 Harri Porten (porten@kde.org)
  3  *  Copyright (C) 2001 Peter Kelly (pmk@post.com)
<a name="1" id="anc1"></a><span class="line-modified">  4  *  Copyright (C) 2003-2019 Apple Inc. All rights reserved.</span>
  5  *
  6  *  This library is free software; you can redistribute it and/or
  7  *  modify it under the terms of the GNU Lesser General Public
  8  *  License as published by the Free Software Foundation; either
  9  *  version 2 of the License, or (at your option) any later version.
 10  *
 11  *  This library is distributed in the hope that it will be useful,
 12  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 13  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 14  *  Lesser General Public License for more details.
 15  *
 16  *  You should have received a copy of the GNU Lesser General Public
 17  *  License along with this library; if not, write to the Free Software
 18  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 19  *
 20  */
 21 
 22 #pragma once
 23 
 24 #include &quot;CellAttributes.h&quot;
 25 #include &quot;DestructionMode.h&quot;
 26 #include &quot;HeapCell.h&quot;
 27 #include &quot;IterationStatus.h&quot;
 28 #include &quot;WeakSet.h&quot;
 29 #include &lt;wtf/Atomics.h&gt;
 30 #include &lt;wtf/Bitmap.h&gt;
 31 #include &lt;wtf/HashFunctions.h&gt;
 32 #include &lt;wtf/CountingLock.h&gt;
 33 #include &lt;wtf/StdLibExtras.h&gt;
 34 
 35 namespace JSC {
 36 
 37 class AlignedMemoryAllocator;
 38 class FreeList;
 39 class Heap;
 40 class JSCell;
 41 class BlockDirectory;
 42 class MarkedSpace;
 43 class SlotVisitor;
 44 class Subspace;
 45 
 46 typedef uint32_t HeapVersion;
 47 
 48 // A marked block is a page-aligned container for heap-allocated objects.
 49 // Objects are allocated within cells of the marked block. For a given
 50 // marked block, all cells have the same size. Objects smaller than the
 51 // cell size may be allocated in the marked block, in which case the
 52 // allocation suffers from internal fragmentation: wasted space whose
 53 // size is equal to the difference between the cell size and the object
 54 // size.
 55 
 56 class MarkedBlock {
 57     WTF_MAKE_NONCOPYABLE(MarkedBlock);
 58     friend class LLIntOffsetsExtractor;
 59     friend struct VerifyMarked;
 60 
 61 public:
 62     class Footer;
 63     class Handle;
 64 private:
 65     friend class Footer;
 66     friend class Handle;
 67 public:
 68     static constexpr size_t atomSize = 16; // bytes
 69 
 70     // Block size must be at least as large as the system page size.
 71 #if CPU(PPC64) || CPU(PPC64LE) || CPU(PPC) || CPU(UNKNOWN)
 72     static constexpr size_t blockSize = 64 * KB;
 73 #else
 74     static constexpr size_t blockSize = 16 * KB;
 75 #endif
 76 
 77     static constexpr size_t blockMask = ~(blockSize - 1); // blockSize must be a power of two.
 78 
 79     static constexpr size_t atomsPerBlock = blockSize / atomSize;
 80 
 81     static_assert(!(MarkedBlock::atomSize &amp; (MarkedBlock::atomSize - 1)), &quot;MarkedBlock::atomSize must be a power of two.&quot;);
 82     static_assert(!(MarkedBlock::blockSize &amp; (MarkedBlock::blockSize - 1)), &quot;MarkedBlock::blockSize must be a power of two.&quot;);
 83 
 84     struct VoidFunctor {
 85         typedef void ReturnType;
 86         void returnValue() { }
 87     };
 88 
 89     class CountFunctor {
 90     public:
 91         typedef size_t ReturnType;
 92 
 93         CountFunctor() : m_count(0) { }
 94         void count(size_t count) const { m_count += count; }
 95         ReturnType returnValue() const { return m_count; }
 96 
 97     private:
 98         // FIXME: This is mutable because we&#39;re using a functor rather than C++ lambdas.
 99         // https://bugs.webkit.org/show_bug.cgi?id=159644
100         mutable ReturnType m_count;
101     };
102 
103     class Handle {
104         WTF_MAKE_NONCOPYABLE(Handle);
105         WTF_MAKE_FAST_ALLOCATED;
106         friend class LLIntOffsetsExtractor;
107         friend class MarkedBlock;
108         friend struct VerifyMarked;
109     public:
110 
111         ~Handle();
112 
113         MarkedBlock&amp; block();
114         MarkedBlock::Footer&amp; blockFooter();
115 
116         void* cellAlign(void*);
117 
118         bool isEmpty();
119 
120         void lastChanceToFinalize();
121 
122         BlockDirectory* directory() const;
123         Subspace* subspace() const;
124         AlignedMemoryAllocator* alignedMemoryAllocator() const;
125         Heap* heap() const;
126         inline MarkedSpace* space() const;
<a name="2" id="anc2"></a><span class="line-modified">127         VM&amp; vm() const;</span>
128         WeakSet&amp; weakSet();
129 
130         enum SweepMode { SweepOnly, SweepToFreeList };
131 
132         // Sweeping ensures that destructors get called and removes the block from the unswept
133         // set. Sweeping to free list also removes the block from the empty set, if it was in that
134         // set. Sweeping with SweepOnly may add this block to the empty set, if the block is found
135         // to be empty. The free-list being null implies SweepOnly.
136         //
137         // Note that you need to make sure that the empty bit reflects reality. If it&#39;s not set
138         // and the block is freshly created, then we&#39;ll make the mistake of running destructors in
139         // the block. If it&#39;s not set and the block has nothing marked, then we&#39;ll make the
140         // mistake of making a pop freelist rather than a bump freelist.
141         void sweep(FreeList*);
142 
143         // This is to be called by Subspace.
144         template&lt;typename DestroyFunc&gt;
145         void finishSweepKnowingHeapCellType(FreeList*, const DestroyFunc&amp;);
146 
147         void unsweepWithNoNewlyAllocated();
148 
<a name="3" id="anc3"></a>

149         void shrink();
150 
151         void visitWeakSet(SlotVisitor&amp;);
152         void reapWeakSet();
153 
154         // While allocating from a free list, MarkedBlock temporarily has bogus
155         // cell liveness data. To restore accurate cell liveness data, call one
156         // of these functions:
157         void didConsumeFreeList(); // Call this once you&#39;ve allocated all the items in the free list.
158         void stopAllocating(const FreeList&amp;);
159         void resumeAllocating(FreeList&amp;); // Call this if you canonicalized a block for some non-collection related purpose.
160 
161         size_t cellSize();
162         inline unsigned cellsPerBlock();
163 
164         const CellAttributes&amp; attributes() const;
165         DestructionMode destruction() const;
166         bool needsDestruction() const;
167         HeapCell::Kind cellKind() const;
168 
169         size_t markCount();
170         size_t size();
171 
172         bool isAllocated();
173 
174         bool isLive(HeapVersion markingVersion, HeapVersion newlyAllocatedVersion, bool isMarking, const HeapCell*);
175         inline bool isLiveCell(HeapVersion markingVersion, HeapVersion newlyAllocatedVersion, bool isMarking, const void*);
176 
177         bool isLive(const HeapCell*);
178         bool isLiveCell(const void*);
179 
180         bool isFreeListedCell(const void* target) const;
181 
182         template &lt;typename Functor&gt; IterationStatus forEachCell(const Functor&amp;);
183         template &lt;typename Functor&gt; inline IterationStatus forEachLiveCell(const Functor&amp;);
184         template &lt;typename Functor&gt; inline IterationStatus forEachDeadCell(const Functor&amp;);
185         template &lt;typename Functor&gt; inline IterationStatus forEachMarkedCell(const Functor&amp;);
186 
187         JS_EXPORT_PRIVATE bool areMarksStale();
188         bool areMarksStaleForSweep();
189 
190         void assertMarksNotStale();
191 
192         bool isFreeListed() const { return m_isFreeListed; }
193 
194         size_t index() const { return m_index; }
195 
196         void removeFromDirectory();
197 
198         void didAddToDirectory(BlockDirectory*, size_t index);
199         void didRemoveFromDirectory();
200 
<a name="4" id="anc4"></a><span class="line-added">201         void* start() const { return &amp;m_block-&gt;atoms()[0]; }</span>
<span class="line-added">202         void* end() const { return &amp;m_block-&gt;atoms()[m_endAtom]; }</span>
<span class="line-added">203         bool contains(void* p) const { return start() &lt;= p &amp;&amp; p &lt; end(); }</span>
<span class="line-added">204 </span>
205         void dumpState(PrintStream&amp;);
206 
207     private:
208         Handle(Heap&amp;, AlignedMemoryAllocator*, void*);
209 
210         enum SweepDestructionMode { BlockHasNoDestructors, BlockHasDestructors, BlockHasDestructorsAndCollectorIsRunning };
211         enum ScribbleMode { DontScribble, Scribble };
212         enum EmptyMode { IsEmpty, NotEmpty };
213         enum NewlyAllocatedMode { HasNewlyAllocated, DoesNotHaveNewlyAllocated };
214         enum MarksMode { MarksStale, MarksNotStale };
215 
216         SweepDestructionMode sweepDestructionMode();
217         EmptyMode emptyMode();
218         ScribbleMode scribbleMode();
219         NewlyAllocatedMode newlyAllocatedMode();
220         MarksMode marksMode();
221 
222         template&lt;bool, EmptyMode, SweepMode, SweepDestructionMode, ScribbleMode, NewlyAllocatedMode, MarksMode, typename DestroyFunc&gt;
223         void specializedSweep(FreeList*, EmptyMode, SweepMode, SweepDestructionMode, ScribbleMode, NewlyAllocatedMode, MarksMode, const DestroyFunc&amp;);
224 
225         void setIsFreeListed();
226 
227         MarkedBlock::Handle* m_prev { nullptr };
228         MarkedBlock::Handle* m_next { nullptr };
229 
230         size_t m_atomsPerCell { std::numeric_limits&lt;size_t&gt;::max() };
231         size_t m_endAtom { std::numeric_limits&lt;size_t&gt;::max() }; // This is a fuzzy end. Always test for &lt; m_endAtom.
232 
233         CellAttributes m_attributes;
234         bool m_isFreeListed { false };
235 
236         AlignedMemoryAllocator* m_alignedMemoryAllocator { nullptr };
237         BlockDirectory* m_directory { nullptr };
238         size_t m_index { std::numeric_limits&lt;size_t&gt;::max() };
239         WeakSet m_weakSet;
240 
241         MarkedBlock* m_block { nullptr };
242     };
243 
244 private:
245     static constexpr size_t atomAlignmentMask = atomSize - 1;
246 
247     typedef char Atom[atomSize];
248 
249 public:
250     class Footer {
251     public:
252         Footer(VM&amp;, Handle&amp;);
253         ~Footer();
254 
255     private:
256         friend class LLIntOffsetsExtractor;
257         friend class MarkedBlock;
258 
259         Handle&amp; m_handle;
<a name="5" id="anc5"></a><span class="line-added">260         // m_vm must remain a pointer (instead of a reference) because JSCLLIntOffsetsExtractor</span>
<span class="line-added">261         // will fail otherwise.</span>
262         VM* m_vm;
263         Subspace* m_subspace;
264 
265         CountingLock m_lock;
266 
267         // The actual mark count can be computed by doing: m_biasedMarkCount - m_markCountBias. Note
268         // that this count is racy. It will accurately detect whether or not exactly zero things were
269         // marked, but if N things got marked, then this may report anything in the range [1, N] (or
270         // before unbiased, it would be [1 + m_markCountBias, N + m_markCountBias].)
271         int16_t m_biasedMarkCount;
272 
273         // We bias the mark count so that if m_biasedMarkCount &gt;= 0 then the block should be retired.
274         // We go to all this trouble to make marking a bit faster: this way, marking knows when to
275         // retire a block using a js/jns on m_biasedMarkCount.
276         //
277         // For example, if a block has room for 100 objects and retirement happens whenever 90% are
278         // live, then m_markCountBias will be -90. This way, when marking begins, this will cause us to
279         // set m_biasedMarkCount to -90 as well, since:
280         //
281         //     m_biasedMarkCount = actualMarkCount + m_markCountBias.
282         //
283         // Marking an object will increment m_biasedMarkCount. Once 90 objects get marked, we will have
284         // m_biasedMarkCount = 0, which will trigger retirement. In other words, we want to set
285         // m_markCountBias like so:
286         //
287         //     m_markCountBias = -(minMarkedBlockUtilization * cellsPerBlock)
288         //
289         // All of this also means that you can detect if any objects are marked by doing:
290         //
291         //     m_biasedMarkCount != m_markCountBias
292         int16_t m_markCountBias;
293 
294         HeapVersion m_markingVersion;
295         HeapVersion m_newlyAllocatedVersion;
296 
297         Bitmap&lt;atomsPerBlock&gt; m_marks;
298         Bitmap&lt;atomsPerBlock&gt; m_newlyAllocated;
299     };
300 
301 private:
302     Footer&amp; footer();
303     const Footer&amp; footer() const;
304 
305 public:
306     static constexpr size_t endAtom = (blockSize - sizeof(Footer)) / atomSize;
307     static constexpr size_t payloadSize = endAtom * atomSize;
308     static constexpr size_t footerSize = blockSize - payloadSize;
309 
310     static_assert(payloadSize == ((blockSize - sizeof(MarkedBlock::Footer)) &amp; ~(atomSize - 1)), &quot;Payload size computed the alternate way should give the same result&quot;);
<a name="6" id="anc6"></a><span class="line-modified">311     // Some of JSCell types assume that the last JSCell in a MarkedBlock has a subsequent memory region (Footer) that can still safely accessed.</span>
<span class="line-added">312     // For example, JSRopeString assumes that it can safely access up to 2 bytes beyond the JSRopeString cell.</span>
<span class="line-added">313     static_assert(sizeof(Footer) &gt;= sizeof(uint16_t));</span>
314 
315     static MarkedBlock::Handle* tryCreate(Heap&amp;, AlignedMemoryAllocator*);
316 
317     Handle&amp; handle();
318     const Handle&amp; handle() const;
319 
<a name="7" id="anc7"></a><span class="line-modified">320     VM&amp; vm() const;</span>
321     inline Heap* heap() const;
322     inline MarkedSpace* space() const;
323 
324     static bool isAtomAligned(const void*);
325     static MarkedBlock* blockFor(const void*);
326     size_t atomNumber(const void*);
327 
328     size_t markCount();
329 
330     bool isMarked(const void*);
331     bool isMarked(HeapVersion markingVersion, const void*);
332     bool isMarked(const void*, Dependency);
333     bool testAndSetMarked(const void*, Dependency);
334 
335     bool isAtom(const void*);
336     void clearMarked(const void*);
337 
338     bool isNewlyAllocated(const void*);
339     void setNewlyAllocated(const void*);
340     void clearNewlyAllocated(const void*);
341     const Bitmap&lt;atomsPerBlock&gt;&amp; newlyAllocated() const;
342 
343     HeapVersion newlyAllocatedVersion() const { return footer().m_newlyAllocatedVersion; }
344 
345     inline bool isNewlyAllocatedStale() const;
346 
347     inline bool hasAnyNewlyAllocated();
348     void resetAllocated();
349 
350     size_t cellSize();
351     const CellAttributes&amp; attributes() const;
352 
353     bool hasAnyMarked() const;
354     void noteMarked();
355 #if ASSERT_DISABLED
356     void assertValidCell(VM&amp;, HeapCell*) const { }
357 #else
358     void assertValidCell(VM&amp;, HeapCell*) const;
359 #endif
360 
361     WeakSet&amp; weakSet();
362 
363     JS_EXPORT_PRIVATE bool areMarksStale();
364     bool areMarksStale(HeapVersion markingVersion);
365 
366     Dependency aboutToMark(HeapVersion markingVersion);
367 
368 #if ASSERT_DISABLED
369     void assertMarksNotStale() { }
370 #else
371     JS_EXPORT_PRIVATE void assertMarksNotStale();
372 #endif
373 
374     void resetMarks();
375 
376     bool isMarkedRaw(const void* p);
377     HeapVersion markingVersion() const { return footer().m_markingVersion; }
378 
379     const Bitmap&lt;atomsPerBlock&gt;&amp; marks() const;
380 
381     CountingLock&amp; lock() { return footer().m_lock; }
382 
383     Subspace* subspace() const { return footer().m_subspace; }
384 
<a name="8" id="anc8"></a><span class="line-added">385     void populatePage() const</span>
<span class="line-added">386     {</span>
<span class="line-added">387         *bitwise_cast&lt;volatile uint8_t*&gt;(&amp;footer());</span>
<span class="line-added">388     }</span>
<span class="line-added">389 </span>
390     static constexpr size_t offsetOfFooter = endAtom * atomSize;
391 
392 private:
393     MarkedBlock(VM&amp;, Handle&amp;);
394     ~MarkedBlock();
395     Atom* atoms();
396 
397     JS_EXPORT_PRIVATE void aboutToMarkSlow(HeapVersion markingVersion);
398     void clearHasAnyMarked();
399 
400     void noteMarkedSlow();
401 
402     inline bool marksConveyLivenessDuringMarking(HeapVersion markingVersion);
403     inline bool marksConveyLivenessDuringMarking(HeapVersion myMarkingVersion, HeapVersion markingVersion);
404 };
405 
406 inline MarkedBlock::Footer&amp; MarkedBlock::footer()
407 {
408     return *bitwise_cast&lt;MarkedBlock::Footer*&gt;(atoms() + endAtom);
409 }
410 
411 inline const MarkedBlock::Footer&amp; MarkedBlock::footer() const
412 {
413     return const_cast&lt;MarkedBlock*&gt;(this)-&gt;footer();
414 }
415 
416 inline MarkedBlock::Handle&amp; MarkedBlock::handle()
417 {
418     return footer().m_handle;
419 }
420 
421 inline const MarkedBlock::Handle&amp; MarkedBlock::handle() const
422 {
423     return const_cast&lt;MarkedBlock*&gt;(this)-&gt;handle();
424 }
425 
426 inline MarkedBlock&amp; MarkedBlock::Handle::block()
427 {
428     return *m_block;
429 }
430 
431 inline MarkedBlock::Footer&amp; MarkedBlock::Handle::blockFooter()
432 {
433     return block().footer();
434 }
435 
436 inline MarkedBlock::Atom* MarkedBlock::atoms()
437 {
438     return reinterpret_cast&lt;Atom*&gt;(this);
439 }
440 
441 inline bool MarkedBlock::isAtomAligned(const void* p)
442 {
443     return !(reinterpret_cast&lt;uintptr_t&gt;(p) &amp; atomAlignmentMask);
444 }
445 
446 inline void* MarkedBlock::Handle::cellAlign(void* p)
447 {
448     uintptr_t base = reinterpret_cast&lt;uintptr_t&gt;(block().atoms());
449     uintptr_t bits = reinterpret_cast&lt;uintptr_t&gt;(p);
450     bits -= base;
451     bits -= bits % cellSize();
452     bits += base;
453     return reinterpret_cast&lt;void*&gt;(bits);
454 }
455 
456 inline MarkedBlock* MarkedBlock::blockFor(const void* p)
457 {
458     return reinterpret_cast&lt;MarkedBlock*&gt;(reinterpret_cast&lt;uintptr_t&gt;(p) &amp; blockMask);
459 }
460 
461 inline BlockDirectory* MarkedBlock::Handle::directory() const
462 {
463     return m_directory;
464 }
465 
466 inline AlignedMemoryAllocator* MarkedBlock::Handle::alignedMemoryAllocator() const
467 {
468     return m_alignedMemoryAllocator;
469 }
470 
471 inline Heap* MarkedBlock::Handle::heap() const
472 {
473     return m_weakSet.heap();
474 }
475 
<a name="9" id="anc9"></a><span class="line-modified">476 inline VM&amp; MarkedBlock::Handle::vm() const</span>
477 {
478     return m_weakSet.vm();
479 }
480 
<a name="10" id="anc10"></a><span class="line-modified">481 inline VM&amp; MarkedBlock::vm() const</span>
482 {
<a name="11" id="anc11"></a><span class="line-modified">483     return *footer().m_vm;</span>
484 }
485 
486 inline WeakSet&amp; MarkedBlock::Handle::weakSet()
487 {
488     return m_weakSet;
489 }
490 
491 inline WeakSet&amp; MarkedBlock::weakSet()
492 {
493     return handle().weakSet();
494 }
495 
496 inline void MarkedBlock::Handle::shrink()
497 {
498     m_weakSet.shrink();
499 }
500 
501 inline void MarkedBlock::Handle::visitWeakSet(SlotVisitor&amp; visitor)
502 {
503     return m_weakSet.visit(visitor);
504 }
505 
506 inline void MarkedBlock::Handle::reapWeakSet()
507 {
508     m_weakSet.reap();
509 }
510 
511 inline size_t MarkedBlock::Handle::cellSize()
512 {
513     return m_atomsPerCell * atomSize;
514 }
515 
516 inline size_t MarkedBlock::cellSize()
517 {
518     return handle().cellSize();
519 }
520 
521 inline const CellAttributes&amp; MarkedBlock::Handle::attributes() const
522 {
523     return m_attributes;
524 }
525 
526 inline const CellAttributes&amp; MarkedBlock::attributes() const
527 {
528     return handle().attributes();
529 }
530 
531 inline bool MarkedBlock::Handle::needsDestruction() const
532 {
533     return m_attributes.destruction == NeedsDestruction;
534 }
535 
536 inline DestructionMode MarkedBlock::Handle::destruction() const
537 {
538     return m_attributes.destruction;
539 }
540 
541 inline HeapCell::Kind MarkedBlock::Handle::cellKind() const
542 {
543     return m_attributes.cellKind;
544 }
545 
546 inline size_t MarkedBlock::Handle::markCount()
547 {
548     return m_block-&gt;markCount();
549 }
550 
551 inline size_t MarkedBlock::Handle::size()
552 {
553     return markCount() * cellSize();
554 }
555 
556 inline size_t MarkedBlock::atomNumber(const void* p)
557 {
558     return (reinterpret_cast&lt;uintptr_t&gt;(p) - reinterpret_cast&lt;uintptr_t&gt;(this)) / atomSize;
559 }
560 
561 inline bool MarkedBlock::areMarksStale(HeapVersion markingVersion)
562 {
563     return markingVersion != footer().m_markingVersion;
564 }
565 
566 inline Dependency MarkedBlock::aboutToMark(HeapVersion markingVersion)
567 {
568     HeapVersion version = footer().m_markingVersion;
569     if (UNLIKELY(version != markingVersion))
570         aboutToMarkSlow(markingVersion);
571     return Dependency::fence(version);
572 }
573 
574 inline void MarkedBlock::Handle::assertMarksNotStale()
575 {
576     block().assertMarksNotStale();
577 }
578 
579 inline bool MarkedBlock::isMarkedRaw(const void* p)
580 {
581     return footer().m_marks.get(atomNumber(p));
582 }
583 
584 inline bool MarkedBlock::isMarked(HeapVersion markingVersion, const void* p)
585 {
586     HeapVersion version = footer().m_markingVersion;
587     if (UNLIKELY(version != markingVersion))
588         return false;
589     return footer().m_marks.get(atomNumber(p), Dependency::fence(version));
590 }
591 
592 inline bool MarkedBlock::isMarked(const void* p, Dependency dependency)
593 {
594     assertMarksNotStale();
595     return footer().m_marks.get(atomNumber(p), dependency);
596 }
597 
598 inline bool MarkedBlock::testAndSetMarked(const void* p, Dependency dependency)
599 {
600     assertMarksNotStale();
601     return footer().m_marks.concurrentTestAndSet(atomNumber(p), dependency);
602 }
603 
604 inline const Bitmap&lt;MarkedBlock::atomsPerBlock&gt;&amp; MarkedBlock::marks() const
605 {
606     return footer().m_marks;
607 }
608 
609 inline bool MarkedBlock::isNewlyAllocated(const void* p)
610 {
611     return footer().m_newlyAllocated.get(atomNumber(p));
612 }
613 
614 inline void MarkedBlock::setNewlyAllocated(const void* p)
615 {
616     footer().m_newlyAllocated.set(atomNumber(p));
617 }
618 
619 inline void MarkedBlock::clearNewlyAllocated(const void* p)
620 {
621     footer().m_newlyAllocated.clear(atomNumber(p));
622 }
623 
624 inline const Bitmap&lt;MarkedBlock::atomsPerBlock&gt;&amp; MarkedBlock::newlyAllocated() const
625 {
626     return footer().m_newlyAllocated;
627 }
628 
629 inline bool MarkedBlock::isAtom(const void* p)
630 {
631     ASSERT(MarkedBlock::isAtomAligned(p));
632     size_t atomNumber = this-&gt;atomNumber(p);
633     if (atomNumber % handle().m_atomsPerCell) // Filters pointers into cell middles.
634         return false;
635     if (atomNumber &gt;= handle().m_endAtom) // Filters pointers into invalid cells out of the range.
636         return false;
637     return true;
638 }
639 
640 template &lt;typename Functor&gt;
641 inline IterationStatus MarkedBlock::Handle::forEachCell(const Functor&amp; functor)
642 {
643     HeapCell::Kind kind = m_attributes.cellKind;
644     for (size_t i = 0; i &lt; m_endAtom; i += m_atomsPerCell) {
645         HeapCell* cell = reinterpret_cast_ptr&lt;HeapCell*&gt;(&amp;m_block-&gt;atoms()[i]);
646         if (functor(cell, kind) == IterationStatus::Done)
647             return IterationStatus::Done;
648     }
649     return IterationStatus::Continue;
650 }
651 
652 inline bool MarkedBlock::hasAnyMarked() const
653 {
654     return footer().m_biasedMarkCount != footer().m_markCountBias;
655 }
656 
657 inline void MarkedBlock::noteMarked()
658 {
659     // This is racy by design. We don&#39;t want to pay the price of an atomic increment!
660     int16_t biasedMarkCount = footer().m_biasedMarkCount;
661     ++biasedMarkCount;
662     footer().m_biasedMarkCount = biasedMarkCount;
663     if (UNLIKELY(!biasedMarkCount))
664         noteMarkedSlow();
665 }
666 
667 } // namespace JSC
668 
669 namespace WTF {
670 
671 struct MarkedBlockHash : PtrHash&lt;JSC::MarkedBlock*&gt; {
672     static unsigned hash(JSC::MarkedBlock* const&amp; key)
673     {
674         // Aligned VM regions tend to be monotonically increasing integers,
675         // which is a great hash function, but we have to remove the low bits,
676         // since they&#39;re always zero, which is a terrible hash function!
677         return reinterpret_cast&lt;uintptr_t&gt;(key) / JSC::MarkedBlock::blockSize;
678     }
679 };
680 
681 template&lt;&gt; struct DefaultHash&lt;JSC::MarkedBlock*&gt; {
682     typedef MarkedBlockHash Hash;
683 };
684 
685 void printInternal(PrintStream&amp; out, JSC::MarkedBlock::Handle::SweepMode);
686 
687 } // namespace WTF
<a name="12" id="anc12"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="12" type="hidden" />
</body>
</html>