<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGSpeculativeJIT.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="DFGSpeculativeJIT.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGSpeculativeJIT32_64.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGSpeculativeJIT.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  96     enum SpillOrder {
  97         SpillOrderConstant = 1, // no spill, and cheap fill
  98         SpillOrderSpilled  = 2, // no spill
  99         SpillOrderJS       = 4, // needs spill
 100         SpillOrderStorage  = 4, // needs spill
 101         SpillOrderDouble   = 4, // needs spill
 102         SpillOrderInteger  = 5, // needs spill and box
 103         SpillOrderCell     = 5, // needs spill and box
 104         SpillOrderBoolean  = 5, // needs spill and box
 105     };
 106 #endif
 107 
 108     enum UseChildrenMode { CallUseChildren, UseChildrenCalledExplicitly };
 109 
 110 public:
 111     SpeculativeJIT(JITCompiler&amp;);
 112     ~SpeculativeJIT();
 113 
 114     VM&amp; vm()
 115     {
<span class="line-modified"> 116         return *m_jit.vm();</span>
 117     }
 118 
 119     struct TrustedImmPtr {
 120         template &lt;typename T&gt;
 121         explicit TrustedImmPtr(T* value)
 122             : m_value(value)
 123         {
 124             static_assert(!std::is_base_of&lt;JSCell, T&gt;::value, &quot;To use a GC pointer, the graph must be aware of it. Use SpeculativeJIT::TrustedImmPtr::weakPointer instead.&quot;);
 125         }
 126 
 127         explicit TrustedImmPtr(RegisteredStructure structure)
 128             : m_value(structure.get())
 129         { }
 130 
 131         explicit TrustedImmPtr(std::nullptr_t)
 132             : m_value(nullptr)
 133         { }
 134 
 135         explicit TrustedImmPtr(FrozenValue* value)
 136             : m_value(value-&gt;cell())
</pre>
<hr />
<pre>
 561 
 562         default:
 563             // The following code handles JSValues.
 564             RELEASE_ASSERT(spillFormat &amp; DataFormatJS);
 565             m_jit.store32(info.tagGPR(), JITCompiler::tagFor(spillMe));
 566             m_jit.store32(info.payloadGPR(), JITCompiler::payloadFor(spillMe));
 567             info.spill(*m_stream, spillMe, spillFormat);
 568             return;
 569 #endif
 570         }
 571     }
 572 
 573     bool isKnownInteger(Node* node) { return m_state.forNode(node).isType(SpecInt32Only); }
 574     bool isKnownCell(Node* node) { return m_state.forNode(node).isType(SpecCell); }
 575 
 576     bool isKnownNotInteger(Node* node) { return !(m_state.forNode(node).m_type &amp; SpecInt32Only); }
 577     bool isKnownNotNumber(Node* node) { return !(m_state.forNode(node).m_type &amp; SpecFullNumber); }
 578     bool isKnownNotCell(Node* node) { return !(m_state.forNode(node).m_type &amp; SpecCell); }
 579     bool isKnownNotOther(Node* node) { return !(m_state.forNode(node).m_type &amp; SpecOther); }
 580 


 581     UniquedStringImpl* identifierUID(unsigned index)
 582     {
 583         return m_jit.graph().identifiers()[index];
 584     }
 585 
 586     // Spill all VirtualRegisters back to the JSStack.
 587     void flushRegisters()
 588     {
 589         for (gpr_iterator iter = m_gprs.begin(); iter != m_gprs.end(); ++iter) {
 590             if (iter.name().isValid()) {
 591                 spill(iter.name());
 592                 iter.release();
 593             }
 594         }
 595         for (fpr_iterator iter = m_fprs.begin(); iter != m_fprs.end(); ++iter) {
 596             if (iter.name().isValid()) {
 597                 spill(iter.name());
 598                 iter.release();
 599             }
 600         }
</pre>
<hr />
<pre>
 644         switch (op) {
 645         case ArithBitAnd:
 646             m_jit.and32(op1, op2, result);
 647             break;
 648         case ArithBitOr:
 649             m_jit.or32(op1, op2, result);
 650             break;
 651         case ArithBitXor:
 652             m_jit.xor32(op1, op2, result);
 653             break;
 654         default:
 655             RELEASE_ASSERT_NOT_REACHED();
 656         }
 657     }
 658     void shiftOp(NodeType op, GPRReg op1, int32_t shiftAmount, GPRReg result)
 659     {
 660         switch (op) {
 661         case BitRShift:
 662             m_jit.rshift32(op1, Imm32(shiftAmount), result);
 663             break;
<span class="line-modified"> 664         case BitLShift:</span>
 665             m_jit.lshift32(op1, Imm32(shiftAmount), result);
 666             break;
 667         case BitURShift:
 668             m_jit.urshift32(op1, Imm32(shiftAmount), result);
 669             break;
 670         default:
 671             RELEASE_ASSERT_NOT_REACHED();
 672         }
 673     }
 674     void shiftOp(NodeType op, GPRReg op1, GPRReg shiftAmount, GPRReg result)
 675     {
 676         switch (op) {
 677         case BitRShift:
 678             m_jit.rshift32(op1, shiftAmount, result);
 679             break;
<span class="line-modified"> 680         case BitLShift:</span>
 681             m_jit.lshift32(op1, shiftAmount, result);
 682             break;
 683         case BitURShift:
 684             m_jit.urshift32(op1, shiftAmount, result);
 685             break;
 686         default:
 687             RELEASE_ASSERT_NOT_REACHED();
 688         }
 689     }
 690 
 691     // Returns the index of the branch node if peephole is okay, UINT_MAX otherwise.
 692     unsigned detectPeepHoleBranch()
 693     {
 694         // Check that no intervening nodes will be generated.
 695         for (unsigned index = m_indexInBlock + 1; index &lt; m_block-&gt;size() - 1; ++index) {
 696             Node* node = m_block-&gt;at(index);
 697             if (!node-&gt;shouldGenerate())
 698                 continue;
 699             // Check if it&#39;s a Phantom that can be safely ignored.
 700             if (node-&gt;op() == Phantom &amp;&amp; !node-&gt;child1())
</pre>
<hr />
<pre>
 972 
 973     JITCompiler::Call callOperationWithCallFrameRollbackOnException(Z_JITOperation_E operation, GPRReg result)
 974     {
 975         m_jit.setupArguments&lt;Z_JITOperation_E&gt;();
 976         return appendCallWithCallFrameRollbackOnExceptionSetResult(operation, result);
 977     }
 978 
 979 #if !defined(NDEBUG) &amp;&amp; !CPU(ARM_THUMB2) &amp;&amp; !CPU(MIPS)
 980     void prepareForExternalCall()
 981     {
 982         // We&#39;re about to call out to a &quot;native&quot; helper function. The helper
 983         // function is expected to set topCallFrame itself with the ExecState
 984         // that is passed to it.
 985         //
 986         // We explicitly trash topCallFrame here so that we&#39;ll know if some of
 987         // the helper functions are not setting topCallFrame when they should
 988         // be doing so. Note: the previous value in topcallFrame was not valid
 989         // anyway since it was not being updated by JIT&#39;ed code by design.
 990 
 991         for (unsigned i = 0; i &lt; sizeof(void*) / 4; i++)
<span class="line-modified"> 992             m_jit.store32(TrustedImm32(0xbadbeef), reinterpret_cast&lt;char*&gt;(&amp;m_jit.vm()-&gt;topCallFrame) + i * 4);</span>
 993     }
 994 #else
 995     void prepareForExternalCall() { }
 996 #endif
 997 
 998     // These methods add call instructions, optionally setting results, and optionally rolling back the call frame on an exception.
 999     JITCompiler::Call appendCall(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
1000     {
1001         prepareForExternalCall();
1002         m_jit.emitStoreCodeOrigin(m_currentNode-&gt;origin.semantic);
1003         return m_jit.appendCall(function);
1004     }
1005 
1006     JITCompiler::Call appendCallWithCallFrameRollbackOnException(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
1007     {
1008         JITCompiler::Call call = appendCall(function);
1009         m_jit.exceptionCheckWithCallFrameRollback();
1010         return call;
1011     }
1012 
</pre>
<hr />
<pre>
1306 
1307     void compileGetScope(Node*);
1308     void compileSkipScope(Node*);
1309     void compileGetGlobalObject(Node*);
1310     void compileGetGlobalThis(Node*);
1311 
1312     void compileGetArrayLength(Node*);
1313 
1314     void compileCheckTypeInfoFlags(Node*);
1315     void compileCheckStringIdent(Node*);
1316 
1317     void compileParseInt(Node*);
1318 
1319     void compileValueRep(Node*);
1320     void compileDoubleRep(Node*);
1321 
1322     void compileValueToInt32(Node*);
1323     void compileUInt32ToNumber(Node*);
1324     void compileDoubleAsInt32(Node*);
1325 

1326     void compileBitwiseNot(Node*);
1327 
1328     template&lt;typename SnippetGenerator, J_JITOperation_EJJ slowPathFunction&gt;
1329     void emitUntypedBitOp(Node*);
1330     void compileBitwiseOp(Node*);
1331     void compileValueBitwiseOp(Node*);
1332 
1333     void emitUntypedRightShiftBitOp(Node*);

1334     void compileShiftOp(Node*);
1335 
1336     template &lt;typename Generator, typename RepatchingFunction, typename NonRepatchingFunction&gt;
1337     void compileMathIC(Node*, JITBinaryMathIC&lt;Generator&gt;*, bool needsScratchGPRReg, bool needsScratchFPRReg, RepatchingFunction, NonRepatchingFunction);
1338     template &lt;typename Generator, typename RepatchingFunction, typename NonRepatchingFunction&gt;
1339     void compileMathIC(Node*, JITUnaryMathIC&lt;Generator&gt;*, bool needsScratchGPRReg, RepatchingFunction, NonRepatchingFunction);
1340 
1341     void compileArithDoubleUnaryOp(Node*, double (*doubleFunction)(double), double (*operation)(ExecState*, EncodedJSValue));
1342     void compileValueAdd(Node*);
1343     void compileValueSub(Node*);
1344     void compileArithAdd(Node*);
1345     void compileMakeRope(Node*);
1346     void compileArithAbs(Node*);
1347     void compileArithClz32(Node*);
1348     void compileArithSub(Node*);
1349     void compileValueNegate(Node*);
1350     void compileArithNegate(Node*);
1351     void compileValueMul(Node*);
1352     void compileArithMul(Node*);
1353     void compileValueDiv(Node*);
1354     void compileArithDiv(Node*);
1355     void compileArithFRound(Node*);

1356     void compileArithMod(Node*);
1357     void compileArithPow(Node*);

1358     void compileArithRounding(Node*);
1359     void compileArithRandom(Node*);
1360     void compileArithUnary(Node*);
1361     void compileArithSqrt(Node*);
1362     void compileArithMinMax(Node*);
1363     void compileConstantStoragePointer(Node*);
1364     void compileGetIndexedPropertyStorage(Node*);
1365     JITCompiler::Jump jumpForTypedArrayOutOfBounds(Node*, GPRReg baseGPR, GPRReg indexGPR);
1366     JITCompiler::Jump jumpForTypedArrayIsNeuteredIfOutOfBounds(Node*, GPRReg baseGPR, JITCompiler::Jump outOfBounds);
1367     void emitTypedArrayBoundsCheck(Node*, GPRReg baseGPR, GPRReg indexGPR);
1368     void compileGetTypedArrayByteOffset(Node*);
1369     void compileGetByValOnIntTypedArray(Node*, TypedArrayType);
1370     void compilePutByValForIntTypedArray(GPRReg base, GPRReg property, Node*, TypedArrayType);
1371     void compileGetByValOnFloatTypedArray(Node*, TypedArrayType);
1372     void compilePutByValForFloatTypedArray(GPRReg base, GPRReg property, Node*, TypedArrayType);
1373     void compileGetByValForObjectWithString(Node*);
1374     void compileGetByValForObjectWithSymbol(Node*);
1375     void compilePutByValForCellWithString(Node*, Edge&amp; child1, Edge&amp; child2, Edge&amp; child3);
1376     void compilePutByValForCellWithSymbol(Node*, Edge&amp; child1, Edge&amp; child2, Edge&amp; child3);
1377     void compileGetByValWithThis(Node*);
</pre>
<hr />
<pre>
1495         GPRReg scratchGPR, MacroAssembler::JumpList&amp; slowPath)
1496     {
1497         m_jit.emitAllocateJSCell(resultGPR, allocator, allocatorGPR, structure, scratchGPR, slowPath);
1498     }
1499 
1500     // Allocator for an object of a specific size.
1501     template &lt;typename StructureType, typename StorageType&gt; // StructureType and StorageType can be GPR or ImmPtr.
1502     void emitAllocateJSObject(
1503         GPRReg resultGPR, const JITAllocator&amp; allocator, GPRReg allocatorGPR, StructureType structure,
1504         StorageType storage, GPRReg scratchGPR, MacroAssembler::JumpList&amp; slowPath)
1505     {
1506         m_jit.emitAllocateJSObject(
1507             resultGPR, allocator, allocatorGPR, structure, storage, scratchGPR, slowPath);
1508     }
1509 
1510     template &lt;typename ClassType, typename StructureType, typename StorageType&gt; // StructureType and StorageType can be GPR or ImmPtr.
1511     void emitAllocateJSObjectWithKnownSize(
1512         GPRReg resultGPR, StructureType structure, StorageType storage, GPRReg scratchGPR1,
1513         GPRReg scratchGPR2, MacroAssembler::JumpList&amp; slowPath, size_t size)
1514     {
<span class="line-modified">1515         m_jit.emitAllocateJSObjectWithKnownSize&lt;ClassType&gt;(*m_jit.vm(), resultGPR, structure, storage, scratchGPR1, scratchGPR2, slowPath, size);</span>
1516     }
1517 
1518     // Convenience allocator for a built-in object.
1519     template &lt;typename ClassType, typename StructureType, typename StorageType&gt; // StructureType and StorageType can be GPR or ImmPtr.
1520     void emitAllocateJSObject(GPRReg resultGPR, StructureType structure, StorageType storage,
1521         GPRReg scratchGPR1, GPRReg scratchGPR2, MacroAssembler::JumpList&amp; slowPath)
1522     {
<span class="line-modified">1523         m_jit.emitAllocateJSObject&lt;ClassType&gt;(*m_jit.vm(), resultGPR, structure, storage, scratchGPR1, scratchGPR2, slowPath);</span>
1524     }
1525 
1526     template &lt;typename ClassType, typename StructureType&gt; // StructureType and StorageType can be GPR or ImmPtr.
1527     void emitAllocateVariableSizedJSObject(GPRReg resultGPR, StructureType structure, GPRReg allocationSize, GPRReg scratchGPR1, GPRReg scratchGPR2, MacroAssembler::JumpList&amp; slowPath)
1528     {
<span class="line-modified">1529         m_jit.emitAllocateVariableSizedJSObject&lt;ClassType&gt;(*m_jit.vm(), resultGPR, structure, allocationSize, scratchGPR1, scratchGPR2, slowPath);</span>
1530     }
1531 
1532     template&lt;typename ClassType&gt;
1533     void emitAllocateDestructibleObject(GPRReg resultGPR, RegisteredStructure structure,
1534         GPRReg scratchGPR1, GPRReg scratchGPR2, MacroAssembler::JumpList&amp; slowPath)
1535     {
<span class="line-modified">1536         m_jit.emitAllocateDestructibleObject&lt;ClassType&gt;(*m_jit.vm(), resultGPR, structure.get(), scratchGPR1, scratchGPR2, slowPath);</span>
1537     }
1538 
1539     void emitAllocateRawObject(GPRReg resultGPR, RegisteredStructure, GPRReg storageGPR, unsigned numElements, unsigned vectorLength);
1540 
1541     void emitGetLength(InlineCallFrame*, GPRReg lengthGPR, bool includeThis = false);
1542     void emitGetLength(CodeOrigin, GPRReg lengthGPR, bool includeThis = false);
1543     void emitGetCallee(CodeOrigin, GPRReg calleeGPR);
1544     void emitGetArgumentStart(CodeOrigin, GPRReg startGPR);
1545     void emitPopulateSliceIndex(Edge&amp;, Optional&lt;GPRReg&gt; indexGPR, GPRReg lengthGPR, GPRReg resultGPR);
1546 
1547     // Generate an OSR exit fuzz check. Returns Jump() if OSR exit fuzz is not enabled, or if
1548     // it&#39;s in training mode.
1549     MacroAssembler::Jump emitOSRExitFuzzCheck();
1550 
1551     // Add a speculation check.
1552     void speculationCheck(ExitKind, JSValueSource, Node*, MacroAssembler::Jump jumpToFail);
1553     void speculationCheck(ExitKind, JSValueSource, Node*, const MacroAssembler::JumpList&amp; jumpsToFail);
1554 
1555     // Add a speculation check without additional recovery, and with a promise to supply a jump later.
1556     OSRExitJumpPlaceholder speculationCheck(ExitKind, JSValueSource, Node*);
</pre>
<hr />
<pre>
1630     void speculateSymbol(Edge);
1631     void speculateBigInt(Edge, GPRReg cell);
1632     void speculateBigInt(Edge);
1633     void speculateNotCell(Edge, JSValueRegs);
1634     void speculateNotCell(Edge);
1635     void speculateOther(Edge, JSValueRegs, GPRReg temp);
1636     void speculateOther(Edge, JSValueRegs);
1637     void speculateOther(Edge);
1638     void speculateMisc(Edge, JSValueRegs);
1639     void speculateMisc(Edge);
1640     void speculate(Node*, Edge);
1641 
1642     JITCompiler::JumpList jumpSlowForUnwantedArrayMode(GPRReg tempWithIndexingTypeReg, ArrayMode);
1643     void checkArray(Node*);
1644     void arrayify(Node*, GPRReg baseReg, GPRReg propertyReg);
1645     void arrayify(Node*);
1646 
1647     template&lt;bool strict&gt;
1648     GPRReg fillSpeculateInt32Internal(Edge, DataFormat&amp; returnFormat);
1649 
<span class="line-modified">1650     void cageTypedArrayStorage(GPRReg);</span>
<span class="line-removed">1651 </span>
<span class="line-removed">1652     // It is possible, during speculative generation, to reach a situation in which we</span>
<span class="line-removed">1653     // can statically determine a speculation will fail (for example, when two nodes</span>
<span class="line-removed">1654     // will make conflicting speculations about the same operand). In such cases this</span>
<span class="line-removed">1655     // flag is cleared, indicating no further code generation should take place.</span>
<span class="line-removed">1656     bool m_compileOkay;</span>
1657 
1658     void recordSetLocal(
1659         VirtualRegister bytecodeReg, VirtualRegister machineReg, DataFormat format)
1660     {
1661         m_stream-&gt;appendAndLog(VariableEvent::setLocal(bytecodeReg, machineReg, format));
1662     }
1663 
1664     void recordSetLocal(DataFormat format)
1665     {
1666         VariableAccessData* variable = m_currentNode-&gt;variableAccessData();
1667         recordSetLocal(variable-&gt;local(), variable-&gt;machineLocal(), format);
1668     }
1669 
1670     GenerationInfo&amp; generationInfoFromVirtualRegister(VirtualRegister virtualRegister)
1671     {
1672         return m_generationInfo[virtualRegister.toLocal()];
1673     }
1674 
1675     GenerationInfo&amp; generationInfo(Node* node)
1676     {
</pre>
<hr />
<pre>
1680     GenerationInfo&amp; generationInfo(Edge edge)
1681     {
1682         return generationInfo(edge.node());
1683     }
1684 
1685     // The JIT, while also provides MacroAssembler functionality.
1686     JITCompiler&amp; m_jit;
1687     Graph&amp; m_graph;
1688 
1689     // The current node being generated.
1690     BasicBlock* m_block;
1691     Node* m_currentNode;
1692     NodeType m_lastGeneratedNode;
1693     unsigned m_indexInBlock;
1694 
1695     // Virtual and physical register maps.
1696     Vector&lt;GenerationInfo, 32&gt; m_generationInfo;
1697     RegisterBank&lt;GPRInfo&gt; m_gprs;
1698     RegisterBank&lt;FPRInfo&gt; m_fprs;
1699 






1700     Vector&lt;MacroAssembler::Label&gt; m_osrEntryHeads;
1701 
1702     struct BranchRecord {
1703         BranchRecord(MacroAssembler::Jump jump, BasicBlock* destination)
1704             : jump(jump)
1705             , destination(destination)
1706         {
1707         }
1708 
1709         MacroAssembler::Jump jump;
1710         BasicBlock* destination;
1711     };
1712     Vector&lt;BranchRecord, 8&gt; m_branches;
1713 
1714     NodeOrigin m_origin;
1715 
1716     InPlaceAbstractState m_state;
1717     AbstractInterpreter&lt;InPlaceAbstractState&gt; m_interpreter;
1718 
1719     VariableEventStream* m_stream;
</pre>
<hr />
<pre>
1724         Function&lt;void()&gt; generator;
1725         Node* currentNode;
1726         unsigned streamIndex;
1727     };
1728     Vector&lt;SlowPathLambda&gt; m_slowPathLambdas;
1729     Vector&lt;SilentRegisterSavePlan&gt; m_plans;
1730     Optional&lt;unsigned&gt; m_outOfLineStreamIndex;
1731 };
1732 
1733 
1734 // === Operand types ===
1735 //
1736 // These classes are used to lock the operands to a node into machine
1737 // registers. These classes implement of pattern of locking a value
1738 // into register at the point of construction only if it is already in
1739 // registers, and otherwise loading it lazily at the point it is first
1740 // used. We do so in order to attempt to avoid spilling one operand
1741 // in order to make space available for another.
1742 
1743 class JSValueOperand {

1744 public:
1745     explicit JSValueOperand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
1746         : m_jit(jit)
1747         , m_edge(edge)
1748 #if USE(JSVALUE64)
1749         , m_gprOrInvalid(InvalidGPRReg)
1750 #elif USE(JSVALUE32_64)
1751         , m_isDouble(false)
1752 #endif
1753     {
1754         ASSERT(m_jit);
1755         if (!edge)
1756             return;
1757         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || edge.useKind() == UntypedUse);
1758 #if USE(JSVALUE64)
1759         if (jit-&gt;isFilled(node()))
1760             gpr();
1761 #elif USE(JSVALUE32_64)
1762         m_register.pair.tagGPR = InvalidGPRReg;
1763         m_register.pair.payloadGPR = InvalidGPRReg;
</pre>
<hr />
<pre>
1879     }
1880 
1881 private:
1882     SpeculativeJIT* m_jit;
1883     Edge m_edge;
1884 #if USE(JSVALUE64)
1885     GPRReg m_gprOrInvalid;
1886 #elif USE(JSVALUE32_64)
1887     union {
1888         struct {
1889             GPRReg tagGPR;
1890             GPRReg payloadGPR;
1891         } pair;
1892         FPRReg fpr;
1893     } m_register;
1894     bool m_isDouble;
1895 #endif
1896 };
1897 
1898 class StorageOperand {

1899 public:
1900     explicit StorageOperand(SpeculativeJIT* jit, Edge edge)
1901         : m_jit(jit)
1902         , m_edge(edge)
1903         , m_gprOrInvalid(InvalidGPRReg)
1904     {
1905         ASSERT(m_jit);
1906         ASSERT(edge.useKind() == UntypedUse || edge.useKind() == KnownCellUse);
1907         if (jit-&gt;isFilled(node()))
1908             gpr();
1909     }
1910 
1911     ~StorageOperand()
1912     {
1913         ASSERT(m_gprOrInvalid != InvalidGPRReg);
1914         m_jit-&gt;unlock(m_gprOrInvalid);
1915     }
1916 
1917     Edge edge() const
1918     {
</pre>
<hr />
<pre>
1936         m_jit-&gt;use(node());
1937     }
1938 
1939 private:
1940     SpeculativeJIT* m_jit;
1941     Edge m_edge;
1942     GPRReg m_gprOrInvalid;
1943 };
1944 
1945 
1946 // === Temporaries ===
1947 //
1948 // These classes are used to allocate temporary registers.
1949 // A mechanism is provided to attempt to reuse the registers
1950 // currently allocated to child nodes whose value is consumed
1951 // by, and not live after, this operation.
1952 
1953 enum ReuseTag { Reuse };
1954 
1955 class GPRTemporary {

1956 public:
1957     GPRTemporary();
1958     GPRTemporary(SpeculativeJIT*);
1959     GPRTemporary(SpeculativeJIT*, GPRReg specific);
1960     template&lt;typename T&gt;
1961     GPRTemporary(SpeculativeJIT* jit, ReuseTag, T&amp; operand)
1962         : m_jit(jit)
1963         , m_gpr(InvalidGPRReg)
1964     {
1965         if (m_jit-&gt;canReuse(operand.node()))
1966             m_gpr = m_jit-&gt;reuse(operand.gpr());
1967         else
1968             m_gpr = m_jit-&gt;allocate();
1969     }
1970     template&lt;typename T1, typename T2&gt;
1971     GPRTemporary(SpeculativeJIT* jit, ReuseTag, T1&amp; op1, T2&amp; op2)
1972         : m_jit(jit)
1973         , m_gpr(InvalidGPRReg)
1974     {
1975         if (m_jit-&gt;canReuse(op1.node()))
</pre>
<hr />
<pre>
2006 
2007     void adopt(GPRTemporary&amp;);
2008 
2009     ~GPRTemporary()
2010     {
2011         if (m_jit &amp;&amp; m_gpr != InvalidGPRReg)
2012             m_jit-&gt;unlock(gpr());
2013     }
2014 
2015     GPRReg gpr()
2016     {
2017         return m_gpr;
2018     }
2019 
2020 private:
2021     SpeculativeJIT* m_jit;
2022     GPRReg m_gpr;
2023 };
2024 
2025 class JSValueRegsTemporary {

2026 public:
2027     JSValueRegsTemporary();
2028     JSValueRegsTemporary(SpeculativeJIT*);
2029     template&lt;typename T&gt;
2030     JSValueRegsTemporary(SpeculativeJIT*, ReuseTag, T&amp; operand, WhichValueWord resultRegWord = PayloadWord);
2031     JSValueRegsTemporary(SpeculativeJIT*, ReuseTag, JSValueOperand&amp;);
2032     ~JSValueRegsTemporary();
2033 
2034     JSValueRegs regs();
2035 
2036 private:
2037 #if USE(JSVALUE64)
2038     GPRTemporary m_gpr;
2039 #else
2040     GPRTemporary m_payloadGPR;
2041     GPRTemporary m_tagGPR;
2042 #endif
2043 };
2044 
2045 class FPRTemporary {
<span class="line-modified">2046     WTF_MAKE_NONCOPYABLE(FPRTemporary);</span>
2047 public:
2048     FPRTemporary(FPRTemporary&amp;&amp;);
2049     FPRTemporary(SpeculativeJIT*);
2050     FPRTemporary(SpeculativeJIT*, SpeculateDoubleOperand&amp;);
2051     FPRTemporary(SpeculativeJIT*, SpeculateDoubleOperand&amp;, SpeculateDoubleOperand&amp;);
2052 #if USE(JSVALUE32_64)
2053     FPRTemporary(SpeculativeJIT*, JSValueOperand&amp;);
2054 #endif
2055 
2056     ~FPRTemporary()
2057     {
2058         if (LIKELY(m_jit))
2059             m_jit-&gt;unlock(fpr());
2060     }
2061 
2062     FPRReg fpr() const
2063     {
2064         ASSERT(m_jit);
2065         ASSERT(m_fpr != InvalidFPRReg);
2066         return m_fpr;
</pre>
<hr />
<pre>
2100     }
2101 };
2102 #endif
2103 
2104 class FPRResult : public FPRTemporary {
2105 public:
2106     FPRResult(SpeculativeJIT* jit)
2107         : FPRTemporary(jit, lockedResult(jit))
2108     {
2109     }
2110 
2111 private:
2112     static FPRReg lockedResult(SpeculativeJIT* jit)
2113     {
2114         jit-&gt;lock(FPRInfo::returnValueFPR);
2115         return FPRInfo::returnValueFPR;
2116     }
2117 };
2118 
2119 class JSValueRegsFlushedCallResult {

2120 public:
2121     JSValueRegsFlushedCallResult(SpeculativeJIT* jit)
2122 #if USE(JSVALUE64)
2123         : m_gpr(jit)
2124 #else
2125         : m_payloadGPR(jit)
2126         , m_tagGPR(jit)
2127 #endif
2128     {
2129     }
2130 
2131     JSValueRegs regs()
2132     {
2133 #if USE(JSVALUE64)
2134         return JSValueRegs { m_gpr.gpr() };
2135 #else
2136         return JSValueRegs { m_tagGPR.gpr(), m_payloadGPR.gpr() };
2137 #endif
2138     }
2139 
</pre>
<hr />
<pre>
2141 #if USE(JSVALUE64)
2142     GPRFlushedCallResult m_gpr;
2143 #else
2144     GPRFlushedCallResult m_payloadGPR;
2145     GPRFlushedCallResult2 m_tagGPR;
2146 #endif
2147 };
2148 
2149 
2150 // === Speculative Operand types ===
2151 //
2152 // SpeculateInt32Operand, SpeculateStrictInt32Operand and SpeculateCellOperand.
2153 //
2154 // These are used to lock the operands to a node into machine registers within the
2155 // SpeculativeJIT. The classes operate like those above, however these will
2156 // perform a speculative check for a more restrictive type than we can statically
2157 // determine the operand to have. If the operand does not have the requested type,
2158 // a bail-out to the non-speculative path will be taken.
2159 
2160 class SpeculateInt32Operand {

2161 public:
2162     explicit SpeculateInt32Operand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
2163         : m_jit(jit)
2164         , m_edge(edge)
2165         , m_gprOrInvalid(InvalidGPRReg)
2166 #ifndef NDEBUG
2167         , m_format(DataFormatNone)
2168 #endif
2169     {
2170         ASSERT(m_jit);
2171         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || (edge.useKind() == Int32Use || edge.useKind() == KnownInt32Use));
2172         if (jit-&gt;isFilled(node()))
2173             gpr();
2174     }
2175 
2176     ~SpeculateInt32Operand()
2177     {
2178         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2179         m_jit-&gt;unlock(m_gprOrInvalid);
2180     }
</pre>
<hr />
<pre>
2199     GPRReg gpr()
2200     {
2201         if (m_gprOrInvalid == InvalidGPRReg)
2202             m_gprOrInvalid = m_jit-&gt;fillSpeculateInt32(edge(), m_format);
2203         return m_gprOrInvalid;
2204     }
2205 
2206     void use()
2207     {
2208         m_jit-&gt;use(node());
2209     }
2210 
2211 private:
2212     SpeculativeJIT* m_jit;
2213     Edge m_edge;
2214     GPRReg m_gprOrInvalid;
2215     DataFormat m_format;
2216 };
2217 
2218 class SpeculateStrictInt32Operand {

2219 public:
2220     explicit SpeculateStrictInt32Operand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
2221         : m_jit(jit)
2222         , m_edge(edge)
2223         , m_gprOrInvalid(InvalidGPRReg)
2224     {
2225         ASSERT(m_jit);
2226         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || (edge.useKind() == Int32Use || edge.useKind() == KnownInt32Use));
2227         if (jit-&gt;isFilled(node()))
2228             gpr();
2229     }
2230 
2231     ~SpeculateStrictInt32Operand()
2232     {
2233         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2234         m_jit-&gt;unlock(m_gprOrInvalid);
2235     }
2236 
2237     Edge edge() const
2238     {
</pre>
<hr />
<pre>
2247     GPRReg gpr()
2248     {
2249         if (m_gprOrInvalid == InvalidGPRReg)
2250             m_gprOrInvalid = m_jit-&gt;fillSpeculateInt32Strict(edge());
2251         return m_gprOrInvalid;
2252     }
2253 
2254     void use()
2255     {
2256         m_jit-&gt;use(node());
2257     }
2258 
2259 private:
2260     SpeculativeJIT* m_jit;
2261     Edge m_edge;
2262     GPRReg m_gprOrInvalid;
2263 };
2264 
2265 // Gives you a canonical Int52 (i.e. it&#39;s left-shifted by 16, low bits zero).
2266 class SpeculateInt52Operand {

2267 public:
2268     explicit SpeculateInt52Operand(SpeculativeJIT* jit, Edge edge)
2269         : m_jit(jit)
2270         , m_edge(edge)
2271         , m_gprOrInvalid(InvalidGPRReg)
2272     {
2273         RELEASE_ASSERT(edge.useKind() == Int52RepUse);
2274         if (jit-&gt;isFilled(node()))
2275             gpr();
2276     }
2277 
2278     ~SpeculateInt52Operand()
2279     {
2280         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2281         m_jit-&gt;unlock(m_gprOrInvalid);
2282     }
2283 
2284     Edge edge() const
2285     {
2286         return m_edge;
</pre>
<hr />
<pre>
2294     GPRReg gpr()
2295     {
2296         if (m_gprOrInvalid == InvalidGPRReg)
2297             m_gprOrInvalid = m_jit-&gt;fillSpeculateInt52(edge(), DataFormatInt52);
2298         return m_gprOrInvalid;
2299     }
2300 
2301     void use()
2302     {
2303         m_jit-&gt;use(node());
2304     }
2305 
2306 private:
2307     SpeculativeJIT* m_jit;
2308     Edge m_edge;
2309     GPRReg m_gprOrInvalid;
2310 };
2311 
2312 // Gives you a strict Int52 (i.e. the payload is in the low 48 bits, high 16 bits are sign-extended).
2313 class SpeculateStrictInt52Operand {

2314 public:
2315     explicit SpeculateStrictInt52Operand(SpeculativeJIT* jit, Edge edge)
2316         : m_jit(jit)
2317         , m_edge(edge)
2318         , m_gprOrInvalid(InvalidGPRReg)
2319     {
2320         RELEASE_ASSERT(edge.useKind() == Int52RepUse);
2321         if (jit-&gt;isFilled(node()))
2322             gpr();
2323     }
2324 
2325     ~SpeculateStrictInt52Operand()
2326     {
2327         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2328         m_jit-&gt;unlock(m_gprOrInvalid);
2329     }
2330 
2331     Edge edge() const
2332     {
2333         return m_edge;
</pre>
<hr />
<pre>
2342     {
2343         if (m_gprOrInvalid == InvalidGPRReg)
2344             m_gprOrInvalid = m_jit-&gt;fillSpeculateInt52(edge(), DataFormatStrictInt52);
2345         return m_gprOrInvalid;
2346     }
2347 
2348     void use()
2349     {
2350         m_jit-&gt;use(node());
2351     }
2352 
2353 private:
2354     SpeculativeJIT* m_jit;
2355     Edge m_edge;
2356     GPRReg m_gprOrInvalid;
2357 };
2358 
2359 enum OppositeShiftTag { OppositeShift };
2360 
2361 class SpeculateWhicheverInt52Operand {

2362 public:
2363     explicit SpeculateWhicheverInt52Operand(SpeculativeJIT* jit, Edge edge)
2364         : m_jit(jit)
2365         , m_edge(edge)
2366         , m_gprOrInvalid(InvalidGPRReg)
2367         , m_strict(jit-&gt;betterUseStrictInt52(edge))
2368     {
2369         RELEASE_ASSERT(edge.useKind() == Int52RepUse);
2370         if (jit-&gt;isFilled(node()))
2371             gpr();
2372     }
2373 
2374     explicit SpeculateWhicheverInt52Operand(SpeculativeJIT* jit, Edge edge, const SpeculateWhicheverInt52Operand&amp; other)
2375         : m_jit(jit)
2376         , m_edge(edge)
2377         , m_gprOrInvalid(InvalidGPRReg)
2378         , m_strict(other.m_strict)
2379     {
2380         RELEASE_ASSERT(edge.useKind() == Int52RepUse);
2381         if (jit-&gt;isFilled(node()))
</pre>
<hr />
<pre>
2419     }
2420 
2421     void use()
2422     {
2423         m_jit-&gt;use(node());
2424     }
2425 
2426     DataFormat format() const
2427     {
2428         return m_strict ? DataFormatStrictInt52 : DataFormatInt52;
2429     }
2430 
2431 private:
2432     SpeculativeJIT* m_jit;
2433     Edge m_edge;
2434     GPRReg m_gprOrInvalid;
2435     bool m_strict;
2436 };
2437 
2438 class SpeculateDoubleOperand {

2439 public:
2440     explicit SpeculateDoubleOperand(SpeculativeJIT* jit, Edge edge)
2441         : m_jit(jit)
2442         , m_edge(edge)
2443         , m_fprOrInvalid(InvalidFPRReg)
2444     {
2445         ASSERT(m_jit);
2446         RELEASE_ASSERT(isDouble(edge.useKind()));
2447         if (jit-&gt;isFilled(node()))
2448             fpr();
2449     }
2450 
2451     ~SpeculateDoubleOperand()
2452     {
2453         ASSERT(m_fprOrInvalid != InvalidFPRReg);
2454         m_jit-&gt;unlock(m_fprOrInvalid);
2455     }
2456 
2457     Edge edge() const
2458     {
</pre>
<hr />
<pre>
2466 
2467     FPRReg fpr()
2468     {
2469         if (m_fprOrInvalid == InvalidFPRReg)
2470             m_fprOrInvalid = m_jit-&gt;fillSpeculateDouble(edge());
2471         return m_fprOrInvalid;
2472     }
2473 
2474     void use()
2475     {
2476         m_jit-&gt;use(node());
2477     }
2478 
2479 private:
2480     SpeculativeJIT* m_jit;
2481     Edge m_edge;
2482     FPRReg m_fprOrInvalid;
2483 };
2484 
2485 class SpeculateCellOperand {
<span class="line-modified">2486     WTF_MAKE_NONCOPYABLE(SpeculateCellOperand);</span>
2487 
2488 public:
2489     explicit SpeculateCellOperand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
2490         : m_jit(jit)
2491         , m_edge(edge)
2492         , m_gprOrInvalid(InvalidGPRReg)
2493     {
2494         ASSERT(m_jit);
2495         if (!edge)
2496             return;
2497         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || isCell(edge.useKind()));
2498         if (jit-&gt;isFilled(node()))
2499             gpr();
2500     }
2501 
2502     explicit SpeculateCellOperand(SpeculateCellOperand&amp;&amp; other)
2503     {
2504         m_jit = other.m_jit;
2505         m_edge = other.m_edge;
2506         m_gprOrInvalid = other.m_gprOrInvalid;
</pre>
<hr />
<pre>
2531     {
2532         ASSERT(m_edge);
2533         if (m_gprOrInvalid == InvalidGPRReg)
2534             m_gprOrInvalid = m_jit-&gt;fillSpeculateCell(edge());
2535         return m_gprOrInvalid;
2536     }
2537 
2538     void use()
2539     {
2540         ASSERT(m_edge);
2541         m_jit-&gt;use(node());
2542     }
2543 
2544 private:
2545     SpeculativeJIT* m_jit;
2546     Edge m_edge;
2547     GPRReg m_gprOrInvalid;
2548 };
2549 
2550 class SpeculateBooleanOperand {

2551 public:
2552     explicit SpeculateBooleanOperand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
2553         : m_jit(jit)
2554         , m_edge(edge)
2555         , m_gprOrInvalid(InvalidGPRReg)
2556     {
2557         ASSERT(m_jit);
2558         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || edge.useKind() == BooleanUse || edge.useKind() == KnownBooleanUse);
2559         if (jit-&gt;isFilled(node()))
2560             gpr();
2561     }
2562 
2563     ~SpeculateBooleanOperand()
2564     {
2565         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2566         m_jit-&gt;unlock(m_gprOrInvalid);
2567     }
2568 
2569     Edge edge() const
2570     {
</pre>
</td>
<td>
<hr />
<pre>
  96     enum SpillOrder {
  97         SpillOrderConstant = 1, // no spill, and cheap fill
  98         SpillOrderSpilled  = 2, // no spill
  99         SpillOrderJS       = 4, // needs spill
 100         SpillOrderStorage  = 4, // needs spill
 101         SpillOrderDouble   = 4, // needs spill
 102         SpillOrderInteger  = 5, // needs spill and box
 103         SpillOrderCell     = 5, // needs spill and box
 104         SpillOrderBoolean  = 5, // needs spill and box
 105     };
 106 #endif
 107 
 108     enum UseChildrenMode { CallUseChildren, UseChildrenCalledExplicitly };
 109 
 110 public:
 111     SpeculativeJIT(JITCompiler&amp;);
 112     ~SpeculativeJIT();
 113 
 114     VM&amp; vm()
 115     {
<span class="line-modified"> 116         return m_jit.vm();</span>
 117     }
 118 
 119     struct TrustedImmPtr {
 120         template &lt;typename T&gt;
 121         explicit TrustedImmPtr(T* value)
 122             : m_value(value)
 123         {
 124             static_assert(!std::is_base_of&lt;JSCell, T&gt;::value, &quot;To use a GC pointer, the graph must be aware of it. Use SpeculativeJIT::TrustedImmPtr::weakPointer instead.&quot;);
 125         }
 126 
 127         explicit TrustedImmPtr(RegisteredStructure structure)
 128             : m_value(structure.get())
 129         { }
 130 
 131         explicit TrustedImmPtr(std::nullptr_t)
 132             : m_value(nullptr)
 133         { }
 134 
 135         explicit TrustedImmPtr(FrozenValue* value)
 136             : m_value(value-&gt;cell())
</pre>
<hr />
<pre>
 561 
 562         default:
 563             // The following code handles JSValues.
 564             RELEASE_ASSERT(spillFormat &amp; DataFormatJS);
 565             m_jit.store32(info.tagGPR(), JITCompiler::tagFor(spillMe));
 566             m_jit.store32(info.payloadGPR(), JITCompiler::payloadFor(spillMe));
 567             info.spill(*m_stream, spillMe, spillFormat);
 568             return;
 569 #endif
 570         }
 571     }
 572 
 573     bool isKnownInteger(Node* node) { return m_state.forNode(node).isType(SpecInt32Only); }
 574     bool isKnownCell(Node* node) { return m_state.forNode(node).isType(SpecCell); }
 575 
 576     bool isKnownNotInteger(Node* node) { return !(m_state.forNode(node).m_type &amp; SpecInt32Only); }
 577     bool isKnownNotNumber(Node* node) { return !(m_state.forNode(node).m_type &amp; SpecFullNumber); }
 578     bool isKnownNotCell(Node* node) { return !(m_state.forNode(node).m_type &amp; SpecCell); }
 579     bool isKnownNotOther(Node* node) { return !(m_state.forNode(node).m_type &amp; SpecOther); }
 580 
<span class="line-added"> 581     bool canBeRope(Edge&amp;);</span>
<span class="line-added"> 582 </span>
 583     UniquedStringImpl* identifierUID(unsigned index)
 584     {
 585         return m_jit.graph().identifiers()[index];
 586     }
 587 
 588     // Spill all VirtualRegisters back to the JSStack.
 589     void flushRegisters()
 590     {
 591         for (gpr_iterator iter = m_gprs.begin(); iter != m_gprs.end(); ++iter) {
 592             if (iter.name().isValid()) {
 593                 spill(iter.name());
 594                 iter.release();
 595             }
 596         }
 597         for (fpr_iterator iter = m_fprs.begin(); iter != m_fprs.end(); ++iter) {
 598             if (iter.name().isValid()) {
 599                 spill(iter.name());
 600                 iter.release();
 601             }
 602         }
</pre>
<hr />
<pre>
 646         switch (op) {
 647         case ArithBitAnd:
 648             m_jit.and32(op1, op2, result);
 649             break;
 650         case ArithBitOr:
 651             m_jit.or32(op1, op2, result);
 652             break;
 653         case ArithBitXor:
 654             m_jit.xor32(op1, op2, result);
 655             break;
 656         default:
 657             RELEASE_ASSERT_NOT_REACHED();
 658         }
 659     }
 660     void shiftOp(NodeType op, GPRReg op1, int32_t shiftAmount, GPRReg result)
 661     {
 662         switch (op) {
 663         case BitRShift:
 664             m_jit.rshift32(op1, Imm32(shiftAmount), result);
 665             break;
<span class="line-modified"> 666         case ArithBitLShift:</span>
 667             m_jit.lshift32(op1, Imm32(shiftAmount), result);
 668             break;
 669         case BitURShift:
 670             m_jit.urshift32(op1, Imm32(shiftAmount), result);
 671             break;
 672         default:
 673             RELEASE_ASSERT_NOT_REACHED();
 674         }
 675     }
 676     void shiftOp(NodeType op, GPRReg op1, GPRReg shiftAmount, GPRReg result)
 677     {
 678         switch (op) {
 679         case BitRShift:
 680             m_jit.rshift32(op1, shiftAmount, result);
 681             break;
<span class="line-modified"> 682         case ArithBitLShift:</span>
 683             m_jit.lshift32(op1, shiftAmount, result);
 684             break;
 685         case BitURShift:
 686             m_jit.urshift32(op1, shiftAmount, result);
 687             break;
 688         default:
 689             RELEASE_ASSERT_NOT_REACHED();
 690         }
 691     }
 692 
 693     // Returns the index of the branch node if peephole is okay, UINT_MAX otherwise.
 694     unsigned detectPeepHoleBranch()
 695     {
 696         // Check that no intervening nodes will be generated.
 697         for (unsigned index = m_indexInBlock + 1; index &lt; m_block-&gt;size() - 1; ++index) {
 698             Node* node = m_block-&gt;at(index);
 699             if (!node-&gt;shouldGenerate())
 700                 continue;
 701             // Check if it&#39;s a Phantom that can be safely ignored.
 702             if (node-&gt;op() == Phantom &amp;&amp; !node-&gt;child1())
</pre>
<hr />
<pre>
 974 
 975     JITCompiler::Call callOperationWithCallFrameRollbackOnException(Z_JITOperation_E operation, GPRReg result)
 976     {
 977         m_jit.setupArguments&lt;Z_JITOperation_E&gt;();
 978         return appendCallWithCallFrameRollbackOnExceptionSetResult(operation, result);
 979     }
 980 
 981 #if !defined(NDEBUG) &amp;&amp; !CPU(ARM_THUMB2) &amp;&amp; !CPU(MIPS)
 982     void prepareForExternalCall()
 983     {
 984         // We&#39;re about to call out to a &quot;native&quot; helper function. The helper
 985         // function is expected to set topCallFrame itself with the ExecState
 986         // that is passed to it.
 987         //
 988         // We explicitly trash topCallFrame here so that we&#39;ll know if some of
 989         // the helper functions are not setting topCallFrame when they should
 990         // be doing so. Note: the previous value in topcallFrame was not valid
 991         // anyway since it was not being updated by JIT&#39;ed code by design.
 992 
 993         for (unsigned i = 0; i &lt; sizeof(void*) / 4; i++)
<span class="line-modified"> 994             m_jit.store32(TrustedImm32(0xbadbeef), reinterpret_cast&lt;char*&gt;(&amp;vm().topCallFrame) + i * 4);</span>
 995     }
 996 #else
 997     void prepareForExternalCall() { }
 998 #endif
 999 
1000     // These methods add call instructions, optionally setting results, and optionally rolling back the call frame on an exception.
1001     JITCompiler::Call appendCall(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
1002     {
1003         prepareForExternalCall();
1004         m_jit.emitStoreCodeOrigin(m_currentNode-&gt;origin.semantic);
1005         return m_jit.appendCall(function);
1006     }
1007 
1008     JITCompiler::Call appendCallWithCallFrameRollbackOnException(const FunctionPtr&lt;CFunctionPtrTag&gt; function)
1009     {
1010         JITCompiler::Call call = appendCall(function);
1011         m_jit.exceptionCheckWithCallFrameRollback();
1012         return call;
1013     }
1014 
</pre>
<hr />
<pre>
1308 
1309     void compileGetScope(Node*);
1310     void compileSkipScope(Node*);
1311     void compileGetGlobalObject(Node*);
1312     void compileGetGlobalThis(Node*);
1313 
1314     void compileGetArrayLength(Node*);
1315 
1316     void compileCheckTypeInfoFlags(Node*);
1317     void compileCheckStringIdent(Node*);
1318 
1319     void compileParseInt(Node*);
1320 
1321     void compileValueRep(Node*);
1322     void compileDoubleRep(Node*);
1323 
1324     void compileValueToInt32(Node*);
1325     void compileUInt32ToNumber(Node*);
1326     void compileDoubleAsInt32(Node*);
1327 
<span class="line-added">1328     void compileValueBitNot(Node*);</span>
1329     void compileBitwiseNot(Node*);
1330 
1331     template&lt;typename SnippetGenerator, J_JITOperation_EJJ slowPathFunction&gt;
1332     void emitUntypedBitOp(Node*);
1333     void compileBitwiseOp(Node*);
1334     void compileValueBitwiseOp(Node*);
1335 
1336     void emitUntypedRightShiftBitOp(Node*);
<span class="line-added">1337     void compileValueLShiftOp(Node*);</span>
1338     void compileShiftOp(Node*);
1339 
1340     template &lt;typename Generator, typename RepatchingFunction, typename NonRepatchingFunction&gt;
1341     void compileMathIC(Node*, JITBinaryMathIC&lt;Generator&gt;*, bool needsScratchGPRReg, bool needsScratchFPRReg, RepatchingFunction, NonRepatchingFunction);
1342     template &lt;typename Generator, typename RepatchingFunction, typename NonRepatchingFunction&gt;
1343     void compileMathIC(Node*, JITUnaryMathIC&lt;Generator&gt;*, bool needsScratchGPRReg, RepatchingFunction, NonRepatchingFunction);
1344 
1345     void compileArithDoubleUnaryOp(Node*, double (*doubleFunction)(double), double (*operation)(ExecState*, EncodedJSValue));
1346     void compileValueAdd(Node*);
1347     void compileValueSub(Node*);
1348     void compileArithAdd(Node*);
1349     void compileMakeRope(Node*);
1350     void compileArithAbs(Node*);
1351     void compileArithClz32(Node*);
1352     void compileArithSub(Node*);
1353     void compileValueNegate(Node*);
1354     void compileArithNegate(Node*);
1355     void compileValueMul(Node*);
1356     void compileArithMul(Node*);
1357     void compileValueDiv(Node*);
1358     void compileArithDiv(Node*);
1359     void compileArithFRound(Node*);
<span class="line-added">1360     void compileValueMod(Node*);</span>
1361     void compileArithMod(Node*);
1362     void compileArithPow(Node*);
<span class="line-added">1363     void compileValuePow(Node*);</span>
1364     void compileArithRounding(Node*);
1365     void compileArithRandom(Node*);
1366     void compileArithUnary(Node*);
1367     void compileArithSqrt(Node*);
1368     void compileArithMinMax(Node*);
1369     void compileConstantStoragePointer(Node*);
1370     void compileGetIndexedPropertyStorage(Node*);
1371     JITCompiler::Jump jumpForTypedArrayOutOfBounds(Node*, GPRReg baseGPR, GPRReg indexGPR);
1372     JITCompiler::Jump jumpForTypedArrayIsNeuteredIfOutOfBounds(Node*, GPRReg baseGPR, JITCompiler::Jump outOfBounds);
1373     void emitTypedArrayBoundsCheck(Node*, GPRReg baseGPR, GPRReg indexGPR);
1374     void compileGetTypedArrayByteOffset(Node*);
1375     void compileGetByValOnIntTypedArray(Node*, TypedArrayType);
1376     void compilePutByValForIntTypedArray(GPRReg base, GPRReg property, Node*, TypedArrayType);
1377     void compileGetByValOnFloatTypedArray(Node*, TypedArrayType);
1378     void compilePutByValForFloatTypedArray(GPRReg base, GPRReg property, Node*, TypedArrayType);
1379     void compileGetByValForObjectWithString(Node*);
1380     void compileGetByValForObjectWithSymbol(Node*);
1381     void compilePutByValForCellWithString(Node*, Edge&amp; child1, Edge&amp; child2, Edge&amp; child3);
1382     void compilePutByValForCellWithSymbol(Node*, Edge&amp; child1, Edge&amp; child2, Edge&amp; child3);
1383     void compileGetByValWithThis(Node*);
</pre>
<hr />
<pre>
1501         GPRReg scratchGPR, MacroAssembler::JumpList&amp; slowPath)
1502     {
1503         m_jit.emitAllocateJSCell(resultGPR, allocator, allocatorGPR, structure, scratchGPR, slowPath);
1504     }
1505 
1506     // Allocator for an object of a specific size.
1507     template &lt;typename StructureType, typename StorageType&gt; // StructureType and StorageType can be GPR or ImmPtr.
1508     void emitAllocateJSObject(
1509         GPRReg resultGPR, const JITAllocator&amp; allocator, GPRReg allocatorGPR, StructureType structure,
1510         StorageType storage, GPRReg scratchGPR, MacroAssembler::JumpList&amp; slowPath)
1511     {
1512         m_jit.emitAllocateJSObject(
1513             resultGPR, allocator, allocatorGPR, structure, storage, scratchGPR, slowPath);
1514     }
1515 
1516     template &lt;typename ClassType, typename StructureType, typename StorageType&gt; // StructureType and StorageType can be GPR or ImmPtr.
1517     void emitAllocateJSObjectWithKnownSize(
1518         GPRReg resultGPR, StructureType structure, StorageType storage, GPRReg scratchGPR1,
1519         GPRReg scratchGPR2, MacroAssembler::JumpList&amp; slowPath, size_t size)
1520     {
<span class="line-modified">1521         m_jit.emitAllocateJSObjectWithKnownSize&lt;ClassType&gt;(vm(), resultGPR, structure, storage, scratchGPR1, scratchGPR2, slowPath, size);</span>
1522     }
1523 
1524     // Convenience allocator for a built-in object.
1525     template &lt;typename ClassType, typename StructureType, typename StorageType&gt; // StructureType and StorageType can be GPR or ImmPtr.
1526     void emitAllocateJSObject(GPRReg resultGPR, StructureType structure, StorageType storage,
1527         GPRReg scratchGPR1, GPRReg scratchGPR2, MacroAssembler::JumpList&amp; slowPath)
1528     {
<span class="line-modified">1529         m_jit.emitAllocateJSObject&lt;ClassType&gt;(vm(), resultGPR, structure, storage, scratchGPR1, scratchGPR2, slowPath);</span>
1530     }
1531 
1532     template &lt;typename ClassType, typename StructureType&gt; // StructureType and StorageType can be GPR or ImmPtr.
1533     void emitAllocateVariableSizedJSObject(GPRReg resultGPR, StructureType structure, GPRReg allocationSize, GPRReg scratchGPR1, GPRReg scratchGPR2, MacroAssembler::JumpList&amp; slowPath)
1534     {
<span class="line-modified">1535         m_jit.emitAllocateVariableSizedJSObject&lt;ClassType&gt;(vm(), resultGPR, structure, allocationSize, scratchGPR1, scratchGPR2, slowPath);</span>
1536     }
1537 
1538     template&lt;typename ClassType&gt;
1539     void emitAllocateDestructibleObject(GPRReg resultGPR, RegisteredStructure structure,
1540         GPRReg scratchGPR1, GPRReg scratchGPR2, MacroAssembler::JumpList&amp; slowPath)
1541     {
<span class="line-modified">1542         m_jit.emitAllocateDestructibleObject&lt;ClassType&gt;(vm(), resultGPR, structure.get(), scratchGPR1, scratchGPR2, slowPath);</span>
1543     }
1544 
1545     void emitAllocateRawObject(GPRReg resultGPR, RegisteredStructure, GPRReg storageGPR, unsigned numElements, unsigned vectorLength);
1546 
1547     void emitGetLength(InlineCallFrame*, GPRReg lengthGPR, bool includeThis = false);
1548     void emitGetLength(CodeOrigin, GPRReg lengthGPR, bool includeThis = false);
1549     void emitGetCallee(CodeOrigin, GPRReg calleeGPR);
1550     void emitGetArgumentStart(CodeOrigin, GPRReg startGPR);
1551     void emitPopulateSliceIndex(Edge&amp;, Optional&lt;GPRReg&gt; indexGPR, GPRReg lengthGPR, GPRReg resultGPR);
1552 
1553     // Generate an OSR exit fuzz check. Returns Jump() if OSR exit fuzz is not enabled, or if
1554     // it&#39;s in training mode.
1555     MacroAssembler::Jump emitOSRExitFuzzCheck();
1556 
1557     // Add a speculation check.
1558     void speculationCheck(ExitKind, JSValueSource, Node*, MacroAssembler::Jump jumpToFail);
1559     void speculationCheck(ExitKind, JSValueSource, Node*, const MacroAssembler::JumpList&amp; jumpsToFail);
1560 
1561     // Add a speculation check without additional recovery, and with a promise to supply a jump later.
1562     OSRExitJumpPlaceholder speculationCheck(ExitKind, JSValueSource, Node*);
</pre>
<hr />
<pre>
1636     void speculateSymbol(Edge);
1637     void speculateBigInt(Edge, GPRReg cell);
1638     void speculateBigInt(Edge);
1639     void speculateNotCell(Edge, JSValueRegs);
1640     void speculateNotCell(Edge);
1641     void speculateOther(Edge, JSValueRegs, GPRReg temp);
1642     void speculateOther(Edge, JSValueRegs);
1643     void speculateOther(Edge);
1644     void speculateMisc(Edge, JSValueRegs);
1645     void speculateMisc(Edge);
1646     void speculate(Node*, Edge);
1647 
1648     JITCompiler::JumpList jumpSlowForUnwantedArrayMode(GPRReg tempWithIndexingTypeReg, ArrayMode);
1649     void checkArray(Node*);
1650     void arrayify(Node*, GPRReg baseReg, GPRReg propertyReg);
1651     void arrayify(Node*);
1652 
1653     template&lt;bool strict&gt;
1654     GPRReg fillSpeculateInt32Internal(Edge, DataFormat&amp; returnFormat);
1655 
<span class="line-modified">1656     void cageTypedArrayStorage(GPRReg, GPRReg);</span>






1657 
1658     void recordSetLocal(
1659         VirtualRegister bytecodeReg, VirtualRegister machineReg, DataFormat format)
1660     {
1661         m_stream-&gt;appendAndLog(VariableEvent::setLocal(bytecodeReg, machineReg, format));
1662     }
1663 
1664     void recordSetLocal(DataFormat format)
1665     {
1666         VariableAccessData* variable = m_currentNode-&gt;variableAccessData();
1667         recordSetLocal(variable-&gt;local(), variable-&gt;machineLocal(), format);
1668     }
1669 
1670     GenerationInfo&amp; generationInfoFromVirtualRegister(VirtualRegister virtualRegister)
1671     {
1672         return m_generationInfo[virtualRegister.toLocal()];
1673     }
1674 
1675     GenerationInfo&amp; generationInfo(Node* node)
1676     {
</pre>
<hr />
<pre>
1680     GenerationInfo&amp; generationInfo(Edge edge)
1681     {
1682         return generationInfo(edge.node());
1683     }
1684 
1685     // The JIT, while also provides MacroAssembler functionality.
1686     JITCompiler&amp; m_jit;
1687     Graph&amp; m_graph;
1688 
1689     // The current node being generated.
1690     BasicBlock* m_block;
1691     Node* m_currentNode;
1692     NodeType m_lastGeneratedNode;
1693     unsigned m_indexInBlock;
1694 
1695     // Virtual and physical register maps.
1696     Vector&lt;GenerationInfo, 32&gt; m_generationInfo;
1697     RegisterBank&lt;GPRInfo&gt; m_gprs;
1698     RegisterBank&lt;FPRInfo&gt; m_fprs;
1699 
<span class="line-added">1700     // It is possible, during speculative generation, to reach a situation in which we</span>
<span class="line-added">1701     // can statically determine a speculation will fail (for example, when two nodes</span>
<span class="line-added">1702     // will make conflicting speculations about the same operand). In such cases this</span>
<span class="line-added">1703     // flag is cleared, indicating no further code generation should take place.</span>
<span class="line-added">1704     bool m_compileOkay;</span>
<span class="line-added">1705 </span>
1706     Vector&lt;MacroAssembler::Label&gt; m_osrEntryHeads;
1707 
1708     struct BranchRecord {
1709         BranchRecord(MacroAssembler::Jump jump, BasicBlock* destination)
1710             : jump(jump)
1711             , destination(destination)
1712         {
1713         }
1714 
1715         MacroAssembler::Jump jump;
1716         BasicBlock* destination;
1717     };
1718     Vector&lt;BranchRecord, 8&gt; m_branches;
1719 
1720     NodeOrigin m_origin;
1721 
1722     InPlaceAbstractState m_state;
1723     AbstractInterpreter&lt;InPlaceAbstractState&gt; m_interpreter;
1724 
1725     VariableEventStream* m_stream;
</pre>
<hr />
<pre>
1730         Function&lt;void()&gt; generator;
1731         Node* currentNode;
1732         unsigned streamIndex;
1733     };
1734     Vector&lt;SlowPathLambda&gt; m_slowPathLambdas;
1735     Vector&lt;SilentRegisterSavePlan&gt; m_plans;
1736     Optional&lt;unsigned&gt; m_outOfLineStreamIndex;
1737 };
1738 
1739 
1740 // === Operand types ===
1741 //
1742 // These classes are used to lock the operands to a node into machine
1743 // registers. These classes implement of pattern of locking a value
1744 // into register at the point of construction only if it is already in
1745 // registers, and otherwise loading it lazily at the point it is first
1746 // used. We do so in order to attempt to avoid spilling one operand
1747 // in order to make space available for another.
1748 
1749 class JSValueOperand {
<span class="line-added">1750     WTF_MAKE_FAST_ALLOCATED;</span>
1751 public:
1752     explicit JSValueOperand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
1753         : m_jit(jit)
1754         , m_edge(edge)
1755 #if USE(JSVALUE64)
1756         , m_gprOrInvalid(InvalidGPRReg)
1757 #elif USE(JSVALUE32_64)
1758         , m_isDouble(false)
1759 #endif
1760     {
1761         ASSERT(m_jit);
1762         if (!edge)
1763             return;
1764         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || edge.useKind() == UntypedUse);
1765 #if USE(JSVALUE64)
1766         if (jit-&gt;isFilled(node()))
1767             gpr();
1768 #elif USE(JSVALUE32_64)
1769         m_register.pair.tagGPR = InvalidGPRReg;
1770         m_register.pair.payloadGPR = InvalidGPRReg;
</pre>
<hr />
<pre>
1886     }
1887 
1888 private:
1889     SpeculativeJIT* m_jit;
1890     Edge m_edge;
1891 #if USE(JSVALUE64)
1892     GPRReg m_gprOrInvalid;
1893 #elif USE(JSVALUE32_64)
1894     union {
1895         struct {
1896             GPRReg tagGPR;
1897             GPRReg payloadGPR;
1898         } pair;
1899         FPRReg fpr;
1900     } m_register;
1901     bool m_isDouble;
1902 #endif
1903 };
1904 
1905 class StorageOperand {
<span class="line-added">1906     WTF_MAKE_FAST_ALLOCATED;</span>
1907 public:
1908     explicit StorageOperand(SpeculativeJIT* jit, Edge edge)
1909         : m_jit(jit)
1910         , m_edge(edge)
1911         , m_gprOrInvalid(InvalidGPRReg)
1912     {
1913         ASSERT(m_jit);
1914         ASSERT(edge.useKind() == UntypedUse || edge.useKind() == KnownCellUse);
1915         if (jit-&gt;isFilled(node()))
1916             gpr();
1917     }
1918 
1919     ~StorageOperand()
1920     {
1921         ASSERT(m_gprOrInvalid != InvalidGPRReg);
1922         m_jit-&gt;unlock(m_gprOrInvalid);
1923     }
1924 
1925     Edge edge() const
1926     {
</pre>
<hr />
<pre>
1944         m_jit-&gt;use(node());
1945     }
1946 
1947 private:
1948     SpeculativeJIT* m_jit;
1949     Edge m_edge;
1950     GPRReg m_gprOrInvalid;
1951 };
1952 
1953 
1954 // === Temporaries ===
1955 //
1956 // These classes are used to allocate temporary registers.
1957 // A mechanism is provided to attempt to reuse the registers
1958 // currently allocated to child nodes whose value is consumed
1959 // by, and not live after, this operation.
1960 
1961 enum ReuseTag { Reuse };
1962 
1963 class GPRTemporary {
<span class="line-added">1964     WTF_MAKE_FAST_ALLOCATED;</span>
1965 public:
1966     GPRTemporary();
1967     GPRTemporary(SpeculativeJIT*);
1968     GPRTemporary(SpeculativeJIT*, GPRReg specific);
1969     template&lt;typename T&gt;
1970     GPRTemporary(SpeculativeJIT* jit, ReuseTag, T&amp; operand)
1971         : m_jit(jit)
1972         , m_gpr(InvalidGPRReg)
1973     {
1974         if (m_jit-&gt;canReuse(operand.node()))
1975             m_gpr = m_jit-&gt;reuse(operand.gpr());
1976         else
1977             m_gpr = m_jit-&gt;allocate();
1978     }
1979     template&lt;typename T1, typename T2&gt;
1980     GPRTemporary(SpeculativeJIT* jit, ReuseTag, T1&amp; op1, T2&amp; op2)
1981         : m_jit(jit)
1982         , m_gpr(InvalidGPRReg)
1983     {
1984         if (m_jit-&gt;canReuse(op1.node()))
</pre>
<hr />
<pre>
2015 
2016     void adopt(GPRTemporary&amp;);
2017 
2018     ~GPRTemporary()
2019     {
2020         if (m_jit &amp;&amp; m_gpr != InvalidGPRReg)
2021             m_jit-&gt;unlock(gpr());
2022     }
2023 
2024     GPRReg gpr()
2025     {
2026         return m_gpr;
2027     }
2028 
2029 private:
2030     SpeculativeJIT* m_jit;
2031     GPRReg m_gpr;
2032 };
2033 
2034 class JSValueRegsTemporary {
<span class="line-added">2035     WTF_MAKE_FAST_ALLOCATED;</span>
2036 public:
2037     JSValueRegsTemporary();
2038     JSValueRegsTemporary(SpeculativeJIT*);
2039     template&lt;typename T&gt;
2040     JSValueRegsTemporary(SpeculativeJIT*, ReuseTag, T&amp; operand, WhichValueWord resultRegWord = PayloadWord);
2041     JSValueRegsTemporary(SpeculativeJIT*, ReuseTag, JSValueOperand&amp;);
2042     ~JSValueRegsTemporary();
2043 
2044     JSValueRegs regs();
2045 
2046 private:
2047 #if USE(JSVALUE64)
2048     GPRTemporary m_gpr;
2049 #else
2050     GPRTemporary m_payloadGPR;
2051     GPRTemporary m_tagGPR;
2052 #endif
2053 };
2054 
2055 class FPRTemporary {
<span class="line-modified">2056     WTF_MAKE_FAST_ALLOCATED;</span>
2057 public:
2058     FPRTemporary(FPRTemporary&amp;&amp;);
2059     FPRTemporary(SpeculativeJIT*);
2060     FPRTemporary(SpeculativeJIT*, SpeculateDoubleOperand&amp;);
2061     FPRTemporary(SpeculativeJIT*, SpeculateDoubleOperand&amp;, SpeculateDoubleOperand&amp;);
2062 #if USE(JSVALUE32_64)
2063     FPRTemporary(SpeculativeJIT*, JSValueOperand&amp;);
2064 #endif
2065 
2066     ~FPRTemporary()
2067     {
2068         if (LIKELY(m_jit))
2069             m_jit-&gt;unlock(fpr());
2070     }
2071 
2072     FPRReg fpr() const
2073     {
2074         ASSERT(m_jit);
2075         ASSERT(m_fpr != InvalidFPRReg);
2076         return m_fpr;
</pre>
<hr />
<pre>
2110     }
2111 };
2112 #endif
2113 
2114 class FPRResult : public FPRTemporary {
2115 public:
2116     FPRResult(SpeculativeJIT* jit)
2117         : FPRTemporary(jit, lockedResult(jit))
2118     {
2119     }
2120 
2121 private:
2122     static FPRReg lockedResult(SpeculativeJIT* jit)
2123     {
2124         jit-&gt;lock(FPRInfo::returnValueFPR);
2125         return FPRInfo::returnValueFPR;
2126     }
2127 };
2128 
2129 class JSValueRegsFlushedCallResult {
<span class="line-added">2130     WTF_MAKE_FAST_ALLOCATED;</span>
2131 public:
2132     JSValueRegsFlushedCallResult(SpeculativeJIT* jit)
2133 #if USE(JSVALUE64)
2134         : m_gpr(jit)
2135 #else
2136         : m_payloadGPR(jit)
2137         , m_tagGPR(jit)
2138 #endif
2139     {
2140     }
2141 
2142     JSValueRegs regs()
2143     {
2144 #if USE(JSVALUE64)
2145         return JSValueRegs { m_gpr.gpr() };
2146 #else
2147         return JSValueRegs { m_tagGPR.gpr(), m_payloadGPR.gpr() };
2148 #endif
2149     }
2150 
</pre>
<hr />
<pre>
2152 #if USE(JSVALUE64)
2153     GPRFlushedCallResult m_gpr;
2154 #else
2155     GPRFlushedCallResult m_payloadGPR;
2156     GPRFlushedCallResult2 m_tagGPR;
2157 #endif
2158 };
2159 
2160 
2161 // === Speculative Operand types ===
2162 //
2163 // SpeculateInt32Operand, SpeculateStrictInt32Operand and SpeculateCellOperand.
2164 //
2165 // These are used to lock the operands to a node into machine registers within the
2166 // SpeculativeJIT. The classes operate like those above, however these will
2167 // perform a speculative check for a more restrictive type than we can statically
2168 // determine the operand to have. If the operand does not have the requested type,
2169 // a bail-out to the non-speculative path will be taken.
2170 
2171 class SpeculateInt32Operand {
<span class="line-added">2172     WTF_MAKE_FAST_ALLOCATED;</span>
2173 public:
2174     explicit SpeculateInt32Operand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
2175         : m_jit(jit)
2176         , m_edge(edge)
2177         , m_gprOrInvalid(InvalidGPRReg)
2178 #ifndef NDEBUG
2179         , m_format(DataFormatNone)
2180 #endif
2181     {
2182         ASSERT(m_jit);
2183         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || (edge.useKind() == Int32Use || edge.useKind() == KnownInt32Use));
2184         if (jit-&gt;isFilled(node()))
2185             gpr();
2186     }
2187 
2188     ~SpeculateInt32Operand()
2189     {
2190         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2191         m_jit-&gt;unlock(m_gprOrInvalid);
2192     }
</pre>
<hr />
<pre>
2211     GPRReg gpr()
2212     {
2213         if (m_gprOrInvalid == InvalidGPRReg)
2214             m_gprOrInvalid = m_jit-&gt;fillSpeculateInt32(edge(), m_format);
2215         return m_gprOrInvalid;
2216     }
2217 
2218     void use()
2219     {
2220         m_jit-&gt;use(node());
2221     }
2222 
2223 private:
2224     SpeculativeJIT* m_jit;
2225     Edge m_edge;
2226     GPRReg m_gprOrInvalid;
2227     DataFormat m_format;
2228 };
2229 
2230 class SpeculateStrictInt32Operand {
<span class="line-added">2231     WTF_MAKE_FAST_ALLOCATED;</span>
2232 public:
2233     explicit SpeculateStrictInt32Operand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
2234         : m_jit(jit)
2235         , m_edge(edge)
2236         , m_gprOrInvalid(InvalidGPRReg)
2237     {
2238         ASSERT(m_jit);
2239         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || (edge.useKind() == Int32Use || edge.useKind() == KnownInt32Use));
2240         if (jit-&gt;isFilled(node()))
2241             gpr();
2242     }
2243 
2244     ~SpeculateStrictInt32Operand()
2245     {
2246         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2247         m_jit-&gt;unlock(m_gprOrInvalid);
2248     }
2249 
2250     Edge edge() const
2251     {
</pre>
<hr />
<pre>
2260     GPRReg gpr()
2261     {
2262         if (m_gprOrInvalid == InvalidGPRReg)
2263             m_gprOrInvalid = m_jit-&gt;fillSpeculateInt32Strict(edge());
2264         return m_gprOrInvalid;
2265     }
2266 
2267     void use()
2268     {
2269         m_jit-&gt;use(node());
2270     }
2271 
2272 private:
2273     SpeculativeJIT* m_jit;
2274     Edge m_edge;
2275     GPRReg m_gprOrInvalid;
2276 };
2277 
2278 // Gives you a canonical Int52 (i.e. it&#39;s left-shifted by 16, low bits zero).
2279 class SpeculateInt52Operand {
<span class="line-added">2280     WTF_MAKE_FAST_ALLOCATED;</span>
2281 public:
2282     explicit SpeculateInt52Operand(SpeculativeJIT* jit, Edge edge)
2283         : m_jit(jit)
2284         , m_edge(edge)
2285         , m_gprOrInvalid(InvalidGPRReg)
2286     {
2287         RELEASE_ASSERT(edge.useKind() == Int52RepUse);
2288         if (jit-&gt;isFilled(node()))
2289             gpr();
2290     }
2291 
2292     ~SpeculateInt52Operand()
2293     {
2294         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2295         m_jit-&gt;unlock(m_gprOrInvalid);
2296     }
2297 
2298     Edge edge() const
2299     {
2300         return m_edge;
</pre>
<hr />
<pre>
2308     GPRReg gpr()
2309     {
2310         if (m_gprOrInvalid == InvalidGPRReg)
2311             m_gprOrInvalid = m_jit-&gt;fillSpeculateInt52(edge(), DataFormatInt52);
2312         return m_gprOrInvalid;
2313     }
2314 
2315     void use()
2316     {
2317         m_jit-&gt;use(node());
2318     }
2319 
2320 private:
2321     SpeculativeJIT* m_jit;
2322     Edge m_edge;
2323     GPRReg m_gprOrInvalid;
2324 };
2325 
2326 // Gives you a strict Int52 (i.e. the payload is in the low 48 bits, high 16 bits are sign-extended).
2327 class SpeculateStrictInt52Operand {
<span class="line-added">2328     WTF_MAKE_FAST_ALLOCATED;</span>
2329 public:
2330     explicit SpeculateStrictInt52Operand(SpeculativeJIT* jit, Edge edge)
2331         : m_jit(jit)
2332         , m_edge(edge)
2333         , m_gprOrInvalid(InvalidGPRReg)
2334     {
2335         RELEASE_ASSERT(edge.useKind() == Int52RepUse);
2336         if (jit-&gt;isFilled(node()))
2337             gpr();
2338     }
2339 
2340     ~SpeculateStrictInt52Operand()
2341     {
2342         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2343         m_jit-&gt;unlock(m_gprOrInvalid);
2344     }
2345 
2346     Edge edge() const
2347     {
2348         return m_edge;
</pre>
<hr />
<pre>
2357     {
2358         if (m_gprOrInvalid == InvalidGPRReg)
2359             m_gprOrInvalid = m_jit-&gt;fillSpeculateInt52(edge(), DataFormatStrictInt52);
2360         return m_gprOrInvalid;
2361     }
2362 
2363     void use()
2364     {
2365         m_jit-&gt;use(node());
2366     }
2367 
2368 private:
2369     SpeculativeJIT* m_jit;
2370     Edge m_edge;
2371     GPRReg m_gprOrInvalid;
2372 };
2373 
2374 enum OppositeShiftTag { OppositeShift };
2375 
2376 class SpeculateWhicheverInt52Operand {
<span class="line-added">2377     WTF_MAKE_FAST_ALLOCATED;</span>
2378 public:
2379     explicit SpeculateWhicheverInt52Operand(SpeculativeJIT* jit, Edge edge)
2380         : m_jit(jit)
2381         , m_edge(edge)
2382         , m_gprOrInvalid(InvalidGPRReg)
2383         , m_strict(jit-&gt;betterUseStrictInt52(edge))
2384     {
2385         RELEASE_ASSERT(edge.useKind() == Int52RepUse);
2386         if (jit-&gt;isFilled(node()))
2387             gpr();
2388     }
2389 
2390     explicit SpeculateWhicheverInt52Operand(SpeculativeJIT* jit, Edge edge, const SpeculateWhicheverInt52Operand&amp; other)
2391         : m_jit(jit)
2392         , m_edge(edge)
2393         , m_gprOrInvalid(InvalidGPRReg)
2394         , m_strict(other.m_strict)
2395     {
2396         RELEASE_ASSERT(edge.useKind() == Int52RepUse);
2397         if (jit-&gt;isFilled(node()))
</pre>
<hr />
<pre>
2435     }
2436 
2437     void use()
2438     {
2439         m_jit-&gt;use(node());
2440     }
2441 
2442     DataFormat format() const
2443     {
2444         return m_strict ? DataFormatStrictInt52 : DataFormatInt52;
2445     }
2446 
2447 private:
2448     SpeculativeJIT* m_jit;
2449     Edge m_edge;
2450     GPRReg m_gprOrInvalid;
2451     bool m_strict;
2452 };
2453 
2454 class SpeculateDoubleOperand {
<span class="line-added">2455     WTF_MAKE_FAST_ALLOCATED;</span>
2456 public:
2457     explicit SpeculateDoubleOperand(SpeculativeJIT* jit, Edge edge)
2458         : m_jit(jit)
2459         , m_edge(edge)
2460         , m_fprOrInvalid(InvalidFPRReg)
2461     {
2462         ASSERT(m_jit);
2463         RELEASE_ASSERT(isDouble(edge.useKind()));
2464         if (jit-&gt;isFilled(node()))
2465             fpr();
2466     }
2467 
2468     ~SpeculateDoubleOperand()
2469     {
2470         ASSERT(m_fprOrInvalid != InvalidFPRReg);
2471         m_jit-&gt;unlock(m_fprOrInvalid);
2472     }
2473 
2474     Edge edge() const
2475     {
</pre>
<hr />
<pre>
2483 
2484     FPRReg fpr()
2485     {
2486         if (m_fprOrInvalid == InvalidFPRReg)
2487             m_fprOrInvalid = m_jit-&gt;fillSpeculateDouble(edge());
2488         return m_fprOrInvalid;
2489     }
2490 
2491     void use()
2492     {
2493         m_jit-&gt;use(node());
2494     }
2495 
2496 private:
2497     SpeculativeJIT* m_jit;
2498     Edge m_edge;
2499     FPRReg m_fprOrInvalid;
2500 };
2501 
2502 class SpeculateCellOperand {
<span class="line-modified">2503     WTF_MAKE_FAST_ALLOCATED;</span>
2504 
2505 public:
2506     explicit SpeculateCellOperand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
2507         : m_jit(jit)
2508         , m_edge(edge)
2509         , m_gprOrInvalid(InvalidGPRReg)
2510     {
2511         ASSERT(m_jit);
2512         if (!edge)
2513             return;
2514         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || isCell(edge.useKind()));
2515         if (jit-&gt;isFilled(node()))
2516             gpr();
2517     }
2518 
2519     explicit SpeculateCellOperand(SpeculateCellOperand&amp;&amp; other)
2520     {
2521         m_jit = other.m_jit;
2522         m_edge = other.m_edge;
2523         m_gprOrInvalid = other.m_gprOrInvalid;
</pre>
<hr />
<pre>
2548     {
2549         ASSERT(m_edge);
2550         if (m_gprOrInvalid == InvalidGPRReg)
2551             m_gprOrInvalid = m_jit-&gt;fillSpeculateCell(edge());
2552         return m_gprOrInvalid;
2553     }
2554 
2555     void use()
2556     {
2557         ASSERT(m_edge);
2558         m_jit-&gt;use(node());
2559     }
2560 
2561 private:
2562     SpeculativeJIT* m_jit;
2563     Edge m_edge;
2564     GPRReg m_gprOrInvalid;
2565 };
2566 
2567 class SpeculateBooleanOperand {
<span class="line-added">2568     WTF_MAKE_FAST_ALLOCATED;</span>
2569 public:
2570     explicit SpeculateBooleanOperand(SpeculativeJIT* jit, Edge edge, OperandSpeculationMode mode = AutomaticOperandSpeculation)
2571         : m_jit(jit)
2572         , m_edge(edge)
2573         , m_gprOrInvalid(InvalidGPRReg)
2574     {
2575         ASSERT(m_jit);
2576         ASSERT_UNUSED(mode, mode == ManualOperandSpeculation || edge.useKind() == BooleanUse || edge.useKind() == KnownBooleanUse);
2577         if (jit-&gt;isFilled(node()))
2578             gpr();
2579     }
2580 
2581     ~SpeculateBooleanOperand()
2582     {
2583         ASSERT(m_gprOrInvalid != InvalidGPRReg);
2584         m_jit-&gt;unlock(m_gprOrInvalid);
2585     }
2586 
2587     Edge edge() const
2588     {
</pre>
</td>
</tr>
</table>
<center><a href="DFGSpeculativeJIT.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGSpeculativeJIT32_64.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>