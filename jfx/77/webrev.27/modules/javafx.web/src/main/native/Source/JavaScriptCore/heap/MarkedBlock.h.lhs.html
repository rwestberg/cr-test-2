<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  *  Copyright (C) 1999-2000 Harri Porten (porten@kde.org)
  3  *  Copyright (C) 2001 Peter Kelly (pmk@post.com)
<a name="1" id="anc1"></a><span class="line-modified">  4  *  Copyright (C) 2003-2018 Apple Inc. All rights reserved.</span>
  5  *
  6  *  This library is free software; you can redistribute it and/or
  7  *  modify it under the terms of the GNU Lesser General Public
  8  *  License as published by the Free Software Foundation; either
  9  *  version 2 of the License, or (at your option) any later version.
 10  *
 11  *  This library is distributed in the hope that it will be useful,
 12  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 13  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 14  *  Lesser General Public License for more details.
 15  *
 16  *  You should have received a copy of the GNU Lesser General Public
 17  *  License along with this library; if not, write to the Free Software
 18  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 19  *
 20  */
 21 
 22 #pragma once
 23 
 24 #include &quot;CellAttributes.h&quot;
 25 #include &quot;DestructionMode.h&quot;
 26 #include &quot;HeapCell.h&quot;
 27 #include &quot;IterationStatus.h&quot;
 28 #include &quot;WeakSet.h&quot;
 29 #include &lt;wtf/Atomics.h&gt;
 30 #include &lt;wtf/Bitmap.h&gt;
 31 #include &lt;wtf/HashFunctions.h&gt;
 32 #include &lt;wtf/CountingLock.h&gt;
 33 #include &lt;wtf/StdLibExtras.h&gt;
 34 
 35 namespace JSC {
 36 
 37 class AlignedMemoryAllocator;
 38 class FreeList;
 39 class Heap;
 40 class JSCell;
 41 class BlockDirectory;
 42 class MarkedSpace;
 43 class SlotVisitor;
 44 class Subspace;
 45 
 46 typedef uint32_t HeapVersion;
 47 
 48 // A marked block is a page-aligned container for heap-allocated objects.
 49 // Objects are allocated within cells of the marked block. For a given
 50 // marked block, all cells have the same size. Objects smaller than the
 51 // cell size may be allocated in the marked block, in which case the
 52 // allocation suffers from internal fragmentation: wasted space whose
 53 // size is equal to the difference between the cell size and the object
 54 // size.
 55 
 56 class MarkedBlock {
 57     WTF_MAKE_NONCOPYABLE(MarkedBlock);
 58     friend class LLIntOffsetsExtractor;
 59     friend struct VerifyMarked;
 60 
 61 public:
 62     class Footer;
 63     class Handle;
 64 private:
 65     friend class Footer;
 66     friend class Handle;
 67 public:
 68     static constexpr size_t atomSize = 16; // bytes
 69 
 70     // Block size must be at least as large as the system page size.
 71 #if CPU(PPC64) || CPU(PPC64LE) || CPU(PPC) || CPU(UNKNOWN)
 72     static constexpr size_t blockSize = 64 * KB;
 73 #else
 74     static constexpr size_t blockSize = 16 * KB;
 75 #endif
 76 
 77     static constexpr size_t blockMask = ~(blockSize - 1); // blockSize must be a power of two.
 78 
 79     static constexpr size_t atomsPerBlock = blockSize / atomSize;
 80 
 81     static_assert(!(MarkedBlock::atomSize &amp; (MarkedBlock::atomSize - 1)), &quot;MarkedBlock::atomSize must be a power of two.&quot;);
 82     static_assert(!(MarkedBlock::blockSize &amp; (MarkedBlock::blockSize - 1)), &quot;MarkedBlock::blockSize must be a power of two.&quot;);
 83 
 84     struct VoidFunctor {
 85         typedef void ReturnType;
 86         void returnValue() { }
 87     };
 88 
 89     class CountFunctor {
 90     public:
 91         typedef size_t ReturnType;
 92 
 93         CountFunctor() : m_count(0) { }
 94         void count(size_t count) const { m_count += count; }
 95         ReturnType returnValue() const { return m_count; }
 96 
 97     private:
 98         // FIXME: This is mutable because we&#39;re using a functor rather than C++ lambdas.
 99         // https://bugs.webkit.org/show_bug.cgi?id=159644
100         mutable ReturnType m_count;
101     };
102 
103     class Handle {
104         WTF_MAKE_NONCOPYABLE(Handle);
105         WTF_MAKE_FAST_ALLOCATED;
106         friend class LLIntOffsetsExtractor;
107         friend class MarkedBlock;
108         friend struct VerifyMarked;
109     public:
110 
111         ~Handle();
112 
113         MarkedBlock&amp; block();
114         MarkedBlock::Footer&amp; blockFooter();
115 
116         void* cellAlign(void*);
117 
118         bool isEmpty();
119 
120         void lastChanceToFinalize();
121 
122         BlockDirectory* directory() const;
123         Subspace* subspace() const;
124         AlignedMemoryAllocator* alignedMemoryAllocator() const;
125         Heap* heap() const;
126         inline MarkedSpace* space() const;
<a name="2" id="anc2"></a><span class="line-modified">127         VM* vm() const;</span>
128         WeakSet&amp; weakSet();
129 
130         enum SweepMode { SweepOnly, SweepToFreeList };
131 
132         // Sweeping ensures that destructors get called and removes the block from the unswept
133         // set. Sweeping to free list also removes the block from the empty set, if it was in that
134         // set. Sweeping with SweepOnly may add this block to the empty set, if the block is found
135         // to be empty. The free-list being null implies SweepOnly.
136         //
137         // Note that you need to make sure that the empty bit reflects reality. If it&#39;s not set
138         // and the block is freshly created, then we&#39;ll make the mistake of running destructors in
139         // the block. If it&#39;s not set and the block has nothing marked, then we&#39;ll make the
140         // mistake of making a pop freelist rather than a bump freelist.
141         void sweep(FreeList*);
142 
143         // This is to be called by Subspace.
144         template&lt;typename DestroyFunc&gt;
145         void finishSweepKnowingHeapCellType(FreeList*, const DestroyFunc&amp;);
146 
147         void unsweepWithNoNewlyAllocated();
148 
<a name="3" id="anc3"></a><span class="line-removed">149         void zap(const FreeList&amp;);</span>
<span class="line-removed">150 </span>
151         void shrink();
152 
153         void visitWeakSet(SlotVisitor&amp;);
154         void reapWeakSet();
155 
156         // While allocating from a free list, MarkedBlock temporarily has bogus
157         // cell liveness data. To restore accurate cell liveness data, call one
158         // of these functions:
159         void didConsumeFreeList(); // Call this once you&#39;ve allocated all the items in the free list.
160         void stopAllocating(const FreeList&amp;);
161         void resumeAllocating(FreeList&amp;); // Call this if you canonicalized a block for some non-collection related purpose.
162 
163         size_t cellSize();
164         inline unsigned cellsPerBlock();
165 
166         const CellAttributes&amp; attributes() const;
167         DestructionMode destruction() const;
168         bool needsDestruction() const;
169         HeapCell::Kind cellKind() const;
170 
171         size_t markCount();
172         size_t size();
173 
174         bool isAllocated();
175 
176         bool isLive(HeapVersion markingVersion, HeapVersion newlyAllocatedVersion, bool isMarking, const HeapCell*);
177         inline bool isLiveCell(HeapVersion markingVersion, HeapVersion newlyAllocatedVersion, bool isMarking, const void*);
178 
179         bool isLive(const HeapCell*);
180         bool isLiveCell(const void*);
181 
182         bool isFreeListedCell(const void* target) const;
183 
184         template &lt;typename Functor&gt; IterationStatus forEachCell(const Functor&amp;);
185         template &lt;typename Functor&gt; inline IterationStatus forEachLiveCell(const Functor&amp;);
186         template &lt;typename Functor&gt; inline IterationStatus forEachDeadCell(const Functor&amp;);
187         template &lt;typename Functor&gt; inline IterationStatus forEachMarkedCell(const Functor&amp;);
188 
189         JS_EXPORT_PRIVATE bool areMarksStale();
190         bool areMarksStaleForSweep();
191 
192         void assertMarksNotStale();
193 
194         bool isFreeListed() const { return m_isFreeListed; }
195 
196         size_t index() const { return m_index; }
197 
198         void removeFromDirectory();
199 
200         void didAddToDirectory(BlockDirectory*, size_t index);
201         void didRemoveFromDirectory();
202 
<a name="4" id="anc4"></a>



203         void dumpState(PrintStream&amp;);
204 
205     private:
206         Handle(Heap&amp;, AlignedMemoryAllocator*, void*);
207 
208         enum SweepDestructionMode { BlockHasNoDestructors, BlockHasDestructors, BlockHasDestructorsAndCollectorIsRunning };
209         enum ScribbleMode { DontScribble, Scribble };
210         enum EmptyMode { IsEmpty, NotEmpty };
211         enum NewlyAllocatedMode { HasNewlyAllocated, DoesNotHaveNewlyAllocated };
212         enum MarksMode { MarksStale, MarksNotStale };
213 
214         SweepDestructionMode sweepDestructionMode();
215         EmptyMode emptyMode();
216         ScribbleMode scribbleMode();
217         NewlyAllocatedMode newlyAllocatedMode();
218         MarksMode marksMode();
219 
220         template&lt;bool, EmptyMode, SweepMode, SweepDestructionMode, ScribbleMode, NewlyAllocatedMode, MarksMode, typename DestroyFunc&gt;
221         void specializedSweep(FreeList*, EmptyMode, SweepMode, SweepDestructionMode, ScribbleMode, NewlyAllocatedMode, MarksMode, const DestroyFunc&amp;);
222 
223         void setIsFreeListed();
224 
225         MarkedBlock::Handle* m_prev { nullptr };
226         MarkedBlock::Handle* m_next { nullptr };
227 
228         size_t m_atomsPerCell { std::numeric_limits&lt;size_t&gt;::max() };
229         size_t m_endAtom { std::numeric_limits&lt;size_t&gt;::max() }; // This is a fuzzy end. Always test for &lt; m_endAtom.
230 
231         CellAttributes m_attributes;
232         bool m_isFreeListed { false };
233 
234         AlignedMemoryAllocator* m_alignedMemoryAllocator { nullptr };
235         BlockDirectory* m_directory { nullptr };
236         size_t m_index { std::numeric_limits&lt;size_t&gt;::max() };
237         WeakSet m_weakSet;
238 
239         MarkedBlock* m_block { nullptr };
240     };
241 
242 private:
243     static constexpr size_t atomAlignmentMask = atomSize - 1;
244 
245     typedef char Atom[atomSize];
246 
247 public:
248     class Footer {
249     public:
250         Footer(VM&amp;, Handle&amp;);
251         ~Footer();
252 
253     private:
254         friend class LLIntOffsetsExtractor;
255         friend class MarkedBlock;
256 
257         Handle&amp; m_handle;
<a name="5" id="anc5"></a>

258         VM* m_vm;
259         Subspace* m_subspace;
260 
261         CountingLock m_lock;
262 
263         // The actual mark count can be computed by doing: m_biasedMarkCount - m_markCountBias. Note
264         // that this count is racy. It will accurately detect whether or not exactly zero things were
265         // marked, but if N things got marked, then this may report anything in the range [1, N] (or
266         // before unbiased, it would be [1 + m_markCountBias, N + m_markCountBias].)
267         int16_t m_biasedMarkCount;
268 
269         // We bias the mark count so that if m_biasedMarkCount &gt;= 0 then the block should be retired.
270         // We go to all this trouble to make marking a bit faster: this way, marking knows when to
271         // retire a block using a js/jns on m_biasedMarkCount.
272         //
273         // For example, if a block has room for 100 objects and retirement happens whenever 90% are
274         // live, then m_markCountBias will be -90. This way, when marking begins, this will cause us to
275         // set m_biasedMarkCount to -90 as well, since:
276         //
277         //     m_biasedMarkCount = actualMarkCount + m_markCountBias.
278         //
279         // Marking an object will increment m_biasedMarkCount. Once 90 objects get marked, we will have
280         // m_biasedMarkCount = 0, which will trigger retirement. In other words, we want to set
281         // m_markCountBias like so:
282         //
283         //     m_markCountBias = -(minMarkedBlockUtilization * cellsPerBlock)
284         //
285         // All of this also means that you can detect if any objects are marked by doing:
286         //
287         //     m_biasedMarkCount != m_markCountBias
288         int16_t m_markCountBias;
289 
290         HeapVersion m_markingVersion;
291         HeapVersion m_newlyAllocatedVersion;
292 
293         Bitmap&lt;atomsPerBlock&gt; m_marks;
294         Bitmap&lt;atomsPerBlock&gt; m_newlyAllocated;
295     };
296 
297 private:
298     Footer&amp; footer();
299     const Footer&amp; footer() const;
300 
301 public:
302     static constexpr size_t endAtom = (blockSize - sizeof(Footer)) / atomSize;
303     static constexpr size_t payloadSize = endAtom * atomSize;
304     static constexpr size_t footerSize = blockSize - payloadSize;
305 
306     static_assert(payloadSize == ((blockSize - sizeof(MarkedBlock::Footer)) &amp; ~(atomSize - 1)), &quot;Payload size computed the alternate way should give the same result&quot;);
<a name="6" id="anc6"></a><span class="line-modified">307     static_assert(footerSize &gt;= minimumDistanceBetweenCellsFromDifferentOrigins, &quot;Footer is not big enough to create the necessary distance between objects from different origins&quot;);</span>


308 
309     static MarkedBlock::Handle* tryCreate(Heap&amp;, AlignedMemoryAllocator*);
310 
311     Handle&amp; handle();
312     const Handle&amp; handle() const;
313 
<a name="7" id="anc7"></a><span class="line-modified">314     VM* vm() const;</span>
315     inline Heap* heap() const;
316     inline MarkedSpace* space() const;
317 
318     static bool isAtomAligned(const void*);
319     static MarkedBlock* blockFor(const void*);
320     size_t atomNumber(const void*);
321 
322     size_t markCount();
323 
324     bool isMarked(const void*);
325     bool isMarked(HeapVersion markingVersion, const void*);
326     bool isMarked(const void*, Dependency);
327     bool testAndSetMarked(const void*, Dependency);
328 
329     bool isAtom(const void*);
330     void clearMarked(const void*);
331 
332     bool isNewlyAllocated(const void*);
333     void setNewlyAllocated(const void*);
334     void clearNewlyAllocated(const void*);
335     const Bitmap&lt;atomsPerBlock&gt;&amp; newlyAllocated() const;
336 
337     HeapVersion newlyAllocatedVersion() const { return footer().m_newlyAllocatedVersion; }
338 
339     inline bool isNewlyAllocatedStale() const;
340 
341     inline bool hasAnyNewlyAllocated();
342     void resetAllocated();
343 
344     size_t cellSize();
345     const CellAttributes&amp; attributes() const;
346 
347     bool hasAnyMarked() const;
348     void noteMarked();
349 #if ASSERT_DISABLED
350     void assertValidCell(VM&amp;, HeapCell*) const { }
351 #else
352     void assertValidCell(VM&amp;, HeapCell*) const;
353 #endif
354 
355     WeakSet&amp; weakSet();
356 
357     JS_EXPORT_PRIVATE bool areMarksStale();
358     bool areMarksStale(HeapVersion markingVersion);
359 
360     Dependency aboutToMark(HeapVersion markingVersion);
361 
362 #if ASSERT_DISABLED
363     void assertMarksNotStale() { }
364 #else
365     JS_EXPORT_PRIVATE void assertMarksNotStale();
366 #endif
367 
368     void resetMarks();
369 
370     bool isMarkedRaw(const void* p);
371     HeapVersion markingVersion() const { return footer().m_markingVersion; }
372 
373     const Bitmap&lt;atomsPerBlock&gt;&amp; marks() const;
374 
375     CountingLock&amp; lock() { return footer().m_lock; }
376 
377     Subspace* subspace() const { return footer().m_subspace; }
378 
<a name="8" id="anc8"></a>




379     static constexpr size_t offsetOfFooter = endAtom * atomSize;
380 
381 private:
382     MarkedBlock(VM&amp;, Handle&amp;);
383     ~MarkedBlock();
384     Atom* atoms();
385 
386     JS_EXPORT_PRIVATE void aboutToMarkSlow(HeapVersion markingVersion);
387     void clearHasAnyMarked();
388 
389     void noteMarkedSlow();
390 
391     inline bool marksConveyLivenessDuringMarking(HeapVersion markingVersion);
392     inline bool marksConveyLivenessDuringMarking(HeapVersion myMarkingVersion, HeapVersion markingVersion);
393 };
394 
395 inline MarkedBlock::Footer&amp; MarkedBlock::footer()
396 {
397     return *bitwise_cast&lt;MarkedBlock::Footer*&gt;(atoms() + endAtom);
398 }
399 
400 inline const MarkedBlock::Footer&amp; MarkedBlock::footer() const
401 {
402     return const_cast&lt;MarkedBlock*&gt;(this)-&gt;footer();
403 }
404 
405 inline MarkedBlock::Handle&amp; MarkedBlock::handle()
406 {
407     return footer().m_handle;
408 }
409 
410 inline const MarkedBlock::Handle&amp; MarkedBlock::handle() const
411 {
412     return const_cast&lt;MarkedBlock*&gt;(this)-&gt;handle();
413 }
414 
415 inline MarkedBlock&amp; MarkedBlock::Handle::block()
416 {
417     return *m_block;
418 }
419 
420 inline MarkedBlock::Footer&amp; MarkedBlock::Handle::blockFooter()
421 {
422     return block().footer();
423 }
424 
425 inline MarkedBlock::Atom* MarkedBlock::atoms()
426 {
427     return reinterpret_cast&lt;Atom*&gt;(this);
428 }
429 
430 inline bool MarkedBlock::isAtomAligned(const void* p)
431 {
432     return !(reinterpret_cast&lt;uintptr_t&gt;(p) &amp; atomAlignmentMask);
433 }
434 
435 inline void* MarkedBlock::Handle::cellAlign(void* p)
436 {
437     uintptr_t base = reinterpret_cast&lt;uintptr_t&gt;(block().atoms());
438     uintptr_t bits = reinterpret_cast&lt;uintptr_t&gt;(p);
439     bits -= base;
440     bits -= bits % cellSize();
441     bits += base;
442     return reinterpret_cast&lt;void*&gt;(bits);
443 }
444 
445 inline MarkedBlock* MarkedBlock::blockFor(const void* p)
446 {
447     return reinterpret_cast&lt;MarkedBlock*&gt;(reinterpret_cast&lt;uintptr_t&gt;(p) &amp; blockMask);
448 }
449 
450 inline BlockDirectory* MarkedBlock::Handle::directory() const
451 {
452     return m_directory;
453 }
454 
455 inline AlignedMemoryAllocator* MarkedBlock::Handle::alignedMemoryAllocator() const
456 {
457     return m_alignedMemoryAllocator;
458 }
459 
460 inline Heap* MarkedBlock::Handle::heap() const
461 {
462     return m_weakSet.heap();
463 }
464 
<a name="9" id="anc9"></a><span class="line-modified">465 inline VM* MarkedBlock::Handle::vm() const</span>
466 {
467     return m_weakSet.vm();
468 }
469 
<a name="10" id="anc10"></a><span class="line-modified">470 inline VM* MarkedBlock::vm() const</span>
471 {
<a name="11" id="anc11"></a><span class="line-modified">472     return footer().m_vm;</span>
473 }
474 
475 inline WeakSet&amp; MarkedBlock::Handle::weakSet()
476 {
477     return m_weakSet;
478 }
479 
480 inline WeakSet&amp; MarkedBlock::weakSet()
481 {
482     return handle().weakSet();
483 }
484 
485 inline void MarkedBlock::Handle::shrink()
486 {
487     m_weakSet.shrink();
488 }
489 
490 inline void MarkedBlock::Handle::visitWeakSet(SlotVisitor&amp; visitor)
491 {
492     return m_weakSet.visit(visitor);
493 }
494 
495 inline void MarkedBlock::Handle::reapWeakSet()
496 {
497     m_weakSet.reap();
498 }
499 
500 inline size_t MarkedBlock::Handle::cellSize()
501 {
502     return m_atomsPerCell * atomSize;
503 }
504 
505 inline size_t MarkedBlock::cellSize()
506 {
507     return handle().cellSize();
508 }
509 
510 inline const CellAttributes&amp; MarkedBlock::Handle::attributes() const
511 {
512     return m_attributes;
513 }
514 
515 inline const CellAttributes&amp; MarkedBlock::attributes() const
516 {
517     return handle().attributes();
518 }
519 
520 inline bool MarkedBlock::Handle::needsDestruction() const
521 {
522     return m_attributes.destruction == NeedsDestruction;
523 }
524 
525 inline DestructionMode MarkedBlock::Handle::destruction() const
526 {
527     return m_attributes.destruction;
528 }
529 
530 inline HeapCell::Kind MarkedBlock::Handle::cellKind() const
531 {
532     return m_attributes.cellKind;
533 }
534 
535 inline size_t MarkedBlock::Handle::markCount()
536 {
537     return m_block-&gt;markCount();
538 }
539 
540 inline size_t MarkedBlock::Handle::size()
541 {
542     return markCount() * cellSize();
543 }
544 
545 inline size_t MarkedBlock::atomNumber(const void* p)
546 {
547     return (reinterpret_cast&lt;uintptr_t&gt;(p) - reinterpret_cast&lt;uintptr_t&gt;(this)) / atomSize;
548 }
549 
550 inline bool MarkedBlock::areMarksStale(HeapVersion markingVersion)
551 {
552     return markingVersion != footer().m_markingVersion;
553 }
554 
555 inline Dependency MarkedBlock::aboutToMark(HeapVersion markingVersion)
556 {
557     HeapVersion version = footer().m_markingVersion;
558     if (UNLIKELY(version != markingVersion))
559         aboutToMarkSlow(markingVersion);
560     return Dependency::fence(version);
561 }
562 
563 inline void MarkedBlock::Handle::assertMarksNotStale()
564 {
565     block().assertMarksNotStale();
566 }
567 
568 inline bool MarkedBlock::isMarkedRaw(const void* p)
569 {
570     return footer().m_marks.get(atomNumber(p));
571 }
572 
573 inline bool MarkedBlock::isMarked(HeapVersion markingVersion, const void* p)
574 {
575     HeapVersion version = footer().m_markingVersion;
576     if (UNLIKELY(version != markingVersion))
577         return false;
578     return footer().m_marks.get(atomNumber(p), Dependency::fence(version));
579 }
580 
581 inline bool MarkedBlock::isMarked(const void* p, Dependency dependency)
582 {
583     assertMarksNotStale();
584     return footer().m_marks.get(atomNumber(p), dependency);
585 }
586 
587 inline bool MarkedBlock::testAndSetMarked(const void* p, Dependency dependency)
588 {
589     assertMarksNotStale();
590     return footer().m_marks.concurrentTestAndSet(atomNumber(p), dependency);
591 }
592 
593 inline const Bitmap&lt;MarkedBlock::atomsPerBlock&gt;&amp; MarkedBlock::marks() const
594 {
595     return footer().m_marks;
596 }
597 
598 inline bool MarkedBlock::isNewlyAllocated(const void* p)
599 {
600     return footer().m_newlyAllocated.get(atomNumber(p));
601 }
602 
603 inline void MarkedBlock::setNewlyAllocated(const void* p)
604 {
605     footer().m_newlyAllocated.set(atomNumber(p));
606 }
607 
608 inline void MarkedBlock::clearNewlyAllocated(const void* p)
609 {
610     footer().m_newlyAllocated.clear(atomNumber(p));
611 }
612 
613 inline const Bitmap&lt;MarkedBlock::atomsPerBlock&gt;&amp; MarkedBlock::newlyAllocated() const
614 {
615     return footer().m_newlyAllocated;
616 }
617 
618 inline bool MarkedBlock::isAtom(const void* p)
619 {
620     ASSERT(MarkedBlock::isAtomAligned(p));
621     size_t atomNumber = this-&gt;atomNumber(p);
622     if (atomNumber % handle().m_atomsPerCell) // Filters pointers into cell middles.
623         return false;
624     if (atomNumber &gt;= handle().m_endAtom) // Filters pointers into invalid cells out of the range.
625         return false;
626     return true;
627 }
628 
629 template &lt;typename Functor&gt;
630 inline IterationStatus MarkedBlock::Handle::forEachCell(const Functor&amp; functor)
631 {
632     HeapCell::Kind kind = m_attributes.cellKind;
633     for (size_t i = 0; i &lt; m_endAtom; i += m_atomsPerCell) {
634         HeapCell* cell = reinterpret_cast_ptr&lt;HeapCell*&gt;(&amp;m_block-&gt;atoms()[i]);
635         if (functor(cell, kind) == IterationStatus::Done)
636             return IterationStatus::Done;
637     }
638     return IterationStatus::Continue;
639 }
640 
641 inline bool MarkedBlock::hasAnyMarked() const
642 {
643     return footer().m_biasedMarkCount != footer().m_markCountBias;
644 }
645 
646 inline void MarkedBlock::noteMarked()
647 {
648     // This is racy by design. We don&#39;t want to pay the price of an atomic increment!
649     int16_t biasedMarkCount = footer().m_biasedMarkCount;
650     ++biasedMarkCount;
651     footer().m_biasedMarkCount = biasedMarkCount;
652     if (UNLIKELY(!biasedMarkCount))
653         noteMarkedSlow();
654 }
655 
656 } // namespace JSC
657 
658 namespace WTF {
659 
660 struct MarkedBlockHash : PtrHash&lt;JSC::MarkedBlock*&gt; {
661     static unsigned hash(JSC::MarkedBlock* const&amp; key)
662     {
663         // Aligned VM regions tend to be monotonically increasing integers,
664         // which is a great hash function, but we have to remove the low bits,
665         // since they&#39;re always zero, which is a terrible hash function!
666         return reinterpret_cast&lt;uintptr_t&gt;(key) / JSC::MarkedBlock::blockSize;
667     }
668 };
669 
670 template&lt;&gt; struct DefaultHash&lt;JSC::MarkedBlock*&gt; {
671     typedef MarkedBlockHash Hash;
672 };
673 
674 void printInternal(PrintStream&amp; out, JSC::MarkedBlock::Handle::SweepMode);
675 
676 } // namespace WTF
<a name="12" id="anc12"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="12" type="hidden" />
</body>
</html>