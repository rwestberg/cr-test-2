<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITOpcodes.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (C) 2009-2019 Apple Inc. All rights reserved.
   3  * Copyright (C) 2010 Patrick Gansterer &lt;paroga@paroga.com&gt;
   4  *
   5  * Redistribution and use in source and binary forms, with or without
   6  * modification, are permitted provided that the following conditions
   7  * are met:
   8  * 1. Redistributions of source code must retain the above copyright
   9  *    notice, this list of conditions and the following disclaimer.
  10  * 2. Redistributions in binary form must reproduce the above copyright
  11  *    notice, this list of conditions and the following disclaimer in the
  12  *    documentation and/or other materials provided with the distribution.
  13  *
  14  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  15  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  16  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  17  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  18  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  19  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  20  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  21  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  22  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  23  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  24  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  25  */
  26 
  27 #include &quot;config.h&quot;
  28 #if ENABLE(JIT)
  29 #include &quot;JIT.h&quot;
  30 
  31 #include &quot;BasicBlockLocation.h&quot;
  32 #include &quot;BytecodeStructs.h&quot;
  33 #include &quot;Exception.h&quot;
  34 #include &quot;Heap.h&quot;
  35 #include &quot;InterpreterInlines.h&quot;
  36 #include &quot;JITInlines.h&quot;
  37 #include &quot;JSArray.h&quot;
  38 #include &quot;JSCast.h&quot;
  39 #include &quot;JSFunction.h&quot;
  40 #include &quot;JSPropertyNameEnumerator.h&quot;
  41 #include &quot;LinkBuffer.h&quot;
  42 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;
  43 #include &quot;OpcodeInlines.h&quot;
  44 #include &quot;SlowPathCall.h&quot;
  45 #include &quot;SuperSampler.h&quot;
  46 #include &quot;ThunkGenerators.h&quot;
  47 #include &quot;TypeLocation.h&quot;
  48 #include &quot;TypeProfilerLog.h&quot;
  49 #include &quot;VirtualRegister.h&quot;
  50 #include &quot;Watchdog.h&quot;
  51 
  52 namespace JSC {
  53 
  54 #if USE(JSVALUE64)
  55 
  56 void JIT::emit_op_mov(const Instruction* currentInstruction)
  57 {
  58     auto bytecode = currentInstruction-&gt;as&lt;OpMov&gt;();
  59     int dst = bytecode.m_dst.offset();
  60     int src = bytecode.m_src.offset();
  61 
  62     if (m_codeBlock-&gt;isConstantRegisterIndex(src)) {
  63         JSValue value = m_codeBlock-&gt;getConstant(src);
  64         if (!value.isNumber())
  65             store64(TrustedImm64(JSValue::encode(value)), addressFor(dst));
  66         else
  67             store64(Imm64(JSValue::encode(value)), addressFor(dst));
  68         return;
  69     }
  70 
  71     load64(addressFor(src), regT0);
  72     store64(regT0, addressFor(dst));
  73 }
  74 
  75 
  76 void JIT::emit_op_end(const Instruction* currentInstruction)
  77 {
  78     auto bytecode = currentInstruction-&gt;as&lt;OpEnd&gt;();
  79     RELEASE_ASSERT(returnValueGPR != callFrameRegister);
  80     emitGetVirtualRegister(bytecode.m_value.offset(), returnValueGPR);
  81     emitRestoreCalleeSaves();
  82     emitFunctionEpilogue();
  83     ret();
  84 }
  85 
  86 void JIT::emit_op_jmp(const Instruction* currentInstruction)
  87 {
  88     auto bytecode = currentInstruction-&gt;as&lt;OpJmp&gt;();
  89     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
  90     addJump(jump(), target);
  91 }
  92 
  93 void JIT::emit_op_new_object(const Instruction* currentInstruction)
  94 {
  95     auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
  96     auto&amp; metadata = bytecode.metadata(m_codeBlock);
  97     Structure* structure = metadata.m_objectAllocationProfile.structure();
  98     size_t allocationSize = JSFinalObject::allocationSize(structure-&gt;inlineCapacity());
  99     Allocator allocator = allocatorForNonVirtualConcurrently&lt;JSFinalObject&gt;(*m_vm, allocationSize, AllocatorForMode::AllocatorIfExists);
 100 
 101     RegisterID resultReg = regT0;
 102     RegisterID allocatorReg = regT1;
 103     RegisterID scratchReg = regT2;
 104 
 105     if (!allocator)
 106         addSlowCase(jump());
 107     else {
 108         JumpList slowCases;
 109         auto butterfly = TrustedImmPtr(nullptr);
 110         emitAllocateJSObject(resultReg, JITAllocator::constant(allocator), allocatorReg, TrustedImmPtr(structure), butterfly, scratchReg, slowCases);
 111         emitInitializeInlineStorage(resultReg, structure-&gt;inlineCapacity());
 112         addSlowCase(slowCases);
 113         emitPutVirtualRegister(bytecode.m_dst.offset());
 114     }
 115 }
 116 
 117 void JIT::emitSlow_op_new_object(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 118 {
 119     linkAllSlowCases(iter);
 120 
 121     auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
 122     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 123     int dst = bytecode.m_dst.offset();
 124     Structure* structure = metadata.m_objectAllocationProfile.structure();
 125     callOperation(operationNewObject, structure);
 126     emitStoreCell(dst, returnValueGPR);
 127 }
 128 
 129 void JIT::emit_op_overrides_has_instance(const Instruction* currentInstruction)
 130 {
 131     auto bytecode = currentInstruction-&gt;as&lt;OpOverridesHasInstance&gt;();
 132     int dst = bytecode.m_dst.offset();
 133     int constructor = bytecode.m_constructor.offset();
 134     int hasInstanceValue = bytecode.m_hasInstanceValue.offset();
 135 
 136     emitGetVirtualRegister(hasInstanceValue, regT0);
 137 
 138     // We don&#39;t jump if we know what Symbol.hasInstance would do.
 139     Jump customhasInstanceValue = branchPtr(NotEqual, regT0, TrustedImmPtr(m_codeBlock-&gt;globalObject()-&gt;functionProtoHasInstanceSymbolFunction()));
 140 
 141     emitGetVirtualRegister(constructor, regT0);
 142 
 143     // Check that constructor &#39;ImplementsDefaultHasInstance&#39; i.e. the object is not a C-API user nor a bound function.
 144     test8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(ImplementsDefaultHasInstance), regT0);
 145     boxBoolean(regT0, JSValueRegs { regT0 });
 146     Jump done = jump();
 147 
 148     customhasInstanceValue.link(this);
 149     move(TrustedImm32(ValueTrue), regT0);
 150 
 151     done.link(this);
 152     emitPutVirtualRegister(dst);
 153 }
 154 
 155 void JIT::emit_op_instanceof(const Instruction* currentInstruction)
 156 {
 157     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceof&gt;();
 158     int dst = bytecode.m_dst.offset();
 159     int value = bytecode.m_value.offset();
 160     int proto = bytecode.m_prototype.offset();
 161 
 162     // Load the operands (baseVal, proto, and value respectively) into registers.
 163     // We use regT0 for baseVal since we will be done with this first, and we can then use it for the result.
 164     emitGetVirtualRegister(value, regT2);
 165     emitGetVirtualRegister(proto, regT1);
 166 
 167     // Check that proto are cells. baseVal must be a cell - this is checked by the get_by_id for Symbol.hasInstance.
 168     emitJumpSlowCaseIfNotJSCell(regT2, value);
 169     emitJumpSlowCaseIfNotJSCell(regT1, proto);
 170 
 171     JITInstanceOfGenerator gen(
 172         m_codeBlock, CodeOrigin(m_bytecodeOffset), CallSiteIndex(m_bytecodeOffset),
 173         RegisterSet::stubUnavailableRegisters(),
 174         regT0, // result
 175         regT2, // value
 176         regT1, // proto
 177         regT3, regT4); // scratch
 178     gen.generateFastPath(*this);
 179     m_instanceOfs.append(gen);
 180 
 181     emitPutVirtualRegister(dst);
 182 }
 183 
 184 void JIT::emitSlow_op_instanceof(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 185 {
 186     linkAllSlowCases(iter);
 187 
 188     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceof&gt;();
 189     int resultVReg = bytecode.m_dst.offset();
 190 
 191     JITInstanceOfGenerator&amp; gen = m_instanceOfs[m_instanceOfIndex++];
 192 
 193     Label coldPathBegin = label();
 194     Call call = callOperation(operationInstanceOfOptimize, resultVReg, gen.stubInfo(), regT2, regT1);
 195     gen.reportSlowPathCall(coldPathBegin, call);
 196 }
 197 
 198 void JIT::emit_op_instanceof_custom(const Instruction*)
 199 {
 200     // This always goes to slow path since we expect it to be rare.
 201     addSlowCase(jump());
 202 }
 203 
 204 void JIT::emit_op_is_empty(const Instruction* currentInstruction)
 205 {
 206     auto bytecode = currentInstruction-&gt;as&lt;OpIsEmpty&gt;();
 207     int dst = bytecode.m_dst.offset();
 208     int value = bytecode.m_operand.offset();
 209 
 210     emitGetVirtualRegister(value, regT0);
 211     compare64(Equal, regT0, TrustedImm32(JSValue::encode(JSValue())), regT0);
 212 
 213     boxBoolean(regT0, JSValueRegs { regT0 });
 214     emitPutVirtualRegister(dst);
 215 }
 216 
 217 void JIT::emit_op_is_undefined(const Instruction* currentInstruction)
 218 {
 219     auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefined&gt;();
 220     int dst = bytecode.m_dst.offset();
 221     int value = bytecode.m_operand.offset();
 222 
 223     emitGetVirtualRegister(value, regT0);
 224     Jump isCell = branchIfCell(regT0);
 225 
 226     compare64(Equal, regT0, TrustedImm32(ValueUndefined), regT0);
 227     Jump done = jump();
 228 
 229     isCell.link(this);
 230     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 231     move(TrustedImm32(0), regT0);
 232     Jump notMasqueradesAsUndefined = jump();
 233 
 234     isMasqueradesAsUndefined.link(this);
<a name="1" id="anc1"></a><span class="line-modified"> 235     emitLoadStructure(vm(), regT0, regT1, regT2);</span>
 236     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 237     loadPtr(Address(regT1, Structure::globalObjectOffset()), regT1);
 238     comparePtr(Equal, regT0, regT1, regT0);
 239 
 240     notMasqueradesAsUndefined.link(this);
 241     done.link(this);
 242     boxBoolean(regT0, JSValueRegs { regT0 });
 243     emitPutVirtualRegister(dst);
 244 }
 245 
 246 void JIT::emit_op_is_undefined_or_null(const Instruction* currentInstruction)
 247 {
 248     auto bytecode = currentInstruction-&gt;as&lt;OpIsUndefinedOrNull&gt;();
 249     int dst = bytecode.m_dst.offset();
 250     int value = bytecode.m_operand.offset();
 251 
 252     emitGetVirtualRegister(value, regT0);
 253 
 254     and64(TrustedImm32(~TagBitUndefined), regT0);
 255     compare64(Equal, regT0, TrustedImm32(ValueNull), regT0);
 256 
 257     boxBoolean(regT0, JSValueRegs { regT0 });
 258     emitPutVirtualRegister(dst);
 259 }
 260 
 261 void JIT::emit_op_is_boolean(const Instruction* currentInstruction)
 262 {
 263     auto bytecode = currentInstruction-&gt;as&lt;OpIsBoolean&gt;();
 264     int dst = bytecode.m_dst.offset();
 265     int value = bytecode.m_operand.offset();
 266 
 267     emitGetVirtualRegister(value, regT0);
 268     xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueFalse)), regT0);
 269     test64(Zero, regT0, TrustedImm32(static_cast&lt;int32_t&gt;(~1)), regT0);
 270     boxBoolean(regT0, JSValueRegs { regT0 });
 271     emitPutVirtualRegister(dst);
 272 }
 273 
 274 void JIT::emit_op_is_number(const Instruction* currentInstruction)
 275 {
 276     auto bytecode = currentInstruction-&gt;as&lt;OpIsNumber&gt;();
 277     int dst = bytecode.m_dst.offset();
 278     int value = bytecode.m_operand.offset();
 279 
 280     emitGetVirtualRegister(value, regT0);
 281     test64(NonZero, regT0, tagTypeNumberRegister, regT0);
 282     boxBoolean(regT0, JSValueRegs { regT0 });
 283     emitPutVirtualRegister(dst);
 284 }
 285 
 286 void JIT::emit_op_is_cell_with_type(const Instruction* currentInstruction)
 287 {
 288     auto bytecode = currentInstruction-&gt;as&lt;OpIsCellWithType&gt;();
 289     int dst = bytecode.m_dst.offset();
 290     int value = bytecode.m_operand.offset();
 291     int type = bytecode.m_type;
 292 
 293     emitGetVirtualRegister(value, regT0);
 294     Jump isNotCell = branchIfNotCell(regT0);
 295 
 296     compare8(Equal, Address(regT0, JSCell::typeInfoTypeOffset()), TrustedImm32(type), regT0);
 297     boxBoolean(regT0, JSValueRegs { regT0 });
 298     Jump done = jump();
 299 
 300     isNotCell.link(this);
 301     move(TrustedImm32(ValueFalse), regT0);
 302 
 303     done.link(this);
 304     emitPutVirtualRegister(dst);
 305 }
 306 
 307 void JIT::emit_op_is_object(const Instruction* currentInstruction)
 308 {
 309     auto bytecode = currentInstruction-&gt;as&lt;OpIsObject&gt;();
 310     int dst = bytecode.m_dst.offset();
 311     int value = bytecode.m_operand.offset();
 312 
 313     emitGetVirtualRegister(value, regT0);
 314     Jump isNotCell = branchIfNotCell(regT0);
 315 
 316     compare8(AboveOrEqual, Address(regT0, JSCell::typeInfoTypeOffset()), TrustedImm32(ObjectType), regT0);
 317     boxBoolean(regT0, JSValueRegs { regT0 });
 318     Jump done = jump();
 319 
 320     isNotCell.link(this);
 321     move(TrustedImm32(ValueFalse), regT0);
 322 
 323     done.link(this);
 324     emitPutVirtualRegister(dst);
 325 }
 326 
 327 void JIT::emit_op_ret(const Instruction* currentInstruction)
 328 {
 329     ASSERT(callFrameRegister != regT1);
 330     ASSERT(regT1 != returnValueGPR);
 331     ASSERT(returnValueGPR != callFrameRegister);
 332 
 333     // Return the result in %eax.
 334     auto bytecode = currentInstruction-&gt;as&lt;OpRet&gt;();
 335     emitGetVirtualRegister(bytecode.m_value.offset(), returnValueGPR);
 336 
 337     checkStackPointerAlignment();
 338     emitRestoreCalleeSaves();
 339     emitFunctionEpilogue();
 340     ret();
 341 }
 342 
 343 void JIT::emit_op_to_primitive(const Instruction* currentInstruction)
 344 {
 345     auto bytecode = currentInstruction-&gt;as&lt;OpToPrimitive&gt;();
 346     int dst = bytecode.m_dst.offset();
 347     int src = bytecode.m_src.offset();
 348 
 349     emitGetVirtualRegister(src, regT0);
 350 
 351     Jump isImm = branchIfNotCell(regT0);
 352     addSlowCase(branchIfObject(regT0));
 353     isImm.link(this);
 354 
 355     if (dst != src)
 356         emitPutVirtualRegister(dst);
 357 
 358 }
 359 
 360 void JIT::emit_op_set_function_name(const Instruction* currentInstruction)
 361 {
 362     auto bytecode = currentInstruction-&gt;as&lt;OpSetFunctionName&gt;();
 363     emitGetVirtualRegister(bytecode.m_function.offset(), regT0);
 364     emitGetVirtualRegister(bytecode.m_name.offset(), regT1);
 365     callOperation(operationSetFunctionName, regT0, regT1);
 366 }
 367 
 368 void JIT::emit_op_not(const Instruction* currentInstruction)
 369 {
 370     auto bytecode = currentInstruction-&gt;as&lt;OpNot&gt;();
 371     emitGetVirtualRegister(bytecode.m_operand.offset(), regT0);
 372 
 373     // Invert against JSValue(false); if the value was tagged as a boolean, then all bits will be
 374     // clear other than the low bit (which will be 0 or 1 for false or true inputs respectively).
 375     // Then invert against JSValue(true), which will add the tag back in, and flip the low bit.
 376     xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueFalse)), regT0);
 377     addSlowCase(branchTestPtr(NonZero, regT0, TrustedImm32(static_cast&lt;int32_t&gt;(~1))));
 378     xor64(TrustedImm32(static_cast&lt;int32_t&gt;(ValueTrue)), regT0);
 379 
 380     emitPutVirtualRegister(bytecode.m_dst.offset());
 381 }
 382 
 383 void JIT::emit_op_jfalse(const Instruction* currentInstruction)
 384 {
 385     auto bytecode = currentInstruction-&gt;as&lt;OpJfalse&gt;();
 386     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 387 
 388     GPRReg value = regT0;
 389     GPRReg scratch1 = regT1;
 390     GPRReg scratch2 = regT2;
 391     bool shouldCheckMasqueradesAsUndefined = true;
 392 
 393     emitGetVirtualRegister(bytecode.m_condition.offset(), value);
<a name="2" id="anc2"></a><span class="line-modified"> 394     addJump(branchIfFalsey(vm(), JSValueRegs(value), scratch1, scratch2, fpRegT0, fpRegT1, shouldCheckMasqueradesAsUndefined, m_codeBlock-&gt;globalObject()), target);</span>
 395 }
 396 
 397 void JIT::emit_op_jeq_null(const Instruction* currentInstruction)
 398 {
 399     auto bytecode = currentInstruction-&gt;as&lt;OpJeqNull&gt;();
 400     int src = bytecode.m_value.offset();
 401     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 402 
 403     emitGetVirtualRegister(src, regT0);
 404     Jump isImmediate = branchIfNotCell(regT0);
 405 
 406     // First, handle JSCell cases - check MasqueradesAsUndefined bit on the structure.
 407     Jump isNotMasqueradesAsUndefined = branchTest8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
<a name="3" id="anc3"></a><span class="line-modified"> 408     emitLoadStructure(vm(), regT0, regT2, regT1);</span>
 409     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 410     addJump(branchPtr(Equal, Address(regT2, Structure::globalObjectOffset()), regT0), target);
 411     Jump masqueradesGlobalObjectIsForeign = jump();
 412 
 413     // Now handle the immediate cases - undefined &amp; null
 414     isImmediate.link(this);
 415     and64(TrustedImm32(~TagBitUndefined), regT0);
 416     addJump(branch64(Equal, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 417 
 418     isNotMasqueradesAsUndefined.link(this);
 419     masqueradesGlobalObjectIsForeign.link(this);
 420 };
 421 void JIT::emit_op_jneq_null(const Instruction* currentInstruction)
 422 {
 423     auto bytecode = currentInstruction-&gt;as&lt;OpJneqNull&gt;();
 424     int src = bytecode.m_value.offset();
 425     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 426 
 427     emitGetVirtualRegister(src, regT0);
 428     Jump isImmediate = branchIfNotCell(regT0);
 429 
 430     // First, handle JSCell cases - check MasqueradesAsUndefined bit on the structure.
 431     addJump(branchTest8(Zero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined)), target);
<a name="4" id="anc4"></a><span class="line-modified"> 432     emitLoadStructure(vm(), regT0, regT2, regT1);</span>
 433     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 434     addJump(branchPtr(NotEqual, Address(regT2, Structure::globalObjectOffset()), regT0), target);
 435     Jump wasNotImmediate = jump();
 436 
 437     // Now handle the immediate cases - undefined &amp; null
 438     isImmediate.link(this);
 439     and64(TrustedImm32(~TagBitUndefined), regT0);
 440     addJump(branch64(NotEqual, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);
 441 
 442     wasNotImmediate.link(this);
 443 }
 444 
<a name="5" id="anc5"></a><span class="line-added"> 445 void JIT::emit_op_jundefined_or_null(const Instruction* currentInstruction)</span>
<span class="line-added"> 446 {</span>
<span class="line-added"> 447     auto bytecode = currentInstruction-&gt;as&lt;OpJundefinedOrNull&gt;();</span>
<span class="line-added"> 448     int value = bytecode.m_value.offset();</span>
<span class="line-added"> 449     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);</span>
<span class="line-added"> 450 </span>
<span class="line-added"> 451     emitGetVirtualRegister(value, regT0);</span>
<span class="line-added"> 452 </span>
<span class="line-added"> 453     and64(TrustedImm32(~TagBitUndefined), regT0);</span>
<span class="line-added"> 454     addJump(branch64(Equal, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);</span>
<span class="line-added"> 455 }</span>
<span class="line-added"> 456 </span>
<span class="line-added"> 457 void JIT::emit_op_jnundefined_or_null(const Instruction* currentInstruction)</span>
<span class="line-added"> 458 {</span>
<span class="line-added"> 459     auto bytecode = currentInstruction-&gt;as&lt;OpJnundefinedOrNull&gt;();</span>
<span class="line-added"> 460     int value = bytecode.m_value.offset();</span>
<span class="line-added"> 461     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);</span>
<span class="line-added"> 462 </span>
<span class="line-added"> 463     emitGetVirtualRegister(value, regT0);</span>
<span class="line-added"> 464 </span>
<span class="line-added"> 465     and64(TrustedImm32(~TagBitUndefined), regT0);</span>
<span class="line-added"> 466     addJump(branch64(NotEqual, regT0, TrustedImm64(JSValue::encode(jsNull()))), target);</span>
<span class="line-added"> 467 }</span>
<span class="line-added"> 468 </span>
 469 void JIT::emit_op_jneq_ptr(const Instruction* currentInstruction)
 470 {
 471     auto bytecode = currentInstruction-&gt;as&lt;OpJneqPtr&gt;();
 472     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 473     int src = bytecode.m_value.offset();
 474     Special::Pointer ptr = bytecode.m_specialPointer;
 475     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 476 
 477     emitGetVirtualRegister(src, regT0);
 478     CCallHelpers::Jump equal = branchPtr(Equal, regT0, TrustedImmPtr(actualPointerFor(m_codeBlock, ptr)));
 479     store8(TrustedImm32(1), &amp;metadata.m_hasJumped);
 480     addJump(jump(), target);
 481     equal.link(this);
 482 }
 483 
 484 void JIT::emit_op_eq(const Instruction* currentInstruction)
 485 {
 486     auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
 487     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 488     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 489     compare32(Equal, regT1, regT0, regT0);
 490     boxBoolean(regT0, JSValueRegs { regT0 });
 491     emitPutVirtualRegister(bytecode.m_dst.offset());
 492 }
 493 
 494 void JIT::emit_op_jeq(const Instruction* currentInstruction)
 495 {
 496     auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
 497     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 498     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 499     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 500     addJump(branch32(Equal, regT0, regT1), target);
 501 }
 502 
 503 void JIT::emit_op_jtrue(const Instruction* currentInstruction)
 504 {
 505     auto bytecode = currentInstruction-&gt;as&lt;OpJtrue&gt;();
 506     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 507 
 508     GPRReg value = regT0;
 509     GPRReg scratch1 = regT1;
 510     GPRReg scratch2 = regT2;
 511     bool shouldCheckMasqueradesAsUndefined = true;
 512     emitGetVirtualRegister(bytecode.m_condition.offset(), value);
<a name="6" id="anc6"></a><span class="line-modified"> 513     addJump(branchIfTruthy(vm(), JSValueRegs(value), scratch1, scratch2, fpRegT0, fpRegT1, shouldCheckMasqueradesAsUndefined, m_codeBlock-&gt;globalObject()), target);</span>
 514 }
 515 
 516 void JIT::emit_op_neq(const Instruction* currentInstruction)
 517 {
 518     auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
 519     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 520     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 521     compare32(NotEqual, regT1, regT0, regT0);
 522     boxBoolean(regT0, JSValueRegs { regT0 });
 523 
 524     emitPutVirtualRegister(bytecode.m_dst.offset());
 525 }
 526 
 527 void JIT::emit_op_jneq(const Instruction* currentInstruction)
 528 {
 529     auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
 530     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 531     emitGetVirtualRegisters(bytecode.m_lhs.offset(), regT0, bytecode.m_rhs.offset(), regT1);
 532     emitJumpSlowCaseIfNotInt(regT0, regT1, regT2);
 533     addJump(branch32(NotEqual, regT0, regT1), target);
 534 }
 535 
 536 void JIT::emit_op_throw(const Instruction* currentInstruction)
 537 {
 538     auto bytecode = currentInstruction-&gt;as&lt;OpThrow&gt;();
 539     ASSERT(regT0 == returnValueGPR);
<a name="7" id="anc7"></a><span class="line-modified"> 540     copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
 541     emitGetVirtualRegister(bytecode.m_value.offset(), regT0);
 542     callOperationNoExceptionCheck(operationThrow, regT0);
<a name="8" id="anc8"></a><span class="line-modified"> 543     jumpToExceptionHandler(vm());</span>
 544 }
 545 
 546 template&lt;typename Op&gt;
 547 void JIT::compileOpStrictEq(const Instruction* currentInstruction, CompileOpStrictEqType type)
 548 {
 549     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 550     int dst = bytecode.m_dst.offset();
 551     int src1 = bytecode.m_lhs.offset();
 552     int src2 = bytecode.m_rhs.offset();
 553 
 554     emitGetVirtualRegisters(src1, regT0, src2, regT1);
 555 
 556     // Jump slow if both are cells (to cover strings).
 557     move(regT0, regT2);
 558     or64(regT1, regT2);
 559     addSlowCase(branchIfCell(regT2));
 560 
 561     // Jump slow if either is a double. First test if it&#39;s an integer, which is fine, and then test
 562     // if it&#39;s a double.
 563     Jump leftOK = branchIfInt32(regT0);
 564     addSlowCase(branchIfNumber(regT0));
 565     leftOK.link(this);
 566     Jump rightOK = branchIfInt32(regT1);
 567     addSlowCase(branchIfNumber(regT1));
 568     rightOK.link(this);
 569 
 570     if (type == CompileOpStrictEqType::StrictEq)
 571         compare64(Equal, regT1, regT0, regT0);
 572     else
 573         compare64(NotEqual, regT1, regT0, regT0);
 574     boxBoolean(regT0, JSValueRegs { regT0 });
 575 
 576     emitPutVirtualRegister(dst);
 577 }
 578 
 579 void JIT::emit_op_stricteq(const Instruction* currentInstruction)
 580 {
 581     compileOpStrictEq&lt;OpStricteq&gt;(currentInstruction, CompileOpStrictEqType::StrictEq);
 582 }
 583 
 584 void JIT::emit_op_nstricteq(const Instruction* currentInstruction)
 585 {
 586     compileOpStrictEq&lt;OpNstricteq&gt;(currentInstruction, CompileOpStrictEqType::NStrictEq);
 587 }
 588 
 589 template&lt;typename Op&gt;
 590 void JIT::compileOpStrictEqJump(const Instruction* currentInstruction, CompileOpStrictEqType type)
 591 {
 592     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 593     int target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 594     int src1 = bytecode.m_lhs.offset();
 595     int src2 = bytecode.m_rhs.offset();
 596 
 597     emitGetVirtualRegisters(src1, regT0, src2, regT1);
 598 
 599     // Jump slow if both are cells (to cover strings).
 600     move(regT0, regT2);
 601     or64(regT1, regT2);
 602     addSlowCase(branchIfCell(regT2));
 603 
 604     // Jump slow if either is a double. First test if it&#39;s an integer, which is fine, and then test
 605     // if it&#39;s a double.
 606     Jump leftOK = branchIfInt32(regT0);
 607     addSlowCase(branchIfNumber(regT0));
 608     leftOK.link(this);
 609     Jump rightOK = branchIfInt32(regT1);
 610     addSlowCase(branchIfNumber(regT1));
 611     rightOK.link(this);
 612 
 613     if (type == CompileOpStrictEqType::StrictEq)
 614         addJump(branch64(Equal, regT1, regT0), target);
 615     else
 616         addJump(branch64(NotEqual, regT1, regT0), target);
 617 }
 618 
 619 void JIT::emit_op_jstricteq(const Instruction* currentInstruction)
 620 {
 621     compileOpStrictEqJump&lt;OpJstricteq&gt;(currentInstruction, CompileOpStrictEqType::StrictEq);
 622 }
 623 
 624 void JIT::emit_op_jnstricteq(const Instruction* currentInstruction)
 625 {
 626     compileOpStrictEqJump&lt;OpJnstricteq&gt;(currentInstruction, CompileOpStrictEqType::NStrictEq);
 627 }
 628 
 629 void JIT::emitSlow_op_jstricteq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 630 {
 631     linkAllSlowCases(iter);
 632 
 633     auto bytecode = currentInstruction-&gt;as&lt;OpJstricteq&gt;();
 634     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 635     callOperation(operationCompareStrictEq, regT0, regT1);
 636     emitJumpSlowToHot(branchTest32(NonZero, returnValueGPR), target);
 637 }
 638 
 639 void JIT::emitSlow_op_jnstricteq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 640 {
 641     linkAllSlowCases(iter);
 642 
 643     auto bytecode = currentInstruction-&gt;as&lt;OpJnstricteq&gt;();
 644     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 645     callOperation(operationCompareStrictEq, regT0, regT1);
 646     emitJumpSlowToHot(branchTest32(Zero, returnValueGPR), target);
 647 }
 648 
 649 void JIT::emit_op_to_number(const Instruction* currentInstruction)
 650 {
 651     auto bytecode = currentInstruction-&gt;as&lt;OpToNumber&gt;();
 652     int dstVReg = bytecode.m_dst.offset();
 653     int srcVReg = bytecode.m_operand.offset();
 654     emitGetVirtualRegister(srcVReg, regT0);
 655 
 656     addSlowCase(branchIfNotNumber(regT0));
 657 
 658     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 659     if (srcVReg != dstVReg)
 660         emitPutVirtualRegister(dstVReg);
 661 }
 662 
 663 void JIT::emit_op_to_string(const Instruction* currentInstruction)
 664 {
 665     auto bytecode = currentInstruction-&gt;as&lt;OpToString&gt;();
 666     int srcVReg = bytecode.m_operand.offset();
 667     emitGetVirtualRegister(srcVReg, regT0);
 668 
 669     addSlowCase(branchIfNotCell(regT0));
 670     addSlowCase(branchIfNotString(regT0));
 671 
 672     emitPutVirtualRegister(bytecode.m_dst.offset());
 673 }
 674 
 675 void JIT::emit_op_to_object(const Instruction* currentInstruction)
 676 {
 677     auto bytecode = currentInstruction-&gt;as&lt;OpToObject&gt;();
 678     int dstVReg = bytecode.m_dst.offset();
 679     int srcVReg = bytecode.m_operand.offset();
 680     emitGetVirtualRegister(srcVReg, regT0);
 681 
 682     addSlowCase(branchIfNotCell(regT0));
 683     addSlowCase(branchIfNotObject(regT0));
 684 
 685     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 686     if (srcVReg != dstVReg)
 687         emitPutVirtualRegister(dstVReg);
 688 }
 689 
 690 void JIT::emit_op_catch(const Instruction* currentInstruction)
 691 {
 692     auto bytecode = currentInstruction-&gt;as&lt;OpCatch&gt;();
 693 
<a name="9" id="anc9"></a><span class="line-modified"> 694     restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
 695 
 696     move(TrustedImmPtr(m_vm), regT3);
 697     load64(Address(regT3, VM::callFrameForCatchOffset()), callFrameRegister);
 698     storePtr(TrustedImmPtr(nullptr), Address(regT3, VM::callFrameForCatchOffset()));
 699 
 700     addPtr(TrustedImm32(stackPointerOffsetFor(codeBlock()) * sizeof(Register)), callFrameRegister, stackPointerRegister);
 701 
 702     callOperationNoExceptionCheck(operationCheckIfExceptionIsUncatchableAndNotifyProfiler);
 703     Jump isCatchableException = branchTest32(Zero, returnValueGPR);
<a name="10" id="anc10"></a><span class="line-modified"> 704     jumpToExceptionHandler(vm());</span>
 705     isCatchableException.link(this);
 706 
 707     move(TrustedImmPtr(m_vm), regT3);
 708     load64(Address(regT3, VM::exceptionOffset()), regT0);
 709     store64(TrustedImm64(JSValue::encode(JSValue())), Address(regT3, VM::exceptionOffset()));
 710     emitPutVirtualRegister(bytecode.m_exception.offset());
 711 
 712     load64(Address(regT0, Exception::valueOffset()), regT0);
 713     emitPutVirtualRegister(bytecode.m_thrownValue.offset());
 714 
 715 #if ENABLE(DFG_JIT)
 716     // FIXME: consider inline caching the process of doing OSR entry, including
 717     // argument type proofs, storing locals to the buffer, etc
 718     // https://bugs.webkit.org/show_bug.cgi?id=175598
 719 
 720     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 721     ValueProfileAndOperandBuffer* buffer = metadata.m_buffer;
 722     if (buffer || !shouldEmitProfiling())
 723         callOperation(operationTryOSREnterAtCatch, m_bytecodeOffset);
 724     else
 725         callOperation(operationTryOSREnterAtCatchAndValueProfile, m_bytecodeOffset);
 726     auto skipOSREntry = branchTestPtr(Zero, returnValueGPR);
 727     emitRestoreCalleeSaves();
<a name="11" id="anc11"></a><span class="line-modified"> 728     farJump(returnValueGPR, ExceptionHandlerPtrTag);</span>
 729     skipOSREntry.link(this);
 730     if (buffer &amp;&amp; shouldEmitProfiling()) {
 731         buffer-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {
 732             JSValueRegs regs(regT0);
 733             emitGetVirtualRegister(profile.m_operand, regs);
<a name="12" id="anc12"></a><span class="line-modified"> 734             emitValueProfilingSite(static_cast&lt;ValueProfile&amp;&gt;(profile));</span>
 735         });
 736     }
 737 #endif // ENABLE(DFG_JIT)
 738 }
 739 
 740 void JIT::emit_op_identity_with_profile(const Instruction*)
 741 {
 742     // We don&#39;t need to do anything here...
 743 }
 744 
 745 void JIT::emit_op_get_parent_scope(const Instruction* currentInstruction)
 746 {
 747     auto bytecode = currentInstruction-&gt;as&lt;OpGetParentScope&gt;();
 748     int currentScope = bytecode.m_scope.offset();
 749     emitGetVirtualRegister(currentScope, regT0);
 750     loadPtr(Address(regT0, JSScope::offsetOfNext()), regT0);
 751     emitStoreCell(bytecode.m_dst.offset(), regT0);
 752 }
 753 
 754 void JIT::emit_op_switch_imm(const Instruction* currentInstruction)
 755 {
 756     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchImm&gt;();
 757     size_t tableIndex = bytecode.m_tableIndex;
 758     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 759     unsigned scrutinee = bytecode.m_scrutinee.offset();
 760 
 761     // create jump table for switch destinations, track this switch statement.
 762     SimpleJumpTable* jumpTable = &amp;m_codeBlock-&gt;switchJumpTable(tableIndex);
 763     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset, SwitchRecord::Immediate));
 764     jumpTable-&gt;ensureCTITable();
 765 
 766     emitGetVirtualRegister(scrutinee, regT0);
 767     callOperation(operationSwitchImmWithUnknownKeyType, regT0, tableIndex);
<a name="13" id="anc13"></a><span class="line-modified"> 768     farJump(returnValueGPR, JSSwitchPtrTag);</span>
 769 }
 770 
 771 void JIT::emit_op_switch_char(const Instruction* currentInstruction)
 772 {
 773     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchChar&gt;();
 774     size_t tableIndex = bytecode.m_tableIndex;
 775     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 776     unsigned scrutinee = bytecode.m_scrutinee.offset();
 777 
 778     // create jump table for switch destinations, track this switch statement.
 779     SimpleJumpTable* jumpTable = &amp;m_codeBlock-&gt;switchJumpTable(tableIndex);
 780     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset, SwitchRecord::Character));
 781     jumpTable-&gt;ensureCTITable();
 782 
 783     emitGetVirtualRegister(scrutinee, regT0);
 784     callOperation(operationSwitchCharWithUnknownKeyType, regT0, tableIndex);
<a name="14" id="anc14"></a><span class="line-modified"> 785     farJump(returnValueGPR, JSSwitchPtrTag);</span>
 786 }
 787 
 788 void JIT::emit_op_switch_string(const Instruction* currentInstruction)
 789 {
 790     auto bytecode = currentInstruction-&gt;as&lt;OpSwitchString&gt;();
 791     size_t tableIndex = bytecode.m_tableIndex;
 792     unsigned defaultOffset = jumpTarget(currentInstruction, bytecode.m_defaultOffset);
 793     unsigned scrutinee = bytecode.m_scrutinee.offset();
 794 
 795     // create jump table for switch destinations, track this switch statement.
 796     StringJumpTable* jumpTable = &amp;m_codeBlock-&gt;stringSwitchJumpTable(tableIndex);
 797     m_switches.append(SwitchRecord(jumpTable, m_bytecodeOffset, defaultOffset));
 798 
 799     emitGetVirtualRegister(scrutinee, regT0);
 800     callOperation(operationSwitchStringWithUnknownKeyType, regT0, tableIndex);
<a name="15" id="anc15"></a><span class="line-modified"> 801     farJump(returnValueGPR, JSSwitchPtrTag);</span>
 802 }
 803 
 804 void JIT::emit_op_debug(const Instruction* currentInstruction)
 805 {
 806     auto bytecode = currentInstruction-&gt;as&lt;OpDebug&gt;();
 807     load32(codeBlock()-&gt;debuggerRequestsAddress(), regT0);
 808     Jump noDebuggerRequests = branchTest32(Zero, regT0);
 809     callOperation(operationDebug, static_cast&lt;int&gt;(bytecode.m_debugHookType));
 810     noDebuggerRequests.link(this);
 811 }
 812 
 813 void JIT::emit_op_eq_null(const Instruction* currentInstruction)
 814 {
 815     auto bytecode = currentInstruction-&gt;as&lt;OpEqNull&gt;();
 816     int dst = bytecode.m_dst.offset();
 817     int src1 = bytecode.m_operand.offset();
 818 
 819     emitGetVirtualRegister(src1, regT0);
 820     Jump isImmediate = branchIfNotCell(regT0);
 821 
 822     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 823     move(TrustedImm32(0), regT0);
 824     Jump wasNotMasqueradesAsUndefined = jump();
 825 
 826     isMasqueradesAsUndefined.link(this);
<a name="16" id="anc16"></a><span class="line-modified"> 827     emitLoadStructure(vm(), regT0, regT2, regT1);</span>
 828     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 829     loadPtr(Address(regT2, Structure::globalObjectOffset()), regT2);
 830     comparePtr(Equal, regT0, regT2, regT0);
 831     Jump wasNotImmediate = jump();
 832 
 833     isImmediate.link(this);
 834 
 835     and64(TrustedImm32(~TagBitUndefined), regT0);
 836     compare64(Equal, regT0, TrustedImm32(ValueNull), regT0);
 837 
 838     wasNotImmediate.link(this);
 839     wasNotMasqueradesAsUndefined.link(this);
 840 
 841     boxBoolean(regT0, JSValueRegs { regT0 });
 842     emitPutVirtualRegister(dst);
 843 
 844 }
 845 
 846 void JIT::emit_op_neq_null(const Instruction* currentInstruction)
 847 {
 848     auto bytecode = currentInstruction-&gt;as&lt;OpNeqNull&gt;();
 849     int dst = bytecode.m_dst.offset();
 850     int src1 = bytecode.m_operand.offset();
 851 
 852     emitGetVirtualRegister(src1, regT0);
 853     Jump isImmediate = branchIfNotCell(regT0);
 854 
 855     Jump isMasqueradesAsUndefined = branchTest8(NonZero, Address(regT0, JSCell::typeInfoFlagsOffset()), TrustedImm32(MasqueradesAsUndefined));
 856     move(TrustedImm32(1), regT0);
 857     Jump wasNotMasqueradesAsUndefined = jump();
 858 
 859     isMasqueradesAsUndefined.link(this);
<a name="17" id="anc17"></a><span class="line-modified"> 860     emitLoadStructure(vm(), regT0, regT2, regT1);</span>
 861     move(TrustedImmPtr(m_codeBlock-&gt;globalObject()), regT0);
 862     loadPtr(Address(regT2, Structure::globalObjectOffset()), regT2);
 863     comparePtr(NotEqual, regT0, regT2, regT0);
 864     Jump wasNotImmediate = jump();
 865 
 866     isImmediate.link(this);
 867 
 868     and64(TrustedImm32(~TagBitUndefined), regT0);
 869     compare64(NotEqual, regT0, TrustedImm32(ValueNull), regT0);
 870 
 871     wasNotImmediate.link(this);
 872     wasNotMasqueradesAsUndefined.link(this);
 873 
 874     boxBoolean(regT0, JSValueRegs { regT0 });
 875     emitPutVirtualRegister(dst);
 876 }
 877 
<a name="18" id="anc18"></a>













 878 void JIT::emit_op_get_scope(const Instruction* currentInstruction)
 879 {
 880     auto bytecode = currentInstruction-&gt;as&lt;OpGetScope&gt;();
 881     int dst = bytecode.m_dst.offset();
 882     emitGetFromCallFrameHeaderPtr(CallFrameSlot::callee, regT0);
 883     loadPtr(Address(regT0, JSFunction::offsetOfScopeChain()), regT0);
 884     emitStoreCell(dst, regT0);
 885 }
 886 
 887 void JIT::emit_op_to_this(const Instruction* currentInstruction)
 888 {
 889     auto bytecode = currentInstruction-&gt;as&lt;OpToThis&gt;();
 890     auto&amp; metadata = bytecode.metadata(m_codeBlock);
<a name="19" id="anc19"></a><span class="line-modified"> 891     StructureID* cachedStructureID = &amp;metadata.m_cachedStructureID;</span>
 892     emitGetVirtualRegister(bytecode.m_srcDst.offset(), regT1);
 893 
 894     emitJumpSlowCaseIfNotJSCell(regT1);
 895 
 896     addSlowCase(branchIfNotType(regT1, FinalObjectType));
<a name="20" id="anc20"></a><span class="line-modified"> 897     load32(cachedStructureID, regT2);</span>


 898     addSlowCase(branch32(NotEqual, Address(regT1, JSCell::structureIDOffset()), regT2));
 899 }
 900 
 901 void JIT::emit_op_create_this(const Instruction* currentInstruction)
 902 {
 903     auto bytecode = currentInstruction-&gt;as&lt;OpCreateThis&gt;();
 904     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 905     int callee = bytecode.m_callee.offset();
 906     WriteBarrierBase&lt;JSCell&gt;* cachedFunction = &amp;metadata.m_cachedCallee;
 907     RegisterID calleeReg = regT0;
 908     RegisterID rareDataReg = regT4;
 909     RegisterID resultReg = regT0;
 910     RegisterID allocatorReg = regT1;
 911     RegisterID structureReg = regT2;
 912     RegisterID cachedFunctionReg = regT4;
 913     RegisterID scratchReg = regT3;
 914 
 915     emitGetVirtualRegister(callee, calleeReg);
 916     addSlowCase(branchIfNotFunction(calleeReg));
 917     loadPtr(Address(calleeReg, JSFunction::offsetOfRareData()), rareDataReg);
 918     addSlowCase(branchTestPtr(Zero, rareDataReg));
<a name="21" id="anc21"></a><span class="line-modified"> 919     loadPtr(Address(rareDataReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfileWithPrototype::offsetOfAllocator()), allocatorReg);</span>
<span class="line-modified"> 920     loadPtr(Address(rareDataReg, FunctionRareData::offsetOfObjectAllocationProfile() + ObjectAllocationProfileWithPrototype::offsetOfStructure()), structureReg);</span>
 921 
 922     loadPtr(cachedFunction, cachedFunctionReg);
 923     Jump hasSeenMultipleCallees = branchPtr(Equal, cachedFunctionReg, TrustedImmPtr(JSCell::seenMultipleCalleeObjects()));
 924     addSlowCase(branchPtr(NotEqual, calleeReg, cachedFunctionReg));
 925     hasSeenMultipleCallees.link(this);
 926 
 927     JumpList slowCases;
 928     auto butterfly = TrustedImmPtr(nullptr);
 929     emitAllocateJSObject(resultReg, JITAllocator::variable(), allocatorReg, structureReg, butterfly, scratchReg, slowCases);
<a name="22" id="anc22"></a><span class="line-modified"> 930     load8(Address(structureReg, Structure::inlineCapacityOffset()), scratchReg);</span>


 931     emitInitializeInlineStorage(resultReg, scratchReg);
 932     addSlowCase(slowCases);
 933     emitPutVirtualRegister(bytecode.m_dst.offset());
 934 }
 935 
 936 void JIT::emit_op_check_tdz(const Instruction* currentInstruction)
 937 {
 938     auto bytecode = currentInstruction-&gt;as&lt;OpCheckTdz&gt;();
 939     emitGetVirtualRegister(bytecode.m_targetVirtualRegister.offset(), regT0);
 940     addSlowCase(branchIfEmpty(regT0));
 941 }
 942 
 943 
 944 // Slow cases
 945 
 946 void JIT::emitSlow_op_eq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 947 {
 948     linkAllSlowCases(iter);
 949 
 950     auto bytecode = currentInstruction-&gt;as&lt;OpEq&gt;();
 951     callOperation(operationCompareEq, regT0, regT1);
 952     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
 953     emitPutVirtualRegister(bytecode.m_dst.offset(), returnValueGPR);
 954 }
 955 
 956 void JIT::emitSlow_op_neq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 957 {
 958     linkAllSlowCases(iter);
 959 
 960     auto bytecode = currentInstruction-&gt;as&lt;OpNeq&gt;();
 961     callOperation(operationCompareEq, regT0, regT1);
 962     xor32(TrustedImm32(0x1), regT0);
 963     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
 964     emitPutVirtualRegister(bytecode.m_dst.offset(), returnValueGPR);
 965 }
 966 
 967 void JIT::emitSlow_op_jeq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 968 {
 969     linkAllSlowCases(iter);
 970 
 971     auto bytecode = currentInstruction-&gt;as&lt;OpJeq&gt;();
 972     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 973     callOperation(operationCompareEq, regT0, regT1);
 974     emitJumpSlowToHot(branchTest32(NonZero, returnValueGPR), target);
 975 }
 976 
 977 void JIT::emitSlow_op_jneq(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 978 {
 979     linkAllSlowCases(iter);
 980 
 981     auto bytecode = currentInstruction-&gt;as&lt;OpJneq&gt;();
 982     unsigned target = jumpTarget(currentInstruction, bytecode.m_targetLabel);
 983     callOperation(operationCompareEq, regT0, regT1);
 984     emitJumpSlowToHot(branchTest32(Zero, returnValueGPR), target);
 985 }
 986 
 987 void JIT::emitSlow_op_instanceof_custom(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 988 {
 989     linkAllSlowCases(iter);
 990 
 991     auto bytecode = currentInstruction-&gt;as&lt;OpInstanceofCustom&gt;();
 992     int dst = bytecode.m_dst.offset();
 993     int value = bytecode.m_value.offset();
 994     int constructor = bytecode.m_constructor.offset();
 995     int hasInstanceValue = bytecode.m_hasInstanceValue.offset();
 996 
 997     emitGetVirtualRegister(value, regT0);
 998     emitGetVirtualRegister(constructor, regT1);
 999     emitGetVirtualRegister(hasInstanceValue, regT2);
1000     callOperation(operationInstanceOfCustom, regT0, regT1, regT2);
1001     boxBoolean(returnValueGPR, JSValueRegs { returnValueGPR });
1002     emitPutVirtualRegister(dst, returnValueGPR);
1003 }
1004 
1005 #endif // USE(JSVALUE64)
1006 
1007 void JIT::emit_op_loop_hint(const Instruction*)
1008 {
<a name="23" id="anc23"></a><span class="line-added">1009     // Check traps.</span>
<span class="line-added">1010     addSlowCase(branchTest8(NonZero, AbsoluteAddress(m_vm-&gt;needTrapHandlingAddress())));</span>
<span class="line-added">1011 #if ENABLE(DFG_JIT)</span>
1012     // Emit the JIT optimization check:
1013     if (canBeOptimized()) {
1014         addSlowCase(branchAdd32(PositiveOrZero, TrustedImm32(Options::executionCounterIncrementForLoop()),
1015             AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())));
1016     }
<a name="24" id="anc24"></a><span class="line-added">1017 #endif</span>
1018 }
1019 
1020 void JIT::emitSlow_op_loop_hint(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1021 {
<a name="25" id="anc25"></a><span class="line-added">1022     linkSlowCase(iter);</span>
<span class="line-added">1023     callOperation(operationHandleTraps);</span>
1024 #if ENABLE(DFG_JIT)
1025     // Emit the slow path for the JIT optimization check:
1026     if (canBeOptimized()) {
<a name="26" id="anc26"></a><span class="line-modified">1027         emitJumpSlowToHot(branchAdd32(Signed, TrustedImm32(Options::executionCounterIncrementForLoop()), AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())), currentInstruction-&gt;size());</span>
<span class="line-added">1028         linkSlowCase(iter);</span>
1029 
<a name="27" id="anc27"></a><span class="line-modified">1030         copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
1031 
1032         callOperation(operationOptimize, m_bytecodeOffset);
<a name="28" id="anc28"></a><span class="line-modified">1033         emitJumpSlowToHot(branchTestPtr(Zero, returnValueGPR), currentInstruction-&gt;size());</span>
1034         if (!ASSERT_DISABLED) {
1035             Jump ok = branchPtr(MacroAssembler::Above, returnValueGPR, TrustedImmPtr(bitwise_cast&lt;void*&gt;(static_cast&lt;intptr_t&gt;(1000))));
1036             abortWithReason(JITUnreasonableLoopHintJumpTarget);
1037             ok.link(this);
1038         }
<a name="29" id="anc29"></a><span class="line-modified">1039         farJump(returnValueGPR, GPRInfo::callFrameRegister);</span>



1040     }
1041 #else
1042     UNUSED_PARAM(currentInstruction);
<a name="30" id="anc30"></a>
1043 #endif
1044 }
1045 
<a name="31" id="anc31"></a>




1046 void JIT::emit_op_nop(const Instruction*)
1047 {
1048 }
1049 
1050 void JIT::emit_op_super_sampler_begin(const Instruction*)
1051 {
1052     add32(TrustedImm32(1), AbsoluteAddress(bitwise_cast&lt;void*&gt;(&amp;g_superSamplerCount)));
1053 }
1054 
1055 void JIT::emit_op_super_sampler_end(const Instruction*)
1056 {
1057     sub32(TrustedImm32(1), AbsoluteAddress(bitwise_cast&lt;void*&gt;(&amp;g_superSamplerCount)));
1058 }
1059 
<a name="32" id="anc32"></a><span class="line-modified">1060 void JIT::emit_op_enter(const Instruction*)</span>
1061 {
<a name="33" id="anc33"></a><span class="line-modified">1062     // Even though JIT doesn&#39;t use them, we initialize our constant</span>
<span class="line-added">1063     // registers to zap stale pointers, to avoid unnecessarily prolonging</span>
<span class="line-added">1064     // object lifetime and increasing GC pressure.</span>
<span class="line-added">1065     size_t count = m_codeBlock-&gt;numVars();</span>
<span class="line-added">1066     for (size_t i = CodeBlock::llintBaselineCalleeSaveSpaceAsVirtualRegisters(); i &lt; count; ++i)</span>
<span class="line-added">1067         emitInitRegister(virtualRegisterForLocal(i).offset());</span>
<span class="line-added">1068 </span>
<span class="line-added">1069     emitWriteBarrier(m_codeBlock);</span>
<span class="line-added">1070 </span>
<span class="line-added">1071     // Check traps.</span>
<span class="line-added">1072     addSlowCase(branchTest8(NonZero, AbsoluteAddress(m_vm-&gt;needTrapHandlingAddress())));</span>
1073 
<a name="34" id="anc34"></a><span class="line-added">1074 #if ENABLE(DFG_JIT)</span>
<span class="line-added">1075     if (canBeOptimized())</span>
<span class="line-added">1076         addSlowCase(branchAdd32(PositiveOrZero, TrustedImm32(Options::executionCounterIncrementForEntry()), AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())));</span>
<span class="line-added">1077 #endif</span>
<span class="line-added">1078 }</span>
<span class="line-added">1079 </span>
<span class="line-added">1080 void JIT::emitSlow_op_enter(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)</span>
<span class="line-added">1081 {</span>
<span class="line-added">1082     linkSlowCase(iter);</span>
1083     callOperation(operationHandleTraps);
<a name="35" id="anc35"></a><span class="line-added">1084 #if ENABLE(DFG_JIT)</span>
<span class="line-added">1085     if (canBeOptimized()) {</span>
<span class="line-added">1086         emitJumpSlowToHot(branchAdd32(Signed, TrustedImm32(Options::executionCounterIncrementForEntry()), AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())), currentInstruction-&gt;size());</span>
<span class="line-added">1087         linkSlowCase(iter);</span>
<span class="line-added">1088 </span>
<span class="line-added">1089         ASSERT(!m_bytecodeOffset);</span>
<span class="line-added">1090 </span>
<span class="line-added">1091         copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
<span class="line-added">1092 </span>
<span class="line-added">1093         callOperation(operationOptimize, m_bytecodeOffset);</span>
<span class="line-added">1094         emitJumpSlowToHot(branchTestPtr(Zero, returnValueGPR), currentInstruction-&gt;size());</span>
<span class="line-added">1095         farJump(returnValueGPR, GPRInfo::callFrameRegister);</span>
<span class="line-added">1096     }</span>
<span class="line-added">1097 #else</span>
<span class="line-added">1098     UNUSED_PARAM(currentInstruction);</span>
<span class="line-added">1099 #endif</span>
1100 }
1101 
1102 void JIT::emit_op_new_regexp(const Instruction* currentInstruction)
1103 {
1104     auto bytecode = currentInstruction-&gt;as&lt;OpNewRegexp&gt;();
1105     int dst = bytecode.m_dst.offset();
1106     int regexp = bytecode.m_regexp.offset();
1107     callOperation(operationNewRegexp, jsCast&lt;RegExp*&gt;(m_codeBlock-&gt;getConstant(regexp)));
1108     emitStoreCell(dst, returnValueGPR);
1109 }
1110 
1111 template&lt;typename Op&gt;
1112 void JIT::emitNewFuncCommon(const Instruction* currentInstruction)
1113 {
1114     Jump lazyJump;
1115     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
1116     int dst = bytecode.m_dst.offset();
1117 
1118 #if USE(JSVALUE64)
1119     emitGetVirtualRegister(bytecode.m_scope.offset(), regT0);
1120 #else
1121     emitLoadPayload(bytecode.m_scope.offset(), regT0);
1122 #endif
1123     FunctionExecutable* funcExec = m_codeBlock-&gt;functionDecl(bytecode.m_functionDecl);
1124 
1125     OpcodeID opcodeID = Op::opcodeID;
1126     if (opcodeID == op_new_func)
1127         callOperation(operationNewFunction, dst, regT0, funcExec);
1128     else if (opcodeID == op_new_generator_func)
1129         callOperation(operationNewGeneratorFunction, dst, regT0, funcExec);
1130     else if (opcodeID == op_new_async_func)
1131         callOperation(operationNewAsyncFunction, dst, regT0, funcExec);
1132     else {
1133         ASSERT(opcodeID == op_new_async_generator_func);
1134         callOperation(operationNewAsyncGeneratorFunction, dst, regT0, funcExec);
1135     }
1136 }
1137 
1138 void JIT::emit_op_new_func(const Instruction* currentInstruction)
1139 {
1140     emitNewFuncCommon&lt;OpNewFunc&gt;(currentInstruction);
1141 }
1142 
1143 void JIT::emit_op_new_generator_func(const Instruction* currentInstruction)
1144 {
1145     emitNewFuncCommon&lt;OpNewGeneratorFunc&gt;(currentInstruction);
1146 }
1147 
1148 void JIT::emit_op_new_async_generator_func(const Instruction* currentInstruction)
1149 {
1150     emitNewFuncCommon&lt;OpNewAsyncGeneratorFunc&gt;(currentInstruction);
1151 }
1152 
1153 void JIT::emit_op_new_async_func(const Instruction* currentInstruction)
1154 {
1155     emitNewFuncCommon&lt;OpNewAsyncFunc&gt;(currentInstruction);
1156 }
1157 
1158 template&lt;typename Op&gt;
1159 void JIT::emitNewFuncExprCommon(const Instruction* currentInstruction)
1160 {
1161     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
1162     int dst = bytecode.m_dst.offset();
1163 #if USE(JSVALUE64)
1164     emitGetVirtualRegister(bytecode.m_scope.offset(), regT0);
1165 #else
1166     emitLoadPayload(bytecode.m_scope.offset(), regT0);
1167 #endif
1168 
1169     FunctionExecutable* function = m_codeBlock-&gt;functionExpr(bytecode.m_functionDecl);
1170     OpcodeID opcodeID = Op::opcodeID;
1171 
1172     if (opcodeID == op_new_func_exp)
1173         callOperation(operationNewFunction, dst, regT0, function);
1174     else if (opcodeID == op_new_generator_func_exp)
1175         callOperation(operationNewGeneratorFunction, dst, regT0, function);
1176     else if (opcodeID == op_new_async_func_exp)
1177         callOperation(operationNewAsyncFunction, dst, regT0, function);
1178     else {
1179         ASSERT(opcodeID == op_new_async_generator_func_exp);
1180         callOperation(operationNewAsyncGeneratorFunction, dst, regT0, function);
1181     }
1182 }
1183 
1184 void JIT::emit_op_new_func_exp(const Instruction* currentInstruction)
1185 {
1186     emitNewFuncExprCommon&lt;OpNewFuncExp&gt;(currentInstruction);
1187 }
1188 
1189 void JIT::emit_op_new_generator_func_exp(const Instruction* currentInstruction)
1190 {
1191     emitNewFuncExprCommon&lt;OpNewGeneratorFuncExp&gt;(currentInstruction);
1192 }
1193 
1194 void JIT::emit_op_new_async_func_exp(const Instruction* currentInstruction)
1195 {
1196     emitNewFuncExprCommon&lt;OpNewAsyncFuncExp&gt;(currentInstruction);
1197 }
1198 
1199 void JIT::emit_op_new_async_generator_func_exp(const Instruction* currentInstruction)
1200 {
1201     emitNewFuncExprCommon&lt;OpNewAsyncGeneratorFuncExp&gt;(currentInstruction);
1202 }
1203 
1204 void JIT::emit_op_new_array(const Instruction* currentInstruction)
1205 {
1206     auto bytecode = currentInstruction-&gt;as&lt;OpNewArray&gt;();
1207     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1208     int dst = bytecode.m_dst.offset();
1209     int valuesIndex = bytecode.m_argv.offset();
1210     int size = bytecode.m_argc;
1211     addPtr(TrustedImm32(valuesIndex * sizeof(Register)), callFrameRegister, regT0);
1212     callOperation(operationNewArrayWithProfile, dst,
1213         &amp;metadata.m_arrayAllocationProfile, regT0, size);
1214 }
1215 
1216 void JIT::emit_op_new_array_with_size(const Instruction* currentInstruction)
1217 {
1218     auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSize&gt;();
1219     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1220     int dst = bytecode.m_dst.offset();
1221     int sizeIndex = bytecode.m_length.offset();
1222 #if USE(JSVALUE64)
1223     emitGetVirtualRegister(sizeIndex, regT0);
1224     callOperation(operationNewArrayWithSizeAndProfile, dst,
1225         &amp;metadata.m_arrayAllocationProfile, regT0);
1226 #else
1227     emitLoad(sizeIndex, regT1, regT0);
1228     callOperation(operationNewArrayWithSizeAndProfile, dst,
1229         &amp;metadata.m_arrayAllocationProfile, JSValueRegs(regT1, regT0));
1230 #endif
1231 }
1232 
1233 #if USE(JSVALUE64)
1234 void JIT::emit_op_has_structure_property(const Instruction* currentInstruction)
1235 {
1236     auto bytecode = currentInstruction-&gt;as&lt;OpHasStructureProperty&gt;();
1237     int dst = bytecode.m_dst.offset();
1238     int base = bytecode.m_base.offset();
1239     int enumerator = bytecode.m_enumerator.offset();
1240 
1241     emitGetVirtualRegister(base, regT0);
1242     emitGetVirtualRegister(enumerator, regT1);
1243     emitJumpSlowCaseIfNotJSCell(regT0, base);
1244 
1245     load32(Address(regT0, JSCell::structureIDOffset()), regT0);
1246     addSlowCase(branch32(NotEqual, regT0, Address(regT1, JSPropertyNameEnumerator::cachedStructureIDOffset())));
1247 
1248     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
1249     emitPutVirtualRegister(dst);
1250 }
1251 
1252 void JIT::privateCompileHasIndexedProperty(ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
1253 {
1254     const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(byValInfo-&gt;bytecodeIndex).ptr();
1255 
1256     PatchableJump badType;
1257 
1258     // FIXME: Add support for other types like TypedArrays and Arguments.
1259     // See https://bugs.webkit.org/show_bug.cgi?id=135033 and https://bugs.webkit.org/show_bug.cgi?id=135034.
1260     JumpList slowCases = emitLoadForArrayMode(currentInstruction, arrayMode, badType);
1261     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
1262     Jump done = jump();
1263 
1264     LinkBuffer patchBuffer(*this, m_codeBlock);
1265 
1266     patchBuffer.link(badType, byValInfo-&gt;slowPathTarget);
1267     patchBuffer.link(slowCases, byValInfo-&gt;slowPathTarget);
1268 
1269     patchBuffer.link(done, byValInfo-&gt;badTypeDoneTarget);
1270 
1271     byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1272         m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1273         &quot;Baseline has_indexed_property stub for %s, return point %p&quot;, toCString(*m_codeBlock).data(), returnAddress.value());
1274 
1275     MacroAssembler::repatchJump(byValInfo-&gt;badTypeJump, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(byValInfo-&gt;stubRoutine-&gt;code().code()));
1276     MacroAssembler::repatchCall(CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)), FunctionPtr&lt;OperationPtrTag&gt;(operationHasIndexedPropertyGeneric));
1277 }
1278 
1279 void JIT::emit_op_has_indexed_property(const Instruction* currentInstruction)
1280 {
1281     auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
1282     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1283     int dst = bytecode.m_dst.offset();
1284     int base = bytecode.m_base.offset();
1285     int property = bytecode.m_property.offset();
1286     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
1287     ByValInfo* byValInfo = m_codeBlock-&gt;addByValInfo();
1288 
1289     emitGetVirtualRegisters(base, regT0, property, regT1);
1290 
<a name="36" id="anc36"></a><span class="line-added">1291     emitJumpSlowCaseIfNotInt(regT1);</span>
<span class="line-added">1292 </span>
1293     // This is technically incorrect - we&#39;re zero-extending an int32. On the hot path this doesn&#39;t matter.
1294     // We check the value as if it was a uint32 against the m_vectorLength - which will always fail if
1295     // number was signed since m_vectorLength is always less than intmax (since the total allocation
1296     // size is always less than 4Gb). As such zero extending will have been correct (and extending the value
1297     // to 64-bits is necessary since it&#39;s used in the address calculation. We zero extend rather than sign
1298     // extending since it makes it easier to re-tag the value in the slow case.
1299     zeroExtend32ToPtr(regT1, regT1);
1300 
1301     emitJumpSlowCaseIfNotJSCell(regT0, base);
1302     emitArrayProfilingSiteWithCell(regT0, regT2, profile);
1303     and32(TrustedImm32(IndexingShapeMask), regT2);
1304 
1305     JITArrayMode mode = chooseArrayMode(profile);
1306     PatchableJump badType;
1307 
1308     // FIXME: Add support for other types like TypedArrays and Arguments.
1309     // See https://bugs.webkit.org/show_bug.cgi?id=135033 and https://bugs.webkit.org/show_bug.cgi?id=135034.
1310     JumpList slowCases = emitLoadForArrayMode(currentInstruction, mode, badType);
1311 
1312     move(TrustedImm64(JSValue::encode(jsBoolean(true))), regT0);
1313 
1314     addSlowCase(badType);
1315     addSlowCase(slowCases);
1316 
1317     Label done = label();
1318 
1319     emitPutVirtualRegister(dst);
1320 
1321     Label nextHotPath = label();
1322 
1323     m_byValCompilationInfo.append(ByValCompilationInfo(byValInfo, m_bytecodeOffset, PatchableJump(), badType, mode, profile, done, nextHotPath));
1324 }
1325 
1326 void JIT::emitSlow_op_has_indexed_property(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1327 {
1328     linkAllSlowCases(iter);
1329 
1330     auto bytecode = currentInstruction-&gt;as&lt;OpHasIndexedProperty&gt;();
1331     int dst = bytecode.m_dst.offset();
1332     int base = bytecode.m_base.offset();
1333     int property = bytecode.m_property.offset();
1334     ByValInfo* byValInfo = m_byValCompilationInfo[m_byValInstructionIndex].byValInfo;
1335 
1336     Label slowPath = label();
1337 
1338     emitGetVirtualRegister(base, regT0);
1339     emitGetVirtualRegister(property, regT1);
1340     Call call = callOperation(operationHasIndexedPropertyDefault, dst, regT0, regT1, byValInfo);
1341 
1342     m_byValCompilationInfo[m_byValInstructionIndex].slowPathTarget = slowPath;
1343     m_byValCompilationInfo[m_byValInstructionIndex].returnAddress = call;
1344     m_byValInstructionIndex++;
1345 }
1346 
1347 void JIT::emit_op_get_direct_pname(const Instruction* currentInstruction)
1348 {
1349     auto bytecode = currentInstruction-&gt;as&lt;OpGetDirectPname&gt;();
1350     int dst = bytecode.m_dst.offset();
1351     int base = bytecode.m_base.offset();
1352     int index = bytecode.m_index.offset();
1353     int enumerator = bytecode.m_enumerator.offset();
1354 
1355     // Check that base is a cell
1356     emitGetVirtualRegister(base, regT0);
1357     emitJumpSlowCaseIfNotJSCell(regT0, base);
1358 
1359     // Check the structure
1360     emitGetVirtualRegister(enumerator, regT2);
1361     load32(Address(regT0, JSCell::structureIDOffset()), regT1);
1362     addSlowCase(branch32(NotEqual, regT1, Address(regT2, JSPropertyNameEnumerator::cachedStructureIDOffset())));
1363 
1364     // Compute the offset
1365     emitGetVirtualRegister(index, regT1);
1366     // If index is less than the enumerator&#39;s cached inline storage, then it&#39;s an inline access
1367     Jump outOfLineAccess = branch32(AboveOrEqual, regT1, Address(regT2, JSPropertyNameEnumerator::cachedInlineCapacityOffset()));
1368     addPtr(TrustedImm32(JSObject::offsetOfInlineStorage()), regT0);
1369     signExtend32ToPtr(regT1, regT1);
1370     load64(BaseIndex(regT0, regT1, TimesEight), regT0);
1371 
1372     Jump done = jump();
1373 
1374     // Otherwise it&#39;s out of line
1375     outOfLineAccess.link(this);
1376     loadPtr(Address(regT0, JSObject::butterflyOffset()), regT0);
1377     sub32(Address(regT2, JSPropertyNameEnumerator::cachedInlineCapacityOffset()), regT1);
1378     neg32(regT1);
1379     signExtend32ToPtr(regT1, regT1);
1380     int32_t offsetOfFirstProperty = static_cast&lt;int32_t&gt;(offsetInButterfly(firstOutOfLineOffset)) * sizeof(EncodedJSValue);
1381     load64(BaseIndex(regT0, regT1, TimesEight, offsetOfFirstProperty), regT0);
1382 
1383     done.link(this);
1384     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
1385     emitPutVirtualRegister(dst, regT0);
1386 }
1387 
1388 void JIT::emit_op_enumerator_structure_pname(const Instruction* currentInstruction)
1389 {
1390     auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorStructurePname&gt;();
1391     int dst = bytecode.m_dst.offset();
1392     int enumerator = bytecode.m_enumerator.offset();
1393     int index = bytecode.m_index.offset();
1394 
1395     emitGetVirtualRegister(index, regT0);
1396     emitGetVirtualRegister(enumerator, regT1);
1397     Jump inBounds = branch32(Below, regT0, Address(regT1, JSPropertyNameEnumerator::endStructurePropertyIndexOffset()));
1398 
1399     move(TrustedImm64(JSValue::encode(jsNull())), regT0);
1400 
1401     Jump done = jump();
1402     inBounds.link(this);
1403 
1404     loadPtr(Address(regT1, JSPropertyNameEnumerator::cachedPropertyNamesVectorOffset()), regT1);
1405     signExtend32ToPtr(regT0, regT0);
1406     load64(BaseIndex(regT1, regT0, TimesEight), regT0);
1407 
1408     done.link(this);
1409     emitPutVirtualRegister(dst);
1410 }
1411 
1412 void JIT::emit_op_enumerator_generic_pname(const Instruction* currentInstruction)
1413 {
1414     auto bytecode = currentInstruction-&gt;as&lt;OpEnumeratorGenericPname&gt;();
1415     int dst = bytecode.m_dst.offset();
1416     int enumerator = bytecode.m_enumerator.offset();
1417     int index = bytecode.m_index.offset();
1418 
1419     emitGetVirtualRegister(index, regT0);
1420     emitGetVirtualRegister(enumerator, regT1);
1421     Jump inBounds = branch32(Below, regT0, Address(regT1, JSPropertyNameEnumerator::endGenericPropertyIndexOffset()));
1422 
1423     move(TrustedImm64(JSValue::encode(jsNull())), regT0);
1424 
1425     Jump done = jump();
1426     inBounds.link(this);
1427 
1428     loadPtr(Address(regT1, JSPropertyNameEnumerator::cachedPropertyNamesVectorOffset()), regT1);
1429     signExtend32ToPtr(regT0, regT0);
1430     load64(BaseIndex(regT1, regT0, TimesEight), regT0);
1431 
1432     done.link(this);
1433     emitPutVirtualRegister(dst);
1434 }
1435 
1436 void JIT::emit_op_profile_type(const Instruction* currentInstruction)
1437 {
1438     auto bytecode = currentInstruction-&gt;as&lt;OpProfileType&gt;();
1439     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1440     TypeLocation* cachedTypeLocation = metadata.m_typeLocation;
1441     int valueToProfile = bytecode.m_targetVirtualRegister.offset();
1442 
1443     emitGetVirtualRegister(valueToProfile, regT0);
1444 
1445     JumpList jumpToEnd;
1446 
1447     jumpToEnd.append(branchIfEmpty(regT0));
1448 
1449     // Compile in a predictive type check, if possible, to see if we can skip writing to the log.
1450     // These typechecks are inlined to match those of the 64-bit JSValue type checks.
1451     if (cachedTypeLocation-&gt;m_lastSeenType == TypeUndefined)
1452         jumpToEnd.append(branchIfUndefined(regT0));
1453     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeNull)
1454         jumpToEnd.append(branchIfNull(regT0));
1455     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeBoolean)
1456         jumpToEnd.append(branchIfBoolean(regT0, regT1));
1457     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeAnyInt)
1458         jumpToEnd.append(branchIfInt32(regT0));
1459     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeNumber)
1460         jumpToEnd.append(branchIfNumber(regT0));
1461     else if (cachedTypeLocation-&gt;m_lastSeenType == TypeString) {
1462         Jump isNotCell = branchIfNotCell(regT0);
1463         jumpToEnd.append(branchIfString(regT0));
1464         isNotCell.link(this);
1465     }
1466 
1467     // Load the type profiling log into T2.
1468     TypeProfilerLog* cachedTypeProfilerLog = m_vm-&gt;typeProfilerLog();
1469     move(TrustedImmPtr(cachedTypeProfilerLog), regT2);
1470     // Load the next log entry into T1.
1471     loadPtr(Address(regT2, TypeProfilerLog::currentLogEntryOffset()), regT1);
1472 
1473     // Store the JSValue onto the log entry.
1474     store64(regT0, Address(regT1, TypeProfilerLog::LogEntry::valueOffset()));
1475 
1476     // Store the structureID of the cell if T0 is a cell, otherwise, store 0 on the log entry.
1477     Jump notCell = branchIfNotCell(regT0);
1478     load32(Address(regT0, JSCell::structureIDOffset()), regT0);
1479     store32(regT0, Address(regT1, TypeProfilerLog::LogEntry::structureIDOffset()));
1480     Jump skipIsCell = jump();
1481     notCell.link(this);
1482     store32(TrustedImm32(0), Address(regT1, TypeProfilerLog::LogEntry::structureIDOffset()));
1483     skipIsCell.link(this);
1484 
1485     // Store the typeLocation on the log entry.
1486     move(TrustedImmPtr(cachedTypeLocation), regT0);
1487     store64(regT0, Address(regT1, TypeProfilerLog::LogEntry::locationOffset()));
1488 
1489     // Increment the current log entry.
1490     addPtr(TrustedImm32(sizeof(TypeProfilerLog::LogEntry)), regT1);
1491     store64(regT1, Address(regT2, TypeProfilerLog::currentLogEntryOffset()));
1492     Jump skipClearLog = branchPtr(NotEqual, regT1, TrustedImmPtr(cachedTypeProfilerLog-&gt;logEndPtr()));
1493     // Clear the log if we&#39;re at the end of the log.
1494     callOperation(operationProcessTypeProfilerLog);
1495     skipClearLog.link(this);
1496 
1497     jumpToEnd.link(this);
1498 }
1499 
1500 void JIT::emit_op_log_shadow_chicken_prologue(const Instruction* currentInstruction)
1501 {
<a name="37" id="anc37"></a><span class="line-modified">1502     RELEASE_ASSERT(vm().shadowChicken());</span>
1503     updateTopCallFrame();
1504     static_assert(nonArgGPR0 != regT0 &amp;&amp; nonArgGPR0 != regT2, &quot;we will have problems if this is true.&quot;);
1505     auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenPrologue&gt;();
1506     GPRReg shadowPacketReg = regT0;
1507     GPRReg scratch1Reg = nonArgGPR0; // This must be a non-argument register.
1508     GPRReg scratch2Reg = regT2;
<a name="38" id="anc38"></a><span class="line-modified">1509     ensureShadowChickenPacket(vm(), shadowPacketReg, scratch1Reg, scratch2Reg);</span>
1510     emitGetVirtualRegister(bytecode.m_scope.offset(), regT3);
1511     logShadowChickenProloguePacket(shadowPacketReg, scratch1Reg, regT3);
1512 }
1513 
1514 void JIT::emit_op_log_shadow_chicken_tail(const Instruction* currentInstruction)
1515 {
<a name="39" id="anc39"></a><span class="line-modified">1516     RELEASE_ASSERT(vm().shadowChicken());</span>
1517     updateTopCallFrame();
1518     static_assert(nonArgGPR0 != regT0 &amp;&amp; nonArgGPR0 != regT2, &quot;we will have problems if this is true.&quot;);
1519     auto bytecode = currentInstruction-&gt;as&lt;OpLogShadowChickenTail&gt;();
1520     GPRReg shadowPacketReg = regT0;
1521     GPRReg scratch1Reg = nonArgGPR0; // This must be a non-argument register.
1522     GPRReg scratch2Reg = regT2;
<a name="40" id="anc40"></a><span class="line-modified">1523     ensureShadowChickenPacket(vm(), shadowPacketReg, scratch1Reg, scratch2Reg);</span>
1524     emitGetVirtualRegister(bytecode.m_thisValue.offset(), regT2);
1525     emitGetVirtualRegister(bytecode.m_scope.offset(), regT3);
1526     logShadowChickenTailPacket(shadowPacketReg, JSValueRegs(regT2), regT3, m_codeBlock, CallSiteIndex(m_bytecodeOffset));
1527 }
1528 
1529 #endif // USE(JSVALUE64)
1530 
1531 void JIT::emit_op_profile_control_flow(const Instruction* currentInstruction)
1532 {
1533     auto bytecode = currentInstruction-&gt;as&lt;OpProfileControlFlow&gt;();
1534     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1535     BasicBlockLocation* basicBlockLocation = metadata.m_basicBlockLocation;
1536 #if USE(JSVALUE64)
1537     basicBlockLocation-&gt;emitExecuteCode(*this);
1538 #else
1539     basicBlockLocation-&gt;emitExecuteCode(*this, regT0);
1540 #endif
1541 }
1542 
1543 void JIT::emit_op_argument_count(const Instruction* currentInstruction)
1544 {
1545     auto bytecode = currentInstruction-&gt;as&lt;OpArgumentCount&gt;();
1546     int dst = bytecode.m_dst.offset();
1547     load32(payloadFor(CallFrameSlot::argumentCount), regT0);
1548     sub32(TrustedImm32(1), regT0);
1549     JSValueRegs result = JSValueRegs::withTwoAvailableRegs(regT0, regT1);
1550     boxInt32(regT0, result);
1551     emitPutVirtualRegister(dst, result);
1552 }
1553 
1554 void JIT::emit_op_get_rest_length(const Instruction* currentInstruction)
1555 {
1556     auto bytecode = currentInstruction-&gt;as&lt;OpGetRestLength&gt;();
1557     int dst = bytecode.m_dst.offset();
1558     unsigned numParamsToSkip = bytecode.m_numParametersToSkip;
1559     load32(payloadFor(CallFrameSlot::argumentCount), regT0);
1560     sub32(TrustedImm32(1), regT0);
1561     Jump zeroLength = branch32(LessThanOrEqual, regT0, Imm32(numParamsToSkip));
1562     sub32(Imm32(numParamsToSkip), regT0);
1563 #if USE(JSVALUE64)
1564     boxInt32(regT0, JSValueRegs(regT0));
1565 #endif
1566     Jump done = jump();
1567 
1568     zeroLength.link(this);
1569 #if USE(JSVALUE64)
1570     move(TrustedImm64(JSValue::encode(jsNumber(0))), regT0);
1571 #else
1572     move(TrustedImm32(0), regT0);
1573 #endif
1574 
1575     done.link(this);
1576 #if USE(JSVALUE64)
1577     emitPutVirtualRegister(dst, regT0);
1578 #else
1579     move(TrustedImm32(JSValue::Int32Tag), regT1);
1580     emitPutVirtualRegister(dst, JSValueRegs(regT1, regT0));
1581 #endif
1582 }
1583 
1584 void JIT::emit_op_get_argument(const Instruction* currentInstruction)
1585 {
1586     auto bytecode = currentInstruction-&gt;as&lt;OpGetArgument&gt;();
1587     int dst = bytecode.m_dst.offset();
1588     int index = bytecode.m_index;
1589 #if USE(JSVALUE64)
1590     JSValueRegs resultRegs(regT0);
1591 #else
1592     JSValueRegs resultRegs(regT1, regT0);
1593 #endif
1594 
1595     load32(payloadFor(CallFrameSlot::argumentCount), regT2);
1596     Jump argumentOutOfBounds = branch32(LessThanOrEqual, regT2, TrustedImm32(index));
1597     loadValue(addressFor(CallFrameSlot::thisArgument + index), resultRegs);
1598     Jump done = jump();
1599 
1600     argumentOutOfBounds.link(this);
1601     moveValue(jsUndefined(), resultRegs);
1602 
1603     done.link(this);
1604     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
1605     emitPutVirtualRegister(dst, resultRegs);
1606 }
1607 
1608 } // namespace JSC
1609 
1610 #endif // ENABLE(JIT)
<a name="41" id="anc41"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="41" type="hidden" />
</body>
</html>