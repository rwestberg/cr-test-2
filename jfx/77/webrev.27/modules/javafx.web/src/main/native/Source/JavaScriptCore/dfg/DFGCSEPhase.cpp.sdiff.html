<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGCSEPhase.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="DFGCPSRethreadingPhase.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGCallArrayAllocatorSlowPathGenerator.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGCSEPhase.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
119     const ImpureDataSlot* add(const HeapLocation&amp; location, const LazyNode&amp; node)
120     {
121         const ImpureDataSlot* result = addImpl(location, node);
122 
123 #if !defined(NDEBUG)
124         auto addResult = m_debugImpureData.add(location, node);
125         ASSERT(!!result == !addResult.isNewEntry);
126 #endif
127         return result;
128     }
129 
130     LazyNode get(const HeapLocation&amp; location) const
131     {
132         LazyNode result = getImpl(location);
133 #if !defined(NDEBUG)
134         ASSERT(result == m_debugImpureData.get(location));
135 #endif
136         return result;
137     }
138 
<span class="line-modified">139     void clobber(AbstractHeap heap)</span>
140     {
141         switch (heap.kind()) {
142         case World: {
143             clear();
144             break;
145         }
146         case SideState:
147             break;
148         case Stack: {
149             ASSERT(!heap.payload().isTop());
150             ASSERT(heap.payload().value() == heap.payload().value32());
151             m_abstractHeapStackMap.remove(heap.payload().value32());
<span class="line-modified">152             clobber(m_fallbackStackMap, heap);</span>



153             break;
154         }
155         default:
<span class="line-modified">156             clobber(m_heapMap, heap);</span>



157             break;
158         }
159 #if !defined(NDEBUG)
<span class="line-modified">160         m_debugImpureData.removeIf([heap](const HashMap&lt;HeapLocation, LazyNode&gt;::KeyValuePairType&amp; pair) -&gt; bool {</span>
























161             return heap.overlaps(pair.key.heap());
162         });
163         ASSERT(m_debugImpureData.size()
164             == (m_heapMap.size()
165                 + m_abstractHeapStackMap.size()
166                 + m_fallbackStackMap.size()));
167 
168         const bool verifyClobber = false;
169         if (verifyClobber) {
170             for (auto&amp; pair : m_debugImpureData)
171                 ASSERT(!!get(pair.key));
172         }
173 #endif
174     }
175 
176     void clear()
177     {
178         m_abstractHeapStackMap.clear();
179         m_fallbackStackMap.clear();
180         m_heapMap.clear();
</pre>
<hr />
<pre>
240             return nullptr;
241         }
242         return result.iterator-&gt;get();
243     }
244 
245     static LazyNode get(const Map&amp; map, const HeapLocation&amp; location)
246     {
247         auto iterator = map.find&lt;ImpureDataTranslator&gt;(location);
248         if (iterator != map.end())
249             return (*iterator)-&gt;value;
250         return LazyNode();
251     }
252 
253     static void clobber(Map&amp; map, AbstractHeap heap)
254     {
255         map.removeIf([heap](const std::unique_ptr&lt;ImpureDataSlot&gt;&amp; slot) -&gt; bool {
256             return heap.overlaps(slot-&gt;key.heap());
257         });
258     }
259 
<span class="line-modified">260     // The majority of Impure Stack Slotsare unique per value.</span>
261     // This is very useful for fast clobber(), we can just remove the slot addressed by AbstractHeap
262     // in O(1).
263     //
264     // When there are conflict, any additional HeapLocation is added in the fallback map.
265     // This works well because fallbackStackMap remains tiny.
266     //
267     // One cannot assume a unique ImpureData is in m_abstractHeapStackMap. It may have been
268     // a duplicate in the past and now only live in m_fallbackStackMap.
269     //
270     // Obviously, TOP always goes into m_fallbackStackMap since it does not have a unique value.
271     HashMap&lt;int32_t, std::unique_ptr&lt;ImpureDataSlot&gt;, DefaultHash&lt;int32_t&gt;::Hash, WTF::SignedWithZeroKeyHashTraits&lt;int32_t&gt;&gt; m_abstractHeapStackMap;
272     Map m_fallbackStackMap;
273 
274     Map m_heapMap;
275 
276 #if !defined(NDEBUG)
277     HashMap&lt;HeapLocation, LazyNode&gt; m_debugImpureData;
278 #endif
279 };
280 
281 class LocalCSEPhase : public Phase {
282 public:
283     LocalCSEPhase(Graph&amp; graph)
284         : Phase(graph, &quot;local common subexpression elimination&quot;)
285         , m_smallBlock(graph)
286         , m_largeBlock(graph)

287     {
288     }
289 
290     bool run()
291     {
292         ASSERT(m_graph.m_fixpointState == FixpointNotConverged);
293         ASSERT(m_graph.m_form == ThreadedCPS || m_graph.m_form == LoadStore);
294 
295         bool changed = false;
296 
297         m_graph.clearReplacements();
298 
299         for (BlockIndex blockIndex = m_graph.numBlocks(); blockIndex--;) {
300             BasicBlock* block = m_graph.block(blockIndex);
301             if (!block)
302                 continue;
303 
304             if (block-&gt;size() &lt;= SmallMaps::capacity)
305                 changed |= m_smallBlock.run(block);
<span class="line-modified">306             else</span>
307                 changed |= m_largeBlock.run(block);


308         }
309 
310         return changed;
311     }
312 
313 private:
314     class SmallMaps {
315     public:
316         // This permits SmallMaps to be used for blocks that have up to 100 nodes. In practice,
317         // fewer than half of the nodes in a block have pure defs, and even fewer have impure defs.
318         // Thus, a capacity limit of 100 probably means that somewhere around ~40 things may end up
319         // in one of these &quot;small&quot; list-based maps. That number still seems largeish, except that
320         // the overhead of HashMaps can be quite high currently: clearing them, or even removing
321         // enough things from them, deletes (or resizes) their backing store eagerly. Hence
322         // HashMaps induce a lot of malloc traffic.
323         static const unsigned capacity = 100;
324 
325         SmallMaps()
326             : m_pureLength(0)
327             , m_impureLength(0)
</pre>
<hr />
<pre>
383         WTF::KeyValuePair&lt;PureValue, Node*&gt; m_pureMap[capacity];
384         WTF::KeyValuePair&lt;HeapLocation, LazyNode&gt; m_impureMap[capacity];
385         unsigned m_pureLength;
386         unsigned m_impureLength;
387     };
388 
389     class LargeMaps {
390     public:
391         LargeMaps()
392         {
393         }
394 
395         void clear()
396         {
397             m_pureMap.clear();
398             m_impureMap.clear();
399         }
400 
401         void write(AbstractHeap heap)
402         {
<span class="line-modified">403             m_impureMap.clobber(heap);</span>















































404         }
405 
406         Node* addPure(PureValue value, Node* node)
407         {
408             auto result = m_pureMap.add(value, node);
409             if (result.isNewEntry)
410                 return nullptr;
411             return result.iterator-&gt;value;
412         }
413 
414         LazyNode findReplacement(HeapLocation location)
415         {
416             return m_impureMap.get(location);
417         }
418 
419         LazyNode addImpure(const HeapLocation&amp; location, const LazyNode&amp; node)
420         {
421             if (const ImpureDataSlot* slot = m_impureMap.add(location, node))
422                 return slot-&gt;value;
423             return LazyNode();
</pre>
<hr />
<pre>
564                 ASSERT(match.isNode());
565                 m_node-&gt;replaceWith(m_graph, match.asNode());
566                 m_changed = true;
567             }
568         }
569 
570     private:
571         Graph&amp; m_graph;
572 
573         bool m_changed;
574         Node* m_node;
575         BasicBlock* m_block;
576 
577         Maps m_maps;
578 
579         InsertionSet m_insertionSet;
580     };
581 
582     BlockCSE&lt;SmallMaps&gt; m_smallBlock;
583     BlockCSE&lt;LargeMaps&gt; m_largeBlock;

584 };
585 
586 class GlobalCSEPhase : public Phase {
587 public:
588     GlobalCSEPhase(Graph&amp; graph)
589         : Phase(graph, &quot;global common subexpression elimination&quot;)
590         , m_impureDataMap(graph)
591         , m_insertionSet(graph)
592     {
593     }
594 
595     bool run()
596     {
597         ASSERT(m_graph.m_fixpointState == FixpointNotConverged);
598         ASSERT(m_graph.m_form == SSA);
599 
600         m_graph.initializeNodeOwners();
601         m_graph.ensureSSADominators();
602 
603         m_preOrder = m_graph.blocksInPreOrder();
</pre>
<hr />
<pre>
651 
652                 if (m_node-&gt;op() == Identity || m_node-&gt;op() == IdentityWithProfile) {
653                     m_node-&gt;replaceWith(m_graph, m_node-&gt;child1().node());
654                     m_changed = true;
655                 } else
656                     clobberize(m_graph, m_node, *this);
657             }
658 
659             m_insertionSet.execute(m_block);
660 
661             m_impureData-&gt;didVisit = true;
662         }
663 
664         return m_changed;
665     }
666 
667     void read(AbstractHeap) { }
668 
669     void write(AbstractHeap heap)
670     {
<span class="line-modified">671         m_impureData-&gt;availableAtTail.clobber(heap);</span>

672         m_writesSoFar.add(heap);
673     }
674 
675     void def(PureValue value)
676     {
677         // With pure values we do not have to worry about the possibility of some control flow path
678         // clobbering the value. So, we just search for all of the like values that have been
679         // computed. We pick one that is in a block that dominates ours. Note that this means that
680         // a PureValue will map to a list of nodes, since there may be many places in the control
681         // flow graph that compute a value but only one of them that dominates us. We may build up
682         // a large list of nodes that compute some value in the case of gnarly control flow. This
683         // is probably OK.
684 
685         auto result = m_pureValues.add(value, Vector&lt;Node*&gt;());
686         if (result.isNewEntry) {
687             result.iterator-&gt;value.append(m_node);
688             return;
689         }
690 
691         for (unsigned i = result.iterator-&gt;value.size(); i--;) {
</pre>
</td>
<td>
<hr />
<pre>
119     const ImpureDataSlot* add(const HeapLocation&amp; location, const LazyNode&amp; node)
120     {
121         const ImpureDataSlot* result = addImpl(location, node);
122 
123 #if !defined(NDEBUG)
124         auto addResult = m_debugImpureData.add(location, node);
125         ASSERT(!!result == !addResult.isNewEntry);
126 #endif
127         return result;
128     }
129 
130     LazyNode get(const HeapLocation&amp; location) const
131     {
132         LazyNode result = getImpl(location);
133 #if !defined(NDEBUG)
134         ASSERT(result == m_debugImpureData.get(location));
135 #endif
136         return result;
137     }
138 
<span class="line-modified">139     void clobber(AbstractHeap heap, bool clobberConservatively)</span>
140     {
141         switch (heap.kind()) {
142         case World: {
143             clear();
144             break;
145         }
146         case SideState:
147             break;
148         case Stack: {
149             ASSERT(!heap.payload().isTop());
150             ASSERT(heap.payload().value() == heap.payload().value32());
151             m_abstractHeapStackMap.remove(heap.payload().value32());
<span class="line-modified">152             if (clobberConservatively)</span>
<span class="line-added">153                 m_fallbackStackMap.clear();</span>
<span class="line-added">154             else</span>
<span class="line-added">155                 clobber(m_fallbackStackMap, heap);</span>
156             break;
157         }
158         default:
<span class="line-modified">159             if (clobberConservatively)</span>
<span class="line-added">160                 m_heapMap.clear();</span>
<span class="line-added">161             else</span>
<span class="line-added">162                 clobber(m_heapMap, heap);</span>
163             break;
164         }
165 #if !defined(NDEBUG)
<span class="line-modified">166         m_debugImpureData.removeIf([heap, clobberConservatively, this](const HashMap&lt;HeapLocation, LazyNode&gt;::KeyValuePairType&amp; pair) -&gt; bool {</span>
<span class="line-added">167             switch (heap.kind()) {</span>
<span class="line-added">168             case World:</span>
<span class="line-added">169             case SideState:</span>
<span class="line-added">170                 break;</span>
<span class="line-added">171             case Stack: {</span>
<span class="line-added">172                 if (!clobberConservatively)</span>
<span class="line-added">173                     break;</span>
<span class="line-added">174                 if (pair.key.heap().kind() == Stack) {</span>
<span class="line-added">175                     auto iterator = m_abstractHeapStackMap.find(pair.key.heap().payload().value32());</span>
<span class="line-added">176                     if (iterator != m_abstractHeapStackMap.end() &amp;&amp; iterator-&gt;value-&gt;key == pair.key)</span>
<span class="line-added">177                         return false;</span>
<span class="line-added">178                     return true;</span>
<span class="line-added">179                 }</span>
<span class="line-added">180                 break;</span>
<span class="line-added">181             }</span>
<span class="line-added">182             default: {</span>
<span class="line-added">183                 if (!clobberConservatively)</span>
<span class="line-added">184                     break;</span>
<span class="line-added">185                 AbstractHeapKind kind = pair.key.heap().kind();</span>
<span class="line-added">186                 if (kind != World &amp;&amp; kind != SideState &amp;&amp; kind != Stack)</span>
<span class="line-added">187                     return true;</span>
<span class="line-added">188                 break;</span>
<span class="line-added">189             }</span>
<span class="line-added">190             }</span>
191             return heap.overlaps(pair.key.heap());
192         });
193         ASSERT(m_debugImpureData.size()
194             == (m_heapMap.size()
195                 + m_abstractHeapStackMap.size()
196                 + m_fallbackStackMap.size()));
197 
198         const bool verifyClobber = false;
199         if (verifyClobber) {
200             for (auto&amp; pair : m_debugImpureData)
201                 ASSERT(!!get(pair.key));
202         }
203 #endif
204     }
205 
206     void clear()
207     {
208         m_abstractHeapStackMap.clear();
209         m_fallbackStackMap.clear();
210         m_heapMap.clear();
</pre>
<hr />
<pre>
270             return nullptr;
271         }
272         return result.iterator-&gt;get();
273     }
274 
275     static LazyNode get(const Map&amp; map, const HeapLocation&amp; location)
276     {
277         auto iterator = map.find&lt;ImpureDataTranslator&gt;(location);
278         if (iterator != map.end())
279             return (*iterator)-&gt;value;
280         return LazyNode();
281     }
282 
283     static void clobber(Map&amp; map, AbstractHeap heap)
284     {
285         map.removeIf([heap](const std::unique_ptr&lt;ImpureDataSlot&gt;&amp; slot) -&gt; bool {
286             return heap.overlaps(slot-&gt;key.heap());
287         });
288     }
289 
<span class="line-modified">290     // The majority of Impure Stack Slots are unique per value.</span>
291     // This is very useful for fast clobber(), we can just remove the slot addressed by AbstractHeap
292     // in O(1).
293     //
294     // When there are conflict, any additional HeapLocation is added in the fallback map.
295     // This works well because fallbackStackMap remains tiny.
296     //
297     // One cannot assume a unique ImpureData is in m_abstractHeapStackMap. It may have been
298     // a duplicate in the past and now only live in m_fallbackStackMap.
299     //
300     // Obviously, TOP always goes into m_fallbackStackMap since it does not have a unique value.
301     HashMap&lt;int32_t, std::unique_ptr&lt;ImpureDataSlot&gt;, DefaultHash&lt;int32_t&gt;::Hash, WTF::SignedWithZeroKeyHashTraits&lt;int32_t&gt;&gt; m_abstractHeapStackMap;
302     Map m_fallbackStackMap;
303 
304     Map m_heapMap;
305 
306 #if !defined(NDEBUG)
307     HashMap&lt;HeapLocation, LazyNode&gt; m_debugImpureData;
308 #endif
309 };
310 
311 class LocalCSEPhase : public Phase {
312 public:
313     LocalCSEPhase(Graph&amp; graph)
314         : Phase(graph, &quot;local common subexpression elimination&quot;)
315         , m_smallBlock(graph)
316         , m_largeBlock(graph)
<span class="line-added">317         , m_hugeBlock(graph)</span>
318     {
319     }
320 
321     bool run()
322     {
323         ASSERT(m_graph.m_fixpointState == FixpointNotConverged);
324         ASSERT(m_graph.m_form == ThreadedCPS || m_graph.m_form == LoadStore);
325 
326         bool changed = false;
327 
328         m_graph.clearReplacements();
329 
330         for (BlockIndex blockIndex = m_graph.numBlocks(); blockIndex--;) {
331             BasicBlock* block = m_graph.block(blockIndex);
332             if (!block)
333                 continue;
334 
335             if (block-&gt;size() &lt;= SmallMaps::capacity)
336                 changed |= m_smallBlock.run(block);
<span class="line-modified">337             else if (block-&gt;size() &lt;= Options::maxDFGNodesInBasicBlockForPreciseAnalysis())</span>
338                 changed |= m_largeBlock.run(block);
<span class="line-added">339             else</span>
<span class="line-added">340                 changed |= m_hugeBlock.run(block);</span>
341         }
342 
343         return changed;
344     }
345 
346 private:
347     class SmallMaps {
348     public:
349         // This permits SmallMaps to be used for blocks that have up to 100 nodes. In practice,
350         // fewer than half of the nodes in a block have pure defs, and even fewer have impure defs.
351         // Thus, a capacity limit of 100 probably means that somewhere around ~40 things may end up
352         // in one of these &quot;small&quot; list-based maps. That number still seems largeish, except that
353         // the overhead of HashMaps can be quite high currently: clearing them, or even removing
354         // enough things from them, deletes (or resizes) their backing store eagerly. Hence
355         // HashMaps induce a lot of malloc traffic.
356         static const unsigned capacity = 100;
357 
358         SmallMaps()
359             : m_pureLength(0)
360             , m_impureLength(0)
</pre>
<hr />
<pre>
416         WTF::KeyValuePair&lt;PureValue, Node*&gt; m_pureMap[capacity];
417         WTF::KeyValuePair&lt;HeapLocation, LazyNode&gt; m_impureMap[capacity];
418         unsigned m_pureLength;
419         unsigned m_impureLength;
420     };
421 
422     class LargeMaps {
423     public:
424         LargeMaps()
425         {
426         }
427 
428         void clear()
429         {
430             m_pureMap.clear();
431             m_impureMap.clear();
432         }
433 
434         void write(AbstractHeap heap)
435         {
<span class="line-modified">436             bool clobberConservatively = false;</span>
<span class="line-added">437             m_impureMap.clobber(heap, clobberConservatively);</span>
<span class="line-added">438         }</span>
<span class="line-added">439 </span>
<span class="line-added">440         Node* addPure(PureValue value, Node* node)</span>
<span class="line-added">441         {</span>
<span class="line-added">442             auto result = m_pureMap.add(value, node);</span>
<span class="line-added">443             if (result.isNewEntry)</span>
<span class="line-added">444                 return nullptr;</span>
<span class="line-added">445             return result.iterator-&gt;value;</span>
<span class="line-added">446         }</span>
<span class="line-added">447 </span>
<span class="line-added">448         LazyNode findReplacement(HeapLocation location)</span>
<span class="line-added">449         {</span>
<span class="line-added">450             return m_impureMap.get(location);</span>
<span class="line-added">451         }</span>
<span class="line-added">452 </span>
<span class="line-added">453         LazyNode addImpure(const HeapLocation&amp; location, const LazyNode&amp; node)</span>
<span class="line-added">454         {</span>
<span class="line-added">455             if (const ImpureDataSlot* slot = m_impureMap.add(location, node))</span>
<span class="line-added">456                 return slot-&gt;value;</span>
<span class="line-added">457             return LazyNode();</span>
<span class="line-added">458         }</span>
<span class="line-added">459 </span>
<span class="line-added">460     private:</span>
<span class="line-added">461         HashMap&lt;PureValue, Node*&gt; m_pureMap;</span>
<span class="line-added">462         ImpureMap m_impureMap;</span>
<span class="line-added">463     };</span>
<span class="line-added">464 </span>
<span class="line-added">465     // This is used only for huge basic blocks. Our usual CSE is quadratic complexity for # of DFG nodes in a basic block.</span>
<span class="line-added">466     // HugeMaps model results conservatively to avoid an O(N^2) algorithm. In particular, we clear all the slots of the specified heap kind</span>
<span class="line-added">467     // in ImpureMap instead of iterating slots and removing a matched slot. This change makes the complexity O(N).</span>
<span class="line-added">468     // FIXME: We can make LargeMap O(N) without introducing conservative behavior if we track clobbering by hierarchical epochs.</span>
<span class="line-added">469     // https://bugs.webkit.org/show_bug.cgi?id=200014</span>
<span class="line-added">470     class HugeMaps {</span>
<span class="line-added">471     public:</span>
<span class="line-added">472         HugeMaps() = default;</span>
<span class="line-added">473 </span>
<span class="line-added">474         void clear()</span>
<span class="line-added">475         {</span>
<span class="line-added">476             m_pureMap.clear();</span>
<span class="line-added">477             m_impureMap.clear();</span>
<span class="line-added">478         }</span>
<span class="line-added">479 </span>
<span class="line-added">480         void write(AbstractHeap heap)</span>
<span class="line-added">481         {</span>
<span class="line-added">482             bool clobberConservatively = true;</span>
<span class="line-added">483             m_impureMap.clobber(heap, clobberConservatively);</span>
484         }
485 
486         Node* addPure(PureValue value, Node* node)
487         {
488             auto result = m_pureMap.add(value, node);
489             if (result.isNewEntry)
490                 return nullptr;
491             return result.iterator-&gt;value;
492         }
493 
494         LazyNode findReplacement(HeapLocation location)
495         {
496             return m_impureMap.get(location);
497         }
498 
499         LazyNode addImpure(const HeapLocation&amp; location, const LazyNode&amp; node)
500         {
501             if (const ImpureDataSlot* slot = m_impureMap.add(location, node))
502                 return slot-&gt;value;
503             return LazyNode();
</pre>
<hr />
<pre>
644                 ASSERT(match.isNode());
645                 m_node-&gt;replaceWith(m_graph, match.asNode());
646                 m_changed = true;
647             }
648         }
649 
650     private:
651         Graph&amp; m_graph;
652 
653         bool m_changed;
654         Node* m_node;
655         BasicBlock* m_block;
656 
657         Maps m_maps;
658 
659         InsertionSet m_insertionSet;
660     };
661 
662     BlockCSE&lt;SmallMaps&gt; m_smallBlock;
663     BlockCSE&lt;LargeMaps&gt; m_largeBlock;
<span class="line-added">664     BlockCSE&lt;HugeMaps&gt; m_hugeBlock;</span>
665 };
666 
667 class GlobalCSEPhase : public Phase {
668 public:
669     GlobalCSEPhase(Graph&amp; graph)
670         : Phase(graph, &quot;global common subexpression elimination&quot;)
671         , m_impureDataMap(graph)
672         , m_insertionSet(graph)
673     {
674     }
675 
676     bool run()
677     {
678         ASSERT(m_graph.m_fixpointState == FixpointNotConverged);
679         ASSERT(m_graph.m_form == SSA);
680 
681         m_graph.initializeNodeOwners();
682         m_graph.ensureSSADominators();
683 
684         m_preOrder = m_graph.blocksInPreOrder();
</pre>
<hr />
<pre>
732 
733                 if (m_node-&gt;op() == Identity || m_node-&gt;op() == IdentityWithProfile) {
734                     m_node-&gt;replaceWith(m_graph, m_node-&gt;child1().node());
735                     m_changed = true;
736                 } else
737                     clobberize(m_graph, m_node, *this);
738             }
739 
740             m_insertionSet.execute(m_block);
741 
742             m_impureData-&gt;didVisit = true;
743         }
744 
745         return m_changed;
746     }
747 
748     void read(AbstractHeap) { }
749 
750     void write(AbstractHeap heap)
751     {
<span class="line-modified">752         bool clobberConservatively = false;</span>
<span class="line-added">753         m_impureData-&gt;availableAtTail.clobber(heap, clobberConservatively);</span>
754         m_writesSoFar.add(heap);
755     }
756 
757     void def(PureValue value)
758     {
759         // With pure values we do not have to worry about the possibility of some control flow path
760         // clobbering the value. So, we just search for all of the like values that have been
761         // computed. We pick one that is in a block that dominates ours. Note that this means that
762         // a PureValue will map to a list of nodes, since there may be many places in the control
763         // flow graph that compute a value but only one of them that dominates us. We may build up
764         // a large list of nodes that compute some value in the case of gnarly control flow. This
765         // is probably OK.
766 
767         auto result = m_pureValues.add(value, Vector&lt;Node*&gt;());
768         if (result.isNewEntry) {
769             result.iterator-&gt;value.append(m_node);
770             return;
771         }
772 
773         for (unsigned i = result.iterator-&gt;value.size(); i--;) {
</pre>
</td>
</tr>
</table>
<center><a href="DFGCPSRethreadingPhase.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGCallArrayAllocatorSlowPathGenerator.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>