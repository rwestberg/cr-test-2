<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/Heap.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="Heap.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="HeapCell.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/Heap.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 69 class JSImmutableButterfly;
 70 class JSValue;
 71 class LLIntOffsetsExtractor;
 72 class MachineThreads;
 73 class MarkStackArray;
 74 class MarkStackMergingConstraint;
 75 class BlockDirectory;
 76 class MarkedArgumentBuffer;
 77 class MarkingConstraint;
 78 class MarkingConstraintSet;
 79 class MutatorScheduler;
 80 class RunningScope;
 81 class SlotVisitor;
 82 class SpaceTimeMutatorScheduler;
 83 class StopIfNecessaryTimer;
 84 class SweepingScope;
 85 class VM;
 86 class WeakGCMapBase;
 87 struct CurrentThreadState;
 88 
<span class="line-modified"> 89 #if USE(GLIB)</span>
 90 class JSCGLibWrapperObject;
 91 #endif
 92 
 93 namespace DFG {
 94 class SpeculativeJIT;
 95 class Worklist;
 96 }
 97 
 98 #if !ASSERT_DISABLED
 99 #define ENABLE_DFG_DOES_GC_VALIDATION 1
100 #else
101 #define ENABLE_DFG_DOES_GC_VALIDATION 0
102 #endif
103 constexpr bool validateDFGDoesGC = ENABLE_DFG_DOES_GC_VALIDATION;
104 
105 typedef HashCountedSet&lt;JSCell*&gt; ProtectCountSet;
106 typedef HashCountedSet&lt;const char*&gt; TypeCountSet;
107 
108 enum HeapType { SmallHeap, LargeHeap };
109 
110 class HeapUtil;
111 
112 class Heap {
113     WTF_MAKE_NONCOPYABLE(Heap);
114 public:
115     friend class JIT;
116     friend class DFG::SpeculativeJIT;
117     static Heap* heap(const JSValue); // 0 for immediate values
118     static Heap* heap(const HeapCell*);
119 
120     // This constant determines how many blocks we iterate between checks of our
121     // deadline when calling Heap::isPagedOut. Decreasing it will cause us to detect
122     // overstepping our deadline more quickly, while increasing it will cause
123     // our scan to run faster.
124     static const unsigned s_timeCheckResolution = 16;
125 
<span class="line-modified">126     static bool isMarked(const void*);</span>
127     static bool testAndSetMarked(HeapVersion, const void*);
128 
129     static size_t cellSize(const void*);
130 
131     void writeBarrier(const JSCell* from);
132     void writeBarrier(const JSCell* from, JSValue to);
133     void writeBarrier(const JSCell* from, JSCell* to);
134 
135     void writeBarrierWithoutFence(const JSCell* from);
136 
137     void mutatorFence();
138 
139     // Take this if you know that from-&gt;cellState() &lt; barrierThreshold.
140     JS_EXPORT_PRIVATE void writeBarrierSlowPath(const JSCell* from);
141 
<span class="line-modified">142     Heap(VM*, HeapType);</span>
143     ~Heap();
144     void lastChanceToFinalize();
145     void releaseDelayedReleasedObjects();
146 
<span class="line-modified">147     VM* vm() const;</span>
148 
149     MarkedSpace&amp; objectSpace() { return m_objectSpace; }
150     MachineThreads&amp; machineThreads() { return *m_machineThreads; }
151 
152     SlotVisitor&amp; collectorSlotVisitor() { return *m_collectorSlotVisitor; }
153 
154     JS_EXPORT_PRIVATE GCActivityCallback* fullActivityCallback();
155     JS_EXPORT_PRIVATE GCActivityCallback* edenActivityCallback();
156     JS_EXPORT_PRIVATE void setGarbageCollectionTimerEnabled(bool);
157 
158     JS_EXPORT_PRIVATE IncrementalSweeper&amp; sweeper();
159 
160     void addObserver(HeapObserver* observer) { m_observers.append(observer); }
161     void removeObserver(HeapObserver* observer) { m_observers.removeFirst(observer); }
162 
163     MutatorState mutatorState() const { return m_mutatorState; }
164     Optional&lt;CollectionScope&gt; collectionScope() const { return m_collectionScope; }
165     bool hasHeapAccess() const;
166     bool worldIsStopped() const;
167     bool worldIsRunning() const { return !worldIsStopped(); }
168 
169     // We&#39;re always busy on the collection threads. On the main thread, this returns true if we&#39;re
170     // helping heap.
171     JS_EXPORT_PRIVATE bool isCurrentThreadBusy();
172 
173     typedef void (*Finalizer)(JSCell*);
174     JS_EXPORT_PRIVATE void addFinalizer(JSCell*, Finalizer);
175 
176     void notifyIsSafeToCollect();
177     bool isSafeToCollect() const { return m_isSafeToCollect; }
178 
179     bool isShuttingDown() const { return m_isShuttingDown; }
180 
<span class="line-modified">181     JS_EXPORT_PRIVATE bool isHeapSnapshotting() const;</span>
182 
183     JS_EXPORT_PRIVATE void sweepSynchronously();
184 
185     bool shouldCollectHeuristic();
186 
187     // Queue up a collection. Returns immediately. This will not queue a collection if a collection
188     // of equal or greater strength exists. Full collections are stronger than WTF::nullopt collections
189     // and WTF::nullopt collections are stronger than Eden collections. WTF::nullopt means that the GC can
190     // choose Eden or Full. This implies that if you request a GC while that GC is ongoing, nothing
191     // will happen.
192     JS_EXPORT_PRIVATE void collectAsync(GCRequest = GCRequest());
193 
194     // Queue up a collection and wait for it to complete. This won&#39;t return until you get your own
195     // complete collection. For example, if there was an ongoing asynchronous collection at the time
196     // you called this, then this would wait for that one to complete and then trigger your
197     // collection and then return. In weird cases, there could be multiple GC requests in the backlog
198     // and this will wait for that backlog before running its GC and returning.
199     JS_EXPORT_PRIVATE void collectSync(GCRequest = GCRequest());
200 
201     JS_EXPORT_PRIVATE void collect(Synchronousness, GCRequest = GCRequest());
</pre>
<hr />
<pre>
262 
263     void deleteAllCodeBlocks(DeleteAllCodeEffort);
264     void deleteAllUnlinkedCodeBlocks(DeleteAllCodeEffort);
265 
266     void didAllocate(size_t);
267     bool isPagedOut(MonotonicTime deadline);
268 
269     const JITStubRoutineSet&amp; jitStubRoutines() { return *m_jitStubRoutines; }
270 
271     void addReference(JSCell*, ArrayBuffer*);
272 
273     bool isDeferred() const { return !!m_deferralDepth; }
274 
275     StructureIDTable&amp; structureIDTable() { return m_structureIDTable; }
276 
277     CodeBlockSet&amp; codeBlockSet() { return *m_codeBlocks; }
278 
279 #if USE(FOUNDATION)
280     template&lt;typename T&gt; void releaseSoon(RetainPtr&lt;T&gt;&amp;&amp;);
281 #endif
<span class="line-modified">282 #if USE(GLIB)</span>
283     void releaseSoon(std::unique_ptr&lt;JSCGLibWrapperObject&gt;&amp;&amp;);
284 #endif
285 
286     JS_EXPORT_PRIVATE void registerWeakGCMap(WeakGCMapBase* weakGCMap);
287     JS_EXPORT_PRIVATE void unregisterWeakGCMap(WeakGCMapBase* weakGCMap);
288 
289     void addLogicallyEmptyWeakBlock(WeakBlock*);
290 
291 #if ENABLE(RESOURCE_USAGE)
292     size_t blockBytesAllocated() const { return m_blockBytesAllocated; }
293 #endif
294 
295     void didAllocateBlock(size_t capacity);
296     void didFreeBlock(size_t capacity);
297 
298     bool mutatorShouldBeFenced() const { return m_mutatorShouldBeFenced; }
299     const bool* addressOfMutatorShouldBeFenced() const { return &amp;m_mutatorShouldBeFenced; }
300 
301     unsigned barrierThreshold() const { return m_barrierThreshold; }
302     const unsigned* addressOfBarrierThreshold() const { return &amp;m_barrierThreshold; }
</pre>
<hr />
<pre>
376 
377     JS_EXPORT_PRIVATE void addMarkingConstraint(std::unique_ptr&lt;MarkingConstraint&gt;);
378 
379     size_t numOpaqueRoots() const { return m_opaqueRoots.size(); }
380 
381     HeapVerifier* verifier() const { return m_verifier.get(); }
382 
383     void addHeapFinalizerCallback(const HeapFinalizerCallback&amp;);
384     void removeHeapFinalizerCallback(const HeapFinalizerCallback&amp;);
385 
386     void runTaskInParallel(RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt;);
387 
388     template&lt;typename Func&gt;
389     void runFunctionInParallel(const Func&amp; func)
390     {
391         runTaskInParallel(createSharedTask&lt;void(SlotVisitor&amp;)&gt;(func));
392     }
393 
394     template&lt;typename Func&gt;
395     void forEachSlotVisitor(const Func&amp;);
<span class="line-removed">396     unsigned numberOfSlotVisitors();</span>
397 
398     Seconds totalGCTime() const { return m_totalGCTime; }
399 
400     HashMap&lt;JSImmutableButterfly*, JSString*&gt; immutableButterflyToStringCache;
401 
402 private:
403     friend class AllocatingScope;
404     friend class CodeBlock;
405     friend class CollectingScope;
406     friend class DeferGC;
407     friend class DeferGCForAWhile;
408     friend class GCAwareJITStubRoutine;
409     friend class GCLogging;
410     friend class GCThread;
411     friend class HandleSet;
412     friend class HeapUtil;
413     friend class HeapVerifier;
414     friend class JITStubRoutine;
415     friend class LLIntOffsetsExtractor;
416     friend class MarkStackMergingConstraint;
417     friend class MarkedSpace;
418     friend class BlockDirectory;
419     friend class MarkedBlock;
420     friend class RunningScope;
421     friend class SlotVisitor;
422     friend class SpaceTimeMutatorScheduler;
423     friend class StochasticSpaceTimeMutatorScheduler;
424     friend class SweepingScope;
425     friend class IncrementalSweeper;
426     friend class VM;
427     friend class WeakSet;
428 
<span class="line-modified">429     class Thread;</span>
<span class="line-modified">430     friend class Thread;</span>
431 
432     static const size_t minExtraMemory = 256;
433 
434     class FinalizerOwner : public WeakHandleOwner {
435         void finalize(Handle&lt;Unknown&gt;, void* context) override;
436     };
437 
438     JS_EXPORT_PRIVATE bool isValidAllocation(size_t);
439     JS_EXPORT_PRIVATE void reportExtraMemoryAllocatedSlowCase(size_t);
440     JS_EXPORT_PRIVATE void deprecatedReportExtraMemorySlowCase(size_t);
441 
442     bool shouldCollectInCollectorThread(const AbstractLocker&amp;);
443     void collectInCollectorThread();
444 
445     void checkConn(GCConductor);
446 
447     enum class RunCurrentPhaseResult {
448         Finished,
449         Continue,
450         NeedCurrentThreadState
</pre>
<hr />
<pre>
514     void endMarking();
515 
516     void reapWeakHandles();
517     void pruneStaleEntriesFromWeakGCMaps();
518     void sweepArrayBuffers();
519     void snapshotUnswept();
520     void deleteSourceProviderCaches();
521     void notifyIncrementalSweeper();
522     void harvestWeakReferences();
523 
524     template&lt;typename CellType, typename CellSet&gt;
525     void finalizeMarkedUnconditionalFinalizers(CellSet&amp;);
526 
527     void finalizeUnconditionalFinalizers();
528 
529     void deleteUnmarkedCompiledCode();
530     JS_EXPORT_PRIVATE void addToRememberedSet(const JSCell*);
531     void updateAllocationLimits();
532     void didFinishCollection();
533     void resumeCompilerThreads();
<span class="line-modified">534     void gatherExtraHeapSnapshotData(HeapProfiler&amp;);</span>
535     void removeDeadHeapSnapshotNodes(HeapProfiler&amp;);
536     void finalize();
537     void sweepInFinalize();
538 
539     void sweepAllLogicallyEmptyWeakBlocks();
540     bool sweepNextLogicallyEmptyWeakBlock();
541 
542     bool shouldDoFullCollection();
543 
544     void incrementDeferralDepth();
545     void decrementDeferralDepth();
546     void decrementDeferralDepthAndGCIfNeeded();
547     JS_EXPORT_PRIVATE void decrementDeferralDepthAndGCIfNeededSlow();
548 
549     size_t visitCount();
550     size_t bytesVisited();
551 
552     void forEachCodeBlockImpl(const ScopedLambda&lt;void(CodeBlock*)&gt;&amp;);
553     void forEachCodeBlockIgnoringJITPlansImpl(const AbstractLocker&amp; codeBlockSetLocker, const ScopedLambda&lt;void(CodeBlock*)&gt;&amp;);
554 
</pre>
<hr />
<pre>
556 
557     void addCoreConstraints();
558 
559     enum class MemoryThresholdCallType {
560         Cached,
561         Direct
562     };
563 
564     bool overCriticalMemoryThreshold(MemoryThresholdCallType memoryThresholdCallType = MemoryThresholdCallType::Cached);
565 
566     template&lt;typename Func&gt;
567     void iterateExecutingAndCompilingCodeBlocks(const Func&amp;);
568 
569     template&lt;typename Func&gt;
570     void iterateExecutingAndCompilingCodeBlocksWithoutHoldingLocks(const Func&amp;);
571 
572     void assertMarkStacksEmpty();
573 
574     void setBonusVisitorTask(RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt;);
575 


576     static bool useGenerationalGC();
577     static bool shouldSweepSynchronously();
578 
579     const HeapType m_heapType;
580     MutatorState m_mutatorState { MutatorState::Running };
581     const size_t m_ramSize;
582     const size_t m_minBytesPerCycle;
583     size_t m_sizeAfterLastCollect { 0 };
584     size_t m_sizeAfterLastFullCollect { 0 };
585     size_t m_sizeBeforeLastFullCollect { 0 };
586     size_t m_sizeAfterLastEdenCollect { 0 };
587     size_t m_sizeBeforeLastEdenCollect { 0 };
588 
589     size_t m_bytesAllocatedThisCycle { 0 };
590     size_t m_bytesAbandonedSinceLastFullCollect { 0 };
591     size_t m_maxEdenSize;
592     size_t m_maxEdenSizeWhenCritical;
593     size_t m_maxHeapSize;
594     size_t m_totalBytesVisited { 0 };
595     size_t m_totalBytesVisitedThisCycle { 0 };
</pre>
<hr />
<pre>
624 
625     // We pool the slot visitors used by parallel marking threads. It&#39;s useful to be able to
626     // enumerate over them, and it&#39;s useful to have them cache some small amount of memory from
627     // one GC to the next. GC marking threads claim these at the start of marking, and return
628     // them at the end.
629     Vector&lt;std::unique_ptr&lt;SlotVisitor&gt;&gt; m_parallelSlotVisitors;
630     Vector&lt;SlotVisitor*&gt; m_availableParallelSlotVisitors;
631 
632     HandleSet m_handleSet;
633     std::unique_ptr&lt;CodeBlockSet&gt; m_codeBlocks;
634     std::unique_ptr&lt;JITStubRoutineSet&gt; m_jitStubRoutines;
635     FinalizerOwner m_finalizerOwner;
636 
637     Lock m_parallelSlotVisitorLock;
638     bool m_isSafeToCollect { false };
639     bool m_isShuttingDown { false };
640     bool m_mutatorShouldBeFenced { Options::forceFencedBarrier() };
641 
642     unsigned m_barrierThreshold { Options::forceFencedBarrier() ? tautologicalThreshold : blackThreshold };
643 
<span class="line-modified">644     VM* m_vm;</span>
645     Seconds m_lastFullGCLength { 10_ms };
646     Seconds m_lastEdenGCLength { 10_ms };
647 
648     Vector&lt;WeakBlock*&gt; m_logicallyEmptyWeakBlocks;
649     size_t m_indexOfNextLogicallyEmptyWeakBlockToSweep { WTF::notFound };
650 
651     RefPtr&lt;FullGCActivityCallback&gt; m_fullActivityCallback;
652     RefPtr&lt;GCActivityCallback&gt; m_edenActivityCallback;
653     Ref&lt;IncrementalSweeper&gt; m_sweeper;
654     Ref&lt;StopIfNecessaryTimer&gt; m_stopIfNecessaryTimer;
655 
656     Vector&lt;HeapObserver*&gt; m_observers;
657 
658     Vector&lt;HeapFinalizerCallback&gt; m_heapFinalizerCallbacks;
659 
660     std::unique_ptr&lt;HeapVerifier&gt; m_verifier;
661 
662 #if USE(FOUNDATION)
663     Vector&lt;RetainPtr&lt;CFTypeRef&gt;&gt; m_delayedReleaseObjects;
664     unsigned m_delayedReleaseRecursionCount { 0 };
665 #endif
<span class="line-modified">666 #if USE(GLIB)</span>
667     Vector&lt;std::unique_ptr&lt;JSCGLibWrapperObject&gt;&gt; m_delayedReleaseObjects;
668     unsigned m_delayedReleaseRecursionCount { 0 };
669 #endif
670     unsigned m_deferralDepth { 0 };
671 
672     HashSet&lt;WeakGCMapBase*&gt; m_weakGCMaps;
673 
674     std::unique_ptr&lt;MarkStackArray&gt; m_sharedCollectorMarkStack;
675     std::unique_ptr&lt;MarkStackArray&gt; m_sharedMutatorMarkStack;
676     unsigned m_numberOfActiveParallelMarkers { 0 };
677     unsigned m_numberOfWaitingParallelMarkers { 0 };
678 
679     ConcurrentPtrHashSet m_opaqueRoots;
680     static const size_t s_blockFragmentLength = 32;
681 
682     ParallelHelperClient m_helperClient;
683     RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt; m_bonusVisitorTask;
684 
685 #if ENABLE(RESOURCE_USAGE)
686     size_t m_blockBytesAllocated { 0 };
</pre>
<hr />
<pre>
696     static const unsigned needFinalizeBit = 1u &lt;&lt; 4u;
697     static const unsigned mutatorWaitingBit = 1u &lt;&lt; 5u; // Allows the mutator to use this as a condition variable.
698     Atomic&lt;unsigned&gt; m_worldState;
699     bool m_worldIsStopped { false };
700     Lock m_visitRaceLock;
701     Lock m_markingMutex;
702     Condition m_markingConditionVariable;
703 
704     MonotonicTime m_beforeGC;
705     MonotonicTime m_afterGC;
706     MonotonicTime m_stopTime;
707 
708     Deque&lt;GCRequest&gt; m_requests;
709     GCRequest m_currentRequest;
710     Ticket m_lastServedTicket { 0 };
711     Ticket m_lastGrantedTicket { 0 };
712 
713     CollectorPhase m_lastPhase { CollectorPhase::NotRunning };
714     CollectorPhase m_currentPhase { CollectorPhase::NotRunning };
715     CollectorPhase m_nextPhase { CollectorPhase::NotRunning };

716     bool m_threadShouldStop { false };
717     bool m_threadIsStopping { false };
718     bool m_mutatorDidRun { true };
719     bool m_didDeferGCWork { false };
720     bool m_shouldStopCollectingContinuously { false };
721 
722     uint64_t m_mutatorExecutionVersion { 0 };
723     uint64_t m_phaseVersion { 0 };
724     Box&lt;Lock&gt; m_threadLock;
725     Ref&lt;AutomaticThreadCondition&gt; m_threadCondition; // The mutator must not wait on this. It would cause a deadlock.
726     RefPtr&lt;AutomaticThread&gt; m_thread;
727 
<span class="line-modified">728     RefPtr&lt;WTF::Thread&gt; m_collectContinuouslyThread { nullptr };</span>
729 
730     MonotonicTime m_lastGCStartTime;
731     MonotonicTime m_lastGCEndTime;
732     MonotonicTime m_currentGCStartTime;
733     Seconds m_totalGCTime;
734 
735     uintptr_t m_barriersExecuted { 0 };
736 
737     CurrentThreadState* m_currentThreadState { nullptr };
<span class="line-modified">738     WTF::Thread* m_currentThread { nullptr }; // It&#39;s OK if this becomes a dangling pointer.</span>
739 
740 #if PLATFORM(IOS_FAMILY)
741     unsigned m_precentAvailableMemoryCachedCallCount;
742     bool m_overCriticalMemoryThreshold;
743 #endif
744 
745     bool m_parallelMarkersShouldExit { false };
746     Lock m_collectContinuouslyLock;
747     Condition m_collectContinuouslyCondition;
748 };
749 
750 } // namespace JSC
</pre>
</td>
<td>
<hr />
<pre>
 69 class JSImmutableButterfly;
 70 class JSValue;
 71 class LLIntOffsetsExtractor;
 72 class MachineThreads;
 73 class MarkStackArray;
 74 class MarkStackMergingConstraint;
 75 class BlockDirectory;
 76 class MarkedArgumentBuffer;
 77 class MarkingConstraint;
 78 class MarkingConstraintSet;
 79 class MutatorScheduler;
 80 class RunningScope;
 81 class SlotVisitor;
 82 class SpaceTimeMutatorScheduler;
 83 class StopIfNecessaryTimer;
 84 class SweepingScope;
 85 class VM;
 86 class WeakGCMapBase;
 87 struct CurrentThreadState;
 88 
<span class="line-modified"> 89 #ifdef JSC_GLIB_API_ENABLED</span>
 90 class JSCGLibWrapperObject;
 91 #endif
 92 
 93 namespace DFG {
 94 class SpeculativeJIT;
 95 class Worklist;
 96 }
 97 
 98 #if !ASSERT_DISABLED
 99 #define ENABLE_DFG_DOES_GC_VALIDATION 1
100 #else
101 #define ENABLE_DFG_DOES_GC_VALIDATION 0
102 #endif
103 constexpr bool validateDFGDoesGC = ENABLE_DFG_DOES_GC_VALIDATION;
104 
105 typedef HashCountedSet&lt;JSCell*&gt; ProtectCountSet;
106 typedef HashCountedSet&lt;const char*&gt; TypeCountSet;
107 
108 enum HeapType { SmallHeap, LargeHeap };
109 
110 class HeapUtil;
111 
112 class Heap {
113     WTF_MAKE_NONCOPYABLE(Heap);
114 public:
115     friend class JIT;
116     friend class DFG::SpeculativeJIT;
117     static Heap* heap(const JSValue); // 0 for immediate values
118     static Heap* heap(const HeapCell*);
119 
120     // This constant determines how many blocks we iterate between checks of our
121     // deadline when calling Heap::isPagedOut. Decreasing it will cause us to detect
122     // overstepping our deadline more quickly, while increasing it will cause
123     // our scan to run faster.
124     static const unsigned s_timeCheckResolution = 16;
125 
<span class="line-modified">126     bool isMarked(const void*);</span>
127     static bool testAndSetMarked(HeapVersion, const void*);
128 
129     static size_t cellSize(const void*);
130 
131     void writeBarrier(const JSCell* from);
132     void writeBarrier(const JSCell* from, JSValue to);
133     void writeBarrier(const JSCell* from, JSCell* to);
134 
135     void writeBarrierWithoutFence(const JSCell* from);
136 
137     void mutatorFence();
138 
139     // Take this if you know that from-&gt;cellState() &lt; barrierThreshold.
140     JS_EXPORT_PRIVATE void writeBarrierSlowPath(const JSCell* from);
141 
<span class="line-modified">142     Heap(VM&amp;, HeapType);</span>
143     ~Heap();
144     void lastChanceToFinalize();
145     void releaseDelayedReleasedObjects();
146 
<span class="line-modified">147     VM&amp; vm() const;</span>
148 
149     MarkedSpace&amp; objectSpace() { return m_objectSpace; }
150     MachineThreads&amp; machineThreads() { return *m_machineThreads; }
151 
152     SlotVisitor&amp; collectorSlotVisitor() { return *m_collectorSlotVisitor; }
153 
154     JS_EXPORT_PRIVATE GCActivityCallback* fullActivityCallback();
155     JS_EXPORT_PRIVATE GCActivityCallback* edenActivityCallback();
156     JS_EXPORT_PRIVATE void setGarbageCollectionTimerEnabled(bool);
157 
158     JS_EXPORT_PRIVATE IncrementalSweeper&amp; sweeper();
159 
160     void addObserver(HeapObserver* observer) { m_observers.append(observer); }
161     void removeObserver(HeapObserver* observer) { m_observers.removeFirst(observer); }
162 
163     MutatorState mutatorState() const { return m_mutatorState; }
164     Optional&lt;CollectionScope&gt; collectionScope() const { return m_collectionScope; }
165     bool hasHeapAccess() const;
166     bool worldIsStopped() const;
167     bool worldIsRunning() const { return !worldIsStopped(); }
168 
169     // We&#39;re always busy on the collection threads. On the main thread, this returns true if we&#39;re
170     // helping heap.
171     JS_EXPORT_PRIVATE bool isCurrentThreadBusy();
172 
173     typedef void (*Finalizer)(JSCell*);
174     JS_EXPORT_PRIVATE void addFinalizer(JSCell*, Finalizer);
175 
176     void notifyIsSafeToCollect();
177     bool isSafeToCollect() const { return m_isSafeToCollect; }
178 
179     bool isShuttingDown() const { return m_isShuttingDown; }
180 
<span class="line-modified">181     JS_EXPORT_PRIVATE bool isAnalyzingHeap() const;</span>
182 
183     JS_EXPORT_PRIVATE void sweepSynchronously();
184 
185     bool shouldCollectHeuristic();
186 
187     // Queue up a collection. Returns immediately. This will not queue a collection if a collection
188     // of equal or greater strength exists. Full collections are stronger than WTF::nullopt collections
189     // and WTF::nullopt collections are stronger than Eden collections. WTF::nullopt means that the GC can
190     // choose Eden or Full. This implies that if you request a GC while that GC is ongoing, nothing
191     // will happen.
192     JS_EXPORT_PRIVATE void collectAsync(GCRequest = GCRequest());
193 
194     // Queue up a collection and wait for it to complete. This won&#39;t return until you get your own
195     // complete collection. For example, if there was an ongoing asynchronous collection at the time
196     // you called this, then this would wait for that one to complete and then trigger your
197     // collection and then return. In weird cases, there could be multiple GC requests in the backlog
198     // and this will wait for that backlog before running its GC and returning.
199     JS_EXPORT_PRIVATE void collectSync(GCRequest = GCRequest());
200 
201     JS_EXPORT_PRIVATE void collect(Synchronousness, GCRequest = GCRequest());
</pre>
<hr />
<pre>
262 
263     void deleteAllCodeBlocks(DeleteAllCodeEffort);
264     void deleteAllUnlinkedCodeBlocks(DeleteAllCodeEffort);
265 
266     void didAllocate(size_t);
267     bool isPagedOut(MonotonicTime deadline);
268 
269     const JITStubRoutineSet&amp; jitStubRoutines() { return *m_jitStubRoutines; }
270 
271     void addReference(JSCell*, ArrayBuffer*);
272 
273     bool isDeferred() const { return !!m_deferralDepth; }
274 
275     StructureIDTable&amp; structureIDTable() { return m_structureIDTable; }
276 
277     CodeBlockSet&amp; codeBlockSet() { return *m_codeBlocks; }
278 
279 #if USE(FOUNDATION)
280     template&lt;typename T&gt; void releaseSoon(RetainPtr&lt;T&gt;&amp;&amp;);
281 #endif
<span class="line-modified">282 #ifdef JSC_GLIB_API_ENABLED</span>
283     void releaseSoon(std::unique_ptr&lt;JSCGLibWrapperObject&gt;&amp;&amp;);
284 #endif
285 
286     JS_EXPORT_PRIVATE void registerWeakGCMap(WeakGCMapBase* weakGCMap);
287     JS_EXPORT_PRIVATE void unregisterWeakGCMap(WeakGCMapBase* weakGCMap);
288 
289     void addLogicallyEmptyWeakBlock(WeakBlock*);
290 
291 #if ENABLE(RESOURCE_USAGE)
292     size_t blockBytesAllocated() const { return m_blockBytesAllocated; }
293 #endif
294 
295     void didAllocateBlock(size_t capacity);
296     void didFreeBlock(size_t capacity);
297 
298     bool mutatorShouldBeFenced() const { return m_mutatorShouldBeFenced; }
299     const bool* addressOfMutatorShouldBeFenced() const { return &amp;m_mutatorShouldBeFenced; }
300 
301     unsigned barrierThreshold() const { return m_barrierThreshold; }
302     const unsigned* addressOfBarrierThreshold() const { return &amp;m_barrierThreshold; }
</pre>
<hr />
<pre>
376 
377     JS_EXPORT_PRIVATE void addMarkingConstraint(std::unique_ptr&lt;MarkingConstraint&gt;);
378 
379     size_t numOpaqueRoots() const { return m_opaqueRoots.size(); }
380 
381     HeapVerifier* verifier() const { return m_verifier.get(); }
382 
383     void addHeapFinalizerCallback(const HeapFinalizerCallback&amp;);
384     void removeHeapFinalizerCallback(const HeapFinalizerCallback&amp;);
385 
386     void runTaskInParallel(RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt;);
387 
388     template&lt;typename Func&gt;
389     void runFunctionInParallel(const Func&amp; func)
390     {
391         runTaskInParallel(createSharedTask&lt;void(SlotVisitor&amp;)&gt;(func));
392     }
393 
394     template&lt;typename Func&gt;
395     void forEachSlotVisitor(const Func&amp;);

396 
397     Seconds totalGCTime() const { return m_totalGCTime; }
398 
399     HashMap&lt;JSImmutableButterfly*, JSString*&gt; immutableButterflyToStringCache;
400 
401 private:
402     friend class AllocatingScope;
403     friend class CodeBlock;
404     friend class CollectingScope;
405     friend class DeferGC;
406     friend class DeferGCForAWhile;
407     friend class GCAwareJITStubRoutine;
408     friend class GCLogging;
409     friend class GCThread;
410     friend class HandleSet;
411     friend class HeapUtil;
412     friend class HeapVerifier;
413     friend class JITStubRoutine;
414     friend class LLIntOffsetsExtractor;
415     friend class MarkStackMergingConstraint;
416     friend class MarkedSpace;
417     friend class BlockDirectory;
418     friend class MarkedBlock;
419     friend class RunningScope;
420     friend class SlotVisitor;
421     friend class SpaceTimeMutatorScheduler;
422     friend class StochasticSpaceTimeMutatorScheduler;
423     friend class SweepingScope;
424     friend class IncrementalSweeper;
425     friend class VM;
426     friend class WeakSet;
427 
<span class="line-modified">428     class HeapThread;</span>
<span class="line-modified">429     friend class HeapThread;</span>
430 
431     static const size_t minExtraMemory = 256;
432 
433     class FinalizerOwner : public WeakHandleOwner {
434         void finalize(Handle&lt;Unknown&gt;, void* context) override;
435     };
436 
437     JS_EXPORT_PRIVATE bool isValidAllocation(size_t);
438     JS_EXPORT_PRIVATE void reportExtraMemoryAllocatedSlowCase(size_t);
439     JS_EXPORT_PRIVATE void deprecatedReportExtraMemorySlowCase(size_t);
440 
441     bool shouldCollectInCollectorThread(const AbstractLocker&amp;);
442     void collectInCollectorThread();
443 
444     void checkConn(GCConductor);
445 
446     enum class RunCurrentPhaseResult {
447         Finished,
448         Continue,
449         NeedCurrentThreadState
</pre>
<hr />
<pre>
513     void endMarking();
514 
515     void reapWeakHandles();
516     void pruneStaleEntriesFromWeakGCMaps();
517     void sweepArrayBuffers();
518     void snapshotUnswept();
519     void deleteSourceProviderCaches();
520     void notifyIncrementalSweeper();
521     void harvestWeakReferences();
522 
523     template&lt;typename CellType, typename CellSet&gt;
524     void finalizeMarkedUnconditionalFinalizers(CellSet&amp;);
525 
526     void finalizeUnconditionalFinalizers();
527 
528     void deleteUnmarkedCompiledCode();
529     JS_EXPORT_PRIVATE void addToRememberedSet(const JSCell*);
530     void updateAllocationLimits();
531     void didFinishCollection();
532     void resumeCompilerThreads();
<span class="line-modified">533     void gatherExtraHeapData(HeapProfiler&amp;);</span>
534     void removeDeadHeapSnapshotNodes(HeapProfiler&amp;);
535     void finalize();
536     void sweepInFinalize();
537 
538     void sweepAllLogicallyEmptyWeakBlocks();
539     bool sweepNextLogicallyEmptyWeakBlock();
540 
541     bool shouldDoFullCollection();
542 
543     void incrementDeferralDepth();
544     void decrementDeferralDepth();
545     void decrementDeferralDepthAndGCIfNeeded();
546     JS_EXPORT_PRIVATE void decrementDeferralDepthAndGCIfNeededSlow();
547 
548     size_t visitCount();
549     size_t bytesVisited();
550 
551     void forEachCodeBlockImpl(const ScopedLambda&lt;void(CodeBlock*)&gt;&amp;);
552     void forEachCodeBlockIgnoringJITPlansImpl(const AbstractLocker&amp; codeBlockSetLocker, const ScopedLambda&lt;void(CodeBlock*)&gt;&amp;);
553 
</pre>
<hr />
<pre>
555 
556     void addCoreConstraints();
557 
558     enum class MemoryThresholdCallType {
559         Cached,
560         Direct
561     };
562 
563     bool overCriticalMemoryThreshold(MemoryThresholdCallType memoryThresholdCallType = MemoryThresholdCallType::Cached);
564 
565     template&lt;typename Func&gt;
566     void iterateExecutingAndCompilingCodeBlocks(const Func&amp;);
567 
568     template&lt;typename Func&gt;
569     void iterateExecutingAndCompilingCodeBlocksWithoutHoldingLocks(const Func&amp;);
570 
571     void assertMarkStacksEmpty();
572 
573     void setBonusVisitorTask(RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt;);
574 
<span class="line-added">575     void dumpHeapStatisticsAtVMDestruction();</span>
<span class="line-added">576 </span>
577     static bool useGenerationalGC();
578     static bool shouldSweepSynchronously();
579 
580     const HeapType m_heapType;
581     MutatorState m_mutatorState { MutatorState::Running };
582     const size_t m_ramSize;
583     const size_t m_minBytesPerCycle;
584     size_t m_sizeAfterLastCollect { 0 };
585     size_t m_sizeAfterLastFullCollect { 0 };
586     size_t m_sizeBeforeLastFullCollect { 0 };
587     size_t m_sizeAfterLastEdenCollect { 0 };
588     size_t m_sizeBeforeLastEdenCollect { 0 };
589 
590     size_t m_bytesAllocatedThisCycle { 0 };
591     size_t m_bytesAbandonedSinceLastFullCollect { 0 };
592     size_t m_maxEdenSize;
593     size_t m_maxEdenSizeWhenCritical;
594     size_t m_maxHeapSize;
595     size_t m_totalBytesVisited { 0 };
596     size_t m_totalBytesVisitedThisCycle { 0 };
</pre>
<hr />
<pre>
625 
626     // We pool the slot visitors used by parallel marking threads. It&#39;s useful to be able to
627     // enumerate over them, and it&#39;s useful to have them cache some small amount of memory from
628     // one GC to the next. GC marking threads claim these at the start of marking, and return
629     // them at the end.
630     Vector&lt;std::unique_ptr&lt;SlotVisitor&gt;&gt; m_parallelSlotVisitors;
631     Vector&lt;SlotVisitor*&gt; m_availableParallelSlotVisitors;
632 
633     HandleSet m_handleSet;
634     std::unique_ptr&lt;CodeBlockSet&gt; m_codeBlocks;
635     std::unique_ptr&lt;JITStubRoutineSet&gt; m_jitStubRoutines;
636     FinalizerOwner m_finalizerOwner;
637 
638     Lock m_parallelSlotVisitorLock;
639     bool m_isSafeToCollect { false };
640     bool m_isShuttingDown { false };
641     bool m_mutatorShouldBeFenced { Options::forceFencedBarrier() };
642 
643     unsigned m_barrierThreshold { Options::forceFencedBarrier() ? tautologicalThreshold : blackThreshold };
644 
<span class="line-modified">645     VM&amp; m_vm;</span>
646     Seconds m_lastFullGCLength { 10_ms };
647     Seconds m_lastEdenGCLength { 10_ms };
648 
649     Vector&lt;WeakBlock*&gt; m_logicallyEmptyWeakBlocks;
650     size_t m_indexOfNextLogicallyEmptyWeakBlockToSweep { WTF::notFound };
651 
652     RefPtr&lt;FullGCActivityCallback&gt; m_fullActivityCallback;
653     RefPtr&lt;GCActivityCallback&gt; m_edenActivityCallback;
654     Ref&lt;IncrementalSweeper&gt; m_sweeper;
655     Ref&lt;StopIfNecessaryTimer&gt; m_stopIfNecessaryTimer;
656 
657     Vector&lt;HeapObserver*&gt; m_observers;
658 
659     Vector&lt;HeapFinalizerCallback&gt; m_heapFinalizerCallbacks;
660 
661     std::unique_ptr&lt;HeapVerifier&gt; m_verifier;
662 
663 #if USE(FOUNDATION)
664     Vector&lt;RetainPtr&lt;CFTypeRef&gt;&gt; m_delayedReleaseObjects;
665     unsigned m_delayedReleaseRecursionCount { 0 };
666 #endif
<span class="line-modified">667 #ifdef JSC_GLIB_API_ENABLED</span>
668     Vector&lt;std::unique_ptr&lt;JSCGLibWrapperObject&gt;&gt; m_delayedReleaseObjects;
669     unsigned m_delayedReleaseRecursionCount { 0 };
670 #endif
671     unsigned m_deferralDepth { 0 };
672 
673     HashSet&lt;WeakGCMapBase*&gt; m_weakGCMaps;
674 
675     std::unique_ptr&lt;MarkStackArray&gt; m_sharedCollectorMarkStack;
676     std::unique_ptr&lt;MarkStackArray&gt; m_sharedMutatorMarkStack;
677     unsigned m_numberOfActiveParallelMarkers { 0 };
678     unsigned m_numberOfWaitingParallelMarkers { 0 };
679 
680     ConcurrentPtrHashSet m_opaqueRoots;
681     static const size_t s_blockFragmentLength = 32;
682 
683     ParallelHelperClient m_helperClient;
684     RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt; m_bonusVisitorTask;
685 
686 #if ENABLE(RESOURCE_USAGE)
687     size_t m_blockBytesAllocated { 0 };
</pre>
<hr />
<pre>
697     static const unsigned needFinalizeBit = 1u &lt;&lt; 4u;
698     static const unsigned mutatorWaitingBit = 1u &lt;&lt; 5u; // Allows the mutator to use this as a condition variable.
699     Atomic&lt;unsigned&gt; m_worldState;
700     bool m_worldIsStopped { false };
701     Lock m_visitRaceLock;
702     Lock m_markingMutex;
703     Condition m_markingConditionVariable;
704 
705     MonotonicTime m_beforeGC;
706     MonotonicTime m_afterGC;
707     MonotonicTime m_stopTime;
708 
709     Deque&lt;GCRequest&gt; m_requests;
710     GCRequest m_currentRequest;
711     Ticket m_lastServedTicket { 0 };
712     Ticket m_lastGrantedTicket { 0 };
713 
714     CollectorPhase m_lastPhase { CollectorPhase::NotRunning };
715     CollectorPhase m_currentPhase { CollectorPhase::NotRunning };
716     CollectorPhase m_nextPhase { CollectorPhase::NotRunning };
<span class="line-added">717     bool m_collectorThreadIsRunning { false };</span>
718     bool m_threadShouldStop { false };
719     bool m_threadIsStopping { false };
720     bool m_mutatorDidRun { true };
721     bool m_didDeferGCWork { false };
722     bool m_shouldStopCollectingContinuously { false };
723 
724     uint64_t m_mutatorExecutionVersion { 0 };
725     uint64_t m_phaseVersion { 0 };
726     Box&lt;Lock&gt; m_threadLock;
727     Ref&lt;AutomaticThreadCondition&gt; m_threadCondition; // The mutator must not wait on this. It would cause a deadlock.
728     RefPtr&lt;AutomaticThread&gt; m_thread;
729 
<span class="line-modified">730     RefPtr&lt;Thread&gt; m_collectContinuouslyThread { nullptr };</span>
731 
732     MonotonicTime m_lastGCStartTime;
733     MonotonicTime m_lastGCEndTime;
734     MonotonicTime m_currentGCStartTime;
735     Seconds m_totalGCTime;
736 
737     uintptr_t m_barriersExecuted { 0 };
738 
739     CurrentThreadState* m_currentThreadState { nullptr };
<span class="line-modified">740     Thread* m_currentThread { nullptr }; // It&#39;s OK if this becomes a dangling pointer.</span>
741 
742 #if PLATFORM(IOS_FAMILY)
743     unsigned m_precentAvailableMemoryCachedCallCount;
744     bool m_overCriticalMemoryThreshold;
745 #endif
746 
747     bool m_parallelMarkersShouldExit { false };
748     Lock m_collectContinuouslyLock;
749     Condition m_collectContinuouslyCondition;
750 };
751 
752 } // namespace JSC
</pre>
</td>
</tr>
</table>
<center><a href="Heap.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="HeapCell.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>