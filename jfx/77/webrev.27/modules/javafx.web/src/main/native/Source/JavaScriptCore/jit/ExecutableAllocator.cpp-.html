<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/ExecutableAllocator.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2008-2018 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;ExecutableAllocator.h&quot;
 28 
 29 #if ENABLE(JIT)
 30 
 31 #include &quot;CodeProfiling.h&quot;
 32 #include &quot;ExecutableAllocationFuzz.h&quot;
 33 #include &quot;JSCInlines.h&quot;
 34 #include &lt;wtf/MetaAllocator.h&gt;
 35 #include &lt;wtf/PageReservation.h&gt;
 36 
 37 #if OS(DARWIN)
 38 #include &lt;sys/mman.h&gt;
 39 #endif
 40 
 41 #if PLATFORM(IOS_FAMILY)
 42 #include &lt;wtf/cocoa/Entitlements.h&gt;
 43 #endif
 44 
 45 #include &quot;LinkBuffer.h&quot;
 46 #include &quot;MacroAssembler.h&quot;
 47 
 48 #if PLATFORM(COCOA)
 49 #define HAVE_REMAP_JIT 1
 50 #endif
 51 
 52 #if HAVE(REMAP_JIT)
 53 #if CPU(ARM64) &amp;&amp; PLATFORM(IOS_FAMILY)
 54 #define USE_EXECUTE_ONLY_JIT_WRITE_FUNCTION 1
 55 #endif
 56 #endif
 57 
 58 #if OS(DARWIN)
 59 #include &lt;mach/mach.h&gt;
 60 extern &quot;C&quot; {
 61     /* Routine mach_vm_remap */
 62 #ifdef mig_external
 63     mig_external
 64 #else
 65     extern
 66 #endif /* mig_external */
 67     kern_return_t mach_vm_remap
 68     (
 69      vm_map_t target_task,
 70      mach_vm_address_t *target_address,
 71      mach_vm_size_t size,
 72      mach_vm_offset_t mask,
 73      int flags,
 74      vm_map_t src_task,
 75      mach_vm_address_t src_address,
 76      boolean_t copy,
 77      vm_prot_t *cur_protection,
 78      vm_prot_t *max_protection,
 79      vm_inherit_t inheritance
 80      );
 81 }
 82 
 83 #endif
 84 
 85 namespace JSC {
 86 
 87 using namespace WTF;
 88 
 89 #if defined(FIXED_EXECUTABLE_MEMORY_POOL_SIZE_IN_MB) &amp;&amp; FIXED_EXECUTABLE_MEMORY_POOL_SIZE_IN_MB &gt; 0
 90 static const size_t fixedExecutableMemoryPoolSize = FIXED_EXECUTABLE_MEMORY_POOL_SIZE_IN_MB * 1024 * 1024;
 91 #elif CPU(ARM)
 92 static const size_t fixedExecutableMemoryPoolSize = 16 * 1024 * 1024;
 93 #elif CPU(ARM64)
 94 static const size_t fixedExecutableMemoryPoolSize = 128 * 1024 * 1024;
 95 #elif CPU(X86_64)
 96 static const size_t fixedExecutableMemoryPoolSize = 1024 * 1024 * 1024;
 97 #else
 98 static const size_t fixedExecutableMemoryPoolSize = 32 * 1024 * 1024;
 99 #endif
100 
101 #if CPU(ARM)
102 static const double executablePoolReservationFraction = 0.15;
103 #else
104 static const double executablePoolReservationFraction = 0.25;
105 #endif
106 
107 #if ENABLE(SEPARATED_WX_HEAP)
108 JS_EXPORT_PRIVATE bool useFastPermisionsJITCopy { false };
109 JS_EXPORT_PRIVATE JITWriteSeparateHeapsFunction jitWriteSeparateHeapsFunction;
110 #endif
111 
112 #if !USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION) &amp;&amp; HAVE(REMAP_JIT)
113 static uintptr_t startOfFixedWritableMemoryPool;
114 #endif
115 
116 class FixedVMPoolExecutableAllocator;
117 static FixedVMPoolExecutableAllocator* allocator = nullptr;
118 static ExecutableAllocator* executableAllocator = nullptr;
119 
120 static bool s_isJITEnabled = true;
121 static bool isJITEnabled()
122 {
123 #if PLATFORM(IOS_FAMILY) &amp;&amp; (CPU(ARM64) || CPU(ARM))
124     return processHasEntitlement(&quot;dynamic-codesigning&quot;) &amp;&amp; s_isJITEnabled;
125 #else
126     return s_isJITEnabled;
127 #endif
128 }
129 
130 void ExecutableAllocator::setJITEnabled(bool enabled)
131 {
132     ASSERT(!allocator);
133     if (s_isJITEnabled == enabled)
134         return;
135 
136     s_isJITEnabled = enabled;
137 
138 #if PLATFORM(IOS_FAMILY) &amp;&amp; (CPU(ARM64) || CPU(ARM))
139     if (!enabled) {
140         constexpr size_t size = 1;
141         constexpr int protection = PROT_READ | PROT_WRITE | PROT_EXEC;
142         constexpr int flags = MAP_PRIVATE | MAP_ANON | MAP_JIT;
143         constexpr int fd = OSAllocator::JSJITCodePages;
144         void* allocation = mmap(nullptr, size, protection, flags, fd, 0);
145         const void* executableMemoryAllocationFailure = reinterpret_cast&lt;void*&gt;(-1);
146         RELEASE_ASSERT_WITH_MESSAGE(allocation &amp;&amp; allocation != executableMemoryAllocationFailure, &quot;We should not have allocated executable memory before disabling the JIT.&quot;);
147         RELEASE_ASSERT_WITH_MESSAGE(!munmap(allocation, size), &quot;Unmapping executable memory should succeed so we do not have any executable memory in the address space&quot;);
148         RELEASE_ASSERT_WITH_MESSAGE(mmap(nullptr, size, protection, flags, fd, 0) == executableMemoryAllocationFailure, &quot;Allocating executable memory should fail after setJITEnabled(false) is called.&quot;);
149     }
150 #endif
151 }
152 
153 class FixedVMPoolExecutableAllocator : public MetaAllocator {
154     WTF_MAKE_FAST_ALLOCATED;
155 public:
156     FixedVMPoolExecutableAllocator()
157         : MetaAllocator(jitAllocationGranule) // round up all allocations to 32 bytes
158     {
159         if (!isJITEnabled())
160             return;
161 
162         size_t reservationSize;
163         if (Options::jitMemoryReservationSize())
164             reservationSize = Options::jitMemoryReservationSize();
165         else
166             reservationSize = fixedExecutableMemoryPoolSize;
167         reservationSize = std::max(roundUpToMultipleOf(pageSize(), reservationSize), pageSize() * 2);
168 
169         auto tryCreatePageReservation = [] (size_t reservationSize) {
170 #if OS(LINUX)
171             // If we use uncommitted reservation, mmap operation is recorded with small page size in perf command&#39;s output.
172             // This makes the following JIT code logging broken and some of JIT code is not recorded correctly.
173             // To avoid this problem, we use committed reservation if we need perf JITDump logging.
174             if (Options::logJITCodeForPerf())
175                 return PageReservation::reserveAndCommitWithGuardPages(reservationSize, OSAllocator::JSJITCodePages, EXECUTABLE_POOL_WRITABLE, true);
176 #endif
177             return PageReservation::reserveWithGuardPages(reservationSize, OSAllocator::JSJITCodePages, EXECUTABLE_POOL_WRITABLE, true);
178         };
179 
180         m_reservation = tryCreatePageReservation(reservationSize);
181         if (m_reservation) {
182             ASSERT(m_reservation.size() == reservationSize);
183             void* reservationBase = m_reservation.base();
184 
185 #if ENABLE(FAST_JIT_PERMISSIONS) &amp;&amp; !ENABLE(SEPARATED_WX_HEAP)
186             RELEASE_ASSERT(os_thread_self_restrict_rwx_is_supported());
187             os_thread_self_restrict_rwx_to_rx();
188 
189 #else // not ENABLE(FAST_JIT_PERMISSIONS) or ENABLE(SEPARATED_WX_HEAP)
190 #if ENABLE(FAST_JIT_PERMISSIONS)
191             if (os_thread_self_restrict_rwx_is_supported()) {
192                 useFastPermisionsJITCopy = true;
193                 os_thread_self_restrict_rwx_to_rx();
194             } else
195 #endif
196             if (Options::useSeparatedWXHeap()) {
197                 // First page of our JIT allocation is reserved.
198                 ASSERT(reservationSize &gt;= pageSize() * 2);
199                 reservationBase = (void*)((uintptr_t)reservationBase + pageSize());
200                 reservationSize -= pageSize();
201                 initializeSeparatedWXHeaps(m_reservation.base(), pageSize(), reservationBase, reservationSize);
202             }
203 #endif // not ENABLE(FAST_JIT_PERMISSIONS) or ENABLE(SEPARATED_WX_HEAP)
204 
205             addFreshFreeSpace(reservationBase, reservationSize);
206 
207             void* reservationEnd = reinterpret_cast&lt;uint8_t*&gt;(reservationBase) + reservationSize;
208 
209             m_memoryStart = MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt;(tagCodePtr&lt;ExecutableMemoryPtrTag&gt;(reservationBase));
210             m_memoryEnd = MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt;(tagCodePtr&lt;ExecutableMemoryPtrTag&gt;(reservationEnd));
211         }
212     }
213 
214     virtual ~FixedVMPoolExecutableAllocator();
215 
216     void* memoryStart() { return m_memoryStart.untaggedExecutableAddress(); }
217     void* memoryEnd() { return m_memoryEnd.untaggedExecutableAddress(); }
218     bool isJITPC(void* pc) { return memoryStart() &lt;= pc &amp;&amp; pc &lt; memoryEnd(); }
219 
220 protected:
221     FreeSpacePtr allocateNewSpace(size_t&amp;) override
222     {
223         // We&#39;re operating in a fixed pool, so new allocation is always prohibited.
224         return nullptr;
225     }
226 
227     void notifyNeedPage(void* page) override
228     {
229 #if USE(MADV_FREE_FOR_JIT_MEMORY)
230         UNUSED_PARAM(page);
231 #else
232         m_reservation.commit(page, pageSize());
233 #endif
234     }
235 
236     void notifyPageIsFree(void* page) override
237     {
238 #if USE(MADV_FREE_FOR_JIT_MEMORY)
239         for (;;) {
240             int result = madvise(page, pageSize(), MADV_FREE);
241             if (!result)
242                 return;
243             ASSERT(result == -1);
244             if (errno != EAGAIN) {
245                 RELEASE_ASSERT_NOT_REACHED(); // In debug mode, this should be a hard failure.
246                 break; // In release mode, we should just ignore the error - not returning memory to the OS is better than crashing, especially since we _will_ be able to reuse the memory internally anyway.
247             }
248         }
249 #else
250         m_reservation.decommit(page, pageSize());
251 #endif
252     }
253 
254 private:
255 #if OS(DARWIN) &amp;&amp; HAVE(REMAP_JIT)
256     void initializeSeparatedWXHeaps(void* stubBase, size_t stubSize, void* jitBase, size_t jitSize)
257     {
258         mach_vm_address_t writableAddr = 0;
259 
260         // Create a second mapping of the JIT region at a random address.
261         vm_prot_t cur, max;
262         int remapFlags = VM_FLAGS_ANYWHERE;
263 #if defined(VM_FLAGS_RANDOM_ADDR)
264         remapFlags |= VM_FLAGS_RANDOM_ADDR;
265 #endif
266         kern_return_t ret = mach_vm_remap(mach_task_self(), &amp;writableAddr, jitSize, 0,
267             remapFlags,
268             mach_task_self(), (mach_vm_address_t)jitBase, FALSE,
269             &amp;cur, &amp;max, VM_INHERIT_DEFAULT);
270 
271         bool remapSucceeded = (ret == KERN_SUCCESS);
272         if (!remapSucceeded)
273             return;
274 
275         // Assemble a thunk that will serve as the means for writing into the JIT region.
276         MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; writeThunk = jitWriteThunkGenerator(reinterpret_cast&lt;void*&gt;(writableAddr), stubBase, stubSize);
277 
278         int result = 0;
279 
280 #if USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
281         // Prevent reading the write thunk code.
282         result = vm_protect(mach_task_self(), reinterpret_cast&lt;vm_address_t&gt;(stubBase), stubSize, true, VM_PROT_EXECUTE);
283         RELEASE_ASSERT(!result);
284 #endif
285 
286         // Prevent writing into the executable JIT mapping.
287         result = vm_protect(mach_task_self(), reinterpret_cast&lt;vm_address_t&gt;(jitBase), jitSize, true, VM_PROT_READ | VM_PROT_EXECUTE);
288         RELEASE_ASSERT(!result);
289 
290         // Prevent execution in the writable JIT mapping.
291         result = vm_protect(mach_task_self(), static_cast&lt;vm_address_t&gt;(writableAddr), jitSize, true, VM_PROT_READ | VM_PROT_WRITE);
292         RELEASE_ASSERT(!result);
293 
294         // Zero out writableAddr to avoid leaking the address of the writable mapping.
295         memset_s(&amp;writableAddr, sizeof(writableAddr), 0, sizeof(writableAddr));
296 
297 #if ENABLE(SEPARATED_WX_HEAP)
298         jitWriteSeparateHeapsFunction = reinterpret_cast&lt;JITWriteSeparateHeapsFunction&gt;(writeThunk.code().executableAddress());
299 #endif
300     }
301 
302 #if CPU(ARM64) &amp;&amp; USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
303     MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; jitWriteThunkGenerator(void* writableAddr, void* stubBase, size_t stubSize)
304     {
305         using namespace ARM64Registers;
306         using TrustedImm32 = MacroAssembler::TrustedImm32;
307 
308         MacroAssembler jit;
309 
310         jit.tagReturnAddress();
311         jit.move(MacroAssembler::TrustedImmPtr(writableAddr), x7);
312         jit.addPtr(x7, x0);
313 
314         jit.move(x0, x3);
315         MacroAssembler::Jump smallCopy = jit.branch64(MacroAssembler::Below, x2, MacroAssembler::TrustedImm64(64));
316 
317         jit.add64(TrustedImm32(32), x3);
318         jit.and64(TrustedImm32(-32), x3);
319         jit.loadPair64(x1, x12, x13);
320         jit.loadPair64(x1, TrustedImm32(16), x14, x15);
321         jit.sub64(x3, x0, x5);
322         jit.addPtr(x5, x1);
323 
324         jit.loadPair64(x1, x8, x9);
325         jit.loadPair64(x1, TrustedImm32(16), x10, x11);
326         jit.add64(TrustedImm32(32), x1);
327         jit.sub64(x5, x2);
328         jit.storePair64(x12, x13, x0);
329         jit.storePair64(x14, x15, x0, TrustedImm32(16));
330         MacroAssembler::Jump cleanup = jit.branchSub64(MacroAssembler::BelowOrEqual, TrustedImm32(64), x2);
331 
332         MacroAssembler::Label copyLoop = jit.label();
333         jit.storePair64WithNonTemporalAccess(x8, x9, x3);
334         jit.storePair64WithNonTemporalAccess(x10, x11, x3, TrustedImm32(16));
335         jit.add64(TrustedImm32(32), x3);
336         jit.loadPair64WithNonTemporalAccess(x1, x8, x9);
337         jit.loadPair64WithNonTemporalAccess(x1, TrustedImm32(16), x10, x11);
338         jit.add64(TrustedImm32(32), x1);
339         jit.branchSub64(MacroAssembler::Above, TrustedImm32(32), x2).linkTo(copyLoop, &amp;jit);
340 
341         cleanup.link(&amp;jit);
342         jit.add64(x2, x1);
343         jit.loadPair64(x1, x12, x13);
344         jit.loadPair64(x1, TrustedImm32(16), x14, x15);
345         jit.storePair64(x8, x9, x3);
346         jit.storePair64(x10, x11, x3, TrustedImm32(16));
347         jit.addPtr(x2, x3);
348         jit.storePair64(x12, x13, x3, TrustedImm32(32));
349         jit.storePair64(x14, x15, x3, TrustedImm32(48));
350         jit.ret();
351 
352         MacroAssembler::Label local0 = jit.label();
353         jit.load64(x1, PostIndex(8), x6);
354         jit.store64(x6, x3, PostIndex(8));
355         smallCopy.link(&amp;jit);
356         jit.branchSub64(MacroAssembler::AboveOrEqual, TrustedImm32(8), x2).linkTo(local0, &amp;jit);
357         MacroAssembler::Jump local2 = jit.branchAdd64(MacroAssembler::Equal, TrustedImm32(8), x2);
358         MacroAssembler::Label local1 = jit.label();
359         jit.load8(x1, PostIndex(1), x6);
360         jit.store8(x6, x3, PostIndex(1));
361         jit.branchSub64(MacroAssembler::NotEqual, TrustedImm32(1), x2).linkTo(local1, &amp;jit);
362         local2.link(&amp;jit);
363         jit.ret();
364 
365         auto stubBaseCodePtr = MacroAssemblerCodePtr&lt;LinkBufferPtrTag&gt;(tagCodePtr&lt;LinkBufferPtrTag&gt;(stubBase));
366         LinkBuffer linkBuffer(jit, stubBaseCodePtr, stubSize);
367         // We don&#39;t use FINALIZE_CODE() for two reasons.
368         // The first is that we don&#39;t want the writeable address, as disassembled instructions,
369         // to appear in the console or anywhere in memory, via the PrintStream buffer.
370         // The second is we can&#39;t guarantee that the code is readable when using the
371         // asyncDisassembly option as our caller will set our pages execute only.
372         return linkBuffer.finalizeCodeWithoutDisassembly&lt;JITThunkPtrTag&gt;();
373     }
374 #else // not CPU(ARM64) &amp;&amp; USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
375     static void genericWriteToJITRegion(off_t offset, const void* data, size_t dataSize)
376     {
377         memcpy((void*)(startOfFixedWritableMemoryPool + offset), data, dataSize);
378     }
379 
380     MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt; jitWriteThunkGenerator(void* address, void*, size_t)
381     {
382         startOfFixedWritableMemoryPool = reinterpret_cast&lt;uintptr_t&gt;(address);
383         void* function = reinterpret_cast&lt;void*&gt;(&amp;genericWriteToJITRegion);
384 #if CPU(ARM_THUMB2)
385         // Handle thumb offset
386         uintptr_t functionAsInt = reinterpret_cast&lt;uintptr_t&gt;(function);
387         functionAsInt -= 1;
388         function = reinterpret_cast&lt;void*&gt;(functionAsInt);
389 #endif
390         auto codePtr = MacroAssemblerCodePtr&lt;JITThunkPtrTag&gt;(tagCFunctionPtr&lt;JITThunkPtrTag&gt;(function));
391         return MacroAssemblerCodeRef&lt;JITThunkPtrTag&gt;::createSelfManagedCodeRef(codePtr);
392     }
393 #endif // CPU(ARM64) &amp;&amp; USE(EXECUTE_ONLY_JIT_WRITE_FUNCTION)
394 
395 #else // OS(DARWIN) &amp;&amp; HAVE(REMAP_JIT)
396     void initializeSeparatedWXHeaps(void*, size_t, void*, size_t)
397     {
398     }
399 #endif
400 
401 private:
402     PageReservation m_reservation;
403     MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt; m_memoryStart;
404     MacroAssemblerCodePtr&lt;ExecutableMemoryPtrTag&gt; m_memoryEnd;
405 };
406 
407 void ExecutableAllocator::initializeAllocator()
408 {
409     ASSERT(!allocator);
410     allocator = new FixedVMPoolExecutableAllocator();
411     CodeProfiling::notifyAllocator(allocator);
412 
413     executableAllocator = new ExecutableAllocator;
414 }
415 
416 ExecutableAllocator&amp; ExecutableAllocator::singleton()
417 {
418     ASSERT(allocator);
419     ASSERT(executableAllocator);
420     return *executableAllocator;
421 }
422 
423 ExecutableAllocator::ExecutableAllocator()
424 {
425     ASSERT(allocator);
426 }
427 
428 ExecutableAllocator::~ExecutableAllocator()
429 {
430 }
431 
432 FixedVMPoolExecutableAllocator::~FixedVMPoolExecutableAllocator()
433 {
434     m_reservation.deallocate();
435 }
436 
437 bool ExecutableAllocator::isValid() const
438 {
439     return !!allocator-&gt;bytesReserved();
440 }
441 
442 bool ExecutableAllocator::underMemoryPressure()
443 {
444     MetaAllocator::Statistics statistics = allocator-&gt;currentStatistics();
445     return statistics.bytesAllocated &gt; statistics.bytesReserved / 2;
446 }
447 
448 double ExecutableAllocator::memoryPressureMultiplier(size_t addedMemoryUsage)
449 {
450     MetaAllocator::Statistics statistics = allocator-&gt;currentStatistics();
451     ASSERT(statistics.bytesAllocated &lt;= statistics.bytesReserved);
452     size_t bytesAllocated = statistics.bytesAllocated + addedMemoryUsage;
453     size_t bytesAvailable = static_cast&lt;size_t&gt;(
454         statistics.bytesReserved * (1 - executablePoolReservationFraction));
455     if (bytesAllocated &gt;= bytesAvailable)
456         bytesAllocated = bytesAvailable;
457     double result = 1.0;
458     size_t divisor = bytesAvailable - bytesAllocated;
459     if (divisor)
460         result = static_cast&lt;double&gt;(bytesAvailable) / divisor;
461     if (result &lt; 1.0)
462         result = 1.0;
463     return result;
464 }
465 
466 RefPtr&lt;ExecutableMemoryHandle&gt; ExecutableAllocator::allocate(size_t sizeInBytes, void* ownerUID, JITCompilationEffort effort)
467 {
468     if (Options::logExecutableAllocation()) {
469         MetaAllocator::Statistics stats = allocator-&gt;currentStatistics();
470         dataLog(&quot;Allocating &quot;, sizeInBytes, &quot; bytes of executable memory with &quot;, stats.bytesAllocated, &quot; bytes allocated, &quot;, stats.bytesReserved, &quot; bytes reserved, and &quot;, stats.bytesCommitted, &quot; committed.\n&quot;);
471     }
472 
473     if (effort != JITCompilationCanFail &amp;&amp; Options::reportMustSucceedExecutableAllocations()) {
474         dataLog(&quot;Allocating &quot;, sizeInBytes, &quot; bytes of executable memory with JITCompilationMustSucceed.\n&quot;);
475         WTFReportBacktrace();
476     }
477 
478     if (effort == JITCompilationCanFail
479         &amp;&amp; doExecutableAllocationFuzzingIfEnabled() == PretendToFailExecutableAllocation)
480         return nullptr;
481 
482     if (effort == JITCompilationCanFail) {
483         // Don&#39;t allow allocations if we are down to reserve.
484         MetaAllocator::Statistics statistics = allocator-&gt;currentStatistics();
485         size_t bytesAllocated = statistics.bytesAllocated + sizeInBytes;
486         size_t bytesAvailable = static_cast&lt;size_t&gt;(
487             statistics.bytesReserved * (1 - executablePoolReservationFraction));
488         if (bytesAllocated &gt; bytesAvailable) {
489             if (Options::logExecutableAllocation())
490                 dataLog(&quot;Allocation failed because bytes allocated &quot;, bytesAllocated,  &quot; &gt; &quot;, bytesAvailable, &quot; bytes available.\n&quot;);
491             return nullptr;
492         }
493     }
494 
495     RefPtr&lt;ExecutableMemoryHandle&gt; result = allocator-&gt;allocate(sizeInBytes, ownerUID);
496     if (!result) {
497         if (effort != JITCompilationCanFail) {
498             dataLog(&quot;Ran out of executable memory while allocating &quot;, sizeInBytes, &quot; bytes.\n&quot;);
499             CRASH();
500         }
501         return nullptr;
502     }
503 
504 #if USE(POINTER_PROFILING)
505     void* start = allocator-&gt;memoryStart();
506     void* end = allocator-&gt;memoryEnd();
507     void* resultStart = result-&gt;start().untaggedPtr();
508     void* resultEnd = result-&gt;end().untaggedPtr();
509     RELEASE_ASSERT(start &lt;= resultStart &amp;&amp; resultStart &lt; end);
510     RELEASE_ASSERT(start &lt; resultEnd &amp;&amp; resultEnd &lt;= end);
511 #endif
512     return result;
513 }
514 
515 bool ExecutableAllocator::isValidExecutableMemory(const AbstractLocker&amp; locker, void* address)
516 {
517     return allocator-&gt;isInAllocatedMemory(locker, address);
518 }
519 
520 Lock&amp; ExecutableAllocator::getLock() const
521 {
522     return allocator-&gt;getLock();
523 }
524 
525 size_t ExecutableAllocator::committedByteCount()
526 {
527     return allocator-&gt;bytesCommitted();
528 }
529 
530 #if ENABLE(META_ALLOCATOR_PROFILE)
531 void ExecutableAllocator::dumpProfile()
532 {
533     allocator-&gt;dumpProfile();
534 }
535 #endif
536 
537 void* startOfFixedExecutableMemoryPoolImpl()
538 {
539     return allocator-&gt;memoryStart();
540 }
541 
542 void* endOfFixedExecutableMemoryPoolImpl()
543 {
544     return allocator-&gt;memoryEnd();
545 }
546 
547 bool isJITPC(void* pc)
548 {
549     return allocator &amp;&amp; allocator-&gt;isJITPC(pc);
550 }
551 
552 } // namespace JSC
553 
554 #else // !ENABLE(JIT)
555 
556 namespace JSC {
557 
558 static ExecutableAllocator* executableAllocator;
559 
560 void ExecutableAllocator::initializeAllocator()
561 {
562     executableAllocator = new ExecutableAllocator;
563 }
564 
565 ExecutableAllocator&amp; ExecutableAllocator::singleton()
566 {
567     ASSERT(executableAllocator);
568     return *executableAllocator;
569 }
570 
571 } // namespace JSC
572 
573 #endif // ENABLE(JIT)
    </pre>
  </body>
</html>