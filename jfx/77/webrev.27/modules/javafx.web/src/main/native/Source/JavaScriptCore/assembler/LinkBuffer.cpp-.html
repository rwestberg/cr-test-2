<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/assembler/LinkBuffer.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2012-2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;LinkBuffer.h&quot;
 28 
 29 #if ENABLE(ASSEMBLER)
 30 
 31 #include &quot;CodeBlock.h&quot;
 32 #include &quot;Disassembler.h&quot;
 33 #include &quot;JITCode.h&quot;
 34 #include &quot;JSCInlines.h&quot;
 35 #include &quot;Options.h&quot;
 36 #include &lt;wtf/CompilationThread.h&gt;
 37 
 38 #if OS(LINUX)
 39 #include &quot;PerfLog.h&quot;
 40 #endif
 41 
 42 namespace JSC {
 43 
 44 bool shouldDumpDisassemblyFor(CodeBlock* codeBlock)
 45 {
 46     if (codeBlock &amp;&amp; JITCode::isOptimizingJIT(codeBlock-&gt;jitType()) &amp;&amp; Options::dumpDFGDisassembly())
 47         return true;
 48     return Options::dumpDisassembly();
 49 }
 50 
 51 LinkBuffer::CodeRef&lt;LinkBufferPtrTag&gt; LinkBuffer::finalizeCodeWithoutDisassemblyImpl()
 52 {
 53     performFinalization();
 54 
 55     ASSERT(m_didAllocate);
 56     if (m_executableMemory)
 57         return CodeRef&lt;LinkBufferPtrTag&gt;(*m_executableMemory);
 58 
 59     return CodeRef&lt;LinkBufferPtrTag&gt;::createSelfManagedCodeRef(m_code);
 60 }
 61 
 62 LinkBuffer::CodeRef&lt;LinkBufferPtrTag&gt; LinkBuffer::finalizeCodeWithDisassemblyImpl(bool dumpDisassembly, const char* format, ...)
 63 {
 64     CodeRef&lt;LinkBufferPtrTag&gt; result = finalizeCodeWithoutDisassemblyImpl();
 65 
 66 #if OS(LINUX)
 67     if (Options::logJITCodeForPerf()) {
 68         StringPrintStream out;
 69         va_list argList;
 70         va_start(argList, format);
 71         va_start(argList, format);
 72         out.vprintf(format, argList);
 73         va_end(argList);
 74         PerfLog::log(out.toCString(), result.code().untaggedExecutableAddress&lt;const uint8_t*&gt;(), result.size());
 75     }
 76 #endif
 77 
 78     if (!dumpDisassembly || m_alreadyDisassembled)
 79         return result;
 80 
 81     StringPrintStream out;
 82     out.printf(&quot;Generated JIT code for &quot;);
 83     va_list argList;
 84     va_start(argList, format);
 85     out.vprintf(format, argList);
 86     va_end(argList);
 87     out.printf(&quot;:\n&quot;);
 88 
 89     uint8_t* executableAddress = result.code().untaggedExecutableAddress&lt;uint8_t*&gt;();
 90     out.printf(&quot;    Code at [%p, %p):\n&quot;, executableAddress, executableAddress + result.size());
 91 
 92     CString header = out.toCString();
 93 
 94     if (Options::asyncDisassembly()) {
 95         CodeRef&lt;DisassemblyPtrTag&gt; codeRefForDisassembly = result.retagged&lt;DisassemblyPtrTag&gt;();
 96         disassembleAsynchronously(header, WTFMove(codeRefForDisassembly), m_size, &quot;    &quot;);
 97         return result;
 98     }
 99 
100     dataLog(header);
101     disassemble(result.retaggedCode&lt;DisassemblyPtrTag&gt;(), m_size, &quot;    &quot;, WTF::dataFile());
102 
103     return result;
104 }
105 
106 #if ENABLE(BRANCH_COMPACTION)
107 static ALWAYS_INLINE void recordLinkOffsets(AssemblerData&amp; assemblerData, int32_t regionStart, int32_t regionEnd, int32_t offset)
108 {
109     int32_t ptr = regionStart / sizeof(int32_t);
110     const int32_t end = regionEnd / sizeof(int32_t);
111     int32_t* offsets = reinterpret_cast_ptr&lt;int32_t*&gt;(assemblerData.buffer());
112     while (ptr &lt; end)
113         offsets[ptr++] = offset;
114 }
115 
116 template &lt;typename InstructionType&gt;
117 void LinkBuffer::copyCompactAndLinkCode(MacroAssembler&amp; macroAssembler, void* ownerUID, JITCompilationEffort effort)
118 {
119     allocate(macroAssembler, ownerUID, effort);
120     const size_t initialSize = macroAssembler.m_assembler.codeSize();
121     if (didFailToAllocate())
122         return;
123 
124     Vector&lt;LinkRecord, 0, UnsafeVectorOverflow&gt;&amp; jumpsToLink = macroAssembler.jumpsToLink();
125     m_assemblerStorage = macroAssembler.m_assembler.buffer().releaseAssemblerData();
126     uint8_t* inData = reinterpret_cast&lt;uint8_t*&gt;(m_assemblerStorage.buffer());
127 
128     uint8_t* codeOutData = m_code.dataLocation&lt;uint8_t*&gt;();
129 #if CPU(ARM64E) &amp;&amp; ENABLE(FAST_JIT_PERMISSIONS)
130     const uint32_t expectedFinalHash = macroAssembler.m_assembler.buffer().hash().finalHash();
131     ARM64EHash verifyUncompactedHash;
132     uint8_t* outData = codeOutData;
133 #else
134     AssemblerData outBuffer(m_size);
135     uint8_t* outData = reinterpret_cast&lt;uint8_t*&gt;(outBuffer.buffer());
136 #endif
137 #if CPU(ARM64)
138     RELEASE_ASSERT(roundUpToMultipleOf&lt;sizeof(unsigned)&gt;(outData) == outData);
139     RELEASE_ASSERT(roundUpToMultipleOf&lt;sizeof(unsigned)&gt;(codeOutData) == codeOutData);
140 #endif
141 
142     int readPtr = 0;
143     int writePtr = 0;
144     unsigned jumpCount = jumpsToLink.size();
145 
146 #if CPU(ARM64E) &amp;&amp; ENABLE(FAST_JIT_PERMISSIONS)
147     os_thread_self_restrict_rwx_to_rw();
148 #endif
149 
150     if (m_shouldPerformBranchCompaction) {
151         for (unsigned i = 0; i &lt; jumpCount; ++i) {
152             int offset = readPtr - writePtr;
153             ASSERT(!(offset &amp; 1));
154 
155             // Copy the instructions from the last jump to the current one.
156             size_t regionSize = jumpsToLink[i].from() - readPtr;
157             InstructionType* copySource = reinterpret_cast_ptr&lt;InstructionType*&gt;(inData + readPtr);
158             InstructionType* copyEnd = reinterpret_cast_ptr&lt;InstructionType*&gt;(inData + readPtr + regionSize);
159             InstructionType* copyDst = reinterpret_cast_ptr&lt;InstructionType*&gt;(outData + writePtr);
160             ASSERT(!(regionSize % 2));
161             ASSERT(!(readPtr % 2));
162             ASSERT(!(writePtr % 2));
163             while (copySource != copyEnd) {
164                 InstructionType insn = *copySource++;
165 #if CPU(ARM64E) &amp;&amp; ENABLE(FAST_JIT_PERMISSIONS)
166                 static_assert(sizeof(InstructionType) == 4, &quot;&quot;);
167                 verifyUncompactedHash.update(insn);
168 #endif
169                 *copyDst++ = insn;
170             }
171             recordLinkOffsets(m_assemblerStorage, readPtr, jumpsToLink[i].from(), offset);
172             readPtr += regionSize;
173             writePtr += regionSize;
174 
175             // Calculate absolute address of the jump target, in the case of backwards
176             // branches we need to be precise, forward branches we are pessimistic
177             const uint8_t* target;
178             if (jumpsToLink[i].to() &gt;= jumpsToLink[i].from())
179                 target = codeOutData + jumpsToLink[i].to() - offset; // Compensate for what we have collapsed so far
180             else
181                 target = codeOutData + jumpsToLink[i].to() - executableOffsetFor(jumpsToLink[i].to());
182 
183             JumpLinkType jumpLinkType = MacroAssembler::computeJumpType(jumpsToLink[i], codeOutData + writePtr, target);
184             // Compact branch if we can...
185             if (MacroAssembler::canCompact(jumpsToLink[i].type())) {
186                 // Step back in the write stream
187                 int32_t delta = MacroAssembler::jumpSizeDelta(jumpsToLink[i].type(), jumpLinkType);
188                 if (delta) {
189                     writePtr -= delta;
190                     recordLinkOffsets(m_assemblerStorage, jumpsToLink[i].from() - delta, readPtr, readPtr - writePtr);
191                 }
192             }
193             jumpsToLink[i].setFrom(writePtr);
194         }
195     } else {
196         if (!ASSERT_DISABLED) {
197             for (unsigned i = 0; i &lt; jumpCount; ++i)
198                 ASSERT(!MacroAssembler::canCompact(jumpsToLink[i].type()));
199         }
200     }
201 
202     // Copy everything after the last jump
203     {
204         InstructionType* dst = bitwise_cast&lt;InstructionType*&gt;(outData + writePtr);
205         InstructionType* src = bitwise_cast&lt;InstructionType*&gt;(inData + readPtr);
206         size_t bytes = initialSize - readPtr;
207 
208         RELEASE_ASSERT(bitwise_cast&lt;uintptr_t&gt;(dst) % sizeof(InstructionType) == 0);
209         RELEASE_ASSERT(bitwise_cast&lt;uintptr_t&gt;(src) % sizeof(InstructionType) == 0);
210         RELEASE_ASSERT(bytes % sizeof(InstructionType) == 0);
211 
212         for (size_t i = 0; i &lt; bytes; i += sizeof(InstructionType)) {
213             InstructionType insn = *src++;
214 #if CPU(ARM64E) &amp;&amp; ENABLE(FAST_JIT_PERMISSIONS)
215             verifyUncompactedHash.update(insn);
216 #endif
217             *dst++ = insn;
218         }
219     }
220 
221 #if CPU(ARM64E) &amp;&amp; ENABLE(FAST_JIT_PERMISSIONS)
222     if (verifyUncompactedHash.finalHash() != expectedFinalHash) {
223         dataLogLn(&quot;Hashes don&#39;t match: &quot;, RawPointer(bitwise_cast&lt;void*&gt;(static_cast&lt;uintptr_t&gt;(verifyUncompactedHash.finalHash()))), &quot; &quot;, RawPointer(bitwise_cast&lt;void*&gt;(static_cast&lt;uintptr_t&gt;(expectedFinalHash))));
224         dataLogLn(&quot;Crashing!&quot;);
225         CRASH();
226     }
227 #endif
228 
229     recordLinkOffsets(m_assemblerStorage, readPtr, initialSize, readPtr - writePtr);
230 
231     for (unsigned i = 0; i &lt; jumpCount; ++i) {
232 #if CPU(ARM64E) &amp;&amp; ENABLE(FAST_JIT_PERMISSIONS)
233         auto memcpyFunction = memcpy;
234 #else
235         auto memcpyFunction = performJITMemcpy;
236 #endif
237 
238         uint8_t* location = codeOutData + jumpsToLink[i].from();
239         uint8_t* target = codeOutData + jumpsToLink[i].to() - executableOffsetFor(jumpsToLink[i].to());
240         MacroAssembler::link(jumpsToLink[i], outData + jumpsToLink[i].from(), location, target, memcpyFunction);
241     }
242 
243     size_t compactSize = writePtr + initialSize - readPtr;
244     if (!m_executableMemory) {
245         size_t nopSizeInBytes = initialSize - compactSize;
246         MacroAssembler::AssemblerType_T::fillNops(outData + compactSize, nopSizeInBytes, memcpy);
247     }
248 
249 #if CPU(ARM64E) &amp;&amp; ENABLE(FAST_JIT_PERMISSIONS)
250     os_thread_self_restrict_rwx_to_rx();
251 #endif
252 
253     if (m_executableMemory) {
254         m_size = compactSize;
255         m_executableMemory-&gt;shrink(m_size);
256     }
257 
258 #if !CPU(ARM64E) || !ENABLE(FAST_JIT_PERMISSIONS)
259     ASSERT(codeOutData != outData);
260     performJITMemcpy(codeOutData, outData, m_size);
261 #else
262     ASSERT(codeOutData == outData);
263 #endif
264 
265     jumpsToLink.clear();
266 
267 #if DUMP_LINK_STATISTICS
268     dumpLinkStatistics(codeOutData, initialSize, m_size);
269 #endif
270 #if DUMP_CODE
271     dumpCode(codeOutData, m_size);
272 #endif
273 }
274 #endif // ENABLE(BRANCH_COMPACTION)
275 
276 
277 void LinkBuffer::linkCode(MacroAssembler&amp; macroAssembler, void* ownerUID, JITCompilationEffort effort)
278 {
279     // Ensure that the end of the last invalidation point does not extend beyond the end of the buffer.
280     macroAssembler.label();
281 
282 #if !ENABLE(BRANCH_COMPACTION)
283 #if defined(ASSEMBLER_HAS_CONSTANT_POOL) &amp;&amp; ASSEMBLER_HAS_CONSTANT_POOL
284     macroAssembler.m_assembler.buffer().flushConstantPool(false);
285 #endif
286     allocate(macroAssembler, ownerUID, effort);
287     if (!m_didAllocate)
288         return;
289     ASSERT(m_code);
290     AssemblerBuffer&amp; buffer = macroAssembler.m_assembler.buffer();
291     void* code = m_code.dataLocation();
292 #if CPU(ARM64)
293     RELEASE_ASSERT(roundUpToMultipleOf&lt;Assembler::instructionSize&gt;(code) == code);
294 #endif
295     performJITMemcpy(code, buffer.data(), buffer.codeSize());
296 #if CPU(MIPS)
297     macroAssembler.m_assembler.relocateJumps(buffer.data(), code);
298 #endif
299 #elif CPU(ARM_THUMB2)
300     copyCompactAndLinkCode&lt;uint16_t&gt;(macroAssembler, ownerUID, effort);
301 #elif CPU(ARM64)
302     copyCompactAndLinkCode&lt;uint32_t&gt;(macroAssembler, ownerUID, effort);
303 #endif // !ENABLE(BRANCH_COMPACTION)
304 
305     m_linkTasks = WTFMove(macroAssembler.m_linkTasks);
306 }
307 
308 void LinkBuffer::allocate(MacroAssembler&amp; macroAssembler, void* ownerUID, JITCompilationEffort effort)
309 {
310     size_t initialSize = macroAssembler.m_assembler.codeSize();
311     if (m_code) {
312         if (initialSize &gt; m_size)
313             return;
314 
315         size_t nopsToFillInBytes = m_size - initialSize;
316         macroAssembler.emitNops(nopsToFillInBytes);
317         m_didAllocate = true;
318         return;
319     }
320 
321     while (initialSize % jitAllocationGranule) {
322         macroAssembler.breakpoint();
323         initialSize = macroAssembler.m_assembler.codeSize();
324     }
325 
326     m_executableMemory = ExecutableAllocator::singleton().allocate(initialSize, ownerUID, effort);
327     if (!m_executableMemory)
328         return;
329     m_code = MacroAssemblerCodePtr&lt;LinkBufferPtrTag&gt;(m_executableMemory-&gt;start().retaggedPtr&lt;LinkBufferPtrTag&gt;());
330     m_size = initialSize;
331     m_didAllocate = true;
332 }
333 
334 void LinkBuffer::performFinalization()
335 {
336     for (auto&amp; task : m_linkTasks)
337         task-&gt;run(*this);
338 
339 #ifndef NDEBUG
340     ASSERT(!isCompilationThread());
341     ASSERT(!m_completed);
342     ASSERT(isValid());
343     m_completed = true;
344 #endif
345 
346     MacroAssembler::cacheFlush(code(), m_size);
347 }
348 
349 #if DUMP_LINK_STATISTICS
350 void LinkBuffer::dumpLinkStatistics(void* code, size_t initializeSize, size_t finalSize)
351 {
352     static unsigned linkCount = 0;
353     static unsigned totalInitialSize = 0;
354     static unsigned totalFinalSize = 0;
355     linkCount++;
356     totalInitialSize += initialSize;
357     totalFinalSize += finalSize;
358     dataLogF(&quot;link %p: orig %u, compact %u (delta %u, %.2f%%)\n&quot;,
359             code, static_cast&lt;unsigned&gt;(initialSize), static_cast&lt;unsigned&gt;(finalSize),
360             static_cast&lt;unsigned&gt;(initialSize - finalSize),
361             100.0 * (initialSize - finalSize) / initialSize);
362     dataLogF(&quot;\ttotal %u: orig %u, compact %u (delta %u, %.2f%%)\n&quot;,
363             linkCount, totalInitialSize, totalFinalSize, totalInitialSize - totalFinalSize,
364             100.0 * (totalInitialSize - totalFinalSize) / totalInitialSize);
365 }
366 #endif
367 
368 #if DUMP_CODE
369 void LinkBuffer::dumpCode(void* code, size_t size)
370 {
371 #if CPU(ARM_THUMB2)
372     // Dump the generated code in an asm file format that can be assembled and then disassembled
373     // for debugging purposes. For example, save this output as jit.s:
374     //   gcc -arch armv7 -c jit.s
375     //   otool -tv jit.o
376     static unsigned codeCount = 0;
377     unsigned short* tcode = static_cast&lt;unsigned short*&gt;(code);
378     size_t tsize = size / sizeof(short);
379     char nameBuf[128];
380     snprintf(nameBuf, sizeof(nameBuf), &quot;_jsc_jit%u&quot;, codeCount++);
381     dataLogF(&quot;\t.syntax unified\n&quot;
382             &quot;\t.section\t__TEXT,__text,regular,pure_instructions\n&quot;
383             &quot;\t.globl\t%s\n&quot;
384             &quot;\t.align 2\n&quot;
385             &quot;\t.code 16\n&quot;
386             &quot;\t.thumb_func\t%s\n&quot;
387             &quot;# %p\n&quot;
388             &quot;%s:\n&quot;, nameBuf, nameBuf, code, nameBuf);
389 
390     for (unsigned i = 0; i &lt; tsize; i++)
391         dataLogF(&quot;\t.short\t0x%x\n&quot;, tcode[i]);
392 #endif
393 }
394 #endif
395 
396 } // namespace JSC
397 
398 #endif // ENABLE(ASSEMBLER)
    </pre>
  </body>
</html>