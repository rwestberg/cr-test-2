<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/WTF/wtf/Atomics.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="Assertions.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="AutodrainedPool.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/WTF/wtf/Atomics.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 36 #include &lt;windows.h&gt;
 37 #include &lt;intrin.h&gt;
 38 #endif
 39 
 40 namespace WTF {
 41 
 42 ALWAYS_INLINE bool hasFence(std::memory_order order)
 43 {
 44     return order != std::memory_order_relaxed;
 45 }
 46 
 47 // Atomic wraps around std::atomic with the sole purpose of making the compare_exchange
 48 // operations not alter the expected value. This is more in line with how we typically
 49 // use CAS in our code.
 50 //
 51 // Atomic is a struct without explicitly defined constructors so that it can be
 52 // initialized at compile time.
 53 
 54 template&lt;typename T&gt;
 55 struct Atomic {


 56     // Don&#39;t pass a non-default value for the order parameter unless you really know
 57     // what you are doing and have thought about it very hard. The cost of seq_cst
 58     // is usually not high enough to justify the risk.
 59 
 60     ALWAYS_INLINE T load(std::memory_order order = std::memory_order_seq_cst) const { return value.load(order); }
 61 
 62     ALWAYS_INLINE T loadRelaxed() const { return load(std::memory_order_relaxed); }
 63 
 64     // This is a load that simultaneously does a full fence - neither loads nor stores will move
 65     // above or below it.
 66     ALWAYS_INLINE T loadFullyFenced() const
 67     {
 68         Atomic&lt;T&gt;* ptr = const_cast&lt;Atomic&lt;T&gt;*&gt;(this);
 69         return ptr-&gt;exchangeAdd(T());
 70     }
 71 
 72     ALWAYS_INLINE void store(T desired, std::memory_order order = std::memory_order_seq_cst) { value.store(desired, order); }
 73 
 74     ALWAYS_INLINE void storeRelaxed(T desired) { store(desired, std::memory_order_relaxed); }
 75 
</pre>
<hr />
<pre>
329 typedef unsigned InternalDependencyType;
330 
331 inline InternalDependencyType opaqueMixture()
332 {
333     return 0;
334 }
335 
336 template&lt;typename... Arguments, typename T&gt;
337 inline InternalDependencyType opaqueMixture(T value, Arguments... arguments)
338 {
339     union {
340         InternalDependencyType copy;
341         T value;
342     } u;
343     u.copy = 0;
344     u.value = value;
345     return opaqueMixture(arguments...) + u.copy;
346 }
347 
348 class Dependency {

349 public:
350     Dependency()
351         : m_value(0)
352     {
353     }
354 
355     // On TSO architectures, this is a load-load fence and the value it returns is not meaningful (it&#39;s
356     // zero). The load-load fence is usually just a compiler fence. On ARM, this is a self-xor that
357     // produces zero, but it&#39;s concealed from the compiler. The CPU understands this dummy op to be a
358     // phantom dependency.
359     template&lt;typename... Arguments&gt;
360     static Dependency fence(Arguments... arguments)
361     {
362         InternalDependencyType input = opaqueMixture(arguments...);
363         InternalDependencyType output;
364 #if CPU(ARM64)
365         // Create a magical zero value through inline assembly, whose computation
366         // isn&#39;t visible to the optimizer. This zero is then usable as an offset in
367         // further address computations: adding zero does nothing, but the compiler
368         // doesn&#39;t know it. It&#39;s magical because it creates an address dependency
</pre>
<hr />
<pre>
391 
392     // On TSO architectures, this just returns the pointer you pass it. On ARM, this produces a new
393     // pointer that is dependent on this dependency and the input pointer.
394     template&lt;typename T&gt;
395     T* consume(T* pointer)
396     {
397 #if CPU(ARM64) || CPU(ARM)
398         return bitwise_cast&lt;T*&gt;(bitwise_cast&lt;char*&gt;(pointer) + m_value);
399 #else
400         UNUSED_PARAM(m_value);
401         return pointer;
402 #endif
403     }
404 
405 private:
406     InternalDependencyType m_value;
407 };
408 
409 template&lt;typename InputType, typename ValueType&gt;
410 struct InputAndValue {


411     InputAndValue() { }
412 
413     InputAndValue(InputType input, ValueType value)
414         : input(input)
415         , value(value)
416     {
417     }
418 
419     InputType input;
420     ValueType value;
421 };
422 
423 template&lt;typename InputType, typename ValueType&gt;
424 InputAndValue&lt;InputType, ValueType&gt; inputAndValue(InputType input, ValueType value)
425 {
426     return InputAndValue&lt;InputType, ValueType&gt;(input, value);
427 }
428 
429 template&lt;typename T, typename Func&gt;
430 ALWAYS_INLINE T&amp; ensurePointer(Atomic&lt;T*&gt;&amp; pointer, const Func&amp; func)
</pre>
</td>
<td>
<hr />
<pre>
 36 #include &lt;windows.h&gt;
 37 #include &lt;intrin.h&gt;
 38 #endif
 39 
 40 namespace WTF {
 41 
 42 ALWAYS_INLINE bool hasFence(std::memory_order order)
 43 {
 44     return order != std::memory_order_relaxed;
 45 }
 46 
 47 // Atomic wraps around std::atomic with the sole purpose of making the compare_exchange
 48 // operations not alter the expected value. This is more in line with how we typically
 49 // use CAS in our code.
 50 //
 51 // Atomic is a struct without explicitly defined constructors so that it can be
 52 // initialized at compile time.
 53 
 54 template&lt;typename T&gt;
 55 struct Atomic {
<span class="line-added"> 56     WTF_MAKE_STRUCT_FAST_ALLOCATED;</span>
<span class="line-added"> 57 </span>
 58     // Don&#39;t pass a non-default value for the order parameter unless you really know
 59     // what you are doing and have thought about it very hard. The cost of seq_cst
 60     // is usually not high enough to justify the risk.
 61 
 62     ALWAYS_INLINE T load(std::memory_order order = std::memory_order_seq_cst) const { return value.load(order); }
 63 
 64     ALWAYS_INLINE T loadRelaxed() const { return load(std::memory_order_relaxed); }
 65 
 66     // This is a load that simultaneously does a full fence - neither loads nor stores will move
 67     // above or below it.
 68     ALWAYS_INLINE T loadFullyFenced() const
 69     {
 70         Atomic&lt;T&gt;* ptr = const_cast&lt;Atomic&lt;T&gt;*&gt;(this);
 71         return ptr-&gt;exchangeAdd(T());
 72     }
 73 
 74     ALWAYS_INLINE void store(T desired, std::memory_order order = std::memory_order_seq_cst) { value.store(desired, order); }
 75 
 76     ALWAYS_INLINE void storeRelaxed(T desired) { store(desired, std::memory_order_relaxed); }
 77 
</pre>
<hr />
<pre>
331 typedef unsigned InternalDependencyType;
332 
333 inline InternalDependencyType opaqueMixture()
334 {
335     return 0;
336 }
337 
338 template&lt;typename... Arguments, typename T&gt;
339 inline InternalDependencyType opaqueMixture(T value, Arguments... arguments)
340 {
341     union {
342         InternalDependencyType copy;
343         T value;
344     } u;
345     u.copy = 0;
346     u.value = value;
347     return opaqueMixture(arguments...) + u.copy;
348 }
349 
350 class Dependency {
<span class="line-added">351     WTF_MAKE_FAST_ALLOCATED;</span>
352 public:
353     Dependency()
354         : m_value(0)
355     {
356     }
357 
358     // On TSO architectures, this is a load-load fence and the value it returns is not meaningful (it&#39;s
359     // zero). The load-load fence is usually just a compiler fence. On ARM, this is a self-xor that
360     // produces zero, but it&#39;s concealed from the compiler. The CPU understands this dummy op to be a
361     // phantom dependency.
362     template&lt;typename... Arguments&gt;
363     static Dependency fence(Arguments... arguments)
364     {
365         InternalDependencyType input = opaqueMixture(arguments...);
366         InternalDependencyType output;
367 #if CPU(ARM64)
368         // Create a magical zero value through inline assembly, whose computation
369         // isn&#39;t visible to the optimizer. This zero is then usable as an offset in
370         // further address computations: adding zero does nothing, but the compiler
371         // doesn&#39;t know it. It&#39;s magical because it creates an address dependency
</pre>
<hr />
<pre>
394 
395     // On TSO architectures, this just returns the pointer you pass it. On ARM, this produces a new
396     // pointer that is dependent on this dependency and the input pointer.
397     template&lt;typename T&gt;
398     T* consume(T* pointer)
399     {
400 #if CPU(ARM64) || CPU(ARM)
401         return bitwise_cast&lt;T*&gt;(bitwise_cast&lt;char*&gt;(pointer) + m_value);
402 #else
403         UNUSED_PARAM(m_value);
404         return pointer;
405 #endif
406     }
407 
408 private:
409     InternalDependencyType m_value;
410 };
411 
412 template&lt;typename InputType, typename ValueType&gt;
413 struct InputAndValue {
<span class="line-added">414     WTF_MAKE_STRUCT_FAST_ALLOCATED;</span>
<span class="line-added">415 </span>
416     InputAndValue() { }
417 
418     InputAndValue(InputType input, ValueType value)
419         : input(input)
420         , value(value)
421     {
422     }
423 
424     InputType input;
425     ValueType value;
426 };
427 
428 template&lt;typename InputType, typename ValueType&gt;
429 InputAndValue&lt;InputType, ValueType&gt; inputAndValue(InputType input, ValueType value)
430 {
431     return InputAndValue&lt;InputType, ValueType&gt;(input, value);
432 }
433 
434 template&lt;typename T, typename Func&gt;
435 ALWAYS_INLINE T&amp; ensurePointer(Atomic&lt;T*&gt;&amp; pointer, const Func&amp; func)
</pre>
</td>
</tr>
</table>
<center><a href="Assertions.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="AutodrainedPool.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>