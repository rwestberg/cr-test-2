<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/WebCore/platform/mediastream/libwebrtc/GStreamerVideoDecoderFactory.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../../../style.css" />
  </head>
<body>
<center><a href="../VideoTrackPrivateMediaStream.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../index.html" target="_top">index</a> <a href="GStreamerVideoDecoderFactory.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/WebCore/platform/mediastream/libwebrtc/GStreamerVideoDecoderFactory.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 37 #include &lt;mutex&gt;
 38 #include &lt;wtf/Lock.h&gt;
 39 #include &lt;wtf/StdMap.h&gt;
 40 #include &lt;wtf/glib/RunLoopSourcePriority.h&gt;
 41 #include &lt;wtf/text/WTFString.h&gt;
 42 
 43 GST_DEBUG_CATEGORY(webkit_webrtcdec_debug);
 44 #define GST_CAT_DEFAULT webkit_webrtcdec_debug
 45 
 46 namespace WebCore {
 47 
 48 typedef struct {
 49     uint64_t timestamp;
 50     int64_t renderTimeMs;
 51 } InputTimestamps;
 52 
 53 class GStreamerVideoDecoder : public webrtc::VideoDecoder {
 54 public:
 55     GStreamerVideoDecoder()
 56         : m_pictureId(0)




 57         , m_firstBufferPts(GST_CLOCK_TIME_NONE)
 58         , m_firstBufferDts(GST_CLOCK_TIME_NONE)
 59     {
 60     }
 61 
 62     static void decodebinPadAddedCb(GstElement*,
 63         GstPad* srcpad,
 64         GstPad* sinkpad)
 65     {
 66         GST_INFO_OBJECT(srcpad, &quot;connecting pad with %&quot; GST_PTR_FORMAT, sinkpad);
 67         if (gst_pad_link(srcpad, sinkpad) != GST_PAD_LINK_OK)
 68             ASSERT_NOT_REACHED();
 69     }
 70 
 71     GstElement* pipeline()
 72     {
 73         return m_pipeline.get();
 74     }
 75 
 76     GstElement* makeElement(const gchar* factoryName)
 77     {
 78         GUniquePtr&lt;char&gt; name(g_strdup_printf(&quot;%s_dec_%s_%p&quot;, Name(), factoryName, this));
 79 
 80         return gst_element_factory_make(factoryName, name.get());
 81     }
 82 
<span class="line-modified"> 83     int32_t InitDecode(const webrtc::VideoCodec*, int32_t) override</span>
 84     {
 85         m_src = makeElement(&quot;appsrc&quot;);
 86 

 87         auto capsfilter = CreateFilter();
 88         auto decoder = makeElement(&quot;decodebin&quot;);
 89 










 90         // Make the decoder output &quot;parsed&quot; frames only and let the main decodebin
 91         // do the real decoding. This allows us to have optimized decoding/rendering
 92         // happening in the main pipeline.
<span class="line-modified"> 93         g_object_set(decoder, &quot;caps&quot;, adoptGRef(gst_caps_from_string(Caps())).get(), nullptr);</span>
<span class="line-modified"> 94         auto sinkpad = gst_element_get_static_pad(capsfilter, &quot;sink&quot;);</span>
<span class="line-modified"> 95         g_signal_connect(decoder, &quot;pad-added&quot;, G_CALLBACK(decodebinPadAddedCb), sinkpad);</span>
 96 
<span class="line-modified"> 97         m_pipeline = makeElement(&quot;pipeline&quot;);</span>
<span class="line-modified"> 98         connectSimpleBusMessageCallback(m_pipeline.get());</span>




























 99 
<span class="line-modified">100         auto sink = makeElement(&quot;appsink&quot;);</span>
<span class="line-modified">101         gst_app_sink_set_emit_signals(GST_APP_SINK(sink), true);</span>
<span class="line-modified">102         g_signal_connect(sink, &quot;new-sample&quot;, G_CALLBACK(newSampleCallbackTramp), this);</span>
<span class="line-removed">103         // This is an encoder, everything should happen as fast as possible and not</span>
104         // be synced on the clock.
<span class="line-modified">105         g_object_set(sink, &quot;sync&quot;, false, nullptr);</span>
106 
<span class="line-modified">107         gst_bin_add_many(GST_BIN(pipeline()), m_src, decoder, capsfilter, sink, nullptr);</span>
108         if (!gst_element_link(m_src, decoder)) {
109             GST_ERROR_OBJECT(pipeline(), &quot;Could not link src to decoder.&quot;);
110             return WEBRTC_VIDEO_CODEC_ERROR;
111         }
112 
<span class="line-modified">113         if (!gst_element_link(capsfilter, sink)) {</span>
114             GST_ERROR_OBJECT(pipeline(), &quot;Could not link capsfilter to sink.&quot;);
115             return WEBRTC_VIDEO_CODEC_ERROR;
116         }
117 
118         if (gst_element_set_state(pipeline(), GST_STATE_PLAYING) == GST_STATE_CHANGE_FAILURE) {
119             GST_ERROR_OBJECT(pipeline(), &quot;Could not set state to PLAYING.&quot;);
120             return WEBRTC_VIDEO_CODEC_ERROR;
121         }
122 
123         return WEBRTC_VIDEO_CODEC_OK;
124     }
125 
126     int32_t RegisterDecodeCompleteCallback(webrtc::DecodedImageCallback* callback)
127     {
128         m_imageReadyCb = callback;
129 
130         return WEBRTC_VIDEO_CODEC_OK;
131     }
132 
133     virtual GstElement* CreateFilter()
134     {
135         return makeElement(&quot;identity&quot;);
136     }
137 
138     int32_t Release() final
139     {
140         if (m_pipeline.get()) {
141             GRefPtr&lt;GstBus&gt; bus = adoptGRef(gst_pipeline_get_bus(GST_PIPELINE(m_pipeline.get())));
142             gst_bus_set_sync_handler(bus.get(), nullptr, nullptr, nullptr);
143 
144             gst_element_set_state(m_pipeline.get(), GST_STATE_NULL);
145             m_src = nullptr;

146             m_pipeline = nullptr;
147         }
148 
149         return WEBRTC_VIDEO_CODEC_OK;
150     }
151 
152     int32_t Decode(const webrtc::EncodedImage&amp; inputImage,
153         bool,
154         const webrtc::CodecSpecificInfo*,
155         int64_t renderTimeMs) override
156     {














157         if (!m_src) {
158             GST_ERROR(&quot;No source set, can&#39;t decode.&quot;);
159 
160             return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
161         }
162 
163         if (!GST_CLOCK_TIME_IS_VALID(m_firstBufferPts)) {
164             GRefPtr&lt;GstPad&gt; srcpad = adoptGRef(gst_element_get_static_pad(m_src, &quot;src&quot;));
165             m_firstBufferPts = (static_cast&lt;guint64&gt;(renderTimeMs)) * GST_MSECOND;
166             m_firstBufferDts = (static_cast&lt;guint64&gt;(inputImage.Timestamp())) * GST_MSECOND;
167         }
168 
169         // FIXME- Use a GstBufferPool.
170         auto buffer = adoptGRef(gst_buffer_new_wrapped(g_memdup(inputImage._buffer, inputImage._size),
171             inputImage._size));
172         GST_BUFFER_DTS(buffer.get()) = (static_cast&lt;guint64&gt;(inputImage.Timestamp()) * GST_MSECOND) - m_firstBufferDts;
173         GST_BUFFER_PTS(buffer.get()) = (static_cast&lt;guint64&gt;(renderTimeMs) * GST_MSECOND) - m_firstBufferPts;
<span class="line-modified">174         {</span>
<span class="line-modified">175             auto locker = holdLock(m_bufferMapLock);</span>
<span class="line-removed">176             InputTimestamps timestamps = {inputImage.Timestamp(), renderTimeMs};</span>
<span class="line-removed">177             m_dtsPtsMap[GST_BUFFER_PTS(buffer.get())] = timestamps;</span>
<span class="line-removed">178         }</span>
179 
<span class="line-modified">180         GST_LOG_OBJECT(pipeline(), &quot;%ld Decoding: %&quot; GST_PTR_FORMAT, renderTimeMs, buffer.get());</span>
181         auto sample = adoptGRef(gst_sample_new(buffer.get(), GetCapsForFrame(inputImage), nullptr, nullptr));
182         switch (gst_app_src_push_sample(GST_APP_SRC(m_src), sample.get())) {
183         case GST_FLOW_OK:
<span class="line-modified">184             return WEBRTC_VIDEO_CODEC_OK;</span>
185         case GST_FLOW_FLUSHING:
186             return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
187         default:
188             return WEBRTC_VIDEO_CODEC_ERROR;
189         }




























190     }
191 
192     virtual GstCaps* GetCapsForFrame(const webrtc::EncodedImage&amp; image)
193     {
194         if (!m_caps) {
195             m_caps = adoptGRef(gst_caps_new_simple(Caps(),
<span class="line-modified">196                 &quot;width&quot;, G_TYPE_INT, image._encodedWidth,</span>
<span class="line-modified">197                 &quot;height&quot;, G_TYPE_INT, image._encodedHeight,</span>
198                 nullptr));
199         }
200 
201         return m_caps.get();
202     }
203 
204     void AddDecoderIfSupported(std::vector&lt;webrtc::SdpVideoFormat&gt; codecList)
205     {
206         if (HasGstDecoder()) {
207             webrtc::SdpVideoFormat format = ConfigureSupportedDecoder();
208 
209             codecList.push_back(format);
210         }
211     }
212 
213     virtual webrtc::SdpVideoFormat ConfigureSupportedDecoder()
214     {
215         return webrtc::SdpVideoFormat(Name());
216     }
217 
</pre>
<hr />
<pre>
220         auto all_decoders = gst_element_factory_list_get_elements(GST_ELEMENT_FACTORY_TYPE_DECODER,
221             GST_RANK_MARGINAL);
222         auto caps = adoptGRef(gst_caps_from_string(capsStr));
223         auto decoders = gst_element_factory_list_filter(all_decoders,
224             caps.get(), GST_PAD_SINK, FALSE);
225 
226         gst_plugin_feature_list_free(all_decoders);
227         GRefPtr&lt;GstElementFactory&gt; res;
228         if (decoders)
229             res = GST_ELEMENT_FACTORY(decoders-&gt;data);
230         gst_plugin_feature_list_free(decoders);
231 
232         return res;
233     }
234 
235     bool HasGstDecoder()
236     {
237         return GstDecoderFactory(Caps());
238     }
239 
<span class="line-removed">240     GstFlowReturn newSampleCallback(GstElement* sink)</span>
<span class="line-removed">241     {</span>
<span class="line-removed">242         auto sample = gst_app_sink_pull_sample(GST_APP_SINK(sink));</span>
<span class="line-removed">243         auto buffer = gst_sample_get_buffer(sample);</span>
<span class="line-removed">244 </span>
<span class="line-removed">245         m_bufferMapLock.lock();</span>
<span class="line-removed">246         // Make sure that the frame.timestamp == previsouly input_frame._timeStamp</span>
<span class="line-removed">247         // as it is required by the VideoDecoder baseclass.</span>
<span class="line-removed">248         auto timestamps = m_dtsPtsMap[GST_BUFFER_PTS(buffer)];</span>
<span class="line-removed">249         m_dtsPtsMap.erase(GST_BUFFER_PTS(buffer));</span>
<span class="line-removed">250         m_bufferMapLock.unlock();</span>
<span class="line-removed">251 </span>
<span class="line-removed">252         auto frame(LibWebRTCVideoFrameFromGStreamerSample(sample, webrtc::kVideoRotation_0,</span>
<span class="line-removed">253             timestamps.timestamp, timestamps.renderTimeMs));</span>
<span class="line-removed">254 </span>
<span class="line-removed">255         GST_BUFFER_DTS(buffer) = GST_CLOCK_TIME_NONE;</span>
<span class="line-removed">256         GST_LOG_OBJECT(pipeline(), &quot;Output decoded frame! %d -&gt; %&quot; GST_PTR_FORMAT,</span>
<span class="line-removed">257             frame-&gt;timestamp(), buffer);</span>
<span class="line-removed">258 </span>
<span class="line-removed">259         m_imageReadyCb-&gt;Decoded(*frame.get(), absl::optional&lt;int32_t&gt;(), absl::optional&lt;uint8_t&gt;());</span>
<span class="line-removed">260 </span>
<span class="line-removed">261         return GST_FLOW_OK;</span>
<span class="line-removed">262     }</span>
<span class="line-removed">263 </span>
264     virtual const gchar* Caps() = 0;
265     virtual webrtc::VideoCodecType CodecType() = 0;
266     const char* ImplementationName() const { return &quot;GStreamer&quot;; }
267     virtual const gchar* Name() = 0;
268 
269 protected:
270     GRefPtr&lt;GstCaps&gt; m_caps;
271     gint m_pictureId;




272 
273 private:
<span class="line-removed">274     static GstFlowReturn newSampleCallbackTramp(GstElement* sink, GStreamerVideoDecoder* enc)</span>
<span class="line-removed">275     {</span>
<span class="line-removed">276         return enc-&gt;newSampleCallback(sink);</span>
<span class="line-removed">277     }</span>
<span class="line-removed">278 </span>
279     GRefPtr&lt;GstElement&gt; m_pipeline;

280     GstElement* m_src;
281 
282     GstVideoInfo m_info;
283     webrtc::DecodedImageCallback* m_imageReadyCb;
284 
<span class="line-removed">285     Lock m_bufferMapLock;</span>
286     StdMap&lt;GstClockTime, InputTimestamps&gt; m_dtsPtsMap;
287     GstClockTime m_firstBufferPts;
288     GstClockTime m_firstBufferDts;
289 };
290 
291 class H264Decoder : public GStreamerVideoDecoder {
292 public:
<span class="line-modified">293     H264Decoder() { }</span>
294 
295     int32_t InitDecode(const webrtc::VideoCodec* codecInfo, int32_t nCores) final
296     {
297         if (codecInfo &amp;&amp; codecInfo-&gt;codecType != webrtc::kVideoCodecH264)
298             return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
299 
300         m_profile = nullptr;
301         if (codecInfo) {
302             auto h264Info = codecInfo-&gt;H264();
303 
304             switch (h264Info.profile) {
305             case webrtc::H264::kProfileConstrainedBaseline:
306                 m_profile = &quot;constrained-baseline&quot;;
307                 break;
308             case webrtc::H264::kProfileBaseline:
309                 m_profile = &quot;baseline&quot;;
310                 break;
311             case webrtc::H264::kProfileMain:
312                 m_profile = &quot;main&quot;;
313                 break;
314             case webrtc::H264::kProfileConstrainedHigh:
315                 m_profile = &quot;constrained-high&quot;;
316                 break;
317             case webrtc::H264::kProfileHigh:
318                 m_profile = &quot;high&quot;;
319                 break;
320             }
321         }
322 
323         return GStreamerVideoDecoder::InitDecode(codecInfo, nCores);
324     }
325 
326     GstCaps* GetCapsForFrame(const webrtc::EncodedImage&amp; image) final
327     {
328         if (!m_caps) {
329             m_caps = adoptGRef(gst_caps_new_simple(Caps(),
<span class="line-modified">330                 &quot;width&quot;, G_TYPE_INT, image._encodedWidth,</span>
<span class="line-modified">331                 &quot;height&quot;, G_TYPE_INT, image._encodedHeight,</span>
<span class="line-removed">332                 &quot;profile&quot;, G_TYPE_STRING, m_profile ? m_profile : &quot;baseline&quot;,</span>
<span class="line-removed">333                 &quot;stream-format&quot;, G_TYPE_STRING, &quot;byte-stream&quot;,</span>
334                 &quot;alignment&quot;, G_TYPE_STRING, &quot;au&quot;,
335                 nullptr));
336         }
337 
338         return m_caps.get();
339     }
340     const gchar* Caps() final { return &quot;video/x-h264&quot;; }
341     const gchar* Name() final { return cricket::kH264CodecName; }
342     webrtc::VideoCodecType CodecType() final { return webrtc::kVideoCodecH264; }
343 
344 private:
345     const gchar* m_profile;
346 };
347 
348 class VP8Decoder : public GStreamerVideoDecoder {
349 public:
350     VP8Decoder() { }
351     const gchar* Caps() final { return &quot;video/x-vp8&quot;; }
352     const gchar* Name() final { return cricket::kVp8CodecName; }
353     webrtc::VideoCodecType CodecType() final { return webrtc::kVideoCodecVP8; }
</pre>
</td>
<td>
<hr />
<pre>
 37 #include &lt;mutex&gt;
 38 #include &lt;wtf/Lock.h&gt;
 39 #include &lt;wtf/StdMap.h&gt;
 40 #include &lt;wtf/glib/RunLoopSourcePriority.h&gt;
 41 #include &lt;wtf/text/WTFString.h&gt;
 42 
 43 GST_DEBUG_CATEGORY(webkit_webrtcdec_debug);
 44 #define GST_CAT_DEFAULT webkit_webrtcdec_debug
 45 
 46 namespace WebCore {
 47 
 48 typedef struct {
 49     uint64_t timestamp;
 50     int64_t renderTimeMs;
 51 } InputTimestamps;
 52 
 53 class GStreamerVideoDecoder : public webrtc::VideoDecoder {
 54 public:
 55     GStreamerVideoDecoder()
 56         : m_pictureId(0)
<span class="line-added"> 57         , m_width(0)</span>
<span class="line-added"> 58         , m_height(0)</span>
<span class="line-added"> 59         , m_requireParse(false)</span>
<span class="line-added"> 60         , m_needsKeyframe(true)</span>
 61         , m_firstBufferPts(GST_CLOCK_TIME_NONE)
 62         , m_firstBufferDts(GST_CLOCK_TIME_NONE)
 63     {
 64     }
 65 
 66     static void decodebinPadAddedCb(GstElement*,
 67         GstPad* srcpad,
 68         GstPad* sinkpad)
 69     {
 70         GST_INFO_OBJECT(srcpad, &quot;connecting pad with %&quot; GST_PTR_FORMAT, sinkpad);
 71         if (gst_pad_link(srcpad, sinkpad) != GST_PAD_LINK_OK)
 72             ASSERT_NOT_REACHED();
 73     }
 74 
 75     GstElement* pipeline()
 76     {
 77         return m_pipeline.get();
 78     }
 79 
 80     GstElement* makeElement(const gchar* factoryName)
 81     {
 82         GUniquePtr&lt;char&gt; name(g_strdup_printf(&quot;%s_dec_%s_%p&quot;, Name(), factoryName, this));
 83 
 84         return gst_element_factory_make(factoryName, name.get());
 85     }
 86 
<span class="line-modified"> 87     int32_t InitDecode(const webrtc::VideoCodec* codecSettings, int32_t) override</span>
 88     {
 89         m_src = makeElement(&quot;appsrc&quot;);
 90 
<span class="line-added"> 91         GRefPtr&lt;GstCaps&gt; caps = nullptr;</span>
 92         auto capsfilter = CreateFilter();
 93         auto decoder = makeElement(&quot;decodebin&quot;);
 94 
<span class="line-added"> 95         if (codecSettings) {</span>
<span class="line-added"> 96             m_width = codecSettings-&gt;width;</span>
<span class="line-added"> 97             m_height = codecSettings-&gt;height;</span>
<span class="line-added"> 98         }</span>
<span class="line-added"> 99 </span>
<span class="line-added">100         m_pipeline = makeElement(&quot;pipeline&quot;);</span>
<span class="line-added">101         connectSimpleBusMessageCallback(m_pipeline.get());</span>
<span class="line-added">102 </span>
<span class="line-added">103         auto sinkpad = adoptGRef(gst_element_get_static_pad(capsfilter, &quot;sink&quot;));</span>
<span class="line-added">104         g_signal_connect(decoder, &quot;pad-added&quot;, G_CALLBACK(decodebinPadAddedCb), sinkpad.get());</span>
105         // Make the decoder output &quot;parsed&quot; frames only and let the main decodebin
106         // do the real decoding. This allows us to have optimized decoding/rendering
107         // happening in the main pipeline.
<span class="line-modified">108         if (m_requireParse) {</span>
<span class="line-modified">109             caps = gst_caps_new_simple(Caps(), &quot;parsed&quot;, G_TYPE_BOOLEAN, TRUE, nullptr);</span>
<span class="line-modified">110             GRefPtr&lt;GstBus&gt; bus = adoptGRef(gst_pipeline_get_bus(GST_PIPELINE(m_pipeline.get())));</span>
111 
<span class="line-modified">112             gst_bus_enable_sync_message_emission(bus.get());</span>
<span class="line-modified">113             g_signal_connect(bus.get(), &quot;sync-message::warning&quot;,</span>
<span class="line-added">114                 G_CALLBACK(+[](GstBus*, GstMessage* message, GStreamerVideoDecoder* justThis) {</span>
<span class="line-added">115                 GUniqueOutPtr&lt;GError&gt; err;</span>
<span class="line-added">116 </span>
<span class="line-added">117                 switch (GST_MESSAGE_TYPE(message)) {</span>
<span class="line-added">118                 case GST_MESSAGE_WARNING: {</span>
<span class="line-added">119                     gst_message_parse_warning(message, &amp;err.outPtr(), nullptr);</span>
<span class="line-added">120                     FALLTHROUGH;</span>
<span class="line-added">121                 }</span>
<span class="line-added">122                 case GST_MESSAGE_ERROR: {</span>
<span class="line-added">123                     if (!err)</span>
<span class="line-added">124                         gst_message_parse_error(message, &amp;err.outPtr(), nullptr);</span>
<span class="line-added">125 </span>
<span class="line-added">126                     if (g_error_matches(err.get(), GST_STREAM_ERROR, GST_STREAM_ERROR_DECODE)) {</span>
<span class="line-added">127                         GST_INFO_OBJECT(justThis-&gt;pipeline(), &quot;--&gt; needs keyframe (%s)&quot;,</span>
<span class="line-added">128                             err-&gt;message);</span>
<span class="line-added">129                         justThis-&gt;m_needsKeyframe = true;</span>
<span class="line-added">130                     }</span>
<span class="line-added">131                     break;</span>
<span class="line-added">132                 }</span>
<span class="line-added">133                 default:</span>
<span class="line-added">134                     break;</span>
<span class="line-added">135                 }</span>
<span class="line-added">136                 }), this);</span>
<span class="line-added">137         } else {</span>
<span class="line-added">138             /* FIXME - How could we handle missing keyframes case we do not plug parsers ? */</span>
<span class="line-added">139             caps = gst_caps_new_empty_simple(Caps());</span>
<span class="line-added">140         }</span>
<span class="line-added">141         g_object_set(decoder, &quot;caps&quot;, caps.get(), nullptr);</span>
142 
<span class="line-modified">143         m_sink = makeElement(&quot;appsink&quot;);</span>
<span class="line-modified">144         gst_app_sink_set_emit_signals(GST_APP_SINK(m_sink), true);</span>
<span class="line-modified">145         // This is an decoder, everything should happen as fast as possible and not</span>

146         // be synced on the clock.
<span class="line-modified">147         g_object_set(m_sink, &quot;sync&quot;, false, nullptr);</span>
148 
<span class="line-modified">149         gst_bin_add_many(GST_BIN(pipeline()), m_src, decoder, capsfilter, m_sink, nullptr);</span>
150         if (!gst_element_link(m_src, decoder)) {
151             GST_ERROR_OBJECT(pipeline(), &quot;Could not link src to decoder.&quot;);
152             return WEBRTC_VIDEO_CODEC_ERROR;
153         }
154 
<span class="line-modified">155         if (!gst_element_link(capsfilter, m_sink)) {</span>
156             GST_ERROR_OBJECT(pipeline(), &quot;Could not link capsfilter to sink.&quot;);
157             return WEBRTC_VIDEO_CODEC_ERROR;
158         }
159 
160         if (gst_element_set_state(pipeline(), GST_STATE_PLAYING) == GST_STATE_CHANGE_FAILURE) {
161             GST_ERROR_OBJECT(pipeline(), &quot;Could not set state to PLAYING.&quot;);
162             return WEBRTC_VIDEO_CODEC_ERROR;
163         }
164 
165         return WEBRTC_VIDEO_CODEC_OK;
166     }
167 
168     int32_t RegisterDecodeCompleteCallback(webrtc::DecodedImageCallback* callback)
169     {
170         m_imageReadyCb = callback;
171 
172         return WEBRTC_VIDEO_CODEC_OK;
173     }
174 
175     virtual GstElement* CreateFilter()
176     {
177         return makeElement(&quot;identity&quot;);
178     }
179 
180     int32_t Release() final
181     {
182         if (m_pipeline.get()) {
183             GRefPtr&lt;GstBus&gt; bus = adoptGRef(gst_pipeline_get_bus(GST_PIPELINE(m_pipeline.get())));
184             gst_bus_set_sync_handler(bus.get(), nullptr, nullptr, nullptr);
185 
186             gst_element_set_state(m_pipeline.get(), GST_STATE_NULL);
187             m_src = nullptr;
<span class="line-added">188             m_sink = nullptr;</span>
189             m_pipeline = nullptr;
190         }
191 
192         return WEBRTC_VIDEO_CODEC_OK;
193     }
194 
195     int32_t Decode(const webrtc::EncodedImage&amp; inputImage,
196         bool,
197         const webrtc::CodecSpecificInfo*,
198         int64_t renderTimeMs) override
199     {
<span class="line-added">200         if (m_needsKeyframe) {</span>
<span class="line-added">201             if (inputImage._frameType != webrtc::kVideoFrameKey) {</span>
<span class="line-added">202                 GST_ERROR(&quot;Waiting for keyframe but got a delta unit... asking for keyframe&quot;);</span>
<span class="line-added">203                 return WEBRTC_VIDEO_CODEC_ERROR;</span>
<span class="line-added">204             }</span>
<span class="line-added">205             if (inputImage._completeFrame)</span>
<span class="line-added">206                 m_needsKeyframe = false;</span>
<span class="line-added">207             else {</span>
<span class="line-added">208                 GST_ERROR(&quot;Waiting for keyframe but didn&#39;t get full frame, getting out&quot;);</span>
<span class="line-added">209                 return WEBRTC_VIDEO_CODEC_ERROR;</span>
<span class="line-added">210             }</span>
<span class="line-added">211         }</span>
<span class="line-added">212 </span>
<span class="line-added">213 </span>
214         if (!m_src) {
215             GST_ERROR(&quot;No source set, can&#39;t decode.&quot;);
216 
217             return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
218         }
219 
220         if (!GST_CLOCK_TIME_IS_VALID(m_firstBufferPts)) {
221             GRefPtr&lt;GstPad&gt; srcpad = adoptGRef(gst_element_get_static_pad(m_src, &quot;src&quot;));
222             m_firstBufferPts = (static_cast&lt;guint64&gt;(renderTimeMs)) * GST_MSECOND;
223             m_firstBufferDts = (static_cast&lt;guint64&gt;(inputImage.Timestamp())) * GST_MSECOND;
224         }
225 
226         // FIXME- Use a GstBufferPool.
227         auto buffer = adoptGRef(gst_buffer_new_wrapped(g_memdup(inputImage._buffer, inputImage._size),
228             inputImage._size));
229         GST_BUFFER_DTS(buffer.get()) = (static_cast&lt;guint64&gt;(inputImage.Timestamp()) * GST_MSECOND) - m_firstBufferDts;
230         GST_BUFFER_PTS(buffer.get()) = (static_cast&lt;guint64&gt;(renderTimeMs) * GST_MSECOND) - m_firstBufferPts;
<span class="line-modified">231         InputTimestamps timestamps = {inputImage.Timestamp(), renderTimeMs};</span>
<span class="line-modified">232         m_dtsPtsMap[GST_BUFFER_PTS(buffer.get())] = timestamps;</span>



233 
<span class="line-modified">234         GST_LOG_OBJECT(pipeline(), &quot;%&quot; G_GINT64_FORMAT &quot; Decoding: %&quot; GST_PTR_FORMAT, renderTimeMs, buffer.get());</span>
235         auto sample = adoptGRef(gst_sample_new(buffer.get(), GetCapsForFrame(inputImage), nullptr, nullptr));
236         switch (gst_app_src_push_sample(GST_APP_SRC(m_src), sample.get())) {
237         case GST_FLOW_OK:
<span class="line-modified">238             break;</span>
239         case GST_FLOW_FLUSHING:
240             return WEBRTC_VIDEO_CODEC_UNINITIALIZED;
241         default:
242             return WEBRTC_VIDEO_CODEC_ERROR;
243         }
<span class="line-added">244 </span>
<span class="line-added">245         return pullSample();</span>
<span class="line-added">246     }</span>
<span class="line-added">247 </span>
<span class="line-added">248     int32_t pullSample()</span>
<span class="line-added">249     {</span>
<span class="line-added">250         auto sample = gst_app_sink_try_pull_sample(GST_APP_SINK(m_sink), GST_SECOND / 30);</span>
<span class="line-added">251         if (!sample) {</span>
<span class="line-added">252             GST_ERROR(&quot;Needs more data&quot;);</span>
<span class="line-added">253             return WEBRTC_VIDEO_CODEC_OK;</span>
<span class="line-added">254         }</span>
<span class="line-added">255         auto buffer = gst_sample_get_buffer(sample);</span>
<span class="line-added">256 </span>
<span class="line-added">257         // Make sure that the frame.timestamp == previsouly input_frame._timeStamp</span>
<span class="line-added">258         // as it is required by the VideoDecoder baseclass.</span>
<span class="line-added">259         auto timestamps = m_dtsPtsMap[GST_BUFFER_PTS(buffer)];</span>
<span class="line-added">260         m_dtsPtsMap.erase(GST_BUFFER_PTS(buffer));</span>
<span class="line-added">261 </span>
<span class="line-added">262         auto frame(LibWebRTCVideoFrameFromGStreamerSample(sample, webrtc::kVideoRotation_0,</span>
<span class="line-added">263             timestamps.timestamp, timestamps.renderTimeMs));</span>
<span class="line-added">264 </span>
<span class="line-added">265         GST_BUFFER_DTS(buffer) = GST_CLOCK_TIME_NONE;</span>
<span class="line-added">266         GST_LOG_OBJECT(pipeline(), &quot;Output decoded frame! %d -&gt; %&quot; GST_PTR_FORMAT,</span>
<span class="line-added">267             frame-&gt;timestamp(), buffer);</span>
<span class="line-added">268 </span>
<span class="line-added">269         m_imageReadyCb-&gt;Decoded(*frame.get(), absl::optional&lt;int32_t&gt;(), absl::optional&lt;uint8_t&gt;());</span>
<span class="line-added">270 </span>
<span class="line-added">271         return WEBRTC_VIDEO_CODEC_OK;</span>
272     }
273 
274     virtual GstCaps* GetCapsForFrame(const webrtc::EncodedImage&amp; image)
275     {
276         if (!m_caps) {
277             m_caps = adoptGRef(gst_caps_new_simple(Caps(),
<span class="line-modified">278                 &quot;width&quot;, G_TYPE_INT, image._encodedWidth ? image._encodedWidth : m_width,</span>
<span class="line-modified">279                 &quot;height&quot;, G_TYPE_INT, image._encodedHeight ? image._encodedHeight : m_height,</span>
280                 nullptr));
281         }
282 
283         return m_caps.get();
284     }
285 
286     void AddDecoderIfSupported(std::vector&lt;webrtc::SdpVideoFormat&gt; codecList)
287     {
288         if (HasGstDecoder()) {
289             webrtc::SdpVideoFormat format = ConfigureSupportedDecoder();
290 
291             codecList.push_back(format);
292         }
293     }
294 
295     virtual webrtc::SdpVideoFormat ConfigureSupportedDecoder()
296     {
297         return webrtc::SdpVideoFormat(Name());
298     }
299 
</pre>
<hr />
<pre>
302         auto all_decoders = gst_element_factory_list_get_elements(GST_ELEMENT_FACTORY_TYPE_DECODER,
303             GST_RANK_MARGINAL);
304         auto caps = adoptGRef(gst_caps_from_string(capsStr));
305         auto decoders = gst_element_factory_list_filter(all_decoders,
306             caps.get(), GST_PAD_SINK, FALSE);
307 
308         gst_plugin_feature_list_free(all_decoders);
309         GRefPtr&lt;GstElementFactory&gt; res;
310         if (decoders)
311             res = GST_ELEMENT_FACTORY(decoders-&gt;data);
312         gst_plugin_feature_list_free(decoders);
313 
314         return res;
315     }
316 
317     bool HasGstDecoder()
318     {
319         return GstDecoderFactory(Caps());
320     }
321 
























322     virtual const gchar* Caps() = 0;
323     virtual webrtc::VideoCodecType CodecType() = 0;
324     const char* ImplementationName() const { return &quot;GStreamer&quot;; }
325     virtual const gchar* Name() = 0;
326 
327 protected:
328     GRefPtr&lt;GstCaps&gt; m_caps;
329     gint m_pictureId;
<span class="line-added">330     gint m_width;</span>
<span class="line-added">331     gint m_height;</span>
<span class="line-added">332     bool m_requireParse = false;</span>
<span class="line-added">333     bool m_needsKeyframe;</span>
334 
335 private:





336     GRefPtr&lt;GstElement&gt; m_pipeline;
<span class="line-added">337     GstElement* m_sink;</span>
338     GstElement* m_src;
339 
340     GstVideoInfo m_info;
341     webrtc::DecodedImageCallback* m_imageReadyCb;
342 

343     StdMap&lt;GstClockTime, InputTimestamps&gt; m_dtsPtsMap;
344     GstClockTime m_firstBufferPts;
345     GstClockTime m_firstBufferDts;
346 };
347 
348 class H264Decoder : public GStreamerVideoDecoder {
349 public:
<span class="line-modified">350     H264Decoder() { m_requireParse = true; }</span>
351 
352     int32_t InitDecode(const webrtc::VideoCodec* codecInfo, int32_t nCores) final
353     {
354         if (codecInfo &amp;&amp; codecInfo-&gt;codecType != webrtc::kVideoCodecH264)
355             return WEBRTC_VIDEO_CODEC_ERR_PARAMETER;
356 
357         m_profile = nullptr;
358         if (codecInfo) {
359             auto h264Info = codecInfo-&gt;H264();
360 
361             switch (h264Info.profile) {
362             case webrtc::H264::kProfileConstrainedBaseline:
363                 m_profile = &quot;constrained-baseline&quot;;
364                 break;
365             case webrtc::H264::kProfileBaseline:
366                 m_profile = &quot;baseline&quot;;
367                 break;
368             case webrtc::H264::kProfileMain:
369                 m_profile = &quot;main&quot;;
370                 break;
371             case webrtc::H264::kProfileConstrainedHigh:
372                 m_profile = &quot;constrained-high&quot;;
373                 break;
374             case webrtc::H264::kProfileHigh:
375                 m_profile = &quot;high&quot;;
376                 break;
377             }
378         }
379 
380         return GStreamerVideoDecoder::InitDecode(codecInfo, nCores);
381     }
382 
383     GstCaps* GetCapsForFrame(const webrtc::EncodedImage&amp; image) final
384     {
385         if (!m_caps) {
386             m_caps = adoptGRef(gst_caps_new_simple(Caps(),
<span class="line-modified">387                 &quot;width&quot;, G_TYPE_INT, image._encodedWidth ? image._encodedWidth : m_width,</span>
<span class="line-modified">388                 &quot;height&quot;, G_TYPE_INT, image._encodedHeight ? image._encodedHeight : m_height,</span>


389                 &quot;alignment&quot;, G_TYPE_STRING, &quot;au&quot;,
390                 nullptr));
391         }
392 
393         return m_caps.get();
394     }
395     const gchar* Caps() final { return &quot;video/x-h264&quot;; }
396     const gchar* Name() final { return cricket::kH264CodecName; }
397     webrtc::VideoCodecType CodecType() final { return webrtc::kVideoCodecH264; }
398 
399 private:
400     const gchar* m_profile;
401 };
402 
403 class VP8Decoder : public GStreamerVideoDecoder {
404 public:
405     VP8Decoder() { }
406     const gchar* Caps() final { return &quot;video/x-vp8&quot;; }
407     const gchar* Name() final { return cricket::kVp8CodecName; }
408     webrtc::VideoCodecType CodecType() final { return webrtc::kVideoCodecVP8; }
</pre>
</td>
</tr>
</table>
<center><a href="../VideoTrackPrivateMediaStream.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../index.html" target="_top">index</a> <a href="GStreamerVideoDecoderFactory.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>