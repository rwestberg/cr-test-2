<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter.asm</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre><a name="1" id="anc1"></a><span class="line-modified">   1 # Copyrsght (C) 2011-2019 Apple Inc. All rights reserved.</span>
   2 #
   3 # Redistribution and use in source and binary forms, with or without
   4 # modification, are permitted provided that the following conditions
   5 # are met:
   6 # 1. Redistributions of source code must retain the above copyright
   7 #    notice, this list of conditions and the following disclaimer.
   8 # 2. Redistributions in binary form must reproduce the above copyright
   9 #    notice, this list of conditions and the following disclaimer in the
  10 #    documentation and/or other materials provided with the distribution.
  11 #
  12 # THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
  13 # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
  14 # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  15 # PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
  16 # BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  17 # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  18 # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  19 # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  20 # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  21 # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
  22 # THE POSSIBILITY OF SUCH DAMAGE.
  23 
  24 # Crash course on the language that this is written in (which I just call
  25 # &quot;assembly&quot; even though it&#39;s more than that):
  26 #
  27 # - Mostly gas-style operand ordering. The last operand tends to be the
  28 #   destination. So &quot;a := b&quot; is written as &quot;mov b, a&quot;. But unlike gas,
  29 #   comparisons are in-order, so &quot;if (a &lt; b)&quot; is written as
  30 #   &quot;bilt a, b, ...&quot;.
  31 #
  32 # - &quot;b&quot; = byte, &quot;h&quot; = 16-bit word, &quot;i&quot; = 32-bit word, &quot;p&quot; = pointer.
  33 #   For 32-bit, &quot;i&quot; and &quot;p&quot; are interchangeable except when an op supports one
  34 #   but not the other.
  35 #
  36 # - In general, valid operands for macro invocations and instructions are
  37 #   registers (eg &quot;t0&quot;), addresses (eg &quot;4[t0]&quot;), base-index addresses
  38 #   (eg &quot;7[t0, t1, 2]&quot;), absolute addresses (eg &quot;0xa0000000[]&quot;), or labels
  39 #   (eg &quot;_foo&quot; or &quot;.foo&quot;). Macro invocations can also take anonymous
  40 #   macros as operands. Instructions cannot take anonymous macros.
  41 #
  42 # - Labels must have names that begin with either &quot;_&quot; or &quot;.&quot;.  A &quot;.&quot; label
  43 #   is local and gets renamed before code gen to minimize namespace
  44 #   pollution. A &quot;_&quot; label is an extern symbol (i.e. &quot;.globl&quot;). The &quot;_&quot;
  45 #   may or may not be removed during code gen depending on whether the asm
  46 #   conventions for C name mangling on the target platform mandate a &quot;_&quot;
  47 #   prefix.
  48 #
  49 # - A &quot;macro&quot; is a lambda expression, which may be either anonymous or
  50 #   named. But this has caveats. &quot;macro&quot; can take zero or more arguments,
  51 #   which may be macros or any valid operands, but it can only return
  52 #   code. But you can do Turing-complete things via continuation passing
  53 #   style: &quot;macro foo (a, b) b(a, a) end foo(foo, foo)&quot;. Actually, don&#39;t do
  54 #   that, since you&#39;ll just crash the assembler.
  55 #
  56 # - An &quot;if&quot; is a conditional on settings. Any identifier supplied in the
  57 #   predicate of an &quot;if&quot; is assumed to be a #define that is available
  58 #   during code gen. So you can&#39;t use &quot;if&quot; for computation in a macro, but
  59 #   you can use it to select different pieces of code for different
  60 #   platforms.
  61 #
  62 # - Arguments to macros follow lexical scoping rather than dynamic scoping.
  63 #   Const&#39;s also follow lexical scoping and may override (hide) arguments
  64 #   or other consts. All variables (arguments and constants) can be bound
  65 #   to operands. Additionally, arguments (but not constants) can be bound
  66 #   to macros.
  67 
  68 # The following general-purpose registers are available:
  69 #
  70 #  - cfr and sp hold the call frame and (native) stack pointer respectively.
  71 #  They are callee-save registers, and guaranteed to be distinct from all other
  72 #  registers on all architectures.
  73 #
  74 #  - lr is defined on non-X86 architectures (ARM64, ARM64E, ARMv7, MIPS and CLOOP)
  75 #  and holds the return PC
  76 #
  77 #  - pc holds the (native) program counter on 32-bits ARM architectures (ARMv7)
  78 #
<a name="2" id="anc2"></a><span class="line-modified">  79 #  - t0, t1, t2, t3, t4, and optionally t5, t6, and t7 are temporary registers that can get trashed on</span>
  80 #  calls, and are pairwise distinct registers. t4 holds the JS program counter, so use
  81 #  with caution in opcodes (actually, don&#39;t use it in opcodes at all, except as PC).
  82 #
  83 #  - r0 and r1 are the platform&#39;s customary return registers, and thus are
  84 #  two distinct registers
  85 #
  86 #  - a0, a1, a2 and a3 are the platform&#39;s customary argument registers, and
  87 #  thus are pairwise distinct registers. Be mindful that:
  88 #    + On X86, there are no argument registers. a0 and a1 are edx and
  89 #    ecx following the fastcall convention, but you should still use the stack
  90 #    to pass your arguments. The cCall2 and cCall4 macros do this for you.
  91 #    + On X86_64_WIN, you should allocate space on the stack for the arguments,
  92 #    and the return convention is weird for &gt; 8 bytes types. The only place we
  93 #    use &gt; 8 bytes return values is on a cCall, and cCall2 and cCall4 handle
  94 #    this for you.
  95 #
  96 #  - The only registers guaranteed to be caller-saved are r0, r1, a0, a1 and a2, and
  97 #  you should be mindful of that in functions that are called directly from C.
  98 #  If you need more registers, you should push and pop them like a good
  99 #  assembly citizen, because any other register will be callee-saved on X86.
 100 #
 101 # You can additionally assume:
 102 #
 103 #  - a3, t2, t3, t4 and t5 are never return registers; t0, t1, a0, a1 and a2
 104 #  can be return registers.
 105 #
 106 #  - t4 and t5 are never argument registers, t3 can only be a3, t1 can only be
 107 #  a1; but t0 and t2 can be either a0 or a2.
 108 #
 109 #  - On 64 bits, there are callee-save registers named csr0, csr1, ... csrN.
 110 #  The last three csr registers are used used to store the PC base and
 111 #  two special tag values. Don&#39;t use them for anything else.
 112 #
 113 # Additional platform-specific details (you shouldn&#39;t rely on this remaining
 114 # true):
 115 #
 116 #  - For consistency with the baseline JIT, t0 is always r0 (and t1 is always
 117 #  r1 on 32 bits platforms). You should use the r version when you need return
 118 #  registers, and the t version otherwise: code using t0 (or t1) should still
 119 #  work if swapped with e.g. t3, while code using r0 (or r1) should not. There
 120 #  *may* be legacy code relying on this.
 121 #
 122 #  - On all platforms other than X86, t0 can only be a0 and t2 can only be a2.
 123 #
 124 #  - On all platforms other than X86 and X86_64, a2 is not a return register.
 125 #  a2 is r0 on X86 (because we have so few registers) and r1 on X86_64 (because
 126 #  the ABI enforces it).
 127 #
 128 # The following floating-point registers are available:
 129 #
 130 #  - ft0-ft5 are temporary floating-point registers that get trashed on calls,
 131 #  and are pairwise distinct.
 132 #
 133 #  - fa0 and fa1 are the platform&#39;s customary floating-point argument
 134 #  registers, and are both distinct. On 64-bits platforms, fa2 and fa3 are
 135 #  additional floating-point argument registers.
 136 #
 137 #  - fr is the platform&#39;s customary floating-point return register
 138 #
 139 # You can assume that ft1-ft5 or fa1-fa3 are never fr, and that ftX is never
 140 # faY if X != Y.
 141 
 142 # First come the common protocols that both interpreters use. Note that each
 143 # of these must have an ASSERT() in LLIntData.cpp
 144 
 145 # Work-around for the fact that the toolchain&#39;s awareness of armv7k / armv7s
 146 # results in a separate slab in the fat binary, yet the offlineasm doesn&#39;t know
 147 # to expect it.
 148 if ARMv7k
 149 end
 150 if ARMv7s
 151 end
 152 
 153 # These declarations must match interpreter/JSStack.h.
 154 
 155 const PtrSize = constexpr (sizeof(void*))
 156 const MachineRegisterSize = constexpr (sizeof(CPURegister))
 157 const SlotSize = constexpr (sizeof(Register))
 158 
 159 if JSVALUE64
 160     const CallFrameHeaderSlots = 5
 161 else
 162     const CallFrameHeaderSlots = 4
 163     const CallFrameAlignSlots = 1
 164 end
 165 
 166 const JSLexicalEnvironment_variables = (sizeof JSLexicalEnvironment + SlotSize - 1) &amp; ~(SlotSize - 1)
 167 const DirectArguments_storage = (sizeof DirectArguments + SlotSize - 1) &amp; ~(SlotSize - 1)
 168 
 169 const StackAlignment = constexpr (stackAlignmentBytes())
 170 const StackAlignmentSlots = constexpr (stackAlignmentRegisters())
 171 const StackAlignmentMask = StackAlignment - 1
 172 
 173 const CallerFrameAndPCSize = constexpr (sizeof(CallerFrameAndPC))
 174 
 175 const CallerFrame = 0
 176 const ReturnPC = CallerFrame + MachineRegisterSize
 177 const CodeBlock = ReturnPC + MachineRegisterSize
 178 const Callee = CodeBlock + SlotSize
 179 const ArgumentCount = Callee + SlotSize
 180 const ThisArgumentOffset = ArgumentCount + SlotSize
 181 const FirstArgumentOffset = ThisArgumentOffset + SlotSize
 182 const CallFrameHeaderSize = ThisArgumentOffset
 183 
<a name="3" id="anc3"></a><span class="line-added"> 184 const MetadataOffsetTable16Offset = 0</span>
<span class="line-added"> 185 const MetadataOffsetTable32Offset = constexpr UnlinkedMetadataTable::s_offset16TableSize</span>
<span class="line-added"> 186 </span>
 187 # Some value representation constants.
 188 if JSVALUE64
 189     const TagBitTypeOther = constexpr TagBitTypeOther
 190     const TagBitBool      = constexpr TagBitBool
 191     const TagBitUndefined = constexpr TagBitUndefined
 192     const ValueEmpty      = constexpr ValueEmpty
 193     const ValueFalse      = constexpr ValueFalse
 194     const ValueTrue       = constexpr ValueTrue
 195     const ValueUndefined  = constexpr ValueUndefined
 196     const ValueNull       = constexpr ValueNull
 197     const TagTypeNumber   = constexpr TagTypeNumber
 198     const TagMask         = constexpr TagMask
 199 else
 200     const Int32Tag = constexpr JSValue::Int32Tag
 201     const BooleanTag = constexpr JSValue::BooleanTag
 202     const NullTag = constexpr JSValue::NullTag
 203     const UndefinedTag = constexpr JSValue::UndefinedTag
 204     const CellTag = constexpr JSValue::CellTag
 205     const EmptyValueTag = constexpr JSValue::EmptyValueTag
 206     const DeletedValueTag = constexpr JSValue::DeletedValueTag
 207     const LowestTag = constexpr JSValue::LowestTag
 208 end
 209 
 210 if JSVALUE64
 211     const NumberOfStructureIDEntropyBits = constexpr StructureIDTable::s_numberOfEntropyBits
 212     const StructureEntropyBitsShift = constexpr StructureIDTable::s_entropyBitsShiftForStructurePointer
 213 end
 214 
 215 const CallOpCodeSize = constexpr op_call_length
 216 
 217 const maxFrameExtentForSlowPathCall = constexpr maxFrameExtentForSlowPathCall
 218 
 219 if X86_64 or X86_64_WIN or ARM64 or ARM64E
 220     const CalleeSaveSpaceAsVirtualRegisters = 4
<a name="4" id="anc4"></a><span class="line-modified"> 221 elsif C_LOOP or C_LOOP_WIN</span>
 222     const CalleeSaveSpaceAsVirtualRegisters = 1
 223 elsif ARMv7
 224     const CalleeSaveSpaceAsVirtualRegisters = 1
 225 elsif MIPS
 226     const CalleeSaveSpaceAsVirtualRegisters = 1
 227 else
 228     const CalleeSaveSpaceAsVirtualRegisters = 0
 229 end
 230 
 231 const CalleeSaveSpaceStackAligned = (CalleeSaveSpaceAsVirtualRegisters * SlotSize + StackAlignment - 1) &amp; ~StackAlignmentMask
 232 
 233 
 234 # Watchpoint states
 235 const ClearWatchpoint = constexpr ClearWatchpoint
 236 const IsWatched = constexpr IsWatched
 237 const IsInvalidated = constexpr IsInvalidated
 238 
 239 # ShadowChicken data
 240 const ShadowChickenTailMarker = constexpr ShadowChicken::Packet::tailMarkerValue
 241 
 242 # ArithProfile data
 243 const ArithProfileInt = constexpr (ArithProfile::observedUnaryInt().bits())
 244 const ArithProfileNumber = constexpr (ArithProfile::observedUnaryNumber().bits())
 245 const ArithProfileIntInt = constexpr (ArithProfile::observedBinaryIntInt().bits())
 246 const ArithProfileNumberInt = constexpr (ArithProfile::observedBinaryNumberInt().bits())
 247 const ArithProfileIntNumber = constexpr (ArithProfile::observedBinaryIntNumber().bits())
 248 const ArithProfileNumberNumber = constexpr (ArithProfile::observedBinaryNumberNumber().bits())
 249 
 250 # Pointer Tags
 251 const BytecodePtrTag = constexpr BytecodePtrTag
 252 const JSEntryPtrTag = constexpr JSEntryPtrTag
 253 const ExceptionHandlerPtrTag = constexpr ExceptionHandlerPtrTag
 254 const NoPtrTag = constexpr NoPtrTag
 255 const SlowPathPtrTag = constexpr SlowPathPtrTag
 256 
 257 # Some register conventions.
 258 if JSVALUE64
 259     # - Use a pair of registers to represent the PC: one register for the
 260     #   base of the bytecodes, and one register for the index.
 261     # - The PC base (or PB for short) must be stored in a callee-save register.
 262     # - C calls are still given the Instruction* rather than the PC index.
 263     #   This requires an add before the call, and a sub after.
 264     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
 265     if ARM64 or ARM64E
 266         const metadataTable = csr6
 267         const PB = csr7
 268         const tagTypeNumber = csr8
 269         const tagMask = csr9
 270     elsif X86_64
 271         const metadataTable = csr1
 272         const PB = csr2
 273         const tagTypeNumber = csr3
 274         const tagMask = csr4
 275     elsif X86_64_WIN
 276         const metadataTable = csr3
 277         const PB = csr4
 278         const tagTypeNumber = csr5
 279         const tagMask = csr6
<a name="5" id="anc5"></a><span class="line-modified"> 280     elsif C_LOOP or C_LOOP_WIN</span>
 281         const PB = csr0
 282         const tagTypeNumber = csr1
 283         const tagMask = csr2
 284         const metadataTable = csr3
 285     end
 286 
 287 else
 288     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
<a name="6" id="anc6"></a><span class="line-modified"> 289     if C_LOOP or C_LOOP_WIN</span>
 290         const metadataTable = csr3
 291     elsif ARMv7
 292         const metadataTable = csr0
 293     elsif MIPS
 294         const metadataTable = csr0
 295     else
 296         error
 297     end
 298 end
 299 
 300 macro dispatch(advanceReg)
 301     addp advanceReg, PC
 302     nextInstruction()
 303 end
 304 
 305 macro dispatchIndirect(offsetReg)
 306     dispatch(offsetReg)
 307 end
 308 
 309 macro dispatchOp(size, opcodeName)
 310     macro dispatchNarrow()
 311         dispatch(constexpr %opcodeName%_length)
 312     end
 313 
<a name="7" id="anc7"></a><span class="line-modified"> 314     macro dispatchWide16()</span>
<span class="line-added"> 315         dispatch(constexpr %opcodeName%_length * 2 + 1)</span>
<span class="line-added"> 316     end</span>
<span class="line-added"> 317 </span>
<span class="line-added"> 318     macro dispatchWide32()</span>
 319         dispatch(constexpr %opcodeName%_length * 4 + 1)
 320     end
 321 
<a name="8" id="anc8"></a><span class="line-modified"> 322     size(dispatchNarrow, dispatchWide16, dispatchWide32, macro (dispatch) dispatch() end)</span>
 323 end
 324 
 325 macro getu(size, opcodeStruct, fieldName, dst)
<a name="9" id="anc9"></a><span class="line-modified"> 326     size(getuOperandNarrow, getuOperandWide16, getuOperandWide32, macro (getu)</span>
 327         getu(opcodeStruct, fieldName, dst)
 328     end)
 329 end
 330 
 331 macro get(size, opcodeStruct, fieldName, dst)
<a name="10" id="anc10"></a><span class="line-modified"> 332     size(getOperandNarrow, getOperandWide16, getOperandWide32, macro (get)</span>
 333         get(opcodeStruct, fieldName, dst)
 334     end)
 335 end
 336 
<a name="11" id="anc11"></a><span class="line-modified"> 337 macro narrow(narrowFn, wide16Fn, wide32Fn, k)</span>
 338     k(narrowFn)
 339 end
 340 
<a name="12" id="anc12"></a><span class="line-modified"> 341 macro wide16(narrowFn, wide16Fn, wide32Fn, k)</span>
<span class="line-modified"> 342     k(wide16Fn)</span>
<span class="line-added"> 343 end</span>
<span class="line-added"> 344 </span>
<span class="line-added"> 345 macro wide32(narrowFn, wide16Fn, wide32Fn, k)</span>
<span class="line-added"> 346     k(wide32Fn)</span>
 347 end
 348 
 349 macro metadata(size, opcode, dst, scratch)
<a name="13" id="anc13"></a><span class="line-modified"> 350     loadh (constexpr %opcode%::opcodeID * 2 + MetadataOffsetTable16Offset)[metadataTable], dst # offset = metadataTable&lt;uint16_t*&gt;[opcodeID]</span>
<span class="line-added"> 351     btinz dst, .setUpOffset</span>
<span class="line-added"> 352     loadi (constexpr %opcode%::opcodeID * 4 + MetadataOffsetTable32Offset)[metadataTable], dst # offset = metadataTable&lt;uint32_t*&gt;[opcodeID]</span>
<span class="line-added"> 353 .setUpOffset:</span>
 354     getu(size, opcode, m_metadataID, scratch) # scratch = bytecode.m_metadataID
 355     muli sizeof %opcode%::Metadata, scratch # scratch *= sizeof(Op::Metadata)
 356     addi scratch, dst # offset += scratch
 357     addp metadataTable, dst # return &amp;metadataTable[offset]
 358 end
 359 
 360 macro jumpImpl(targetOffsetReg)
 361     btiz targetOffsetReg, .outOfLineJumpTarget
 362     dispatchIndirect(targetOffsetReg)
 363 .outOfLineJumpTarget:
 364     callSlowPath(_llint_slow_path_out_of_line_jump_target)
 365     nextInstruction()
 366 end
 367 
 368 macro commonOp(label, prologue, fn)
 369 _%label%:
 370     prologue()
 371     fn(narrow)
 372 
<a name="14" id="anc14"></a><span class="line-modified"> 373 # FIXME: We cannot enable wide16 bytecode in Windows CLoop. With MSVC, as CLoop::execute gets larger code</span>
<span class="line-added"> 374 # size, CLoop::execute gets higher stack height requirement. This makes CLoop::execute takes 160KB stack</span>
<span class="line-added"> 375 # per call, causes stack overflow error easily. For now, we disable wide16 optimization for Windows CLoop.</span>
<span class="line-added"> 376 # https://bugs.webkit.org/show_bug.cgi?id=198283</span>
<span class="line-added"> 377 if not C_LOOP_WIN</span>
<span class="line-added"> 378 _%label%_wide16:</span>
 379     prologue()
<a name="15" id="anc15"></a><span class="line-modified"> 380     fn(wide16)</span>
<span class="line-added"> 381 end</span>
<span class="line-added"> 382 </span>
<span class="line-added"> 383 _%label%_wide32:</span>
<span class="line-added"> 384     prologue()</span>
<span class="line-added"> 385     fn(wide32)</span>
 386 end
 387 
 388 macro op(l, fn)
<a name="16" id="anc16"></a><span class="line-modified"> 389     commonOp(l, macro () end, macro (size)</span>
<span class="line-modified"> 390         size(fn, macro() end, macro() end, macro(gen) gen() end)</span>
 391     end)
 392 end
 393 
 394 macro llintOp(opcodeName, opcodeStruct, fn)
 395     commonOp(llint_%opcodeName%, traceExecution, macro(size)
 396         macro getImpl(fieldName, dst)
 397             get(size, opcodeStruct, fieldName, dst)
 398         end
 399 
 400         macro dispatchImpl()
 401             dispatchOp(size, opcodeName)
 402         end
 403 
 404         fn(size, getImpl, dispatchImpl)
 405     end)
 406 end
 407 
 408 macro llintOpWithReturn(opcodeName, opcodeStruct, fn)
 409     llintOp(opcodeName, opcodeStruct, macro(size, get, dispatch)
 410         makeReturn(get, dispatch, macro (return)
 411             fn(size, get, dispatch, return)
 412         end)
 413     end)
 414 end
 415 
 416 macro llintOpWithMetadata(opcodeName, opcodeStruct, fn)
 417     llintOpWithReturn(opcodeName, opcodeStruct, macro (size, get, dispatch, return)
 418         macro meta(dst, scratch)
 419             metadata(size, opcodeStruct, dst, scratch)
 420         end
 421         fn(size, get, dispatch, meta, return)
 422     end)
 423 end
 424 
 425 macro llintOpWithJump(opcodeName, opcodeStruct, impl)
 426     llintOpWithMetadata(opcodeName, opcodeStruct, macro(size, get, dispatch, metadata, return)
 427         macro jump(fieldName)
 428             get(fieldName, t0)
 429             jumpImpl(t0)
 430         end
 431 
 432         impl(size, get, jump, dispatch)
 433     end)
 434 end
 435 
 436 macro llintOpWithProfile(opcodeName, opcodeStruct, fn)
 437     llintOpWithMetadata(opcodeName, opcodeStruct, macro(size, get, dispatch, metadata, return)
 438         makeReturnProfiled(opcodeStruct, get, metadata, dispatch, macro (returnProfiled)
 439             fn(size, get, dispatch, returnProfiled)
 440         end)
 441     end)
 442 end
 443 
 444 
 445 if X86_64_WIN
 446     const extraTempReg = t0
 447 else
 448     const extraTempReg = t5
 449 end
 450 
 451 # Constants for reasoning about value representation.
 452 const TagOffset = constexpr TagOffset
 453 const PayloadOffset = constexpr PayloadOffset
 454 
 455 # Constant for reasoning about butterflies.
 456 const IsArray                  = constexpr IsArray
 457 const IndexingShapeMask        = constexpr IndexingShapeMask
 458 const NoIndexingShape          = constexpr NoIndexingShape
 459 const Int32Shape               = constexpr Int32Shape
 460 const DoubleShape              = constexpr DoubleShape
 461 const ContiguousShape          = constexpr ContiguousShape
 462 const ArrayStorageShape        = constexpr ArrayStorageShape
 463 const SlowPutArrayStorageShape = constexpr SlowPutArrayStorageShape
 464 const CopyOnWrite              = constexpr CopyOnWrite
 465 
 466 # Type constants.
 467 const StringType = constexpr StringType
 468 const SymbolType = constexpr SymbolType
 469 const ObjectType = constexpr ObjectType
 470 const FinalObjectType = constexpr FinalObjectType
 471 const JSFunctionType = constexpr JSFunctionType
 472 const ArrayType = constexpr ArrayType
 473 const DerivedArrayType = constexpr DerivedArrayType
 474 const ProxyObjectType = constexpr ProxyObjectType
 475 
 476 # The typed array types need to be numbered in a particular order because of the manually written
 477 # switch statement in get_by_val and put_by_val.
 478 const Int8ArrayType = constexpr Int8ArrayType
 479 const Uint8ArrayType = constexpr Uint8ArrayType
 480 const Uint8ClampedArrayType = constexpr Uint8ClampedArrayType
 481 const Int16ArrayType = constexpr Int16ArrayType
 482 const Uint16ArrayType = constexpr Uint16ArrayType
 483 const Int32ArrayType = constexpr Int32ArrayType
 484 const Uint32ArrayType = constexpr Uint32ArrayType
 485 const Float32ArrayType = constexpr Float32ArrayType
 486 const Float64ArrayType = constexpr Float64ArrayType
 487 
 488 const FirstTypedArrayType = constexpr FirstTypedArrayType
 489 const NumberOfTypedArrayTypesExcludingDataView = constexpr NumberOfTypedArrayTypesExcludingDataView
 490 
 491 # Type flags constants.
 492 const MasqueradesAsUndefined = constexpr MasqueradesAsUndefined
 493 const ImplementsDefaultHasInstance = constexpr ImplementsDefaultHasInstance
 494 
 495 # Bytecode operand constants.
<a name="17" id="anc17"></a><span class="line-modified"> 496 const FirstConstantRegisterIndexNarrow = constexpr FirstConstantRegisterIndex8</span>
<span class="line-modified"> 497 const FirstConstantRegisterIndexWide16 = constexpr FirstConstantRegisterIndex16</span>
<span class="line-added"> 498 const FirstConstantRegisterIndexWide32 = constexpr FirstConstantRegisterIndex</span>
 499 
 500 # Code type constants.
 501 const GlobalCode = constexpr GlobalCode
 502 const EvalCode = constexpr EvalCode
 503 const FunctionCode = constexpr FunctionCode
 504 const ModuleCode = constexpr ModuleCode
 505 
 506 # The interpreter steals the tag word of the argument count.
 507 const LLIntReturnPC = ArgumentCount + TagOffset
 508 
 509 # String flags.
 510 const isRopeInPointer = constexpr JSString::isRopeInPointer
 511 const HashFlags8BitBuffer = constexpr StringImpl::s_hashFlag8BitBuffer
 512 
 513 # Copied from PropertyOffset.h
 514 const firstOutOfLineOffset = constexpr firstOutOfLineOffset
 515 
 516 # ResolveType
 517 const GlobalProperty = constexpr GlobalProperty
 518 const GlobalVar = constexpr GlobalVar
 519 const GlobalLexicalVar = constexpr GlobalLexicalVar
 520 const ClosureVar = constexpr ClosureVar
 521 const LocalClosureVar = constexpr LocalClosureVar
 522 const ModuleVar = constexpr ModuleVar
 523 const GlobalPropertyWithVarInjectionChecks = constexpr GlobalPropertyWithVarInjectionChecks
 524 const GlobalVarWithVarInjectionChecks = constexpr GlobalVarWithVarInjectionChecks
 525 const GlobalLexicalVarWithVarInjectionChecks = constexpr GlobalLexicalVarWithVarInjectionChecks
 526 const ClosureVarWithVarInjectionChecks = constexpr ClosureVarWithVarInjectionChecks
 527 
 528 const ResolveTypeMask = constexpr GetPutInfo::typeBits
 529 const InitializationModeMask = constexpr GetPutInfo::initializationBits
 530 const InitializationModeShift = constexpr GetPutInfo::initializationShift
 531 const NotInitialization = constexpr InitializationMode::NotInitialization
 532 
 533 const MarkedBlockSize = constexpr MarkedBlock::blockSize
 534 const MarkedBlockMask = ~(MarkedBlockSize - 1)
 535 const MarkedBlockFooterOffset = constexpr MarkedBlock::offsetOfFooter
 536 
 537 const BlackThreshold = constexpr blackThreshold
 538 
 539 const VectorBufferOffset = Vector::m_buffer
 540 const VectorSizeOffset = Vector::m_size
 541 
 542 # Some common utilities.
 543 macro crash()
<a name="18" id="anc18"></a><span class="line-modified"> 544     if C_LOOP or C_LOOP_WIN</span>
 545         cloopCrash
 546     else
 547         call _llint_crash
 548     end
 549 end
 550 
 551 macro assert(assertion)
 552     if ASSERT_ENABLED
 553         assertion(.ok)
 554         crash()
 555     .ok:
 556     end
 557 end
 558 
 559 # The probe macro can be used to insert some debugging code without perturbing scalar
 560 # registers. Presently, the probe macro only preserves scalar registers. Hence, the
 561 # C probe callback function should not trash floating point registers.
 562 #
 563 # The macro you pass to probe() can pass whatever registers you like to your probe
 564 # callback function. However, you need to be mindful of which of the registers are
 565 # also used as argument registers, and ensure that you don&#39;t trash the register value
 566 # before storing it in the probe callback argument register that you desire.
 567 #
 568 # Here&#39;s an example of how it&#39;s used:
 569 #
 570 #     probe(
 571 #         macro()
 572 #             move cfr, a0 # pass the ExecState* as arg0.
 573 #             move t0, a1 # pass the value of register t0 as arg1.
 574 #             call _cProbeCallbackFunction # to do whatever you want.
 575 #         end
 576 #     )
 577 #
 578 if X86_64 or ARM64 or ARM64E or ARMv7
 579     macro probe(action)
 580         # save all the registers that the LLInt may use.
 581         if ARM64 or ARM64E or ARMv7
 582             push cfr, lr
 583         end
 584         push a0, a1
 585         push a2, a3
 586         push t0, t1
 587         push t2, t3
 588         push t4, t5
 589         if ARM64 or ARM64E
 590             push csr0, csr1
 591             push csr2, csr3
 592             push csr4, csr5
 593             push csr6, csr7
 594             push csr8, csr9
 595         elsif ARMv7
 596             push csr0
 597         end
 598 
 599         action()
 600 
 601         # restore all the registers we saved previously.
 602         if ARM64 or ARM64E
 603             pop csr9, csr8
 604             pop csr7, csr6
 605             pop csr5, csr4
 606             pop csr3, csr2
 607             pop csr1, csr0
 608         elsif ARMv7
 609             pop csr0
 610         end
 611         pop t5, t4
 612         pop t3, t2
 613         pop t1, t0
 614         pop a3, a2
 615         pop a1, a0
 616         if ARM64 or ARM64E or ARMv7
 617             pop lr, cfr
 618         end
 619     end
 620 else
 621     macro probe(action)
 622     end
 623 end
 624 
 625 macro checkStackPointerAlignment(tempReg, location)
 626     if ASSERT_ENABLED
<a name="19" id="anc19"></a><span class="line-modified"> 627         if ARM64 or ARM64E or C_LOOP or C_LOOP_WIN</span>
 628             # ARM64 and ARM64E will check for us!
<a name="20" id="anc20"></a><span class="line-modified"> 629             # C_LOOP or C_LOOP_WIN does not need the alignment, and can use a little perf</span>
 630             # improvement from avoiding useless work.
 631         else
 632             if ARMv7
 633                 # ARM can&#39;t do logical ops with the sp as a source
 634                 move sp, tempReg
 635                 andp StackAlignmentMask, tempReg
 636             else
 637                 andp sp, StackAlignmentMask, tempReg
 638             end
 639             btpz tempReg, .stackPointerOkay
 640             move location, tempReg
 641             break
 642         .stackPointerOkay:
 643         end
 644     end
 645 end
 646 
<a name="21" id="anc21"></a><span class="line-modified"> 647 if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN</span>
 648     const CalleeSaveRegisterCount = 0
 649 elsif ARMv7
 650     const CalleeSaveRegisterCount = 7
 651 elsif MIPS
 652     const CalleeSaveRegisterCount = 2
 653 elsif X86 or X86_WIN
 654     const CalleeSaveRegisterCount = 3
 655 end
 656 
 657 const CalleeRegisterSaveSize = CalleeSaveRegisterCount * MachineRegisterSize
 658 
 659 # VMEntryTotalFrameSize includes the space for struct VMEntryRecord and the
 660 # callee save registers rounded up to keep the stack aligned
 661 const VMEntryTotalFrameSize = (CalleeRegisterSaveSize + sizeof VMEntryRecord + StackAlignment - 1) &amp; ~StackAlignmentMask
 662 
 663 macro pushCalleeSaves()
<a name="22" id="anc22"></a><span class="line-modified"> 664     if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN</span>
 665     elsif ARMv7
 666         emit &quot;push {r4-r6, r8-r11}&quot;
 667     elsif MIPS
 668         emit &quot;addiu $sp, $sp, -8&quot;
 669         emit &quot;sw $s0, 0($sp)&quot; # csr0/metaData
 670         emit &quot;sw $s4, 4($sp)&quot;
 671         # save $gp to $s4 so that we can restore it after a function call
 672         emit &quot;move $s4, $gp&quot;
 673     elsif X86
 674         emit &quot;push %esi&quot;
 675         emit &quot;push %edi&quot;
 676         emit &quot;push %ebx&quot;
 677     elsif X86_WIN
 678         emit &quot;push esi&quot;
 679         emit &quot;push edi&quot;
 680         emit &quot;push ebx&quot;
 681     end
 682 end
 683 
 684 macro popCalleeSaves()
<a name="23" id="anc23"></a><span class="line-modified"> 685     if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN</span>
 686     elsif ARMv7
 687         emit &quot;pop {r4-r6, r8-r11}&quot;
 688     elsif MIPS
 689         emit &quot;lw $s0, 0($sp)&quot;
 690         emit &quot;lw $s4, 4($sp)&quot;
 691         emit &quot;addiu $sp, $sp, 8&quot;
 692     elsif X86
 693         emit &quot;pop %ebx&quot;
 694         emit &quot;pop %edi&quot;
 695         emit &quot;pop %esi&quot;
 696     elsif X86_WIN
 697         emit &quot;pop ebx&quot;
 698         emit &quot;pop edi&quot;
 699         emit &quot;pop esi&quot;
 700     end
 701 end
 702 
 703 macro preserveCallerPCAndCFR()
<a name="24" id="anc24"></a><span class="line-modified"> 704     if C_LOOP or C_LOOP_WIN or ARMv7 or MIPS</span>
 705         push lr
 706         push cfr
 707     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 708         push cfr
 709     elsif ARM64 or ARM64E
 710         push cfr, lr
 711     else
 712         error
 713     end
 714     move sp, cfr
 715 end
 716 
 717 macro restoreCallerPCAndCFR()
 718     move cfr, sp
<a name="25" id="anc25"></a><span class="line-modified"> 719     if C_LOOP or C_LOOP_WIN or ARMv7 or MIPS</span>
 720         pop cfr
 721         pop lr
 722     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 723         pop cfr
 724     elsif ARM64 or ARM64E
 725         pop lr, cfr
 726     end
 727 end
 728 
 729 macro preserveCalleeSavesUsedByLLInt()
 730     subp CalleeSaveSpaceStackAligned, sp
<a name="26" id="anc26"></a><span class="line-modified"> 731     if C_LOOP or C_LOOP_WIN</span>
 732         storep metadataTable, -PtrSize[cfr]
 733     elsif ARMv7 or MIPS
 734         storep metadataTable, -4[cfr]
 735     elsif ARM64 or ARM64E
 736         emit &quot;stp x27, x28, [x29, #-16]&quot;
 737         emit &quot;stp x25, x26, [x29, #-32]&quot;
 738     elsif X86
 739     elsif X86_WIN
 740     elsif X86_64
 741         storep csr4, -8[cfr]
 742         storep csr3, -16[cfr]
 743         storep csr2, -24[cfr]
 744         storep csr1, -32[cfr]
 745     elsif X86_64_WIN
 746         storep csr6, -8[cfr]
 747         storep csr5, -16[cfr]
 748         storep csr4, -24[cfr]
 749         storep csr3, -32[cfr]
 750     end
 751 end
 752 
 753 macro restoreCalleeSavesUsedByLLInt()
<a name="27" id="anc27"></a><span class="line-modified"> 754     if C_LOOP or C_LOOP_WIN</span>
 755         loadp -PtrSize[cfr], metadataTable
 756     elsif ARMv7 or MIPS
 757         loadp -4[cfr], metadataTable
 758     elsif ARM64 or ARM64E
 759         emit &quot;ldp x25, x26, [x29, #-32]&quot;
 760         emit &quot;ldp x27, x28, [x29, #-16]&quot;
 761     elsif X86
 762     elsif X86_WIN
 763     elsif X86_64
 764         loadp -32[cfr], csr1
 765         loadp -24[cfr], csr2
 766         loadp -16[cfr], csr3
 767         loadp -8[cfr], csr4
 768     elsif X86_64_WIN
 769         loadp -32[cfr], csr3
 770         loadp -24[cfr], csr4
 771         loadp -16[cfr], csr5
 772         loadp -8[cfr], csr6
 773     end
 774 end
 775 
 776 macro copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(vm, temp)
 777     if ARM64 or ARM64E or X86_64 or X86_64_WIN or ARMv7 or MIPS
 778         loadp VM::topEntryFrame[vm], temp
 779         vmEntryRecord(temp, temp)
 780         leap VMEntryRecord::calleeSaveRegistersBuffer[temp], temp
 781         if ARM64 or ARM64E
 782             storeq csr0, [temp]
 783             storeq csr1, 8[temp]
 784             storeq csr2, 16[temp]
 785             storeq csr3, 24[temp]
 786             storeq csr4, 32[temp]
 787             storeq csr5, 40[temp]
 788             storeq csr6, 48[temp]
 789             storeq csr7, 56[temp]
 790             storeq csr8, 64[temp]
 791             storeq csr9, 72[temp]
 792             stored csfr0, 80[temp]
 793             stored csfr1, 88[temp]
 794             stored csfr2, 96[temp]
 795             stored csfr3, 104[temp]
 796             stored csfr4, 112[temp]
 797             stored csfr5, 120[temp]
 798             stored csfr6, 128[temp]
 799             stored csfr7, 136[temp]
 800         elsif X86_64
 801             storeq csr0, [temp]
 802             storeq csr1, 8[temp]
 803             storeq csr2, 16[temp]
 804             storeq csr3, 24[temp]
 805             storeq csr4, 32[temp]
 806         elsif X86_64_WIN
 807             storeq csr0, [temp]
 808             storeq csr1, 8[temp]
 809             storeq csr2, 16[temp]
 810             storeq csr3, 24[temp]
 811             storeq csr4, 32[temp]
 812             storeq csr5, 40[temp]
 813             storeq csr6, 48[temp]
 814         elsif ARMv7 or MIPS
 815             storep csr0, [temp]
 816         end
 817     end
 818 end
 819 
 820 macro restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(vm, temp)
 821     if ARM64 or ARM64E or X86_64 or X86_64_WIN or ARMv7 or MIPS
 822         loadp VM::topEntryFrame[vm], temp
 823         vmEntryRecord(temp, temp)
 824         leap VMEntryRecord::calleeSaveRegistersBuffer[temp], temp
 825         if ARM64 or ARM64E
 826             loadq [temp], csr0
 827             loadq 8[temp], csr1
 828             loadq 16[temp], csr2
 829             loadq 24[temp], csr3
 830             loadq 32[temp], csr4
 831             loadq 40[temp], csr5
 832             loadq 48[temp], csr6
 833             loadq 56[temp], csr7
 834             loadq 64[temp], csr8
 835             loadq 72[temp], csr9
 836             loadd 80[temp], csfr0
 837             loadd 88[temp], csfr1
 838             loadd 96[temp], csfr2
 839             loadd 104[temp], csfr3
 840             loadd 112[temp], csfr4
 841             loadd 120[temp], csfr5
 842             loadd 128[temp], csfr6
 843             loadd 136[temp], csfr7
 844         elsif X86_64
 845             loadq [temp], csr0
 846             loadq 8[temp], csr1
 847             loadq 16[temp], csr2
 848             loadq 24[temp], csr3
 849             loadq 32[temp], csr4
 850         elsif X86_64_WIN
 851             loadq [temp], csr0
 852             loadq 8[temp], csr1
 853             loadq 16[temp], csr2
 854             loadq 24[temp], csr3
 855             loadq 32[temp], csr4
 856             loadq 40[temp], csr5
 857             loadq 48[temp], csr6
 858         elsif ARMv7 or MIPS
 859             loadp [temp], csr0
 860         end
 861     end
 862 end
 863 
 864 macro preserveReturnAddressAfterCall(destinationRegister)
<a name="28" id="anc28"></a><span class="line-modified"> 865     if C_LOOP or C_LOOP_WIN or ARMv7 or ARM64 or ARM64E or MIPS</span>
<span class="line-modified"> 866         # In C_LOOP or C_LOOP_WIN case, we&#39;re only preserving the bytecode vPC.</span>
 867         move lr, destinationRegister
 868     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
 869         pop destinationRegister
 870     else
 871         error
 872     end
 873 end
 874 
 875 macro functionPrologue()
 876     tagReturnAddress sp
 877     if X86 or X86_WIN or X86_64 or X86_64_WIN
 878         push cfr
 879     elsif ARM64 or ARM64E
 880         push cfr, lr
<a name="29" id="anc29"></a><span class="line-modified"> 881     elsif C_LOOP or C_LOOP_WIN or ARMv7 or MIPS</span>
 882         push lr
 883         push cfr
 884     end
 885     move sp, cfr
 886 end
 887 
 888 macro functionEpilogue()
 889     if X86 or X86_WIN or X86_64 or X86_64_WIN
 890         pop cfr
 891     elsif ARM64 or ARM64E
 892         pop lr, cfr
<a name="30" id="anc30"></a><span class="line-modified"> 893     elsif C_LOOP or C_LOOP_WIN or ARMv7 or MIPS</span>
 894         pop cfr
 895         pop lr
 896     end
 897 end
 898 
 899 macro vmEntryRecord(entryFramePointer, resultReg)
 900     subp entryFramePointer, VMEntryTotalFrameSize, resultReg
 901 end
 902 
 903 macro getFrameRegisterSizeForCodeBlock(codeBlock, size)
 904     loadi CodeBlock::m_numCalleeLocals[codeBlock], size
 905     lshiftp 3, size
 906     addp maxFrameExtentForSlowPathCall, size
 907 end
 908 
 909 macro restoreStackPointerAfterCall()
 910     loadp CodeBlock[cfr], t2
 911     getFrameRegisterSizeForCodeBlock(t2, t2)
 912     if ARMv7
 913         subp cfr, t2, t2
 914         move t2, sp
 915     else
 916         subp cfr, t2, sp
 917     end
 918 end
 919 
 920 macro traceExecution()
 921     if TRACING
 922         callSlowPath(_llint_trace)
 923     end
 924 end
 925 
 926 macro callTargetFunction(size, opcodeStruct, dispatch, callee, callPtrTag)
<a name="31" id="anc31"></a><span class="line-modified"> 927     if C_LOOP or C_LOOP_WIN</span>
 928         cloopCallJSFunction callee
 929     else
 930         call callee, callPtrTag
 931     end
 932     restoreStackPointerAfterCall()
 933     dispatchAfterCall(size, opcodeStruct, dispatch)
 934 end
 935 
 936 macro prepareForRegularCall(callee, temp1, temp2, temp3, callPtrTag)
 937     addp CallerFrameAndPCSize, sp
 938 end
 939 
 940 # sp points to the new frame
 941 macro prepareForTailCall(callee, temp1, temp2, temp3, callPtrTag)
 942     restoreCalleeSavesUsedByLLInt()
 943 
 944     loadi PayloadOffset + ArgumentCount[cfr], temp2
 945     loadp CodeBlock[cfr], temp1
 946     loadi CodeBlock::m_numParameters[temp1], temp1
 947     bilteq temp1, temp2, .noArityFixup
 948     move temp1, temp2
 949 
 950 .noArityFixup:
 951     # We assume &lt; 2^28 arguments
 952     muli SlotSize, temp2
 953     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
 954     andi ~StackAlignmentMask, temp2
 955 
 956     move cfr, temp1
 957     addp temp2, temp1
 958 
 959     loadi PayloadOffset + ArgumentCount[sp], temp2
 960     # We assume &lt; 2^28 arguments
 961     muli SlotSize, temp2
 962     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
 963     andi ~StackAlignmentMask, temp2
 964 
<a name="32" id="anc32"></a><span class="line-modified"> 965     if ARMv7 or ARM64 or ARM64E or C_LOOP or C_LOOP_WIN or MIPS</span>
 966         addp CallerFrameAndPCSize, sp
 967         subi CallerFrameAndPCSize, temp2
 968         loadp CallerFrameAndPC::returnPC[cfr], lr
 969     else
 970         addp PtrSize, sp
 971         subi PtrSize, temp2
 972         loadp PtrSize[cfr], temp3
 973         storep temp3, [sp]
 974     end
 975 
<a name="33" id="anc33"></a><span class="line-modified"> 976     if ARM64E</span>
 977         addp 16, cfr, temp3
 978         untagReturnAddress temp3
 979     end
 980 
 981     subp temp2, temp1
 982     loadp [cfr], cfr
 983 
 984 .copyLoop:
 985     if ARM64 and not ADDRESS64
 986         subi MachineRegisterSize, temp2
 987         loadq [sp, temp2, 1], temp3
 988         storeq temp3, [temp1, temp2, 1]
 989         btinz temp2, .copyLoop
 990     else
 991         subi PtrSize, temp2
 992         loadp [sp, temp2, 1], temp3
 993         storep temp3, [temp1, temp2, 1]
 994         btinz temp2, .copyLoop
 995     end
 996 
 997     move temp1, sp
 998     jmp callee, callPtrTag
 999 end
1000 
1001 macro slowPathForCall(size, opcodeStruct, dispatch, slowPath, prepareCall)
1002     callCallSlowPath(
1003         slowPath,
1004         # Those are r0 and r1
1005         macro (callee, calleeFramePtr)
1006             btpz calleeFramePtr, .dontUpdateSP
1007             move calleeFramePtr, sp
1008             prepareCall(callee, t2, t3, t4, SlowPathPtrTag)
1009         .dontUpdateSP:
1010             callTargetFunction(size, opcodeStruct, dispatch, callee, SlowPathPtrTag)
1011         end)
1012 end
1013 
1014 macro arrayProfile(offset, cellAndIndexingType, metadata, scratch)
1015     const cell = cellAndIndexingType
1016     const indexingType = cellAndIndexingType 
1017     loadi JSCell::m_structureID[cell], scratch
1018     storei scratch, offset + ArrayProfile::m_lastSeenStructureID[metadata]
1019     loadb JSCell::m_indexingTypeAndMisc[cell], indexingType
1020 end
1021 
1022 macro skipIfIsRememberedOrInEden(cell, slowPath)
1023     memfence
1024     bba JSCell::m_cellState[cell], BlackThreshold, .done
1025     slowPath()
1026 .done:
1027 end
1028 
1029 macro notifyWrite(set, slow)
1030     bbneq WatchpointSet::m_state[set], IsInvalidated, slow
1031 end
1032 
1033 macro checkSwitchToJIT(increment, action)
1034     loadp CodeBlock[cfr], t0
1035     baddis increment, CodeBlock::m_llintExecuteCounter + BaselineExecutionCounter::m_counter[t0], .continue
1036     action()
1037     .continue:
1038 end
1039 
1040 macro checkSwitchToJITForEpilogue()
1041     checkSwitchToJIT(
1042         10,
1043         macro ()
1044             callSlowPath(_llint_replace)
1045         end)
1046 end
1047 
1048 macro assertNotConstant(size, index)
<a name="34" id="anc34"></a><span class="line-modified">1049     size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide16, FirstConstantRegisterIndexWide32, macro (FirstConstantRegisterIndex)</span>
1050         assert(macro (ok) bilt index, FirstConstantRegisterIndex, ok end)
1051     end)
1052 end
1053 
1054 macro functionForCallCodeBlockGetter(targetRegister)
1055     if JSVALUE64
1056         loadp Callee[cfr], targetRegister
1057     else
1058         loadp Callee + PayloadOffset[cfr], targetRegister
1059     end
1060     loadp JSFunction::m_executable[targetRegister], targetRegister
1061     loadp FunctionExecutable::m_codeBlockForCall[targetRegister], targetRegister
1062     loadp ExecutableToCodeBlockEdge::m_codeBlock[targetRegister], targetRegister
1063 end
1064 
1065 macro functionForConstructCodeBlockGetter(targetRegister)
1066     if JSVALUE64
1067         loadp Callee[cfr], targetRegister
1068     else
1069         loadp Callee + PayloadOffset[cfr], targetRegister
1070     end
1071     loadp JSFunction::m_executable[targetRegister], targetRegister
1072     loadp FunctionExecutable::m_codeBlockForConstruct[targetRegister], targetRegister
1073     loadp ExecutableToCodeBlockEdge::m_codeBlock[targetRegister], targetRegister
1074 end
1075 
1076 macro notFunctionCodeBlockGetter(targetRegister)
1077     loadp CodeBlock[cfr], targetRegister
1078 end
1079 
1080 macro functionCodeBlockSetter(sourceRegister)
1081     storep sourceRegister, CodeBlock[cfr]
1082 end
1083 
1084 macro notFunctionCodeBlockSetter(sourceRegister)
1085     # Nothing to do!
1086 end
1087 
1088 # Do the bare minimum required to execute code. Sets up the PC, leave the CodeBlock*
1089 # in t1. May also trigger prologue entry OSR.
1090 macro prologue(codeBlockGetter, codeBlockSetter, osrSlowPath, traceSlowPath)
1091     # Set up the call frame and check if we should OSR.
1092     tagReturnAddress sp
1093     preserveCallerPCAndCFR()
1094 
1095     if TRACING
1096         subp maxFrameExtentForSlowPathCall, sp
1097         callSlowPath(traceSlowPath)
1098         addp maxFrameExtentForSlowPathCall, sp
1099     end
1100     codeBlockGetter(t1)
<a name="35" id="anc35"></a><span class="line-modified">1101     if not (C_LOOP or C_LOOP_WIN)</span>
1102         baddis 5, CodeBlock::m_llintExecuteCounter + BaselineExecutionCounter::m_counter[t1], .continue
1103         if JSVALUE64
1104             move cfr, a0
1105             move PC, a1
1106             cCall2(osrSlowPath)
1107         else
1108             # We are after the function prologue, but before we have set up sp from the CodeBlock.
1109             # Temporarily align stack pointer for this call.
1110             subp 8, sp
1111             move cfr, a0
1112             move PC, a1
1113             cCall2(osrSlowPath)
1114             addp 8, sp
1115         end
1116         btpz r0, .recover
1117         move cfr, sp # restore the previous sp
1118         # pop the callerFrame since we will jump to a function that wants to save it
1119         if ARM64 or ARM64E
1120             pop lr, cfr
1121             untagReturnAddress sp
1122         elsif ARMv7 or MIPS
1123             pop cfr
1124             pop lr
1125         else
1126             pop cfr
1127         end
1128         jmp r0, JSEntryPtrTag
1129     .recover:
1130         codeBlockGetter(t1)
1131     .continue:
1132     end
1133 
1134     codeBlockSetter(t1)
1135 
1136     preserveCalleeSavesUsedByLLInt()
1137 
1138     # Set up the PC.
1139     if JSVALUE64
1140         loadp CodeBlock::m_instructionsRawPointer[t1], PB
1141         move 0, PC
1142     else
1143         loadp CodeBlock::m_instructionsRawPointer[t1], PC
1144     end
1145 
1146     # Get new sp in t0 and check stack height.
1147     getFrameRegisterSizeForCodeBlock(t1, t0)
1148     subp cfr, t0, t0
1149     bpa t0, cfr, .needStackCheck
1150     loadp CodeBlock::m_vm[t1], t2
<a name="36" id="anc36"></a><span class="line-modified">1151     if C_LOOP or C_LOOP_WIN</span>
1152         bpbeq VM::m_cloopStackLimit[t2], t0, .stackHeightOK
1153     else
1154         bpbeq VM::m_softStackLimit[t2], t0, .stackHeightOK
1155     end
1156 
1157 .needStackCheck:
1158     # Stack height check failed - need to call a slow_path.
1159     # Set up temporary stack pointer for call including callee saves
1160     subp maxFrameExtentForSlowPathCall, sp
1161     callSlowPath(_llint_stack_check)
1162     bpeq r1, 0, .stackHeightOKGetCodeBlock
1163 
1164     # We&#39;re throwing before the frame is fully set up. This frame will be
1165     # ignored by the unwinder. So, let&#39;s restore the callee saves before we
1166     # start unwinding. We need to do this before we change the cfr.
1167     restoreCalleeSavesUsedByLLInt()
1168 
1169     move r1, cfr
1170     jmp _llint_throw_from_slow_path_trampoline
1171 
1172 .stackHeightOKGetCodeBlock:
1173     # Stack check slow path returned that the stack was ok.
1174     # Since they were clobbered, need to get CodeBlock and new sp
1175     codeBlockGetter(t1)
1176     getFrameRegisterSizeForCodeBlock(t1, t0)
1177     subp cfr, t0, t0
1178 
1179 .stackHeightOK:
1180     if X86_64 or ARM64
1181         # We need to start zeroing from sp as it has been adjusted after saving callee saves.
1182         move sp, t2
1183         move t0, sp
1184 .zeroStackLoop:
1185         bpeq sp, t2, .zeroStackDone
1186         subp PtrSize, t2
1187         storep 0, [t2]
1188         jmp .zeroStackLoop
1189 .zeroStackDone:
1190     else
1191         move t0, sp
1192     end
1193 
1194     loadp CodeBlock::m_metadata[t1], metadataTable
1195 
1196     if JSVALUE64
1197         move TagTypeNumber, tagTypeNumber
1198         addq TagBitTypeOther, tagTypeNumber, tagMask
1199     end
1200 end
1201 
1202 # Expects that CodeBlock is in t1, which is what prologue() leaves behind.
1203 # Must call dispatch(0) after calling this.
1204 macro functionInitialization(profileArgSkip)
1205     # Profile the arguments. Unfortunately, we have no choice but to do this. This
1206     # code is pretty horrendous because of the difference in ordering between
1207     # arguments and value profiles, the desire to have a simple loop-down-to-zero
1208     # loop, and the desire to use only three registers so as to preserve the PC and
1209     # the code block. It is likely that this code should be rewritten in a more
1210     # optimal way for architectures that have more than five registers available
1211     # for arbitrary use in the interpreter.
1212     loadi CodeBlock::m_numParameters[t1], t0
1213     addp -profileArgSkip, t0 # Use addi because that&#39;s what has the peephole
1214     assert(macro (ok) bpgteq t0, 0, ok end)
1215     btpz t0, .argumentProfileDone
1216     loadp CodeBlock::m_argumentValueProfiles + RefCountedArray::m_data[t1], t3
1217     btpz t3, .argumentProfileDone # When we can&#39;t JIT, we don&#39;t allocate any argument value profiles.
1218     mulp sizeof ValueProfile, t0, t2 # Aaaaahhhh! Need strength reduction!
1219     lshiftp 3, t0 # offset of last JSValue arguments on the stack.
1220     addp t2, t3 # pointer to end of ValueProfile array in CodeBlock::m_argumentValueProfiles.
1221 .argumentProfileLoop:
1222     if JSVALUE64
1223         loadq ThisArgumentOffset - 8 + profileArgSkip * 8[cfr, t0], t2
1224         subp sizeof ValueProfile, t3
1225         storeq t2, profileArgSkip * sizeof ValueProfile + ValueProfile::m_buckets[t3]
1226     else
1227         loadi ThisArgumentOffset + TagOffset - 8 + profileArgSkip * 8[cfr, t0], t2
1228         subp sizeof ValueProfile, t3
1229         storei t2, profileArgSkip * sizeof ValueProfile + ValueProfile::m_buckets + TagOffset[t3]
1230         loadi ThisArgumentOffset + PayloadOffset - 8 + profileArgSkip * 8[cfr, t0], t2
1231         storei t2, profileArgSkip * sizeof ValueProfile + ValueProfile::m_buckets + PayloadOffset[t3]
1232     end
1233     baddpnz -8, t0, .argumentProfileLoop
1234 .argumentProfileDone:
1235 end
1236 
1237 macro doReturn()
1238     restoreCalleeSavesUsedByLLInt()
1239     restoreCallerPCAndCFR()
1240     ret
1241 end
1242 
1243 # This break instruction is needed so that the synthesized llintPCRangeStart label
1244 # doesn&#39;t point to the exact same location as vmEntryToJavaScript which comes after it.
1245 # Otherwise, libunwind will report vmEntryToJavaScript as llintPCRangeStart in
1246 # stack traces.
1247 
1248     break
1249 
1250 # stub to call into JavaScript or Native functions
1251 # EncodedJSValue vmEntryToJavaScript(void* code, VM* vm, ProtoCallFrame* protoFrame)
1252 # EncodedJSValue vmEntryToNativeFunction(void* code, VM* vm, ProtoCallFrame* protoFrame)
1253 
<a name="37" id="anc37"></a><span class="line-modified">1254 if C_LOOP or C_LOOP_WIN</span>
1255     _llint_vm_entry_to_javascript:
1256 else
1257     global _vmEntryToJavaScript
1258     _vmEntryToJavaScript:
1259 end
1260     doVMEntry(makeJavaScriptCall)
1261 
1262 
<a name="38" id="anc38"></a><span class="line-modified">1263 if C_LOOP or C_LOOP_WIN</span>
1264     _llint_vm_entry_to_native:
1265 else
1266     global _vmEntryToNative
1267     _vmEntryToNative:
1268 end
1269     doVMEntry(makeHostFunctionCall)
1270 
1271 
<a name="39" id="anc39"></a><span class="line-modified">1272 if not (C_LOOP or C_LOOP_WIN)</span>
1273     # void sanitizeStackForVMImpl(VM* vm)
1274     global _sanitizeStackForVMImpl
1275     _sanitizeStackForVMImpl:
1276         tagReturnAddress sp
1277         # We need three non-aliased caller-save registers. We are guaranteed
1278         # this for a0, a1 and a2 on all architectures.
1279         if X86 or X86_WIN
1280             loadp 4[sp], a0
1281         end
1282         const vm = a0
1283         const address = a1
1284         const zeroValue = a2
1285     
1286         loadp VM::m_lastStackTop[vm], address
1287         bpbeq sp, address, .zeroFillDone
1288     
1289         move 0, zeroValue
1290     .zeroFillLoop:
1291         storep zeroValue, [address]
1292         addp PtrSize, address
1293         bpa sp, address, .zeroFillLoop
1294 
1295     .zeroFillDone:
1296         move sp, address
1297         storep address, VM::m_lastStackTop[vm]
1298         ret
1299     
1300     # VMEntryRecord* vmEntryRecord(const EntryFrame* entryFrame)
1301     global _vmEntryRecord
1302     _vmEntryRecord:
1303         tagReturnAddress sp
1304         if X86 or X86_WIN
1305             loadp 4[sp], a0
1306         end
1307 
1308         vmEntryRecord(a0, r0)
1309         ret
1310 end
1311 
<a name="40" id="anc40"></a><span class="line-modified">1312 if C_LOOP or C_LOOP_WIN</span>
1313     # Dummy entry point the C Loop uses to initialize.
1314     _llint_entry:
1315         crash()
1316 else
1317     macro initPCRelative(pcBase)
1318         if X86_64 or X86_64_WIN or X86 or X86_WIN
1319             call _relativePCBase
1320         _relativePCBase:
1321             pop pcBase
1322         elsif ARM64 or ARM64E
1323         elsif ARMv7
1324         _relativePCBase:
1325             move pc, pcBase
1326             subp 3, pcBase   # Need to back up the PC and set the Thumb2 bit
1327         elsif MIPS
1328             la _relativePCBase, pcBase
1329             setcallreg pcBase # needed to set $t9 to the right value for the .cpload created by the label.
1330         _relativePCBase:
1331         end
1332 end
1333 
<a name="41" id="anc41"></a><span class="line-modified">1334 # The PC base is in t3, as this is what _llint_entry leaves behind through</span>
<span class="line-modified">1335 # initPCRelative(t3)</span>
1336 macro setEntryAddress(index, label)
1337     setEntryAddressCommon(index, label, a0)
1338 end
1339 
<a name="42" id="anc42"></a><span class="line-modified">1340 macro setEntryAddressWide16(index, label)</span>
1341      setEntryAddressCommon(index, label, a1)
1342 end
1343 
<a name="43" id="anc43"></a><span class="line-added">1344 macro setEntryAddressWide32(index, label)</span>
<span class="line-added">1345      setEntryAddressCommon(index, label, a2)</span>
<span class="line-added">1346 end</span>
<span class="line-added">1347 </span>
1348 macro setEntryAddressCommon(index, label, map)
<a name="44" id="anc44"></a><span class="line-modified">1349     if X86_64</span>
<span class="line-modified">1350         leap (label - _relativePCBase)[t3], t4</span>
<span class="line-modified">1351         move index, t5</span>
<span class="line-modified">1352         storep t4, [map, t5, 8]</span>
<span class="line-added">1353     elsif X86_64_WIN</span>
<span class="line-added">1354         leap (label - _relativePCBase)[t3], t4</span>
<span class="line-added">1355         move index, t0</span>
<span class="line-added">1356         storep t4, [map, t0, 8]</span>
1357     elsif X86 or X86_WIN
<a name="45" id="anc45"></a><span class="line-modified">1358         leap (label - _relativePCBase)[t3], t4</span>
<span class="line-modified">1359         move index, t5</span>
<span class="line-modified">1360         storep t4, [map, t5, 4]</span>
1361     elsif ARM64 or ARM64E
<a name="46" id="anc46"></a><span class="line-modified">1362         pcrtoaddr label, t3</span>
1363         move index, t4
<a name="47" id="anc47"></a><span class="line-modified">1364         storep t3, [map, t4, PtrSize]</span>
1365     elsif ARMv7
1366         mvlbl (label - _relativePCBase), t4
<a name="48" id="anc48"></a><span class="line-modified">1367         addp t4, t3, t4</span>
<span class="line-modified">1368         move index, t5</span>
<span class="line-modified">1369         storep t4, [map, t5, 4]</span>
1370     elsif MIPS
1371         la label, t4
1372         la _relativePCBase, t3
1373         subp t3, t4
<a name="49" id="anc49"></a><span class="line-modified">1374         addp t4, t3, t4</span>
<span class="line-modified">1375         move index, t5</span>
<span class="line-modified">1376         storep t4, [map, t5, 4]</span>
1377     end
1378 end
1379 
1380 global _llint_entry
1381 # Entry point for the llint to initialize.
1382 _llint_entry:
1383     functionPrologue()
1384     pushCalleeSaves()
1385     if X86 or X86_WIN
1386         loadp 20[sp], a0
1387         loadp 24[sp], a1
<a name="50" id="anc50"></a><span class="line-added">1388         loadp 28[sp], a2</span>
1389     end
1390 
<a name="51" id="anc51"></a><span class="line-modified">1391     initPCRelative(t3)</span>
1392 
1393     # Include generated bytecode initialization file.
1394     include InitBytecodes
1395 
1396     popCalleeSaves()
1397     functionEpilogue()
1398     ret
1399 end
1400 
<a name="52" id="anc52"></a><span class="line-modified">1401 _llint_op_wide16:</span>
<span class="line-modified">1402     nextInstructionWide16()</span>
1403 
<a name="53" id="anc53"></a><span class="line-modified">1404 _llint_op_wide32:</span>
<span class="line-added">1405     nextInstructionWide32()</span>
<span class="line-added">1406 </span>
<span class="line-added">1407 macro noWide(label)</span>
<span class="line-added">1408 _llint_%label%_wide16:</span>
1409     crash()
1410 
<a name="54" id="anc54"></a><span class="line-modified">1411 _llint_%label%_wide32:</span>
1412     crash()
<a name="55" id="anc55"></a><span class="line-added">1413 end</span>
<span class="line-added">1414 </span>
<span class="line-added">1415 noWide(op_wide16)</span>
<span class="line-added">1416 noWide(op_wide32)</span>
<span class="line-added">1417 noWide(op_enter)</span>
1418 
1419 op(llint_program_prologue, macro ()
1420     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1421     dispatch(0)
1422 end)
1423 
1424 
1425 op(llint_module_program_prologue, macro ()
1426     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1427     dispatch(0)
1428 end)
1429 
1430 
1431 op(llint_eval_prologue, macro ()
1432     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
1433     dispatch(0)
1434 end)
1435 
1436 
1437 op(llint_function_for_call_prologue, macro ()
1438     prologue(functionForCallCodeBlockGetter, functionCodeBlockSetter, _llint_entry_osr_function_for_call, _llint_trace_prologue_function_for_call)
1439     functionInitialization(0)
1440     dispatch(0)
1441 end)
1442     
1443 
1444 op(llint_function_for_construct_prologue, macro ()
1445     prologue(functionForConstructCodeBlockGetter, functionCodeBlockSetter, _llint_entry_osr_function_for_construct, _llint_trace_prologue_function_for_construct)
1446     functionInitialization(1)
1447     dispatch(0)
1448 end)
1449     
1450 
1451 op(llint_function_for_call_arity_check, macro ()
1452     prologue(functionForCallCodeBlockGetter, functionCodeBlockSetter, _llint_entry_osr_function_for_call_arityCheck, _llint_trace_arityCheck_for_call)
1453     functionArityCheck(.functionForCallBegin, _slow_path_call_arityCheck)
1454 .functionForCallBegin:
1455     functionInitialization(0)
1456     dispatch(0)
1457 end)
1458 
1459 
1460 op(llint_function_for_construct_arity_check, macro ()
1461     prologue(functionForConstructCodeBlockGetter, functionCodeBlockSetter, _llint_entry_osr_function_for_construct_arityCheck, _llint_trace_arityCheck_for_construct)
1462     functionArityCheck(.functionForConstructBegin, _slow_path_construct_arityCheck)
1463 .functionForConstructBegin:
1464     functionInitialization(1)
1465     dispatch(0)
1466 end)
1467 
1468 
1469 # Value-representation-specific code.
1470 if JSVALUE64
1471     include LowLevelInterpreter64
1472 else
1473     include LowLevelInterpreter32_64
1474 end
1475 
1476 
1477 # Value-representation-agnostic code.
1478 macro slowPathOp(opcodeName)
1479     llintOp(op_%opcodeName%, unused, macro (unused, unused, dispatch)
1480         callSlowPath(_slow_path_%opcodeName%)
1481         dispatch()
1482     end)
1483 end
1484 
1485 slowPathOp(create_cloned_arguments)
1486 slowPathOp(create_direct_arguments)
1487 slowPathOp(create_lexical_environment)
1488 slowPathOp(create_rest)
1489 slowPathOp(create_scoped_arguments)
1490 slowPathOp(create_this)
1491 slowPathOp(define_accessor_property)
1492 slowPathOp(define_data_property)
1493 slowPathOp(enumerator_generic_pname)
1494 slowPathOp(enumerator_structure_pname)
1495 slowPathOp(get_by_id_with_this)
1496 slowPathOp(get_by_val_with_this)
1497 slowPathOp(get_direct_pname)
1498 slowPathOp(get_enumerable_length)
1499 slowPathOp(get_property_enumerator)
1500 slowPathOp(greater)
1501 slowPathOp(greatereq)
1502 slowPathOp(has_generic_property)
1503 slowPathOp(has_indexed_property)
1504 slowPathOp(has_structure_property)
1505 slowPathOp(in_by_id)
1506 slowPathOp(in_by_val)
1507 slowPathOp(is_function)
1508 slowPathOp(is_object_or_null)
1509 slowPathOp(less)
1510 slowPathOp(lesseq)
1511 slowPathOp(mod)
1512 slowPathOp(new_array_buffer)
1513 slowPathOp(new_array_with_spread)
1514 slowPathOp(pow)
1515 slowPathOp(push_with_scope)
1516 slowPathOp(put_by_id_with_this)
1517 slowPathOp(put_by_val_with_this)
1518 slowPathOp(resolve_scope_for_hoisting_func_decl_in_eval)
1519 slowPathOp(spread)
1520 slowPathOp(strcat)
1521 slowPathOp(throw_static_error)
1522 slowPathOp(to_index_string)
1523 slowPathOp(typeof)
1524 slowPathOp(unreachable)
1525 
1526 macro llintSlowPathOp(opcodeName)
1527     llintOp(op_%opcodeName%, unused, macro (unused, unused, dispatch)
1528         callSlowPath(_llint_slow_path_%opcodeName%)
1529         dispatch()
1530     end)
1531 end
1532 
1533 llintSlowPathOp(del_by_id)
1534 llintSlowPathOp(del_by_val)
1535 llintSlowPathOp(instanceof)
1536 llintSlowPathOp(instanceof_custom)
1537 llintSlowPathOp(new_array)
1538 llintSlowPathOp(new_array_with_size)
1539 llintSlowPathOp(new_async_func)
1540 llintSlowPathOp(new_async_func_exp)
1541 llintSlowPathOp(new_async_generator_func)
1542 llintSlowPathOp(new_async_generator_func_exp)
1543 llintSlowPathOp(new_func)
1544 llintSlowPathOp(new_func_exp)
1545 llintSlowPathOp(new_generator_func)
1546 llintSlowPathOp(new_generator_func_exp)
1547 llintSlowPathOp(new_object)
1548 llintSlowPathOp(new_regexp)
1549 llintSlowPathOp(put_getter_by_id)
1550 llintSlowPathOp(put_getter_by_val)
1551 llintSlowPathOp(put_getter_setter_by_id)
1552 llintSlowPathOp(put_setter_by_id)
1553 llintSlowPathOp(put_setter_by_val)
1554 llintSlowPathOp(set_function_name)
1555 llintSlowPathOp(super_sampler_begin)
1556 llintSlowPathOp(super_sampler_end)
1557 llintSlowPathOp(throw)
1558 llintSlowPathOp(try_get_by_id)
1559 
1560 llintOp(op_switch_string, unused, macro (unused, unused, unused)
1561     callSlowPath(_llint_slow_path_switch_string)
1562     nextInstruction()
1563 end)
1564 
1565 
1566 equalityComparisonOp(eq, OpEq,
1567     macro (left, right, result) cieq left, right, result end)
1568 
1569 
1570 equalityComparisonOp(neq, OpNeq,
1571     macro (left, right, result) cineq left, right, result end)
1572 
1573 
1574 compareUnsignedOp(below, OpBelow,
1575         macro (left, right, result) cib left, right, result end)
1576 
1577 
1578 compareUnsignedOp(beloweq, OpBeloweq,
1579         macro (left, right, result) cibeq left, right, result end)
1580 
1581 
1582 llintOpWithJump(op_jmp, OpJmp, macro (size, get, jump, dispatch)
1583     jump(m_targetLabel)
1584 end)
1585 
1586 
1587 llintJumpTrueOrFalseOp(
1588     jtrue, OpJtrue,
1589     macro (value, target) btinz value, 1, target end)
1590 
1591 
1592 llintJumpTrueOrFalseOp(
1593     jfalse, OpJfalse,
1594     macro (value, target) btiz value, 1, target end)
1595 
1596 
1597 compareJumpOp(
1598     jless, OpJless,
1599     macro (left, right, target) bilt left, right, target end,
1600     macro (left, right, target) bdlt left, right, target end)
1601 
1602 
1603 compareJumpOp(
1604     jnless, OpJnless,
1605     macro (left, right, target) bigteq left, right, target end,
1606     macro (left, right, target) bdgtequn left, right, target end)
1607 
1608 
1609 compareJumpOp(
1610     jgreater, OpJgreater,
1611     macro (left, right, target) bigt left, right, target end,
1612     macro (left, right, target) bdgt left, right, target end)
1613 
1614 
1615 compareJumpOp(
1616     jngreater, OpJngreater,
1617     macro (left, right, target) bilteq left, right, target end,
1618     macro (left, right, target) bdltequn left, right, target end)
1619 
1620 
1621 compareJumpOp(
1622     jlesseq, OpJlesseq,
1623     macro (left, right, target) bilteq left, right, target end,
1624     macro (left, right, target) bdlteq left, right, target end)
1625 
1626 
1627 compareJumpOp(
1628     jnlesseq, OpJnlesseq,
1629     macro (left, right, target) bigt left, right, target end,
1630     macro (left, right, target) bdgtun left, right, target end)
1631 
1632 
1633 compareJumpOp(
1634     jgreatereq, OpJgreatereq,
1635     macro (left, right, target) bigteq left, right, target end,
1636     macro (left, right, target) bdgteq left, right, target end)
1637 
1638 
1639 compareJumpOp(
1640     jngreatereq, OpJngreatereq,
1641     macro (left, right, target) bilt left, right, target end,
1642     macro (left, right, target) bdltun left, right, target end)
1643 
1644 
1645 equalityJumpOp(
1646     jeq, OpJeq,
1647     macro (left, right, target) bieq left, right, target end)
1648 
1649 
1650 equalityJumpOp(
1651     jneq, OpJneq,
1652     macro (left, right, target) bineq left, right, target end)
1653 
1654 
1655 compareUnsignedJumpOp(
1656     jbelow, OpJbelow,
1657     macro (left, right, target) bib left, right, target end)
1658 
1659 
1660 compareUnsignedJumpOp(
1661     jbeloweq, OpJbeloweq,
1662     macro (left, right, target) bibeq left, right, target end)
1663 
1664 
1665 preOp(inc, OpInc,
1666     macro (value, slow) baddio 1, value, slow end)
1667 
1668 
1669 preOp(dec, OpDec,
1670     macro (value, slow) bsubio 1, value, slow end)
1671 
1672 
1673 llintOp(op_loop_hint, OpLoopHint, macro (unused, unused, dispatch)
<a name="56" id="anc56"></a><span class="line-modified">1674     # CheckTraps.</span>





1675     loadp CodeBlock[cfr], t1
1676     loadp CodeBlock::m_vm[t1], t1
<a name="57" id="anc57"></a><span class="line-modified">1677     btbnz VM::m_traps + VMTraps::m_needTrapHandling[t1], .handleTraps</span>

1678 .afterHandlingTraps:
<a name="58" id="anc58"></a><span class="line-added">1679     checkSwitchToJITForLoop()</span>
1680     dispatch()
1681 .handleTraps:
<a name="59" id="anc59"></a><span class="line-modified">1682     callTrapHandler(_llint_throw_from_slow_path_trampoline)</span>
1683     jmp .afterHandlingTraps
<a name="60" id="anc60"></a>

1684 end)
1685 
1686 
1687 # Returns the packet pointer in t0.
1688 macro acquireShadowChickenPacket(slow)
1689     loadp CodeBlock[cfr], t1
1690     loadp CodeBlock::m_vm[t1], t1
1691     loadp VM::m_shadowChicken[t1], t2
1692     loadp ShadowChicken::m_logCursor[t2], t0
1693     bpaeq t0, ShadowChicken::m_logEnd[t2], slow
1694     addp sizeof ShadowChicken::Packet, t0, t1
1695     storep t1, ShadowChicken::m_logCursor[t2]
1696 end
1697 
1698 
1699 llintOp(op_nop, OpNop, macro (unused, unused, dispatch)
1700     dispatch()
1701 end)
1702 
1703 
1704 # we can&#39;t use callOp because we can&#39;t pass `call` as the opcode name, since it&#39;s an instruction name
1705 commonCallOp(op_call, _llint_slow_path_call, OpCall, prepareForRegularCall, macro (getu, metadata)
1706     arrayProfileForCall(OpCall, getu)
1707 end)
1708 
1709 
1710 macro callOp(opcodeName, opcodeStruct, prepareCall, fn)
1711     commonCallOp(op_%opcodeName%, _llint_slow_path_%opcodeName%, opcodeStruct, prepareCall, fn)
1712 end
1713 
1714 
1715 callOp(tail_call, OpTailCall, prepareForTailCall, macro (getu, metadata)
1716     arrayProfileForCall(OpTailCall, getu)
1717     checkSwitchToJITForEpilogue()
1718     # reload metadata since checkSwitchToJITForEpilogue() might have trashed t5
1719     metadata(t5, t0)
1720 end)
1721 
1722 
1723 callOp(construct, OpConstruct, prepareForRegularCall, macro (getu, metadata) end)
1724 
1725 
1726 macro doCallVarargs(size, opcodeStruct, dispatch, frameSlowPath, slowPath, prepareCall)
1727     callSlowPath(frameSlowPath)
1728     branchIfException(_llint_throw_from_slow_path_trampoline)
1729     # calleeFrame in r1
1730     if JSVALUE64
1731         move r1, sp
1732     else
1733         # The calleeFrame is not stack aligned, move down by CallerFrameAndPCSize to align
1734         if ARMv7
1735             subp r1, CallerFrameAndPCSize, t2
1736             move t2, sp
1737         else
1738             subp r1, CallerFrameAndPCSize, sp
1739         end
1740     end
1741     slowPathForCall(size, opcodeStruct, dispatch, slowPath, prepareCall)
1742 end
1743 
1744 
1745 llintOp(op_call_varargs, OpCallVarargs, macro (size, get, dispatch)
1746     doCallVarargs(size, OpCallVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_call_varargs, prepareForRegularCall)
1747 end)
1748 
1749 llintOp(op_tail_call_varargs, OpTailCallVarargs, macro (size, get, dispatch)
1750     checkSwitchToJITForEpilogue()
1751     # We lie and perform the tail call instead of preparing it since we can&#39;t
1752     # prepare the frame for a call opcode
1753     doCallVarargs(size, OpTailCallVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_tail_call_varargs, prepareForTailCall)
1754 end)
1755 
1756 
1757 llintOp(op_tail_call_forward_arguments, OpTailCallForwardArguments, macro (size, get, dispatch)
1758     checkSwitchToJITForEpilogue()
1759     # We lie and perform the tail call instead of preparing it since we can&#39;t
1760     # prepare the frame for a call opcode
1761     doCallVarargs(size, OpTailCallForwardArguments, dispatch, _llint_slow_path_size_frame_for_forward_arguments, _llint_slow_path_tail_call_forward_arguments, prepareForTailCall)
1762 end)
1763 
1764 
1765 llintOp(op_construct_varargs, OpConstructVarargs, macro (size, get, dispatch)
1766     doCallVarargs(size, OpConstructVarargs, dispatch, _llint_slow_path_size_frame_for_varargs, _llint_slow_path_construct_varargs, prepareForRegularCall)
1767 end)
1768 
1769 
1770 # Eval is executed in one of two modes:
1771 #
1772 # 1) We find that we&#39;re really invoking eval() in which case the
1773 #    execution is perfomed entirely inside the slow_path, and it
1774 #    returns the PC of a function that just returns the return value
1775 #    that the eval returned.
1776 #
1777 # 2) We find that we&#39;re invoking something called eval() that is not
1778 #    the real eval. Then the slow_path returns the PC of the thing to
1779 #    call, and we call it.
1780 #
1781 # This allows us to handle two cases, which would require a total of
1782 # up to four pieces of state that cannot be easily packed into two
1783 # registers (C functions can return up to two registers, easily):
1784 #
1785 # - The call frame register. This may or may not have been modified
1786 #   by the slow_path, but the convention is that it returns it. It&#39;s not
1787 #   totally clear if that&#39;s necessary, since the cfr is callee save.
1788 #   But that&#39;s our style in this here interpreter so we stick with it.
1789 #
1790 # - A bit to say if the slow_path successfully executed the eval and has
1791 #   the return value, or did not execute the eval but has a PC for us
1792 #   to call.
1793 #
1794 # - Either:
1795 #   - The JS return value (two registers), or
1796 #
1797 #   - The PC to call.
1798 #
1799 # It turns out to be easier to just always have this return the cfr
1800 # and a PC to call, and that PC may be a dummy thunk that just
1801 # returns the JS value that the eval returned.
1802 
1803 _llint_op_call_eval:
1804     slowPathForCall(
1805         narrow,
1806         OpCallEval,
1807         macro () dispatchOp(narrow, op_call_eval) end,
1808         _llint_slow_path_call_eval,
1809         prepareForRegularCall)
1810 
<a name="61" id="anc61"></a><span class="line-modified">1811 _llint_op_call_eval_wide16:</span>
1812     slowPathForCall(
<a name="62" id="anc62"></a><span class="line-modified">1813         wide16,</span>
1814         OpCallEval,
<a name="63" id="anc63"></a><span class="line-modified">1815         macro () dispatchOp(wide16, op_call_eval) end,</span>
<span class="line-modified">1816         _llint_slow_path_call_eval_wide16,</span>
<span class="line-added">1817         prepareForRegularCall)</span>
<span class="line-added">1818 </span>
<span class="line-added">1819 _llint_op_call_eval_wide32:</span>
<span class="line-added">1820     slowPathForCall(</span>
<span class="line-added">1821         wide32,</span>
<span class="line-added">1822         OpCallEval,</span>
<span class="line-added">1823         macro () dispatchOp(wide32, op_call_eval) end,</span>
<span class="line-added">1824         _llint_slow_path_call_eval_wide32,</span>
1825         prepareForRegularCall)
1826 
<a name="64" id="anc64"></a>



1827 
<a name="65" id="anc65"></a><span class="line-modified">1828 commonOp(llint_generic_return_point, macro () end, macro (size)</span>
<span class="line-modified">1829     dispatchAfterCall(size, OpCallEval, macro ()</span>
<span class="line-modified">1830         dispatchOp(size, op_call_eval)</span>
1831     end)
<a name="66" id="anc66"></a><span class="line-added">1832 end)</span>
<span class="line-added">1833 </span>
1834 
1835 llintOp(op_identity_with_profile, OpIdentityWithProfile, macro (unused, unused, dispatch)
1836     dispatch()
1837 end)
1838 
1839 
1840 llintOp(op_yield, OpYield, macro (unused, unused, unused)
1841     notSupported()
1842 end)
1843 
1844 
<a name="67" id="anc67"></a><span class="line-added">1845 llintOp(op_create_generator_frame_environment, OpYield, macro (unused, unused, unused)</span>
<span class="line-added">1846     notSupported()</span>
<span class="line-added">1847 end)</span>
<span class="line-added">1848 </span>
<span class="line-added">1849 </span>
1850 llintOp(op_debug, OpDebug, macro (unused, unused, dispatch)
1851     loadp CodeBlock[cfr], t0
1852     loadi CodeBlock::m_debuggerRequests[t0], t0
1853     btiz t0, .opDebugDone
1854     callSlowPath(_llint_slow_path_debug)
1855 .opDebugDone:                    
1856     dispatch()
1857 end)
1858 
1859 
1860 op(llint_native_call_trampoline, macro ()
1861     nativeCallTrampoline(NativeExecutable::m_function)
1862 end)
1863 
1864 
1865 op(llint_native_construct_trampoline, macro ()
1866     nativeCallTrampoline(NativeExecutable::m_constructor)
1867 end)
1868 
1869 
1870 op(llint_internal_function_call_trampoline, macro ()
1871     internalFunctionCallTrampoline(InternalFunction::m_functionForCall)
1872 end)
1873 
1874 
1875 op(llint_internal_function_construct_trampoline, macro ()
1876     internalFunctionCallTrampoline(InternalFunction::m_functionForConstruct)
1877 end)
1878 
1879 
1880 # Lastly, make sure that we can link even though we don&#39;t support all opcodes.
1881 # These opcodes should never arise when using LLInt or either JIT. We assert
1882 # as much.
1883 
1884 macro notSupported()
1885     if ASSERT_ENABLED
1886         crash()
1887     else
1888         # We should use whatever the smallest possible instruction is, just to
1889         # ensure that there is a gap between instruction labels. If multiple
1890         # smallest instructions exist, we should pick the one that is most
1891         # likely result in execution being halted. Currently that is the break
1892         # instruction on all architectures we&#39;re interested in. (Break is int3
1893         # on Intel, which is 1 byte, and bkpt on ARMv7, which is 2 bytes.)
1894         break
1895     end
1896 end
<a name="68" id="anc68"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="68" type="hidden" />
</body>
</html>