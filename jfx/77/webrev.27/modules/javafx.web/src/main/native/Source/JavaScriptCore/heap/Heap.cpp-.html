<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/Heap.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  *  Copyright (C) 2003-2019 Apple Inc. All rights reserved.
   3  *  Copyright (C) 2007 Eric Seidel &lt;eric@webkit.org&gt;
   4  *
   5  *  This library is free software; you can redistribute it and/or
   6  *  modify it under the terms of the GNU Lesser General Public
   7  *  License as published by the Free Software Foundation; either
   8  *  version 2 of the License, or (at your option) any later version.
   9  *
  10  *  This library is distributed in the hope that it will be useful,
  11  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
  12  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
  13  *  Lesser General Public License for more details.
  14  *
  15  *  You should have received a copy of the GNU Lesser General Public
  16  *  License along with this library; if not, write to the Free Software
  17  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
  18  *
  19  */
  20 
  21 #include &quot;config.h&quot;
  22 #include &quot;Heap.h&quot;
  23 
  24 #include &quot;BlockDirectoryInlines.h&quot;
  25 #include &quot;CodeBlock.h&quot;
  26 #include &quot;CodeBlockSetInlines.h&quot;
  27 #include &quot;CollectingScope.h&quot;
  28 #include &quot;ConservativeRoots.h&quot;
  29 #include &quot;DFGWorklistInlines.h&quot;
  30 #include &quot;EdenGCActivityCallback.h&quot;
  31 #include &quot;Exception.h&quot;
  32 #include &quot;FullGCActivityCallback.h&quot;
  33 #include &quot;GCActivityCallback.h&quot;
  34 #include &quot;GCIncomingRefCountedSetInlines.h&quot;
  35 #include &quot;GCSegmentedArrayInlines.h&quot;
  36 #include &quot;GCTypeMap.h&quot;
  37 #include &quot;HasOwnPropertyCache.h&quot;
  38 #include &quot;HeapHelperPool.h&quot;
  39 #include &quot;HeapIterationScope.h&quot;
  40 #include &quot;HeapProfiler.h&quot;
  41 #include &quot;HeapSnapshot.h&quot;
  42 #include &quot;HeapVerifier.h&quot;
  43 #include &quot;IncrementalSweeper.h&quot;
  44 #include &quot;InferredValueInlines.h&quot;
  45 #include &quot;Interpreter.h&quot;
  46 #include &quot;IsoCellSetInlines.h&quot;
  47 #include &quot;JITStubRoutineSet.h&quot;
  48 #include &quot;JITWorklist.h&quot;
  49 #include &quot;JSCInlines.h&quot;
  50 #include &quot;JSGlobalObject.h&quot;
  51 #include &quot;JSLock.h&quot;
  52 #include &quot;JSVirtualMachineInternal.h&quot;
  53 #include &quot;JSWeakMap.h&quot;
  54 #include &quot;JSWeakSet.h&quot;
  55 #include &quot;JSWebAssemblyCodeBlock.h&quot;
  56 #include &quot;MachineStackMarker.h&quot;
  57 #include &quot;MarkStackMergingConstraint.h&quot;
  58 #include &quot;MarkedSpaceInlines.h&quot;
  59 #include &quot;MarkingConstraintSet.h&quot;
  60 #include &quot;PreventCollectionScope.h&quot;
  61 #include &quot;SamplingProfiler.h&quot;
  62 #include &quot;ShadowChicken.h&quot;
  63 #include &quot;SpaceTimeMutatorScheduler.h&quot;
  64 #include &quot;StochasticSpaceTimeMutatorScheduler.h&quot;
  65 #include &quot;StopIfNecessaryTimer.h&quot;
  66 #include &quot;SubspaceInlines.h&quot;
  67 #include &quot;SuperSampler.h&quot;
  68 #include &quot;SweepingScope.h&quot;
  69 #include &quot;SynchronousStopTheWorldMutatorScheduler.h&quot;
  70 #include &quot;TypeProfiler.h&quot;
  71 #include &quot;TypeProfilerLog.h&quot;
  72 #include &quot;UnlinkedCodeBlock.h&quot;
  73 #include &quot;VM.h&quot;
  74 #include &quot;VisitCounter.h&quot;
  75 #include &quot;WasmMemory.h&quot;
  76 #include &quot;WeakMapImplInlines.h&quot;
  77 #include &quot;WeakSetInlines.h&quot;
  78 #include &lt;algorithm&gt;
  79 #include &lt;wtf/ListDump.h&gt;
  80 #include &lt;wtf/MainThread.h&gt;
  81 #include &lt;wtf/ParallelVectorIterator.h&gt;
  82 #include &lt;wtf/ProcessID.h&gt;
  83 #include &lt;wtf/RAMSize.h&gt;
  84 #include &lt;wtf/SimpleStats.h&gt;
  85 #include &lt;wtf/Threading.h&gt;
  86 
  87 #if PLATFORM(IOS_FAMILY)
  88 #include &lt;bmalloc/bmalloc.h&gt;
  89 #endif
  90 
  91 #if USE(FOUNDATION)
  92 #include &lt;wtf/spi/cocoa/objcSPI.h&gt;
  93 #endif
  94 
  95 #if USE(GLIB)
  96 #include &quot;JSCGLibWrapperObject.h&quot;
  97 #endif
  98 
  99 namespace JSC {
 100 
 101 namespace {
 102 
 103 bool verboseStop = false;
 104 
 105 double maxPauseMS(double thisPauseMS)
 106 {
 107     static double maxPauseMS;
 108     maxPauseMS = std::max(thisPauseMS, maxPauseMS);
 109     return maxPauseMS;
 110 }
 111 
 112 size_t minHeapSize(HeapType heapType, size_t ramSize)
 113 {
 114     if (heapType == LargeHeap) {
 115         double result = std::min(
 116             static_cast&lt;double&gt;(Options::largeHeapSize()),
 117             ramSize * Options::smallHeapRAMFraction());
 118         return static_cast&lt;size_t&gt;(result);
 119     }
 120     return Options::smallHeapSize();
 121 }
 122 
 123 size_t proportionalHeapSize(size_t heapSize, size_t ramSize)
 124 {
 125     if (VM::isInMiniMode())
 126         return Options::miniVMHeapGrowthFactor() * heapSize;
 127 
 128 #if PLATFORM(IOS_FAMILY)
 129     size_t memoryFootprint = bmalloc::api::memoryFootprint();
 130     if (memoryFootprint &lt; ramSize * Options::smallHeapRAMFraction())
 131         return Options::smallHeapGrowthFactor() * heapSize;
 132     if (memoryFootprint &lt; ramSize * Options::mediumHeapRAMFraction())
 133         return Options::mediumHeapGrowthFactor() * heapSize;
 134 #else
 135     if (heapSize &lt; ramSize * Options::smallHeapRAMFraction())
 136         return Options::smallHeapGrowthFactor() * heapSize;
 137     if (heapSize &lt; ramSize * Options::mediumHeapRAMFraction())
 138         return Options::mediumHeapGrowthFactor() * heapSize;
 139 #endif
 140     return Options::largeHeapGrowthFactor() * heapSize;
 141 }
 142 
 143 bool isValidSharedInstanceThreadState(VM* vm)
 144 {
 145     return vm-&gt;currentThreadIsHoldingAPILock();
 146 }
 147 
 148 bool isValidThreadState(VM* vm)
 149 {
 150     if (vm-&gt;atomicStringTable() != WTF::Thread::current().atomicStringTable())
 151         return false;
 152 
 153     if (vm-&gt;isSharedInstance() &amp;&amp; !isValidSharedInstanceThreadState(vm))
 154         return false;
 155 
 156     return true;
 157 }
 158 
 159 void recordType(VM&amp; vm, TypeCountSet&amp; set, JSCell* cell)
 160 {
 161     const char* typeName = &quot;[unknown]&quot;;
 162     const ClassInfo* info = cell-&gt;classInfo(vm);
 163     if (info &amp;&amp; info-&gt;className)
 164         typeName = info-&gt;className;
 165     set.add(typeName);
 166 }
 167 
 168 bool measurePhaseTiming()
 169 {
 170     return false;
 171 }
 172 
 173 HashMap&lt;const char*, GCTypeMap&lt;SimpleStats&gt;&gt;&amp; timingStats()
 174 {
 175     static HashMap&lt;const char*, GCTypeMap&lt;SimpleStats&gt;&gt;* result;
 176     static std::once_flag once;
 177     std::call_once(
 178         once,
 179         [] {
 180             result = new HashMap&lt;const char*, GCTypeMap&lt;SimpleStats&gt;&gt;();
 181         });
 182     return *result;
 183 }
 184 
 185 SimpleStats&amp; timingStats(const char* name, CollectionScope scope)
 186 {
 187     return timingStats().add(name, GCTypeMap&lt;SimpleStats&gt;()).iterator-&gt;value[scope];
 188 }
 189 
 190 class TimingScope {
 191 public:
 192     TimingScope(Optional&lt;CollectionScope&gt; scope, const char* name)
 193         : m_scope(scope)
 194         , m_name(name)
 195     {
 196         if (measurePhaseTiming())
 197             m_before = MonotonicTime::now();
 198     }
 199 
 200     TimingScope(Heap&amp; heap, const char* name)
 201         : TimingScope(heap.collectionScope(), name)
 202     {
 203     }
 204 
 205     void setScope(Optional&lt;CollectionScope&gt; scope)
 206     {
 207         m_scope = scope;
 208     }
 209 
 210     void setScope(Heap&amp; heap)
 211     {
 212         setScope(heap.collectionScope());
 213     }
 214 
 215     ~TimingScope()
 216     {
 217         if (measurePhaseTiming()) {
 218             MonotonicTime after = MonotonicTime::now();
 219             Seconds timing = after - m_before;
 220             SimpleStats&amp; stats = timingStats(m_name, *m_scope);
 221             stats.add(timing.milliseconds());
 222             dataLog(&quot;[GC:&quot;, *m_scope, &quot;] &quot;, m_name, &quot; took: &quot;, timing.milliseconds(), &quot;ms (average &quot;, stats.mean(), &quot;ms).\n&quot;);
 223         }
 224     }
 225 private:
 226     Optional&lt;CollectionScope&gt; m_scope;
 227     MonotonicTime m_before;
 228     const char* m_name;
 229 };
 230 
 231 } // anonymous namespace
 232 
 233 class Heap::Thread : public AutomaticThread {
 234 public:
 235     Thread(const AbstractLocker&amp; locker, Heap&amp; heap)
 236         : AutomaticThread(locker, heap.m_threadLock, heap.m_threadCondition.copyRef())
 237         , m_heap(heap)
 238     {
 239     }
 240 
 241     const char* name() const override
 242     {
 243         return &quot;JSC Heap Collector Thread&quot;;
 244     }
 245 
 246 protected:
 247     PollResult poll(const AbstractLocker&amp; locker) override
 248     {
 249         if (m_heap.m_threadShouldStop) {
 250             m_heap.notifyThreadStopping(locker);
 251             return PollResult::Stop;
 252         }
 253         if (m_heap.shouldCollectInCollectorThread(locker))
 254             return PollResult::Work;
 255         return PollResult::Wait;
 256     }
 257 
 258     WorkResult work() override
 259     {
 260         m_heap.collectInCollectorThread();
 261         return WorkResult::Continue;
 262     }
 263 
 264     void threadDidStart() override
 265     {
 266         WTF::registerGCThread(GCThreadType::Main);
 267     }
 268 
 269 private:
 270     Heap&amp; m_heap;
 271 };
 272 
 273 Heap::Heap(VM* vm, HeapType heapType)
 274     : m_heapType(heapType)
 275     , m_ramSize(Options::forceRAMSize() ? Options::forceRAMSize() : ramSize())
 276     , m_minBytesPerCycle(minHeapSize(m_heapType, m_ramSize))
 277     , m_maxEdenSize(m_minBytesPerCycle)
 278     , m_maxHeapSize(m_minBytesPerCycle)
 279     , m_objectSpace(this)
 280     , m_machineThreads(std::make_unique&lt;MachineThreads&gt;())
 281     , m_collectorSlotVisitor(std::make_unique&lt;SlotVisitor&gt;(*this, &quot;C&quot;))
 282     , m_mutatorSlotVisitor(std::make_unique&lt;SlotVisitor&gt;(*this, &quot;M&quot;))
 283     , m_mutatorMarkStack(std::make_unique&lt;MarkStackArray&gt;())
 284     , m_raceMarkStack(std::make_unique&lt;MarkStackArray&gt;())
 285     , m_constraintSet(std::make_unique&lt;MarkingConstraintSet&gt;(*this))
 286     , m_handleSet(vm)
 287     , m_codeBlocks(std::make_unique&lt;CodeBlockSet&gt;())
 288     , m_jitStubRoutines(std::make_unique&lt;JITStubRoutineSet&gt;())
 289     , m_vm(vm)
 290     // We seed with 10ms so that GCActivityCallback::didAllocate doesn&#39;t continuously
 291     // schedule the timer if we&#39;ve never done a collection.
 292     , m_fullActivityCallback(GCActivityCallback::tryCreateFullTimer(this))
 293     , m_edenActivityCallback(GCActivityCallback::tryCreateEdenTimer(this))
 294     , m_sweeper(adoptRef(*new IncrementalSweeper(this)))
 295     , m_stopIfNecessaryTimer(adoptRef(*new StopIfNecessaryTimer(vm)))
 296     , m_sharedCollectorMarkStack(std::make_unique&lt;MarkStackArray&gt;())
 297     , m_sharedMutatorMarkStack(std::make_unique&lt;MarkStackArray&gt;())
 298     , m_helperClient(&amp;heapHelperPool())
 299     , m_threadLock(Box&lt;Lock&gt;::create())
 300     , m_threadCondition(AutomaticThreadCondition::create())
 301 {
 302     m_worldState.store(0);
 303 
 304     if (Options::useConcurrentGC()) {
 305         if (Options::useStochasticMutatorScheduler())
 306             m_scheduler = std::make_unique&lt;StochasticSpaceTimeMutatorScheduler&gt;(*this);
 307         else
 308             m_scheduler = std::make_unique&lt;SpaceTimeMutatorScheduler&gt;(*this);
 309     } else {
 310         // We simulate turning off concurrent GC by making the scheduler say that the world
 311         // should always be stopped when the collector is running.
 312         m_scheduler = std::make_unique&lt;SynchronousStopTheWorldMutatorScheduler&gt;();
 313     }
 314 
 315     if (Options::verifyHeap())
 316         m_verifier = std::make_unique&lt;HeapVerifier&gt;(this, Options::numberOfGCCyclesToRecordForVerification());
 317 
 318     m_collectorSlotVisitor-&gt;optimizeForStoppedMutator();
 319 
 320     // When memory is critical, allow allocating 25% of the amount above the critical threshold before collecting.
 321     size_t memoryAboveCriticalThreshold = static_cast&lt;size_t&gt;(static_cast&lt;double&gt;(m_ramSize) * (1.0 - Options::criticalGCMemoryThreshold()));
 322     m_maxEdenSizeWhenCritical = memoryAboveCriticalThreshold / 4;
 323 
 324     LockHolder locker(*m_threadLock);
 325     m_thread = adoptRef(new Thread(locker, *this));
 326 }
 327 
 328 Heap::~Heap()
 329 {
 330     forEachSlotVisitor(
 331         [&amp;] (SlotVisitor&amp; visitor) {
 332             visitor.clearMarkStacks();
 333         });
 334     m_mutatorMarkStack-&gt;clear();
 335     m_raceMarkStack-&gt;clear();
 336 
 337     for (WeakBlock* block : m_logicallyEmptyWeakBlocks)
 338         WeakBlock::destroy(*this, block);
 339 }
 340 
 341 bool Heap::isPagedOut(MonotonicTime deadline)
 342 {
 343     return m_objectSpace.isPagedOut(deadline);
 344 }
 345 
 346 // The VM is being destroyed and the collector will never run again.
 347 // Run all pending finalizers now because we won&#39;t get another chance.
 348 void Heap::lastChanceToFinalize()
 349 {
 350     MonotonicTime before;
 351     if (Options::logGC()) {
 352         before = MonotonicTime::now();
 353         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: shutdown &quot;);
 354     }
 355 
 356     m_isShuttingDown = true;
 357 
 358     RELEASE_ASSERT(!m_vm-&gt;entryScope);
 359     RELEASE_ASSERT(m_mutatorState == MutatorState::Running);
 360 
 361     if (m_collectContinuouslyThread) {
 362         {
 363             LockHolder locker(m_collectContinuouslyLock);
 364             m_shouldStopCollectingContinuously = true;
 365             m_collectContinuouslyCondition.notifyOne();
 366         }
 367         m_collectContinuouslyThread-&gt;waitForCompletion();
 368     }
 369 
 370     if (Options::logGC())
 371         dataLog(&quot;1&quot;);
 372 
 373     // Prevent new collections from being started. This is probably not even necessary, since we&#39;re not
 374     // going to call into anything that starts collections. Still, this makes the algorithm more
 375     // obviously sound.
 376     m_isSafeToCollect = false;
 377 
 378     if (Options::logGC())
 379         dataLog(&quot;2&quot;);
 380 
 381     bool isCollecting;
 382     {
 383         auto locker = holdLock(*m_threadLock);
 384         RELEASE_ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
 385         isCollecting = m_lastServedTicket &lt; m_lastGrantedTicket;
 386     }
 387     if (isCollecting) {
 388         if (Options::logGC())
 389             dataLog(&quot;...]\n&quot;);
 390 
 391         // Wait for the current collection to finish.
 392         waitForCollector(
 393             [&amp;] (const AbstractLocker&amp;) -&gt; bool {
 394                 RELEASE_ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
 395                 return m_lastServedTicket == m_lastGrantedTicket;
 396             });
 397 
 398         if (Options::logGC())
 399             dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: shutdown &quot;);
 400     }
 401     if (Options::logGC())
 402         dataLog(&quot;3&quot;);
 403 
 404     RELEASE_ASSERT(m_requests.isEmpty());
 405     RELEASE_ASSERT(m_lastServedTicket == m_lastGrantedTicket);
 406 
 407     // Carefully bring the thread down.
 408     bool stopped = false;
 409     {
 410         LockHolder locker(*m_threadLock);
 411         stopped = m_thread-&gt;tryStop(locker);
 412         m_threadShouldStop = true;
 413         if (!stopped)
 414             m_threadCondition-&gt;notifyOne(locker);
 415     }
 416 
 417     if (Options::logGC())
 418         dataLog(&quot;4&quot;);
 419 
 420     if (!stopped)
 421         m_thread-&gt;join();
 422 
 423     if (Options::logGC())
 424         dataLog(&quot;5 &quot;);
 425 
 426     m_arrayBuffers.lastChanceToFinalize();
 427     m_objectSpace.stopAllocatingForGood();
 428     m_objectSpace.lastChanceToFinalize();
 429     releaseDelayedReleasedObjects();
 430 
 431     sweepAllLogicallyEmptyWeakBlocks();
 432 
 433     m_objectSpace.freeMemory();
 434 
 435     if (Options::logGC())
 436         dataLog((MonotonicTime::now() - before).milliseconds(), &quot;ms]\n&quot;);
 437 }
 438 
 439 void Heap::releaseDelayedReleasedObjects()
 440 {
 441 #if USE(FOUNDATION) || USE(GLIB)
 442     // We need to guard against the case that releasing an object can create more objects due to the
 443     // release calling into JS. When those JS call(s) exit and all locks are being dropped we end up
 444     // back here and could try to recursively release objects. We guard that with a recursive entry
 445     // count. Only the initial call will release objects, recursive calls simple return and let the
 446     // the initial call to the function take care of any objects created during release time.
 447     // This also means that we need to loop until there are no objects in m_delayedReleaseObjects
 448     // and use a temp Vector for the actual releasing.
 449     if (!m_delayedReleaseRecursionCount++) {
 450         while (!m_delayedReleaseObjects.isEmpty()) {
 451             ASSERT(m_vm-&gt;currentThreadIsHoldingAPILock());
 452 
 453             auto objectsToRelease = WTFMove(m_delayedReleaseObjects);
 454 
 455             {
 456                 // We need to drop locks before calling out to arbitrary code.
 457                 JSLock::DropAllLocks dropAllLocks(m_vm);
 458 
 459 #if USE(FOUNDATION)
 460                 void* context = objc_autoreleasePoolPush();
 461 #endif
 462                 objectsToRelease.clear();
 463 #if USE(FOUNDATION)
 464                 objc_autoreleasePoolPop(context);
 465 #endif
 466             }
 467         }
 468     }
 469     m_delayedReleaseRecursionCount--;
 470 #endif
 471 }
 472 
 473 void Heap::reportExtraMemoryAllocatedSlowCase(size_t size)
 474 {
 475     didAllocate(size);
 476     collectIfNecessaryOrDefer();
 477 }
 478 
 479 void Heap::deprecatedReportExtraMemorySlowCase(size_t size)
 480 {
 481     // FIXME: Change this to use SaturatedArithmetic when available.
 482     // https://bugs.webkit.org/show_bug.cgi?id=170411
 483     Checked&lt;size_t, RecordOverflow&gt; checkedNewSize = m_deprecatedExtraMemorySize;
 484     checkedNewSize += size;
 485     m_deprecatedExtraMemorySize = UNLIKELY(checkedNewSize.hasOverflowed()) ? std::numeric_limits&lt;size_t&gt;::max() : checkedNewSize.unsafeGet();
 486     reportExtraMemoryAllocatedSlowCase(size);
 487 }
 488 
 489 bool Heap::overCriticalMemoryThreshold(MemoryThresholdCallType memoryThresholdCallType)
 490 {
 491 #if PLATFORM(IOS_FAMILY)
 492     if (memoryThresholdCallType == MemoryThresholdCallType::Direct || ++m_precentAvailableMemoryCachedCallCount &gt;= 100) {
 493         m_overCriticalMemoryThreshold = bmalloc::api::percentAvailableMemoryInUse() &gt; Options::criticalGCMemoryThreshold();
 494         m_precentAvailableMemoryCachedCallCount = 0;
 495     }
 496 
 497     return m_overCriticalMemoryThreshold;
 498 #else
 499     UNUSED_PARAM(memoryThresholdCallType);
 500     return false;
 501 #endif
 502 }
 503 
 504 void Heap::reportAbandonedObjectGraph()
 505 {
 506     // Our clients don&#39;t know exactly how much memory they
 507     // are abandoning so we just guess for them.
 508     size_t abandonedBytes = static_cast&lt;size_t&gt;(0.1 * capacity());
 509 
 510     // We want to accelerate the next collection. Because memory has just
 511     // been abandoned, the next collection has the potential to
 512     // be more profitable. Since allocation is the trigger for collection,
 513     // we hasten the next collection by pretending that we&#39;ve allocated more memory.
 514     if (m_fullActivityCallback) {
 515         m_fullActivityCallback-&gt;didAllocate(*this,
 516             m_sizeAfterLastCollect - m_sizeAfterLastFullCollect + m_bytesAllocatedThisCycle + m_bytesAbandonedSinceLastFullCollect);
 517     }
 518     m_bytesAbandonedSinceLastFullCollect += abandonedBytes;
 519 }
 520 
 521 void Heap::protect(JSValue k)
 522 {
 523     ASSERT(k);
 524     ASSERT(m_vm-&gt;currentThreadIsHoldingAPILock());
 525 
 526     if (!k.isCell())
 527         return;
 528 
 529     m_protectedValues.add(k.asCell());
 530 }
 531 
 532 bool Heap::unprotect(JSValue k)
 533 {
 534     ASSERT(k);
 535     ASSERT(m_vm-&gt;currentThreadIsHoldingAPILock());
 536 
 537     if (!k.isCell())
 538         return false;
 539 
 540     return m_protectedValues.remove(k.asCell());
 541 }
 542 
 543 void Heap::addReference(JSCell* cell, ArrayBuffer* buffer)
 544 {
 545     if (m_arrayBuffers.addReference(cell, buffer)) {
 546         collectIfNecessaryOrDefer();
 547         didAllocate(buffer-&gt;gcSizeEstimateInBytes());
 548     }
 549 }
 550 
 551 template&lt;typename CellType, typename CellSet&gt;
 552 void Heap::finalizeMarkedUnconditionalFinalizers(CellSet&amp; cellSet)
 553 {
 554     cellSet.forEachMarkedCell(
 555         [&amp;] (HeapCell* cell, HeapCell::Kind) {
 556             static_cast&lt;CellType*&gt;(cell)-&gt;finalizeUnconditionally(*vm());
 557         });
 558 }
 559 
 560 void Heap::finalizeUnconditionalFinalizers()
 561 {
 562     if (vm()-&gt;m_inferredValueSpace)
 563         finalizeMarkedUnconditionalFinalizers&lt;InferredValue&gt;(vm()-&gt;m_inferredValueSpace-&gt;space);
 564     vm()-&gt;forEachCodeBlockSpace(
 565         [&amp;] (auto&amp; space) {
 566             this-&gt;finalizeMarkedUnconditionalFinalizers&lt;CodeBlock&gt;(space.set);
 567         });
 568     finalizeMarkedUnconditionalFinalizers&lt;ExecutableToCodeBlockEdge&gt;(vm()-&gt;executableToCodeBlockEdgesWithFinalizers);
 569     if (vm()-&gt;m_weakSetSpace)
 570         finalizeMarkedUnconditionalFinalizers&lt;JSWeakSet&gt;(*vm()-&gt;m_weakSetSpace);
 571     if (vm()-&gt;m_weakMapSpace)
 572         finalizeMarkedUnconditionalFinalizers&lt;JSWeakMap&gt;(*vm()-&gt;m_weakMapSpace);
 573     if (vm()-&gt;m_errorInstanceSpace)
 574         finalizeMarkedUnconditionalFinalizers&lt;ErrorInstance&gt;(*vm()-&gt;m_errorInstanceSpace);
 575 
 576 #if ENABLE(WEBASSEMBLY)
 577     if (vm()-&gt;m_webAssemblyCodeBlockSpace)
 578         finalizeMarkedUnconditionalFinalizers&lt;JSWebAssemblyCodeBlock&gt;(*vm()-&gt;m_webAssemblyCodeBlockSpace);
 579 #endif
 580 }
 581 
 582 void Heap::willStartIterating()
 583 {
 584     m_objectSpace.willStartIterating();
 585 }
 586 
 587 void Heap::didFinishIterating()
 588 {
 589     m_objectSpace.didFinishIterating();
 590 }
 591 
 592 void Heap::completeAllJITPlans()
 593 {
 594     if (!VM::canUseJIT())
 595         return;
 596 #if ENABLE(JIT)
 597     JITWorklist::ensureGlobalWorklist().completeAllForVM(*m_vm);
 598 #endif // ENABLE(JIT)
 599     DFG::completeAllPlansForVM(*m_vm);
 600 }
 601 
 602 template&lt;typename Func&gt;
 603 void Heap::iterateExecutingAndCompilingCodeBlocks(const Func&amp; func)
 604 {
 605     m_codeBlocks-&gt;iterateCurrentlyExecuting(func);
 606     if (VM::canUseJIT())
 607         DFG::iterateCodeBlocksForGC(*m_vm, func);
 608 }
 609 
 610 template&lt;typename Func&gt;
 611 void Heap::iterateExecutingAndCompilingCodeBlocksWithoutHoldingLocks(const Func&amp; func)
 612 {
 613     Vector&lt;CodeBlock*, 256&gt; codeBlocks;
 614     iterateExecutingAndCompilingCodeBlocks(
 615         [&amp;] (CodeBlock* codeBlock) {
 616             codeBlocks.append(codeBlock);
 617         });
 618     for (CodeBlock* codeBlock : codeBlocks)
 619         func(codeBlock);
 620 }
 621 
 622 void Heap::assertMarkStacksEmpty()
 623 {
 624     bool ok = true;
 625 
 626     if (!m_sharedCollectorMarkStack-&gt;isEmpty()) {
 627         dataLog(&quot;FATAL: Shared collector mark stack not empty! It has &quot;, m_sharedCollectorMarkStack-&gt;size(), &quot; elements.\n&quot;);
 628         ok = false;
 629     }
 630 
 631     if (!m_sharedMutatorMarkStack-&gt;isEmpty()) {
 632         dataLog(&quot;FATAL: Shared mutator mark stack not empty! It has &quot;, m_sharedMutatorMarkStack-&gt;size(), &quot; elements.\n&quot;);
 633         ok = false;
 634     }
 635 
 636     forEachSlotVisitor(
 637         [&amp;] (SlotVisitor&amp; visitor) {
 638             if (visitor.isEmpty())
 639                 return;
 640 
 641             dataLog(&quot;FATAL: Visitor &quot;, RawPointer(&amp;visitor), &quot; is not empty!\n&quot;);
 642             ok = false;
 643         });
 644 
 645     RELEASE_ASSERT(ok);
 646 }
 647 
 648 void Heap::gatherStackRoots(ConservativeRoots&amp; roots)
 649 {
 650     m_machineThreads-&gt;gatherConservativeRoots(roots, *m_jitStubRoutines, *m_codeBlocks, m_currentThreadState, m_currentThread);
 651 }
 652 
 653 void Heap::gatherJSStackRoots(ConservativeRoots&amp; roots)
 654 {
 655 #if ENABLE(C_LOOP)
 656     m_vm-&gt;interpreter-&gt;cloopStack().gatherConservativeRoots(roots, *m_jitStubRoutines, *m_codeBlocks);
 657 #else
 658     UNUSED_PARAM(roots);
 659 #endif
 660 }
 661 
 662 void Heap::gatherScratchBufferRoots(ConservativeRoots&amp; roots)
 663 {
 664 #if ENABLE(DFG_JIT)
 665     if (!VM::canUseJIT())
 666         return;
 667     m_vm-&gt;gatherScratchBufferRoots(roots);
 668 #else
 669     UNUSED_PARAM(roots);
 670 #endif
 671 }
 672 
 673 void Heap::beginMarking()
 674 {
 675     TimingScope timingScope(*this, &quot;Heap::beginMarking&quot;);
 676     m_jitStubRoutines-&gt;clearMarks();
 677     m_objectSpace.beginMarking();
 678     setMutatorShouldBeFenced(true);
 679 }
 680 
 681 void Heap::removeDeadCompilerWorklistEntries()
 682 {
 683 #if ENABLE(DFG_JIT)
 684     if (!VM::canUseJIT())
 685         return;
 686     for (unsigned i = DFG::numberOfWorklists(); i--;)
 687         DFG::existingWorklistForIndex(i).removeDeadPlans(*m_vm);
 688 #endif
 689 }
 690 
 691 bool Heap::isHeapSnapshotting() const
 692 {
 693     HeapProfiler* heapProfiler = m_vm-&gt;heapProfiler();
 694     if (UNLIKELY(heapProfiler))
 695         return heapProfiler-&gt;activeSnapshotBuilder();
 696     return false;
 697 }
 698 
 699 struct GatherHeapSnapshotData : MarkedBlock::CountFunctor {
 700     GatherHeapSnapshotData(VM&amp; vm, HeapSnapshotBuilder&amp; builder)
 701         : m_vm(vm)
 702         , m_builder(builder)
 703     {
 704     }
 705 
 706     IterationStatus operator()(HeapCell* heapCell, HeapCell::Kind kind) const
 707     {
 708         if (isJSCellKind(kind)) {
 709             JSCell* cell = static_cast&lt;JSCell*&gt;(heapCell);
 710             cell-&gt;methodTable(m_vm)-&gt;heapSnapshot(cell, m_builder);
 711         }
 712         return IterationStatus::Continue;
 713     }
 714 
 715     VM&amp; m_vm;
 716     HeapSnapshotBuilder&amp; m_builder;
 717 };
 718 
 719 void Heap::gatherExtraHeapSnapshotData(HeapProfiler&amp; heapProfiler)
 720 {
 721     if (HeapSnapshotBuilder* builder = heapProfiler.activeSnapshotBuilder()) {
 722         HeapIterationScope heapIterationScope(*this);
 723         GatherHeapSnapshotData functor(*m_vm, *builder);
 724         m_objectSpace.forEachLiveCell(heapIterationScope, functor);
 725     }
 726 }
 727 
 728 struct RemoveDeadHeapSnapshotNodes : MarkedBlock::CountFunctor {
 729     RemoveDeadHeapSnapshotNodes(HeapSnapshot&amp; snapshot)
 730         : m_snapshot(snapshot)
 731     {
 732     }
 733 
 734     IterationStatus operator()(HeapCell* cell, HeapCell::Kind kind) const
 735     {
 736         if (isJSCellKind(kind))
 737             m_snapshot.sweepCell(static_cast&lt;JSCell*&gt;(cell));
 738         return IterationStatus::Continue;
 739     }
 740 
 741     HeapSnapshot&amp; m_snapshot;
 742 };
 743 
 744 void Heap::removeDeadHeapSnapshotNodes(HeapProfiler&amp; heapProfiler)
 745 {
 746     if (HeapSnapshot* snapshot = heapProfiler.mostRecentSnapshot()) {
 747         HeapIterationScope heapIterationScope(*this);
 748         RemoveDeadHeapSnapshotNodes functor(*snapshot);
 749         m_objectSpace.forEachDeadCell(heapIterationScope, functor);
 750         snapshot-&gt;shrinkToFit();
 751     }
 752 }
 753 
 754 void Heap::updateObjectCounts()
 755 {
 756     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full)
 757         m_totalBytesVisited = 0;
 758 
 759     m_totalBytesVisitedThisCycle = bytesVisited();
 760 
 761     m_totalBytesVisited += m_totalBytesVisitedThisCycle;
 762 }
 763 
 764 void Heap::endMarking()
 765 {
 766     forEachSlotVisitor(
 767         [&amp;] (SlotVisitor&amp; visitor) {
 768             visitor.reset();
 769         });
 770 
 771     assertMarkStacksEmpty();
 772 
 773     RELEASE_ASSERT(m_raceMarkStack-&gt;isEmpty());
 774 
 775     m_objectSpace.endMarking();
 776     setMutatorShouldBeFenced(Options::forceFencedBarrier());
 777 }
 778 
 779 size_t Heap::objectCount()
 780 {
 781     return m_objectSpace.objectCount();
 782 }
 783 
 784 size_t Heap::extraMemorySize()
 785 {
 786     // FIXME: Change this to use SaturatedArithmetic when available.
 787     // https://bugs.webkit.org/show_bug.cgi?id=170411
 788     Checked&lt;size_t, RecordOverflow&gt; checkedTotal = m_extraMemorySize;
 789     checkedTotal += m_deprecatedExtraMemorySize;
 790     checkedTotal += m_arrayBuffers.size();
 791     size_t total = UNLIKELY(checkedTotal.hasOverflowed()) ? std::numeric_limits&lt;size_t&gt;::max() : checkedTotal.unsafeGet();
 792 
 793     ASSERT(m_objectSpace.capacity() &gt;= m_objectSpace.size());
 794     return std::min(total, std::numeric_limits&lt;size_t&gt;::max() - m_objectSpace.capacity());
 795 }
 796 
 797 size_t Heap::size()
 798 {
 799     return m_objectSpace.size() + extraMemorySize();
 800 }
 801 
 802 size_t Heap::capacity()
 803 {
 804     return m_objectSpace.capacity() + extraMemorySize();
 805 }
 806 
 807 size_t Heap::protectedGlobalObjectCount()
 808 {
 809     size_t result = 0;
 810     forEachProtectedCell(
 811         [&amp;] (JSCell* cell) {
 812             if (cell-&gt;isObject() &amp;&amp; asObject(cell)-&gt;isGlobalObject())
 813                 result++;
 814         });
 815     return result;
 816 }
 817 
 818 size_t Heap::globalObjectCount()
 819 {
 820     HeapIterationScope iterationScope(*this);
 821     size_t result = 0;
 822     m_objectSpace.forEachLiveCell(
 823         iterationScope,
 824         [&amp;] (HeapCell* heapCell, HeapCell::Kind kind) -&gt; IterationStatus {
 825             if (!isJSCellKind(kind))
 826                 return IterationStatus::Continue;
 827             JSCell* cell = static_cast&lt;JSCell*&gt;(heapCell);
 828             if (cell-&gt;isObject() &amp;&amp; asObject(cell)-&gt;isGlobalObject())
 829                 result++;
 830             return IterationStatus::Continue;
 831         });
 832     return result;
 833 }
 834 
 835 size_t Heap::protectedObjectCount()
 836 {
 837     size_t result = 0;
 838     forEachProtectedCell(
 839         [&amp;] (JSCell*) {
 840             result++;
 841         });
 842     return result;
 843 }
 844 
 845 std::unique_ptr&lt;TypeCountSet&gt; Heap::protectedObjectTypeCounts()
 846 {
 847     std::unique_ptr&lt;TypeCountSet&gt; result = std::make_unique&lt;TypeCountSet&gt;();
 848     forEachProtectedCell(
 849         [&amp;] (JSCell* cell) {
 850             recordType(*vm(), *result, cell);
 851         });
 852     return result;
 853 }
 854 
 855 std::unique_ptr&lt;TypeCountSet&gt; Heap::objectTypeCounts()
 856 {
 857     std::unique_ptr&lt;TypeCountSet&gt; result = std::make_unique&lt;TypeCountSet&gt;();
 858     HeapIterationScope iterationScope(*this);
 859     m_objectSpace.forEachLiveCell(
 860         iterationScope,
 861         [&amp;] (HeapCell* cell, HeapCell::Kind kind) -&gt; IterationStatus {
 862             if (isJSCellKind(kind))
 863                 recordType(*vm(), *result, static_cast&lt;JSCell*&gt;(cell));
 864             return IterationStatus::Continue;
 865         });
 866     return result;
 867 }
 868 
 869 void Heap::deleteAllCodeBlocks(DeleteAllCodeEffort effort)
 870 {
 871     if (m_collectionScope &amp;&amp; effort == DeleteAllCodeIfNotCollecting)
 872         return;
 873 
 874     VM&amp; vm = *m_vm;
 875     PreventCollectionScope preventCollectionScope(*this);
 876 
 877     // If JavaScript is running, it&#39;s not safe to delete all JavaScript code, since
 878     // we&#39;ll end up returning to deleted code.
 879     RELEASE_ASSERT(!vm.entryScope);
 880     RELEASE_ASSERT(!m_collectionScope);
 881 
 882     completeAllJITPlans();
 883 
 884     vm.forEachScriptExecutableSpace(
 885         [&amp;] (auto&amp; spaceAndSet) {
 886             HeapIterationScope heapIterationScope(*this);
 887             auto&amp; set = spaceAndSet.set;
 888             set.forEachLiveCell(
 889                 [&amp;] (HeapCell* cell, HeapCell::Kind) {
 890                     ScriptExecutable* executable = static_cast&lt;ScriptExecutable*&gt;(cell);
 891                     executable-&gt;clearCode(set);
 892                 });
 893         });
 894 
 895 #if ENABLE(WEBASSEMBLY)
 896     {
 897         // We must ensure that we clear the JS call ICs from Wasm. Otherwise, Wasm will
 898         // have no idea that we cleared the code from all of the Executables in the
 899         // VM. This could leave Wasm in an inconsistent state where it has an IC that
 900         // points into a CodeBlock that could be dead. The IC will still succeed because
 901         // it uses a callee check, but then it will call into dead code.
 902         HeapIterationScope heapIterationScope(*this);
 903         if (vm.m_webAssemblyCodeBlockSpace) {
 904             vm.m_webAssemblyCodeBlockSpace-&gt;forEachLiveCell([&amp;] (HeapCell* cell, HeapCell::Kind kind) {
 905                 ASSERT_UNUSED(kind, kind == HeapCell::JSCell);
 906                 JSWebAssemblyCodeBlock* codeBlock = static_cast&lt;JSWebAssemblyCodeBlock*&gt;(cell);
 907                 codeBlock-&gt;clearJSCallICs(vm);
 908             });
 909         }
 910     }
 911 #endif
 912 }
 913 
 914 void Heap::deleteAllUnlinkedCodeBlocks(DeleteAllCodeEffort effort)
 915 {
 916     if (m_collectionScope &amp;&amp; effort == DeleteAllCodeIfNotCollecting)
 917         return;
 918 
 919     VM&amp; vm = *m_vm;
 920     PreventCollectionScope preventCollectionScope(*this);
 921 
 922     RELEASE_ASSERT(!m_collectionScope);
 923 
 924     HeapIterationScope heapIterationScope(*this);
 925     vm.unlinkedFunctionExecutableSpace.set.forEachLiveCell(
 926         [&amp;] (HeapCell* cell, HeapCell::Kind) {
 927             UnlinkedFunctionExecutable* executable = static_cast&lt;UnlinkedFunctionExecutable*&gt;(cell);
 928             executable-&gt;clearCode(vm);
 929         });
 930 }
 931 
 932 void Heap::deleteUnmarkedCompiledCode()
 933 {
 934     vm()-&gt;forEachScriptExecutableSpace([] (auto&amp; space) { space.space.sweep(); });
 935     vm()-&gt;forEachCodeBlockSpace([] (auto&amp; space) { space.space.sweep(); }); // Sweeping must occur before deleting stubs, otherwise the stubs might still think they&#39;re alive as they get deleted.
 936     m_jitStubRoutines-&gt;deleteUnmarkedJettisonedStubRoutines();
 937 }
 938 
 939 void Heap::addToRememberedSet(const JSCell* constCell)
 940 {
 941     JSCell* cell = const_cast&lt;JSCell*&gt;(constCell);
 942     ASSERT(cell);
 943     ASSERT(!Options::useConcurrentJIT() || !isCompilationThread());
 944     m_barriersExecuted++;
 945     if (m_mutatorShouldBeFenced) {
 946         WTF::loadLoadFence();
 947         if (!isMarked(cell)) {
 948             // During a full collection a store into an unmarked object that had surivived past
 949             // collections will manifest as a store to an unmarked PossiblyBlack object. If the
 950             // object gets marked at some time after this then it will go down the normal marking
 951             // path. So, we don&#39;t have to remember this object. We could return here. But we go
 952             // further and attempt to re-white the object.
 953 
 954             RELEASE_ASSERT(m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full);
 955 
 956             if (cell-&gt;atomicCompareExchangeCellStateStrong(CellState::PossiblyBlack, CellState::DefinitelyWhite) == CellState::PossiblyBlack) {
 957                 // Now we protect against this race:
 958                 //
 959                 //     1) Object starts out black + unmarked.
 960                 //     --&gt; We do isMarked here.
 961                 //     2) Object is marked and greyed.
 962                 //     3) Object is scanned and blacked.
 963                 //     --&gt; We do atomicCompareExchangeCellStateStrong here.
 964                 //
 965                 // In this case we would have made the object white again, even though it should
 966                 // be black. This check lets us correct our mistake. This relies on the fact that
 967                 // isMarked converges monotonically to true.
 968                 if (isMarked(cell)) {
 969                     // It&#39;s difficult to work out whether the object should be grey or black at
 970                     // this point. We say black conservatively.
 971                     cell-&gt;setCellState(CellState::PossiblyBlack);
 972                 }
 973 
 974                 // Either way, we can return. Most likely, the object was not marked, and so the
 975                 // object is now labeled white. This means that future barrier executions will not
 976                 // fire. In the unlikely event that the object had become marked, we can still
 977                 // return anyway, since we proved that the object was not marked at the time that
 978                 // we executed this slow path.
 979             }
 980 
 981             return;
 982         }
 983     } else
 984         ASSERT(Heap::isMarked(cell));
 985     // It could be that the object was *just* marked. This means that the collector may set the
 986     // state to DefinitelyGrey and then to PossiblyOldOrBlack at any time. It&#39;s OK for us to
 987     // race with the collector here. If we win then this is accurate because the object _will_
 988     // get scanned again. If we lose then someone else will barrier the object again. That would
 989     // be unfortunate but not the end of the world.
 990     cell-&gt;setCellState(CellState::PossiblyGrey);
 991     m_mutatorMarkStack-&gt;append(cell);
 992 }
 993 
 994 void Heap::sweepSynchronously()
 995 {
 996     MonotonicTime before { };
 997     if (Options::logGC()) {
 998         dataLog(&quot;Full sweep: &quot;, capacity() / 1024, &quot;kb &quot;);
 999         before = MonotonicTime::now();
1000     }
1001     m_objectSpace.sweep();
1002     m_objectSpace.shrink();
1003     if (Options::logGC()) {
1004         MonotonicTime after = MonotonicTime::now();
1005         dataLog(&quot;=&gt; &quot;, capacity() / 1024, &quot;kb, &quot;, (after - before).milliseconds(), &quot;ms&quot;);
1006     }
1007 }
1008 
1009 void Heap::collect(Synchronousness synchronousness, GCRequest request)
1010 {
1011     switch (synchronousness) {
1012     case Async:
1013         collectAsync(request);
1014         return;
1015     case Sync:
1016         collectSync(request);
1017         return;
1018     }
1019     RELEASE_ASSERT_NOT_REACHED();
1020 }
1021 
1022 void Heap::collectNow(Synchronousness synchronousness, GCRequest request)
1023 {
1024     if (validateDFGDoesGC)
1025         RELEASE_ASSERT(expectDoesGC());
1026 
1027     switch (synchronousness) {
1028     case Async: {
1029         collectAsync(request);
1030         stopIfNecessary();
1031         return;
1032     }
1033 
1034     case Sync: {
1035         collectSync(request);
1036 
1037         DeferGCForAWhile deferGC(*this);
1038         if (UNLIKELY(Options::useImmortalObjects()))
1039             sweeper().stopSweeping();
1040 
1041         bool alreadySweptInCollectSync = shouldSweepSynchronously();
1042         if (!alreadySweptInCollectSync) {
1043             if (Options::logGC())
1044                 dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: &quot;);
1045             sweepSynchronously();
1046             if (Options::logGC())
1047                 dataLog(&quot;]\n&quot;);
1048         }
1049         m_objectSpace.assertNoUnswept();
1050 
1051         sweepAllLogicallyEmptyWeakBlocks();
1052         return;
1053     } }
1054     RELEASE_ASSERT_NOT_REACHED();
1055 }
1056 
1057 void Heap::collectAsync(GCRequest request)
1058 {
1059     if (validateDFGDoesGC)
1060         RELEASE_ASSERT(expectDoesGC());
1061 
1062     if (!m_isSafeToCollect)
1063         return;
1064 
1065     bool alreadyRequested = false;
1066     {
1067         LockHolder locker(*m_threadLock);
1068         for (const GCRequest&amp; previousRequest : m_requests) {
1069             if (request.subsumedBy(previousRequest)) {
1070                 alreadyRequested = true;
1071                 break;
1072             }
1073         }
1074     }
1075     if (alreadyRequested)
1076         return;
1077 
1078     requestCollection(request);
1079 }
1080 
1081 void Heap::collectSync(GCRequest request)
1082 {
1083     if (validateDFGDoesGC)
1084         RELEASE_ASSERT(expectDoesGC());
1085 
1086     if (!m_isSafeToCollect)
1087         return;
1088 
1089     waitForCollection(requestCollection(request));
1090 }
1091 
1092 bool Heap::shouldCollectInCollectorThread(const AbstractLocker&amp;)
1093 {
1094     RELEASE_ASSERT(m_requests.isEmpty() == (m_lastServedTicket == m_lastGrantedTicket));
1095     RELEASE_ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
1096 
1097     if (false)
1098         dataLog(&quot;Mutator has the conn = &quot;, !!(m_worldState.load() &amp; mutatorHasConnBit), &quot;\n&quot;);
1099 
1100     return !m_requests.isEmpty() &amp;&amp; !(m_worldState.load() &amp; mutatorHasConnBit);
1101 }
1102 
1103 void Heap::collectInCollectorThread()
1104 {
1105     for (;;) {
1106         RunCurrentPhaseResult result = runCurrentPhase(GCConductor::Collector, nullptr);
1107         switch (result) {
1108         case RunCurrentPhaseResult::Finished:
1109             return;
1110         case RunCurrentPhaseResult::Continue:
1111             break;
1112         case RunCurrentPhaseResult::NeedCurrentThreadState:
1113             RELEASE_ASSERT_NOT_REACHED();
1114             break;
1115         }
1116     }
1117 }
1118 
1119 ALWAYS_INLINE int asInt(CollectorPhase phase)
1120 {
1121     return static_cast&lt;int&gt;(phase);
1122 }
1123 
1124 void Heap::checkConn(GCConductor conn)
1125 {
1126     unsigned worldState = m_worldState.load();
1127     switch (conn) {
1128     case GCConductor::Mutator:
1129         RELEASE_ASSERT(worldState &amp; mutatorHasConnBit, worldState, asInt(m_lastPhase), asInt(m_currentPhase), asInt(m_nextPhase), vm()-&gt;id(), VM::numberOfIDs(), vm()-&gt;isEntered());
1130         return;
1131     case GCConductor::Collector:
1132         RELEASE_ASSERT(!(worldState &amp; mutatorHasConnBit), worldState, asInt(m_lastPhase), asInt(m_currentPhase), asInt(m_nextPhase), vm()-&gt;id(), VM::numberOfIDs(), vm()-&gt;isEntered());
1133         return;
1134     }
1135     RELEASE_ASSERT_NOT_REACHED();
1136 }
1137 
1138 auto Heap::runCurrentPhase(GCConductor conn, CurrentThreadState* currentThreadState) -&gt; RunCurrentPhaseResult
1139 {
1140     checkConn(conn);
1141     m_currentThreadState = currentThreadState;
1142     m_currentThread = &amp;WTF::Thread::current();
1143 
1144     if (conn == GCConductor::Mutator)
1145         sanitizeStackForVM(vm());
1146 
1147     // If the collector transfers the conn to the mutator, it leaves us in between phases.
1148     if (!finishChangingPhase(conn)) {
1149         // A mischevious mutator could repeatedly relinquish the conn back to us. We try to avoid doing
1150         // this, but it&#39;s probably not the end of the world if it did happen.
1151         if (false)
1152             dataLog(&quot;Conn bounce-back.\n&quot;);
1153         return RunCurrentPhaseResult::Finished;
1154     }
1155 
1156     bool result = false;
1157     switch (m_currentPhase) {
1158     case CollectorPhase::NotRunning:
1159         result = runNotRunningPhase(conn);
1160         break;
1161 
1162     case CollectorPhase::Begin:
1163         result = runBeginPhase(conn);
1164         break;
1165 
1166     case CollectorPhase::Fixpoint:
1167         if (!currentThreadState &amp;&amp; conn == GCConductor::Mutator)
1168             return RunCurrentPhaseResult::NeedCurrentThreadState;
1169 
1170         result = runFixpointPhase(conn);
1171         break;
1172 
1173     case CollectorPhase::Concurrent:
1174         result = runConcurrentPhase(conn);
1175         break;
1176 
1177     case CollectorPhase::Reloop:
1178         result = runReloopPhase(conn);
1179         break;
1180 
1181     case CollectorPhase::End:
1182         result = runEndPhase(conn);
1183         break;
1184     }
1185 
1186     return result ? RunCurrentPhaseResult::Continue : RunCurrentPhaseResult::Finished;
1187 }
1188 
1189 NEVER_INLINE bool Heap::runNotRunningPhase(GCConductor conn)
1190 {
1191     // Check m_requests since the mutator calls this to poll what&#39;s going on.
1192     {
1193         auto locker = holdLock(*m_threadLock);
1194         if (m_requests.isEmpty())
1195             return false;
1196     }
1197 
1198     return changePhase(conn, CollectorPhase::Begin);
1199 }
1200 
1201 NEVER_INLINE bool Heap::runBeginPhase(GCConductor conn)
1202 {
1203     m_currentGCStartTime = MonotonicTime::now();
1204 
1205     {
1206         LockHolder locker(*m_threadLock);
1207         RELEASE_ASSERT(!m_requests.isEmpty());
1208         m_currentRequest = m_requests.first();
1209     }
1210 
1211     if (Options::logGC())
1212         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: START &quot;, gcConductorShortName(conn), &quot; &quot;, capacity() / 1024, &quot;kb &quot;);
1213 
1214     m_beforeGC = MonotonicTime::now();
1215 
1216     if (m_collectionScope) {
1217         dataLog(&quot;Collection scope already set during GC: &quot;, *m_collectionScope, &quot;\n&quot;);
1218         RELEASE_ASSERT_NOT_REACHED();
1219     }
1220 
1221     willStartCollection();
1222 
1223     if (UNLIKELY(m_verifier)) {
1224         // Verify that live objects from the last GC cycle haven&#39;t been corrupted by
1225         // mutators before we begin this new GC cycle.
1226         m_verifier-&gt;verify(HeapVerifier::Phase::BeforeGC);
1227 
1228         m_verifier-&gt;startGC();
1229         m_verifier-&gt;gatherLiveCells(HeapVerifier::Phase::BeforeMarking);
1230     }
1231 
1232     prepareForMarking();
1233 
1234     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
1235         m_opaqueRoots.clear();
1236         m_collectorSlotVisitor-&gt;clearMarkStacks();
1237         m_mutatorMarkStack-&gt;clear();
1238     }
1239 
1240     RELEASE_ASSERT(m_raceMarkStack-&gt;isEmpty());
1241 
1242     beginMarking();
1243 
1244     forEachSlotVisitor(
1245         [&amp;] (SlotVisitor&amp; visitor) {
1246             visitor.didStartMarking();
1247         });
1248 
1249     m_parallelMarkersShouldExit = false;
1250 
1251     m_helperClient.setFunction(
1252         [this] () {
1253             SlotVisitor* slotVisitor;
1254             {
1255                 LockHolder locker(m_parallelSlotVisitorLock);
1256                 if (m_availableParallelSlotVisitors.isEmpty()) {
1257                     std::unique_ptr&lt;SlotVisitor&gt; newVisitor = std::make_unique&lt;SlotVisitor&gt;(
1258                         *this, toCString(&quot;P&quot;, m_parallelSlotVisitors.size() + 1));
1259 
1260                     if (Options::optimizeParallelSlotVisitorsForStoppedMutator())
1261                         newVisitor-&gt;optimizeForStoppedMutator();
1262 
1263                     newVisitor-&gt;didStartMarking();
1264 
1265                     slotVisitor = newVisitor.get();
1266                     m_parallelSlotVisitors.append(WTFMove(newVisitor));
1267                 } else
1268                 slotVisitor = m_availableParallelSlotVisitors.takeLast();
1269             }
1270 
1271             WTF::registerGCThread(GCThreadType::Helper);
1272 
1273             {
1274                 ParallelModeEnabler parallelModeEnabler(*slotVisitor);
1275                 slotVisitor-&gt;drainFromShared(SlotVisitor::SlaveDrain);
1276             }
1277 
1278             {
1279                 LockHolder locker(m_parallelSlotVisitorLock);
1280                 m_availableParallelSlotVisitors.append(slotVisitor);
1281             }
1282         });
1283 
1284     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1285 
1286     m_constraintSet-&gt;didStartMarking();
1287 
1288     m_scheduler-&gt;beginCollection();
1289     if (Options::logGC())
1290         m_scheduler-&gt;log();
1291 
1292     // After this, we will almost certainly fall through all of the &quot;slotVisitor.isEmpty()&quot;
1293     // checks because bootstrap would have put things into the visitor. So, we should fall
1294     // through to draining.
1295 
1296     if (!slotVisitor.didReachTermination()) {
1297         dataLog(&quot;Fatal: SlotVisitor should think that GC should terminate before constraint solving, but it does not think this.\n&quot;);
1298         dataLog(&quot;slotVisitor.isEmpty(): &quot;, slotVisitor.isEmpty(), &quot;\n&quot;);
1299         dataLog(&quot;slotVisitor.collectorMarkStack().isEmpty(): &quot;, slotVisitor.collectorMarkStack().isEmpty(), &quot;\n&quot;);
1300         dataLog(&quot;slotVisitor.mutatorMarkStack().isEmpty(): &quot;, slotVisitor.mutatorMarkStack().isEmpty(), &quot;\n&quot;);
1301         dataLog(&quot;m_numberOfActiveParallelMarkers: &quot;, m_numberOfActiveParallelMarkers, &quot;\n&quot;);
1302         dataLog(&quot;m_sharedCollectorMarkStack-&gt;isEmpty(): &quot;, m_sharedCollectorMarkStack-&gt;isEmpty(), &quot;\n&quot;);
1303         dataLog(&quot;m_sharedMutatorMarkStack-&gt;isEmpty(): &quot;, m_sharedMutatorMarkStack-&gt;isEmpty(), &quot;\n&quot;);
1304         dataLog(&quot;slotVisitor.didReachTermination(): &quot;, slotVisitor.didReachTermination(), &quot;\n&quot;);
1305         RELEASE_ASSERT_NOT_REACHED();
1306     }
1307 
1308     return changePhase(conn, CollectorPhase::Fixpoint);
1309 }
1310 
1311 NEVER_INLINE bool Heap::runFixpointPhase(GCConductor conn)
1312 {
1313     RELEASE_ASSERT(conn == GCConductor::Collector || m_currentThreadState);
1314 
1315     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1316 
1317     if (Options::logGC()) {
1318         HashMap&lt;const char*, size_t&gt; visitMap;
1319         forEachSlotVisitor(
1320             [&amp;] (SlotVisitor&amp; slotVisitor) {
1321                 visitMap.add(slotVisitor.codeName(), slotVisitor.bytesVisited() / 1024);
1322             });
1323 
1324         auto perVisitorDump = sortedMapDump(
1325             visitMap,
1326             [] (const char* a, const char* b) -&gt; bool {
1327                 return strcmp(a, b) &lt; 0;
1328             },
1329             &quot;:&quot;, &quot; &quot;);
1330 
1331         dataLog(&quot;v=&quot;, bytesVisited() / 1024, &quot;kb (&quot;, perVisitorDump, &quot;) o=&quot;, m_opaqueRoots.size(), &quot; b=&quot;, m_barriersExecuted, &quot; &quot;);
1332     }
1333 
1334     if (slotVisitor.didReachTermination()) {
1335         m_opaqueRoots.deleteOldTables();
1336 
1337         m_scheduler-&gt;didReachTermination();
1338 
1339         assertMarkStacksEmpty();
1340 
1341         // FIXME: Take m_mutatorDidRun into account when scheduling constraints. Most likely,
1342         // we don&#39;t have to execute root constraints again unless the mutator did run. At a
1343         // minimum, we could use this for work estimates - but it&#39;s probably more than just an
1344         // estimate.
1345         // https://bugs.webkit.org/show_bug.cgi?id=166828
1346 
1347         // Wondering what this does? Look at Heap::addCoreConstraints(). The DOM and others can also
1348         // add their own using Heap::addMarkingConstraint().
1349         bool converged = m_constraintSet-&gt;executeConvergence(slotVisitor);
1350 
1351         // FIXME: The slotVisitor.isEmpty() check is most likely not needed.
1352         // https://bugs.webkit.org/show_bug.cgi?id=180310
1353         if (converged &amp;&amp; slotVisitor.isEmpty()) {
1354             assertMarkStacksEmpty();
1355             return changePhase(conn, CollectorPhase::End);
1356         }
1357 
1358         m_scheduler-&gt;didExecuteConstraints();
1359     }
1360 
1361     if (Options::logGC())
1362         dataLog(slotVisitor.collectorMarkStack().size(), &quot;+&quot;, m_mutatorMarkStack-&gt;size() + slotVisitor.mutatorMarkStack().size(), &quot; &quot;);
1363 
1364     {
1365         ParallelModeEnabler enabler(slotVisitor);
1366         slotVisitor.drainInParallel(m_scheduler-&gt;timeToResume());
1367     }
1368 
1369     m_scheduler-&gt;synchronousDrainingDidStall();
1370 
1371     // This is kinda tricky. The termination check looks at:
1372     //
1373     // - Whether the marking threads are active. If they are not, this means that the marking threads&#39;
1374     //   SlotVisitors are empty.
1375     // - Whether the collector&#39;s slot visitor is empty.
1376     // - Whether the shared mark stacks are empty.
1377     //
1378     // This doesn&#39;t have to check the mutator SlotVisitor because that one becomes empty after every GC
1379     // work increment, so it must be empty now.
1380     if (slotVisitor.didReachTermination())
1381         return true; // This is like relooping to the top if runFixpointPhase().
1382 
1383     if (!m_scheduler-&gt;shouldResume())
1384         return true;
1385 
1386     m_scheduler-&gt;willResume();
1387 
1388     if (Options::logGC()) {
1389         double thisPauseMS = (MonotonicTime::now() - m_stopTime).milliseconds();
1390         dataLog(&quot;p=&quot;, thisPauseMS, &quot;ms (max &quot;, maxPauseMS(thisPauseMS), &quot;)...]\n&quot;);
1391     }
1392 
1393     // Forgive the mutator for its past failures to keep up.
1394     // FIXME: Figure out if moving this to different places results in perf changes.
1395     m_incrementBalance = 0;
1396 
1397     return changePhase(conn, CollectorPhase::Concurrent);
1398 }
1399 
1400 NEVER_INLINE bool Heap::runConcurrentPhase(GCConductor conn)
1401 {
1402     SlotVisitor&amp; slotVisitor = *m_collectorSlotVisitor;
1403 
1404     switch (conn) {
1405     case GCConductor::Mutator: {
1406         // When the mutator has the conn, we poll runConcurrentPhase() on every time someone says
1407         // stopIfNecessary(), so on every allocation slow path. When that happens we poll if it&#39;s time
1408         // to stop and do some work.
1409         if (slotVisitor.didReachTermination()
1410             || m_scheduler-&gt;shouldStop())
1411             return changePhase(conn, CollectorPhase::Reloop);
1412 
1413         // We could be coming from a collector phase that stuffed our SlotVisitor, so make sure we donate
1414         // everything. This is super cheap if the SlotVisitor is already empty.
1415         slotVisitor.donateAll();
1416         return false;
1417     }
1418     case GCConductor::Collector: {
1419         {
1420             ParallelModeEnabler enabler(slotVisitor);
1421             slotVisitor.drainInParallelPassively(m_scheduler-&gt;timeToStop());
1422         }
1423         return changePhase(conn, CollectorPhase::Reloop);
1424     } }
1425 
1426     RELEASE_ASSERT_NOT_REACHED();
1427     return false;
1428 }
1429 
1430 NEVER_INLINE bool Heap::runReloopPhase(GCConductor conn)
1431 {
1432     if (Options::logGC())
1433         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: &quot;, gcConductorShortName(conn), &quot; &quot;);
1434 
1435     m_scheduler-&gt;didStop();
1436 
1437     if (Options::logGC())
1438         m_scheduler-&gt;log();
1439 
1440     return changePhase(conn, CollectorPhase::Fixpoint);
1441 }
1442 
1443 NEVER_INLINE bool Heap::runEndPhase(GCConductor conn)
1444 {
1445     m_scheduler-&gt;endCollection();
1446 
1447     {
1448         auto locker = holdLock(m_markingMutex);
1449         m_parallelMarkersShouldExit = true;
1450         m_markingConditionVariable.notifyAll();
1451     }
1452     m_helperClient.finish();
1453 
1454     iterateExecutingAndCompilingCodeBlocks(
1455         [&amp;] (CodeBlock* codeBlock) {
1456             writeBarrier(codeBlock);
1457         });
1458 
1459     updateObjectCounts();
1460     endMarking();
1461 
1462     if (UNLIKELY(m_verifier)) {
1463         m_verifier-&gt;gatherLiveCells(HeapVerifier::Phase::AfterMarking);
1464         m_verifier-&gt;verify(HeapVerifier::Phase::AfterMarking);
1465     }
1466 
1467     if (vm()-&gt;typeProfiler())
1468         vm()-&gt;typeProfiler()-&gt;invalidateTypeSetCache();
1469 
1470     if (ValueProfile* profile = vm()-&gt;noJITValueProfileSingleton.get())
1471         *profile = ValueProfile(0);
1472 
1473     reapWeakHandles();
1474     pruneStaleEntriesFromWeakGCMaps();
1475     sweepArrayBuffers();
1476     snapshotUnswept();
1477     finalizeUnconditionalFinalizers();
1478     removeDeadCompilerWorklistEntries();
1479     notifyIncrementalSweeper();
1480 
1481     m_codeBlocks-&gt;iterateCurrentlyExecuting(
1482         [&amp;] (CodeBlock* codeBlock) {
1483             writeBarrier(codeBlock);
1484         });
1485     m_codeBlocks-&gt;clearCurrentlyExecuting();
1486 
1487     m_objectSpace.prepareForAllocation();
1488     updateAllocationLimits();
1489 
1490     if (UNLIKELY(m_verifier)) {
1491         m_verifier-&gt;trimDeadCells();
1492         m_verifier-&gt;verify(HeapVerifier::Phase::AfterGC);
1493     }
1494 
1495     didFinishCollection();
1496 
1497     if (m_currentRequest.didFinishEndPhase)
1498         m_currentRequest.didFinishEndPhase-&gt;run();
1499 
1500     if (false) {
1501         dataLog(&quot;Heap state after GC:\n&quot;);
1502         m_objectSpace.dumpBits();
1503     }
1504 
1505     if (Options::logGC()) {
1506         double thisPauseMS = (m_afterGC - m_stopTime).milliseconds();
1507         dataLog(&quot;p=&quot;, thisPauseMS, &quot;ms (max &quot;, maxPauseMS(thisPauseMS), &quot;), cycle &quot;, (m_afterGC - m_beforeGC).milliseconds(), &quot;ms END]\n&quot;);
1508     }
1509 
1510     {
1511         auto locker = holdLock(*m_threadLock);
1512         m_requests.removeFirst();
1513         m_lastServedTicket++;
1514         clearMutatorWaiting();
1515     }
1516     ParkingLot::unparkAll(&amp;m_worldState);
1517 
1518     if (false)
1519         dataLog(&quot;GC END!\n&quot;);
1520 
1521     setNeedFinalize();
1522 
1523     m_lastGCStartTime = m_currentGCStartTime;
1524     m_lastGCEndTime = MonotonicTime::now();
1525     m_totalGCTime += m_lastGCEndTime - m_lastGCStartTime;
1526 
1527     return changePhase(conn, CollectorPhase::NotRunning);
1528 }
1529 
1530 bool Heap::changePhase(GCConductor conn, CollectorPhase nextPhase)
1531 {
1532     checkConn(conn);
1533 
1534     m_lastPhase = m_currentPhase;
1535     m_nextPhase = nextPhase;
1536 
1537     return finishChangingPhase(conn);
1538 }
1539 
1540 NEVER_INLINE bool Heap::finishChangingPhase(GCConductor conn)
1541 {
1542     checkConn(conn);
1543 
1544     if (m_nextPhase == m_currentPhase)
1545         return true;
1546 
1547     if (false)
1548         dataLog(conn, &quot;: Going to phase: &quot;, m_nextPhase, &quot; (from &quot;, m_currentPhase, &quot;)\n&quot;);
1549 
1550     m_phaseVersion++;
1551 
1552     bool suspendedBefore = worldShouldBeSuspended(m_currentPhase);
1553     bool suspendedAfter = worldShouldBeSuspended(m_nextPhase);
1554 
1555     if (suspendedBefore != suspendedAfter) {
1556         if (suspendedBefore) {
1557             RELEASE_ASSERT(!suspendedAfter);
1558 
1559             resumeThePeriphery();
1560             if (conn == GCConductor::Collector)
1561                 resumeTheMutator();
1562             else
1563                 handleNeedFinalize();
1564         } else {
1565             RELEASE_ASSERT(!suspendedBefore);
1566             RELEASE_ASSERT(suspendedAfter);
1567 
1568             if (conn == GCConductor::Collector) {
1569                 waitWhileNeedFinalize();
1570                 if (!stopTheMutator()) {
1571                     if (false)
1572                         dataLog(&quot;Returning false.\n&quot;);
1573                     return false;
1574                 }
1575             } else {
1576                 sanitizeStackForVM(m_vm);
1577                 handleNeedFinalize();
1578             }
1579             stopThePeriphery(conn);
1580         }
1581     }
1582 
1583     m_currentPhase = m_nextPhase;
1584     return true;
1585 }
1586 
1587 void Heap::stopThePeriphery(GCConductor conn)
1588 {
1589     if (m_worldIsStopped) {
1590         dataLog(&quot;FATAL: world already stopped.\n&quot;);
1591         RELEASE_ASSERT_NOT_REACHED();
1592     }
1593 
1594     if (m_mutatorDidRun)
1595         m_mutatorExecutionVersion++;
1596 
1597     m_mutatorDidRun = false;
1598 
1599     suspendCompilerThreads();
1600     m_worldIsStopped = true;
1601 
1602     forEachSlotVisitor(
1603         [&amp;] (SlotVisitor&amp; slotVisitor) {
1604             slotVisitor.updateMutatorIsStopped(NoLockingNecessary);
1605         });
1606 
1607 #if ENABLE(JIT)
1608     if (VM::canUseJIT()) {
1609         DeferGCForAWhile awhile(*this);
1610         if (JITWorklist::ensureGlobalWorklist().completeAllForVM(*m_vm)
1611             &amp;&amp; conn == GCConductor::Collector)
1612             setGCDidJIT();
1613     }
1614 #endif // ENABLE(JIT)
1615     UNUSED_PARAM(conn);
1616 
1617     if (auto* shadowChicken = vm()-&gt;shadowChicken())
1618         shadowChicken-&gt;update(*vm(), vm()-&gt;topCallFrame);
1619 
1620     m_structureIDTable.flushOldTables();
1621     m_objectSpace.stopAllocating();
1622 
1623     m_stopTime = MonotonicTime::now();
1624 }
1625 
1626 NEVER_INLINE void Heap::resumeThePeriphery()
1627 {
1628     // Calling resumeAllocating does the Right Thing depending on whether this is the end of a
1629     // collection cycle or this is just a concurrent phase within a collection cycle:
1630     // - At end of collection cycle: it&#39;s a no-op because prepareForAllocation already cleared the
1631     //   last active block.
1632     // - During collection cycle: it reinstates the last active block.
1633     m_objectSpace.resumeAllocating();
1634 
1635     m_barriersExecuted = 0;
1636 
1637     if (!m_worldIsStopped) {
1638         dataLog(&quot;Fatal: collector does not believe that the world is stopped.\n&quot;);
1639         RELEASE_ASSERT_NOT_REACHED();
1640     }
1641     m_worldIsStopped = false;
1642 
1643     // FIXME: This could be vastly improved: we want to grab the locks in the order in which they
1644     // become available. We basically want a lockAny() method that will lock whatever lock is available
1645     // and tell you which one it locked. That would require teaching ParkingLot how to park on multiple
1646     // queues at once, which is totally achievable - it would just require memory allocation, which is
1647     // suboptimal but not a disaster. Alternatively, we could replace the SlotVisitor rightToRun lock
1648     // with a DLG-style handshake mechanism, but that seems not as general.
1649     Vector&lt;SlotVisitor*, 8&gt; slotVisitorsToUpdate;
1650 
1651     forEachSlotVisitor(
1652         [&amp;] (SlotVisitor&amp; slotVisitor) {
1653             slotVisitorsToUpdate.append(&amp;slotVisitor);
1654         });
1655 
1656     for (unsigned countdown = 40; !slotVisitorsToUpdate.isEmpty() &amp;&amp; countdown--;) {
1657         for (unsigned index = 0; index &lt; slotVisitorsToUpdate.size(); ++index) {
1658             SlotVisitor&amp; slotVisitor = *slotVisitorsToUpdate[index];
1659             bool remove = false;
1660             if (slotVisitor.hasAcknowledgedThatTheMutatorIsResumed())
1661                 remove = true;
1662             else if (auto locker = tryHoldLock(slotVisitor.rightToRun())) {
1663                 slotVisitor.updateMutatorIsStopped(locker);
1664                 remove = true;
1665             }
1666             if (remove) {
1667                 slotVisitorsToUpdate[index--] = slotVisitorsToUpdate.last();
1668                 slotVisitorsToUpdate.takeLast();
1669             }
1670         }
1671         WTF::Thread::yield();
1672     }
1673 
1674     for (SlotVisitor* slotVisitor : slotVisitorsToUpdate)
1675         slotVisitor-&gt;updateMutatorIsStopped();
1676 
1677     resumeCompilerThreads();
1678 }
1679 
1680 bool Heap::stopTheMutator()
1681 {
1682     for (;;) {
1683         unsigned oldState = m_worldState.load();
1684         if (oldState &amp; stoppedBit) {
1685             RELEASE_ASSERT(!(oldState &amp; hasAccessBit));
1686             RELEASE_ASSERT(!(oldState &amp; mutatorWaitingBit));
1687             RELEASE_ASSERT(!(oldState &amp; mutatorHasConnBit));
1688             return true;
1689         }
1690 
1691         if (oldState &amp; mutatorHasConnBit) {
1692             RELEASE_ASSERT(!(oldState &amp; hasAccessBit));
1693             RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1694             return false;
1695         }
1696 
1697         if (!(oldState &amp; hasAccessBit)) {
1698             RELEASE_ASSERT(!(oldState &amp; mutatorHasConnBit));
1699             RELEASE_ASSERT(!(oldState &amp; mutatorWaitingBit));
1700             // We can stop the world instantly.
1701             if (m_worldState.compareExchangeWeak(oldState, oldState | stoppedBit))
1702                 return true;
1703             continue;
1704         }
1705 
1706         // Transfer the conn to the mutator and bail.
1707         RELEASE_ASSERT(oldState &amp; hasAccessBit);
1708         RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1709         unsigned newState = (oldState | mutatorHasConnBit) &amp; ~mutatorWaitingBit;
1710         if (m_worldState.compareExchangeWeak(oldState, newState)) {
1711             if (false)
1712                 dataLog(&quot;Handed off the conn.\n&quot;);
1713             m_stopIfNecessaryTimer-&gt;scheduleSoon();
1714             ParkingLot::unparkAll(&amp;m_worldState);
1715             return false;
1716         }
1717     }
1718 }
1719 
1720 NEVER_INLINE void Heap::resumeTheMutator()
1721 {
1722     if (false)
1723         dataLog(&quot;Resuming the mutator.\n&quot;);
1724     for (;;) {
1725         unsigned oldState = m_worldState.load();
1726         if (!!(oldState &amp; hasAccessBit) != !(oldState &amp; stoppedBit)) {
1727             dataLog(&quot;Fatal: hasAccess = &quot;, !!(oldState &amp; hasAccessBit), &quot;, stopped = &quot;, !!(oldState &amp; stoppedBit), &quot;\n&quot;);
1728             RELEASE_ASSERT_NOT_REACHED();
1729         }
1730         if (oldState &amp; mutatorHasConnBit) {
1731             dataLog(&quot;Fatal: mutator has the conn.\n&quot;);
1732             RELEASE_ASSERT_NOT_REACHED();
1733         }
1734 
1735         if (!(oldState &amp; stoppedBit)) {
1736             if (false)
1737                 dataLog(&quot;Returning because not stopped.\n&quot;);
1738             return;
1739         }
1740 
1741         if (m_worldState.compareExchangeWeak(oldState, oldState &amp; ~stoppedBit)) {
1742             if (false)
1743                 dataLog(&quot;CASing and returning.\n&quot;);
1744             ParkingLot::unparkAll(&amp;m_worldState);
1745             return;
1746         }
1747     }
1748 }
1749 
1750 void Heap::stopIfNecessarySlow()
1751 {
1752     if (validateDFGDoesGC)
1753         RELEASE_ASSERT(expectDoesGC());
1754 
1755     while (stopIfNecessarySlow(m_worldState.load())) { }
1756 
1757     RELEASE_ASSERT(m_worldState.load() &amp; hasAccessBit);
1758     RELEASE_ASSERT(!(m_worldState.load() &amp; stoppedBit));
1759 
1760     handleGCDidJIT();
1761     handleNeedFinalize();
1762     m_mutatorDidRun = true;
1763 }
1764 
1765 bool Heap::stopIfNecessarySlow(unsigned oldState)
1766 {
1767     if (validateDFGDoesGC)
1768         RELEASE_ASSERT(expectDoesGC());
1769 
1770     RELEASE_ASSERT(oldState &amp; hasAccessBit);
1771     RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1772 
1773     // It&#39;s possible for us to wake up with finalization already requested but the world not yet
1774     // resumed. If that happens, we can&#39;t run finalization yet.
1775     if (handleNeedFinalize(oldState))
1776         return true;
1777 
1778     // FIXME: When entering the concurrent phase, we could arrange for this branch not to fire, and then
1779     // have the SlotVisitor do things to the m_worldState to make this branch fire again. That would
1780     // prevent us from polling this so much. Ideally, stopIfNecessary would ignore the mutatorHasConnBit
1781     // and there would be some other bit indicating whether we were in some GC phase other than the
1782     // NotRunning or Concurrent ones.
1783     if (oldState &amp; mutatorHasConnBit)
1784         collectInMutatorThread();
1785 
1786     return false;
1787 }
1788 
1789 NEVER_INLINE void Heap::collectInMutatorThread()
1790 {
1791     CollectingScope collectingScope(*this);
1792     for (;;) {
1793         RunCurrentPhaseResult result = runCurrentPhase(GCConductor::Mutator, nullptr);
1794         switch (result) {
1795         case RunCurrentPhaseResult::Finished:
1796             return;
1797         case RunCurrentPhaseResult::Continue:
1798             break;
1799         case RunCurrentPhaseResult::NeedCurrentThreadState:
1800             sanitizeStackForVM(m_vm);
1801             auto lambda = [&amp;] (CurrentThreadState&amp; state) {
1802                 for (;;) {
1803                     RunCurrentPhaseResult result = runCurrentPhase(GCConductor::Mutator, &amp;state);
1804                     switch (result) {
1805                     case RunCurrentPhaseResult::Finished:
1806                         return;
1807                     case RunCurrentPhaseResult::Continue:
1808                         break;
1809                     case RunCurrentPhaseResult::NeedCurrentThreadState:
1810                         RELEASE_ASSERT_NOT_REACHED();
1811                         break;
1812                     }
1813                 }
1814             };
1815             callWithCurrentThreadState(scopedLambda&lt;void(CurrentThreadState&amp;)&gt;(WTFMove(lambda)));
1816             return;
1817         }
1818     }
1819 }
1820 
1821 template&lt;typename Func&gt;
1822 void Heap::waitForCollector(const Func&amp; func)
1823 {
1824     for (;;) {
1825         bool done;
1826         {
1827             LockHolder locker(*m_threadLock);
1828             done = func(locker);
1829             if (!done) {
1830                 setMutatorWaiting();
1831 
1832                 // At this point, the collector knows that we intend to wait, and he will clear the
1833                 // waiting bit and then unparkAll when the GC cycle finishes. Clearing the bit
1834                 // prevents us from parking except if there is also stop-the-world. Unparking after
1835                 // clearing means that if the clearing happens after we park, then we will unpark.
1836             }
1837         }
1838 
1839         // If we&#39;re in a stop-the-world scenario, we need to wait for that even if done is true.
1840         unsigned oldState = m_worldState.load();
1841         if (stopIfNecessarySlow(oldState))
1842             continue;
1843 
1844         // FIXME: We wouldn&#39;t need this if stopIfNecessarySlow() had a mode where it knew to just
1845         // do the collection.
1846         relinquishConn();
1847 
1848         if (done) {
1849             clearMutatorWaiting(); // Clean up just in case.
1850             return;
1851         }
1852 
1853         // If mutatorWaitingBit is still set then we want to wait.
1854         ParkingLot::compareAndPark(&amp;m_worldState, oldState | mutatorWaitingBit);
1855     }
1856 }
1857 
1858 void Heap::acquireAccessSlow()
1859 {
1860     for (;;) {
1861         unsigned oldState = m_worldState.load();
1862         RELEASE_ASSERT(!(oldState &amp; hasAccessBit));
1863 
1864         if (oldState &amp; stoppedBit) {
1865             if (verboseStop) {
1866                 dataLog(&quot;Stopping in acquireAccess!\n&quot;);
1867                 WTFReportBacktrace();
1868             }
1869             // Wait until we&#39;re not stopped anymore.
1870             ParkingLot::compareAndPark(&amp;m_worldState, oldState);
1871             continue;
1872         }
1873 
1874         RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1875         unsigned newState = oldState | hasAccessBit;
1876         if (m_worldState.compareExchangeWeak(oldState, newState)) {
1877             handleGCDidJIT();
1878             handleNeedFinalize();
1879             m_mutatorDidRun = true;
1880             stopIfNecessary();
1881             return;
1882         }
1883     }
1884 }
1885 
1886 void Heap::releaseAccessSlow()
1887 {
1888     for (;;) {
1889         unsigned oldState = m_worldState.load();
1890         if (!(oldState &amp; hasAccessBit)) {
1891             dataLog(&quot;FATAL: Attempting to release access but the mutator does not have access.\n&quot;);
1892             RELEASE_ASSERT_NOT_REACHED();
1893         }
1894         if (oldState &amp; stoppedBit) {
1895             dataLog(&quot;FATAL: Attempting to release access but the mutator is stopped.\n&quot;);
1896             RELEASE_ASSERT_NOT_REACHED();
1897         }
1898 
1899         if (handleNeedFinalize(oldState))
1900             continue;
1901 
1902         unsigned newState = oldState &amp; ~(hasAccessBit | mutatorHasConnBit);
1903 
1904         if ((oldState &amp; mutatorHasConnBit)
1905             &amp;&amp; m_nextPhase != m_currentPhase) {
1906             // This means that the collector thread had given us the conn so that we would do something
1907             // for it. Stop ourselves as we release access. This ensures that acquireAccess blocks. In
1908             // the meantime, since we&#39;re handing the conn over, the collector will be awoken and it is
1909             // sure to have work to do.
1910             newState |= stoppedBit;
1911         }
1912 
1913         if (m_worldState.compareExchangeWeak(oldState, newState)) {
1914             if (oldState &amp; mutatorHasConnBit)
1915                 finishRelinquishingConn();
1916             return;
1917         }
1918     }
1919 }
1920 
1921 bool Heap::relinquishConn(unsigned oldState)
1922 {
1923     RELEASE_ASSERT(oldState &amp; hasAccessBit);
1924     RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1925 
1926     if (!(oldState &amp; mutatorHasConnBit))
1927         return false; // Done.
1928 
1929     if (m_threadShouldStop)
1930         return false;
1931 
1932     if (!m_worldState.compareExchangeWeak(oldState, oldState &amp; ~mutatorHasConnBit))
1933         return true; // Loop around.
1934 
1935     finishRelinquishingConn();
1936     return true;
1937 }
1938 
1939 void Heap::finishRelinquishingConn()
1940 {
1941     if (false)
1942         dataLog(&quot;Relinquished the conn.\n&quot;);
1943 
1944     sanitizeStackForVM(m_vm);
1945 
1946     auto locker = holdLock(*m_threadLock);
1947     if (!m_requests.isEmpty())
1948         m_threadCondition-&gt;notifyOne(locker);
1949     ParkingLot::unparkAll(&amp;m_worldState);
1950 }
1951 
1952 void Heap::relinquishConn()
1953 {
1954     while (relinquishConn(m_worldState.load())) { }
1955 }
1956 
1957 bool Heap::handleGCDidJIT(unsigned oldState)
1958 {
1959     RELEASE_ASSERT(oldState &amp; hasAccessBit);
1960     if (!(oldState &amp; gcDidJITBit))
1961         return false;
1962     if (m_worldState.compareExchangeWeak(oldState, oldState &amp; ~gcDidJITBit)) {
1963         WTF::crossModifyingCodeFence();
1964         return true;
1965     }
1966     return true;
1967 }
1968 
1969 NEVER_INLINE bool Heap::handleNeedFinalize(unsigned oldState)
1970 {
1971     RELEASE_ASSERT(oldState &amp; hasAccessBit);
1972     RELEASE_ASSERT(!(oldState &amp; stoppedBit));
1973 
1974     if (!(oldState &amp; needFinalizeBit))
1975         return false;
1976     if (m_worldState.compareExchangeWeak(oldState, oldState &amp; ~needFinalizeBit)) {
1977         finalize();
1978         // Wake up anyone waiting for us to finalize. Note that they may have woken up already, in
1979         // which case they would be waiting for us to release heap access.
1980         ParkingLot::unparkAll(&amp;m_worldState);
1981         return true;
1982     }
1983     return true;
1984 }
1985 
1986 void Heap::handleGCDidJIT()
1987 {
1988     while (handleGCDidJIT(m_worldState.load())) { }
1989 }
1990 
1991 void Heap::handleNeedFinalize()
1992 {
1993     while (handleNeedFinalize(m_worldState.load())) { }
1994 }
1995 
1996 void Heap::setGCDidJIT()
1997 {
1998     m_worldState.transaction(
1999         [&amp;] (unsigned&amp; state) -&gt; bool {
2000             RELEASE_ASSERT(state &amp; stoppedBit);
2001             state |= gcDidJITBit;
2002             return true;
2003         });
2004 }
2005 
2006 void Heap::setNeedFinalize()
2007 {
2008     m_worldState.exchangeOr(needFinalizeBit);
2009     ParkingLot::unparkAll(&amp;m_worldState);
2010     m_stopIfNecessaryTimer-&gt;scheduleSoon();
2011 }
2012 
2013 void Heap::waitWhileNeedFinalize()
2014 {
2015     for (;;) {
2016         unsigned oldState = m_worldState.load();
2017         if (!(oldState &amp; needFinalizeBit)) {
2018             // This means that either there was no finalize request or the main thread will finalize
2019             // with heap access, so a subsequent call to stopTheWorld() will return only when
2020             // finalize finishes.
2021             return;
2022         }
2023         ParkingLot::compareAndPark(&amp;m_worldState, oldState);
2024     }
2025 }
2026 
2027 void Heap::setMutatorWaiting()
2028 {
2029     m_worldState.exchangeOr(mutatorWaitingBit);
2030 }
2031 
2032 void Heap::clearMutatorWaiting()
2033 {
2034     m_worldState.exchangeAnd(~mutatorWaitingBit);
2035 }
2036 
2037 void Heap::notifyThreadStopping(const AbstractLocker&amp;)
2038 {
2039     m_threadIsStopping = true;
2040     clearMutatorWaiting();
2041     ParkingLot::unparkAll(&amp;m_worldState);
2042 }
2043 
2044 void Heap::finalize()
2045 {
2046     MonotonicTime before;
2047     if (Options::logGC()) {
2048         before = MonotonicTime::now();
2049         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: finalize &quot;);
2050     }
2051 
2052     {
2053         SweepingScope sweepingScope(*this);
2054         deleteUnmarkedCompiledCode();
2055         deleteSourceProviderCaches();
2056         sweepInFinalize();
2057     }
2058 
2059     if (HasOwnPropertyCache* cache = vm()-&gt;hasOwnPropertyCache())
2060         cache-&gt;clear();
2061 
2062     immutableButterflyToStringCache.clear();
2063 
2064     for (const HeapFinalizerCallback&amp; callback : m_heapFinalizerCallbacks)
2065         callback.run(*vm());
2066 
2067     if (shouldSweepSynchronously())
2068         sweepSynchronously();
2069 
2070     if (Options::logGC()) {
2071         MonotonicTime after = MonotonicTime::now();
2072         dataLog((after - before).milliseconds(), &quot;ms]\n&quot;);
2073     }
2074 }
2075 
2076 Heap::Ticket Heap::requestCollection(GCRequest request)
2077 {
2078     stopIfNecessary();
2079 
2080     ASSERT(vm()-&gt;currentThreadIsHoldingAPILock());
2081     RELEASE_ASSERT(vm()-&gt;atomicStringTable() == WTF::Thread::current().atomicStringTable());
2082 
2083     LockHolder locker(*m_threadLock);
2084     // We may be able to steal the conn. That only works if the collector is definitely not running
2085     // right now. This is an optimization that prevents the collector thread from ever starting in most
2086     // cases.
2087     ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
2088     if ((m_lastServedTicket == m_lastGrantedTicket) &amp;&amp; (m_currentPhase == CollectorPhase::NotRunning)) {
2089         if (false)
2090             dataLog(&quot;Taking the conn.\n&quot;);
2091         m_worldState.exchangeOr(mutatorHasConnBit);
2092     }
2093 
2094     m_requests.append(request);
2095     m_lastGrantedTicket++;
2096     if (!(m_worldState.load() &amp; mutatorHasConnBit))
2097         m_threadCondition-&gt;notifyOne(locker);
2098     return m_lastGrantedTicket;
2099 }
2100 
2101 void Heap::waitForCollection(Ticket ticket)
2102 {
2103     waitForCollector(
2104         [&amp;] (const AbstractLocker&amp;) -&gt; bool {
2105             return m_lastServedTicket &gt;= ticket;
2106         });
2107 }
2108 
2109 void Heap::sweepInFinalize()
2110 {
2111     m_objectSpace.sweepLargeAllocations();
2112     vm()-&gt;eagerlySweptDestructibleObjectSpace.sweep();
2113 }
2114 
2115 void Heap::suspendCompilerThreads()
2116 {
2117 #if ENABLE(DFG_JIT)
2118     // We ensure the worklists so that it&#39;s not possible for the mutator to start a new worklist
2119     // after we have suspended the ones that he had started before. That&#39;s not very expensive since
2120     // the worklists use AutomaticThreads anyway.
2121     if (!VM::canUseJIT())
2122         return;
2123     for (unsigned i = DFG::numberOfWorklists(); i--;)
2124         DFG::ensureWorklistForIndex(i).suspendAllThreads();
2125 #endif
2126 }
2127 
2128 void Heap::willStartCollection()
2129 {
2130     if (Options::logGC())
2131         dataLog(&quot;=&gt; &quot;);
2132 
2133     if (shouldDoFullCollection()) {
2134         m_collectionScope = CollectionScope::Full;
2135         m_shouldDoFullCollection = false;
2136         if (Options::logGC())
2137             dataLog(&quot;FullCollection, &quot;);
2138         if (false)
2139             dataLog(&quot;Full collection!\n&quot;);
2140     } else {
2141         m_collectionScope = CollectionScope::Eden;
2142         if (Options::logGC())
2143             dataLog(&quot;EdenCollection, &quot;);
2144         if (false)
2145             dataLog(&quot;Eden collection!\n&quot;);
2146     }
2147     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2148         m_sizeBeforeLastFullCollect = m_sizeAfterLastCollect + m_bytesAllocatedThisCycle;
2149         m_extraMemorySize = 0;
2150         m_deprecatedExtraMemorySize = 0;
2151 #if ENABLE(RESOURCE_USAGE)
2152         m_externalMemorySize = 0;
2153 #endif
2154 
2155         if (m_fullActivityCallback)
2156             m_fullActivityCallback-&gt;willCollect();
2157     } else {
2158         ASSERT(m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Eden);
2159         m_sizeBeforeLastEdenCollect = m_sizeAfterLastCollect + m_bytesAllocatedThisCycle;
2160     }
2161 
2162     if (m_edenActivityCallback)
2163         m_edenActivityCallback-&gt;willCollect();
2164 
2165     for (auto* observer : m_observers)
2166         observer-&gt;willGarbageCollect();
2167 }
2168 
2169 void Heap::prepareForMarking()
2170 {
2171     m_objectSpace.prepareForMarking();
2172 }
2173 
2174 void Heap::reapWeakHandles()
2175 {
2176     m_objectSpace.reapWeakSets();
2177 }
2178 
2179 void Heap::pruneStaleEntriesFromWeakGCMaps()
2180 {
2181     if (!m_collectionScope || m_collectionScope.value() != CollectionScope::Full)
2182         return;
2183     for (WeakGCMapBase* weakGCMap : m_weakGCMaps)
2184         weakGCMap-&gt;pruneStaleEntries();
2185 }
2186 
2187 void Heap::sweepArrayBuffers()
2188 {
2189     m_arrayBuffers.sweep();
2190 }
2191 
2192 void Heap::snapshotUnswept()
2193 {
2194     TimingScope timingScope(*this, &quot;Heap::snapshotUnswept&quot;);
2195     m_objectSpace.snapshotUnswept();
2196 }
2197 
2198 void Heap::deleteSourceProviderCaches()
2199 {
2200     if (m_lastCollectionScope &amp;&amp; m_lastCollectionScope.value() == CollectionScope::Full)
2201         m_vm-&gt;clearSourceProviderCaches();
2202 }
2203 
2204 void Heap::notifyIncrementalSweeper()
2205 {
2206     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2207         if (!m_logicallyEmptyWeakBlocks.isEmpty())
2208             m_indexOfNextLogicallyEmptyWeakBlockToSweep = 0;
2209     }
2210 
2211     m_sweeper-&gt;startSweeping(*this);
2212 }
2213 
2214 void Heap::updateAllocationLimits()
2215 {
2216     static const bool verbose = false;
2217 
2218     if (verbose) {
2219         dataLog(&quot;\n&quot;);
2220         dataLog(&quot;bytesAllocatedThisCycle = &quot;, m_bytesAllocatedThisCycle, &quot;\n&quot;);
2221     }
2222 
2223     // Calculate our current heap size threshold for the purpose of figuring out when we should
2224     // run another collection. This isn&#39;t the same as either size() or capacity(), though it should
2225     // be somewhere between the two. The key is to match the size calculations involved calls to
2226     // didAllocate(), while never dangerously underestimating capacity(). In extreme cases of
2227     // fragmentation, we may have size() much smaller than capacity().
2228     size_t currentHeapSize = 0;
2229 
2230     // For marked space, we use the total number of bytes visited. This matches the logic for
2231     // BlockDirectory&#39;s calls to didAllocate(), which effectively accounts for the total size of
2232     // objects allocated rather than blocks used. This will underestimate capacity(), and in case
2233     // of fragmentation, this may be substantial. Fortunately, marked space rarely fragments because
2234     // cells usually have a narrow range of sizes. So, the underestimation is probably OK.
2235     currentHeapSize += m_totalBytesVisited;
2236     if (verbose)
2237         dataLog(&quot;totalBytesVisited = &quot;, m_totalBytesVisited, &quot;, currentHeapSize = &quot;, currentHeapSize, &quot;\n&quot;);
2238 
2239     // It&#39;s up to the user to ensure that extraMemorySize() ends up corresponding to allocation-time
2240     // extra memory reporting.
2241     currentHeapSize += extraMemorySize();
2242     if (!ASSERT_DISABLED) {
2243         Checked&lt;size_t, RecordOverflow&gt; checkedCurrentHeapSize = m_totalBytesVisited;
2244         checkedCurrentHeapSize += extraMemorySize();
2245         ASSERT(!checkedCurrentHeapSize.hasOverflowed() &amp;&amp; checkedCurrentHeapSize.unsafeGet() == currentHeapSize);
2246     }
2247 
2248     if (verbose)
2249         dataLog(&quot;extraMemorySize() = &quot;, extraMemorySize(), &quot;, currentHeapSize = &quot;, currentHeapSize, &quot;\n&quot;);
2250 
2251     if (m_collectionScope &amp;&amp; m_collectionScope.value() == CollectionScope::Full) {
2252         // To avoid pathological GC churn in very small and very large heaps, we set
2253         // the new allocation limit based on the current size of the heap, with a
2254         // fixed minimum.
2255         m_maxHeapSize = std::max(minHeapSize(m_heapType, m_ramSize), proportionalHeapSize(currentHeapSize, m_ramSize));
2256         if (verbose)
2257             dataLog(&quot;Full: maxHeapSize = &quot;, m_maxHeapSize, &quot;\n&quot;);
2258         m_maxEdenSize = m_maxHeapSize - currentHeapSize;
2259         if (verbose)
2260             dataLog(&quot;Full: maxEdenSize = &quot;, m_maxEdenSize, &quot;\n&quot;);
2261         m_sizeAfterLastFullCollect = currentHeapSize;
2262         if (verbose)
2263             dataLog(&quot;Full: sizeAfterLastFullCollect = &quot;, currentHeapSize, &quot;\n&quot;);
2264         m_bytesAbandonedSinceLastFullCollect = 0;
2265         if (verbose)
2266             dataLog(&quot;Full: bytesAbandonedSinceLastFullCollect = &quot;, 0, &quot;\n&quot;);
2267     } else {
2268         ASSERT(currentHeapSize &gt;= m_sizeAfterLastCollect);
2269         // Theoretically, we shouldn&#39;t ever scan more memory than the heap size we planned to have.
2270         // But we are sloppy, so we have to defend against the overflow.
2271         m_maxEdenSize = currentHeapSize &gt; m_maxHeapSize ? 0 : m_maxHeapSize - currentHeapSize;
2272         if (verbose)
2273             dataLog(&quot;Eden: maxEdenSize = &quot;, m_maxEdenSize, &quot;\n&quot;);
2274         m_sizeAfterLastEdenCollect = currentHeapSize;
2275         if (verbose)
2276             dataLog(&quot;Eden: sizeAfterLastEdenCollect = &quot;, currentHeapSize, &quot;\n&quot;);
2277         double edenToOldGenerationRatio = (double)m_maxEdenSize / (double)m_maxHeapSize;
2278         double minEdenToOldGenerationRatio = 1.0 / 3.0;
2279         if (edenToOldGenerationRatio &lt; minEdenToOldGenerationRatio)
2280             m_shouldDoFullCollection = true;
2281         // This seems suspect at first, but what it does is ensure that the nursery size is fixed.
2282         m_maxHeapSize += currentHeapSize - m_sizeAfterLastCollect;
2283         if (verbose)
2284             dataLog(&quot;Eden: maxHeapSize = &quot;, m_maxHeapSize, &quot;\n&quot;);
2285         m_maxEdenSize = m_maxHeapSize - currentHeapSize;
2286         if (verbose)
2287             dataLog(&quot;Eden: maxEdenSize = &quot;, m_maxEdenSize, &quot;\n&quot;);
2288         if (m_fullActivityCallback) {
2289             ASSERT(currentHeapSize &gt;= m_sizeAfterLastFullCollect);
2290             m_fullActivityCallback-&gt;didAllocate(*this, currentHeapSize - m_sizeAfterLastFullCollect);
2291         }
2292     }
2293 
2294 #if PLATFORM(IOS_FAMILY)
2295     // Get critical memory threshold for next cycle.
2296     overCriticalMemoryThreshold(MemoryThresholdCallType::Direct);
2297 #endif
2298 
2299     m_sizeAfterLastCollect = currentHeapSize;
2300     if (verbose)
2301         dataLog(&quot;sizeAfterLastCollect = &quot;, m_sizeAfterLastCollect, &quot;\n&quot;);
2302     m_bytesAllocatedThisCycle = 0;
2303 
2304     if (Options::logGC())
2305         dataLog(&quot;=&gt; &quot;, currentHeapSize / 1024, &quot;kb, &quot;);
2306 }
2307 
2308 void Heap::didFinishCollection()
2309 {
2310     m_afterGC = MonotonicTime::now();
2311     CollectionScope scope = *m_collectionScope;
2312     if (scope == CollectionScope::Full)
2313         m_lastFullGCLength = m_afterGC - m_beforeGC;
2314     else
2315         m_lastEdenGCLength = m_afterGC - m_beforeGC;
2316 
2317 #if ENABLE(RESOURCE_USAGE)
2318     ASSERT(externalMemorySize() &lt;= extraMemorySize());
2319 #endif
2320 
2321     if (HeapProfiler* heapProfiler = m_vm-&gt;heapProfiler()) {
2322         gatherExtraHeapSnapshotData(*heapProfiler);
2323         removeDeadHeapSnapshotNodes(*heapProfiler);
2324     }
2325 
2326     if (UNLIKELY(m_verifier))
2327         m_verifier-&gt;endGC();
2328 
2329     RELEASE_ASSERT(m_collectionScope);
2330     m_lastCollectionScope = m_collectionScope;
2331     m_collectionScope = WTF::nullopt;
2332 
2333     for (auto* observer : m_observers)
2334         observer-&gt;didGarbageCollect(scope);
2335 }
2336 
2337 void Heap::resumeCompilerThreads()
2338 {
2339 #if ENABLE(DFG_JIT)
2340     if (!VM::canUseJIT())
2341         return;
2342     for (unsigned i = DFG::numberOfWorklists(); i--;)
2343         DFG::existingWorklistForIndex(i).resumeAllThreads();
2344 #endif
2345 }
2346 
2347 GCActivityCallback* Heap::fullActivityCallback()
2348 {
2349     return m_fullActivityCallback.get();
2350 }
2351 
2352 GCActivityCallback* Heap::edenActivityCallback()
2353 {
2354     return m_edenActivityCallback.get();
2355 }
2356 
2357 IncrementalSweeper&amp; Heap::sweeper()
2358 {
2359     return m_sweeper.get();
2360 }
2361 
2362 void Heap::setGarbageCollectionTimerEnabled(bool enable)
2363 {
2364     if (m_fullActivityCallback)
2365         m_fullActivityCallback-&gt;setEnabled(enable);
2366     if (m_edenActivityCallback)
2367         m_edenActivityCallback-&gt;setEnabled(enable);
2368 }
2369 
2370 void Heap::didAllocate(size_t bytes)
2371 {
2372     if (m_edenActivityCallback)
2373         m_edenActivityCallback-&gt;didAllocate(*this, m_bytesAllocatedThisCycle + m_bytesAbandonedSinceLastFullCollect);
2374     m_bytesAllocatedThisCycle += bytes;
2375     performIncrement(bytes);
2376 }
2377 
2378 bool Heap::isValidAllocation(size_t)
2379 {
2380     if (!isValidThreadState(m_vm))
2381         return false;
2382 
2383     if (isCurrentThreadBusy())
2384         return false;
2385 
2386     return true;
2387 }
2388 
2389 void Heap::addFinalizer(JSCell* cell, Finalizer finalizer)
2390 {
2391     WeakSet::allocate(cell, &amp;m_finalizerOwner, reinterpret_cast&lt;void*&gt;(finalizer)); // Balanced by FinalizerOwner::finalize().
2392 }
2393 
2394 void Heap::FinalizerOwner::finalize(Handle&lt;Unknown&gt; handle, void* context)
2395 {
2396     HandleSlot slot = handle.slot();
2397     Finalizer finalizer = reinterpret_cast&lt;Finalizer&gt;(context);
2398     finalizer(slot-&gt;asCell());
2399     WeakSet::deallocate(WeakImpl::asWeakImpl(slot));
2400 }
2401 
2402 void Heap::collectNowFullIfNotDoneRecently(Synchronousness synchronousness)
2403 {
2404     if (!m_fullActivityCallback) {
2405         collectNow(synchronousness, CollectionScope::Full);
2406         return;
2407     }
2408 
2409     if (m_fullActivityCallback-&gt;didGCRecently()) {
2410         // A synchronous GC was already requested recently so we merely accelerate next collection.
2411         reportAbandonedObjectGraph();
2412         return;
2413     }
2414 
2415     m_fullActivityCallback-&gt;setDidGCRecently();
2416     collectNow(synchronousness, CollectionScope::Full);
2417 }
2418 
2419 bool Heap::useGenerationalGC()
2420 {
2421     return Options::useGenerationalGC() &amp;&amp; !VM::isInMiniMode();
2422 }
2423 
2424 bool Heap::shouldSweepSynchronously()
2425 {
2426     return Options::sweepSynchronously() || VM::isInMiniMode();
2427 }
2428 
2429 bool Heap::shouldDoFullCollection()
2430 {
2431     if (!useGenerationalGC())
2432         return true;
2433 
2434     if (!m_currentRequest.scope)
2435         return m_shouldDoFullCollection || overCriticalMemoryThreshold();
2436     return *m_currentRequest.scope == CollectionScope::Full;
2437 }
2438 
2439 void Heap::addLogicallyEmptyWeakBlock(WeakBlock* block)
2440 {
2441     m_logicallyEmptyWeakBlocks.append(block);
2442 }
2443 
2444 void Heap::sweepAllLogicallyEmptyWeakBlocks()
2445 {
2446     if (m_logicallyEmptyWeakBlocks.isEmpty())
2447         return;
2448 
2449     m_indexOfNextLogicallyEmptyWeakBlockToSweep = 0;
2450     while (sweepNextLogicallyEmptyWeakBlock()) { }
2451 }
2452 
2453 bool Heap::sweepNextLogicallyEmptyWeakBlock()
2454 {
2455     if (m_indexOfNextLogicallyEmptyWeakBlockToSweep == WTF::notFound)
2456         return false;
2457 
2458     WeakBlock* block = m_logicallyEmptyWeakBlocks[m_indexOfNextLogicallyEmptyWeakBlockToSweep];
2459 
2460     block-&gt;sweep();
2461     if (block-&gt;isEmpty()) {
2462         std::swap(m_logicallyEmptyWeakBlocks[m_indexOfNextLogicallyEmptyWeakBlockToSweep], m_logicallyEmptyWeakBlocks.last());
2463         m_logicallyEmptyWeakBlocks.removeLast();
2464         WeakBlock::destroy(*this, block);
2465     } else
2466         m_indexOfNextLogicallyEmptyWeakBlockToSweep++;
2467 
2468     if (m_indexOfNextLogicallyEmptyWeakBlockToSweep &gt;= m_logicallyEmptyWeakBlocks.size()) {
2469         m_indexOfNextLogicallyEmptyWeakBlockToSweep = WTF::notFound;
2470         return false;
2471     }
2472 
2473     return true;
2474 }
2475 
2476 size_t Heap::visitCount()
2477 {
2478     size_t result = 0;
2479     forEachSlotVisitor(
2480         [&amp;] (SlotVisitor&amp; visitor) {
2481             result += visitor.visitCount();
2482         });
2483     return result;
2484 }
2485 
2486 size_t Heap::bytesVisited()
2487 {
2488     size_t result = 0;
2489     forEachSlotVisitor(
2490         [&amp;] (SlotVisitor&amp; visitor) {
2491             result += visitor.bytesVisited();
2492         });
2493     return result;
2494 }
2495 
2496 void Heap::forEachCodeBlockImpl(const ScopedLambda&lt;void(CodeBlock*)&gt;&amp; func)
2497 {
2498     // We don&#39;t know the full set of CodeBlocks until compilation has terminated.
2499     completeAllJITPlans();
2500 
2501     return m_codeBlocks-&gt;iterate(func);
2502 }
2503 
2504 void Heap::forEachCodeBlockIgnoringJITPlansImpl(const AbstractLocker&amp; locker, const ScopedLambda&lt;void(CodeBlock*)&gt;&amp; func)
2505 {
2506     return m_codeBlocks-&gt;iterate(locker, func);
2507 }
2508 
2509 void Heap::writeBarrierSlowPath(const JSCell* from)
2510 {
2511     if (UNLIKELY(mutatorShouldBeFenced())) {
2512         // In this case, the barrierThreshold is the tautological threshold, so from could still be
2513         // not black. But we can&#39;t know for sure until we fire off a fence.
2514         WTF::storeLoadFence();
2515         if (from-&gt;cellState() != CellState::PossiblyBlack)
2516             return;
2517     }
2518 
2519     addToRememberedSet(from);
2520 }
2521 
2522 bool Heap::isCurrentThreadBusy()
2523 {
2524     return mayBeGCThread() || mutatorState() != MutatorState::Running;
2525 }
2526 
2527 void Heap::reportExtraMemoryVisited(size_t size)
2528 {
2529     size_t* counter = &amp;m_extraMemorySize;
2530 
2531     for (;;) {
2532         size_t oldSize = *counter;
2533         // FIXME: Change this to use SaturatedArithmetic when available.
2534         // https://bugs.webkit.org/show_bug.cgi?id=170411
2535         Checked&lt;size_t, RecordOverflow&gt; checkedNewSize = oldSize;
2536         checkedNewSize += size;
2537         size_t newSize = UNLIKELY(checkedNewSize.hasOverflowed()) ? std::numeric_limits&lt;size_t&gt;::max() : checkedNewSize.unsafeGet();
2538         if (WTF::atomicCompareExchangeWeakRelaxed(counter, oldSize, newSize))
2539             return;
2540     }
2541 }
2542 
2543 #if ENABLE(RESOURCE_USAGE)
2544 void Heap::reportExternalMemoryVisited(size_t size)
2545 {
2546     size_t* counter = &amp;m_externalMemorySize;
2547 
2548     for (;;) {
2549         size_t oldSize = *counter;
2550         if (WTF::atomicCompareExchangeWeakRelaxed(counter, oldSize, oldSize + size))
2551             return;
2552     }
2553 }
2554 #endif
2555 
2556 void Heap::collectIfNecessaryOrDefer(GCDeferralContext* deferralContext)
2557 {
2558     ASSERT(deferralContext || isDeferred() || !DisallowGC::isInEffectOnCurrentThread());
2559     if (validateDFGDoesGC)
2560         RELEASE_ASSERT(expectDoesGC());
2561 
2562     if (!m_isSafeToCollect)
2563         return;
2564 
2565     switch (mutatorState()) {
2566     case MutatorState::Running:
2567     case MutatorState::Allocating:
2568         break;
2569     case MutatorState::Sweeping:
2570     case MutatorState::Collecting:
2571         return;
2572     }
2573     if (!Options::useGC())
2574         return;
2575 
2576     if (mayNeedToStop()) {
2577         if (deferralContext)
2578             deferralContext-&gt;m_shouldGC = true;
2579         else if (isDeferred())
2580             m_didDeferGCWork = true;
2581         else
2582             stopIfNecessary();
2583     }
2584 
2585     if (UNLIKELY(Options::gcMaxHeapSize())) {
2586         if (m_bytesAllocatedThisCycle &lt;= Options::gcMaxHeapSize())
2587             return;
2588     } else {
2589         size_t bytesAllowedThisCycle = m_maxEdenSize;
2590 
2591 #if PLATFORM(IOS_FAMILY)
2592         if (overCriticalMemoryThreshold())
2593             bytesAllowedThisCycle = std::min(m_maxEdenSizeWhenCritical, bytesAllowedThisCycle);
2594 #endif
2595 
2596         if (m_bytesAllocatedThisCycle &lt;= bytesAllowedThisCycle)
2597             return;
2598     }
2599 
2600     if (deferralContext)
2601         deferralContext-&gt;m_shouldGC = true;
2602     else if (isDeferred())
2603         m_didDeferGCWork = true;
2604     else {
2605         collectAsync();
2606         stopIfNecessary(); // This will immediately start the collection if we have the conn.
2607     }
2608 }
2609 
2610 void Heap::decrementDeferralDepthAndGCIfNeededSlow()
2611 {
2612     // Can&#39;t do anything if we&#39;re still deferred.
2613     if (m_deferralDepth)
2614         return;
2615 
2616     ASSERT(!isDeferred());
2617 
2618     m_didDeferGCWork = false;
2619     // FIXME: Bring back something like the DeferGCProbability mode.
2620     // https://bugs.webkit.org/show_bug.cgi?id=166627
2621     collectIfNecessaryOrDefer();
2622 }
2623 
2624 void Heap::registerWeakGCMap(WeakGCMapBase* weakGCMap)
2625 {
2626     m_weakGCMaps.add(weakGCMap);
2627 }
2628 
2629 void Heap::unregisterWeakGCMap(WeakGCMapBase* weakGCMap)
2630 {
2631     m_weakGCMaps.remove(weakGCMap);
2632 }
2633 
2634 void Heap::didAllocateBlock(size_t capacity)
2635 {
2636 #if ENABLE(RESOURCE_USAGE)
2637     m_blockBytesAllocated += capacity;
2638 #else
2639     UNUSED_PARAM(capacity);
2640 #endif
2641 }
2642 
2643 void Heap::didFreeBlock(size_t capacity)
2644 {
2645 #if ENABLE(RESOURCE_USAGE)
2646     m_blockBytesAllocated -= capacity;
2647 #else
2648     UNUSED_PARAM(capacity);
2649 #endif
2650 }
2651 
2652 void Heap::addCoreConstraints()
2653 {
2654     m_constraintSet-&gt;add(
2655         &quot;Cs&quot;, &quot;Conservative Scan&quot;,
2656         [this, lastVersion = static_cast&lt;uint64_t&gt;(0)] (SlotVisitor&amp; slotVisitor) mutable {
2657             bool shouldNotProduceWork = lastVersion == m_phaseVersion;
2658             if (shouldNotProduceWork)
2659                 return;
2660 
2661             TimingScope preConvergenceTimingScope(*this, &quot;Constraint: conservative scan&quot;);
2662             m_objectSpace.prepareForConservativeScan();
2663 
2664             {
2665                 ConservativeRoots conservativeRoots(*this);
2666                 SuperSamplerScope superSamplerScope(false);
2667 
2668                 gatherStackRoots(conservativeRoots);
2669                 gatherJSStackRoots(conservativeRoots);
2670                 gatherScratchBufferRoots(conservativeRoots);
2671 
2672                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ConservativeScan);
2673                 slotVisitor.append(conservativeRoots);
2674             }
2675             if (VM::canUseJIT()) {
2676                 // JITStubRoutines must be visited after scanning ConservativeRoots since JITStubRoutines depend on the hook executed during gathering ConservativeRoots.
2677                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::JITStubRoutines);
2678                 m_jitStubRoutines-&gt;traceMarkedStubRoutines(slotVisitor);
2679             }
2680 
2681             lastVersion = m_phaseVersion;
2682         },
2683         ConstraintVolatility::GreyedByExecution);
2684 
2685     m_constraintSet-&gt;add(
2686         &quot;Msr&quot;, &quot;Misc Small Roots&quot;,
2687         [this] (SlotVisitor&amp; slotVisitor) {
2688 
2689 #if JSC_OBJC_API_ENABLED
2690             scanExternalRememberedSet(*m_vm, slotVisitor);
2691 #endif
2692             if (m_vm-&gt;smallStrings.needsToBeVisited(*m_collectionScope)) {
2693                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::StrongReferences);
2694                 m_vm-&gt;smallStrings.visitStrongReferences(slotVisitor);
2695             }
2696 
2697             {
2698                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ProtectedValues);
2699                 for (auto&amp; pair : m_protectedValues)
2700                     slotVisitor.appendUnbarriered(pair.key);
2701             }
2702 
2703             if (m_markListSet &amp;&amp; m_markListSet-&gt;size()) {
2704                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::ConservativeScan);
2705                 MarkedArgumentBuffer::markLists(slotVisitor, *m_markListSet);
2706             }
2707 
2708             {
2709                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::VMExceptions);
2710                 slotVisitor.appendUnbarriered(m_vm-&gt;exception());
2711                 slotVisitor.appendUnbarriered(m_vm-&gt;lastException());
2712             }
2713         },
2714         ConstraintVolatility::GreyedByExecution);
2715 
2716     m_constraintSet-&gt;add(
2717         &quot;Sh&quot;, &quot;Strong Handles&quot;,
2718         [this] (SlotVisitor&amp; slotVisitor) {
2719             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::StrongHandles);
2720             m_handleSet.visitStrongHandles(slotVisitor);
2721         },
2722         ConstraintVolatility::GreyedByExecution);
2723 
2724     m_constraintSet-&gt;add(
2725         &quot;D&quot;, &quot;Debugger&quot;,
2726         [this] (SlotVisitor&amp; slotVisitor) {
2727             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::Debugger);
2728 
2729 #if ENABLE(SAMPLING_PROFILER)
2730             if (SamplingProfiler* samplingProfiler = m_vm-&gt;samplingProfiler()) {
2731                 LockHolder locker(samplingProfiler-&gt;getLock());
2732                 samplingProfiler-&gt;processUnverifiedStackTraces();
2733                 samplingProfiler-&gt;visit(slotVisitor);
2734                 if (Options::logGC() == GCLogging::Verbose)
2735                     dataLog(&quot;Sampling Profiler data:\n&quot;, slotVisitor);
2736             }
2737 #endif // ENABLE(SAMPLING_PROFILER)
2738 
2739             if (m_vm-&gt;typeProfiler())
2740                 m_vm-&gt;typeProfilerLog()-&gt;visit(slotVisitor);
2741 
2742             if (auto* shadowChicken = m_vm-&gt;shadowChicken())
2743                 shadowChicken-&gt;visitChildren(slotVisitor);
2744         },
2745         ConstraintVolatility::GreyedByExecution);
2746 
2747     m_constraintSet-&gt;add(
2748         &quot;Ws&quot;, &quot;Weak Sets&quot;,
2749         [this] (SlotVisitor&amp; slotVisitor) {
2750             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::WeakSets);
2751             m_objectSpace.visitWeakSets(slotVisitor);
2752         },
2753         ConstraintVolatility::GreyedByMarking);
2754 
2755     m_constraintSet-&gt;add(
2756         &quot;O&quot;, &quot;Output&quot;,
2757         [] (SlotVisitor&amp; slotVisitor) {
2758             VM&amp; vm = slotVisitor.vm();
2759 
2760             auto callOutputConstraint = [] (SlotVisitor&amp; slotVisitor, HeapCell* heapCell, HeapCell::Kind) {
2761                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::Output);
2762                 VM&amp; vm = slotVisitor.vm();
2763                 JSCell* cell = static_cast&lt;JSCell*&gt;(heapCell);
2764                 cell-&gt;methodTable(vm)-&gt;visitOutputConstraints(cell, slotVisitor);
2765             };
2766 
2767             auto add = [&amp;] (auto&amp; set) {
2768                 slotVisitor.addParallelConstraintTask(set.forEachMarkedCellInParallel(callOutputConstraint));
2769             };
2770 
2771             add(vm.executableToCodeBlockEdgesWithConstraints);
2772             if (vm.m_weakMapSpace)
2773                 add(*vm.m_weakMapSpace);
2774         },
2775         ConstraintVolatility::GreyedByMarking,
2776         ConstraintParallelism::Parallel);
2777 
2778 #if ENABLE(DFG_JIT)
2779     if (VM::canUseJIT()) {
2780         m_constraintSet-&gt;add(
2781             &quot;Dw&quot;, &quot;DFG Worklists&quot;,
2782             [this] (SlotVisitor&amp; slotVisitor) {
2783                 SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::DFGWorkLists);
2784 
2785                 for (unsigned i = DFG::numberOfWorklists(); i--;)
2786                     DFG::existingWorklistForIndex(i).visitWeakReferences(slotVisitor);
2787 
2788                 // FIXME: This is almost certainly unnecessary.
2789                 // https://bugs.webkit.org/show_bug.cgi?id=166829
2790                 DFG::iterateCodeBlocksForGC(
2791                     *m_vm,
2792                     [&amp;] (CodeBlock* codeBlock) {
2793                         slotVisitor.appendUnbarriered(codeBlock);
2794                     });
2795 
2796                 if (Options::logGC() == GCLogging::Verbose)
2797                     dataLog(&quot;DFG Worklists:\n&quot;, slotVisitor);
2798             },
2799             ConstraintVolatility::GreyedByMarking);
2800     }
2801 #endif
2802 
2803     m_constraintSet-&gt;add(
2804         &quot;Cb&quot;, &quot;CodeBlocks&quot;,
2805         [this] (SlotVisitor&amp; slotVisitor) {
2806             SetRootMarkReasonScope rootScope(slotVisitor, SlotVisitor::RootMarkReason::CodeBlocks);
2807             iterateExecutingAndCompilingCodeBlocksWithoutHoldingLocks(
2808                 [&amp;] (CodeBlock* codeBlock) {
2809                     // Visit the CodeBlock as a constraint only if it&#39;s black.
2810                     if (Heap::isMarked(codeBlock)
2811                         &amp;&amp; codeBlock-&gt;cellState() == CellState::PossiblyBlack)
2812                         slotVisitor.visitAsConstraint(codeBlock);
2813                 });
2814         },
2815         ConstraintVolatility::SeldomGreyed);
2816 
2817     m_constraintSet-&gt;add(std::make_unique&lt;MarkStackMergingConstraint&gt;(*this));
2818 }
2819 
2820 void Heap::addMarkingConstraint(std::unique_ptr&lt;MarkingConstraint&gt; constraint)
2821 {
2822     PreventCollectionScope preventCollectionScope(*this);
2823     m_constraintSet-&gt;add(WTFMove(constraint));
2824 }
2825 
2826 void Heap::notifyIsSafeToCollect()
2827 {
2828     MonotonicTime before;
2829     if (Options::logGC()) {
2830         before = MonotonicTime::now();
2831         dataLog(&quot;[GC&lt;&quot;, RawPointer(this), &quot;&gt;: starting &quot;);
2832     }
2833 
2834     addCoreConstraints();
2835 
2836     m_isSafeToCollect = true;
2837 
2838     if (Options::collectContinuously()) {
2839         m_collectContinuouslyThread = WTF::Thread::create(
2840             &quot;JSC DEBUG Continuous GC&quot;,
2841             [this] () {
2842                 MonotonicTime initialTime = MonotonicTime::now();
2843                 Seconds period = Seconds::fromMilliseconds(Options::collectContinuouslyPeriodMS());
2844                 while (!m_shouldStopCollectingContinuously) {
2845                     {
2846                         LockHolder locker(*m_threadLock);
2847                         if (m_requests.isEmpty()) {
2848                             m_requests.append(WTF::nullopt);
2849                             m_lastGrantedTicket++;
2850                             m_threadCondition-&gt;notifyOne(locker);
2851                         }
2852                     }
2853 
2854                     {
2855                         LockHolder locker(m_collectContinuouslyLock);
2856                         Seconds elapsed = MonotonicTime::now() - initialTime;
2857                         Seconds elapsedInPeriod = elapsed % period;
2858                         MonotonicTime timeToWakeUp =
2859                             initialTime + elapsed - elapsedInPeriod + period;
2860                         while (!hasElapsed(timeToWakeUp) &amp;&amp; !m_shouldStopCollectingContinuously) {
2861                             m_collectContinuouslyCondition.waitUntil(
2862                                 m_collectContinuouslyLock, timeToWakeUp);
2863                         }
2864                     }
2865                 }
2866             });
2867     }
2868 
2869     if (Options::logGC())
2870         dataLog((MonotonicTime::now() - before).milliseconds(), &quot;ms]\n&quot;);
2871 }
2872 
2873 void Heap::preventCollection()
2874 {
2875     if (!m_isSafeToCollect)
2876         return;
2877 
2878     // This prevents the collectContinuously thread from starting a collection.
2879     m_collectContinuouslyLock.lock();
2880 
2881     // Wait for all collections to finish.
2882     waitForCollector(
2883         [&amp;] (const AbstractLocker&amp;) -&gt; bool {
2884             ASSERT(m_lastServedTicket &lt;= m_lastGrantedTicket);
2885             return m_lastServedTicket == m_lastGrantedTicket;
2886         });
2887 
2888     // Now a collection can only start if this thread starts it.
2889     RELEASE_ASSERT(!m_collectionScope);
2890 }
2891 
2892 void Heap::allowCollection()
2893 {
2894     if (!m_isSafeToCollect)
2895         return;
2896 
2897     m_collectContinuouslyLock.unlock();
2898 }
2899 
2900 void Heap::setMutatorShouldBeFenced(bool value)
2901 {
2902     m_mutatorShouldBeFenced = value;
2903     m_barrierThreshold = value ? tautologicalThreshold : blackThreshold;
2904 }
2905 
2906 void Heap::performIncrement(size_t bytes)
2907 {
2908     if (!m_objectSpace.isMarking())
2909         return;
2910 
2911     if (isDeferred())
2912         return;
2913 
2914     m_incrementBalance += bytes * Options::gcIncrementScale();
2915 
2916     // Save ourselves from crazy. Since this is an optimization, it&#39;s OK to go back to any consistent
2917     // state when the double goes wild.
2918     if (std::isnan(m_incrementBalance) || std::isinf(m_incrementBalance))
2919         m_incrementBalance = 0;
2920 
2921     if (m_incrementBalance &lt; static_cast&lt;double&gt;(Options::gcIncrementBytes()))
2922         return;
2923 
2924     double targetBytes = m_incrementBalance;
2925     if (targetBytes &lt;= 0)
2926         return;
2927     targetBytes = std::min(targetBytes, Options::gcIncrementMaxBytes());
2928 
2929     SlotVisitor&amp; slotVisitor = *m_mutatorSlotVisitor;
2930     ParallelModeEnabler parallelModeEnabler(slotVisitor);
2931     size_t bytesVisited = slotVisitor.performIncrementOfDraining(static_cast&lt;size_t&gt;(targetBytes));
2932     // incrementBalance may go negative here because it&#39;ll remember how many bytes we overshot.
2933     m_incrementBalance -= bytesVisited;
2934 }
2935 
2936 void Heap::addHeapFinalizerCallback(const HeapFinalizerCallback&amp; callback)
2937 {
2938     m_heapFinalizerCallbacks.append(callback);
2939 }
2940 
2941 void Heap::removeHeapFinalizerCallback(const HeapFinalizerCallback&amp; callback)
2942 {
2943     m_heapFinalizerCallbacks.removeFirst(callback);
2944 }
2945 
2946 void Heap::setBonusVisitorTask(RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt; task)
2947 {
2948     auto locker = holdLock(m_markingMutex);
2949     m_bonusVisitorTask = task;
2950     m_markingConditionVariable.notifyAll();
2951 }
2952 
2953 void Heap::runTaskInParallel(RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt; task)
2954 {
2955     unsigned initialRefCount = task-&gt;refCount();
2956     setBonusVisitorTask(task);
2957     task-&gt;run(*m_collectorSlotVisitor);
2958     setBonusVisitorTask(nullptr);
2959     // The constraint solver expects return of this function to imply termination of the task in all
2960     // threads. This ensures that property.
2961     {
2962         auto locker = holdLock(m_markingMutex);
2963         while (task-&gt;refCount() &gt; initialRefCount)
2964             m_markingConditionVariable.wait(m_markingMutex);
2965     }
2966 }
2967 
2968 } // namespace JSC
    </pre>
  </body>
</html>