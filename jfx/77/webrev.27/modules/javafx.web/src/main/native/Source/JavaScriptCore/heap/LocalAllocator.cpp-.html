<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/LocalAllocator.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2018 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;LocalAllocator.h&quot;
 28 
 29 #include &quot;AllocatingScope.h&quot;
 30 #include &quot;LocalAllocatorInlines.h&quot;
 31 #include &quot;Options.h&quot;
 32 
 33 namespace JSC {
 34 
 35 LocalAllocator::LocalAllocator(BlockDirectory* directory)
 36     : m_directory(directory)
 37     , m_freeList(directory-&gt;m_cellSize)
 38 {
 39     auto locker = holdLock(directory-&gt;m_localAllocatorsLock);
 40     directory-&gt;m_localAllocators.append(this);
 41 }
 42 
 43 void LocalAllocator::reset()
 44 {
 45     m_freeList.clear();
 46     m_currentBlock = nullptr;
 47     m_lastActiveBlock = nullptr;
 48     m_allocationCursor = 0;
 49 }
 50 
 51 LocalAllocator::~LocalAllocator()
 52 {
 53     if (isOnList()) {
 54         auto locker = holdLock(m_directory-&gt;m_localAllocatorsLock);
 55         remove();
 56     }
 57 
 58     bool ok = true;
 59     if (!m_freeList.allocationWillFail()) {
 60         dataLog(&quot;FATAL: &quot;, RawPointer(this), &quot;-&gt;~LocalAllocator has non-empty free-list.\n&quot;);
 61         ok = false;
 62     }
 63     if (m_currentBlock) {
 64         dataLog(&quot;FATAL: &quot;, RawPointer(this), &quot;-&gt;~LocalAllocator has non-null current block.\n&quot;);
 65         ok = false;
 66     }
 67     if (m_lastActiveBlock) {
 68         dataLog(&quot;FATAL: &quot;, RawPointer(this), &quot;-&gt;~LocalAllocator has non-null last active block.\n&quot;);
 69         ok = false;
 70     }
 71     RELEASE_ASSERT(ok);
 72 }
 73 
 74 void LocalAllocator::stopAllocating()
 75 {
 76     ASSERT(!m_lastActiveBlock);
 77     if (!m_currentBlock) {
 78         ASSERT(m_freeList.allocationWillFail());
 79         return;
 80     }
 81 
 82     m_currentBlock-&gt;stopAllocating(m_freeList);
 83     m_lastActiveBlock = m_currentBlock;
 84     m_currentBlock = nullptr;
 85     m_freeList.clear();
 86 }
 87 
 88 void LocalAllocator::resumeAllocating()
 89 {
 90     if (!m_lastActiveBlock)
 91         return;
 92 
 93     m_lastActiveBlock-&gt;resumeAllocating(m_freeList);
 94     m_currentBlock = m_lastActiveBlock;
 95     m_lastActiveBlock = nullptr;
 96 }
 97 
 98 void LocalAllocator::prepareForAllocation()
 99 {
100     reset();
101 }
102 
103 void LocalAllocator::stopAllocatingForGood()
104 {
105     stopAllocating();
106     reset();
107 }
108 
109 void* LocalAllocator::allocateSlowCase(GCDeferralContext* deferralContext, AllocationFailureMode failureMode)
110 {
111     SuperSamplerScope superSamplerScope(false);
112     Heap&amp; heap = *m_directory-&gt;m_heap;
113     ASSERT(heap.vm()-&gt;currentThreadIsHoldingAPILock());
114     doTestCollectionsIfNeeded(deferralContext);
115 
116     ASSERT(!m_directory-&gt;markedSpace().isIterating());
117     heap.didAllocate(m_freeList.originalSize());
118 
119     didConsumeFreeList();
120 
121     AllocatingScope helpingHeap(heap);
122 
123     heap.collectIfNecessaryOrDefer(deferralContext);
124 
125     // Goofy corner case: the GC called a callback and now this directory has a currentBlock. This only
126     // happens when running WebKit tests, which inject a callback into the GC&#39;s finalization.
127     if (UNLIKELY(m_currentBlock))
128         return allocate(deferralContext, failureMode);
129 
130     void* result = tryAllocateWithoutCollecting();
131 
132     if (LIKELY(result != 0))
133         return result;
134 
135     MarkedBlock::Handle* block = m_directory-&gt;tryAllocateBlock();
136     if (!block) {
137         if (failureMode == AllocationFailureMode::Assert)
138             RELEASE_ASSERT_NOT_REACHED();
139         else
140             return nullptr;
141     }
142     m_directory-&gt;addBlock(block);
143     result = allocateIn(block);
144     ASSERT(result);
145     return result;
146 }
147 
148 void LocalAllocator::didConsumeFreeList()
149 {
150     if (m_currentBlock)
151         m_currentBlock-&gt;didConsumeFreeList();
152 
153     m_freeList.clear();
154     m_currentBlock = nullptr;
155 }
156 
157 void* LocalAllocator::tryAllocateWithoutCollecting()
158 {
159     // FIXME: If we wanted this to be used for real multi-threaded allocations then we would have to
160     // come up with some concurrency protocol here. That protocol would need to be able to handle:
161     //
162     // - The basic case of multiple LocalAllocators trying to do an allocationCursor search on the
163     //   same bitvector. That probably needs the bitvector lock at least.
164     //
165     // - The harder case of some LocalAllocator triggering a steal from a different BlockDirectory
166     //   via a search in the AlignedMemoryAllocator&#39;s list. Who knows what locks that needs.
167     //
168     // One way to make this work is to have a single per-Heap lock that protects all mutator lock
169     // allocation slow paths. That would probably be scalable enough for years. It would certainly be
170     // for using TLC allocation from JIT threads.
171     // https://bugs.webkit.org/show_bug.cgi?id=181635
172 
173     SuperSamplerScope superSamplerScope(false);
174 
175     ASSERT(!m_currentBlock);
176     ASSERT(m_freeList.allocationWillFail());
177 
178     for (;;) {
179         MarkedBlock::Handle* block = m_directory-&gt;findBlockForAllocation(*this);
180         if (!block)
181             break;
182 
183         if (void* result = tryAllocateIn(block))
184             return result;
185     }
186 
187     if (Options::stealEmptyBlocksFromOtherAllocators()
188         &amp;&amp; (Options::tradeDestructorBlocks() || !m_directory-&gt;needsDestruction())) {
189         if (MarkedBlock::Handle* block = m_directory-&gt;m_subspace-&gt;findEmptyBlockToSteal()) {
190             RELEASE_ASSERT(block-&gt;alignedMemoryAllocator() == m_directory-&gt;m_subspace-&gt;alignedMemoryAllocator());
191 
192             block-&gt;sweep(nullptr);
193 
194             // It&#39;s good that this clears canAllocateButNotEmpty as well as all other bits,
195             // because there is a remote chance that a block may have both canAllocateButNotEmpty
196             // and empty set at the same time.
197             block-&gt;removeFromDirectory();
198             m_directory-&gt;addBlock(block);
199             return allocateIn(block);
200         }
201     }
202 
203     return nullptr;
204 }
205 
206 void* LocalAllocator::allocateIn(MarkedBlock::Handle* block)
207 {
208     void* result = tryAllocateIn(block);
209     RELEASE_ASSERT(result);
210     return result;
211 }
212 
213 void* LocalAllocator::tryAllocateIn(MarkedBlock::Handle* block)
214 {
215     ASSERT(block);
216     ASSERT(!block-&gt;isFreeListed());
217 
218     block-&gt;sweep(&amp;m_freeList);
219 
220     // It&#39;s possible to stumble on a completely full block. Marking tries to retire these, but
221     // that algorithm is racy and may forget to do it sometimes.
222     if (m_freeList.allocationWillFail()) {
223         ASSERT(block-&gt;isFreeListed());
224         block-&gt;unsweepWithNoNewlyAllocated();
225         ASSERT(!block-&gt;isFreeListed());
226         ASSERT(!m_directory-&gt;isEmpty(NoLockingNecessary, block));
227         ASSERT(!m_directory-&gt;isCanAllocateButNotEmpty(NoLockingNecessary, block));
228         return nullptr;
229     }
230 
231     m_currentBlock = block;
232 
233     void* result = m_freeList.allocate(
234         [] () -&gt; HeapCell* {
235             RELEASE_ASSERT_NOT_REACHED();
236             return nullptr;
237         });
238     m_directory-&gt;setIsEden(NoLockingNecessary, m_currentBlock, true);
239     m_directory-&gt;markedSpace().didAllocateInBlock(m_currentBlock);
240     return result;
241 }
242 
243 void LocalAllocator::doTestCollectionsIfNeeded(GCDeferralContext* deferralContext)
244 {
245     if (!Options::slowPathAllocsBetweenGCs())
246         return;
247 
248     static unsigned allocationCount = 0;
249     if (!allocationCount) {
250         if (!m_directory-&gt;m_heap-&gt;isDeferred()) {
251             if (deferralContext)
252                 deferralContext-&gt;m_shouldGC = true;
253             else
254                 m_directory-&gt;m_heap-&gt;collectNow(Sync, CollectionScope::Full);
255         }
256     }
257     if (++allocationCount &gt;= Options::slowPathAllocsBetweenGCs())
258         allocationCount = 0;
259 }
260 
261 bool LocalAllocator::isFreeListedCell(const void* target) const
262 {
263     // This abomination exists to detect when an object is in the dead-but-not-destructed state.
264     // Therefore, it&#39;s not even clear that this needs to do anything beyond returning &quot;false&quot;, since
265     // if we know that the block owning the object is free-listed, then it&#39;s impossible for any
266     // objects to be in the dead-but-not-destructed state.
267     // FIXME: Get rid of this abomination. https://bugs.webkit.org/show_bug.cgi?id=181655
268     return m_freeList.contains(bitwise_cast&lt;HeapCell*&gt;(target));
269 }
270 
271 } // namespace JSC
272 
    </pre>
  </body>
</html>