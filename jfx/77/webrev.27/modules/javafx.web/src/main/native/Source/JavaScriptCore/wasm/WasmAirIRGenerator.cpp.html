<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.web/src/main/native/Source/JavaScriptCore/wasm/WasmAirIRGenerator.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;WasmAirIRGenerator.h&quot;
  28 
  29 #if ENABLE(WEBASSEMBLY)
  30 
  31 #include &quot;AirCode.h&quot;
  32 #include &quot;AirGenerate.h&quot;
  33 #include &quot;AirOpcodeUtils.h&quot;
  34 #include &quot;AirValidate.h&quot;
  35 #include &quot;AllowMacroScratchRegisterUsageIf.h&quot;
  36 #include &quot;B3CCallValue.h&quot;
  37 #include &quot;B3CheckSpecial.h&quot;
  38 #include &quot;B3CheckValue.h&quot;
  39 #include &quot;B3PatchpointSpecial.h&quot;
  40 #include &quot;B3Procedure.h&quot;
  41 #include &quot;B3ProcedureInlines.h&quot;
  42 #include &quot;BinarySwitch.h&quot;
  43 #include &quot;DisallowMacroScratchRegisterUsage.h&quot;
  44 #include &quot;JSCInlines.h&quot;
  45 #include &quot;JSWebAssemblyInstance.h&quot;
  46 #include &quot;ScratchRegisterAllocator.h&quot;
  47 #include &quot;VirtualRegister.h&quot;
  48 #include &quot;WasmCallingConvention.h&quot;
  49 #include &quot;WasmContextInlines.h&quot;
  50 #include &quot;WasmExceptionType.h&quot;
  51 #include &quot;WasmFunctionParser.h&quot;
  52 #include &quot;WasmInstance.h&quot;
  53 #include &quot;WasmMemory.h&quot;
  54 #include &quot;WasmOMGPlan.h&quot;
  55 #include &quot;WasmOSREntryData.h&quot;
  56 #include &quot;WasmOpcodeOrigin.h&quot;
  57 #include &quot;WasmOperations.h&quot;
  58 #include &quot;WasmSignatureInlines.h&quot;
  59 #include &quot;WasmThunks.h&quot;
  60 #include &lt;limits&gt;
  61 #include &lt;wtf/Box.h&gt;
  62 #include &lt;wtf/Optional.h&gt;
  63 #include &lt;wtf/StdLibExtras.h&gt;
  64 
  65 namespace JSC { namespace Wasm {
  66 
  67 using namespace B3::Air;
  68 
  69 struct ConstrainedTmp {
  70     ConstrainedTmp(Tmp tmp)
  71         : ConstrainedTmp(tmp, tmp.isReg() ? B3::ValueRep::reg(tmp.reg()) : B3::ValueRep::SomeRegister)
  72     { }
  73 
  74     ConstrainedTmp(Tmp tmp, B3::ValueRep rep)
  75         : tmp(tmp)
  76         , rep(rep)
  77     {
  78     }
  79 
  80     Tmp tmp;
  81     B3::ValueRep rep;
  82 };
  83 
  84 class TypedTmp {
  85 public:
  86     constexpr TypedTmp()
  87         : m_tmp()
  88         , m_type(Type::Void)
  89     { }
  90 
  91     TypedTmp(Tmp tmp, Type type)
  92         : m_tmp(tmp)
  93         , m_type(type)
  94     { }
  95 
  96     TypedTmp(const TypedTmp&amp;) = default;
  97     TypedTmp(TypedTmp&amp;&amp;) = default;
  98     TypedTmp&amp; operator=(TypedTmp&amp;&amp;) = default;
  99     TypedTmp&amp; operator=(const TypedTmp&amp;) = default;
 100 
 101     bool operator==(const TypedTmp&amp; other) const
 102     {
 103         return m_tmp == other.m_tmp &amp;&amp; m_type == other.m_type;
 104     }
 105     bool operator!=(const TypedTmp&amp; other) const
 106     {
 107         return !(*this == other);
 108     }
 109 
 110     explicit operator bool() const { return !!tmp(); }
 111 
 112     operator Tmp() const { return tmp(); }
 113     operator Arg() const { return Arg(tmp()); }
 114     Tmp tmp() const { return m_tmp; }
 115     Type type() const { return m_type; }
 116 
 117 private:
 118 
 119     Tmp m_tmp;
 120     Type m_type;
 121 };
 122 
 123 class AirIRGenerator {
 124 public:
 125     struct ControlData {
 126         ControlData(B3::Origin origin, Type returnType, TypedTmp resultTmp, BlockType type, BasicBlock* continuation, BasicBlock* special = nullptr)
 127             : blockType(type)
 128             , continuation(continuation)
 129             , special(special)
 130             , returnType(returnType)
 131         {
 132             UNUSED_PARAM(origin); // FIXME: Use origin.
 133             if (resultTmp) {
 134                 ASSERT(returnType != Type::Void);
 135                 result.append(resultTmp);
 136             } else
 137                 ASSERT(returnType == Type::Void);
 138         }
 139 
 140         ControlData()
 141         {
 142         }
 143 
 144         void dump(PrintStream&amp; out) const
 145         {
 146             switch (type()) {
 147             case BlockType::If:
 148                 out.print(&quot;If:       &quot;);
 149                 break;
 150             case BlockType::Block:
 151                 out.print(&quot;Block:    &quot;);
 152                 break;
 153             case BlockType::Loop:
 154                 out.print(&quot;Loop:     &quot;);
 155                 break;
 156             case BlockType::TopLevel:
 157                 out.print(&quot;TopLevel: &quot;);
 158                 break;
 159             }
 160             out.print(&quot;Continuation: &quot;, *continuation, &quot;, Special: &quot;);
 161             if (special)
 162                 out.print(*special);
 163             else
 164                 out.print(&quot;None&quot;);
 165         }
 166 
 167         BlockType type() const { return blockType; }
 168 
 169         Type signature() const { return returnType; }
 170 
 171         bool hasNonVoidSignature() const { return result.size(); }
 172 
 173         BasicBlock* targetBlockForBranch()
 174         {
 175             if (type() == BlockType::Loop)
 176                 return special;
 177             return continuation;
 178         }
 179 
 180         void convertIfToBlock()
 181         {
 182             ASSERT(type() == BlockType::If);
 183             blockType = BlockType::Block;
 184             special = nullptr;
 185         }
 186 
 187         using ResultList = Vector&lt;TypedTmp, 1&gt;;
 188 
 189         ResultList resultForBranch() const
 190         {
 191             if (type() == BlockType::Loop)
 192                 return ResultList();
 193             return result;
 194         }
 195 
 196     private:
 197         friend class AirIRGenerator;
 198         BlockType blockType;
 199         BasicBlock* continuation;
 200         BasicBlock* special;
 201         ResultList result;
 202         Type returnType;
 203     };
 204 
 205     using ExpressionType = TypedTmp;
 206     using ControlType = ControlData;
 207     using ExpressionList = Vector&lt;ExpressionType, 1&gt;;
 208     using Stack = ExpressionList;
 209     using ResultList = ControlData::ResultList;
 210     using ControlEntry = FunctionParser&lt;AirIRGenerator&gt;::ControlEntry;
 211 
 212     static ExpressionType emptyExpression() { return { }; };
 213     Stack createStack() { return Stack(); }
 214 
 215     using ErrorType = String;
 216     using UnexpectedResult = Unexpected&lt;ErrorType&gt;;
 217     using Result = Expected&lt;std::unique_ptr&lt;InternalFunction&gt;, ErrorType&gt;;
 218     using PartialResult = Expected&lt;void, ErrorType&gt;;
 219 
 220     template &lt;typename ...Args&gt;
 221     NEVER_INLINE UnexpectedResult WARN_UNUSED_RETURN fail(Args... args) const
 222     {
 223         using namespace FailureHelper; // See ADL comment in WasmParser.h.
 224         return UnexpectedResult(makeString(&quot;WebAssembly.Module failed compiling: &quot;_s, makeString(args)...));
 225     }
 226 
 227 #define WASM_COMPILE_FAIL_IF(condition, ...) do { \
 228         if (UNLIKELY(condition))                  \
 229             return fail(__VA_ARGS__);             \
 230     } while (0)
 231 
 232     AirIRGenerator(const ModuleInformation&amp;, B3::Procedure&amp;, InternalFunction*, Vector&lt;UnlinkedWasmToWasmCall&gt;&amp;, MemoryMode, unsigned functionIndex, TierUpCount*, ThrowWasmException, const Signature&amp;);
 233 
 234     PartialResult WARN_UNUSED_RETURN addArguments(const Signature&amp;);
 235     PartialResult WARN_UNUSED_RETURN addLocal(Type, uint32_t);
 236     ExpressionType addConstant(Type, uint64_t);
 237     ExpressionType addConstant(BasicBlock*, Type, uint64_t);
 238 
 239     // References
 240     PartialResult WARN_UNUSED_RETURN addRefIsNull(ExpressionType&amp; value, ExpressionType&amp; result);
 241     PartialResult WARN_UNUSED_RETURN addRefFunc(uint32_t index, ExpressionType&amp; result);
 242 
 243     // Tables
 244     PartialResult WARN_UNUSED_RETURN addTableGet(unsigned, ExpressionType&amp; index, ExpressionType&amp; result);
 245     PartialResult WARN_UNUSED_RETURN addTableSet(unsigned, ExpressionType&amp; index, ExpressionType&amp; value);
 246     PartialResult WARN_UNUSED_RETURN addTableSize(unsigned, ExpressionType&amp; result);
 247     PartialResult WARN_UNUSED_RETURN addTableGrow(unsigned, ExpressionType&amp; fill, ExpressionType&amp; delta, ExpressionType&amp; result);
 248     PartialResult WARN_UNUSED_RETURN addTableFill(unsigned, ExpressionType&amp; offset, ExpressionType&amp; fill, ExpressionType&amp; count);
 249 
 250     // Locals
 251     PartialResult WARN_UNUSED_RETURN getLocal(uint32_t index, ExpressionType&amp; result);
 252     PartialResult WARN_UNUSED_RETURN setLocal(uint32_t index, ExpressionType value);
 253 
 254     // Globals
 255     PartialResult WARN_UNUSED_RETURN getGlobal(uint32_t index, ExpressionType&amp; result);
 256     PartialResult WARN_UNUSED_RETURN setGlobal(uint32_t index, ExpressionType value);
 257 
 258     // Memory
 259     PartialResult WARN_UNUSED_RETURN load(LoadOpType, ExpressionType pointer, ExpressionType&amp; result, uint32_t offset);
 260     PartialResult WARN_UNUSED_RETURN store(StoreOpType, ExpressionType pointer, ExpressionType value, uint32_t offset);
 261     PartialResult WARN_UNUSED_RETURN addGrowMemory(ExpressionType delta, ExpressionType&amp; result);
 262     PartialResult WARN_UNUSED_RETURN addCurrentMemory(ExpressionType&amp; result);
 263 
 264     // Basic operators
 265     template&lt;OpType&gt;
 266     PartialResult WARN_UNUSED_RETURN addOp(ExpressionType arg, ExpressionType&amp; result);
 267     template&lt;OpType&gt;
 268     PartialResult WARN_UNUSED_RETURN addOp(ExpressionType left, ExpressionType right, ExpressionType&amp; result);
 269     PartialResult WARN_UNUSED_RETURN addSelect(ExpressionType condition, ExpressionType nonZero, ExpressionType zero, ExpressionType&amp; result);
 270 
 271     // Control flow
 272     ControlData WARN_UNUSED_RETURN addTopLevel(Type signature);
 273     ControlData WARN_UNUSED_RETURN addBlock(Type signature);
 274     ControlData WARN_UNUSED_RETURN addLoop(Type signature, const Stack&amp;, uint32_t loopIndex);
 275     PartialResult WARN_UNUSED_RETURN addIf(ExpressionType condition, Type signature, ControlData&amp; result);
 276     PartialResult WARN_UNUSED_RETURN addElse(ControlData&amp;, const Stack&amp;);
 277     PartialResult WARN_UNUSED_RETURN addElseToUnreachable(ControlData&amp;);
 278 
 279     PartialResult WARN_UNUSED_RETURN addReturn(const ControlData&amp;, const ExpressionList&amp; returnValues);
 280     PartialResult WARN_UNUSED_RETURN addBranch(ControlData&amp;, ExpressionType condition, const Stack&amp; returnValues);
 281     PartialResult WARN_UNUSED_RETURN addSwitch(ExpressionType condition, const Vector&lt;ControlData*&gt;&amp; targets, ControlData&amp; defaultTargets, const Stack&amp; expressionStack);
 282     PartialResult WARN_UNUSED_RETURN endBlock(ControlEntry&amp;, Stack&amp; expressionStack);
 283     PartialResult WARN_UNUSED_RETURN addEndToUnreachable(ControlEntry&amp;);
 284 
 285     // Calls
 286     PartialResult WARN_UNUSED_RETURN addCall(uint32_t calleeIndex, const Signature&amp;, Vector&lt;ExpressionType&gt;&amp; args, ExpressionType&amp; result);
 287     PartialResult WARN_UNUSED_RETURN addCallIndirect(unsigned tableIndex, const Signature&amp;, Vector&lt;ExpressionType&gt;&amp; args, ExpressionType&amp; result);
 288     PartialResult WARN_UNUSED_RETURN addUnreachable();
 289 
 290     PartialResult addShift(Type, B3::Air::Opcode, ExpressionType value, ExpressionType shift, ExpressionType&amp; result);
 291     PartialResult addIntegerSub(B3::Air::Opcode, ExpressionType lhs, ExpressionType rhs, ExpressionType&amp; result);
 292     PartialResult addFloatingPointAbs(B3::Air::Opcode, ExpressionType value, ExpressionType&amp; result);
 293     PartialResult addFloatingPointBinOp(Type, B3::Air::Opcode, ExpressionType lhs, ExpressionType rhs, ExpressionType&amp; result);
 294 
 295     void dump(const Vector&lt;ControlEntry&gt;&amp; controlStack, const Stack* expressionStack);
 296     void setParser(FunctionParser&lt;AirIRGenerator&gt;* parser) { m_parser = parser; };
 297 
 298     static Vector&lt;Tmp&gt; toTmpVector(const Vector&lt;TypedTmp&gt;&amp; vector)
 299     {
 300         Vector&lt;Tmp&gt; result;
 301         for (const auto&amp; item : vector)
 302             result.append(item.tmp());
 303         return result;
 304     }
 305 
 306     ALWAYS_INLINE void didKill(const ExpressionType&amp; typedTmp)
 307     {
 308         Tmp tmp = typedTmp.tmp();
 309         if (!tmp)
 310             return;
 311         if (tmp.isGP())
 312             m_freeGPs.append(tmp);
 313         else
 314             m_freeFPs.append(tmp);
 315     }
 316 
 317     const Bag&lt;B3::PatchpointValue*&gt;&amp; patchpoints() const
 318     {
 319         return m_patchpoints;
 320     }
 321 
 322 private:
 323     ALWAYS_INLINE void validateInst(Inst&amp; inst)
 324     {
 325         if (!ASSERT_DISABLED) {
 326             if (!inst.isValidForm()) {
 327                 dataLogLn(inst);
 328                 CRASH();
 329             }
 330         }
 331     }
 332 
 333     static Arg extractArg(const TypedTmp&amp; tmp) { return tmp.tmp(); }
 334     static Arg extractArg(const Tmp&amp; tmp) { return Arg(tmp); }
 335     static Arg extractArg(const Arg&amp; arg) { return arg; }
 336 
 337     template&lt;typename... Arguments&gt;
 338     void append(BasicBlock* block, Kind kind, Arguments&amp;&amp;... arguments)
 339     {
 340         // FIXME: Find a way to use origin here.
 341         auto&amp; inst = block-&gt;append(kind, nullptr, extractArg(arguments)...);
 342         validateInst(inst);
 343     }
 344 
 345     template&lt;typename... Arguments&gt;
 346     void append(Kind kind, Arguments&amp;&amp;... arguments)
 347     {
 348         append(m_currentBlock, kind, std::forward&lt;Arguments&gt;(arguments)...);
 349     }
 350 
 351     template&lt;typename... Arguments&gt;
 352     void appendEffectful(B3::Air::Opcode op, Arguments&amp;&amp;... arguments)
 353     {
 354         Kind kind = op;
 355         kind.effects = true;
 356         append(m_currentBlock, kind, std::forward&lt;Arguments&gt;(arguments)...);
 357     }
 358 
 359     Tmp newTmp(B3::Bank bank)
 360     {
 361         switch (bank) {
 362         case B3::GP:
 363             if (m_freeGPs.size())
 364                 return m_freeGPs.takeLast();
 365             break;
 366         case B3::FP:
 367             if (m_freeFPs.size())
 368                 return m_freeFPs.takeLast();
 369             break;
 370         }
 371         return m_code.newTmp(bank);
 372     }
 373 
 374     TypedTmp g32() { return { newTmp(B3::GP), Type::I32 }; }
 375     TypedTmp g64() { return { newTmp(B3::GP), Type::I64 }; }
 376     TypedTmp gAnyref() { return { newTmp(B3::GP), Type::Anyref }; }
 377     TypedTmp gFuncref() { return { newTmp(B3::GP), Type::Funcref }; }
 378     TypedTmp f32() { return { newTmp(B3::FP), Type::F32 }; }
 379     TypedTmp f64() { return { newTmp(B3::FP), Type::F64 }; }
 380 
 381     TypedTmp tmpForType(Type type)
 382     {
 383         switch (type) {
 384         case Type::I32:
 385             return g32();
 386         case Type::I64:
 387             return g64();
 388         case Type::Funcref:
 389             return gFuncref();
 390         case Type::Anyref:
 391             return gAnyref();
 392         case Type::F32:
 393             return f32();
 394         case Type::F64:
 395             return f64();
 396         case Type::Void:
 397             return { };
 398         default:
 399             RELEASE_ASSERT_NOT_REACHED();
 400         }
 401     }
 402 
 403     B3::PatchpointValue* addPatchpoint(B3::Type type)
 404     {
 405         auto* result = m_proc.add&lt;B3::PatchpointValue&gt;(type, B3::Origin());
 406         if (UNLIKELY(shouldDumpIRAtEachPhase(B3::AirMode)))
 407             m_patchpoints.add(result);
 408         return result;
 409     }
 410 
 411     template &lt;typename ...Args&gt;
 412     void emitPatchpoint(B3::PatchpointValue* patch, Tmp result, Args... theArgs)
 413     {
 414         emitPatchpoint(m_currentBlock, patch, result, std::forward&lt;Args&gt;(theArgs)...);
 415     }
 416 
 417     template &lt;typename ...Args&gt;
 418     void emitPatchpoint(BasicBlock* basicBlock, B3::PatchpointValue* patch, Tmp result, Args... theArgs)
 419     {
 420         emitPatchpoint(basicBlock, patch, result, Vector&lt;ConstrainedTmp, sizeof...(Args)&gt;::from(theArgs...));
 421     }
 422 
 423     void emitPatchpoint(BasicBlock* basicBlock, B3::PatchpointValue* patch, Tmp result)
 424     {
 425         emitPatchpoint(basicBlock, patch, result, Vector&lt;ConstrainedTmp&gt;());
 426     }
 427 
 428     template &lt;size_t inlineSize&gt;
 429     void emitPatchpoint(BasicBlock* basicBlock, B3::PatchpointValue* patch, Tmp result, Vector&lt;ConstrainedTmp, inlineSize&gt;&amp;&amp; args)
 430     {
 431         if (!m_patchpointSpecial)
 432             m_patchpointSpecial = static_cast&lt;B3::PatchpointSpecial*&gt;(m_code.addSpecial(makeUnique&lt;B3::PatchpointSpecial&gt;()));
 433 
 434         Inst inst(Patch, patch, Arg::special(m_patchpointSpecial));
 435         Inst resultMov;
 436         if (result) {
 437             ASSERT(patch-&gt;type() != B3::Void);
 438             switch (patch-&gt;resultConstraints[0].kind()) {
 439             case B3::ValueRep::Register:
 440                 inst.args.append(Tmp(patch-&gt;resultConstraints[0].reg()));
 441                 resultMov = Inst(result.isGP() ? Move : MoveDouble, nullptr, Tmp(patch-&gt;resultConstraints[0].reg()), result);
 442                 break;
 443             case B3::ValueRep::SomeRegister:
 444                 inst.args.append(result);
 445                 break;
 446             default:
 447                 RELEASE_ASSERT_NOT_REACHED();
 448             }
 449         } else
 450             ASSERT(patch-&gt;type() == B3::Void);
 451 
 452         for (ConstrainedTmp&amp; tmp : args) {
 453             // FIXME: This is less than ideal to create dummy values just to satisfy Air&#39;s
 454             // validation. We should abstrcat Patch enough so ValueRep&#39;s don&#39;t need to be
 455             // backed by Values.
 456             // https://bugs.webkit.org/show_bug.cgi?id=194040
 457             B3::Value* dummyValue = m_proc.addConstant(B3::Origin(), tmp.tmp.isGP() ? B3::Int64 : B3::Double, 0);
 458             patch-&gt;append(dummyValue, tmp.rep);
 459             switch (tmp.rep.kind()) {
 460             case B3::ValueRep::ColdAny: // B3::Value propagates ColdAny information and later Air will allocate appropriate stack.
 461             case B3::ValueRep::SomeRegister:
 462                 inst.args.append(tmp.tmp);
 463                 break;
 464             case B3::ValueRep::Register:
 465                 patch-&gt;earlyClobbered().clear(tmp.rep.reg());
 466                 append(basicBlock, tmp.tmp.isGP() ? Move : MoveDouble, tmp.tmp, tmp.rep.reg());
 467                 inst.args.append(Tmp(tmp.rep.reg()));
 468                 break;
 469             case B3::ValueRep::StackArgument: {
 470                 auto arg = Arg::callArg(tmp.rep.offsetFromSP());
 471                 append(basicBlock, tmp.tmp.isGP() ? Move : MoveDouble, tmp.tmp, arg);
 472                 inst.args.append(arg);
 473                 break;
 474             }
 475             default:
 476                 RELEASE_ASSERT_NOT_REACHED();
 477             }
 478         }
 479 
 480         if (patch-&gt;resultConstraints[0].isReg())
 481             patch-&gt;lateClobbered().clear(patch-&gt;resultConstraints[0].reg());
 482         for (unsigned i = patch-&gt;numGPScratchRegisters; i--;)
 483             inst.args.append(g64().tmp());
 484         for (unsigned i = patch-&gt;numFPScratchRegisters; i--;)
 485             inst.args.append(f64().tmp());
 486 
 487         validateInst(inst);
 488         basicBlock-&gt;append(WTFMove(inst));
 489         if (resultMov) {
 490             validateInst(resultMov);
 491             basicBlock-&gt;append(WTFMove(resultMov));
 492         }
 493     }
 494 
 495     template &lt;typename Branch, typename Generator&gt;
 496     void emitCheck(const Branch&amp; makeBranch, const Generator&amp; generator)
 497     {
 498         // We fail along the truthy edge of &#39;branch&#39;.
 499         Inst branch = makeBranch();
 500 
 501         // FIXME: Make a hashmap of these.
 502         B3::CheckSpecial::Key key(branch);
 503         B3::CheckSpecial* special = static_cast&lt;B3::CheckSpecial*&gt;(m_code.addSpecial(makeUnique&lt;B3::CheckSpecial&gt;(key)));
 504 
 505         // FIXME: Remove the need for dummy values
 506         // https://bugs.webkit.org/show_bug.cgi?id=194040
 507         B3::Value* dummyPredicate = m_proc.addConstant(B3::Origin(), B3::Int32, 42);
 508         B3::CheckValue* checkValue = m_proc.add&lt;B3::CheckValue&gt;(B3::Check, B3::Origin(), dummyPredicate);
 509         checkValue-&gt;setGenerator(generator);
 510 
 511         Inst inst(Patch, checkValue, Arg::special(special));
 512         inst.args.appendVector(branch.args);
 513         m_currentBlock-&gt;append(WTFMove(inst));
 514     }
 515 
 516     template &lt;typename Func, typename ...Args&gt;
 517     void emitCCall(Func func, TypedTmp result, Args... args)
 518     {
 519         emitCCall(m_currentBlock, func, result, std::forward&lt;Args&gt;(args)...);
 520     }
 521     template &lt;typename Func, typename ...Args&gt;
 522     void emitCCall(BasicBlock* block, Func func, TypedTmp result, Args... theArgs)
 523     {
 524         B3::Type resultType = B3::Void;
 525         if (result) {
 526             switch (result.type()) {
 527             case Type::I32:
 528                 resultType = B3::Int32;
 529                 break;
 530             case Type::I64:
 531             case Type::Anyref:
 532             case Type::Funcref:
 533                 resultType = B3::Int64;
 534                 break;
 535             case Type::F32:
 536                 resultType = B3::Float;
 537                 break;
 538             case Type::F64:
 539                 resultType = B3::Double;
 540                 break;
 541             default:
 542                 RELEASE_ASSERT_NOT_REACHED();
 543             }
 544         }
 545 
 546         auto makeDummyValue = [&amp;] (Tmp tmp) {
 547             // FIXME: This is less than ideal to create dummy values just to satisfy Air&#39;s
 548             // validation. We should abstrcat CCall enough so we&#39;re not reliant on arguments
 549             // to the B3::CCallValue.
 550             // https://bugs.webkit.org/show_bug.cgi?id=194040
 551             if (tmp.isGP())
 552                 return m_proc.addConstant(B3::Origin(), B3::Int64, 0);
 553             return m_proc.addConstant(B3::Origin(), B3::Double, 0);
 554         };
 555 
 556         B3::Value* dummyFunc = m_proc.addConstant(B3::Origin(), B3::Int64, bitwise_cast&lt;uintptr_t&gt;(func));
 557         B3::Value* origin = m_proc.add&lt;B3::CCallValue&gt;(resultType, B3::Origin(), B3::Effects::none(), dummyFunc, makeDummyValue(theArgs)...);
 558 
 559         Inst inst(CCall, origin);
 560 
 561         Tmp callee = g64();
 562         append(block, Move, Arg::immPtr(tagCFunctionPtr&lt;void*&gt;(func, B3CCallPtrTag)), callee);
 563         inst.args.append(callee);
 564 
 565         if (result)
 566             inst.args.append(result.tmp());
 567 
 568         for (Tmp tmp : Vector&lt;Tmp, sizeof...(Args)&gt;::from(theArgs.tmp()...))
 569             inst.args.append(tmp);
 570 
 571         block-&gt;append(WTFMove(inst));
 572     }
 573 
 574     static B3::Air::Opcode moveOpForValueType(Type type)
 575     {
 576         switch (type) {
 577         case Type::I32:
 578             return Move32;
 579         case Type::I64:
 580         case Type::Anyref:
 581         case Type::Funcref:
 582             return Move;
 583         case Type::F32:
 584             return MoveFloat;
 585         case Type::F64:
 586             return MoveDouble;
 587         default:
 588             RELEASE_ASSERT_NOT_REACHED();
 589         }
 590     }
 591 
 592     void emitThrowException(CCallHelpers&amp;, ExceptionType);
 593 
 594     void emitEntryTierUpCheck(int32_t incrementCount, B3::Origin);
 595     void emitLoopTierUpCheck(int32_t incrementCount, const Stack&amp;, uint32_t, uint32_t, B3::Origin);
 596 
 597     void emitWriteBarrierForJSWrapper();
 598     ExpressionType emitCheckAndPreparePointer(ExpressionType pointer, uint32_t offset, uint32_t sizeOfOp);
 599     ExpressionType emitLoadOp(LoadOpType, ExpressionType pointer, uint32_t offset);
 600     void emitStoreOp(StoreOpType, ExpressionType pointer, ExpressionType value, uint32_t offset);
 601 
 602     void unify(const ExpressionType&amp; dst, const ExpressionType&amp; source);
 603     void unifyValuesWithBlock(const Stack&amp; resultStack, const ResultList&amp; stack);
 604 
 605     template &lt;typename IntType&gt;
 606     void emitChecksForModOrDiv(bool isSignedDiv, ExpressionType left, ExpressionType right);
 607 
 608     template &lt;typename IntType&gt;
 609     void emitModOrDiv(bool isDiv, ExpressionType lhs, ExpressionType rhs, ExpressionType&amp; result);
 610 
 611     enum class MinOrMax { Min, Max };
 612 
 613     PartialResult addFloatingPointMinOrMax(Type, MinOrMax, ExpressionType lhs, ExpressionType rhs, ExpressionType&amp; result);
 614 
 615     int32_t WARN_UNUSED_RETURN fixupPointerPlusOffset(ExpressionType&amp;, uint32_t);
 616 
 617     void restoreWasmContextInstance(BasicBlock*, TypedTmp);
 618     enum class RestoreCachedStackLimit { No, Yes };
 619     void restoreWebAssemblyGlobalState(RestoreCachedStackLimit, const MemoryInformation&amp;, TypedTmp instance, BasicBlock*);
 620 
 621     B3::Origin origin();
 622 
 623     uint32_t outerLoopIndex() const
 624     {
 625         if (m_outerLoops.isEmpty())
 626             return UINT32_MAX;
 627         return m_outerLoops.last();
 628     }
 629 
 630     FunctionParser&lt;AirIRGenerator&gt;* m_parser { nullptr };
 631     const ModuleInformation&amp; m_info;
 632     const MemoryMode m_mode { MemoryMode::BoundsChecking };
 633     const unsigned m_functionIndex { UINT_MAX };
 634     TierUpCount* m_tierUp { nullptr };
 635 
 636     B3::Procedure&amp; m_proc;
 637     Code&amp; m_code;
 638     Vector&lt;uint32_t&gt; m_outerLoops;
 639     BasicBlock* m_currentBlock { nullptr };
 640     BasicBlock* m_rootBlock { nullptr };
 641     Vector&lt;TypedTmp&gt; m_locals;
 642     Vector&lt;UnlinkedWasmToWasmCall&gt;&amp; m_unlinkedWasmToWasmCalls; // List each call site and the function index whose address it should be patched with.
 643     GPRReg m_memoryBaseGPR { InvalidGPRReg };
 644     GPRReg m_memorySizeGPR { InvalidGPRReg };
 645     GPRReg m_wasmContextInstanceGPR { InvalidGPRReg };
 646     bool m_makesCalls { false };
 647 
 648     Vector&lt;Tmp, 8&gt; m_freeGPs;
 649     Vector&lt;Tmp, 8&gt; m_freeFPs;
 650 
 651     // This is only filled if we are dumping IR.
 652     Bag&lt;B3::PatchpointValue*&gt; m_patchpoints;
 653 
 654     TypedTmp m_instanceValue; // Always use the accessor below to ensure the instance value is materialized when used.
 655     bool m_usesInstanceValue { false };
 656     TypedTmp instanceValue()
 657     {
 658         m_usesInstanceValue = true;
 659         return m_instanceValue;
 660     }
 661 
 662     uint32_t m_maxNumJSCallArguments { 0 };
 663     unsigned m_numImportFunctions;
 664 
 665     B3::PatchpointSpecial* m_patchpointSpecial { nullptr };
 666 };
 667 
 668 // Memory accesses in WebAssembly have unsigned 32-bit offsets, whereas they have signed 32-bit offsets in B3.
 669 int32_t AirIRGenerator::fixupPointerPlusOffset(ExpressionType&amp; ptr, uint32_t offset)
 670 {
 671     if (static_cast&lt;uint64_t&gt;(offset) &gt; static_cast&lt;uint64_t&gt;(std::numeric_limits&lt;int32_t&gt;::max())) {
 672         auto previousPtr = ptr;
 673         ptr = g64();
 674         auto constant = g64();
 675         append(Move, Arg::bigImm(offset), constant);
 676         append(Add64, constant, previousPtr, ptr);
 677         return 0;
 678     }
 679     return offset;
 680 }
 681 
 682 void AirIRGenerator::restoreWasmContextInstance(BasicBlock* block, TypedTmp instance)
 683 {
 684     if (Context::useFastTLS()) {
 685         auto* patchpoint = addPatchpoint(B3::Void);
 686         if (CCallHelpers::storeWasmContextInstanceNeedsMacroScratchRegister())
 687             patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
 688         patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
 689             AllowMacroScratchRegisterUsageIf allowScratch(jit, CCallHelpers::storeWasmContextInstanceNeedsMacroScratchRegister());
 690             jit.storeWasmContextInstance(params[0].gpr());
 691         });
 692         emitPatchpoint(block, patchpoint, Tmp(), instance);
 693         return;
 694     }
 695 
 696     // FIXME: Because WasmToWasm call clobbers wasmContextInstance register and does not restore it, we need to restore it in the caller side.
 697     // This prevents us from using ArgumentReg to this (logically) immutable pinned register.
 698     auto* patchpoint = addPatchpoint(B3::Void);
 699     B3::Effects effects = B3::Effects::none();
 700     effects.writesPinned = true;
 701     effects.reads = B3::HeapRange::top();
 702     patchpoint-&gt;effects = effects;
 703     patchpoint-&gt;clobberLate(RegisterSet(m_wasmContextInstanceGPR));
 704     GPRReg wasmContextInstanceGPR = m_wasmContextInstanceGPR;
 705     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; param) {
 706         jit.move(param[0].gpr(), wasmContextInstanceGPR);
 707     });
 708     emitPatchpoint(block, patchpoint, Tmp(), instance);
 709 }
 710 
 711 AirIRGenerator::AirIRGenerator(const ModuleInformation&amp; info, B3::Procedure&amp; procedure, InternalFunction* compilation, Vector&lt;UnlinkedWasmToWasmCall&gt;&amp; unlinkedWasmToWasmCalls, MemoryMode mode, unsigned functionIndex, TierUpCount* tierUp, ThrowWasmException throwWasmException, const Signature&amp; signature)
 712     : m_info(info)
 713     , m_mode(mode)
 714     , m_functionIndex(functionIndex)
 715     , m_tierUp(tierUp)
 716     , m_proc(procedure)
 717     , m_code(m_proc.code())
 718     , m_unlinkedWasmToWasmCalls(unlinkedWasmToWasmCalls)
 719     , m_numImportFunctions(info.importFunctionCount())
 720 {
 721     m_currentBlock = m_code.addBlock();
 722     m_rootBlock = m_currentBlock;
 723 
 724     // FIXME we don&#39;t really need to pin registers here if there&#39;s no memory. It makes wasm -&gt; wasm thunks simpler for now. https://bugs.webkit.org/show_bug.cgi?id=166623
 725     const PinnedRegisterInfo&amp; pinnedRegs = PinnedRegisterInfo::get();
 726 
 727     m_memoryBaseGPR = pinnedRegs.baseMemoryPointer;
 728     m_code.pinRegister(m_memoryBaseGPR);
 729 
 730     m_wasmContextInstanceGPR = pinnedRegs.wasmContextInstancePointer;
 731     if (!Context::useFastTLS())
 732         m_code.pinRegister(m_wasmContextInstanceGPR);
 733 
 734     if (mode != MemoryMode::Signaling) {
 735         m_memorySizeGPR = pinnedRegs.sizeRegister;
 736         m_code.pinRegister(m_memorySizeGPR);
 737     }
 738 
 739     if (throwWasmException)
 740         Thunks::singleton().setThrowWasmException(throwWasmException);
 741 
 742     if (info.memory) {
 743         switch (m_mode) {
 744         case MemoryMode::BoundsChecking:
 745             break;
 746         case MemoryMode::Signaling:
 747             // Most memory accesses in signaling mode don&#39;t do an explicit
 748             // exception check because they can rely on fault handling to detect
 749             // out-of-bounds accesses. FaultSignalHandler nonetheless needs the
 750             // thunk to exist so that it can jump to that thunk.
 751             if (UNLIKELY(!Thunks::singleton().stub(throwExceptionFromWasmThunkGenerator)))
 752                 CRASH();
 753             break;
 754         }
 755     }
 756 
 757     m_code.setNumEntrypoints(1);
 758 
 759     GPRReg contextInstance = Context::useFastTLS() ? wasmCallingConventionAir().prologueScratch(1) : m_wasmContextInstanceGPR;
 760 
 761     Ref&lt;B3::Air::PrologueGenerator&gt; prologueGenerator = createSharedTask&lt;B3::Air::PrologueGeneratorFunction&gt;([=] (CCallHelpers&amp; jit, B3::Air::Code&amp; code) {
 762         AllowMacroScratchRegisterUsage allowScratch(jit);
 763         code.emitDefaultPrologue(jit);
 764 
 765         {
 766             GPRReg calleeGPR = wasmCallingConventionAir().prologueScratch(0);
 767             auto moveLocation = jit.moveWithPatch(MacroAssembler::TrustedImmPtr(nullptr), calleeGPR);
 768             jit.addLinkTask([compilation, moveLocation] (LinkBuffer&amp; linkBuffer) {
 769                 compilation-&gt;calleeMoveLocation = linkBuffer.locationOf&lt;WasmEntryPtrTag&gt;(moveLocation);
 770             });
 771             jit.emitPutToCallFrameHeader(calleeGPR, CallFrameSlot::callee);
 772             jit.emitPutToCallFrameHeader(nullptr, CallFrameSlot::codeBlock);
 773         }
 774 
 775         {
 776             const Checked&lt;int32_t&gt; wasmFrameSize = m_code.frameSize();
 777             const unsigned minimumParentCheckSize = WTF::roundUpToMultipleOf(stackAlignmentBytes(), 1024);
 778             const unsigned extraFrameSize = WTF::roundUpToMultipleOf(stackAlignmentBytes(), std::max&lt;uint32_t&gt;(
 779                 // This allows us to elide stack checks for functions that are terminal nodes in the call
 780                 // tree, (e.g they don&#39;t make any calls) and have a small enough frame size. This works by
 781                 // having any such terminal node have its parent caller include some extra size in its
 782                 // own check for it. The goal here is twofold:
 783                 // 1. Emit less code.
 784                 // 2. Try to speed things up by skipping stack checks.
 785                 minimumParentCheckSize,
 786                 // This allows us to elide stack checks in the Wasm -&gt; Embedder call IC stub. Since these will
 787                 // spill all arguments to the stack, we ensure that a stack check here covers the
 788                 // stack that such a stub would use.
 789                 (Checked&lt;uint32_t&gt;(m_maxNumJSCallArguments) * sizeof(Register) + jscCallingConvention().headerSizeInBytes()).unsafeGet()
 790             ));
 791             const int32_t checkSize = m_makesCalls ? (wasmFrameSize + extraFrameSize).unsafeGet() : wasmFrameSize.unsafeGet();
 792             bool needUnderflowCheck = static_cast&lt;unsigned&gt;(checkSize) &gt; Options::reservedZoneSize();
 793             bool needsOverflowCheck = m_makesCalls || wasmFrameSize &gt;= minimumParentCheckSize || needUnderflowCheck;
 794 
 795             // This allows leaf functions to not do stack checks if their frame size is within
 796             // certain limits since their caller would have already done the check.
 797             if (needsOverflowCheck) {
 798                 GPRReg scratch = wasmCallingConventionAir().prologueScratch(0);
 799 
 800                 if (Context::useFastTLS())
 801                     jit.loadWasmContextInstance(contextInstance);
 802 
 803                 jit.addPtr(CCallHelpers::TrustedImm32(-checkSize), GPRInfo::callFrameRegister, scratch);
 804                 MacroAssembler::JumpList overflow;
 805                 if (UNLIKELY(needUnderflowCheck))
 806                     overflow.append(jit.branchPtr(CCallHelpers::Above, scratch, GPRInfo::callFrameRegister));
 807                 overflow.append(jit.branchPtr(CCallHelpers::Below, scratch, CCallHelpers::Address(contextInstance, Instance::offsetOfCachedStackLimit())));
 808                 jit.addLinkTask([overflow] (LinkBuffer&amp; linkBuffer) {
 809                     linkBuffer.link(overflow, CodeLocationLabel&lt;JITThunkPtrTag&gt;(Thunks::singleton().stub(throwStackOverflowFromWasmThunkGenerator).code()));
 810                 });
 811             } else if (m_usesInstanceValue &amp;&amp; Context::useFastTLS()) {
 812                 // No overflow check is needed, but the instance values still needs to be correct.
 813                 jit.loadWasmContextInstance(contextInstance);
 814             }
 815         }
 816     });
 817 
 818     m_code.setPrologueForEntrypoint(0, WTFMove(prologueGenerator));
 819 
 820     if (Context::useFastTLS()) {
 821         m_instanceValue = g64();
 822         // FIXME: Would be nice to only do this if we use instance value.
 823         append(Move, Tmp(contextInstance), m_instanceValue);
 824     } else
 825         m_instanceValue = { Tmp(contextInstance), Type::I64 };
 826 
 827     ASSERT(!m_locals.size());
 828     m_locals.grow(signature.argumentCount());
 829     for (unsigned i = 0; i &lt; signature.argumentCount(); ++i) {
 830         Type type = signature.argument(i);
 831         m_locals[i] = tmpForType(type);
 832     }
 833 
 834     wasmCallingConventionAir().loadArguments(signature, [&amp;] (const Arg&amp; arg, unsigned i) {
 835         switch (signature.argument(i)) {
 836         case Type::I32:
 837             append(Move32, arg, m_locals[i]);
 838             break;
 839         case Type::I64:
 840         case Type::Anyref:
 841         case Type::Funcref:
 842             append(Move, arg, m_locals[i]);
 843             break;
 844         case Type::F32:
 845             append(MoveFloat, arg, m_locals[i]);
 846             break;
 847         case Type::F64:
 848             append(MoveDouble, arg, m_locals[i]);
 849             break;
 850         default:
 851             RELEASE_ASSERT_NOT_REACHED();
 852         }
 853     });
 854 
 855     emitEntryTierUpCheck(TierUpCount::functionEntryIncrement(), B3::Origin());
 856 }
 857 
 858 void AirIRGenerator::restoreWebAssemblyGlobalState(RestoreCachedStackLimit restoreCachedStackLimit, const MemoryInformation&amp; memory, TypedTmp instance, BasicBlock* block)
 859 {
 860     restoreWasmContextInstance(block, instance);
 861 
 862     if (restoreCachedStackLimit == RestoreCachedStackLimit::Yes) {
 863         // The Instance caches the stack limit, but also knows where its canonical location is.
 864         static_assert(sizeof(decltype(static_cast&lt;Instance*&gt;(nullptr)-&gt;cachedStackLimit())) == sizeof(uint64_t), &quot;&quot;);
 865 
 866         RELEASE_ASSERT(Arg::isValidAddrForm(Instance::offsetOfPointerToActualStackLimit(), B3::Width64));
 867         RELEASE_ASSERT(Arg::isValidAddrForm(Instance::offsetOfCachedStackLimit(), B3::Width64));
 868         auto temp = g64();
 869         append(block, Move, Arg::addr(instanceValue(), Instance::offsetOfPointerToActualStackLimit()), temp);
 870         append(block, Move, Arg::addr(temp), temp);
 871         append(block, Move, temp, Arg::addr(instanceValue(), Instance::offsetOfCachedStackLimit()));
 872     }
 873 
 874     if (!!memory) {
 875         const PinnedRegisterInfo* pinnedRegs = &amp;PinnedRegisterInfo::get();
 876         RegisterSet clobbers;
 877         clobbers.set(pinnedRegs-&gt;baseMemoryPointer);
 878         clobbers.set(pinnedRegs-&gt;sizeRegister);
 879         if (!isARM64())
 880             clobbers.set(RegisterSet::macroScratchRegisters());
 881 
 882         auto* patchpoint = addPatchpoint(B3::Void);
 883         B3::Effects effects = B3::Effects::none();
 884         effects.writesPinned = true;
 885         effects.reads = B3::HeapRange::top();
 886         patchpoint-&gt;effects = effects;
 887         patchpoint-&gt;clobber(clobbers);
 888         patchpoint-&gt;numGPScratchRegisters = Gigacage::isEnabled(Gigacage::Primitive) ? 1 : 0;
 889 
 890         patchpoint-&gt;setGenerator([pinnedRegs] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
 891             AllowMacroScratchRegisterUsage allowScratch(jit);
 892             GPRReg baseMemory = pinnedRegs-&gt;baseMemoryPointer;
 893             GPRReg scratchOrSize = Gigacage::isEnabled(Gigacage::Primitive) ? params.gpScratch(0) : pinnedRegs-&gt;sizeRegister;
 894 
 895             jit.loadPtr(CCallHelpers::Address(params[0].gpr(), Instance::offsetOfCachedMemorySize()), pinnedRegs-&gt;sizeRegister);
 896             jit.loadPtr(CCallHelpers::Address(params[0].gpr(), Instance::offsetOfCachedMemory()), baseMemory);
 897 
 898             jit.cageConditionally(Gigacage::Primitive, baseMemory, pinnedRegs-&gt;sizeRegister, scratchOrSize);
 899         });
 900 
 901         emitPatchpoint(block, patchpoint, Tmp(), instance);
 902     }
 903 }
 904 
 905 void AirIRGenerator::emitThrowException(CCallHelpers&amp; jit, ExceptionType type)
 906 {
 907     jit.move(CCallHelpers::TrustedImm32(static_cast&lt;uint32_t&gt;(type)), GPRInfo::argumentGPR1);
 908     auto jumpToExceptionStub = jit.jump();
 909 
 910     jit.addLinkTask([jumpToExceptionStub] (LinkBuffer&amp; linkBuffer) {
 911         linkBuffer.link(jumpToExceptionStub, CodeLocationLabel&lt;JITThunkPtrTag&gt;(Thunks::singleton().stub(throwExceptionFromWasmThunkGenerator).code()));
 912     });
 913 }
 914 
 915 auto AirIRGenerator::addLocal(Type type, uint32_t count) -&gt; PartialResult
 916 {
 917     size_t newSize = m_locals.size() + count;
 918     ASSERT(!(CheckedUint32(count) + m_locals.size()).hasOverflowed());
 919     ASSERT(newSize &lt;= maxFunctionLocals);
 920     WASM_COMPILE_FAIL_IF(!m_locals.tryReserveCapacity(newSize), &quot;can&#39;t allocate memory for &quot;, newSize, &quot; locals&quot;);
 921 
 922     for (uint32_t i = 0; i &lt; count; ++i) {
 923         auto local = tmpForType(type);
 924         m_locals.uncheckedAppend(local);
 925         switch (type) {
 926         case Type::Anyref:
 927         case Type::Funcref:
 928             append(Move, Arg::imm(JSValue::encode(jsNull())), local);
 929             break;
 930         case Type::I32:
 931         case Type::I64: {
 932             append(Xor64, local, local);
 933             break;
 934         }
 935         case Type::F32:
 936         case Type::F64: {
 937             auto temp = g64();
 938             // IEEE 754 &quot;0&quot; is just int32/64 zero.
 939             append(Xor64, temp, temp);
 940             append(type == Type::F32 ? Move32ToFloat : Move64ToDouble, temp, local);
 941             break;
 942         }
 943         default:
 944             RELEASE_ASSERT_NOT_REACHED();
 945         }
 946     }
 947     return { };
 948 }
 949 
 950 auto AirIRGenerator::addConstant(Type type, uint64_t value) -&gt; ExpressionType
 951 {
 952     return addConstant(m_currentBlock, type, value);
 953 }
 954 
 955 auto AirIRGenerator::addConstant(BasicBlock* block, Type type, uint64_t value) -&gt; ExpressionType
 956 {
 957     auto result = tmpForType(type);
 958     switch (type) {
 959     case Type::I32:
 960     case Type::I64:
 961     case Type::Anyref:
 962     case Type::Funcref:
 963         append(block, Move, Arg::bigImm(value), result);
 964         break;
 965     case Type::F32:
 966     case Type::F64: {
 967         auto tmp = g64();
 968         append(block, Move, Arg::bigImm(value), tmp);
 969         append(block, type == Type::F32 ? Move32ToFloat : Move64ToDouble, tmp, result);
 970         break;
 971     }
 972 
 973     default:
 974         RELEASE_ASSERT_NOT_REACHED();
 975     }
 976 
 977     return result;
 978 }
 979 
 980 auto AirIRGenerator::addArguments(const Signature&amp; signature) -&gt; PartialResult
 981 {
 982     RELEASE_ASSERT(m_locals.size() == signature.argumentCount()); // We handle arguments in the prologue
 983     return { };
 984 }
 985 
 986 auto AirIRGenerator::addRefIsNull(ExpressionType&amp; value, ExpressionType&amp; result) -&gt; PartialResult
 987 {
 988     ASSERT(value.tmp());
 989     result = tmpForType(Type::I32);
 990     auto tmp = g64();
 991 
 992     append(Move, Arg::bigImm(JSValue::encode(jsNull())), tmp);
 993     append(Compare64, Arg::relCond(MacroAssembler::Equal), value, tmp, result);
 994 
 995     return { };
 996 }
 997 
 998 auto AirIRGenerator::addRefFunc(uint32_t index, ExpressionType&amp; result) -&gt; PartialResult
 999 {
1000     // FIXME: Emit this inline &lt;https://bugs.webkit.org/show_bug.cgi?id=198506&gt;.
1001     result = tmpForType(Type::Funcref);
1002     emitCCall(&amp;doWasmRefFunc, result, instanceValue(), addConstant(Type::I32, index));
1003 
1004     return { };
1005 }
1006 
1007 auto AirIRGenerator::addTableGet(unsigned tableIndex, ExpressionType&amp; index, ExpressionType&amp; result) -&gt; PartialResult
1008 {
1009     // FIXME: Emit this inline &lt;https://bugs.webkit.org/show_bug.cgi?id=198506&gt;.
1010     ASSERT(index.tmp());
1011     ASSERT(index.type() == Type::I32);
1012     result = tmpForType(m_info.tables[tableIndex].wasmType());
1013 
1014     emitCCall(&amp;getWasmTableElement, result, instanceValue(), addConstant(Type::I32, tableIndex), index);
1015     emitCheck([&amp;] {
1016         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::Zero), result, result);
1017     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1018         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTableAccess);
1019     });
1020 
1021     return { };
1022 }
1023 
1024 auto AirIRGenerator::addTableSet(unsigned tableIndex, ExpressionType&amp; index, ExpressionType&amp; value) -&gt; PartialResult
1025 {
1026     // FIXME: Emit this inline &lt;https://bugs.webkit.org/show_bug.cgi?id=198506&gt;.
1027     ASSERT(index.tmp());
1028     ASSERT(index.type() == Type::I32);
1029     ASSERT(value.tmp());
1030 
1031     auto shouldThrow = g32();
1032     emitCCall(&amp;setWasmTableElement, shouldThrow, instanceValue(), addConstant(Type::I32, tableIndex), index, value);
1033 
1034     emitCheck([&amp;] {
1035         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::Zero), shouldThrow, shouldThrow);
1036     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1037         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTableAccess);
1038     });
1039 
1040     return { };
1041 }
1042 
1043 auto AirIRGenerator::addTableSize(unsigned tableIndex, ExpressionType&amp; result) -&gt; PartialResult
1044 {
1045     // FIXME: Emit this inline &lt;https://bugs.webkit.org/show_bug.cgi?id=198506&gt;.
1046     result = tmpForType(Type::I32);
1047 
1048     int32_t (*doSize)(Instance*, unsigned) = [] (Instance* instance, unsigned tableIndex) -&gt; int32_t {
1049         return instance-&gt;table(tableIndex)-&gt;length();
1050     };
1051 
1052     emitCCall(doSize, result, instanceValue(), addConstant(Type::I32, tableIndex));
1053 
1054     return { };
1055 }
1056 
1057 auto AirIRGenerator::addTableGrow(unsigned tableIndex, ExpressionType&amp; fill, ExpressionType&amp; delta, ExpressionType&amp; result) -&gt; PartialResult
1058 {
1059     ASSERT(fill.tmp());
1060     ASSERT(isSubtype(fill.type(), m_info.tables[tableIndex].wasmType()));
1061     ASSERT(delta.tmp());
1062     ASSERT(delta.type() == Type::I32);
1063     result = tmpForType(Type::I32);
1064 
1065     emitCCall(&amp;doWasmTableGrow, result, instanceValue(), addConstant(Type::I32, tableIndex), fill, delta);
1066 
1067     return { };
1068 }
1069 
1070 auto AirIRGenerator::addTableFill(unsigned tableIndex, ExpressionType&amp; offset, ExpressionType&amp; fill, ExpressionType&amp; count) -&gt; PartialResult
1071 {
1072     ASSERT(fill.tmp());
1073     ASSERT(isSubtype(fill.type(), m_info.tables[tableIndex].wasmType()));
1074     ASSERT(offset.tmp());
1075     ASSERT(offset.type() == Type::I32);
1076     ASSERT(count.tmp());
1077     ASSERT(count.type() == Type::I32);
1078 
1079     auto result = tmpForType(Type::I32);
1080     emitCCall(&amp;doWasmTableFill, result, instanceValue(), addConstant(Type::I32, tableIndex), offset, fill, count);
1081 
1082     emitCheck([&amp;] {
1083         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::Zero), result, result);
1084     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1085         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTableAccess);
1086     });
1087 
1088     return { };
1089 }
1090 
1091 auto AirIRGenerator::getLocal(uint32_t index, ExpressionType&amp; result) -&gt; PartialResult
1092 {
1093     ASSERT(m_locals[index].tmp());
1094     result = tmpForType(m_locals[index].type());
1095     append(moveOpForValueType(m_locals[index].type()), m_locals[index].tmp(), result);
1096     return { };
1097 }
1098 
1099 auto AirIRGenerator::addUnreachable() -&gt; PartialResult
1100 {
1101     B3::PatchpointValue* unreachable = addPatchpoint(B3::Void);
1102     unreachable-&gt;setGenerator([this] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1103         this-&gt;emitThrowException(jit, ExceptionType::Unreachable);
1104     });
1105     unreachable-&gt;effects.terminal = true;
1106     emitPatchpoint(unreachable, Tmp());
1107     return { };
1108 }
1109 
1110 auto AirIRGenerator::addGrowMemory(ExpressionType delta, ExpressionType&amp; result) -&gt; PartialResult
1111 {
1112     int32_t (*growMemory)(void*, Instance*, int32_t) = [] (void* callFrame, Instance* instance, int32_t delta) -&gt; int32_t {
1113         instance-&gt;storeTopCallFrame(callFrame);
1114 
1115         if (delta &lt; 0)
1116             return -1;
1117 
1118         auto grown = instance-&gt;memory()-&gt;grow(PageCount(delta));
1119         if (!grown) {
1120             switch (grown.error()) {
1121             case Memory::GrowFailReason::InvalidDelta:
1122             case Memory::GrowFailReason::InvalidGrowSize:
1123             case Memory::GrowFailReason::WouldExceedMaximum:
1124             case Memory::GrowFailReason::OutOfMemory:
1125                 return -1;
1126             }
1127             RELEASE_ASSERT_NOT_REACHED();
1128         }
1129 
1130         return grown.value().pageCount();
1131     };
1132 
1133     result = g32();
1134     emitCCall(growMemory, result, TypedTmp { Tmp(GPRInfo::callFrameRegister), Type::I64 }, instanceValue(), delta);
1135     restoreWebAssemblyGlobalState(RestoreCachedStackLimit::No, m_info.memory, instanceValue(), m_currentBlock);
1136 
1137     return { };
1138 }
1139 
1140 auto AirIRGenerator::addCurrentMemory(ExpressionType&amp; result) -&gt; PartialResult
1141 {
1142     static_assert(sizeof(decltype(static_cast&lt;Memory*&gt;(nullptr)-&gt;size())) == sizeof(uint64_t), &quot;codegen relies on this size&quot;);
1143 
1144     auto temp1 = g64();
1145     auto temp2 = g64();
1146 
1147     RELEASE_ASSERT(Arg::isValidAddrForm(Instance::offsetOfCachedMemorySize(), B3::Width64));
1148     append(Move, Arg::addr(instanceValue(), Instance::offsetOfCachedMemorySize()), temp1);
1149     constexpr uint32_t shiftValue = 16;
1150     static_assert(PageCount::pageSize == 1ull &lt;&lt; shiftValue, &quot;This must hold for the code below to be correct.&quot;);
1151     append(Move, Arg::imm(16), temp2);
1152     addShift(Type::I32, Urshift64, temp1, temp2, result);
1153     append(Move32, result, result);
1154 
1155     return { };
1156 }
1157 
1158 auto AirIRGenerator::setLocal(uint32_t index, ExpressionType value) -&gt; PartialResult
1159 {
1160     ASSERT(m_locals[index].tmp());
1161     append(moveOpForValueType(m_locals[index].type()), value, m_locals[index].tmp());
1162     return { };
1163 }
1164 
1165 auto AirIRGenerator::getGlobal(uint32_t index, ExpressionType&amp; result) -&gt; PartialResult
1166 {
1167     Type type = m_info.globals[index].type;
1168 
1169     result = tmpForType(type);
1170 
1171     auto temp = g64();
1172 
1173     RELEASE_ASSERT(Arg::isValidAddrForm(Instance::offsetOfGlobals(), B3::Width64));
1174     append(Move, Arg::addr(instanceValue(), Instance::offsetOfGlobals()), temp);
1175 
1176     int32_t offset = safeCast&lt;int32_t&gt;(index * sizeof(Register));
1177     if (Arg::isValidAddrForm(offset, B3::widthForType(toB3Type(type))))
1178         append(moveOpForValueType(type), Arg::addr(temp, offset), result);
1179     else {
1180         auto temp2 = g64();
1181         append(Move, Arg::bigImm(offset), temp2);
1182         append(Add64, temp2, temp, temp);
1183         append(moveOpForValueType(type), Arg::addr(temp), result);
1184     }
1185     return { };
1186 }
1187 
1188 auto AirIRGenerator::setGlobal(uint32_t index, ExpressionType value) -&gt; PartialResult
1189 {
1190     auto temp = g64();
1191 
1192     RELEASE_ASSERT(Arg::isValidAddrForm(Instance::offsetOfGlobals(), B3::Width64));
1193     append(Move, Arg::addr(instanceValue(), Instance::offsetOfGlobals()), temp);
1194 
1195     Type type = m_info.globals[index].type;
1196 
1197     int32_t offset = safeCast&lt;int32_t&gt;(index * sizeof(Register));
1198     if (Arg::isValidAddrForm(offset, B3::widthForType(toB3Type(type))))
1199         append(moveOpForValueType(type), value, Arg::addr(temp, offset));
1200     else {
1201         auto temp2 = g64();
1202         append(Move, Arg::bigImm(offset), temp2);
1203         append(Add64, temp2, temp, temp);
1204         append(moveOpForValueType(type), value, Arg::addr(temp));
1205     }
1206 
1207     if (isSubtype(type, Anyref))
1208         emitWriteBarrierForJSWrapper();
1209 
1210     return { };
1211 }
1212 
1213 inline void AirIRGenerator::emitWriteBarrierForJSWrapper()
1214 {
1215     auto cell = g64();
1216     auto vm = g64();
1217     auto cellState = g32();
1218     auto threshold = g32();
1219 
1220     BasicBlock* fenceCheckPath = m_code.addBlock();
1221     BasicBlock* fencePath = m_code.addBlock();
1222     BasicBlock* doSlowPath = m_code.addBlock();
1223     BasicBlock* continuation = m_code.addBlock();
1224 
1225     append(Move, Arg::addr(instanceValue(), Instance::offsetOfOwner()), cell);
1226     append(Move, Arg::addr(cell, JSWebAssemblyInstance::offsetOfVM()), vm);
1227     append(Load8, Arg::addr(cell, JSCell::cellStateOffset()), cellState);
1228     append(Move32, Arg::addr(vm, VM::offsetOfHeapBarrierThreshold()), threshold);
1229 
1230     append(Branch32, Arg::relCond(MacroAssembler::Above), cellState, threshold);
1231     m_currentBlock-&gt;setSuccessors(continuation, fenceCheckPath);
1232     m_currentBlock = fenceCheckPath;
1233 
1234     append(Load8, Arg::addr(vm, VM::offsetOfHeapMutatorShouldBeFenced()), threshold);
1235     append(BranchTest32, Arg::resCond(MacroAssembler::Zero), threshold, threshold);
1236     m_currentBlock-&gt;setSuccessors(doSlowPath, fencePath);
1237     m_currentBlock = fencePath;
1238 
1239     auto* doFence = addPatchpoint(B3::Void);
1240     doFence-&gt;setGenerator([] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1241         jit.memoryFence();
1242     });
1243     emitPatchpoint(doFence, Tmp());
1244 
1245     append(Load8, Arg::addr(cell, JSCell::cellStateOffset()), cellState);
1246     append(Branch32, Arg::relCond(MacroAssembler::Above), cellState, Arg::imm(blackThreshold));
1247     m_currentBlock-&gt;setSuccessors(continuation, doSlowPath);
1248     m_currentBlock = doSlowPath;
1249 
1250     void (*writeBarrier)(JSWebAssemblyInstance*, VM*) = [] (JSWebAssemblyInstance* cell, VM* vm) -&gt; void {
1251         ASSERT(cell);
1252         ASSERT(vm);
1253         vm-&gt;heap.writeBarrierSlowPath(cell);
1254     };
1255     emitCCall(writeBarrier, TypedTmp(), cell, vm);
1256     append(Jump);
1257     m_currentBlock-&gt;setSuccessors(continuation);
1258     m_currentBlock = continuation;
1259 }
1260 
1261 inline AirIRGenerator::ExpressionType AirIRGenerator::emitCheckAndPreparePointer(ExpressionType pointer, uint32_t offset, uint32_t sizeOfOperation)
1262 {
1263     ASSERT(m_memoryBaseGPR);
1264 
1265     auto result = g64();
1266     append(Move32, pointer, result);
1267 
1268     switch (m_mode) {
1269     case MemoryMode::BoundsChecking: {
1270         // We&#39;re not using signal handling at all, we must therefore check that no memory access exceeds the current memory size.
1271         ASSERT(m_memorySizeGPR);
1272         ASSERT(sizeOfOperation + offset &gt; offset);
1273         auto temp = g64();
1274         append(Move, Arg::bigImm(static_cast&lt;uint64_t&gt;(sizeOfOperation) + offset - 1), temp);
1275         append(Add64, result, temp);
1276 
1277         emitCheck([&amp;] {
1278             return Inst(Branch64, nullptr, Arg::relCond(MacroAssembler::AboveOrEqual), temp, Tmp(m_memorySizeGPR));
1279         }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1280             this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsMemoryAccess);
1281         });
1282         break;
1283     }
1284 
1285     case MemoryMode::Signaling: {
1286         // We&#39;ve virtually mapped 4GiB+redzone for this memory. Only the user-allocated pages are addressable, contiguously in range [0, current],
1287         // and everything above is mapped PROT_NONE. We don&#39;t need to perform any explicit bounds check in the 4GiB range because WebAssembly register
1288         // memory accesses are 32-bit. However WebAssembly register + offset accesses perform the addition in 64-bit which can push an access above
1289         // the 32-bit limit (the offset is unsigned 32-bit). The redzone will catch most small offsets, and we&#39;ll explicitly bounds check any
1290         // register + large offset access. We don&#39;t think this will be generated frequently.
1291         //
1292         // We could check that register + large offset doesn&#39;t exceed 4GiB+redzone since that&#39;s technically the limit we need to avoid overflowing the
1293         // PROT_NONE region, but it&#39;s better if we use a smaller immediate because it can codegens better. We know that anything equal to or greater
1294         // than the declared &#39;maximum&#39; will trap, so we can compare against that number. If there was no declared &#39;maximum&#39; then we still know that
1295         // any access equal to or greater than 4GiB will trap, no need to add the redzone.
1296         if (offset &gt;= Memory::fastMappedRedzoneBytes()) {
1297             uint64_t maximum = m_info.memory.maximum() ? m_info.memory.maximum().bytes() : std::numeric_limits&lt;uint32_t&gt;::max();
1298             auto temp = g64();
1299             append(Move, Arg::bigImm(static_cast&lt;uint64_t&gt;(sizeOfOperation) + offset - 1), temp);
1300             append(Add64, result, temp);
1301             auto sizeMax = addConstant(Type::I64, maximum);
1302 
1303             emitCheck([&amp;] {
1304                 return Inst(Branch64, nullptr, Arg::relCond(MacroAssembler::AboveOrEqual), temp, sizeMax);
1305             }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1306                 this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsMemoryAccess);
1307             });
1308         }
1309         break;
1310     }
1311     }
1312 
1313     append(Add64, Tmp(m_memoryBaseGPR), result);
1314     return result;
1315 }
1316 
1317 inline uint32_t sizeOfLoadOp(LoadOpType op)
1318 {
1319     switch (op) {
1320     case LoadOpType::I32Load8S:
1321     case LoadOpType::I32Load8U:
1322     case LoadOpType::I64Load8S:
1323     case LoadOpType::I64Load8U:
1324         return 1;
1325     case LoadOpType::I32Load16S:
1326     case LoadOpType::I64Load16S:
1327     case LoadOpType::I32Load16U:
1328     case LoadOpType::I64Load16U:
1329         return 2;
1330     case LoadOpType::I32Load:
1331     case LoadOpType::I64Load32S:
1332     case LoadOpType::I64Load32U:
1333     case LoadOpType::F32Load:
1334         return 4;
1335     case LoadOpType::I64Load:
1336     case LoadOpType::F64Load:
1337         return 8;
1338     }
1339     RELEASE_ASSERT_NOT_REACHED();
1340 }
1341 
1342 inline TypedTmp AirIRGenerator::emitLoadOp(LoadOpType op, ExpressionType pointer, uint32_t uoffset)
1343 {
1344     uint32_t offset = fixupPointerPlusOffset(pointer, uoffset);
1345 
1346     TypedTmp immTmp;
1347     TypedTmp newPtr;
1348     TypedTmp result;
1349 
1350     Arg addrArg;
1351     if (Arg::isValidAddrForm(offset, B3::widthForBytes(sizeOfLoadOp(op))))
1352         addrArg = Arg::addr(pointer, offset);
1353     else {
1354         immTmp = g64();
1355         newPtr = g64();
1356         append(Move, Arg::bigImm(offset), immTmp);
1357         append(Add64, immTmp, pointer, newPtr);
1358         addrArg = Arg::addr(newPtr);
1359     }
1360 
1361     switch (op) {
1362     case LoadOpType::I32Load8S: {
1363         result = g32();
1364         appendEffectful(Load8SignedExtendTo32, addrArg, result);
1365         break;
1366     }
1367 
1368     case LoadOpType::I64Load8S: {
1369         result = g64();
1370         appendEffectful(Load8SignedExtendTo32, addrArg, result);
1371         append(SignExtend32ToPtr, result, result);
1372         break;
1373     }
1374 
1375     case LoadOpType::I32Load8U: {
1376         result = g32();
1377         appendEffectful(Load8, addrArg, result);
1378         break;
1379     }
1380 
1381     case LoadOpType::I64Load8U: {
1382         result = g64();
1383         appendEffectful(Load8, addrArg, result);
1384         break;
1385     }
1386 
1387     case LoadOpType::I32Load16S: {
1388         result = g32();
1389         appendEffectful(Load16SignedExtendTo32, addrArg, result);
1390         break;
1391     }
1392 
1393     case LoadOpType::I64Load16S: {
1394         result = g64();
1395         appendEffectful(Load16SignedExtendTo32, addrArg, result);
1396         append(SignExtend32ToPtr, result, result);
1397         break;
1398     }
1399 
1400     case LoadOpType::I32Load16U: {
1401         result = g32();
1402         appendEffectful(Load16, addrArg, result);
1403         break;
1404     }
1405 
1406     case LoadOpType::I64Load16U: {
1407         result = g64();
1408         appendEffectful(Load16, addrArg, result);
1409         break;
1410     }
1411 
1412     case LoadOpType::I32Load:
1413         result = g32();
1414         appendEffectful(Move32, addrArg, result);
1415         break;
1416 
1417     case LoadOpType::I64Load32U: {
1418         result = g64();
1419         appendEffectful(Move32, addrArg, result);
1420         break;
1421     }
1422 
1423     case LoadOpType::I64Load32S: {
1424         result = g64();
1425         appendEffectful(Move32, addrArg, result);
1426         append(SignExtend32ToPtr, result, result);
1427         break;
1428     }
1429 
1430     case LoadOpType::I64Load: {
1431         result = g64();
1432         appendEffectful(Move, addrArg, result);
1433         break;
1434     }
1435 
1436     case LoadOpType::F32Load: {
1437         result = f32();
1438         appendEffectful(MoveFloat, addrArg, result);
1439         break;
1440     }
1441 
1442     case LoadOpType::F64Load: {
1443         result = f64();
1444         appendEffectful(MoveDouble, addrArg, result);
1445         break;
1446     }
1447     }
1448 
1449     return result;
1450 }
1451 
1452 auto AirIRGenerator::load(LoadOpType op, ExpressionType pointer, ExpressionType&amp; result, uint32_t offset) -&gt; PartialResult
1453 {
1454     ASSERT(pointer.tmp().isGP());
1455 
1456     if (UNLIKELY(sumOverflows&lt;uint32_t&gt;(offset, sizeOfLoadOp(op)))) {
1457         // FIXME: Even though this is provably out of bounds, it&#39;s not a validation error, so we have to handle it
1458         // as a runtime exception. However, this may change: https://bugs.webkit.org/show_bug.cgi?id=166435
1459         auto* patch = addPatchpoint(B3::Void);
1460         patch-&gt;setGenerator([this] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1461             this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsMemoryAccess);
1462         });
1463         emitPatchpoint(patch, Tmp());
1464 
1465         // We won&#39;t reach here, so we just pick a random reg.
1466         switch (op) {
1467         case LoadOpType::I32Load8S:
1468         case LoadOpType::I32Load16S:
1469         case LoadOpType::I32Load:
1470         case LoadOpType::I32Load16U:
1471         case LoadOpType::I32Load8U:
1472             result = g32();
1473             break;
1474         case LoadOpType::I64Load8S:
1475         case LoadOpType::I64Load8U:
1476         case LoadOpType::I64Load16S:
1477         case LoadOpType::I64Load32U:
1478         case LoadOpType::I64Load32S:
1479         case LoadOpType::I64Load:
1480         case LoadOpType::I64Load16U:
1481             result = g64();
1482             break;
1483         case LoadOpType::F32Load:
1484             result = f32();
1485             break;
1486         case LoadOpType::F64Load:
1487             result = f64();
1488             break;
1489         }
1490     } else
1491         result = emitLoadOp(op, emitCheckAndPreparePointer(pointer, offset, sizeOfLoadOp(op)), offset);
1492 
1493     return { };
1494 }
1495 
1496 inline uint32_t sizeOfStoreOp(StoreOpType op)
1497 {
1498     switch (op) {
1499     case StoreOpType::I32Store8:
1500     case StoreOpType::I64Store8:
1501         return 1;
1502     case StoreOpType::I32Store16:
1503     case StoreOpType::I64Store16:
1504         return 2;
1505     case StoreOpType::I32Store:
1506     case StoreOpType::I64Store32:
1507     case StoreOpType::F32Store:
1508         return 4;
1509     case StoreOpType::I64Store:
1510     case StoreOpType::F64Store:
1511         return 8;
1512     }
1513     RELEASE_ASSERT_NOT_REACHED();
1514 }
1515 
1516 
1517 inline void AirIRGenerator::emitStoreOp(StoreOpType op, ExpressionType pointer, ExpressionType value, uint32_t uoffset)
1518 {
1519     uint32_t offset = fixupPointerPlusOffset(pointer, uoffset);
1520 
1521     TypedTmp immTmp;
1522     TypedTmp newPtr;
1523 
1524     Arg addrArg;
1525     if (Arg::isValidAddrForm(offset, B3::widthForBytes(sizeOfStoreOp(op))))
1526         addrArg = Arg::addr(pointer, offset);
1527     else {
1528         immTmp = g64();
1529         newPtr = g64();
1530         append(Move, Arg::bigImm(offset), immTmp);
1531         append(Add64, immTmp, pointer, newPtr);
1532         addrArg = Arg::addr(newPtr);
1533     }
1534 
1535     switch (op) {
1536     case StoreOpType::I64Store8:
1537     case StoreOpType::I32Store8:
1538         append(Store8, value, addrArg);
1539         return;
1540 
1541     case StoreOpType::I64Store16:
1542     case StoreOpType::I32Store16:
1543         append(Store16, value, addrArg);
1544         return;
1545 
1546     case StoreOpType::I64Store32:
1547     case StoreOpType::I32Store:
1548         append(Move32, value, addrArg);
1549         return;
1550 
1551     case StoreOpType::I64Store:
1552         append(Move, value, addrArg);
1553         return;
1554 
1555     case StoreOpType::F32Store:
1556         append(MoveFloat, value, addrArg);
1557         return;
1558 
1559     case StoreOpType::F64Store:
1560         append(MoveDouble, value, addrArg);
1561         return;
1562     }
1563 
1564     RELEASE_ASSERT_NOT_REACHED();
1565 }
1566 
1567 auto AirIRGenerator::store(StoreOpType op, ExpressionType pointer, ExpressionType value, uint32_t offset) -&gt; PartialResult
1568 {
1569     ASSERT(pointer.tmp().isGP());
1570 
1571     if (UNLIKELY(sumOverflows&lt;uint32_t&gt;(offset, sizeOfStoreOp(op)))) {
1572         // FIXME: Even though this is provably out of bounds, it&#39;s not a validation error, so we have to handle it
1573         // as a runtime exception. However, this may change: https://bugs.webkit.org/show_bug.cgi?id=166435
1574         auto* throwException = addPatchpoint(B3::Void);
1575         throwException-&gt;setGenerator([this] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1576             this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsMemoryAccess);
1577         });
1578         emitPatchpoint(throwException, Tmp());
1579     } else
1580         emitStoreOp(op, emitCheckAndPreparePointer(pointer, offset, sizeOfStoreOp(op)), value, offset);
1581 
1582     return { };
1583 }
1584 
1585 auto AirIRGenerator::addSelect(ExpressionType condition, ExpressionType nonZero, ExpressionType zero, ExpressionType&amp; result) -&gt; PartialResult
1586 {
1587     ASSERT(nonZero.type() == zero.type());
1588     result = tmpForType(nonZero.type());
1589     append(moveOpForValueType(nonZero.type()), nonZero, result);
1590 
1591     BasicBlock* isZero = m_code.addBlock();
1592     BasicBlock* continuation = m_code.addBlock();
1593 
1594     append(BranchTest32, Arg::resCond(MacroAssembler::Zero), condition, condition);
1595     m_currentBlock-&gt;setSuccessors(isZero, continuation);
1596 
1597     append(isZero, moveOpForValueType(zero.type()), zero, result);
1598     append(isZero, Jump);
1599     isZero-&gt;setSuccessors(continuation);
1600 
1601     m_currentBlock = continuation;
1602 
1603     return { };
1604 }
1605 
1606 void AirIRGenerator::emitEntryTierUpCheck(int32_t incrementCount, B3::Origin origin)
1607 {
1608     UNUSED_PARAM(origin);
1609 
1610     if (!m_tierUp)
1611         return;
1612 
1613     auto countdownPtr = g64();
1614 
1615     append(Move, Arg::bigImm(reinterpret_cast&lt;uint64_t&gt;(&amp;m_tierUp-&gt;m_counter)), countdownPtr);
1616 
1617     auto* patch = addPatchpoint(B3::Void);
1618     B3::Effects effects = B3::Effects::none();
1619     effects.reads = B3::HeapRange::top();
1620     effects.writes = B3::HeapRange::top();
1621     patch-&gt;effects = effects;
1622     patch-&gt;clobber(RegisterSet::macroScratchRegisters());
1623 
1624     patch-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
1625         AllowMacroScratchRegisterUsage allowScratch(jit);
1626 
1627         CCallHelpers::Jump tierUp = jit.branchAdd32(CCallHelpers::PositiveOrZero, CCallHelpers::TrustedImm32(incrementCount), CCallHelpers::Address(params[0].gpr()));
1628         CCallHelpers::Label tierUpResume = jit.label();
1629 
1630         params.addLatePath([=] (CCallHelpers&amp; jit) {
1631             tierUp.link(&amp;jit);
1632 
1633             const unsigned extraPaddingBytes = 0;
1634             RegisterSet registersToSpill = { };
1635             registersToSpill.add(GPRInfo::argumentGPR1);
1636             unsigned numberOfStackBytesUsedForRegisterPreservation = ScratchRegisterAllocator::preserveRegistersToStackForCall(jit, registersToSpill, extraPaddingBytes);
1637 
1638             jit.move(MacroAssembler::TrustedImm32(m_functionIndex), GPRInfo::argumentGPR1);
1639             MacroAssembler::Call call = jit.nearCall();
1640 
1641             ScratchRegisterAllocator::restoreRegistersFromStackForCall(jit, registersToSpill, RegisterSet(), numberOfStackBytesUsedForRegisterPreservation, extraPaddingBytes);
1642             jit.jump(tierUpResume);
1643 
1644             jit.addLinkTask([=] (LinkBuffer&amp; linkBuffer) {
1645                 MacroAssembler::repatchNearCall(linkBuffer.locationOfNearCall&lt;NoPtrTag&gt;(call), CodeLocationLabel&lt;JITThunkPtrTag&gt;(Thunks::singleton().stub(triggerOMGEntryTierUpThunkGenerator).code()));
1646             });
1647         });
1648     });
1649 
1650     emitPatchpoint(patch, Tmp(), countdownPtr);
1651 }
1652 
1653 void AirIRGenerator::emitLoopTierUpCheck(int32_t incrementCount, const Stack&amp; expressionStack, uint32_t loopIndex, uint32_t outerLoopIndex, B3::Origin origin)
1654 {
1655     UNUSED_PARAM(origin);
1656 
1657     if (!m_tierUp)
1658         return;
1659 
1660     ASSERT(m_tierUp-&gt;osrEntryTriggers().size() == loopIndex);
1661     m_tierUp-&gt;osrEntryTriggers().append(TierUpCount::TriggerReason::DontTrigger);
1662     m_tierUp-&gt;outerLoops().append(outerLoopIndex);
1663 
1664     auto countdownPtr = g64();
1665 
1666     append(Move, Arg::bigImm(reinterpret_cast&lt;uint64_t&gt;(&amp;m_tierUp-&gt;m_counter)), countdownPtr);
1667 
1668     auto* patch = addPatchpoint(B3::Void);
1669     B3::Effects effects = B3::Effects::none();
1670     effects.reads = B3::HeapRange::top();
1671     effects.writes = B3::HeapRange::top();
1672     effects.exitsSideways = true;
1673     patch-&gt;effects = effects;
1674 
1675     patch-&gt;clobber(RegisterSet::macroScratchRegisters());
1676     RegisterSet clobberLate;
1677     clobberLate.add(GPRInfo::argumentGPR0);
1678     patch-&gt;clobberLate(clobberLate);
1679 
1680     Vector&lt;ConstrainedTmp&gt; patchArgs;
1681     patchArgs.append(countdownPtr);
1682 
1683     Vector&lt;B3::Type&gt; types;
1684     for (auto&amp; local : m_locals) {
1685         patchArgs.append(ConstrainedTmp(local, B3::ValueRep::ColdAny));
1686         types.append(toB3Type(local.type()));
1687     }
1688     for (auto&amp; expression : expressionStack) {
1689         patchArgs.append(ConstrainedTmp(expression, B3::ValueRep::ColdAny));
1690         types.append(toB3Type(expression.type()));
1691     }
1692 
1693     TierUpCount::TriggerReason* forceEntryTrigger = &amp;(m_tierUp-&gt;osrEntryTriggers().last());
1694     static_assert(!static_cast&lt;uint8_t&gt;(TierUpCount::TriggerReason::DontTrigger), &quot;the JIT code assumes non-zero means &#39;enter&#39;&quot;);
1695     static_assert(sizeof(TierUpCount::TriggerReason) == 1, &quot;branchTest8 assumes this size&quot;);
1696     patch-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
1697         AllowMacroScratchRegisterUsage allowScratch(jit);
1698         CCallHelpers::Jump forceOSREntry = jit.branchTest8(CCallHelpers::NonZero, CCallHelpers::AbsoluteAddress(forceEntryTrigger));
1699         CCallHelpers::Jump tierUp = jit.branchAdd32(CCallHelpers::PositiveOrZero, CCallHelpers::TrustedImm32(incrementCount), CCallHelpers::Address(params[0].gpr()));
1700         MacroAssembler::Label tierUpResume = jit.label();
1701 
1702         OSREntryData&amp; osrEntryData = m_tierUp-&gt;addOSREntryData(m_functionIndex, loopIndex);
1703         for (unsigned index = 0; index &lt; types.size(); ++index)
1704             osrEntryData.values().constructAndAppend(params[index + 1], types[index]);
1705         OSREntryData* osrEntryDataPtr = &amp;osrEntryData;
1706 
1707         params.addLatePath([=] (CCallHelpers&amp; jit) {
1708             AllowMacroScratchRegisterUsage allowScratch(jit);
1709             forceOSREntry.link(&amp;jit);
1710             tierUp.link(&amp;jit);
1711 
1712             jit.probe(triggerOSREntryNow, osrEntryDataPtr);
1713             jit.branchTestPtr(CCallHelpers::Zero, GPRInfo::argumentGPR0).linkTo(tierUpResume, &amp;jit);
1714             jit.farJump(GPRInfo::argumentGPR1, WasmEntryPtrTag);
1715         });
1716     });
1717 
1718     emitPatchpoint(patch, Tmp(), WTFMove(patchArgs));
1719 }
1720 
1721 AirIRGenerator::ControlData AirIRGenerator::addLoop(Type signature, const Stack&amp; expressionStack, uint32_t loopIndex)
1722 {
1723     BasicBlock* body = m_code.addBlock();
1724     BasicBlock* continuation = m_code.addBlock();
1725 
1726     append(Jump);
1727     m_currentBlock-&gt;setSuccessors(body);
1728 
1729     uint32_t outerLoopIndex = this-&gt;outerLoopIndex();
1730     m_outerLoops.append(loopIndex);
1731     m_currentBlock = body;
1732     emitLoopTierUpCheck(TierUpCount::loopIncrement(), expressionStack, loopIndex, outerLoopIndex, origin());
1733 
1734     return ControlData(origin(), signature, tmpForType(signature), BlockType::Loop, continuation, body);
1735 }
1736 
1737 AirIRGenerator::ControlData AirIRGenerator::addTopLevel(Type signature)
1738 {
1739     return ControlData(B3::Origin(), signature, tmpForType(signature), BlockType::TopLevel, m_code.addBlock());
1740 }
1741 
1742 AirIRGenerator::ControlData AirIRGenerator::addBlock(Type signature)
1743 {
1744     return ControlData(origin(), signature, tmpForType(signature), BlockType::Block, m_code.addBlock());
1745 }
1746 
1747 auto AirIRGenerator::addIf(ExpressionType condition, Type signature, ControlType&amp; result) -&gt; PartialResult
1748 {
1749     BasicBlock* taken = m_code.addBlock();
1750     BasicBlock* notTaken = m_code.addBlock();
1751     BasicBlock* continuation = m_code.addBlock();
1752 
1753     // Wasm bools are i32.
1754     append(BranchTest32, Arg::resCond(MacroAssembler::NonZero), condition, condition);
1755     m_currentBlock-&gt;setSuccessors(taken, notTaken);
1756 
1757     m_currentBlock = taken;
1758     result = ControlData(origin(), signature, tmpForType(signature), BlockType::If, continuation, notTaken);
1759     return { };
1760 }
1761 
1762 auto AirIRGenerator::addElse(ControlData&amp; data, const Stack&amp; currentStack) -&gt; PartialResult
1763 {
1764     unifyValuesWithBlock(currentStack, data.result);
1765     append(Jump);
1766     m_currentBlock-&gt;setSuccessors(data.continuation);
1767     return addElseToUnreachable(data);
1768 }
1769 
1770 auto AirIRGenerator::addElseToUnreachable(ControlData&amp; data) -&gt; PartialResult
1771 {
1772     ASSERT(data.type() == BlockType::If);
1773     m_currentBlock = data.special;
1774     data.convertIfToBlock();
1775     return { };
1776 }
1777 
1778 auto AirIRGenerator::addReturn(const ControlData&amp; data, const ExpressionList&amp; returnValues) -&gt; PartialResult
1779 {
1780     ASSERT(returnValues.size() &lt;= 1);
1781     if (returnValues.size()) {
1782         Tmp returnValueGPR = Tmp(GPRInfo::returnValueGPR);
1783         Tmp returnValueFPR = Tmp(FPRInfo::returnValueFPR);
1784         switch (data.signature()) {
1785         case Type::I32:
1786             append(Move32, returnValues[0], returnValueGPR);
1787             append(Ret32, returnValueGPR);
1788             break;
1789         case Type::I64:
1790         case Type::Anyref:
1791         case Type::Funcref:
1792             append(Move, returnValues[0], returnValueGPR);
1793             append(Ret64, returnValueGPR);
1794             break;
1795         case Type::F32:
1796             append(MoveFloat, returnValues[0], returnValueFPR);
1797             append(RetFloat, returnValueFPR);
1798             break;
1799         case Type::F64:
1800             append(MoveDouble, returnValues[0], returnValueFPR);
1801             append(RetFloat, returnValueFPR);
1802             break;
1803         default:
1804             RELEASE_ASSERT_NOT_REACHED();
1805         }
1806     } else
1807         append(RetVoid);
1808     return { };
1809 }
1810 
1811 // NOTE: All branches in Wasm are on 32-bit ints
1812 
1813 auto AirIRGenerator::addBranch(ControlData&amp; data, ExpressionType condition, const Stack&amp; returnValues) -&gt; PartialResult
1814 {
1815     unifyValuesWithBlock(returnValues, data.resultForBranch());
1816 
1817     BasicBlock* target = data.targetBlockForBranch();
1818     if (condition) {
1819         BasicBlock* continuation = m_code.addBlock();
1820         append(BranchTest32, Arg::resCond(MacroAssembler::NonZero), condition, condition);
1821         m_currentBlock-&gt;setSuccessors(target, continuation);
1822         m_currentBlock = continuation;
1823     } else {
1824         append(Jump);
1825         m_currentBlock-&gt;setSuccessors(target);
1826     }
1827 
1828     return { };
1829 }
1830 
1831 auto AirIRGenerator::addSwitch(ExpressionType condition, const Vector&lt;ControlData*&gt;&amp; targets, ControlData&amp; defaultTarget, const Stack&amp; expressionStack) -&gt; PartialResult
1832 {
1833     auto&amp; successors = m_currentBlock-&gt;successors();
1834     ASSERT(successors.isEmpty());
1835     for (const auto&amp; target : targets) {
1836         unifyValuesWithBlock(expressionStack, target-&gt;resultForBranch());
1837         successors.append(target-&gt;targetBlockForBranch());
1838     }
1839     unifyValuesWithBlock(expressionStack, defaultTarget.resultForBranch());
1840     successors.append(defaultTarget.targetBlockForBranch());
1841 
1842     ASSERT(condition.type() == Type::I32);
1843 
1844     // FIXME: We should consider dynamically switching between a jump table
1845     // and a binary switch depending on the number of successors.
1846     // https://bugs.webkit.org/show_bug.cgi?id=194477
1847 
1848     size_t numTargets = targets.size();
1849 
1850     auto* patchpoint = addPatchpoint(B3::Void);
1851     patchpoint-&gt;effects = B3::Effects::none();
1852     patchpoint-&gt;effects.terminal = true;
1853     patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
1854 
1855     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
1856         AllowMacroScratchRegisterUsage allowScratch(jit);
1857 
1858         Vector&lt;int64_t&gt; cases;
1859         cases.reserveInitialCapacity(numTargets);
1860         for (size_t i = 0; i &lt; numTargets; ++i)
1861             cases.uncheckedAppend(i);
1862 
1863         GPRReg valueReg = params[0].gpr();
1864         BinarySwitch binarySwitch(valueReg, cases, BinarySwitch::Int32);
1865 
1866         Vector&lt;CCallHelpers::Jump&gt; caseJumps;
1867         caseJumps.resize(numTargets);
1868 
1869         while (binarySwitch.advance(jit)) {
1870             unsigned value = binarySwitch.caseValue();
1871             unsigned index = binarySwitch.caseIndex();
1872             ASSERT_UNUSED(value, value == index);
1873             ASSERT(index &lt; numTargets);
1874             caseJumps[index] = jit.jump();
1875         }
1876 
1877         CCallHelpers::JumpList fallThrough = binarySwitch.fallThrough();
1878 
1879         Vector&lt;Box&lt;CCallHelpers::Label&gt;&gt; successorLabels = params.successorLabels();
1880         ASSERT(successorLabels.size() == caseJumps.size() + 1);
1881 
1882         params.addLatePath([=, caseJumps = WTFMove(caseJumps), successorLabels = WTFMove(successorLabels)] (CCallHelpers&amp; jit) {
1883             for (size_t i = 0; i &lt; numTargets; ++i)
1884                 caseJumps[i].linkTo(*successorLabels[i], &amp;jit);
1885             fallThrough.linkTo(*successorLabels[numTargets], &amp;jit);
1886         });
1887     });
1888 
1889     emitPatchpoint(patchpoint, TypedTmp(), condition);
1890 
1891     return { };
1892 }
1893 
1894 auto AirIRGenerator::endBlock(ControlEntry&amp; entry, Stack&amp; expressionStack) -&gt; PartialResult
1895 {
1896     ControlData&amp; data = entry.controlData;
1897 
1898     unifyValuesWithBlock(expressionStack, data.result);
1899     append(Jump);
1900     m_currentBlock-&gt;setSuccessors(data.continuation);
1901 
1902     return addEndToUnreachable(entry);
1903 }
1904 
1905 
1906 auto AirIRGenerator::addEndToUnreachable(ControlEntry&amp; entry) -&gt; PartialResult
1907 {
1908     ControlData&amp; data = entry.controlData;
1909     m_currentBlock = data.continuation;
1910 
1911     if (data.type() == BlockType::If) {
1912         append(data.special, Jump);
1913         data.special-&gt;setSuccessors(m_currentBlock);
1914     }
1915 
1916     if (data.type() == BlockType::Loop)
1917         m_outerLoops.removeLast();
1918 
1919     for (const auto&amp; result : data.result)
1920         entry.enclosedExpressionStack.append(result);
1921 
1922     // TopLevel does not have any code after this so we need to make sure we emit a return here.
1923     if (data.type() == BlockType::TopLevel)
1924         return addReturn(data, entry.enclosedExpressionStack);
1925 
1926     return { };
1927 }
1928 
1929 auto AirIRGenerator::addCall(uint32_t functionIndex, const Signature&amp; signature, Vector&lt;ExpressionType&gt;&amp; args, ExpressionType&amp; result) -&gt; PartialResult
1930 {
1931     ASSERT(signature.argumentCount() == args.size());
1932 
1933     m_makesCalls = true;
1934 
1935     Type returnType = signature.returnType();
1936     if (returnType != Type::Void)
1937         result = tmpForType(returnType);
1938 
1939     Vector&lt;UnlinkedWasmToWasmCall&gt;* unlinkedWasmToWasmCalls = &amp;m_unlinkedWasmToWasmCalls;
1940 
1941     if (m_info.isImportedFunctionFromFunctionIndexSpace(functionIndex)) {
1942         m_maxNumJSCallArguments = std::max(m_maxNumJSCallArguments, static_cast&lt;uint32_t&gt;(args.size()));
1943 
1944         auto currentInstance = g64();
1945         append(Move, instanceValue(), currentInstance);
1946 
1947         auto targetInstance = g64();
1948 
1949         // FIXME: We should have better isel here.
1950         // https://bugs.webkit.org/show_bug.cgi?id=193999
1951         append(Move, Arg::bigImm(Instance::offsetOfTargetInstance(functionIndex)), targetInstance);
1952         append(Add64, instanceValue(), targetInstance);
1953         append(Move, Arg::addr(targetInstance), targetInstance);
1954 
1955         BasicBlock* isWasmBlock = m_code.addBlock();
1956         BasicBlock* isEmbedderBlock = m_code.addBlock();
1957         BasicBlock* continuation = m_code.addBlock();
1958 
1959         append(BranchTest64, Arg::resCond(MacroAssembler::NonZero), targetInstance, targetInstance);
1960         m_currentBlock-&gt;setSuccessors(isWasmBlock, isEmbedderBlock);
1961 
1962         {
1963             auto* patchpoint = addPatchpoint(toB3Type(returnType));
1964             patchpoint-&gt;effects.writesPinned = true;
1965             patchpoint-&gt;effects.readsPinned = true;
1966             // We need to clobber all potential pinned registers since we might be leaving the instance.
1967             // We pessimistically assume we could be calling to something that is bounds checking.
1968             // FIXME: We shouldn&#39;t have to do this: https://bugs.webkit.org/show_bug.cgi?id=172181
1969             patchpoint-&gt;clobberLate(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
1970 
1971             Vector&lt;ConstrainedTmp&gt; patchArgs;
1972             wasmCallingConventionAir().setupCall(m_code, returnType, patchpoint, toTmpVector(args), [&amp;] (Tmp tmp, B3::ValueRep rep) {
1973                 patchArgs.append({ tmp, rep });
1974             });
1975 
1976             patchpoint-&gt;setGenerator([unlinkedWasmToWasmCalls, functionIndex] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
1977                 AllowMacroScratchRegisterUsage allowScratch(jit);
1978                 CCallHelpers::Call call = jit.threadSafePatchableNearCall();
1979                 jit.addLinkTask([unlinkedWasmToWasmCalls, call, functionIndex] (LinkBuffer&amp; linkBuffer) {
1980                     unlinkedWasmToWasmCalls-&gt;append({ linkBuffer.locationOfNearCall&lt;WasmEntryPtrTag&gt;(call), functionIndex });
1981                 });
1982             });
1983 
1984             emitPatchpoint(isWasmBlock, patchpoint, result, WTFMove(patchArgs));
1985             append(isWasmBlock, Jump);
1986             isWasmBlock-&gt;setSuccessors(continuation);
1987         }
1988 
1989         {
1990             auto jumpDestination = g64();
1991             append(isEmbedderBlock, Move, Arg::bigImm(Instance::offsetOfWasmToEmbedderStub(functionIndex)), jumpDestination);
1992             append(isEmbedderBlock, Add64, instanceValue(), jumpDestination);
1993             append(isEmbedderBlock, Move, Arg::addr(jumpDestination), jumpDestination);
1994 
1995             auto* patchpoint = addPatchpoint(toB3Type(returnType));
1996             patchpoint-&gt;effects.writesPinned = true;
1997             patchpoint-&gt;effects.readsPinned = true;
1998             // We need to clobber all potential pinned registers since we might be leaving the instance.
1999             // We pessimistically assume we could be calling to something that is bounds checking.
2000             // FIXME: We shouldn&#39;t have to do this: https://bugs.webkit.org/show_bug.cgi?id=172181
2001             patchpoint-&gt;clobberLate(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
2002 
2003             Vector&lt;ConstrainedTmp&gt; patchArgs;
2004             patchArgs.append(jumpDestination);
2005 
2006             wasmCallingConventionAir().setupCall(m_code, returnType, patchpoint, toTmpVector(args), [&amp;] (Tmp tmp, B3::ValueRep rep) {
2007                 patchArgs.append({ tmp, rep });
2008             });
2009 
2010             patchpoint-&gt;setGenerator([returnType] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2011                 AllowMacroScratchRegisterUsage allowScratch(jit);
2012                 jit.call(params[returnType == Void ? 0 : 1].gpr(), WasmEntryPtrTag);
2013             });
2014 
2015             emitPatchpoint(isEmbedderBlock, patchpoint, result, WTFMove(patchArgs));
2016             append(isEmbedderBlock, Jump);
2017             isEmbedderBlock-&gt;setSuccessors(continuation);
2018         }
2019 
2020         m_currentBlock = continuation;
2021         // The call could have been to another WebAssembly instance, and / or could have modified our Memory.
2022         restoreWebAssemblyGlobalState(RestoreCachedStackLimit::Yes, m_info.memory, currentInstance, continuation);
2023     } else {
2024         auto* patchpoint = addPatchpoint(toB3Type(returnType));
2025         patchpoint-&gt;effects.writesPinned = true;
2026         patchpoint-&gt;effects.readsPinned = true;
2027 
2028         Vector&lt;ConstrainedTmp&gt; patchArgs;
2029         wasmCallingConventionAir().setupCall(m_code, returnType, patchpoint, toTmpVector(args), [&amp;] (Tmp tmp, B3::ValueRep rep) {
2030             patchArgs.append({ tmp, rep });
2031         });
2032 
2033         patchpoint-&gt;setGenerator([unlinkedWasmToWasmCalls, functionIndex] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2034             AllowMacroScratchRegisterUsage allowScratch(jit);
2035             CCallHelpers::Call call = jit.threadSafePatchableNearCall();
2036             jit.addLinkTask([unlinkedWasmToWasmCalls, call, functionIndex] (LinkBuffer&amp; linkBuffer) {
2037                 unlinkedWasmToWasmCalls-&gt;append({ linkBuffer.locationOfNearCall&lt;WasmEntryPtrTag&gt;(call), functionIndex });
2038             });
2039         });
2040 
2041         emitPatchpoint(m_currentBlock, patchpoint, result, WTFMove(patchArgs));
2042     }
2043 
2044     return { };
2045 }
2046 
2047 auto AirIRGenerator::addCallIndirect(unsigned tableIndex, const Signature&amp; signature, Vector&lt;ExpressionType&gt;&amp; args, ExpressionType&amp; result) -&gt; PartialResult
2048 {
2049     ExpressionType calleeIndex = args.takeLast();
2050     ASSERT(signature.argumentCount() == args.size());
2051     ASSERT(m_info.tableCount() &gt; tableIndex);
2052     ASSERT(m_info.tables[tableIndex].type() == TableElementType::Funcref);
2053 
2054     m_makesCalls = true;
2055     // Note: call indirect can call either WebAssemblyFunction or WebAssemblyWrapperFunction. Because
2056     // WebAssemblyWrapperFunction is like calling into the embedder, we conservatively assume all call indirects
2057     // can be to the embedder for our stack check calculation.
2058     m_maxNumJSCallArguments = std::max(m_maxNumJSCallArguments, static_cast&lt;uint32_t&gt;(args.size()));
2059 
2060     auto currentInstance = g64();
2061     append(Move, instanceValue(), currentInstance);
2062 
2063     ExpressionType callableFunctionBuffer = g64();
2064     ExpressionType instancesBuffer = g64();
2065     ExpressionType callableFunctionBufferLength = g64();
2066     {
2067         RELEASE_ASSERT(Arg::isValidAddrForm(FuncRefTable::offsetOfFunctions(), B3::Width64));
2068         RELEASE_ASSERT(Arg::isValidAddrForm(FuncRefTable::offsetOfInstances(), B3::Width64));
2069         RELEASE_ASSERT(Arg::isValidAddrForm(FuncRefTable::offsetOfLength(), B3::Width64));
2070 
2071         if (UNLIKELY(!Arg::isValidAddrForm(Instance::offsetOfTablePtr(m_numImportFunctions, tableIndex), B3::Width64))) {
2072             append(Move, Arg::bigImm(Instance::offsetOfTablePtr(m_numImportFunctions, tableIndex)), callableFunctionBufferLength);
2073             append(Add64, instanceValue(), callableFunctionBufferLength);
2074             append(Move, Arg::addr(callableFunctionBufferLength), callableFunctionBufferLength);
2075         } else
2076             append(Move, Arg::addr(instanceValue(), Instance::offsetOfTablePtr(m_numImportFunctions, tableIndex)), callableFunctionBufferLength);
2077         append(Move, Arg::addr(callableFunctionBufferLength, FuncRefTable::offsetOfFunctions()), callableFunctionBuffer);
2078         append(Move, Arg::addr(callableFunctionBufferLength, FuncRefTable::offsetOfInstances()), instancesBuffer);
2079         append(Move32, Arg::addr(callableFunctionBufferLength, Table::offsetOfLength()), callableFunctionBufferLength);
2080     }
2081 
2082     append(Move32, calleeIndex, calleeIndex);
2083 
2084     // Check the index we are looking for is valid.
2085     emitCheck([&amp;] {
2086         return Inst(Branch32, nullptr, Arg::relCond(MacroAssembler::AboveOrEqual), calleeIndex, callableFunctionBufferLength);
2087     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2088         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsCallIndirect);
2089     });
2090 
2091     ExpressionType calleeCode = g64();
2092     {
2093         ExpressionType calleeSignatureIndex = g64();
2094         // Compute the offset in the table index space we are looking for.
2095         append(Move, Arg::imm(sizeof(WasmToWasmImportableFunction)), calleeSignatureIndex);
2096         append(Mul64, calleeIndex, calleeSignatureIndex);
2097         append(Add64, callableFunctionBuffer, calleeSignatureIndex);
2098 
2099         append(Move, Arg::addr(calleeSignatureIndex, WasmToWasmImportableFunction::offsetOfEntrypointLoadLocation()), calleeCode); // Pointer to callee code.
2100 
2101         // Check that the WasmToWasmImportableFunction is initialized. We trap if it isn&#39;t. An &quot;invalid&quot; SignatureIndex indicates it&#39;s not initialized.
2102         // FIXME: when we have trap handlers, we can just let the call fail because Signature::invalidIndex is 0. https://bugs.webkit.org/show_bug.cgi?id=177210
2103         static_assert(sizeof(WasmToWasmImportableFunction::signatureIndex) == sizeof(uint64_t), &quot;Load codegen assumes i64&quot;);
2104 
2105         // FIXME: This seems dumb to do two checks just for a nicer error message.
2106         // We should move just to use a single branch and then figure out what
2107         // error to use in the exception handler.
2108 
2109         append(Move, Arg::addr(calleeSignatureIndex, WasmToWasmImportableFunction::offsetOfSignatureIndex()), calleeSignatureIndex);
2110 
2111         emitCheck([&amp;] {
2112             static_assert(Signature::invalidIndex == 0, &quot;&quot;);
2113             return Inst(BranchTest64, nullptr, Arg::resCond(MacroAssembler::Zero), calleeSignatureIndex, calleeSignatureIndex);
2114         }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2115             this-&gt;emitThrowException(jit, ExceptionType::NullTableEntry);
2116         });
2117 
2118         ExpressionType expectedSignatureIndex = g64();
2119         append(Move, Arg::bigImm(SignatureInformation::get(signature)), expectedSignatureIndex);
2120         emitCheck([&amp;] {
2121             return Inst(Branch64, nullptr, Arg::relCond(MacroAssembler::NotEqual), calleeSignatureIndex, expectedSignatureIndex);
2122         }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2123             this-&gt;emitThrowException(jit, ExceptionType::BadSignature);
2124         });
2125     }
2126 
2127     // Do a context switch if needed.
2128     {
2129         auto newContextInstance = g64();
2130         append(Move, Arg::index(instancesBuffer, calleeIndex, 8, 0), newContextInstance);
2131 
2132         BasicBlock* doContextSwitch = m_code.addBlock();
2133         BasicBlock* continuation = m_code.addBlock();
2134 
2135         append(Branch64, Arg::relCond(MacroAssembler::Equal), newContextInstance, instanceValue());
2136         m_currentBlock-&gt;setSuccessors(continuation, doContextSwitch);
2137 
2138         auto* patchpoint = addPatchpoint(B3::Void);
2139         patchpoint-&gt;effects.writesPinned = true;
2140         // We pessimistically assume we&#39;re calling something with BoundsChecking memory.
2141         // FIXME: We shouldn&#39;t have to do this: https://bugs.webkit.org/show_bug.cgi?id=172181
2142         patchpoint-&gt;clobber(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
2143         patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
2144         patchpoint-&gt;numGPScratchRegisters = Gigacage::isEnabled(Gigacage::Primitive) ? 1 : 0;
2145 
2146         patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2147             AllowMacroScratchRegisterUsage allowScratch(jit);
2148             GPRReg newContextInstance = params[0].gpr();
2149             GPRReg oldContextInstance = params[1].gpr();
2150             const PinnedRegisterInfo&amp; pinnedRegs = PinnedRegisterInfo::get();
2151             GPRReg baseMemory = pinnedRegs.baseMemoryPointer;
2152             ASSERT(newContextInstance != baseMemory);
2153             jit.loadPtr(CCallHelpers::Address(oldContextInstance, Instance::offsetOfCachedStackLimit()), baseMemory);
2154             jit.storePtr(baseMemory, CCallHelpers::Address(newContextInstance, Instance::offsetOfCachedStackLimit()));
2155             jit.storeWasmContextInstance(newContextInstance);
2156             // FIXME: We should support more than one memory size register
2157             //   see: https://bugs.webkit.org/show_bug.cgi?id=162952
2158             ASSERT(pinnedRegs.sizeRegister != newContextInstance);
2159             GPRReg scratchOrSize = Gigacage::isEnabled(Gigacage::Primitive) ? params.gpScratch(0) : pinnedRegs.sizeRegister;
2160 
2161             jit.loadPtr(CCallHelpers::Address(newContextInstance, Instance::offsetOfCachedMemorySize()), pinnedRegs.sizeRegister); // Memory size.
2162             jit.loadPtr(CCallHelpers::Address(newContextInstance, Instance::offsetOfCachedMemory()), baseMemory); // Memory::void*.
2163 
2164             jit.cageConditionally(Gigacage::Primitive, baseMemory, pinnedRegs.sizeRegister, scratchOrSize);
2165         });
2166 
2167         emitPatchpoint(doContextSwitch, patchpoint, Tmp(), newContextInstance, instanceValue());
2168         append(doContextSwitch, Jump);
2169         doContextSwitch-&gt;setSuccessors(continuation);
2170 
2171         m_currentBlock = continuation;
2172     }
2173 
2174     append(Move, Arg::addr(calleeCode), calleeCode);
2175 
2176     Type returnType = signature.returnType();
2177     if (returnType != Type::Void)
2178         result = tmpForType(returnType);
2179 
2180     auto* patch = addPatchpoint(toB3Type(returnType));
2181     patch-&gt;effects.writesPinned = true;
2182     patch-&gt;effects.readsPinned = true;
2183     // We need to clobber all potential pinned registers since we might be leaving the instance.
2184     // We pessimistically assume we&#39;re always calling something that is bounds checking so
2185     // because the wasm-&gt;wasm thunk unconditionally overrides the size registers.
2186     // FIXME: We should not have to do this, but the wasm-&gt;wasm stub assumes it can
2187     // use all the pinned registers as scratch: https://bugs.webkit.org/show_bug.cgi?id=172181
2188     patch-&gt;clobberLate(PinnedRegisterInfo::get().toSave(MemoryMode::BoundsChecking));
2189 
2190     Vector&lt;ConstrainedTmp&gt; emitArgs;
2191     emitArgs.append(calleeCode);
2192     wasmCallingConventionAir().setupCall(m_code, returnType, patch, toTmpVector(args), [&amp;] (Tmp tmp, B3::ValueRep rep) {
2193         emitArgs.append({ tmp, rep });
2194     });
2195     patch-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2196         AllowMacroScratchRegisterUsage allowScratch(jit);
2197         jit.call(params[returnType == Void ? 0 : 1].gpr(), WasmEntryPtrTag);
2198     });
2199 
2200     emitPatchpoint(m_currentBlock, patch, result, WTFMove(emitArgs));
2201 
2202     // The call could have been to another WebAssembly instance, and / or could have modified our Memory.
2203     restoreWebAssemblyGlobalState(RestoreCachedStackLimit::Yes, m_info.memory, currentInstance, m_currentBlock);
2204 
2205     return { };
2206 }
2207 
2208 void AirIRGenerator::unify(const ExpressionType&amp; dst, const ExpressionType&amp; source)
2209 {
2210     ASSERT(isSubtype(source.type(), dst.type()));
2211     append(moveOpForValueType(dst.type()), source, dst);
2212 }
2213 
2214 void AirIRGenerator::unifyValuesWithBlock(const Stack&amp; resultStack, const ResultList&amp; result)
2215 {
2216     ASSERT(result.size() &lt;= resultStack.size());
2217 
2218     for (size_t i = 0; i &lt; result.size(); ++i)
2219         unify(result[result.size() - 1 - i], resultStack[resultStack.size() - 1 - i]);
2220 }
2221 
2222 void AirIRGenerator::dump(const Vector&lt;ControlEntry&gt;&amp;, const Stack*)
2223 {
2224 }
2225 
2226 auto AirIRGenerator::origin() -&gt; B3::Origin
2227 {
2228     // FIXME: We should implement a way to give Inst&#39;s an origin.
2229     return B3::Origin();
2230 }
2231 
2232 Expected&lt;std::unique_ptr&lt;InternalFunction&gt;, String&gt; parseAndCompileAir(CompilationContext&amp; compilationContext, const uint8_t* functionStart, size_t functionLength, const Signature&amp; signature, Vector&lt;UnlinkedWasmToWasmCall&gt;&amp; unlinkedWasmToWasmCalls, const ModuleInformation&amp; info, MemoryMode mode, uint32_t functionIndex, TierUpCount* tierUp, ThrowWasmException throwWasmException)
2233 {
2234     auto result = makeUnique&lt;InternalFunction&gt;();
2235 
2236     compilationContext.embedderEntrypointJIT = makeUnique&lt;CCallHelpers&gt;();
2237     compilationContext.wasmEntrypointJIT = makeUnique&lt;CCallHelpers&gt;();
2238 
2239     B3::Procedure procedure;
2240     Code&amp; code = procedure.code();
2241 
2242     procedure.setOriginPrinter([] (PrintStream&amp; out, B3::Origin origin) {
2243         if (origin.data())
2244             out.print(&quot;Wasm: &quot;, bitwise_cast&lt;OpcodeOrigin&gt;(origin));
2245     });
2246 
2247     // This means we cannot use either StackmapGenerationParams::usedRegisters() or
2248     // StackmapGenerationParams::unavailableRegisters(). In exchange for this concession, we
2249     // don&#39;t strictly need to run Air::reportUsedRegisters(), which saves a bit of CPU time at
2250     // optLevel=1.
2251     procedure.setNeedsUsedRegisters(false);
2252 
2253     procedure.setOptLevel(Options::webAssemblyBBQAirOptimizationLevel());
2254 
2255     AirIRGenerator irGenerator(info, procedure, result.get(), unlinkedWasmToWasmCalls, mode, functionIndex, tierUp, throwWasmException, signature);
2256     FunctionParser&lt;AirIRGenerator&gt; parser(irGenerator, functionStart, functionLength, signature, info);
2257     WASM_FAIL_IF_HELPER_FAILS(parser.parse());
2258 
2259 
2260     for (BasicBlock* block : code) {
2261         for (size_t i = 0; i &lt; block-&gt;numSuccessors(); ++i)
2262             block-&gt;successorBlock(i)-&gt;addPredecessor(block);
2263     }
2264 
2265     {
2266         if (UNLIKELY(shouldDumpIRAtEachPhase(B3::AirMode))) {
2267             dataLogLn(&quot;Generated patchpoints&quot;);
2268             for (B3::PatchpointValue** patch : irGenerator.patchpoints())
2269                 dataLogLn(deepDump(procedure, *patch));
2270         }
2271 
2272         B3::Air::prepareForGeneration(code);
2273         B3::Air::generate(code, *compilationContext.wasmEntrypointJIT);
2274         compilationContext.wasmEntrypointByproducts = procedure.releaseByproducts();
2275         result-&gt;entrypoint.calleeSaveRegisters = code.calleeSaveRegisterAtOffsetList();
2276     }
2277 
2278     return result;
2279 }
2280 
2281 template &lt;typename IntType&gt;
2282 void AirIRGenerator::emitChecksForModOrDiv(bool isSignedDiv, ExpressionType left, ExpressionType right)
2283 {
2284     static_assert(sizeof(IntType) == 4 || sizeof(IntType) == 8, &quot;&quot;);
2285 
2286     emitCheck([&amp;] {
2287         return Inst(sizeof(IntType) == 4 ? BranchTest32 : BranchTest64, nullptr, Arg::resCond(MacroAssembler::Zero), right, right);
2288     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2289         this-&gt;emitThrowException(jit, ExceptionType::DivisionByZero);
2290     });
2291 
2292     if (isSignedDiv) {
2293         ASSERT(std::is_signed&lt;IntType&gt;::value);
2294         IntType min = std::numeric_limits&lt;IntType&gt;::min();
2295 
2296         // FIXME: Better isel for compare with imms here.
2297         // https://bugs.webkit.org/show_bug.cgi?id=193999
2298         auto minTmp = sizeof(IntType) == 4 ? g32() : g64();
2299         auto negOne = sizeof(IntType) == 4 ? g32() : g64();
2300 
2301         B3::Air::Opcode op = sizeof(IntType) == 4 ? Compare32 : Compare64;
2302         append(Move, Arg::bigImm(static_cast&lt;uint64_t&gt;(min)), minTmp);
2303         append(op, Arg::relCond(MacroAssembler::Equal), left, minTmp, minTmp);
2304 
2305         append(Move, Arg::imm(-1), negOne);
2306         append(op, Arg::relCond(MacroAssembler::Equal), right, negOne, negOne);
2307 
2308         emitCheck([&amp;] {
2309             return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), minTmp, negOne);
2310         },
2311         [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2312             this-&gt;emitThrowException(jit, ExceptionType::IntegerOverflow);
2313         });
2314     }
2315 }
2316 
2317 template &lt;typename IntType&gt;
2318 void AirIRGenerator::emitModOrDiv(bool isDiv, ExpressionType lhs, ExpressionType rhs, ExpressionType&amp; result)
2319 {
2320     static_assert(sizeof(IntType) == 4 || sizeof(IntType) == 8, &quot;&quot;);
2321 
2322     result = sizeof(IntType) == 4 ? g32() : g64();
2323 
2324     bool isSigned = std::is_signed&lt;IntType&gt;::value;
2325 
2326     if (isARM64()) {
2327         B3::Air::Opcode div;
2328         switch (sizeof(IntType)) {
2329         case 4:
2330             div = isSigned ? Div32 : UDiv32;
2331             break;
2332         case 8:
2333             div = isSigned ? Div64 : UDiv64;
2334             break;
2335         }
2336 
2337         append(div, lhs, rhs, result);
2338 
2339         if (!isDiv) {
2340             append(sizeof(IntType) == 4 ? Mul32 : Mul64, result, rhs, result);
2341             append(sizeof(IntType) == 4 ? Sub32 : Sub64, lhs, result, result);
2342         }
2343 
2344         return;
2345     }
2346 
2347 #if CPU(X86) || CPU(X86_64)
2348     Tmp eax(X86Registers::eax);
2349     Tmp edx(X86Registers::edx);
2350 
2351     if (isSigned) {
2352         B3::Air::Opcode convertToDoubleWord;
2353         B3::Air::Opcode div;
2354         switch (sizeof(IntType)) {
2355         case 4:
2356             convertToDoubleWord = X86ConvertToDoubleWord32;
2357             div = X86Div32;
2358             break;
2359         case 8:
2360             convertToDoubleWord = X86ConvertToQuadWord64;
2361             div = X86Div64;
2362             break;
2363         default:
2364             RELEASE_ASSERT_NOT_REACHED();
2365         }
2366 
2367         // We implement &quot;res = Div&lt;Chill&gt;/Mod&lt;Chill&gt;(num, den)&quot; as follows:
2368         //
2369         //     if (den + 1 &lt;=_unsigned 1) {
2370         //         if (!den) {
2371         //             res = 0;
2372         //             goto done;
2373         //         }
2374         //         if (num == -2147483648) {
2375         //             res = isDiv ? num : 0;
2376         //             goto done;
2377         //         }
2378         //     }
2379         //     res = num (/ or %) dev;
2380         // done:
2381 
2382         BasicBlock* denIsGood = m_code.addBlock();
2383         BasicBlock* denMayBeBad = m_code.addBlock();
2384         BasicBlock* denNotZero = m_code.addBlock();
2385         BasicBlock* continuation = m_code.addBlock();
2386 
2387         auto temp = sizeof(IntType) == 4 ? g32() : g64();
2388         auto one = addConstant(sizeof(IntType) == 4 ? Type::I32 : Type::I64, 1);
2389 
2390         append(sizeof(IntType) == 4 ? Add32 : Add64, rhs, one, temp);
2391         append(sizeof(IntType) == 4 ? Branch32 : Branch64, Arg::relCond(MacroAssembler::Above), temp, one);
2392         m_currentBlock-&gt;setSuccessors(denIsGood, denMayBeBad);
2393 
2394         append(denMayBeBad, Xor64, result, result);
2395         append(denMayBeBad, sizeof(IntType) == 4 ? BranchTest32 : BranchTest64, Arg::resCond(MacroAssembler::Zero), rhs, rhs);
2396         denMayBeBad-&gt;setSuccessors(continuation, denNotZero);
2397 
2398         auto min = addConstant(denNotZero, sizeof(IntType) == 4 ? Type::I32 : Type::I64, std::numeric_limits&lt;IntType&gt;::min());
2399         if (isDiv)
2400             append(denNotZero, sizeof(IntType) == 4 ? Move32 : Move, min, result);
2401         else {
2402             // Result is zero, as set above...
2403         }
2404         append(denNotZero, sizeof(IntType) == 4 ? Branch32 : Branch64, Arg::relCond(MacroAssembler::Equal), lhs, min);
2405         denNotZero-&gt;setSuccessors(continuation, denIsGood);
2406 
2407         auto divResult = isDiv ? eax : edx;
2408         append(denIsGood, Move, lhs, eax);
2409         append(denIsGood, convertToDoubleWord, eax, edx);
2410         append(denIsGood, div, eax, edx, rhs);
2411         append(denIsGood, sizeof(IntType) == 4 ? Move32 : Move, divResult, result);
2412         append(denIsGood, Jump);
2413         denIsGood-&gt;setSuccessors(continuation);
2414 
2415         m_currentBlock = continuation;
2416         return;
2417     }
2418 
2419     B3::Air::Opcode div = sizeof(IntType) == 4 ? X86UDiv32 : X86UDiv64;
2420 
2421     Tmp divResult = isDiv ? eax : edx;
2422 
2423     append(Move, lhs, eax);
2424     append(Xor64, edx, edx);
2425     append(div, eax, edx, rhs);
2426     append(sizeof(IntType) == 4 ? Move32 : Move, divResult, result);
2427 #else
2428     RELEASE_ASSERT_NOT_REACHED();
2429 #endif
2430 }
2431 
2432 template&lt;&gt;
2433 auto AirIRGenerator::addOp&lt;OpType::I32DivS&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2434 {
2435     emitChecksForModOrDiv&lt;int32_t&gt;(true, left, right);
2436     emitModOrDiv&lt;int32_t&gt;(true, left, right, result);
2437     return { };
2438 }
2439 
2440 template&lt;&gt;
2441 auto AirIRGenerator::addOp&lt;OpType::I32RemS&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2442 {
2443     emitChecksForModOrDiv&lt;int32_t&gt;(false, left, right);
2444     emitModOrDiv&lt;int32_t&gt;(false, left, right, result);
2445     return { };
2446 }
2447 
2448 template&lt;&gt;
2449 auto AirIRGenerator::addOp&lt;OpType::I32DivU&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2450 {
2451     emitChecksForModOrDiv&lt;uint32_t&gt;(false, left, right);
2452     emitModOrDiv&lt;uint32_t&gt;(true, left, right, result);
2453     return { };
2454 }
2455 
2456 template&lt;&gt;
2457 auto AirIRGenerator::addOp&lt;OpType::I32RemU&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2458 {
2459     emitChecksForModOrDiv&lt;uint32_t&gt;(false, left, right);
2460     emitModOrDiv&lt;uint32_t&gt;(false, left, right, result);
2461     return { };
2462 }
2463 
2464 template&lt;&gt;
2465 auto AirIRGenerator::addOp&lt;OpType::I64DivS&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2466 {
2467     emitChecksForModOrDiv&lt;int64_t&gt;(true, left, right);
2468     emitModOrDiv&lt;int64_t&gt;(true, left, right, result);
2469     return { };
2470 }
2471 
2472 template&lt;&gt;
2473 auto AirIRGenerator::addOp&lt;OpType::I64RemS&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2474 {
2475     emitChecksForModOrDiv&lt;int64_t&gt;(false, left, right);
2476     emitModOrDiv&lt;int64_t&gt;(false, left, right, result);
2477     return { };
2478 }
2479 
2480 template&lt;&gt;
2481 auto AirIRGenerator::addOp&lt;OpType::I64DivU&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2482 {
2483     emitChecksForModOrDiv&lt;uint64_t&gt;(false, left, right);
2484     emitModOrDiv&lt;uint64_t&gt;(true, left, right, result);
2485     return { };
2486 }
2487 
2488 template&lt;&gt;
2489 auto AirIRGenerator::addOp&lt;OpType::I64RemU&gt;(ExpressionType left, ExpressionType right, ExpressionType&amp; result) -&gt; PartialResult
2490 {
2491     emitChecksForModOrDiv&lt;uint64_t&gt;(false, left, right);
2492     emitModOrDiv&lt;uint64_t&gt;(false, left, right, result);
2493     return { };
2494 }
2495 
2496 template&lt;&gt;
2497 auto AirIRGenerator::addOp&lt;OpType::I32Ctz&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2498 {
2499     auto* patchpoint = addPatchpoint(B3::Int32);
2500     patchpoint-&gt;effects = B3::Effects::none();
2501     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2502         jit.countTrailingZeros32(params[1].gpr(), params[0].gpr());
2503     });
2504     result = g32();
2505     emitPatchpoint(patchpoint, result, arg);
2506     return { };
2507 }
2508 
2509 template&lt;&gt;
2510 auto AirIRGenerator::addOp&lt;OpType::I64Ctz&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2511 {
2512     auto* patchpoint = addPatchpoint(B3::Int64);
2513     patchpoint-&gt;effects = B3::Effects::none();
2514     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2515         jit.countTrailingZeros64(params[1].gpr(), params[0].gpr());
2516     });
2517     result = g64();
2518     emitPatchpoint(patchpoint, result, arg);
2519     return { };
2520 }
2521 
2522 template&lt;&gt;
2523 auto AirIRGenerator::addOp&lt;OpType::I32Popcnt&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2524 {
2525     result = g32();
2526 
2527 #if CPU(X86_64)
2528     if (MacroAssembler::supportsCountPopulation()) {
2529         auto* patchpoint = addPatchpoint(B3::Int32);
2530         patchpoint-&gt;effects = B3::Effects::none();
2531         patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2532             jit.countPopulation32(params[1].gpr(), params[0].gpr());
2533         });
2534         emitPatchpoint(patchpoint, result, arg);
2535         return { };
2536     }
2537 #endif
2538 
2539     uint32_t (*popcount)(int32_t) = [] (int32_t value) -&gt; uint32_t { return __builtin_popcount(value); };
2540     emitCCall(popcount, result, arg);
2541     return { };
2542 }
2543 
2544 template&lt;&gt;
2545 auto AirIRGenerator::addOp&lt;OpType::I64Popcnt&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2546 {
2547     result = g64();
2548 
2549 #if CPU(X86_64)
2550     if (MacroAssembler::supportsCountPopulation()) {
2551         auto* patchpoint = addPatchpoint(B3::Int64);
2552         patchpoint-&gt;effects = B3::Effects::none();
2553         patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2554             jit.countPopulation64(params[1].gpr(), params[0].gpr());
2555         });
2556         emitPatchpoint(patchpoint, result, arg);
2557         return { };
2558     }
2559 #endif
2560 
2561     uint64_t (*popcount)(int64_t) = [] (int64_t value) -&gt; uint64_t { return __builtin_popcountll(value); };
2562     emitCCall(popcount, result, arg);
2563     return { };
2564 }
2565 
2566 template&lt;&gt;
2567 auto AirIRGenerator::addOp&lt;F64ConvertUI64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2568 {
2569     auto* patchpoint = addPatchpoint(B3::Double);
2570     patchpoint-&gt;effects = B3::Effects::none();
2571     if (isX86())
2572         patchpoint-&gt;numGPScratchRegisters = 1;
2573     patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
2574     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2575         AllowMacroScratchRegisterUsage allowScratch(jit);
2576 #if CPU(X86_64)
2577         jit.convertUInt64ToDouble(params[1].gpr(), params[0].fpr(), params.gpScratch(0));
2578 #else
2579         jit.convertUInt64ToDouble(params[1].gpr(), params[0].fpr());
2580 #endif
2581     });
2582     result = f64();
2583     emitPatchpoint(patchpoint, result, arg);
2584     return { };
2585 }
2586 
2587 template&lt;&gt;
2588 auto AirIRGenerator::addOp&lt;OpType::F32ConvertUI64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2589 {
2590     auto* patchpoint = addPatchpoint(B3::Float);
2591     patchpoint-&gt;effects = B3::Effects::none();
2592     if (isX86())
2593         patchpoint-&gt;numGPScratchRegisters = 1;
2594     patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
2595     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2596         AllowMacroScratchRegisterUsage allowScratch(jit);
2597 #if CPU(X86_64)
2598         jit.convertUInt64ToFloat(params[1].gpr(), params[0].fpr(), params.gpScratch(0));
2599 #else
2600         jit.convertUInt64ToFloat(params[1].gpr(), params[0].fpr());
2601 #endif
2602     });
2603     result = f32();
2604     emitPatchpoint(patchpoint, result, arg);
2605     return { };
2606 }
2607 
2608 template&lt;&gt;
2609 auto AirIRGenerator::addOp&lt;OpType::F64Nearest&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2610 {
2611     auto* patchpoint = addPatchpoint(B3::Double);
2612     patchpoint-&gt;effects = B3::Effects::none();
2613     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2614         jit.roundTowardNearestIntDouble(params[1].fpr(), params[0].fpr());
2615     });
2616     result = f64();
2617     emitPatchpoint(patchpoint, result, arg);
2618     return { };
2619 }
2620 
2621 template&lt;&gt;
2622 auto AirIRGenerator::addOp&lt;OpType::F32Nearest&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2623 {
2624     auto* patchpoint = addPatchpoint(B3::Float);
2625     patchpoint-&gt;effects = B3::Effects::none();
2626     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2627         jit.roundTowardNearestIntFloat(params[1].fpr(), params[0].fpr());
2628     });
2629     result = f32();
2630     emitPatchpoint(patchpoint, result, arg);
2631     return { };
2632 }
2633 
2634 template&lt;&gt;
2635 auto AirIRGenerator::addOp&lt;OpType::F64Trunc&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2636 {
2637     auto* patchpoint = addPatchpoint(B3::Double);
2638     patchpoint-&gt;effects = B3::Effects::none();
2639     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2640         jit.roundTowardZeroDouble(params[1].fpr(), params[0].fpr());
2641     });
2642     result = f64();
2643     emitPatchpoint(patchpoint, result, arg);
2644     return { };
2645 }
2646 
2647 template&lt;&gt;
2648 auto AirIRGenerator::addOp&lt;OpType::F32Trunc&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2649 {
2650     auto* patchpoint = addPatchpoint(B3::Float);
2651     patchpoint-&gt;effects = B3::Effects::none();
2652     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2653         jit.roundTowardZeroFloat(params[1].fpr(), params[0].fpr());
2654     });
2655     result = f32();
2656     emitPatchpoint(patchpoint, result, arg);
2657     return { };
2658 }
2659 
2660 template&lt;&gt;
2661 auto AirIRGenerator::addOp&lt;OpType::I32TruncSF64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2662 {
2663     auto max = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(-static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::min())));
2664     auto min = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::min())));
2665 
2666     auto temp1 = g32();
2667     auto temp2 = g32();
2668     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleLessThanOrUnordered), arg, min, temp1);
2669     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
2670     append(Or32, temp1, temp2);
2671 
2672     emitCheck([&amp;] {
2673         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
2674     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2675         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
2676     });
2677 
2678     auto* patchpoint = addPatchpoint(B3::Int32);
2679     patchpoint-&gt;effects = B3::Effects::none();
2680     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2681         jit.truncateDoubleToInt32(params[1].fpr(), params[0].gpr());
2682     });
2683     result = g32();
2684     emitPatchpoint(patchpoint, result, arg);
2685 
2686     return { };
2687 }
2688 
2689 template&lt;&gt;
2690 auto AirIRGenerator::addOp&lt;OpType::I32TruncSF32&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2691 {
2692     auto max = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(-static_cast&lt;float&gt;(std::numeric_limits&lt;int32_t&gt;::min())));
2693     auto min = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;int32_t&gt;::min())));
2694 
2695     auto temp1 = g32();
2696     auto temp2 = g32();
2697     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleLessThanOrUnordered), arg, min, temp1);
2698     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
2699     append(Or32, temp1, temp2);
2700 
2701     emitCheck([&amp;] {
2702         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
2703     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2704         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
2705     });
2706 
2707     auto* patchpoint = addPatchpoint(B3::Int32);
2708     patchpoint-&gt;effects = B3::Effects::none();
2709     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2710         jit.truncateFloatToInt32(params[1].fpr(), params[0].gpr());
2711     });
2712     result = g32();
2713     emitPatchpoint(patchpoint, result, arg);
2714     return { };
2715 }
2716 
2717 
2718 template&lt;&gt;
2719 auto AirIRGenerator::addOp&lt;OpType::I32TruncUF64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2720 {
2721     auto max = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::min()) * -2.0));
2722     auto min = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(-1.0));
2723 
2724     auto temp1 = g32();
2725     auto temp2 = g32();
2726     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleLessThanOrEqualOrUnordered), arg, min, temp1);
2727     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
2728     append(Or32, temp1, temp2);
2729 
2730     emitCheck([&amp;] {
2731         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
2732     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2733         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
2734     });
2735 
2736     auto* patchpoint = addPatchpoint(B3::Int32);
2737     patchpoint-&gt;effects = B3::Effects::none();
2738     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2739         jit.truncateDoubleToUint32(params[1].fpr(), params[0].gpr());
2740     });
2741     result = g32();
2742     emitPatchpoint(patchpoint, result, arg);
2743     return { };
2744 }
2745 
2746 template&lt;&gt;
2747 auto AirIRGenerator::addOp&lt;OpType::I32TruncUF32&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2748 {
2749     auto max = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;int32_t&gt;::min()) * static_cast&lt;float&gt;(-2.0)));
2750     auto min = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(-1.0)));
2751 
2752     auto temp1 = g32();
2753     auto temp2 = g32();
2754     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleLessThanOrEqualOrUnordered), arg, min, temp1);
2755     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
2756     append(Or32, temp1, temp2);
2757 
2758     emitCheck([&amp;] {
2759         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
2760     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2761         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
2762     });
2763 
2764     auto* patchpoint = addPatchpoint(B3::Int32);
2765     patchpoint-&gt;effects = B3::Effects::none();
2766     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2767         jit.truncateFloatToUint32(params[1].fpr(), params[0].gpr());
2768     });
2769     result = g32();
2770     emitPatchpoint(patchpoint, result, arg);
2771     return { };
2772 }
2773 
2774 template&lt;&gt;
2775 auto AirIRGenerator::addOp&lt;OpType::I64TruncSF64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2776 {
2777     auto max = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(-static_cast&lt;double&gt;(std::numeric_limits&lt;int64_t&gt;::min())));
2778     auto min = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;int64_t&gt;::min())));
2779 
2780     auto temp1 = g32();
2781     auto temp2 = g32();
2782     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleLessThanOrUnordered), arg, min, temp1);
2783     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
2784     append(Or32, temp1, temp2);
2785 
2786     emitCheck([&amp;] {
2787         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
2788     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2789         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
2790     });
2791 
2792     auto* patchpoint = addPatchpoint(B3::Int64);
2793     patchpoint-&gt;effects = B3::Effects::none();
2794     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2795         jit.truncateDoubleToInt64(params[1].fpr(), params[0].gpr());
2796     });
2797 
2798     result = g64();
2799     emitPatchpoint(patchpoint, result, arg);
2800     return { };
2801 }
2802 
2803 template&lt;&gt;
2804 auto AirIRGenerator::addOp&lt;OpType::I64TruncUF64&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2805 {
2806     auto max = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;int64_t&gt;::min()) * -2.0));
2807     auto min = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(-1.0));
2808 
2809     auto temp1 = g32();
2810     auto temp2 = g32();
2811     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleLessThanOrEqualOrUnordered), arg, min, temp1);
2812     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
2813     append(Or32, temp1, temp2);
2814 
2815     emitCheck([&amp;] {
2816         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
2817     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2818         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
2819     });
2820 
2821     TypedTmp signBitConstant;
2822     if (isX86())
2823         signBitConstant = addConstant(Type::F64, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(std::numeric_limits&lt;uint64_t&gt;::max() - std::numeric_limits&lt;int64_t&gt;::max())));
2824 
2825     Vector&lt;ConstrainedTmp&gt; args;
2826     auto* patchpoint = addPatchpoint(B3::Int64);
2827     patchpoint-&gt;effects = B3::Effects::none();
2828     patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
2829     args.append(arg);
2830     if (isX86()) {
2831         args.append(signBitConstant);
2832         patchpoint-&gt;numFPScratchRegisters = 1;
2833     }
2834     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2835         AllowMacroScratchRegisterUsage allowScratch(jit);
2836         FPRReg scratch = InvalidFPRReg;
2837         FPRReg constant = InvalidFPRReg;
2838         if (isX86()) {
2839             scratch = params.fpScratch(0);
2840             constant = params[2].fpr();
2841         }
2842         jit.truncateDoubleToUint64(params[1].fpr(), params[0].gpr(), scratch, constant);
2843     });
2844 
2845     result = g64();
2846     emitPatchpoint(m_currentBlock, patchpoint, result, WTFMove(args));
2847     return { };
2848 }
2849 
2850 template&lt;&gt;
2851 auto AirIRGenerator::addOp&lt;OpType::I64TruncSF32&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2852 {
2853     auto max = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(-static_cast&lt;float&gt;(std::numeric_limits&lt;int64_t&gt;::min())));
2854     auto min = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;int64_t&gt;::min())));
2855 
2856     auto temp1 = g32();
2857     auto temp2 = g32();
2858     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleLessThanOrUnordered), arg, min, temp1);
2859     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
2860     append(Or32, temp1, temp2);
2861 
2862     emitCheck([&amp;] {
2863         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
2864     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2865         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
2866     });
2867 
2868     auto* patchpoint = addPatchpoint(B3::Int64);
2869     patchpoint-&gt;effects = B3::Effects::none();
2870     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2871         jit.truncateFloatToInt64(params[1].fpr(), params[0].gpr());
2872     });
2873     result = g64();
2874     emitPatchpoint(patchpoint, result, arg);
2875     return { };
2876 }
2877 
2878 template&lt;&gt;
2879 auto AirIRGenerator::addOp&lt;OpType::I64TruncUF32&gt;(ExpressionType arg, ExpressionType&amp; result) -&gt; PartialResult
2880 {
2881     auto max = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;int64_t&gt;::min()) * static_cast&lt;float&gt;(-2.0)));
2882     auto min = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(-1.0)));
2883 
2884     auto temp1 = g32();
2885     auto temp2 = g32();
2886     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleLessThanOrEqualOrUnordered), arg, min, temp1);
2887     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqualOrUnordered), arg, max, temp2);
2888     append(Or32, temp1, temp2);
2889 
2890     emitCheck([&amp;] {
2891         return Inst(BranchTest32, nullptr, Arg::resCond(MacroAssembler::NonZero), temp2, temp2);
2892     }, [=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp;) {
2893         this-&gt;emitThrowException(jit, ExceptionType::OutOfBoundsTrunc);
2894     });
2895 
2896     TypedTmp signBitConstant;
2897     if (isX86())
2898         signBitConstant = addConstant(Type::F32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(std::numeric_limits&lt;uint64_t&gt;::max() - std::numeric_limits&lt;int64_t&gt;::max())));
2899 
2900     auto* patchpoint = addPatchpoint(B3::Int64);
2901     patchpoint-&gt;effects = B3::Effects::none();
2902     patchpoint-&gt;clobber(RegisterSet::macroScratchRegisters());
2903     Vector&lt;ConstrainedTmp&gt; args;
2904     args.append(arg);
2905     if (isX86()) {
2906         args.append(signBitConstant);
2907         patchpoint-&gt;numFPScratchRegisters = 1;
2908     }
2909     patchpoint-&gt;setGenerator([=] (CCallHelpers&amp; jit, const B3::StackmapGenerationParams&amp; params) {
2910         AllowMacroScratchRegisterUsage allowScratch(jit);
2911         FPRReg scratch = InvalidFPRReg;
2912         FPRReg constant = InvalidFPRReg;
2913         if (isX86()) {
2914             scratch = params.fpScratch(0);
2915             constant = params[2].fpr();
2916         }
2917         jit.truncateFloatToUint64(params[1].fpr(), params[0].gpr(), scratch, constant);
2918     });
2919 
2920     result = g64();
2921     emitPatchpoint(m_currentBlock, patchpoint, result, WTFMove(args));
2922 
2923     return { };
2924 }
2925 
2926 auto AirIRGenerator::addShift(Type type, B3::Air::Opcode op, ExpressionType value, ExpressionType shift, ExpressionType&amp; result) -&gt; PartialResult
2927 {
2928     ASSERT(type == Type::I64 || type == Type::I32);
2929     result = tmpForType(type);
2930 
2931     if (isValidForm(op, Arg::Tmp, Arg::Tmp, Arg::Tmp)) {
2932         append(op, value, shift, result);
2933         return { };
2934     }
2935 
2936 #if CPU(X86_64)
2937     Tmp ecx = Tmp(X86Registers::ecx);
2938     append(Move, value, result);
2939     append(Move, shift, ecx);
2940     append(op, ecx, result);
2941 #else
2942     RELEASE_ASSERT_NOT_REACHED();
2943 #endif
2944     return { };
2945 }
2946 
2947 auto AirIRGenerator::addIntegerSub(B3::Air::Opcode op, ExpressionType lhs, ExpressionType rhs, ExpressionType&amp; result) -&gt; PartialResult
2948 {
2949     ASSERT(op == Sub32 || op == Sub64);
2950 
2951     result = op == Sub32 ? g32() : g64();
2952 
2953     if (isValidForm(op, Arg::Tmp, Arg::Tmp, Arg::Tmp)) {
2954         append(op, lhs, rhs, result);
2955         return { };
2956     }
2957 
2958     RELEASE_ASSERT(isX86());
2959     // Sub a, b
2960     // means
2961     // b = b Sub a
2962     append(Move, lhs, result);
2963     append(op, rhs, result);
2964     return { };
2965 }
2966 
2967 auto AirIRGenerator::addFloatingPointAbs(B3::Air::Opcode op, ExpressionType value, ExpressionType&amp; result) -&gt; PartialResult
2968 {
2969     RELEASE_ASSERT(op == AbsFloat || op == AbsDouble);
2970 
2971     result = op == AbsFloat ? f32() : f64();
2972 
2973     if (isValidForm(op, Arg::Tmp, Arg::Tmp)) {
2974         append(op, value, result);
2975         return { };
2976     }
2977 
2978     RELEASE_ASSERT(isX86());
2979 
2980     if (op == AbsFloat) {
2981         auto constant = g32();
2982         append(Move, Arg::imm(static_cast&lt;uint32_t&gt;(~(1ull &lt;&lt; 31))), constant);
2983         append(Move32ToFloat, constant, result);
2984         append(AndFloat, value, result);
2985     } else {
2986         auto constant = g64();
2987         append(Move, Arg::bigImm(~(1ull &lt;&lt; 63)), constant);
2988         append(Move64ToDouble, constant, result);
2989         append(AndDouble, value, result);
2990     }
2991     return { };
2992 }
2993 
2994 auto AirIRGenerator::addFloatingPointBinOp(Type type, B3::Air::Opcode op, ExpressionType lhs, ExpressionType rhs, ExpressionType&amp; result) -&gt; PartialResult
2995 {
2996     ASSERT(type == Type::F32 || type == Type::F64);
2997     result = tmpForType(type);
2998 
2999     if (isValidForm(op, Arg::Tmp, Arg::Tmp, Arg::Tmp)) {
3000         append(op, lhs, rhs, result);
3001         return { };
3002     }
3003 
3004     RELEASE_ASSERT(isX86());
3005 
3006     // Op a, b
3007     // means
3008     // b = b Op a
3009     append(moveOpForValueType(type), lhs, result);
3010     append(op, rhs, result);
3011     return { };
3012 }
3013 
3014 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Ceil&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3015 {
3016     result = f32();
3017     append(CeilFloat, arg0, result);
3018     return { };
3019 }
3020 
3021 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Mul&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3022 {
3023     result = g32();
3024     append(Mul32, arg0, arg1, result);
3025     return { };
3026 }
3027 
3028 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Sub&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3029 {
3030     return addIntegerSub(Sub32, arg0, arg1, result);
3031 }
3032 
3033 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Le&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3034 {
3035     result = g32();
3036     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleLessThanOrEqual), arg0, arg1, result);
3037     return { };
3038 }
3039 
3040 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32DemoteF64&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3041 {
3042     result = f32();
3043     append(ConvertDoubleToFloat, arg0, result);
3044     return { };
3045 }
3046 
3047 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Min&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3048 {
3049     return addFloatingPointMinOrMax(F32, MinOrMax::Min, arg0, arg1, result);
3050 }
3051 
3052 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Ne&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3053 {
3054     result = g32();
3055     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleNotEqualOrUnordered), arg0, arg1, result);
3056     return { };
3057 }
3058 
3059 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Lt&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3060 {
3061     result = g32();
3062     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleLessThan), arg0, arg1, result);
3063     return { };
3064 }
3065 
3066 auto AirIRGenerator::addFloatingPointMinOrMax(Type floatType, MinOrMax minOrMax, ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3067 {
3068     ASSERT(floatType == F32 || floatType == F64);
3069     result = tmpForType(floatType);
3070 
3071     BasicBlock* isEqual = m_code.addBlock();
3072     BasicBlock* notEqual = m_code.addBlock();
3073     BasicBlock* isLessThan = m_code.addBlock();
3074     BasicBlock* notLessThan = m_code.addBlock();
3075     BasicBlock* isGreaterThan = m_code.addBlock();
3076     BasicBlock* isNaN = m_code.addBlock();
3077     BasicBlock* continuation = m_code.addBlock();
3078 
3079     auto branchOp = floatType == F32 ? BranchFloat : BranchDouble;
3080     append(m_currentBlock, branchOp, Arg::doubleCond(MacroAssembler::DoubleEqual), arg0, arg1);
3081     m_currentBlock-&gt;setSuccessors(isEqual, notEqual);
3082 
3083     append(notEqual, branchOp, Arg::doubleCond(MacroAssembler::DoubleLessThan), arg0, arg1);
3084     notEqual-&gt;setSuccessors(isLessThan, notLessThan);
3085 
3086     append(notLessThan, branchOp, Arg::doubleCond(MacroAssembler::DoubleGreaterThan), arg0, arg1);
3087     notLessThan-&gt;setSuccessors(isGreaterThan, isNaN);
3088 
3089     auto andOp = floatType == F32 ? AndFloat : AndDouble;
3090     auto orOp = floatType == F32 ? OrFloat : OrDouble;
3091     append(isEqual, minOrMax == MinOrMax::Max ? andOp : orOp, arg0, arg1, result);
3092     append(isEqual, Jump);
3093     isEqual-&gt;setSuccessors(continuation);
3094 
3095     auto isLessThanResult = minOrMax == MinOrMax::Max ? arg1 : arg0;
3096     append(isLessThan, moveOpForValueType(floatType), isLessThanResult, result);
3097     append(isLessThan, Jump);
3098     isLessThan-&gt;setSuccessors(continuation);
3099 
3100     auto isGreaterThanResult = minOrMax == MinOrMax::Max ? arg0 : arg1;
3101     append(isGreaterThan, moveOpForValueType(floatType), isGreaterThanResult, result);
3102     append(isGreaterThan, Jump);
3103     isGreaterThan-&gt;setSuccessors(continuation);
3104 
3105     auto addOp = floatType == F32 ? AddFloat : AddDouble;
3106     append(isNaN, addOp, arg0, arg1, result);
3107     append(isNaN, Jump);
3108     isNaN-&gt;setSuccessors(continuation);
3109 
3110     m_currentBlock = continuation;
3111 
3112     return { };
3113 }
3114 
3115 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Max&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3116 {
3117     return addFloatingPointMinOrMax(F32, MinOrMax::Max, arg0, arg1, result);
3118 }
3119 
3120 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Mul&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3121 {
3122     return addFloatingPointBinOp(Type::F64, MulDouble, arg0, arg1, result);
3123 }
3124 
3125 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Div&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3126 {
3127     return addFloatingPointBinOp(Type::F32, DivFloat, arg0, arg1, result);
3128 }
3129 
3130 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Clz&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3131 {
3132     result = g32();
3133     append(CountLeadingZeros32, arg0, result);
3134     return { };
3135 }
3136 
3137 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Copysign&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3138 {
3139     // FIXME: We can have better codegen here for the imms and two operand forms on x86
3140     // https://bugs.webkit.org/show_bug.cgi?id=193999
3141     result = f32();
3142     auto temp1 = g32();
3143     auto sign = g32();
3144     auto value = g32();
3145 
3146     // FIXME: Try to use Imm where possible:
3147     // https://bugs.webkit.org/show_bug.cgi?id=193999
3148     append(MoveFloatTo32, arg1, temp1);
3149     append(Move, Arg::bigImm(0x80000000), sign);
3150     append(And32, temp1, sign, sign);
3151 
3152     append(MoveDoubleTo64, arg0, temp1);
3153     append(Move, Arg::bigImm(0x7fffffff), value);
3154     append(And32, temp1, value, value);
3155 
3156     append(Or32, sign, value, value);
3157     append(Move32ToFloat, value, result);
3158 
3159     return { };
3160 }
3161 
3162 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64ConvertUI32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3163 {
3164     result = f64();
3165     auto temp = g64();
3166     append(Move32, arg0, temp);
3167     append(ConvertInt64ToDouble, temp, result);
3168     return { };
3169 }
3170 
3171 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32ReinterpretI32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3172 {
3173     result = f32();
3174     append(Move32ToFloat, arg0, result);
3175     return { };
3176 }
3177 
3178 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64And&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3179 {
3180     result = g64();
3181     append(And64, arg0, arg1, result);
3182     return { };
3183 }
3184 
3185 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Ne&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3186 {
3187     result = g32();
3188     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleNotEqualOrUnordered), arg0, arg1, result);
3189     return { };
3190 }
3191 
3192 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Gt&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3193 {
3194     result = g32();
3195     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleGreaterThan), arg0, arg1, result);
3196     return { };
3197 }
3198 
3199 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Sqrt&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3200 {
3201     result = f32();
3202     append(SqrtFloat, arg0, result);
3203     return { };
3204 }
3205 
3206 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Ge&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3207 {
3208     result = g32();
3209     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqual), arg0, arg1, result);
3210     return { };
3211 }
3212 
3213 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64GtS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3214 {
3215     result = g32();
3216     append(Compare64, Arg::relCond(MacroAssembler::GreaterThan), arg0, arg1, result);
3217     return { };
3218 }
3219 
3220 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64GtU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3221 {
3222     result = g32();
3223     append(Compare64, Arg::relCond(MacroAssembler::Above), arg0, arg1, result);
3224     return { };
3225 }
3226 
3227 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Eqz&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3228 {
3229     result = g32();
3230     append(Test64, Arg::resCond(MacroAssembler::Zero), arg0, arg0, result);
3231     return { };
3232 }
3233 
3234 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Div&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3235 {
3236     return addFloatingPointBinOp(Type::F64, DivDouble, arg0, arg1, result);
3237 }
3238 
3239 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Add&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3240 {
3241     result = f32();
3242     append(AddFloat, arg0, arg1, result);
3243     return { };
3244 }
3245 
3246 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Or&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3247 {
3248     result = g64();
3249     append(Or64, arg0, arg1, result);
3250     return { };
3251 }
3252 
3253 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32LeU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3254 {
3255     result = g32();
3256     append(Compare32, Arg::relCond(MacroAssembler::BelowOrEqual), arg0, arg1, result);
3257     return { };
3258 }
3259 
3260 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32LeS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3261 {
3262     result = g32();
3263     append(Compare32, Arg::relCond(MacroAssembler::LessThanOrEqual), arg0, arg1, result);
3264     return { };
3265 }
3266 
3267 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Ne&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3268 {
3269     result = g32();
3270     append(Compare64, Arg::relCond(MacroAssembler::NotEqual), arg0, arg1, result);
3271     return { };
3272 }
3273 
3274 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Clz&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3275 {
3276     result = g64();
3277     append(CountLeadingZeros64, arg0, result);
3278     return { };
3279 }
3280 
3281 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Neg&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3282 {
3283     result = f32();
3284     if (isValidForm(NegateFloat, Arg::Tmp, Arg::Tmp))
3285         append(NegateFloat, arg0, result);
3286     else {
3287         auto constant = addConstant(Type::I32, bitwise_cast&lt;uint32_t&gt;(static_cast&lt;float&gt;(-0.0)));
3288         auto temp = g32();
3289         append(MoveFloatTo32, arg0, temp);
3290         append(Xor32, constant, temp);
3291         append(Move32ToFloat, temp, result);
3292     }
3293     return { };
3294 }
3295 
3296 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32And&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3297 {
3298     result = g32();
3299     append(And32, arg0, arg1, result);
3300     return { };
3301 }
3302 
3303 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32LtU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3304 {
3305     result = g32();
3306     append(Compare32, Arg::relCond(MacroAssembler::Below), arg0, arg1, result);
3307     return { };
3308 }
3309 
3310 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Rotr&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3311 {
3312     return addShift(Type::I64, RotateRight64, arg0, arg1, result);
3313 }
3314 
3315 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Abs&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3316 {
3317     return addFloatingPointAbs(AbsDouble, arg0, result);
3318 }
3319 
3320 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32LtS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3321 {
3322     result = g32();
3323     append(Compare32, Arg::relCond(MacroAssembler::LessThan), arg0, arg1, result);
3324     return { };
3325 }
3326 
3327 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Eq&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3328 {
3329     result = g32();
3330     append(Compare32, Arg::relCond(MacroAssembler::Equal), arg0, arg1, result);
3331     return { };
3332 }
3333 
3334 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Copysign&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3335 {
3336     // FIXME: We can have better codegen here for the imms and two operand forms on x86
3337     // https://bugs.webkit.org/show_bug.cgi?id=193999
3338     result = f64();
3339     auto temp1 = g64();
3340     auto sign = g64();
3341     auto value = g64();
3342 
3343     append(MoveDoubleTo64, arg1, temp1);
3344     append(Move, Arg::bigImm(0x8000000000000000), sign);
3345     append(And64, temp1, sign, sign);
3346 
3347     append(MoveDoubleTo64, arg0, temp1);
3348     append(Move, Arg::bigImm(0x7fffffffffffffff), value);
3349     append(And64, temp1, value, value);
3350 
3351     append(Or64, sign, value, value);
3352     append(Move64ToDouble, value, result);
3353 
3354     return { };
3355 }
3356 
3357 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32ConvertSI64&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3358 {
3359     result = f32();
3360     append(ConvertInt64ToFloat, arg0, result);
3361     return { };
3362 }
3363 
3364 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Rotl&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3365 {
3366     if (isARM64()) {
3367         // ARM64 doesn&#39;t have a rotate left.
3368         auto newShift = g64();
3369         append(Move, arg1, newShift);
3370         append(Neg64, newShift);
3371         return addShift(Type::I64, RotateRight64, arg0, newShift, result);
3372     } else
3373         return addShift(Type::I64, RotateLeft64, arg0, arg1, result);
3374 }
3375 
3376 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Lt&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3377 {
3378     result = g32();
3379     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleLessThan), arg0, arg1, result);
3380     return { };
3381 }
3382 
3383 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64ConvertSI32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3384 {
3385     result = f64();
3386     append(ConvertInt32ToDouble, arg0, result);
3387     return { };
3388 }
3389 
3390 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Eq&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3391 {
3392     result = g32();
3393     append(CompareDouble, Arg::doubleCond(MacroAssembler::DoubleEqual), arg0, arg1, result);
3394     return { };
3395 }
3396 
3397 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Le&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3398 {
3399     result = g32();
3400     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleLessThanOrEqual), arg0, arg1, result);
3401     return { };
3402 }
3403 
3404 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Ge&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3405 {
3406     result = g32();
3407     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleGreaterThanOrEqual), arg0, arg1, result);
3408     return { };
3409 }
3410 
3411 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32ShrU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3412 {
3413     return addShift(Type::I32, Urshift32, arg0, arg1, result);
3414 }
3415 
3416 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32ConvertUI32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3417 {
3418     result = f32();
3419     auto temp = g64();
3420     append(Move32, arg0, temp);
3421     append(ConvertInt64ToFloat, temp, result);
3422     return { };
3423 }
3424 
3425 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32ShrS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3426 {
3427     return addShift(Type::I32, Rshift32, arg0, arg1, result);
3428 }
3429 
3430 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32GeU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3431 {
3432     result = g32();
3433     append(Compare32, Arg::relCond(MacroAssembler::AboveOrEqual), arg0, arg1, result);
3434     return { };
3435 }
3436 
3437 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Ceil&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3438 {
3439     result = f64();
3440     append(CeilDouble, arg0, result);
3441     return { };
3442 }
3443 
3444 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32GeS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3445 {
3446     result = g32();
3447     append(Compare32, Arg::relCond(MacroAssembler::GreaterThanOrEqual), arg0, arg1, result);
3448     return { };
3449 }
3450 
3451 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Shl&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3452 {
3453     return addShift(Type::I32, Lshift32, arg0, arg1, result);
3454 }
3455 
3456 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Floor&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3457 {
3458     result = f64();
3459     append(FloorDouble, arg0, result);
3460     return { };
3461 }
3462 
3463 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Xor&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3464 {
3465     result = g32();
3466     append(Xor32, arg0, arg1, result);
3467     return { };
3468 }
3469 
3470 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Abs&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3471 {
3472     return addFloatingPointAbs(AbsFloat, arg0, result);
3473 }
3474 
3475 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Min&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3476 {
3477     return addFloatingPointMinOrMax(F64, MinOrMax::Min, arg0, arg1, result);
3478 }
3479 
3480 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Mul&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3481 {
3482     result = f32();
3483     append(MulFloat, arg0, arg1, result);
3484     return { };
3485 }
3486 
3487 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Sub&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3488 {
3489     return addIntegerSub(Sub64, arg0, arg1, result);
3490 }
3491 
3492 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32ReinterpretF32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3493 {
3494     result = g32();
3495     append(MoveFloatTo32, arg0, result);
3496     return { };
3497 }
3498 
3499 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Add&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3500 {
3501     result = g32();
3502     append(Add32, arg0, arg1, result);
3503     return { };
3504 }
3505 
3506 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Sub&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3507 {
3508     return addFloatingPointBinOp(Type::F64, SubDouble, arg0, arg1, result);
3509 }
3510 
3511 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Or&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3512 {
3513     result = g32();
3514     append(Or32, arg0, arg1, result);
3515     return { };
3516 }
3517 
3518 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64LtU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3519 {
3520     result = g32();
3521     append(Compare64, Arg::relCond(MacroAssembler::Below), arg0, arg1, result);
3522     return { };
3523 }
3524 
3525 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64LtS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3526 {
3527     result = g32();
3528     append(Compare64, Arg::relCond(MacroAssembler::LessThan), arg0, arg1, result);
3529     return { };
3530 }
3531 
3532 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64ConvertSI64&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3533 {
3534     result = f64();
3535     append(ConvertInt64ToDouble, arg0, result);
3536     return { };
3537 }
3538 
3539 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Xor&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3540 {
3541     result = g64();
3542     append(Xor64, arg0, arg1, result);
3543     return { };
3544 }
3545 
3546 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64GeU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3547 {
3548     result = g32();
3549     append(Compare64, Arg::relCond(MacroAssembler::AboveOrEqual), arg0, arg1, result);
3550     return { };
3551 }
3552 
3553 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Mul&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3554 {
3555     result = g64();
3556     append(Mul64, arg0, arg1, result);
3557     return { };
3558 }
3559 
3560 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Sub&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3561 {
3562     result = f32();
3563     if (isValidForm(SubFloat, Arg::Tmp, Arg::Tmp, Arg::Tmp))
3564         append(SubFloat, arg0, arg1, result);
3565     else {
3566         RELEASE_ASSERT(isX86());
3567         append(MoveFloat, arg0, result);
3568         append(SubFloat, arg1, result);
3569     }
3570     return { };
3571 }
3572 
3573 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64PromoteF32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3574 {
3575     result = f64();
3576     append(ConvertFloatToDouble, arg0, result);
3577     return { };
3578 }
3579 
3580 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Add&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3581 {
3582     result = f64();
3583     append(AddDouble, arg0, arg1, result);
3584     return { };
3585 }
3586 
3587 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64GeS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3588 {
3589     result = g32();
3590     append(Compare64, Arg::relCond(MacroAssembler::GreaterThanOrEqual), arg0, arg1, result);
3591     return { };
3592 }
3593 
3594 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64ExtendUI32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3595 {
3596     result = g64();
3597     append(Move32, arg0, result);
3598     return { };
3599 }
3600 
3601 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Ne&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3602 {
3603     result = g32();
3604     RELEASE_ASSERT(arg0 &amp;&amp; arg1);
3605     append(Compare32, Arg::relCond(MacroAssembler::NotEqual), arg0, arg1, result);
3606     return { };
3607 }
3608 
3609 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64ReinterpretI64&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3610 {
3611     result = f64();
3612     append(Move64ToDouble, arg0, result);
3613     return { };
3614 }
3615 
3616 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Eq&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3617 {
3618     result = g32();
3619     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleEqual), arg0, arg1, result);
3620     return { };
3621 }
3622 
3623 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Eq&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3624 {
3625     result = g32();
3626     append(Compare64, Arg::relCond(MacroAssembler::Equal), arg0, arg1, result);
3627     return { };
3628 }
3629 
3630 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Floor&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3631 {
3632     result = f32();
3633     append(FloorFloat, arg0, result);
3634     return { };
3635 }
3636 
3637 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32ConvertSI32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3638 {
3639     result = f32();
3640     append(ConvertInt32ToFloat, arg0, result);
3641     return { };
3642 }
3643 
3644 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Eqz&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3645 {
3646     result = g32();
3647     append(Test32, Arg::resCond(MacroAssembler::Zero), arg0, arg0, result);
3648     return { };
3649 }
3650 
3651 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64ReinterpretF64&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3652 {
3653     result = g64();
3654     append(MoveDoubleTo64, arg0, result);
3655     return { };
3656 }
3657 
3658 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64ShrS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3659 {
3660     return addShift(Type::I64, Rshift64, arg0, arg1, result);
3661 }
3662 
3663 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64ShrU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3664 {
3665     return addShift(Type::I64, Urshift64, arg0, arg1, result);
3666 }
3667 
3668 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Sqrt&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3669 {
3670     result = f64();
3671     append(SqrtDouble, arg0, result);
3672     return { };
3673 }
3674 
3675 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Shl&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3676 {
3677     return addShift(Type::I64, Lshift64, arg0, arg1, result);
3678 }
3679 
3680 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F32Gt&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3681 {
3682     result = g32();
3683     append(CompareFloat, Arg::doubleCond(MacroAssembler::DoubleGreaterThan), arg0, arg1, result);
3684     return { };
3685 }
3686 
3687 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32WrapI64&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3688 {
3689     result = g32();
3690     append(Move32, arg0, result);
3691     return { };
3692 }
3693 
3694 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Rotl&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3695 {
3696     if (isARM64()) {
3697         // ARM64 doesn&#39;t have a rotate left.
3698         auto newShift = g64();
3699         append(Move, arg1, newShift);
3700         append(Neg64, newShift);
3701         return addShift(Type::I32, RotateRight32, arg0, newShift, result);
3702     } else
3703         return addShift(Type::I32, RotateLeft32, arg0, arg1, result);
3704 }
3705 
3706 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32Rotr&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3707 {
3708     return addShift(Type::I32, RotateRight32, arg0, arg1, result);
3709 }
3710 
3711 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32GtU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3712 {
3713     result = g32();
3714     append(Compare32, Arg::relCond(MacroAssembler::Above), arg0, arg1, result);
3715     return { };
3716 }
3717 
3718 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64ExtendSI32&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3719 {
3720     result = g64();
3721     append(SignExtend32ToPtr, arg0, result);
3722     return { };
3723 }
3724 
3725 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I32GtS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3726 {
3727     result = g32();
3728     append(Compare32, Arg::relCond(MacroAssembler::GreaterThan), arg0, arg1, result);
3729     return { };
3730 }
3731 
3732 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Neg&gt;(ExpressionType arg0, ExpressionType&amp; result) -&gt; PartialResult
3733 {
3734     result = f64();
3735     if (isValidForm(NegateDouble, Arg::Tmp, Arg::Tmp))
3736         append(NegateDouble, arg0, result);
3737     else {
3738         auto constant = addConstant(Type::I64, bitwise_cast&lt;uint64_t&gt;(static_cast&lt;double&gt;(-0.0)));
3739         auto temp = g64();
3740         append(MoveDoubleTo64, arg0, temp);
3741         append(Xor64, constant, temp);
3742         append(Move64ToDouble, temp, result);
3743     }
3744     return { };
3745 }
3746 
3747 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::F64Max&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3748 {
3749     return addFloatingPointMinOrMax(F64, MinOrMax::Max, arg0, arg1, result);
3750 }
3751 
3752 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64LeU&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3753 {
3754     result = g32();
3755     append(Compare64, Arg::relCond(MacroAssembler::BelowOrEqual), arg0, arg1, result);
3756     return { };
3757 }
3758 
3759 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64LeS&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3760 {
3761     result = g32();
3762     append(Compare64, Arg::relCond(MacroAssembler::LessThanOrEqual), arg0, arg1, result);
3763     return { };
3764 }
3765 
3766 template&lt;&gt; auto AirIRGenerator::addOp&lt;OpType::I64Add&gt;(ExpressionType arg0, ExpressionType arg1, ExpressionType&amp; result) -&gt; PartialResult
3767 {
3768     result = g64();
3769     append(Add64, arg0, arg1, result);
3770     return { };
3771 }
3772 
3773 } } // namespace JSC::Wasm
3774 
3775 #endif // ENABLE(WEBASSEMBLY)
    </pre>
  </body>
</html>