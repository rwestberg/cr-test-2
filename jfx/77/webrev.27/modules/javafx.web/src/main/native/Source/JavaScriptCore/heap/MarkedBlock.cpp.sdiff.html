<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="MachineStackMarker.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="MarkedBlock.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (C) 2011-2018 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
 14  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 15  * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
 17  * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 18  * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 19  * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 20  * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 21  * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 22  * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 23  * THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;MarkedBlock.h&quot;
 28 
 29 #include &quot;AlignedMemoryAllocator.h&quot;
 30 #include &quot;BlockDirectoryInlines.h&quot;
 31 #include &quot;FreeListInlines.h&quot;
 32 #include &quot;JSCast.h&quot;
 33 #include &quot;JSDestructibleObject.h&quot;
 34 #include &quot;JSCInlines.h&quot;
 35 #include &quot;MarkedBlockInlines.h&quot;
 36 #include &quot;SuperSampler.h&quot;
 37 #include &quot;SweepingScope.h&quot;
 38 #include &lt;wtf/CommaPrinter.h&gt;
 39 
 40 namespace JSC {



 41 
 42 const size_t MarkedBlock::blockSize;
 43 
 44 static const bool computeBalance = false;
 45 static size_t balance;
 46 
 47 MarkedBlock::Handle* MarkedBlock::tryCreate(Heap&amp; heap, AlignedMemoryAllocator* alignedMemoryAllocator)
 48 {
 49     if (computeBalance) {
 50         balance++;
 51         if (!(balance % 10))
 52             dataLog(&quot;MarkedBlock Balance: &quot;, balance, &quot;\n&quot;);
 53     }
 54     void* blockSpace = alignedMemoryAllocator-&gt;tryAllocateAlignedMemory(blockSize, blockSize);
 55     if (!blockSpace)
 56         return nullptr;
 57     if (scribbleFreeCells())
 58         scribble(blockSpace, blockSize);
 59     return new Handle(heap, alignedMemoryAllocator, blockSpace);
 60 }
 61 
 62 MarkedBlock::Handle::Handle(Heap&amp; heap, AlignedMemoryAllocator* alignedMemoryAllocator, void* blockSpace)
 63     : m_alignedMemoryAllocator(alignedMemoryAllocator)
 64     , m_weakSet(heap.vm(), CellContainer())
 65 {
<span class="line-modified"> 66     m_block = new (NotNull, blockSpace) MarkedBlock(*heap.vm(), *this);</span>
 67 
 68     m_weakSet.setContainer(*m_block);
 69 
 70     heap.didAllocateBlock(blockSize);
 71 }
 72 
 73 MarkedBlock::Handle::~Handle()
 74 {
 75     Heap&amp; heap = *this-&gt;heap();
 76     if (computeBalance) {
 77         balance--;
 78         if (!(balance % 10))
 79             dataLog(&quot;MarkedBlock Balance: &quot;, balance, &quot;\n&quot;);
 80     }
 81     removeFromDirectory();
 82     m_block-&gt;~MarkedBlock();
 83     m_alignedMemoryAllocator-&gt;freeAlignedMemory(m_block);
 84     heap.didFreeBlock(blockSize);
 85 }
 86 
 87 MarkedBlock::MarkedBlock(VM&amp; vm, Handle&amp; handle)
 88 {
 89     new (&amp;footer()) Footer(vm, handle);
<span class="line-modified"> 90     if (false)</span>
 91         dataLog(RawPointer(this), &quot;: Allocated.\n&quot;);
 92 }
 93 
 94 MarkedBlock::~MarkedBlock()
 95 {
 96     footer().~Footer();
 97 }
 98 
 99 MarkedBlock::Footer::Footer(VM&amp; vm, Handle&amp; handle)
100     : m_handle(handle)
101     , m_vm(&amp;vm)
102     , m_markingVersion(MarkedSpace::nullVersion)
103     , m_newlyAllocatedVersion(MarkedSpace::nullVersion)
104 {
105 }
106 
107 MarkedBlock::Footer::~Footer()
108 {
109 }
110 
111 void MarkedBlock::Handle::unsweepWithNoNewlyAllocated()
112 {
113     RELEASE_ASSERT(m_isFreeListed);
114     m_isFreeListed = false;
115 }
116 
117 void MarkedBlock::Handle::setIsFreeListed()
118 {
119     m_directory-&gt;setIsEmpty(NoLockingNecessary, this, false);
120     m_isFreeListed = true;
121 }
122 
123 void MarkedBlock::Handle::stopAllocating(const FreeList&amp; freeList)
124 {
125     auto locker = holdLock(blockFooter().m_lock);
126 
<span class="line-modified">127     if (false)</span>
128         dataLog(RawPointer(this), &quot;: MarkedBlock::Handle::stopAllocating!\n&quot;);
129     ASSERT(!directory()-&gt;isAllocated(NoLockingNecessary, this));
130 
131     if (!isFreeListed()) {
<span class="line-modified">132         if (false)</span>
133             dataLog(&quot;There ain&#39;t no newly allocated.\n&quot;);
134         // This means that we either didn&#39;t use this block at all for allocation since last GC,
135         // or someone had already done stopAllocating() before.
136         ASSERT(freeList.allocationWillFail());
137         return;
138     }
139 
<span class="line-modified">140     if (false)</span>
141         dataLog(&quot;Free list: &quot;, freeList, &quot;\n&quot;);
142 
143     // Roll back to a coherent state for Heap introspection. Cells newly
144     // allocated from our free list are not currently marked, so we need another
145     // way to tell what&#39;s live vs dead.
146 
147     blockFooter().m_newlyAllocated.clearAll();
148     blockFooter().m_newlyAllocatedVersion = heap()-&gt;objectSpace().newlyAllocatedVersion();
149 
150     forEachCell(
151         [&amp;] (HeapCell* cell, HeapCell::Kind) -&gt; IterationStatus {
152             block().setNewlyAllocated(cell);
153             return IterationStatus::Continue;
154         });
155 
156     freeList.forEach(
157         [&amp;] (HeapCell* cell) {
<span class="line-modified">158             if (false)</span>
159                 dataLog(&quot;Free cell: &quot;, RawPointer(cell), &quot;\n&quot;);
160             if (m_attributes.destruction == NeedsDestruction)
<span class="line-modified">161                 cell-&gt;zap();</span>
162             block().clearNewlyAllocated(cell);
163         });
164 
165     m_isFreeListed = false;
166 }
167 
168 void MarkedBlock::Handle::lastChanceToFinalize()
169 {
170     directory()-&gt;setIsAllocated(NoLockingNecessary, this, false);
171     directory()-&gt;setIsDestructible(NoLockingNecessary, this, true);
172     blockFooter().m_marks.clearAll();
173     block().clearHasAnyMarked();
174     blockFooter().m_markingVersion = heap()-&gt;objectSpace().markingVersion();
175     m_weakSet.lastChanceToFinalize();
176     blockFooter().m_newlyAllocated.clearAll();
177     blockFooter().m_newlyAllocatedVersion = heap()-&gt;objectSpace().newlyAllocatedVersion();
178     sweep(nullptr);
179 }
180 
181 void MarkedBlock::Handle::resumeAllocating(FreeList&amp; freeList)
182 {
183     {
184         auto locker = holdLock(blockFooter().m_lock);
185 
<span class="line-modified">186         if (false)</span>
187             dataLog(RawPointer(this), &quot;: MarkedBlock::Handle::resumeAllocating!\n&quot;);
188         ASSERT(!directory()-&gt;isAllocated(NoLockingNecessary, this));
189         ASSERT(!isFreeListed());
190 
191         if (!block().hasAnyNewlyAllocated()) {
<span class="line-modified">192             if (false)</span>
193                 dataLog(&quot;There ain&#39;t no newly allocated.\n&quot;);
194             // This means we had already exhausted the block when we stopped allocation.
195             freeList.clear();
196             return;
197         }
198     }
199 
200     // Re-create our free list from before stopping allocation. Note that this may return an empty
201     // freelist, in which case the block will still be Marked!
202     sweep(&amp;freeList);
203 }
204 
<span class="line-removed">205 void MarkedBlock::Handle::zap(const FreeList&amp; freeList)</span>
<span class="line-removed">206 {</span>
<span class="line-removed">207     freeList.forEach(</span>
<span class="line-removed">208         [&amp;] (HeapCell* cell) {</span>
<span class="line-removed">209             if (m_attributes.destruction == NeedsDestruction)</span>
<span class="line-removed">210                 cell-&gt;zap();</span>
<span class="line-removed">211         });</span>
<span class="line-removed">212 }</span>
<span class="line-removed">213 </span>
214 void MarkedBlock::aboutToMarkSlow(HeapVersion markingVersion)
215 {
<span class="line-modified">216     ASSERT(vm()-&gt;heap.objectSpace().isMarking());</span>
217     auto locker = holdLock(footer().m_lock);
218 
219     if (!areMarksStale(markingVersion))
220         return;
221 
222     BlockDirectory* directory = handle().directory();
223 
224     if (handle().directory()-&gt;isAllocated(holdLock(directory-&gt;bitvectorLock()), &amp;handle())
225         || !marksConveyLivenessDuringMarking(markingVersion)) {
<span class="line-modified">226         if (false)</span>
227             dataLog(RawPointer(this), &quot;: Clearing marks without doing anything else.\n&quot;);
228         // We already know that the block is full and is already recognized as such, or that the
229         // block did not survive the previous GC. So, we can clear mark bits the old fashioned
230         // way. Note that it&#39;s possible for such a block to have newlyAllocated with an up-to-
231         // date version! If it does, then we want to leave the newlyAllocated alone, since that
232         // means that we had allocated in this previously empty block but did not fill it up, so
233         // we created a newlyAllocated.
234         footer().m_marks.clearAll();
235     } else {
<span class="line-modified">236         if (false)</span>
237             dataLog(RawPointer(this), &quot;: Doing things.\n&quot;);
238         HeapVersion newlyAllocatedVersion = space()-&gt;newlyAllocatedVersion();
239         if (footer().m_newlyAllocatedVersion == newlyAllocatedVersion) {
240             // When do we get here? The block could not have been filled up. The newlyAllocated bits would
241             // have had to be created since the end of the last collection. The only things that create
242             // them are aboutToMarkSlow, lastChanceToFinalize, and stopAllocating. If it had been
243             // aboutToMarkSlow, then we shouldn&#39;t be here since the marks wouldn&#39;t be stale anymore. It
244             // cannot be lastChanceToFinalize. So it must be stopAllocating. That means that we just
245             // computed the newlyAllocated bits just before the start of an increment. When we are in that
246             // mode, it seems as if newlyAllocated should subsume marks.
247             ASSERT(footer().m_newlyAllocated.subsumes(footer().m_marks));
248             footer().m_marks.clearAll();
249         } else {
250             footer().m_newlyAllocated.setAndClear(footer().m_marks);
251             footer().m_newlyAllocatedVersion = newlyAllocatedVersion;
252         }
253     }
254     clearHasAnyMarked();
255     WTF::storeStoreFence();
256     footer().m_markingVersion = markingVersion;
</pre>
<hr />
<pre>
264     footer().m_newlyAllocated.clearAll();
265     footer().m_newlyAllocatedVersion = MarkedSpace::nullVersion;
266 }
267 
268 void MarkedBlock::resetMarks()
269 {
270     // We want aboutToMarkSlow() to see what the mark bits were after the last collection. It uses
271     // the version number to distinguish between the marks having already been stale before
272     // beginMarking(), or just stale now that beginMarking() bumped the version. If we have a version
273     // wraparound, then we will call this method before resetting the version to null. When the
274     // version is null, aboutToMarkSlow() will assume that the marks were not stale as of before
275     // beginMarking(). Hence the need to whip the marks into shape.
276     if (areMarksStale())
277         footer().m_marks.clearAll();
278     footer().m_markingVersion = MarkedSpace::nullVersion;
279 }
280 
281 #if !ASSERT_DISABLED
282 void MarkedBlock::assertMarksNotStale()
283 {
<span class="line-modified">284     ASSERT(footer().m_markingVersion == vm()-&gt;heap.objectSpace().markingVersion());</span>
285 }
286 #endif // !ASSERT_DISABLED
287 
288 bool MarkedBlock::areMarksStale()
289 {
<span class="line-modified">290     return areMarksStale(vm()-&gt;heap.objectSpace().markingVersion());</span>
291 }
292 
293 bool MarkedBlock::Handle::areMarksStale()
294 {
295     return m_block-&gt;areMarksStale();
296 }
297 
298 bool MarkedBlock::isMarked(const void* p)
299 {
<span class="line-modified">300     return isMarked(vm()-&gt;heap.objectSpace().markingVersion(), p);</span>
301 }
302 
303 void MarkedBlock::Handle::didConsumeFreeList()
304 {
305     auto locker = holdLock(blockFooter().m_lock);
<span class="line-modified">306     if (false)</span>
307         dataLog(RawPointer(this), &quot;: MarkedBlock::Handle::didConsumeFreeList!\n&quot;);
308     ASSERT(isFreeListed());
309     m_isFreeListed = false;
310     directory()-&gt;setIsAllocated(NoLockingNecessary, this, true);
311 }
312 
313 size_t MarkedBlock::markCount()
314 {
315     return areMarksStale() ? 0 : footer().m_marks.count();
316 }
317 
318 void MarkedBlock::clearHasAnyMarked()
319 {
320     footer().m_biasedMarkCount = footer().m_markCountBias;
321 }
322 
323 void MarkedBlock::noteMarkedSlow()
324 {
325     BlockDirectory* directory = handle().directory();
326     directory-&gt;setIsMarkingRetired(holdLock(directory-&gt;bitvectorLock()), &amp;handle(), true);
</pre>
<hr />
<pre>
360     RELEASE_ASSERT(markCountBias &gt; static_cast&lt;double&gt;(std::numeric_limits&lt;int16_t&gt;::min()));
361     RELEASE_ASSERT(markCountBias &lt; 0);
362 
363     // This means we haven&#39;t marked anything yet.
364     blockFooter().m_biasedMarkCount = blockFooter().m_markCountBias = static_cast&lt;int16_t&gt;(markCountBias);
365 }
366 
367 void MarkedBlock::Handle::didRemoveFromDirectory()
368 {
369     ASSERT(m_index != std::numeric_limits&lt;size_t&gt;::max());
370     ASSERT(m_directory);
371 
372     m_index = std::numeric_limits&lt;size_t&gt;::max();
373     m_directory = nullptr;
374     blockFooter().m_subspace = nullptr;
375 }
376 
377 #if !ASSERT_DISABLED
378 void MarkedBlock::assertValidCell(VM&amp; vm, HeapCell* cell) const
379 {
<span class="line-modified">380     RELEASE_ASSERT(&amp;vm == this-&gt;vm());</span>
381     RELEASE_ASSERT(const_cast&lt;MarkedBlock*&gt;(this)-&gt;handle().cellAlign(cell) == cell);
382 }
383 #endif
384 
385 void MarkedBlock::Handle::dumpState(PrintStream&amp; out)
386 {
387     CommaPrinter comma;
388     directory()-&gt;forEachBitVectorWithName(
389         holdLock(directory()-&gt;bitvectorLock()),
390         [&amp;] (FastBitVector&amp; bitvector, const char* name) {
391             out.print(comma, name, &quot;:&quot;, bitvector[index()] ? &quot;YES&quot; : &quot;no&quot;);
392         });
393 }
394 
395 Subspace* MarkedBlock::Handle::subspace() const
396 {
397     return directory()-&gt;subspace();
398 }
399 
400 void MarkedBlock::Handle::sweep(FreeList* freeList)
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (C) 2011-2019 Apple Inc. All rights reserved.</span>
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
 14  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 15  * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
 17  * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 18  * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 19  * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 20  * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 21  * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 22  * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 23  * THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;MarkedBlock.h&quot;
 28 
 29 #include &quot;AlignedMemoryAllocator.h&quot;
 30 #include &quot;BlockDirectoryInlines.h&quot;
 31 #include &quot;FreeListInlines.h&quot;
 32 #include &quot;JSCast.h&quot;
 33 #include &quot;JSDestructibleObject.h&quot;
 34 #include &quot;JSCInlines.h&quot;
 35 #include &quot;MarkedBlockInlines.h&quot;
 36 #include &quot;SuperSampler.h&quot;
 37 #include &quot;SweepingScope.h&quot;
 38 #include &lt;wtf/CommaPrinter.h&gt;
 39 
 40 namespace JSC {
<span class="line-added"> 41 namespace MarkedBlockInternal {</span>
<span class="line-added"> 42 static constexpr bool verbose = false;</span>
<span class="line-added"> 43 }</span>
 44 
 45 const size_t MarkedBlock::blockSize;
 46 
 47 static const bool computeBalance = false;
 48 static size_t balance;
 49 
 50 MarkedBlock::Handle* MarkedBlock::tryCreate(Heap&amp; heap, AlignedMemoryAllocator* alignedMemoryAllocator)
 51 {
 52     if (computeBalance) {
 53         balance++;
 54         if (!(balance % 10))
 55             dataLog(&quot;MarkedBlock Balance: &quot;, balance, &quot;\n&quot;);
 56     }
 57     void* blockSpace = alignedMemoryAllocator-&gt;tryAllocateAlignedMemory(blockSize, blockSize);
 58     if (!blockSpace)
 59         return nullptr;
 60     if (scribbleFreeCells())
 61         scribble(blockSpace, blockSize);
 62     return new Handle(heap, alignedMemoryAllocator, blockSpace);
 63 }
 64 
 65 MarkedBlock::Handle::Handle(Heap&amp; heap, AlignedMemoryAllocator* alignedMemoryAllocator, void* blockSpace)
 66     : m_alignedMemoryAllocator(alignedMemoryAllocator)
 67     , m_weakSet(heap.vm(), CellContainer())
 68 {
<span class="line-modified"> 69     m_block = new (NotNull, blockSpace) MarkedBlock(heap.vm(), *this);</span>
 70 
 71     m_weakSet.setContainer(*m_block);
 72 
 73     heap.didAllocateBlock(blockSize);
 74 }
 75 
 76 MarkedBlock::Handle::~Handle()
 77 {
 78     Heap&amp; heap = *this-&gt;heap();
 79     if (computeBalance) {
 80         balance--;
 81         if (!(balance % 10))
 82             dataLog(&quot;MarkedBlock Balance: &quot;, balance, &quot;\n&quot;);
 83     }
 84     removeFromDirectory();
 85     m_block-&gt;~MarkedBlock();
 86     m_alignedMemoryAllocator-&gt;freeAlignedMemory(m_block);
 87     heap.didFreeBlock(blockSize);
 88 }
 89 
 90 MarkedBlock::MarkedBlock(VM&amp; vm, Handle&amp; handle)
 91 {
 92     new (&amp;footer()) Footer(vm, handle);
<span class="line-modified"> 93     if (MarkedBlockInternal::verbose)</span>
 94         dataLog(RawPointer(this), &quot;: Allocated.\n&quot;);
 95 }
 96 
 97 MarkedBlock::~MarkedBlock()
 98 {
 99     footer().~Footer();
100 }
101 
102 MarkedBlock::Footer::Footer(VM&amp; vm, Handle&amp; handle)
103     : m_handle(handle)
104     , m_vm(&amp;vm)
105     , m_markingVersion(MarkedSpace::nullVersion)
106     , m_newlyAllocatedVersion(MarkedSpace::nullVersion)
107 {
108 }
109 
110 MarkedBlock::Footer::~Footer()
111 {
112 }
113 
114 void MarkedBlock::Handle::unsweepWithNoNewlyAllocated()
115 {
116     RELEASE_ASSERT(m_isFreeListed);
117     m_isFreeListed = false;
118 }
119 
120 void MarkedBlock::Handle::setIsFreeListed()
121 {
122     m_directory-&gt;setIsEmpty(NoLockingNecessary, this, false);
123     m_isFreeListed = true;
124 }
125 
126 void MarkedBlock::Handle::stopAllocating(const FreeList&amp; freeList)
127 {
128     auto locker = holdLock(blockFooter().m_lock);
129 
<span class="line-modified">130     if (MarkedBlockInternal::verbose)</span>
131         dataLog(RawPointer(this), &quot;: MarkedBlock::Handle::stopAllocating!\n&quot;);
132     ASSERT(!directory()-&gt;isAllocated(NoLockingNecessary, this));
133 
134     if (!isFreeListed()) {
<span class="line-modified">135         if (MarkedBlockInternal::verbose)</span>
136             dataLog(&quot;There ain&#39;t no newly allocated.\n&quot;);
137         // This means that we either didn&#39;t use this block at all for allocation since last GC,
138         // or someone had already done stopAllocating() before.
139         ASSERT(freeList.allocationWillFail());
140         return;
141     }
142 
<span class="line-modified">143     if (MarkedBlockInternal::verbose)</span>
144         dataLog(&quot;Free list: &quot;, freeList, &quot;\n&quot;);
145 
146     // Roll back to a coherent state for Heap introspection. Cells newly
147     // allocated from our free list are not currently marked, so we need another
148     // way to tell what&#39;s live vs dead.
149 
150     blockFooter().m_newlyAllocated.clearAll();
151     blockFooter().m_newlyAllocatedVersion = heap()-&gt;objectSpace().newlyAllocatedVersion();
152 
153     forEachCell(
154         [&amp;] (HeapCell* cell, HeapCell::Kind) -&gt; IterationStatus {
155             block().setNewlyAllocated(cell);
156             return IterationStatus::Continue;
157         });
158 
159     freeList.forEach(
160         [&amp;] (HeapCell* cell) {
<span class="line-modified">161             if (MarkedBlockInternal::verbose)</span>
162                 dataLog(&quot;Free cell: &quot;, RawPointer(cell), &quot;\n&quot;);
163             if (m_attributes.destruction == NeedsDestruction)
<span class="line-modified">164                 cell-&gt;zap(HeapCell::StopAllocating);</span>
165             block().clearNewlyAllocated(cell);
166         });
167 
168     m_isFreeListed = false;
169 }
170 
171 void MarkedBlock::Handle::lastChanceToFinalize()
172 {
173     directory()-&gt;setIsAllocated(NoLockingNecessary, this, false);
174     directory()-&gt;setIsDestructible(NoLockingNecessary, this, true);
175     blockFooter().m_marks.clearAll();
176     block().clearHasAnyMarked();
177     blockFooter().m_markingVersion = heap()-&gt;objectSpace().markingVersion();
178     m_weakSet.lastChanceToFinalize();
179     blockFooter().m_newlyAllocated.clearAll();
180     blockFooter().m_newlyAllocatedVersion = heap()-&gt;objectSpace().newlyAllocatedVersion();
181     sweep(nullptr);
182 }
183 
184 void MarkedBlock::Handle::resumeAllocating(FreeList&amp; freeList)
185 {
186     {
187         auto locker = holdLock(blockFooter().m_lock);
188 
<span class="line-modified">189         if (MarkedBlockInternal::verbose)</span>
190             dataLog(RawPointer(this), &quot;: MarkedBlock::Handle::resumeAllocating!\n&quot;);
191         ASSERT(!directory()-&gt;isAllocated(NoLockingNecessary, this));
192         ASSERT(!isFreeListed());
193 
194         if (!block().hasAnyNewlyAllocated()) {
<span class="line-modified">195             if (MarkedBlockInternal::verbose)</span>
196                 dataLog(&quot;There ain&#39;t no newly allocated.\n&quot;);
197             // This means we had already exhausted the block when we stopped allocation.
198             freeList.clear();
199             return;
200         }
201     }
202 
203     // Re-create our free list from before stopping allocation. Note that this may return an empty
204     // freelist, in which case the block will still be Marked!
205     sweep(&amp;freeList);
206 }
207 









208 void MarkedBlock::aboutToMarkSlow(HeapVersion markingVersion)
209 {
<span class="line-modified">210     ASSERT(vm().heap.objectSpace().isMarking());</span>
211     auto locker = holdLock(footer().m_lock);
212 
213     if (!areMarksStale(markingVersion))
214         return;
215 
216     BlockDirectory* directory = handle().directory();
217 
218     if (handle().directory()-&gt;isAllocated(holdLock(directory-&gt;bitvectorLock()), &amp;handle())
219         || !marksConveyLivenessDuringMarking(markingVersion)) {
<span class="line-modified">220         if (MarkedBlockInternal::verbose)</span>
221             dataLog(RawPointer(this), &quot;: Clearing marks without doing anything else.\n&quot;);
222         // We already know that the block is full and is already recognized as such, or that the
223         // block did not survive the previous GC. So, we can clear mark bits the old fashioned
224         // way. Note that it&#39;s possible for such a block to have newlyAllocated with an up-to-
225         // date version! If it does, then we want to leave the newlyAllocated alone, since that
226         // means that we had allocated in this previously empty block but did not fill it up, so
227         // we created a newlyAllocated.
228         footer().m_marks.clearAll();
229     } else {
<span class="line-modified">230         if (MarkedBlockInternal::verbose)</span>
231             dataLog(RawPointer(this), &quot;: Doing things.\n&quot;);
232         HeapVersion newlyAllocatedVersion = space()-&gt;newlyAllocatedVersion();
233         if (footer().m_newlyAllocatedVersion == newlyAllocatedVersion) {
234             // When do we get here? The block could not have been filled up. The newlyAllocated bits would
235             // have had to be created since the end of the last collection. The only things that create
236             // them are aboutToMarkSlow, lastChanceToFinalize, and stopAllocating. If it had been
237             // aboutToMarkSlow, then we shouldn&#39;t be here since the marks wouldn&#39;t be stale anymore. It
238             // cannot be lastChanceToFinalize. So it must be stopAllocating. That means that we just
239             // computed the newlyAllocated bits just before the start of an increment. When we are in that
240             // mode, it seems as if newlyAllocated should subsume marks.
241             ASSERT(footer().m_newlyAllocated.subsumes(footer().m_marks));
242             footer().m_marks.clearAll();
243         } else {
244             footer().m_newlyAllocated.setAndClear(footer().m_marks);
245             footer().m_newlyAllocatedVersion = newlyAllocatedVersion;
246         }
247     }
248     clearHasAnyMarked();
249     WTF::storeStoreFence();
250     footer().m_markingVersion = markingVersion;
</pre>
<hr />
<pre>
258     footer().m_newlyAllocated.clearAll();
259     footer().m_newlyAllocatedVersion = MarkedSpace::nullVersion;
260 }
261 
262 void MarkedBlock::resetMarks()
263 {
264     // We want aboutToMarkSlow() to see what the mark bits were after the last collection. It uses
265     // the version number to distinguish between the marks having already been stale before
266     // beginMarking(), or just stale now that beginMarking() bumped the version. If we have a version
267     // wraparound, then we will call this method before resetting the version to null. When the
268     // version is null, aboutToMarkSlow() will assume that the marks were not stale as of before
269     // beginMarking(). Hence the need to whip the marks into shape.
270     if (areMarksStale())
271         footer().m_marks.clearAll();
272     footer().m_markingVersion = MarkedSpace::nullVersion;
273 }
274 
275 #if !ASSERT_DISABLED
276 void MarkedBlock::assertMarksNotStale()
277 {
<span class="line-modified">278     ASSERT(footer().m_markingVersion == vm().heap.objectSpace().markingVersion());</span>
279 }
280 #endif // !ASSERT_DISABLED
281 
282 bool MarkedBlock::areMarksStale()
283 {
<span class="line-modified">284     return areMarksStale(vm().heap.objectSpace().markingVersion());</span>
285 }
286 
287 bool MarkedBlock::Handle::areMarksStale()
288 {
289     return m_block-&gt;areMarksStale();
290 }
291 
292 bool MarkedBlock::isMarked(const void* p)
293 {
<span class="line-modified">294     return isMarked(vm().heap.objectSpace().markingVersion(), p);</span>
295 }
296 
297 void MarkedBlock::Handle::didConsumeFreeList()
298 {
299     auto locker = holdLock(blockFooter().m_lock);
<span class="line-modified">300     if (MarkedBlockInternal::verbose)</span>
301         dataLog(RawPointer(this), &quot;: MarkedBlock::Handle::didConsumeFreeList!\n&quot;);
302     ASSERT(isFreeListed());
303     m_isFreeListed = false;
304     directory()-&gt;setIsAllocated(NoLockingNecessary, this, true);
305 }
306 
307 size_t MarkedBlock::markCount()
308 {
309     return areMarksStale() ? 0 : footer().m_marks.count();
310 }
311 
312 void MarkedBlock::clearHasAnyMarked()
313 {
314     footer().m_biasedMarkCount = footer().m_markCountBias;
315 }
316 
317 void MarkedBlock::noteMarkedSlow()
318 {
319     BlockDirectory* directory = handle().directory();
320     directory-&gt;setIsMarkingRetired(holdLock(directory-&gt;bitvectorLock()), &amp;handle(), true);
</pre>
<hr />
<pre>
354     RELEASE_ASSERT(markCountBias &gt; static_cast&lt;double&gt;(std::numeric_limits&lt;int16_t&gt;::min()));
355     RELEASE_ASSERT(markCountBias &lt; 0);
356 
357     // This means we haven&#39;t marked anything yet.
358     blockFooter().m_biasedMarkCount = blockFooter().m_markCountBias = static_cast&lt;int16_t&gt;(markCountBias);
359 }
360 
361 void MarkedBlock::Handle::didRemoveFromDirectory()
362 {
363     ASSERT(m_index != std::numeric_limits&lt;size_t&gt;::max());
364     ASSERT(m_directory);
365 
366     m_index = std::numeric_limits&lt;size_t&gt;::max();
367     m_directory = nullptr;
368     blockFooter().m_subspace = nullptr;
369 }
370 
371 #if !ASSERT_DISABLED
372 void MarkedBlock::assertValidCell(VM&amp; vm, HeapCell* cell) const
373 {
<span class="line-modified">374     RELEASE_ASSERT(&amp;vm == &amp;this-&gt;vm());</span>
375     RELEASE_ASSERT(const_cast&lt;MarkedBlock*&gt;(this)-&gt;handle().cellAlign(cell) == cell);
376 }
377 #endif
378 
379 void MarkedBlock::Handle::dumpState(PrintStream&amp; out)
380 {
381     CommaPrinter comma;
382     directory()-&gt;forEachBitVectorWithName(
383         holdLock(directory()-&gt;bitvectorLock()),
384         [&amp;] (FastBitVector&amp; bitvector, const char* name) {
385             out.print(comma, name, &quot;:&quot;, bitvector[index()] ? &quot;YES&quot; : &quot;no&quot;);
386         });
387 }
388 
389 Subspace* MarkedBlock::Handle::subspace() const
390 {
391     return directory()-&gt;subspace();
392 }
393 
394 void MarkedBlock::Handle::sweep(FreeList* freeList)
</pre>
</td>
</tr>
</table>
<center><a href="MachineStackMarker.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="MarkedBlock.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>