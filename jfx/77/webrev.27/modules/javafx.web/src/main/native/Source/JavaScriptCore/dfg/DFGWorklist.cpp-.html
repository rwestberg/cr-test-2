<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGWorklist.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2013-2018 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;DFGWorklist.h&quot;
 28 
 29 #include &quot;CodeBlock.h&quot;
 30 #include &quot;DFGSafepoint.h&quot;
 31 #include &quot;DeferGC.h&quot;
 32 #include &quot;JSCInlines.h&quot;
 33 #include &quot;ReleaseHeapAccessScope.h&quot;
 34 #include &lt;mutex&gt;
 35 
 36 namespace JSC { namespace DFG {
 37 
 38 #if ENABLE(DFG_JIT)
 39 
 40 class Worklist::ThreadBody : public AutomaticThread {
 41 public:
 42     ThreadBody(const AbstractLocker&amp; locker, Worklist&amp; worklist, ThreadData&amp; data, Box&lt;Lock&gt; lock, Ref&lt;AutomaticThreadCondition&gt;&amp;&amp; condition, int relativePriority)
 43         : AutomaticThread(locker, lock, WTFMove(condition))
 44         , m_worklist(worklist)
 45         , m_data(data)
 46         , m_relativePriority(relativePriority)
 47     {
 48     }
 49 
 50     const char* name() const override
 51     {
 52         return m_worklist.m_threadName.data();
 53     }
 54 
 55 protected:
 56     PollResult poll(const AbstractLocker&amp; locker) override
 57     {
 58         if (m_worklist.m_queue.isEmpty())
 59             return PollResult::Wait;
 60 
 61         m_plan = m_worklist.m_queue.takeFirst();
 62         if (!m_plan) {
 63             if (Options::verboseCompilationQueue()) {
 64                 m_worklist.dump(locker, WTF::dataFile());
 65                 dataLog(&quot;: Thread shutting down\n&quot;);
 66             }
 67             return PollResult::Stop;
 68         }
 69         RELEASE_ASSERT(m_plan-&gt;stage() == Plan::Preparing);
 70         m_worklist.m_numberOfActiveThreads++;
 71         return PollResult::Work;
 72     }
 73 
 74     class WorkScope;
 75     friend class WorkScope;
 76     class WorkScope {
 77     public:
 78         WorkScope(ThreadBody&amp; thread)
 79             : m_thread(thread)
 80         {
 81             RELEASE_ASSERT(m_thread.m_plan);
 82             RELEASE_ASSERT(m_thread.m_worklist.m_numberOfActiveThreads);
 83         }
 84 
 85         ~WorkScope()
 86         {
 87             LockHolder locker(*m_thread.m_worklist.m_lock);
 88             m_thread.m_plan = nullptr;
 89             m_thread.m_worklist.m_numberOfActiveThreads--;
 90         }
 91 
 92     private:
 93         ThreadBody&amp; m_thread;
 94     };
 95 
 96     WorkResult work() override
 97     {
 98         WorkScope workScope(*this);
 99 
100         LockHolder locker(m_data.m_rightToRun);
101         {
102             LockHolder locker(*m_worklist.m_lock);
103             if (m_plan-&gt;stage() == Plan::Cancelled)
104                 return WorkResult::Continue;
105             m_plan-&gt;notifyCompiling();
106         }
107 
108         if (Options::verboseCompilationQueue())
109             dataLog(m_worklist, &quot;: Compiling &quot;, m_plan-&gt;key(), &quot; asynchronously\n&quot;);
110 
111         // There&#39;s no way for the GC to be safepointing since we own rightToRun.
112         if (m_plan-&gt;vm()-&gt;heap.worldIsStopped()) {
113             dataLog(&quot;Heap is stoped but here we are! (1)\n&quot;);
114             RELEASE_ASSERT_NOT_REACHED();
115         }
116         m_plan-&gt;compileInThread(&amp;m_data);
117         if (m_plan-&gt;stage() != Plan::Cancelled) {
118             if (m_plan-&gt;vm()-&gt;heap.worldIsStopped()) {
119                 dataLog(&quot;Heap is stopped but here we are! (2)\n&quot;);
120                 RELEASE_ASSERT_NOT_REACHED();
121             }
122         }
123 
124         {
125             LockHolder locker(*m_worklist.m_lock);
126             if (m_plan-&gt;stage() == Plan::Cancelled)
127                 return WorkResult::Continue;
128 
129             m_plan-&gt;notifyReady();
130 
131             if (Options::verboseCompilationQueue()) {
132                 m_worklist.dump(locker, WTF::dataFile());
133                 dataLog(&quot;: Compiled &quot;, m_plan-&gt;key(), &quot; asynchronously\n&quot;);
134             }
135 
136             m_worklist.m_readyPlans.append(m_plan);
137 
138             RELEASE_ASSERT(!m_plan-&gt;vm()-&gt;heap.worldIsStopped());
139             m_worklist.m_planCompiled.notifyAll();
140         }
141 
142         return WorkResult::Continue;
143     }
144 
145     void threadDidStart() override
146     {
147         if (Options::verboseCompilationQueue())
148             dataLog(m_worklist, &quot;: Thread started\n&quot;);
149 
150         if (m_relativePriority)
151             Thread::current().changePriority(m_relativePriority);
152 
153         m_compilationScope = std::make_unique&lt;CompilationScope&gt;();
154     }
155 
156     void threadIsStopping(const AbstractLocker&amp;) override
157     {
158         // We&#39;re holding the Worklist::m_lock, so we should be careful not to deadlock.
159 
160         if (Options::verboseCompilationQueue())
161             dataLog(m_worklist, &quot;: Thread will stop\n&quot;);
162 
163         ASSERT(!m_plan);
164 
165         m_compilationScope = nullptr;
166         m_plan = nullptr;
167     }
168 
169 private:
170     Worklist&amp; m_worklist;
171     ThreadData&amp; m_data;
172     int m_relativePriority;
173     std::unique_ptr&lt;CompilationScope&gt; m_compilationScope;
174     RefPtr&lt;Plan&gt; m_plan;
175 };
176 
177 static CString createWorklistName(CString&amp;&amp; tierName)
178 {
179 #if OS(LINUX)
180     return toCString(WTFMove(tierName), &quot;Worker&quot;);
181 #else
182     return toCString(WTFMove(tierName), &quot; Worklist Worker Thread&quot;);
183 #endif
184 }
185 
186 Worklist::Worklist(CString&amp;&amp; tierName)
187     : m_threadName(createWorklistName(WTFMove(tierName)))
188     , m_lock(Box&lt;Lock&gt;::create())
189     , m_planEnqueued(AutomaticThreadCondition::create())
190     , m_numberOfActiveThreads(0)
191 {
192 }
193 
194 Worklist::~Worklist()
195 {
196     {
197         LockHolder locker(*m_lock);
198         for (unsigned i = m_threads.size(); i--;)
199             m_queue.append(nullptr); // Use null plan to indicate that we want the thread to terminate.
200         m_planEnqueued-&gt;notifyAll(locker);
201     }
202     for (unsigned i = m_threads.size(); i--;)
203         m_threads[i]-&gt;m_thread-&gt;join();
204     ASSERT(!m_numberOfActiveThreads);
205 }
206 
207 void Worklist::finishCreation(unsigned numberOfThreads, int relativePriority)
208 {
209     RELEASE_ASSERT(numberOfThreads);
210     LockHolder locker(*m_lock);
211     for (unsigned i = numberOfThreads; i--;) {
212         createNewThread(locker, relativePriority);
213     }
214 }
215 
216 void Worklist::createNewThread(const AbstractLocker&amp; locker, int relativePriority)
217 {
218     std::unique_ptr&lt;ThreadData&gt; data = std::make_unique&lt;ThreadData&gt;(this);
219     data-&gt;m_thread = adoptRef(new ThreadBody(locker, *this, *data, m_lock, m_planEnqueued.copyRef(), relativePriority));
220     m_threads.append(WTFMove(data));
221 }
222 
223 Ref&lt;Worklist&gt; Worklist::create(CString&amp;&amp; tierName, unsigned numberOfThreads, int relativePriority)
224 {
225     Ref&lt;Worklist&gt; result = adoptRef(*new Worklist(WTFMove(tierName)));
226     result-&gt;finishCreation(numberOfThreads, relativePriority);
227     return result;
228 }
229 
230 bool Worklist::isActiveForVM(VM&amp; vm) const
231 {
232     LockHolder locker(*m_lock);
233     PlanMap::const_iterator end = m_plans.end();
234     for (PlanMap::const_iterator iter = m_plans.begin(); iter != end; ++iter) {
235         if (iter-&gt;value-&gt;vm() == &amp;vm)
236             return true;
237     }
238     return false;
239 }
240 
241 void Worklist::enqueue(Ref&lt;Plan&gt;&amp;&amp; plan)
242 {
243     LockHolder locker(*m_lock);
244     if (Options::verboseCompilationQueue()) {
245         dump(locker, WTF::dataFile());
246         dataLog(&quot;: Enqueueing plan to optimize &quot;, plan-&gt;key(), &quot;\n&quot;);
247     }
248     ASSERT(m_plans.find(plan-&gt;key()) == m_plans.end());
249     m_plans.add(plan-&gt;key(), plan.copyRef());
250     m_queue.append(WTFMove(plan));
251     m_planEnqueued-&gt;notifyOne(locker);
252 }
253 
254 Worklist::State Worklist::compilationState(CompilationKey key)
255 {
256     LockHolder locker(*m_lock);
257     PlanMap::iterator iter = m_plans.find(key);
258     if (iter == m_plans.end())
259         return NotKnown;
260     return iter-&gt;value-&gt;stage() == Plan::Ready ? Compiled : Compiling;
261 }
262 
263 void Worklist::waitUntilAllPlansForVMAreReady(VM&amp; vm)
264 {
265     DeferGC deferGC(vm.heap);
266 
267     // While we are waiting for the compiler to finish, the collector might have already suspended
268     // the compiler and then it will be waiting for us to stop. That&#39;s a deadlock. We avoid that
269     // deadlock by relinquishing our heap access, so that the collector pretends that we are stopped
270     // even if we aren&#39;t.
271     ReleaseHeapAccessScope releaseHeapAccessScope(vm.heap);
272 
273     // Wait for all of the plans for the given VM to complete. The idea here
274     // is that we want all of the caller VM&#39;s plans to be done. We don&#39;t care
275     // about any other VM&#39;s plans, and we won&#39;t attempt to wait on those.
276     // After we release this lock, we know that although other VMs may still
277     // be adding plans, our VM will not be.
278 
279     LockHolder locker(*m_lock);
280 
281     if (Options::verboseCompilationQueue()) {
282         dump(locker, WTF::dataFile());
283         dataLog(&quot;: Waiting for all in VM to complete.\n&quot;);
284     }
285 
286     for (;;) {
287         bool allAreCompiled = true;
288         PlanMap::iterator end = m_plans.end();
289         for (PlanMap::iterator iter = m_plans.begin(); iter != end; ++iter) {
290             if (iter-&gt;value-&gt;vm() != &amp;vm)
291                 continue;
292             if (iter-&gt;value-&gt;stage() != Plan::Ready) {
293                 allAreCompiled = false;
294                 break;
295             }
296         }
297 
298         if (allAreCompiled)
299             break;
300 
301         m_planCompiled.wait(*m_lock);
302     }
303 }
304 
305 void Worklist::removeAllReadyPlansForVM(VM&amp; vm, Vector&lt;RefPtr&lt;Plan&gt;, 8&gt;&amp; myReadyPlans)
306 {
307     DeferGC deferGC(vm.heap);
308     LockHolder locker(*m_lock);
309     for (size_t i = 0; i &lt; m_readyPlans.size(); ++i) {
310         RefPtr&lt;Plan&gt; plan = m_readyPlans[i];
311         if (plan-&gt;vm() != &amp;vm)
312             continue;
313         if (plan-&gt;stage() != Plan::Ready)
314             continue;
315         myReadyPlans.append(plan);
316         m_readyPlans[i--] = m_readyPlans.last();
317         m_readyPlans.removeLast();
318         m_plans.remove(plan-&gt;key());
319     }
320 }
321 
322 void Worklist::removeAllReadyPlansForVM(VM&amp; vm)
323 {
324     Vector&lt;RefPtr&lt;Plan&gt;, 8&gt; myReadyPlans;
325     removeAllReadyPlansForVM(vm, myReadyPlans);
326 }
327 
328 Worklist::State Worklist::completeAllReadyPlansForVM(VM&amp; vm, CompilationKey requestedKey)
329 {
330     DeferGC deferGC(vm.heap);
331     Vector&lt;RefPtr&lt;Plan&gt;, 8&gt; myReadyPlans;
332 
333     removeAllReadyPlansForVM(vm, myReadyPlans);
334 
335     State resultingState = NotKnown;
336 
337     while (!myReadyPlans.isEmpty()) {
338         RefPtr&lt;Plan&gt; plan = myReadyPlans.takeLast();
339         CompilationKey currentKey = plan-&gt;key();
340 
341         if (Options::verboseCompilationQueue())
342             dataLog(*this, &quot;: Completing &quot;, currentKey, &quot;\n&quot;);
343 
344         RELEASE_ASSERT(plan-&gt;stage() == Plan::Ready);
345 
346         plan-&gt;finalizeAndNotifyCallback();
347 
348         if (currentKey == requestedKey)
349             resultingState = Compiled;
350     }
351 
352     if (!!requestedKey &amp;&amp; resultingState == NotKnown) {
353         LockHolder locker(*m_lock);
354         if (m_plans.contains(requestedKey))
355             resultingState = Compiling;
356     }
357 
358     return resultingState;
359 }
360 
361 void Worklist::completeAllPlansForVM(VM&amp; vm)
362 {
363     DeferGC deferGC(vm.heap);
364     waitUntilAllPlansForVMAreReady(vm);
365     completeAllReadyPlansForVM(vm);
366 }
367 
368 void Worklist::suspendAllThreads()
369 {
370     m_suspensionLock.lock();
371     for (unsigned i = m_threads.size(); i--;)
372         m_threads[i]-&gt;m_rightToRun.lock();
373 }
374 
375 void Worklist::resumeAllThreads()
376 {
377     for (unsigned i = m_threads.size(); i--;)
378         m_threads[i]-&gt;m_rightToRun.unlock();
379     m_suspensionLock.unlock();
380 }
381 
382 void Worklist::visitWeakReferences(SlotVisitor&amp; visitor)
383 {
384     VM* vm = visitor.heap()-&gt;vm();
385     {
386         LockHolder locker(*m_lock);
387         for (PlanMap::iterator iter = m_plans.begin(); iter != m_plans.end(); ++iter) {
388             Plan* plan = iter-&gt;value.get();
389             if (plan-&gt;vm() != vm)
390                 continue;
391             plan-&gt;checkLivenessAndVisitChildren(visitor);
392         }
393     }
394     // This loop doesn&#39;t need locking because:
395     // (1) no new threads can be added to m_threads. Hence, it is immutable and needs no locks.
396     // (2) ThreadData::m_safepoint is protected by that thread&#39;s m_rightToRun which we must be
397     //     holding here because of a prior call to suspendAllThreads().
398     for (unsigned i = m_threads.size(); i--;) {
399         ThreadData* data = m_threads[i].get();
400         Safepoint* safepoint = data-&gt;m_safepoint;
401         if (safepoint &amp;&amp; safepoint-&gt;vm() == vm)
402             safepoint-&gt;checkLivenessAndVisitChildren(visitor);
403     }
404 }
405 
406 void Worklist::removeDeadPlans(VM&amp; vm)
407 {
408     {
409         LockHolder locker(*m_lock);
410         HashSet&lt;CompilationKey&gt; deadPlanKeys;
411         for (PlanMap::iterator iter = m_plans.begin(); iter != m_plans.end(); ++iter) {
412             Plan* plan = iter-&gt;value.get();
413             if (plan-&gt;vm() != &amp;vm)
414                 continue;
415             if (plan-&gt;isKnownToBeLiveDuringGC()) {
416                 plan-&gt;finalizeInGC();
417                 continue;
418             }
419             RELEASE_ASSERT(plan-&gt;stage() != Plan::Cancelled); // Should not be cancelled, yet.
420             ASSERT(!deadPlanKeys.contains(plan-&gt;key()));
421             deadPlanKeys.add(plan-&gt;key());
422         }
423         if (!deadPlanKeys.isEmpty()) {
424             for (HashSet&lt;CompilationKey&gt;::iterator iter = deadPlanKeys.begin(); iter != deadPlanKeys.end(); ++iter)
425                 m_plans.take(*iter)-&gt;cancel();
426             Deque&lt;RefPtr&lt;Plan&gt;&gt; newQueue;
427             while (!m_queue.isEmpty()) {
428                 RefPtr&lt;Plan&gt; plan = m_queue.takeFirst();
429                 if (plan-&gt;stage() != Plan::Cancelled)
430                     newQueue.append(plan);
431             }
432             m_queue.swap(newQueue);
433             for (unsigned i = 0; i &lt; m_readyPlans.size(); ++i) {
434                 if (m_readyPlans[i]-&gt;stage() != Plan::Cancelled)
435                     continue;
436                 m_readyPlans[i--] = m_readyPlans.last();
437                 m_readyPlans.removeLast();
438             }
439         }
440     }
441 
442     // No locking needed for this part, see comment in visitWeakReferences().
443     for (unsigned i = m_threads.size(); i--;) {
444         ThreadData* data = m_threads[i].get();
445         Safepoint* safepoint = data-&gt;m_safepoint;
446         if (!safepoint)
447             continue;
448         if (safepoint-&gt;vm() != &amp;vm)
449             continue;
450         if (safepoint-&gt;isKnownToBeLiveDuringGC())
451             continue;
452         safepoint-&gt;cancel();
453     }
454 }
455 
456 void Worklist::removeNonCompilingPlansForVM(VM&amp; vm)
457 {
458     LockHolder locker(*m_lock);
459     HashSet&lt;CompilationKey&gt; deadPlanKeys;
460     Vector&lt;RefPtr&lt;Plan&gt;&gt; deadPlans;
461     for (auto&amp; entry : m_plans) {
462         Plan* plan = entry.value.get();
463         if (plan-&gt;vm() != &amp;vm)
464             continue;
465         if (plan-&gt;stage() == Plan::Compiling)
466             continue;
467         deadPlanKeys.add(plan-&gt;key());
468         deadPlans.append(plan);
469     }
470     for (CompilationKey key : deadPlanKeys)
471         m_plans.remove(key);
472     Deque&lt;RefPtr&lt;Plan&gt;&gt; newQueue;
473     while (!m_queue.isEmpty()) {
474         RefPtr&lt;Plan&gt; plan = m_queue.takeFirst();
475         if (!deadPlanKeys.contains(plan-&gt;key()))
476             newQueue.append(WTFMove(plan));
477     }
478     m_queue = WTFMove(newQueue);
479     m_readyPlans.removeAllMatching(
480         [&amp;] (RefPtr&lt;Plan&gt;&amp; plan) -&gt; bool {
481             return deadPlanKeys.contains(plan-&gt;key());
482         });
483     for (auto&amp; plan : deadPlans)
484         plan-&gt;cancel();
485 }
486 
487 size_t Worklist::queueLength()
488 {
489     LockHolder locker(*m_lock);
490     return m_queue.size();
491 }
492 
493 void Worklist::dump(PrintStream&amp; out) const
494 {
495     LockHolder locker(*m_lock);
496     dump(locker, out);
497 }
498 
499 void Worklist::dump(const AbstractLocker&amp;, PrintStream&amp; out) const
500 {
501     out.print(
502         &quot;Worklist(&quot;, RawPointer(this), &quot;)[Queue Length = &quot;, m_queue.size(),
503         &quot;, Map Size = &quot;, m_plans.size(), &quot;, Num Ready = &quot;, m_readyPlans.size(),
504         &quot;, Num Active Threads = &quot;, m_numberOfActiveThreads, &quot;/&quot;, m_threads.size(), &quot;]&quot;);
505 }
506 
507 unsigned Worklist::setNumberOfThreads(unsigned numberOfThreads, int relativePriority)
508 {
509     LockHolder locker(m_suspensionLock);
510     auto currentNumberOfThreads = m_threads.size();
511     if (numberOfThreads &lt; currentNumberOfThreads) {
512         {
513             LockHolder locker(*m_lock);
514             for (unsigned i = currentNumberOfThreads; i-- &gt; numberOfThreads;) {
515                 if (m_threads[i]-&gt;m_thread-&gt;hasUnderlyingThread(locker)) {
516                     m_queue.append(nullptr);
517                     m_threads[i]-&gt;m_thread-&gt;notify(locker);
518                 }
519             }
520         }
521         for (unsigned i = currentNumberOfThreads; i-- &gt; numberOfThreads;) {
522             bool isStopped = false;
523             {
524                 LockHolder locker(*m_lock);
525                 isStopped = m_threads[i]-&gt;m_thread-&gt;tryStop(locker);
526             }
527             if (!isStopped)
528                 m_threads[i]-&gt;m_thread-&gt;join();
529             m_threads.remove(i);
530         }
531         m_threads.shrinkToFit();
532         ASSERT(m_numberOfActiveThreads &lt;= numberOfThreads);
533     } else if (numberOfThreads &gt; currentNumberOfThreads) {
534         LockHolder locker(*m_lock);
535         for (unsigned i = currentNumberOfThreads; i &lt; numberOfThreads; i++)
536             createNewThread(locker, relativePriority);
537     }
538     return currentNumberOfThreads;
539 }
540 
541 static Worklist* theGlobalDFGWorklist;
542 static unsigned numberOfDFGCompilerThreads;
543 static unsigned numberOfFTLCompilerThreads;
544 
545 static unsigned getNumberOfDFGCompilerThreads()
546 {
547     return numberOfDFGCompilerThreads ? numberOfDFGCompilerThreads : Options::numberOfDFGCompilerThreads();
548 }
549 
550 static unsigned getNumberOfFTLCompilerThreads()
551 {
552     return numberOfFTLCompilerThreads ? numberOfFTLCompilerThreads : Options::numberOfFTLCompilerThreads();
553 }
554 
555 unsigned setNumberOfDFGCompilerThreads(unsigned numberOfThreads)
556 {
557     auto previousNumberOfThreads = getNumberOfDFGCompilerThreads();
558     numberOfDFGCompilerThreads = numberOfThreads;
559     return previousNumberOfThreads;
560 }
561 
562 unsigned setNumberOfFTLCompilerThreads(unsigned numberOfThreads)
563 {
564     auto previousNumberOfThreads = getNumberOfFTLCompilerThreads();
565     numberOfFTLCompilerThreads = numberOfThreads;
566     return previousNumberOfThreads;
567 }
568 
569 Worklist&amp; ensureGlobalDFGWorklist()
570 {
571     static std::once_flag initializeGlobalWorklistOnceFlag;
572     std::call_once(initializeGlobalWorklistOnceFlag, [] {
573         Worklist* worklist = &amp;Worklist::create(&quot;DFG&quot;, getNumberOfDFGCompilerThreads(), Options::priorityDeltaOfDFGCompilerThreads()).leakRef();
574         WTF::storeStoreFence();
575         theGlobalDFGWorklist = worklist;
576     });
577     return *theGlobalDFGWorklist;
578 }
579 
580 Worklist* existingGlobalDFGWorklistOrNull()
581 {
582     return theGlobalDFGWorklist;
583 }
584 
585 static Worklist* theGlobalFTLWorklist;
586 
587 Worklist&amp; ensureGlobalFTLWorklist()
588 {
589     static std::once_flag initializeGlobalWorklistOnceFlag;
590     std::call_once(initializeGlobalWorklistOnceFlag, [] {
591         Worklist* worklist = &amp;Worklist::create(&quot;FTL&quot;, getNumberOfFTLCompilerThreads(), Options::priorityDeltaOfFTLCompilerThreads()).leakRef();
592         WTF::storeStoreFence();
593         theGlobalFTLWorklist = worklist;
594     });
595     return *theGlobalFTLWorklist;
596 }
597 
598 Worklist* existingGlobalFTLWorklistOrNull()
599 {
600     return theGlobalFTLWorklist;
601 }
602 
603 Worklist&amp; ensureGlobalWorklistFor(CompilationMode mode)
604 {
605     switch (mode) {
606     case InvalidCompilationMode:
607         RELEASE_ASSERT_NOT_REACHED();
608         return ensureGlobalDFGWorklist();
609     case DFGMode:
610         return ensureGlobalDFGWorklist();
611     case FTLMode:
612     case FTLForOSREntryMode:
613         return ensureGlobalFTLWorklist();
614     }
615     RELEASE_ASSERT_NOT_REACHED();
616     return ensureGlobalDFGWorklist();
617 }
618 
619 unsigned numberOfWorklists() { return 2; }
620 
621 Worklist&amp; ensureWorklistForIndex(unsigned index)
622 {
623     switch (index) {
624     case 0:
625         return ensureGlobalDFGWorklist();
626     case 1:
627         return ensureGlobalFTLWorklist();
628     default:
629         RELEASE_ASSERT_NOT_REACHED();
630         return ensureGlobalDFGWorklist();
631     }
632 }
633 
634 Worklist* existingWorklistForIndexOrNull(unsigned index)
635 {
636     switch (index) {
637     case 0:
638         return existingGlobalDFGWorklistOrNull();
639     case 1:
640         return existingGlobalFTLWorklistOrNull();
641     default:
642         RELEASE_ASSERT_NOT_REACHED();
643         return 0;
644     }
645 }
646 
647 Worklist&amp; existingWorklistForIndex(unsigned index)
648 {
649     Worklist* result = existingWorklistForIndexOrNull(index);
650     RELEASE_ASSERT(result);
651     return *result;
652 }
653 
654 void completeAllPlansForVM(VM&amp; vm)
655 {
656     for (unsigned i = DFG::numberOfWorklists(); i--;) {
657         if (DFG::Worklist* worklist = DFG::existingWorklistForIndexOrNull(i))
658             worklist-&gt;completeAllPlansForVM(vm);
659     }
660 }
661 
662 #else // ENABLE(DFG_JIT)
663 
664 void completeAllPlansForVM(VM&amp;)
665 {
666 }
667 
668 #endif // ENABLE(DFG_JIT)
669 
670 } } // namespace JSC::DFG
671 
672 
    </pre>
  </body>
</html>