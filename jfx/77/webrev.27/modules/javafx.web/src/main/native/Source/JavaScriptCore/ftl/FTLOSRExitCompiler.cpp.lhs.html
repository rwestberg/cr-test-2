<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/ftl/FTLOSRExitCompiler.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (C) 2013-2019 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
 14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
 17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;FTLOSRExitCompiler.h&quot;
 28 
 29 #if ENABLE(FTL_JIT)
 30 
 31 #include &quot;DFGOSRExitCompilerCommon.h&quot;
 32 #include &quot;DFGOSRExitPreparation.h&quot;
 33 #include &quot;FTLExitArgumentForOperand.h&quot;
 34 #include &quot;FTLJITCode.h&quot;
 35 #include &quot;FTLLocation.h&quot;
 36 #include &quot;FTLOSRExit.h&quot;
 37 #include &quot;FTLOperations.h&quot;
 38 #include &quot;FTLState.h&quot;
 39 #include &quot;FTLSaveRestore.h&quot;
 40 #include &quot;LinkBuffer.h&quot;
 41 #include &quot;MaxFrameExtentForSlowPathCall.h&quot;
 42 #include &quot;OperandsInlines.h&quot;
 43 #include &quot;JSCInlines.h&quot;
 44 
 45 namespace JSC { namespace FTL {
 46 
 47 using namespace DFG;
 48 
 49 static void reboxAccordingToFormat(
 50     DataFormat format, AssemblyHelpers&amp; jit, GPRReg value, GPRReg scratch1, GPRReg scratch2)
 51 {
 52     switch (format) {
 53     case DataFormatInt32: {
 54         jit.zeroExtend32ToPtr(value, value);
 55         jit.or64(GPRInfo::tagTypeNumberRegister, value);
 56         break;
 57     }
 58 
 59     case DataFormatInt52: {
 60         jit.rshift64(AssemblyHelpers::TrustedImm32(JSValue::int52ShiftAmount), value);
 61         jit.moveDoubleTo64(FPRInfo::fpRegT0, scratch2);
 62         jit.boxInt52(value, value, scratch1, FPRInfo::fpRegT0);
 63         jit.move64ToDouble(scratch2, FPRInfo::fpRegT0);
 64         break;
 65     }
 66 
 67     case DataFormatStrictInt52: {
 68         jit.moveDoubleTo64(FPRInfo::fpRegT0, scratch2);
 69         jit.boxInt52(value, value, scratch1, FPRInfo::fpRegT0);
 70         jit.move64ToDouble(scratch2, FPRInfo::fpRegT0);
 71         break;
 72     }
 73 
 74     case DataFormatBoolean: {
 75         jit.zeroExtend32ToPtr(value, value);
 76         jit.or32(MacroAssembler::TrustedImm32(ValueFalse), value);
 77         break;
 78     }
 79 
 80     case DataFormatJS: {
 81         // Done already!
 82         break;
 83     }
 84 
 85     case DataFormatDouble: {
 86         jit.moveDoubleTo64(FPRInfo::fpRegT0, scratch1);
 87         jit.move64ToDouble(value, FPRInfo::fpRegT0);
 88         jit.purifyNaN(FPRInfo::fpRegT0);
 89         jit.boxDouble(FPRInfo::fpRegT0, value);
 90         jit.move64ToDouble(scratch1, FPRInfo::fpRegT0);
 91         break;
 92     }
 93 
 94     default:
 95         RELEASE_ASSERT_NOT_REACHED();
 96         break;
 97     }
 98 }
 99 
100 static void compileRecovery(
101     CCallHelpers&amp; jit, const ExitValue&amp; value,
102     Vector&lt;B3::ValueRep&gt;&amp; valueReps,
103     char* registerScratch,
104     const HashMap&lt;ExitTimeObjectMaterialization*, EncodedJSValue*&gt;&amp; materializationToPointer)
105 {
106     switch (value.kind()) {
107     case ExitValueDead:
108         jit.move(MacroAssembler::TrustedImm64(JSValue::encode(jsUndefined())), GPRInfo::regT0);
109         break;
110 
111     case ExitValueConstant:
112         jit.move(MacroAssembler::TrustedImm64(JSValue::encode(value.constant())), GPRInfo::regT0);
113         break;
114 
115     case ExitValueArgument:
116         Location::forValueRep(valueReps[value.exitArgument().argument()]).restoreInto(
117             jit, registerScratch, GPRInfo::regT0);
118         break;
119 
120     case ExitValueInJSStack:
121     case ExitValueInJSStackAsInt32:
122     case ExitValueInJSStackAsInt52:
123     case ExitValueInJSStackAsDouble:
124         jit.load64(AssemblyHelpers::addressFor(value.virtualRegister()), GPRInfo::regT0);
125         break;
126 
<a name="1" id="anc1"></a><span class="line-removed">127     case ExitValueRecovery:</span>
<span class="line-removed">128         Location::forValueRep(valueReps[value.rightRecoveryArgument()]).restoreInto(</span>
<span class="line-removed">129             jit, registerScratch, GPRInfo::regT1);</span>
<span class="line-removed">130         Location::forValueRep(valueReps[value.leftRecoveryArgument()]).restoreInto(</span>
<span class="line-removed">131             jit, registerScratch, GPRInfo::regT0);</span>
<span class="line-removed">132         switch (value.recoveryOpcode()) {</span>
<span class="line-removed">133         case AddRecovery:</span>
<span class="line-removed">134             switch (value.recoveryFormat()) {</span>
<span class="line-removed">135             case DataFormatInt32:</span>
<span class="line-removed">136                 jit.add32(GPRInfo::regT1, GPRInfo::regT0);</span>
<span class="line-removed">137                 break;</span>
<span class="line-removed">138             case DataFormatInt52:</span>
<span class="line-removed">139                 jit.add64(GPRInfo::regT1, GPRInfo::regT0);</span>
<span class="line-removed">140                 break;</span>
<span class="line-removed">141             default:</span>
<span class="line-removed">142                 RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-removed">143                 break;</span>
<span class="line-removed">144             }</span>
<span class="line-removed">145             break;</span>
<span class="line-removed">146         case SubRecovery:</span>
<span class="line-removed">147             switch (value.recoveryFormat()) {</span>
<span class="line-removed">148             case DataFormatInt32:</span>
<span class="line-removed">149                 jit.sub32(GPRInfo::regT1, GPRInfo::regT0);</span>
<span class="line-removed">150                 break;</span>
<span class="line-removed">151             case DataFormatInt52:</span>
<span class="line-removed">152                 jit.sub64(GPRInfo::regT1, GPRInfo::regT0);</span>
<span class="line-removed">153                 break;</span>
<span class="line-removed">154             default:</span>
<span class="line-removed">155                 RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-removed">156                 break;</span>
<span class="line-removed">157             }</span>
<span class="line-removed">158             break;</span>
<span class="line-removed">159         default:</span>
<span class="line-removed">160             RELEASE_ASSERT_NOT_REACHED();</span>
<span class="line-removed">161             break;</span>
<span class="line-removed">162         }</span>
<span class="line-removed">163         break;</span>
<span class="line-removed">164 </span>
165     case ExitValueMaterializeNewObject:
166         jit.loadPtr(materializationToPointer.get(value.objectMaterialization()), GPRInfo::regT0);
167         break;
168 
169     default:
170         RELEASE_ASSERT_NOT_REACHED();
171         break;
172     }
173 
174     reboxAccordingToFormat(
175         value.dataFormat(), jit, GPRInfo::regT0, GPRInfo::regT1, GPRInfo::regT2);
176 }
177 
178 static void compileStub(
179     unsigned exitID, JITCode* jitCode, OSRExit&amp; exit, VM* vm, CodeBlock* codeBlock)
180 {
181     // This code requires framePointerRegister is the same as callFrameRegister
182     static_assert(MacroAssembler::framePointerRegister == GPRInfo::callFrameRegister, &quot;MacroAssembler::framePointerRegister and GPRInfo::callFrameRegister must be the same&quot;);
183 
184     CCallHelpers jit(codeBlock);
185 
186     // The first thing we need to do is restablish our frame in the case of an exception.
187     if (exit.isGenericUnwindHandler()) {
188         RELEASE_ASSERT(vm-&gt;callFrameForCatch); // The first time we hit this exit, like at all other times, this field should be non-null.
189         jit.restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm-&gt;topEntryFrame);
190         jit.loadPtr(vm-&gt;addressOfCallFrameForCatch(), MacroAssembler::framePointerRegister);
191         jit.addPtr(CCallHelpers::TrustedImm32(codeBlock-&gt;stackPointerOffset() * sizeof(Register)),
192             MacroAssembler::framePointerRegister, CCallHelpers::stackPointerRegister);
193 
194         // Do a pushToSave because that&#39;s what the exit compiler below expects the stack
195         // to look like because that&#39;s the last thing the ExitThunkGenerator does. The code
196         // below doesn&#39;t actually use the value that was pushed, but it does rely on the
197         // general shape of the stack being as it is in the non-exception OSR case.
198         jit.pushToSaveImmediateWithoutTouchingRegisters(CCallHelpers::TrustedImm32(0xbadbeef));
199     }
200 
201     // We need scratch space to save all registers, to build up the JS stack, to deal with unwind
202     // fixup, pointers to all of the objects we materialize, and the elements inside those objects
203     // that we materialize.
204 
205     // Figure out how much space we need for those object allocations.
206     unsigned numMaterializations = 0;
207     size_t maxMaterializationNumArguments = 0;
208     for (ExitTimeObjectMaterialization* materialization : exit.m_descriptor-&gt;m_materializations) {
209         numMaterializations++;
210 
211         maxMaterializationNumArguments = std::max(
212             maxMaterializationNumArguments,
213             materialization-&gt;properties().size());
214     }
215 
216     ScratchBuffer* scratchBuffer = vm-&gt;scratchBufferForSize(
217         sizeof(EncodedJSValue) * (
218             exit.m_descriptor-&gt;m_values.size() + numMaterializations + maxMaterializationNumArguments) +
219         requiredScratchMemorySizeInBytes() +
220         codeBlock-&gt;calleeSaveRegisters()-&gt;size() * sizeof(uint64_t));
221     EncodedJSValue* scratch = scratchBuffer ? static_cast&lt;EncodedJSValue*&gt;(scratchBuffer-&gt;dataBuffer()) : 0;
222     EncodedJSValue* materializationPointers = scratch + exit.m_descriptor-&gt;m_values.size();
223     EncodedJSValue* materializationArguments = materializationPointers + numMaterializations;
224     char* registerScratch = bitwise_cast&lt;char*&gt;(materializationArguments + maxMaterializationNumArguments);
225     uint64_t* unwindScratch = bitwise_cast&lt;uint64_t*&gt;(registerScratch + requiredScratchMemorySizeInBytes());
226 
227     HashMap&lt;ExitTimeObjectMaterialization*, EncodedJSValue*&gt; materializationToPointer;
228     unsigned materializationCount = 0;
229     for (ExitTimeObjectMaterialization* materialization : exit.m_descriptor-&gt;m_materializations) {
230         materializationToPointer.add(
231             materialization, materializationPointers + materializationCount++);
232     }
233 
234     auto recoverValue = [&amp;] (const ExitValue&amp; value) {
235         compileRecovery(
236             jit, value,
237             exit.m_valueReps,
238             registerScratch, materializationToPointer);
239     };
240 
241     // Note that we come in here, the stack used to be as B3 left it except that someone called pushToSave().
242     // We don&#39;t care about the value they saved. But, we do appreciate the fact that they did it, because we use
243     // that slot for saveAllRegisters().
244 
245     saveAllRegisters(jit, registerScratch);
246 
247     if (validateDFGDoesGC) {
248         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
249         // code running that expects no GC. We need to set this before object
250         // materialization below.
251 
252         // Even though we set Heap::m_expectDoesGC in compileFTLOSRExit(), we also need
253         // to set it here because compileFTLOSRExit() is only called on the first time
254         // we exit from this site, but all subsequent exits will take this compiled
255         // ramp without calling compileFTLOSRExit() first.
256         jit.store8(CCallHelpers::TrustedImm32(true), vm-&gt;heap.addressOfExpectDoesGC());
257     }
258 
259     // Bring the stack back into a sane form and assert that it&#39;s sane.
260     jit.popToRestore(GPRInfo::regT0);
261     jit.checkStackPointerAlignment();
262 
263     if (UNLIKELY(vm-&gt;m_perBytecodeProfiler &amp;&amp; jitCode-&gt;dfgCommon()-&gt;compilation)) {
264         Profiler::Database&amp; database = *vm-&gt;m_perBytecodeProfiler;
265         Profiler::Compilation* compilation = jitCode-&gt;dfgCommon()-&gt;compilation.get();
266 
267         Profiler::OSRExit* profilerExit = compilation-&gt;addOSRExit(
268             exitID, Profiler::OriginStack(database, codeBlock, exit.m_codeOrigin),
269             exit.m_kind, exit.m_kind == UncountableInvalidation);
270         jit.add64(CCallHelpers::TrustedImm32(1), CCallHelpers::AbsoluteAddress(profilerExit-&gt;counterAddress()));
271     }
272 
273     // The remaining code assumes that SP/FP are in the same state that they were in the FTL&#39;s
274     // call frame.
275 
276     // Get the call frame and tag thingies.
277     // Restore the exiting function&#39;s callFrame value into a regT4
278     jit.move(MacroAssembler::TrustedImm64(TagTypeNumber), GPRInfo::tagTypeNumberRegister);
279     jit.move(MacroAssembler::TrustedImm64(TagMask), GPRInfo::tagMaskRegister);
280 
281     // Do some value profiling.
282     if (exit.m_descriptor-&gt;m_profileDataFormat != DataFormatNone) {
283         Location::forValueRep(exit.m_valueReps[0]).restoreInto(jit, registerScratch, GPRInfo::regT0);
284         reboxAccordingToFormat(
285             exit.m_descriptor-&gt;m_profileDataFormat, jit, GPRInfo::regT0, GPRInfo::regT1, GPRInfo::regT2);
286 
287         if (exit.m_kind == BadCache || exit.m_kind == BadIndexingType) {
288             CodeOrigin codeOrigin = exit.m_codeOriginForExitProfile;
<a name="2" id="anc2"></a><span class="line-modified">289             if (ArrayProfile* arrayProfile = jit.baselineCodeBlockFor(codeOrigin)-&gt;getArrayProfile(codeOrigin.bytecodeIndex)) {</span>
290                 jit.load32(MacroAssembler::Address(GPRInfo::regT0, JSCell::structureIDOffset()), GPRInfo::regT1);
291                 jit.store32(GPRInfo::regT1, arrayProfile-&gt;addressOfLastSeenStructureID());
292 
293                 jit.load8(MacroAssembler::Address(GPRInfo::regT0, JSCell::typeInfoTypeOffset()), GPRInfo::regT2);
294                 jit.sub32(MacroAssembler::TrustedImm32(FirstTypedArrayType), GPRInfo::regT2);
295                 auto notTypedArray = jit.branch32(MacroAssembler::AboveOrEqual, GPRInfo::regT2, MacroAssembler::TrustedImm32(NumberOfTypedArrayTypesExcludingDataView));
296                 jit.move(MacroAssembler::TrustedImmPtr(typedArrayModes), GPRInfo::regT1);
297                 jit.load32(MacroAssembler::BaseIndex(GPRInfo::regT1, GPRInfo::regT2, MacroAssembler::TimesFour), GPRInfo::regT2);
298                 auto storeArrayModes = jit.jump();
299 
300                 notTypedArray.link(&amp;jit);
301                 jit.load8(MacroAssembler::Address(GPRInfo::regT0, JSCell::indexingTypeAndMiscOffset()), GPRInfo::regT1);
302                 jit.and32(MacroAssembler::TrustedImm32(IndexingModeMask), GPRInfo::regT1);
303                 jit.move(MacroAssembler::TrustedImm32(1), GPRInfo::regT2);
304                 jit.lshift32(GPRInfo::regT1, GPRInfo::regT2);
305                 storeArrayModes.link(&amp;jit);
306                 jit.or32(GPRInfo::regT2, MacroAssembler::AbsoluteAddress(arrayProfile-&gt;addressOfArrayModes()));
307             }
308         }
309 
310         if (exit.m_descriptor-&gt;m_valueProfile)
311             exit.m_descriptor-&gt;m_valueProfile.emitReportValue(jit, JSValueRegs(GPRInfo::regT0));
312     }
313 
314     // Materialize all objects. Don&#39;t materialize an object until all
315     // of the objects it needs have been materialized. We break cycles
316     // by populating objects late - we only consider an object as
317     // needing another object if the later is needed for the
318     // allocation of the former.
319 
320     HashSet&lt;ExitTimeObjectMaterialization*&gt; toMaterialize;
321     for (ExitTimeObjectMaterialization* materialization : exit.m_descriptor-&gt;m_materializations)
322         toMaterialize.add(materialization);
323 
324     while (!toMaterialize.isEmpty()) {
325         unsigned previousToMaterializeSize = toMaterialize.size();
326 
327         Vector&lt;ExitTimeObjectMaterialization*&gt; worklist;
328         worklist.appendRange(toMaterialize.begin(), toMaterialize.end());
329         for (ExitTimeObjectMaterialization* materialization : worklist) {
330             // Check if we can do anything about this right now.
331             bool allGood = true;
332             for (ExitPropertyValue value : materialization-&gt;properties()) {
333                 if (!value.value().isObjectMaterialization())
334                     continue;
335                 if (!value.location().neededForMaterialization())
336                     continue;
337                 if (toMaterialize.contains(value.value().objectMaterialization())) {
338                     // Gotta skip this one, since it needs a
339                     // materialization that hasn&#39;t been materialized.
340                     allGood = false;
341                     break;
342                 }
343             }
344             if (!allGood)
345                 continue;
346 
347             // All systems go for materializing the object. First we
348             // recover the values of all of its fields and then we
349             // call a function to actually allocate the beast.
350             // We only recover the fields that are needed for the allocation.
351             for (unsigned propertyIndex = materialization-&gt;properties().size(); propertyIndex--;) {
352                 const ExitPropertyValue&amp; property = materialization-&gt;properties()[propertyIndex];
353                 if (!property.location().neededForMaterialization())
354                     continue;
355 
356                 recoverValue(property.value());
357                 jit.storePtr(GPRInfo::regT0, materializationArguments + propertyIndex);
358             }
359 
360             static_assert(FunctionTraits&lt;decltype(operationMaterializeObjectInOSR)&gt;::arity &lt; GPRInfo::numberOfArgumentRegisters, &quot;This call assumes that we don&#39;t pass arguments on the stack.&quot;);
361             jit.setupArguments&lt;decltype(operationMaterializeObjectInOSR)&gt;(
362                 CCallHelpers::TrustedImmPtr(materialization),
363                 CCallHelpers::TrustedImmPtr(materializationArguments));
364             jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationMaterializeObjectInOSR)), GPRInfo::nonArgGPR0);
365             jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
366             jit.storePtr(GPRInfo::returnValueGPR, materializationToPointer.get(materialization));
367 
368             // Let everyone know that we&#39;re done.
369             toMaterialize.remove(materialization);
370         }
371 
372         // We expect progress! This ensures that we crash rather than looping infinitely if there
373         // is something broken about this fixpoint. Or, this could happen if we ever violate the
374         // &quot;materializations form a DAG&quot; rule.
375         RELEASE_ASSERT(toMaterialize.size() &lt; previousToMaterializeSize);
376     }
377 
378     // Now that all the objects have been allocated, we populate them
379     // with the correct values. This time we can recover all the
380     // fields, including those that are only needed for the allocation.
381     for (ExitTimeObjectMaterialization* materialization : exit.m_descriptor-&gt;m_materializations) {
382         for (unsigned propertyIndex = materialization-&gt;properties().size(); propertyIndex--;) {
383             recoverValue(materialization-&gt;properties()[propertyIndex].value());
384             jit.storePtr(GPRInfo::regT0, materializationArguments + propertyIndex);
385         }
386 
387         static_assert(FunctionTraits&lt;decltype(operationPopulateObjectInOSR)&gt;::arity &lt; GPRInfo::numberOfArgumentRegisters, &quot;This call assumes that we don&#39;t pass arguments on the stack.&quot;);
388         jit.setupArguments&lt;decltype(operationPopulateObjectInOSR)&gt;(
389             CCallHelpers::TrustedImmPtr(materialization),
390             CCallHelpers::TrustedImmPtr(materializationToPointer.get(materialization)),
391             CCallHelpers::TrustedImmPtr(materializationArguments));
392         jit.move(CCallHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationPopulateObjectInOSR)), GPRInfo::nonArgGPR0);
393         jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
394     }
395 
396     // Save all state from wherever the exit data tells us it was, into the appropriate place in
397     // the scratch buffer. This also does the reboxing.
398 
399     for (unsigned index = exit.m_descriptor-&gt;m_values.size(); index--;) {
400         recoverValue(exit.m_descriptor-&gt;m_values[index]);
401         jit.store64(GPRInfo::regT0, scratch + index);
402     }
403 
404     // Henceforth we make it look like the exiting function was called through a register
405     // preservation wrapper. This implies that FP must be nudged down by a certain amount. Then
406     // we restore the various things according to either exit.m_descriptor-&gt;m_values or by copying from the
407     // old frame, and finally we save the various callee-save registers into where the
408     // restoration thunk would restore them from.
409 
410     // Before we start messing with the frame, we need to set aside any registers that the
411     // FTL code was preserving.
412     for (unsigned i = codeBlock-&gt;calleeSaveRegisters()-&gt;size(); i--;) {
413         RegisterAtOffset entry = codeBlock-&gt;calleeSaveRegisters()-&gt;at(i);
414         jit.load64(
415             MacroAssembler::Address(MacroAssembler::framePointerRegister, entry.offset()),
416             GPRInfo::regT0);
417         jit.store64(GPRInfo::regT0, unwindScratch + i);
418     }
419 
420     CodeBlock* baselineCodeBlock = jit.baselineCodeBlockFor(exit.m_codeOrigin);
421 
422     // First set up SP so that our data doesn&#39;t get clobbered by signals.
423     unsigned conservativeStackDelta =
424         (exit.m_descriptor-&gt;m_values.numberOfLocals() + baselineCodeBlock-&gt;calleeSaveSpaceAsVirtualRegisters()) * sizeof(Register) +
425         maxFrameExtentForSlowPathCall;
426     conservativeStackDelta = WTF::roundUpToMultipleOf(
427         stackAlignmentBytes(), conservativeStackDelta);
428     jit.addPtr(
429         MacroAssembler::TrustedImm32(-conservativeStackDelta),
430         MacroAssembler::framePointerRegister, MacroAssembler::stackPointerRegister);
431     jit.checkStackPointerAlignment();
432 
433     RegisterSet allFTLCalleeSaves = RegisterSet::ftlCalleeSaveRegisters();
434     const RegisterAtOffsetList* baselineCalleeSaves = baselineCodeBlock-&gt;calleeSaveRegisters();
435     RegisterAtOffsetList* vmCalleeSaves = RegisterSet::vmCalleeSaveRegisterOffsets();
436     RegisterSet vmCalleeSavesToSkip = RegisterSet::stackRegisters();
437     if (exit.isExceptionHandler()) {
438         jit.loadPtr(&amp;vm-&gt;topEntryFrame, GPRInfo::regT1);
439         jit.addPtr(CCallHelpers::TrustedImm32(EntryFrame::calleeSaveRegistersBufferOffset()), GPRInfo::regT1);
440     }
441 
442     for (Reg reg = Reg::first(); reg &lt;= Reg::last(); reg = reg.next()) {
443         if (!allFTLCalleeSaves.get(reg)) {
444             if (exit.isExceptionHandler())
445                 RELEASE_ASSERT(!vmCalleeSaves-&gt;find(reg));
446             continue;
447         }
448         unsigned unwindIndex = codeBlock-&gt;calleeSaveRegisters()-&gt;indexOf(reg);
449         const RegisterAtOffset* baselineRegisterOffset = baselineCalleeSaves-&gt;find(reg);
450         RegisterAtOffset* vmCalleeSave = nullptr;
451         if (exit.isExceptionHandler())
452             vmCalleeSave = vmCalleeSaves-&gt;find(reg);
453 
454         if (reg.isGPR()) {
455             GPRReg regToLoad = baselineRegisterOffset ? GPRInfo::regT0 : reg.gpr();
456             RELEASE_ASSERT(regToLoad != GPRInfo::regT1);
457 
458             if (unwindIndex == UINT_MAX) {
459                 // The FTL compilation didn&#39;t preserve this register. This means that it also
460                 // didn&#39;t use the register. So its value at the beginning of OSR exit should be
461                 // preserved by the thunk. Luckily, we saved all registers into the register
462                 // scratch buffer, so we can restore them from there.
463                 jit.load64(registerScratch + offsetOfReg(reg), regToLoad);
464             } else {
465                 // The FTL compilation preserved the register. Its new value is therefore
466                 // irrelevant, but we can get the value that was preserved by using the unwind
467                 // data. We&#39;ve already copied all unwind-able preserved registers into the unwind
468                 // scratch buffer, so we can get it from there.
469                 jit.load64(unwindScratch + unwindIndex, regToLoad);
470             }
471 
472             if (baselineRegisterOffset)
473                 jit.store64(regToLoad, MacroAssembler::Address(MacroAssembler::framePointerRegister, baselineRegisterOffset-&gt;offset()));
474             if (vmCalleeSave &amp;&amp; !vmCalleeSavesToSkip.get(vmCalleeSave-&gt;reg()))
475                 jit.store64(regToLoad, MacroAssembler::Address(GPRInfo::regT1, vmCalleeSave-&gt;offset()));
476         } else {
477             FPRReg fpRegToLoad = baselineRegisterOffset ? FPRInfo::fpRegT0 : reg.fpr();
478 
479             if (unwindIndex == UINT_MAX)
480                 jit.loadDouble(MacroAssembler::TrustedImmPtr(registerScratch + offsetOfReg(reg)), fpRegToLoad);
481             else
482                 jit.loadDouble(MacroAssembler::TrustedImmPtr(unwindScratch + unwindIndex), fpRegToLoad);
483 
484             if (baselineRegisterOffset)
485                 jit.storeDouble(fpRegToLoad, MacroAssembler::Address(MacroAssembler::framePointerRegister, baselineRegisterOffset-&gt;offset()));
486             if (vmCalleeSave &amp;&amp; !vmCalleeSavesToSkip.get(vmCalleeSave-&gt;reg()))
487                 jit.storeDouble(fpRegToLoad, MacroAssembler::Address(GPRInfo::regT1, vmCalleeSave-&gt;offset()));
488         }
489     }
490 
491     if (exit.isExceptionHandler()) {
492         RegisterAtOffset* vmCalleeSave = vmCalleeSaves-&gt;find(GPRInfo::tagTypeNumberRegister);
493         jit.store64(GPRInfo::tagTypeNumberRegister, MacroAssembler::Address(GPRInfo::regT1, vmCalleeSave-&gt;offset()));
494 
495         vmCalleeSave = vmCalleeSaves-&gt;find(GPRInfo::tagMaskRegister);
496         jit.store64(GPRInfo::tagMaskRegister, MacroAssembler::Address(GPRInfo::regT1, vmCalleeSave-&gt;offset()));
497     }
498 
499     size_t baselineVirtualRegistersForCalleeSaves = baselineCodeBlock-&gt;calleeSaveSpaceAsVirtualRegisters();
500 
501     // Now get state out of the scratch buffer and place it back into the stack. The values are
502     // already reboxed so we just move them.
503     for (unsigned index = exit.m_descriptor-&gt;m_values.size(); index--;) {
504         VirtualRegister reg = exit.m_descriptor-&gt;m_values.virtualRegisterForIndex(index);
505 
506         if (reg.isLocal() &amp;&amp; reg.toLocal() &lt; static_cast&lt;int&gt;(baselineVirtualRegistersForCalleeSaves))
507             continue;
508 
509         jit.load64(scratch + index, GPRInfo::regT0);
510         jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(reg));
511     }
512 
513     handleExitCounts(jit, exit);
514     reifyInlinedCallFrames(jit, exit);
515     adjustAndJumpToTarget(*vm, jit, exit);
516 
517     LinkBuffer patchBuffer(jit, codeBlock);
518     exit.m_code = FINALIZE_CODE_IF(
519         shouldDumpDisassembly() || Options::verboseOSR() || Options::verboseFTLOSRExit(),
520         patchBuffer, OSRExitPtrTag,
521         &quot;FTL OSR exit #%u (%s, %s) from %s, with operands = %s&quot;,
522             exitID, toCString(exit.m_codeOrigin).data(),
523             exitKindToString(exit.m_kind), toCString(*codeBlock).data(),
524             toCString(ignoringContext&lt;DumpContext&gt;(exit.m_descriptor-&gt;m_values)).data()
525         );
526 }
527 
528 extern &quot;C&quot; void* compileFTLOSRExit(ExecState* exec, unsigned exitID)
529 {
530     if (shouldDumpDisassembly() || Options::verboseOSR() || Options::verboseFTLOSRExit())
531         dataLog(&quot;Compiling OSR exit with exitID = &quot;, exitID, &quot;\n&quot;);
532 
533     VM&amp; vm = exec-&gt;vm();
534 
535     if (validateDFGDoesGC) {
536         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
537         // code running that expects no GC.
538         vm.heap.setExpectDoesGC(true);
539     }
540 
541     if (vm.callFrameForCatch)
542         RELEASE_ASSERT(vm.callFrameForCatch == exec);
543 
544     CodeBlock* codeBlock = exec-&gt;codeBlock();
545 
546     ASSERT(codeBlock);
<a name="3" id="anc3"></a><span class="line-modified">547     ASSERT(codeBlock-&gt;jitType() == JITCode::FTLJIT);</span>
548 
549     // It&#39;s sort of preferable that we don&#39;t GC while in here. Anyways, doing so wouldn&#39;t
550     // really be profitable.
551     DeferGCForAWhile deferGC(vm.heap);
552 
553     JITCode* jitCode = codeBlock-&gt;jitCode()-&gt;ftl();
554     OSRExit&amp; exit = jitCode-&gt;osrExit[exitID];
555 
556     if (shouldDumpDisassembly() || Options::verboseOSR() || Options::verboseFTLOSRExit()) {
557         dataLog(&quot;    Owning block: &quot;, pointerDump(codeBlock), &quot;\n&quot;);
558         dataLog(&quot;    Origin: &quot;, exit.m_codeOrigin, &quot;\n&quot;);
559         if (exit.m_codeOriginForExitProfile != exit.m_codeOrigin)
560             dataLog(&quot;    Origin for exit profile: &quot;, exit.m_codeOriginForExitProfile, &quot;\n&quot;);
561         dataLog(&quot;    Current call site index: &quot;, exec-&gt;callSiteIndex().bits(), &quot;\n&quot;);
562         dataLog(&quot;    Exit is exception handler: &quot;, exit.isExceptionHandler(), &quot;\n&quot;);
563         dataLog(&quot;    Is unwind handler: &quot;, exit.isGenericUnwindHandler(), &quot;\n&quot;);
564         dataLog(&quot;    Exit values: &quot;, exit.m_descriptor-&gt;m_values, &quot;\n&quot;);
565         dataLog(&quot;    Value reps: &quot;, listDump(exit.m_valueReps), &quot;\n&quot;);
566         if (!exit.m_descriptor-&gt;m_materializations.isEmpty()) {
567             dataLog(&quot;    Materializations:\n&quot;);
568             for (ExitTimeObjectMaterialization* materialization : exit.m_descriptor-&gt;m_materializations)
569                 dataLog(&quot;        &quot;, pointerDump(materialization), &quot;\n&quot;);
570         }
571     }
572 
573     prepareCodeOriginForOSRExit(exec, exit.m_codeOrigin);
574 
575     compileStub(exitID, jitCode, exit, &amp;vm, codeBlock);
576 
577     MacroAssembler::repatchJump(
578         exit.codeLocationForRepatch(codeBlock), CodeLocationLabel&lt;OSRExitPtrTag&gt;(exit.m_code.code()));
579 
580     return exit.m_code.code().executableAddress();
581 }
582 
583 } } // namespace JSC::FTL
584 
585 #endif // ENABLE(FTL_JIT)
586 
<a name="4" id="anc4"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="4" type="hidden" />
</body>
</html>