<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/MarkedBlock.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (C) 2011-2018 Apple Inc. All rights reserved.
  3  *
  4  * Redistribution and use in source and binary forms, with or without
  5  * modification, are permitted provided that the following conditions
  6  * are met:
  7  * 1. Redistributions of source code must retain the above copyright
  8  *    notice, this list of conditions and the following disclaimer.
  9  * 2. Redistributions in binary form must reproduce the above copyright
 10  *    notice, this list of conditions and the following disclaimer in the
 11  *    documentation and/or other materials provided with the distribution.
 12  *
 13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. AND ITS CONTRIBUTORS ``AS IS&#39;&#39;
 14  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 15  * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 16  * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL APPLE INC. OR ITS CONTRIBUTORS
 17  * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 18  * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 19  * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 20  * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 21  * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 22  * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 23  * THE POSSIBILITY OF SUCH DAMAGE.
 24  */
 25 
 26 #include &quot;config.h&quot;
 27 #include &quot;MarkedBlock.h&quot;
 28 
 29 #include &quot;AlignedMemoryAllocator.h&quot;
 30 #include &quot;BlockDirectoryInlines.h&quot;
 31 #include &quot;FreeListInlines.h&quot;
 32 #include &quot;JSCast.h&quot;
 33 #include &quot;JSDestructibleObject.h&quot;
 34 #include &quot;JSCInlines.h&quot;
 35 #include &quot;MarkedBlockInlines.h&quot;
 36 #include &quot;SuperSampler.h&quot;
 37 #include &quot;SweepingScope.h&quot;
 38 #include &lt;wtf/CommaPrinter.h&gt;
 39 
 40 namespace JSC {
 41 
 42 const size_t MarkedBlock::blockSize;
 43 
 44 static const bool computeBalance = false;
 45 static size_t balance;
 46 
 47 MarkedBlock::Handle* MarkedBlock::tryCreate(Heap&amp; heap, AlignedMemoryAllocator* alignedMemoryAllocator)
 48 {
 49     if (computeBalance) {
 50         balance++;
 51         if (!(balance % 10))
 52             dataLog(&quot;MarkedBlock Balance: &quot;, balance, &quot;\n&quot;);
 53     }
 54     void* blockSpace = alignedMemoryAllocator-&gt;tryAllocateAlignedMemory(blockSize, blockSize);
 55     if (!blockSpace)
 56         return nullptr;
 57     if (scribbleFreeCells())
 58         scribble(blockSpace, blockSize);
 59     return new Handle(heap, alignedMemoryAllocator, blockSpace);
 60 }
 61 
 62 MarkedBlock::Handle::Handle(Heap&amp; heap, AlignedMemoryAllocator* alignedMemoryAllocator, void* blockSpace)
 63     : m_alignedMemoryAllocator(alignedMemoryAllocator)
 64     , m_weakSet(heap.vm(), CellContainer())
 65 {
 66     m_block = new (NotNull, blockSpace) MarkedBlock(*heap.vm(), *this);
 67 
 68     m_weakSet.setContainer(*m_block);
 69 
 70     heap.didAllocateBlock(blockSize);
 71 }
 72 
 73 MarkedBlock::Handle::~Handle()
 74 {
 75     Heap&amp; heap = *this-&gt;heap();
 76     if (computeBalance) {
 77         balance--;
 78         if (!(balance % 10))
 79             dataLog(&quot;MarkedBlock Balance: &quot;, balance, &quot;\n&quot;);
 80     }
 81     removeFromDirectory();
 82     m_block-&gt;~MarkedBlock();
 83     m_alignedMemoryAllocator-&gt;freeAlignedMemory(m_block);
 84     heap.didFreeBlock(blockSize);
 85 }
 86 
 87 MarkedBlock::MarkedBlock(VM&amp; vm, Handle&amp; handle)
 88 {
 89     new (&amp;footer()) Footer(vm, handle);
 90     if (false)
 91         dataLog(RawPointer(this), &quot;: Allocated.\n&quot;);
 92 }
 93 
 94 MarkedBlock::~MarkedBlock()
 95 {
 96     footer().~Footer();
 97 }
 98 
 99 MarkedBlock::Footer::Footer(VM&amp; vm, Handle&amp; handle)
100     : m_handle(handle)
101     , m_vm(&amp;vm)
102     , m_markingVersion(MarkedSpace::nullVersion)
103     , m_newlyAllocatedVersion(MarkedSpace::nullVersion)
104 {
105 }
106 
107 MarkedBlock::Footer::~Footer()
108 {
109 }
110 
111 void MarkedBlock::Handle::unsweepWithNoNewlyAllocated()
112 {
113     RELEASE_ASSERT(m_isFreeListed);
114     m_isFreeListed = false;
115 }
116 
117 void MarkedBlock::Handle::setIsFreeListed()
118 {
119     m_directory-&gt;setIsEmpty(NoLockingNecessary, this, false);
120     m_isFreeListed = true;
121 }
122 
123 void MarkedBlock::Handle::stopAllocating(const FreeList&amp; freeList)
124 {
125     auto locker = holdLock(blockFooter().m_lock);
126 
127     if (false)
128         dataLog(RawPointer(this), &quot;: MarkedBlock::Handle::stopAllocating!\n&quot;);
129     ASSERT(!directory()-&gt;isAllocated(NoLockingNecessary, this));
130 
131     if (!isFreeListed()) {
132         if (false)
133             dataLog(&quot;There ain&#39;t no newly allocated.\n&quot;);
134         // This means that we either didn&#39;t use this block at all for allocation since last GC,
135         // or someone had already done stopAllocating() before.
136         ASSERT(freeList.allocationWillFail());
137         return;
138     }
139 
140     if (false)
141         dataLog(&quot;Free list: &quot;, freeList, &quot;\n&quot;);
142 
143     // Roll back to a coherent state for Heap introspection. Cells newly
144     // allocated from our free list are not currently marked, so we need another
145     // way to tell what&#39;s live vs dead.
146 
147     blockFooter().m_newlyAllocated.clearAll();
148     blockFooter().m_newlyAllocatedVersion = heap()-&gt;objectSpace().newlyAllocatedVersion();
149 
150     forEachCell(
151         [&amp;] (HeapCell* cell, HeapCell::Kind) -&gt; IterationStatus {
152             block().setNewlyAllocated(cell);
153             return IterationStatus::Continue;
154         });
155 
156     freeList.forEach(
157         [&amp;] (HeapCell* cell) {
158             if (false)
159                 dataLog(&quot;Free cell: &quot;, RawPointer(cell), &quot;\n&quot;);
160             if (m_attributes.destruction == NeedsDestruction)
161                 cell-&gt;zap();
162             block().clearNewlyAllocated(cell);
163         });
164 
165     m_isFreeListed = false;
166 }
167 
168 void MarkedBlock::Handle::lastChanceToFinalize()
169 {
170     directory()-&gt;setIsAllocated(NoLockingNecessary, this, false);
171     directory()-&gt;setIsDestructible(NoLockingNecessary, this, true);
172     blockFooter().m_marks.clearAll();
173     block().clearHasAnyMarked();
174     blockFooter().m_markingVersion = heap()-&gt;objectSpace().markingVersion();
175     m_weakSet.lastChanceToFinalize();
176     blockFooter().m_newlyAllocated.clearAll();
177     blockFooter().m_newlyAllocatedVersion = heap()-&gt;objectSpace().newlyAllocatedVersion();
178     sweep(nullptr);
179 }
180 
181 void MarkedBlock::Handle::resumeAllocating(FreeList&amp; freeList)
182 {
183     {
184         auto locker = holdLock(blockFooter().m_lock);
185 
186         if (false)
187             dataLog(RawPointer(this), &quot;: MarkedBlock::Handle::resumeAllocating!\n&quot;);
188         ASSERT(!directory()-&gt;isAllocated(NoLockingNecessary, this));
189         ASSERT(!isFreeListed());
190 
191         if (!block().hasAnyNewlyAllocated()) {
192             if (false)
193                 dataLog(&quot;There ain&#39;t no newly allocated.\n&quot;);
194             // This means we had already exhausted the block when we stopped allocation.
195             freeList.clear();
196             return;
197         }
198     }
199 
200     // Re-create our free list from before stopping allocation. Note that this may return an empty
201     // freelist, in which case the block will still be Marked!
202     sweep(&amp;freeList);
203 }
204 
205 void MarkedBlock::Handle::zap(const FreeList&amp; freeList)
206 {
207     freeList.forEach(
208         [&amp;] (HeapCell* cell) {
209             if (m_attributes.destruction == NeedsDestruction)
210                 cell-&gt;zap();
211         });
212 }
213 
214 void MarkedBlock::aboutToMarkSlow(HeapVersion markingVersion)
215 {
216     ASSERT(vm()-&gt;heap.objectSpace().isMarking());
217     auto locker = holdLock(footer().m_lock);
218 
219     if (!areMarksStale(markingVersion))
220         return;
221 
222     BlockDirectory* directory = handle().directory();
223 
224     if (handle().directory()-&gt;isAllocated(holdLock(directory-&gt;bitvectorLock()), &amp;handle())
225         || !marksConveyLivenessDuringMarking(markingVersion)) {
226         if (false)
227             dataLog(RawPointer(this), &quot;: Clearing marks without doing anything else.\n&quot;);
228         // We already know that the block is full and is already recognized as such, or that the
229         // block did not survive the previous GC. So, we can clear mark bits the old fashioned
230         // way. Note that it&#39;s possible for such a block to have newlyAllocated with an up-to-
231         // date version! If it does, then we want to leave the newlyAllocated alone, since that
232         // means that we had allocated in this previously empty block but did not fill it up, so
233         // we created a newlyAllocated.
234         footer().m_marks.clearAll();
235     } else {
236         if (false)
237             dataLog(RawPointer(this), &quot;: Doing things.\n&quot;);
238         HeapVersion newlyAllocatedVersion = space()-&gt;newlyAllocatedVersion();
239         if (footer().m_newlyAllocatedVersion == newlyAllocatedVersion) {
240             // When do we get here? The block could not have been filled up. The newlyAllocated bits would
241             // have had to be created since the end of the last collection. The only things that create
242             // them are aboutToMarkSlow, lastChanceToFinalize, and stopAllocating. If it had been
243             // aboutToMarkSlow, then we shouldn&#39;t be here since the marks wouldn&#39;t be stale anymore. It
244             // cannot be lastChanceToFinalize. So it must be stopAllocating. That means that we just
245             // computed the newlyAllocated bits just before the start of an increment. When we are in that
246             // mode, it seems as if newlyAllocated should subsume marks.
247             ASSERT(footer().m_newlyAllocated.subsumes(footer().m_marks));
248             footer().m_marks.clearAll();
249         } else {
250             footer().m_newlyAllocated.setAndClear(footer().m_marks);
251             footer().m_newlyAllocatedVersion = newlyAllocatedVersion;
252         }
253     }
254     clearHasAnyMarked();
255     WTF::storeStoreFence();
256     footer().m_markingVersion = markingVersion;
257 
258     // This means we&#39;re the first ones to mark any object in this block.
259     directory-&gt;setIsMarkingNotEmpty(holdLock(directory-&gt;bitvectorLock()), &amp;handle(), true);
260 }
261 
262 void MarkedBlock::resetAllocated()
263 {
264     footer().m_newlyAllocated.clearAll();
265     footer().m_newlyAllocatedVersion = MarkedSpace::nullVersion;
266 }
267 
268 void MarkedBlock::resetMarks()
269 {
270     // We want aboutToMarkSlow() to see what the mark bits were after the last collection. It uses
271     // the version number to distinguish between the marks having already been stale before
272     // beginMarking(), or just stale now that beginMarking() bumped the version. If we have a version
273     // wraparound, then we will call this method before resetting the version to null. When the
274     // version is null, aboutToMarkSlow() will assume that the marks were not stale as of before
275     // beginMarking(). Hence the need to whip the marks into shape.
276     if (areMarksStale())
277         footer().m_marks.clearAll();
278     footer().m_markingVersion = MarkedSpace::nullVersion;
279 }
280 
281 #if !ASSERT_DISABLED
282 void MarkedBlock::assertMarksNotStale()
283 {
284     ASSERT(footer().m_markingVersion == vm()-&gt;heap.objectSpace().markingVersion());
285 }
286 #endif // !ASSERT_DISABLED
287 
288 bool MarkedBlock::areMarksStale()
289 {
290     return areMarksStale(vm()-&gt;heap.objectSpace().markingVersion());
291 }
292 
293 bool MarkedBlock::Handle::areMarksStale()
294 {
295     return m_block-&gt;areMarksStale();
296 }
297 
298 bool MarkedBlock::isMarked(const void* p)
299 {
300     return isMarked(vm()-&gt;heap.objectSpace().markingVersion(), p);
301 }
302 
303 void MarkedBlock::Handle::didConsumeFreeList()
304 {
305     auto locker = holdLock(blockFooter().m_lock);
306     if (false)
307         dataLog(RawPointer(this), &quot;: MarkedBlock::Handle::didConsumeFreeList!\n&quot;);
308     ASSERT(isFreeListed());
309     m_isFreeListed = false;
310     directory()-&gt;setIsAllocated(NoLockingNecessary, this, true);
311 }
312 
313 size_t MarkedBlock::markCount()
314 {
315     return areMarksStale() ? 0 : footer().m_marks.count();
316 }
317 
318 void MarkedBlock::clearHasAnyMarked()
319 {
320     footer().m_biasedMarkCount = footer().m_markCountBias;
321 }
322 
323 void MarkedBlock::noteMarkedSlow()
324 {
325     BlockDirectory* directory = handle().directory();
326     directory-&gt;setIsMarkingRetired(holdLock(directory-&gt;bitvectorLock()), &amp;handle(), true);
327 }
328 
329 void MarkedBlock::Handle::removeFromDirectory()
330 {
331     if (!m_directory)
332         return;
333 
334     m_directory-&gt;removeBlock(this);
335 }
336 
337 void MarkedBlock::Handle::didAddToDirectory(BlockDirectory* directory, size_t index)
338 {
339     ASSERT(m_index == std::numeric_limits&lt;size_t&gt;::max());
340     ASSERT(!m_directory);
341 
342     RELEASE_ASSERT(directory-&gt;subspace()-&gt;alignedMemoryAllocator() == m_alignedMemoryAllocator);
343 
344     m_index = index;
345     m_directory = directory;
346     blockFooter().m_subspace = directory-&gt;subspace();
347 
348     size_t cellSize = directory-&gt;cellSize();
349     m_atomsPerCell = (cellSize + atomSize - 1) / atomSize;
350     m_endAtom = endAtom - m_atomsPerCell + 1;
351 
352     m_attributes = directory-&gt;attributes();
353 
354     if (!isJSCellKind(m_attributes.cellKind))
355         RELEASE_ASSERT(m_attributes.destruction == DoesNotNeedDestruction);
356 
357     double markCountBias = -(Options::minMarkedBlockUtilization() * cellsPerBlock());
358 
359     // The mark count bias should be comfortably within this range.
360     RELEASE_ASSERT(markCountBias &gt; static_cast&lt;double&gt;(std::numeric_limits&lt;int16_t&gt;::min()));
361     RELEASE_ASSERT(markCountBias &lt; 0);
362 
363     // This means we haven&#39;t marked anything yet.
364     blockFooter().m_biasedMarkCount = blockFooter().m_markCountBias = static_cast&lt;int16_t&gt;(markCountBias);
365 }
366 
367 void MarkedBlock::Handle::didRemoveFromDirectory()
368 {
369     ASSERT(m_index != std::numeric_limits&lt;size_t&gt;::max());
370     ASSERT(m_directory);
371 
372     m_index = std::numeric_limits&lt;size_t&gt;::max();
373     m_directory = nullptr;
374     blockFooter().m_subspace = nullptr;
375 }
376 
377 #if !ASSERT_DISABLED
378 void MarkedBlock::assertValidCell(VM&amp; vm, HeapCell* cell) const
379 {
380     RELEASE_ASSERT(&amp;vm == this-&gt;vm());
381     RELEASE_ASSERT(const_cast&lt;MarkedBlock*&gt;(this)-&gt;handle().cellAlign(cell) == cell);
382 }
383 #endif
384 
385 void MarkedBlock::Handle::dumpState(PrintStream&amp; out)
386 {
387     CommaPrinter comma;
388     directory()-&gt;forEachBitVectorWithName(
389         holdLock(directory()-&gt;bitvectorLock()),
390         [&amp;] (FastBitVector&amp; bitvector, const char* name) {
391             out.print(comma, name, &quot;:&quot;, bitvector[index()] ? &quot;YES&quot; : &quot;no&quot;);
392         });
393 }
394 
395 Subspace* MarkedBlock::Handle::subspace() const
396 {
397     return directory()-&gt;subspace();
398 }
399 
400 void MarkedBlock::Handle::sweep(FreeList* freeList)
401 {
402     SweepingScope sweepingScope(*heap());
403 
404     SweepMode sweepMode = freeList ? SweepToFreeList : SweepOnly;
405 
406     m_directory-&gt;setIsUnswept(NoLockingNecessary, this, false);
407 
408     m_weakSet.sweep();
409 
410     bool needsDestruction = m_attributes.destruction == NeedsDestruction
411         &amp;&amp; m_directory-&gt;isDestructible(NoLockingNecessary, this);
412 
413     if (sweepMode == SweepOnly &amp;&amp; !needsDestruction)
414         return;
415 
416     if (m_isFreeListed) {
417         dataLog(&quot;FATAL: &quot;, RawPointer(this), &quot;-&gt;sweep: block is free-listed.\n&quot;);
418         RELEASE_ASSERT_NOT_REACHED();
419     }
420 
421     if (isAllocated()) {
422         dataLog(&quot;FATAL: &quot;, RawPointer(this), &quot;-&gt;sweep: block is allocated.\n&quot;);
423         RELEASE_ASSERT_NOT_REACHED();
424     }
425 
426     if (space()-&gt;isMarking())
427         blockFooter().m_lock.lock();
428 
429     subspace()-&gt;didBeginSweepingToFreeList(this);
430 
431     if (needsDestruction) {
432         subspace()-&gt;finishSweep(*this, freeList);
433         return;
434     }
435 
436     // Handle the no-destructor specializations here, since we have the most of those. This
437     // ensures that they don&#39;t get re-specialized for every destructor space.
438 
439     EmptyMode emptyMode = this-&gt;emptyMode();
440     ScribbleMode scribbleMode = this-&gt;scribbleMode();
441     NewlyAllocatedMode newlyAllocatedMode = this-&gt;newlyAllocatedMode();
442     MarksMode marksMode = this-&gt;marksMode();
443 
444     auto trySpecialized = [&amp;] () -&gt; bool {
445         if (sweepMode != SweepToFreeList)
446             return false;
447         if (scribbleMode != DontScribble)
448             return false;
449         if (newlyAllocatedMode != DoesNotHaveNewlyAllocated)
450             return false;
451 
452         switch (emptyMode) {
453         case IsEmpty:
454             switch (marksMode) {
455             case MarksNotStale:
456                 specializedSweep&lt;true, IsEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksNotStale&gt;(freeList, IsEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksNotStale, [] (VM&amp;, JSCell*) { });
457                 return true;
458             case MarksStale:
459                 specializedSweep&lt;true, IsEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksStale&gt;(freeList, IsEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksStale, [] (VM&amp;, JSCell*) { });
460                 return true;
461             }
462             break;
463         case NotEmpty:
464             switch (marksMode) {
465             case MarksNotStale:
466                 specializedSweep&lt;true, NotEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksNotStale&gt;(freeList, IsEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksNotStale, [] (VM&amp;, JSCell*) { });
467                 return true;
468             case MarksStale:
469                 specializedSweep&lt;true, NotEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksStale&gt;(freeList, IsEmpty, SweepToFreeList, BlockHasNoDestructors, DontScribble, DoesNotHaveNewlyAllocated, MarksStale, [] (VM&amp;, JSCell*) { });
470                 return true;
471             }
472             break;
473         }
474 
475         return false;
476     };
477 
478     if (trySpecialized())
479         return;
480 
481     // The template arguments don&#39;t matter because the first one is false.
482     specializedSweep&lt;false, IsEmpty, SweepOnly, BlockHasNoDestructors, DontScribble, HasNewlyAllocated, MarksStale&gt;(freeList, emptyMode, sweepMode, BlockHasNoDestructors, scribbleMode, newlyAllocatedMode, marksMode, [] (VM&amp;, JSCell*) { });
483 }
484 
485 bool MarkedBlock::Handle::isFreeListedCell(const void* target) const
486 {
487     ASSERT(isFreeListed());
488     return m_directory-&gt;isFreeListedCell(target);
489 }
490 
491 } // namespace JSC
492 
493 namespace WTF {
494 
495 void printInternal(PrintStream&amp; out, JSC::MarkedBlock::Handle::SweepMode mode)
496 {
497     switch (mode) {
498     case JSC::MarkedBlock::Handle::SweepToFreeList:
499         out.print(&quot;SweepToFreeList&quot;);
500         return;
501     case JSC::MarkedBlock::Handle::SweepOnly:
502         out.print(&quot;SweepOnly&quot;);
503         return;
504     }
505     RELEASE_ASSERT_NOT_REACHED();
506 }
507 
508 } // namespace WTF
509 
    </pre>
  </body>
</html>