<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITOperations.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2013-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;JITOperations.h&quot;
  28 
  29 #if ENABLE(JIT)
  30 
  31 #include &quot;ArithProfile.h&quot;
  32 #include &quot;ArrayConstructor.h&quot;
  33 #include &quot;CommonSlowPaths.h&quot;
  34 #include &quot;DFGCompilationMode.h&quot;
  35 #include &quot;DFGDriver.h&quot;
  36 #include &quot;DFGOSREntry.h&quot;
  37 #include &quot;DFGThunks.h&quot;
  38 #include &quot;DFGWorklist.h&quot;
  39 #include &quot;Debugger.h&quot;
  40 #include &quot;DirectArguments.h&quot;
  41 #include &quot;Error.h&quot;
  42 #include &quot;ErrorHandlingScope.h&quot;
  43 #include &quot;EvalCodeBlock.h&quot;
  44 #include &quot;ExceptionFuzz.h&quot;
  45 #include &quot;ExecutableBaseInlines.h&quot;
  46 #include &quot;FTLOSREntry.h&quot;
  47 #include &quot;FrameTracers.h&quot;
  48 #include &quot;FunctionCodeBlock.h&quot;
  49 #include &quot;GetterSetter.h&quot;
  50 #include &quot;HostCallReturnValue.h&quot;
  51 #include &quot;ICStats.h&quot;
  52 #include &quot;Interpreter.h&quot;
  53 #include &quot;JIT.h&quot;
  54 #include &quot;JITExceptions.h&quot;
  55 #include &quot;JITToDFGDeferredCompilationCallback.h&quot;
  56 #include &quot;JSAsyncFunction.h&quot;
  57 #include &quot;JSAsyncGeneratorFunction.h&quot;
  58 #include &quot;JSCInlines.h&quot;
  59 #include &quot;JSCPtrTag.h&quot;
  60 #include &quot;JSGeneratorFunction.h&quot;
  61 #include &quot;JSGlobalObjectFunctions.h&quot;
  62 #include &quot;JSLexicalEnvironment.h&quot;
  63 #include &quot;JSWithScope.h&quot;
  64 #include &quot;ModuleProgramCodeBlock.h&quot;
  65 #include &quot;ObjectConstructor.h&quot;
  66 #include &quot;PolymorphicAccess.h&quot;
  67 #include &quot;ProgramCodeBlock.h&quot;
  68 #include &quot;PropertyName.h&quot;
  69 #include &quot;RegExpObject.h&quot;
  70 #include &quot;Repatch.h&quot;
  71 #include &quot;ScopedArguments.h&quot;
  72 #include &quot;ShadowChicken.h&quot;
  73 #include &quot;StructureStubInfo.h&quot;
  74 #include &quot;SuperSampler.h&quot;
  75 #include &quot;TestRunnerUtils.h&quot;
  76 #include &quot;ThunkGenerators.h&quot;
  77 #include &quot;TypeProfilerLog.h&quot;
  78 #include &quot;VMInlines.h&quot;
  79 #include &lt;wtf/InlineASM.h&gt;
  80 
  81 namespace JSC {
  82 
  83 extern &quot;C&quot; {
  84 
  85 #if COMPILER(MSVC)
  86 void * _ReturnAddress(void);
  87 #pragma intrinsic(_ReturnAddress)
  88 
  89 #define OUR_RETURN_ADDRESS _ReturnAddress()
  90 #else
  91 #define OUR_RETURN_ADDRESS __builtin_return_address(0)
  92 #endif
  93 
  94 #if ENABLE(OPCODE_SAMPLING)
  95 #define CTI_SAMPLER vm-&gt;interpreter-&gt;sampler()
  96 #else
  97 #define CTI_SAMPLER 0
  98 #endif
  99 
 100 
 101 void JIT_OPERATION operationThrowStackOverflowError(ExecState* exec, CodeBlock* codeBlock)
 102 {
 103     // We pass in our own code block, because the callframe hasn&#39;t been populated.
 104     VM* vm = codeBlock-&gt;vm();
 105     auto scope = DECLARE_THROW_SCOPE(*vm);
 106     exec-&gt;convertToStackOverflowFrame(*vm, codeBlock);
 107     NativeCallFrameTracer tracer(vm, exec);
 108     throwStackOverflowError(exec, scope);
 109 }
 110 
 111 int32_t JIT_OPERATION operationCallArityCheck(ExecState* exec)
 112 {
 113     VM* vm = &amp;exec-&gt;vm();
 114     auto scope = DECLARE_THROW_SCOPE(*vm);
 115 
 116     int32_t missingArgCount = CommonSlowPaths::arityCheckFor(exec, *vm, CodeForCall);
 117     if (UNLIKELY(missingArgCount &lt; 0)) {
 118         CodeBlock* codeBlock = CommonSlowPaths::codeBlockFromCallFrameCallee(exec, CodeForCall);
 119         exec-&gt;convertToStackOverflowFrame(*vm, codeBlock);
 120         NativeCallFrameTracer tracer(vm, exec);
 121         throwStackOverflowError(vm-&gt;topCallFrame, scope);
 122     }
 123 
 124     return missingArgCount;
 125 }
 126 
 127 int32_t JIT_OPERATION operationConstructArityCheck(ExecState* exec)
 128 {
 129     VM* vm = &amp;exec-&gt;vm();
 130     auto scope = DECLARE_THROW_SCOPE(*vm);
 131 
 132     int32_t missingArgCount = CommonSlowPaths::arityCheckFor(exec, *vm, CodeForConstruct);
 133     if (UNLIKELY(missingArgCount &lt; 0)) {
 134         CodeBlock* codeBlock = CommonSlowPaths::codeBlockFromCallFrameCallee(exec, CodeForConstruct);
 135         exec-&gt;convertToStackOverflowFrame(*vm, codeBlock);
 136         NativeCallFrameTracer tracer(vm, exec);
 137         throwStackOverflowError(vm-&gt;topCallFrame, scope);
 138     }
 139 
 140     return missingArgCount;
 141 }
 142 
 143 EncodedJSValue JIT_OPERATION operationTryGetById(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue base, UniquedStringImpl* uid)
 144 {
 145     VM* vm = &amp;exec-&gt;vm();
 146     NativeCallFrameTracer tracer(vm, exec);
 147     Identifier ident = Identifier::fromUid(vm, uid);
 148     stubInfo-&gt;tookSlowPath = true;
 149 
 150     JSValue baseValue = JSValue::decode(base);
 151     PropertySlot slot(baseValue, PropertySlot::InternalMethodType::VMInquiry);
 152     baseValue.getPropertySlot(exec, ident, slot);
 153 
 154     return JSValue::encode(slot.getPureResult());
 155 }
 156 
 157 
 158 EncodedJSValue JIT_OPERATION operationTryGetByIdGeneric(ExecState* exec, EncodedJSValue base, UniquedStringImpl* uid)
 159 {
 160     VM* vm = &amp;exec-&gt;vm();
 161     NativeCallFrameTracer tracer(vm, exec);
 162     Identifier ident = Identifier::fromUid(vm, uid);
 163 
 164     JSValue baseValue = JSValue::decode(base);
 165     PropertySlot slot(baseValue, PropertySlot::InternalMethodType::VMInquiry);
 166     baseValue.getPropertySlot(exec, ident, slot);
 167 
 168     return JSValue::encode(slot.getPureResult());
 169 }
 170 
 171 EncodedJSValue JIT_OPERATION operationTryGetByIdOptimize(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue base, UniquedStringImpl* uid)
 172 {
 173     VM* vm = &amp;exec-&gt;vm();
 174     NativeCallFrameTracer tracer(vm, exec);
 175     auto scope = DECLARE_THROW_SCOPE(*vm);
 176     Identifier ident = Identifier::fromUid(vm, uid);
 177 
 178     JSValue baseValue = JSValue::decode(base);
 179     PropertySlot slot(baseValue, PropertySlot::InternalMethodType::VMInquiry);
 180 
 181     baseValue.getPropertySlot(exec, ident, slot);
 182     RETURN_IF_EXCEPTION(scope, encodedJSValue());
 183 
 184     if (stubInfo-&gt;considerCaching(exec-&gt;codeBlock(), baseValue.structureOrNull()) &amp;&amp; !slot.isTaintedByOpaqueObject() &amp;&amp; (slot.isCacheableValue() || slot.isCacheableGetter() || slot.isUnset()))
 185         repatchGetByID(exec, baseValue, ident, slot, *stubInfo, GetByIDKind::Try);
 186 
 187     return JSValue::encode(slot.getPureResult());
 188 }
 189 
 190 EncodedJSValue JIT_OPERATION operationGetByIdDirect(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue base, UniquedStringImpl* uid)
 191 {
 192     VM&amp; vm = exec-&gt;vm();
 193     NativeCallFrameTracer tracer(&amp;vm, exec);
 194     auto scope = DECLARE_THROW_SCOPE(vm);
 195     Identifier ident = Identifier::fromUid(&amp;vm, uid);
 196     stubInfo-&gt;tookSlowPath = true;
 197 
 198     JSValue baseValue = JSValue::decode(base);
 199     PropertySlot slot(baseValue, PropertySlot::InternalMethodType::GetOwnProperty);
 200 
 201     bool found = baseValue.getOwnPropertySlot(exec, ident, slot);
 202     RETURN_IF_EXCEPTION(scope, encodedJSValue());
 203 
 204     RELEASE_AND_RETURN(scope, JSValue::encode(found ? slot.getValue(exec, ident) : jsUndefined()));
 205 }
 206 
 207 EncodedJSValue JIT_OPERATION operationGetByIdDirectGeneric(ExecState* exec, EncodedJSValue base, UniquedStringImpl* uid)
 208 {
 209     VM&amp; vm = exec-&gt;vm();
 210     NativeCallFrameTracer tracer(&amp;vm, exec);
 211     auto scope = DECLARE_THROW_SCOPE(vm);
 212     Identifier ident = Identifier::fromUid(&amp;vm, uid);
 213 
 214     JSValue baseValue = JSValue::decode(base);
 215     PropertySlot slot(baseValue, PropertySlot::InternalMethodType::GetOwnProperty);
 216 
 217     bool found = baseValue.getOwnPropertySlot(exec, ident, slot);
 218     RETURN_IF_EXCEPTION(scope, encodedJSValue());
 219 
 220     RELEASE_AND_RETURN(scope, JSValue::encode(found ? slot.getValue(exec, ident) : jsUndefined()));
 221 }
 222 
 223 EncodedJSValue JIT_OPERATION operationGetByIdDirectOptimize(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue base, UniquedStringImpl* uid)
 224 {
 225     VM&amp; vm = exec-&gt;vm();
 226     NativeCallFrameTracer tracer(&amp;vm, exec);
 227     auto scope = DECLARE_THROW_SCOPE(vm);
 228     Identifier ident = Identifier::fromUid(&amp;vm, uid);
 229 
 230     JSValue baseValue = JSValue::decode(base);
 231     PropertySlot slot(baseValue, PropertySlot::InternalMethodType::GetOwnProperty);
 232 
 233     bool found = baseValue.getOwnPropertySlot(exec, ident, slot);
 234     RETURN_IF_EXCEPTION(scope, encodedJSValue());
 235 
 236     if (stubInfo-&gt;considerCaching(exec-&gt;codeBlock(), baseValue.structureOrNull()))
 237         repatchGetByID(exec, baseValue, ident, slot, *stubInfo, GetByIDKind::Direct);
 238 
 239     RELEASE_AND_RETURN(scope, JSValue::encode(found ? slot.getValue(exec, ident) : jsUndefined()));
 240 }
 241 
 242 EncodedJSValue JIT_OPERATION operationGetById(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue base, UniquedStringImpl* uid)
 243 {
 244     SuperSamplerScope superSamplerScope(false);
 245 
 246     VM* vm = &amp;exec-&gt;vm();
 247     NativeCallFrameTracer tracer(vm, exec);
 248 
 249     stubInfo-&gt;tookSlowPath = true;
 250 
 251     JSValue baseValue = JSValue::decode(base);
 252     PropertySlot slot(baseValue, PropertySlot::InternalMethodType::Get);
 253     Identifier ident = Identifier::fromUid(vm, uid);
 254 
 255     LOG_IC((ICEvent::OperationGetById, baseValue.classInfoOrNull(*vm), ident));
 256     return JSValue::encode(baseValue.get(exec, ident, slot));
 257 }
 258 
 259 EncodedJSValue JIT_OPERATION operationGetByIdGeneric(ExecState* exec, EncodedJSValue base, UniquedStringImpl* uid)
 260 {
 261     SuperSamplerScope superSamplerScope(false);
 262 
 263     VM* vm = &amp;exec-&gt;vm();
 264     NativeCallFrameTracer tracer(vm, exec);
 265 
 266     JSValue baseValue = JSValue::decode(base);
 267     PropertySlot slot(baseValue, PropertySlot::InternalMethodType::Get);
 268     Identifier ident = Identifier::fromUid(vm, uid);
 269     LOG_IC((ICEvent::OperationGetByIdGeneric, baseValue.classInfoOrNull(*vm), ident));
 270     return JSValue::encode(baseValue.get(exec, ident, slot));
 271 }
 272 
 273 EncodedJSValue JIT_OPERATION operationGetByIdOptimize(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue base, UniquedStringImpl* uid)
 274 {
 275     SuperSamplerScope superSamplerScope(false);
 276 
 277     VM* vm = &amp;exec-&gt;vm();
 278     NativeCallFrameTracer tracer(vm, exec);
 279     Identifier ident = Identifier::fromUid(vm, uid);
 280 
 281     JSValue baseValue = JSValue::decode(base);
 282     LOG_IC((ICEvent::OperationGetByIdOptimize, baseValue.classInfoOrNull(*vm), ident));
 283 
 284     return JSValue::encode(baseValue.getPropertySlot(exec, ident, [&amp;] (bool found, PropertySlot&amp; slot) -&gt; JSValue {
 285         if (stubInfo-&gt;considerCaching(exec-&gt;codeBlock(), baseValue.structureOrNull()))
 286             repatchGetByID(exec, baseValue, ident, slot, *stubInfo, GetByIDKind::Normal);
 287         return found ? slot.getValue(exec, ident) : jsUndefined();
 288     }));
 289 }
 290 
 291 EncodedJSValue JIT_OPERATION operationGetByIdWithThis(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue base, EncodedJSValue thisEncoded, UniquedStringImpl* uid)
 292 {
 293     SuperSamplerScope superSamplerScope(false);
 294 
 295     VM* vm = &amp;exec-&gt;vm();
 296     NativeCallFrameTracer tracer(vm, exec);
 297     Identifier ident = Identifier::fromUid(vm, uid);
 298 
 299     stubInfo-&gt;tookSlowPath = true;
 300 
 301     JSValue baseValue = JSValue::decode(base);
 302     JSValue thisValue = JSValue::decode(thisEncoded);
 303     PropertySlot slot(thisValue, PropertySlot::InternalMethodType::Get);
 304 
 305     return JSValue::encode(baseValue.get(exec, ident, slot));
 306 }
 307 
 308 EncodedJSValue JIT_OPERATION operationGetByIdWithThisGeneric(ExecState* exec, EncodedJSValue base, EncodedJSValue thisEncoded, UniquedStringImpl* uid)
 309 {
 310     SuperSamplerScope superSamplerScope(false);
 311 
 312     VM* vm = &amp;exec-&gt;vm();
 313     NativeCallFrameTracer tracer(vm, exec);
 314     Identifier ident = Identifier::fromUid(vm, uid);
 315 
 316     JSValue baseValue = JSValue::decode(base);
 317     JSValue thisValue = JSValue::decode(thisEncoded);
 318     PropertySlot slot(thisValue, PropertySlot::InternalMethodType::Get);
 319 
 320     return JSValue::encode(baseValue.get(exec, ident, slot));
 321 }
 322 
 323 EncodedJSValue JIT_OPERATION operationGetByIdWithThisOptimize(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue base, EncodedJSValue thisEncoded, UniquedStringImpl* uid)
 324 {
 325     SuperSamplerScope superSamplerScope(false);
 326 
 327     VM* vm = &amp;exec-&gt;vm();
 328     NativeCallFrameTracer tracer(vm, exec);
 329     Identifier ident = Identifier::fromUid(vm, uid);
 330 
 331     JSValue baseValue = JSValue::decode(base);
 332     JSValue thisValue = JSValue::decode(thisEncoded);
 333     LOG_IC((ICEvent::OperationGetByIdWithThisOptimize, baseValue.classInfoOrNull(*vm), ident));
 334 
 335     PropertySlot slot(thisValue, PropertySlot::InternalMethodType::Get);
 336     return JSValue::encode(baseValue.getPropertySlot(exec, ident, slot, [&amp;] (bool found, PropertySlot&amp; slot) -&gt; JSValue {
 337         if (stubInfo-&gt;considerCaching(exec-&gt;codeBlock(), baseValue.structureOrNull()))
 338             repatchGetByID(exec, baseValue, ident, slot, *stubInfo, GetByIDKind::WithThis);
 339         return found ? slot.getValue(exec, ident) : jsUndefined();
 340     }));
 341 }
 342 
 343 EncodedJSValue JIT_OPERATION operationInById(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue base, UniquedStringImpl* uid)
 344 {
 345     SuperSamplerScope superSamplerScope(false);
 346 
 347     VM&amp; vm = exec-&gt;vm();
 348     NativeCallFrameTracer tracer(&amp;vm, exec);
 349     auto scope = DECLARE_THROW_SCOPE(vm);
 350 
 351     stubInfo-&gt;tookSlowPath = true;
 352 
 353     Identifier ident = Identifier::fromUid(&amp;vm, uid);
 354 
 355     JSValue baseValue = JSValue::decode(base);
 356     if (!baseValue.isObject()) {
 357         throwException(exec, scope, createInvalidInParameterError(exec, baseValue));
 358         return JSValue::encode(jsUndefined());
 359     }
 360     JSObject* baseObject = asObject(baseValue);
 361 
 362     LOG_IC((ICEvent::OperationInById, baseObject-&gt;classInfo(vm), ident));
 363 
 364     scope.release();
 365     PropertySlot slot(baseObject, PropertySlot::InternalMethodType::HasProperty);
 366     return JSValue::encode(jsBoolean(baseObject-&gt;getPropertySlot(exec, ident, slot)));
 367 }
 368 
 369 EncodedJSValue JIT_OPERATION operationInByIdGeneric(ExecState* exec, EncodedJSValue base, UniquedStringImpl* uid)
 370 {
 371     SuperSamplerScope superSamplerScope(false);
 372 
 373     VM&amp; vm = exec-&gt;vm();
 374     NativeCallFrameTracer tracer(&amp;vm, exec);
 375     auto scope = DECLARE_THROW_SCOPE(vm);
 376 
 377     Identifier ident = Identifier::fromUid(&amp;vm, uid);
 378 
 379     JSValue baseValue = JSValue::decode(base);
 380     if (!baseValue.isObject()) {
 381         throwException(exec, scope, createInvalidInParameterError(exec, baseValue));
 382         return JSValue::encode(jsUndefined());
 383     }
 384     JSObject* baseObject = asObject(baseValue);
 385 
 386     LOG_IC((ICEvent::OperationInByIdGeneric, baseObject-&gt;classInfo(vm), ident));
 387 
 388     scope.release();
 389     PropertySlot slot(baseObject, PropertySlot::InternalMethodType::HasProperty);
 390     return JSValue::encode(jsBoolean(baseObject-&gt;getPropertySlot(exec, ident, slot)));
 391 }
 392 
 393 EncodedJSValue JIT_OPERATION operationInByIdOptimize(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue base, UniquedStringImpl* uid)
 394 {
 395     SuperSamplerScope superSamplerScope(false);
 396 
 397     VM&amp; vm = exec-&gt;vm();
 398     NativeCallFrameTracer tracer(&amp;vm, exec);
 399     auto scope = DECLARE_THROW_SCOPE(vm);
 400 
 401     Identifier ident = Identifier::fromUid(&amp;vm, uid);
 402 
 403     JSValue baseValue = JSValue::decode(base);
 404     if (!baseValue.isObject()) {
 405         throwException(exec, scope, createInvalidInParameterError(exec, baseValue));
 406         return JSValue::encode(jsUndefined());
 407     }
 408     JSObject* baseObject = asObject(baseValue);
 409 
 410     LOG_IC((ICEvent::OperationInByIdOptimize, baseObject-&gt;classInfo(vm), ident));
 411 
 412     scope.release();
 413     PropertySlot slot(baseObject, PropertySlot::InternalMethodType::HasProperty);
 414     bool found = baseObject-&gt;getPropertySlot(exec, ident, slot);
 415     if (stubInfo-&gt;considerCaching(exec-&gt;codeBlock(), baseObject-&gt;structure(vm)))
 416         repatchInByID(exec, baseObject, ident, found, slot, *stubInfo);
 417     return JSValue::encode(jsBoolean(found));
 418 }
 419 
 420 EncodedJSValue JIT_OPERATION operationInByVal(ExecState* exec, JSCell* base, EncodedJSValue key)
 421 {
 422     SuperSamplerScope superSamplerScope(false);
 423 
 424     VM* vm = &amp;exec-&gt;vm();
 425     NativeCallFrameTracer tracer(vm, exec);
 426 
 427     return JSValue::encode(jsBoolean(CommonSlowPaths::opInByVal(exec, base, JSValue::decode(key))));
 428 }
 429 
 430 void JIT_OPERATION operationPutByIdStrict(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue encodedValue, EncodedJSValue encodedBase, UniquedStringImpl* uid)
 431 {
 432     SuperSamplerScope superSamplerScope(false);
 433 
 434     VM* vm = &amp;exec-&gt;vm();
 435     NativeCallFrameTracer tracer(vm, exec);
 436 
 437     stubInfo-&gt;tookSlowPath = true;
 438 
 439     JSValue baseValue = JSValue::decode(encodedBase);
 440     Identifier ident = Identifier::fromUid(vm, uid);
 441     LOG_IC((ICEvent::OperationPutByIdStrict, baseValue.classInfoOrNull(*vm), ident));
 442 
 443     PutPropertySlot slot(baseValue, true, exec-&gt;codeBlock()-&gt;putByIdContext());
 444     baseValue.putInline(exec, ident, JSValue::decode(encodedValue), slot);
 445 }
 446 
 447 void JIT_OPERATION operationPutByIdNonStrict(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue encodedValue, EncodedJSValue encodedBase, UniquedStringImpl* uid)
 448 {
 449     SuperSamplerScope superSamplerScope(false);
 450 
 451     VM* vm = &amp;exec-&gt;vm();
 452     NativeCallFrameTracer tracer(vm, exec);
 453 
 454     stubInfo-&gt;tookSlowPath = true;
 455 
 456     JSValue baseValue = JSValue::decode(encodedBase);
 457     Identifier ident = Identifier::fromUid(vm, uid);
 458     LOG_IC((ICEvent::OperationPutByIdNonStrict, baseValue.classInfoOrNull(*vm), ident));
 459     PutPropertySlot slot(baseValue, false, exec-&gt;codeBlock()-&gt;putByIdContext());
 460     baseValue.putInline(exec, ident, JSValue::decode(encodedValue), slot);
 461 }
 462 
 463 void JIT_OPERATION operationPutByIdDirectStrict(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue encodedValue, EncodedJSValue encodedBase, UniquedStringImpl* uid)
 464 {
 465     SuperSamplerScope superSamplerScope(false);
 466 
 467     VM&amp; vm = exec-&gt;vm();
 468     NativeCallFrameTracer tracer(&amp;vm, exec);
 469 
 470     stubInfo-&gt;tookSlowPath = true;
 471 
 472     JSValue baseValue = JSValue::decode(encodedBase);
 473     Identifier ident = Identifier::fromUid(&amp;vm, uid);
 474     LOG_IC((ICEvent::OperationPutByIdDirectStrict, baseValue.classInfoOrNull(vm), ident));
 475     PutPropertySlot slot(baseValue, true, exec-&gt;codeBlock()-&gt;putByIdContext());
 476     CommonSlowPaths::putDirectWithReify(vm, exec, asObject(baseValue), ident, JSValue::decode(encodedValue), slot);
 477 }
 478 
 479 void JIT_OPERATION operationPutByIdDirectNonStrict(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue encodedValue, EncodedJSValue encodedBase, UniquedStringImpl* uid)
 480 {
 481     SuperSamplerScope superSamplerScope(false);
 482 
 483     VM&amp; vm = exec-&gt;vm();
 484     NativeCallFrameTracer tracer(&amp;vm, exec);
 485 
 486     stubInfo-&gt;tookSlowPath = true;
 487 
 488     JSValue baseValue = JSValue::decode(encodedBase);
 489     Identifier ident = Identifier::fromUid(&amp;vm, uid);
 490     LOG_IC((ICEvent::OperationPutByIdDirectNonStrict, baseValue.classInfoOrNull(vm), ident));
 491     PutPropertySlot slot(baseValue, false, exec-&gt;codeBlock()-&gt;putByIdContext());
 492     CommonSlowPaths::putDirectWithReify(vm, exec, asObject(baseValue), ident, JSValue::decode(encodedValue), slot);
 493 }
 494 
 495 void JIT_OPERATION operationPutByIdStrictOptimize(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue encodedValue, EncodedJSValue encodedBase, UniquedStringImpl* uid)
 496 {
 497     SuperSamplerScope superSamplerScope(false);
 498 
 499     VM* vm = &amp;exec-&gt;vm();
 500     NativeCallFrameTracer tracer(vm, exec);
 501     auto scope = DECLARE_THROW_SCOPE(*vm);
 502 
 503     Identifier ident = Identifier::fromUid(vm, uid);
 504     AccessType accessType = static_cast&lt;AccessType&gt;(stubInfo-&gt;accessType);
 505 
 506     JSValue value = JSValue::decode(encodedValue);
 507     JSValue baseValue = JSValue::decode(encodedBase);
 508     LOG_IC((ICEvent::OperationPutByIdStrictOptimize, baseValue.classInfoOrNull(*vm), ident));
 509     CodeBlock* codeBlock = exec-&gt;codeBlock();
 510     PutPropertySlot slot(baseValue, true, codeBlock-&gt;putByIdContext());
 511 
 512     Structure* structure = baseValue.isCell() ? baseValue.asCell()-&gt;structure(*vm) : nullptr;
 513     baseValue.putInline(exec, ident, value, slot);
 514     RETURN_IF_EXCEPTION(scope, void());
 515 
 516     if (accessType != static_cast&lt;AccessType&gt;(stubInfo-&gt;accessType))
 517         return;
 518 
 519     if (stubInfo-&gt;considerCaching(codeBlock, structure))
 520         repatchPutByID(exec, baseValue, structure, ident, slot, *stubInfo, NotDirect);
 521 }
 522 
 523 void JIT_OPERATION operationPutByIdNonStrictOptimize(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue encodedValue, EncodedJSValue encodedBase, UniquedStringImpl* uid)
 524 {
 525     SuperSamplerScope superSamplerScope(false);
 526 
 527     VM* vm = &amp;exec-&gt;vm();
 528     NativeCallFrameTracer tracer(vm, exec);
 529     auto scope = DECLARE_THROW_SCOPE(*vm);
 530 
 531     Identifier ident = Identifier::fromUid(vm, uid);
 532     AccessType accessType = static_cast&lt;AccessType&gt;(stubInfo-&gt;accessType);
 533 
 534     JSValue value = JSValue::decode(encodedValue);
 535     JSValue baseValue = JSValue::decode(encodedBase);
 536     LOG_IC((ICEvent::OperationPutByIdNonStrictOptimize, baseValue.classInfoOrNull(*vm), ident));
 537     CodeBlock* codeBlock = exec-&gt;codeBlock();
 538     PutPropertySlot slot(baseValue, false, codeBlock-&gt;putByIdContext());
 539 
 540     Structure* structure = baseValue.isCell() ? baseValue.asCell()-&gt;structure(*vm) : nullptr;
 541     baseValue.putInline(exec, ident, value, slot);
 542     RETURN_IF_EXCEPTION(scope, void());
 543 
 544     if (accessType != static_cast&lt;AccessType&gt;(stubInfo-&gt;accessType))
 545         return;
 546 
 547     if (stubInfo-&gt;considerCaching(codeBlock, structure))
 548         repatchPutByID(exec, baseValue, structure, ident, slot, *stubInfo, NotDirect);
 549 }
 550 
 551 void JIT_OPERATION operationPutByIdDirectStrictOptimize(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue encodedValue, EncodedJSValue encodedBase, UniquedStringImpl* uid)
 552 {
 553     SuperSamplerScope superSamplerScope(false);
 554 
 555     VM&amp; vm = exec-&gt;vm();
 556     NativeCallFrameTracer tracer(&amp;vm, exec);
 557     auto scope = DECLARE_THROW_SCOPE(vm);
 558 
 559     Identifier ident = Identifier::fromUid(&amp;vm, uid);
 560     AccessType accessType = static_cast&lt;AccessType&gt;(stubInfo-&gt;accessType);
 561 
 562     JSValue value = JSValue::decode(encodedValue);
 563     JSObject* baseObject = asObject(JSValue::decode(encodedBase));
 564     LOG_IC((ICEvent::OperationPutByIdDirectStrictOptimize, baseObject-&gt;classInfo(vm), ident));
 565     CodeBlock* codeBlock = exec-&gt;codeBlock();
 566     PutPropertySlot slot(baseObject, true, codeBlock-&gt;putByIdContext());
 567     Structure* structure = nullptr;
 568     CommonSlowPaths::putDirectWithReify(vm, exec, baseObject, ident, value, slot, &amp;structure);
 569     RETURN_IF_EXCEPTION(scope, void());
 570 
 571     if (accessType != static_cast&lt;AccessType&gt;(stubInfo-&gt;accessType))
 572         return;
 573 
 574     if (stubInfo-&gt;considerCaching(codeBlock, structure))
 575         repatchPutByID(exec, baseObject, structure, ident, slot, *stubInfo, Direct);
 576 }
 577 
 578 void JIT_OPERATION operationPutByIdDirectNonStrictOptimize(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue encodedValue, EncodedJSValue encodedBase, UniquedStringImpl* uid)
 579 {
 580     SuperSamplerScope superSamplerScope(false);
 581 
 582     VM&amp; vm = exec-&gt;vm();
 583     NativeCallFrameTracer tracer(&amp;vm, exec);
 584     auto scope = DECLARE_THROW_SCOPE(vm);
 585 
 586     Identifier ident = Identifier::fromUid(&amp;vm, uid);
 587     AccessType accessType = static_cast&lt;AccessType&gt;(stubInfo-&gt;accessType);
 588 
 589     JSValue value = JSValue::decode(encodedValue);
 590     JSObject* baseObject = asObject(JSValue::decode(encodedBase));
 591     LOG_IC((ICEvent::OperationPutByIdDirectNonStrictOptimize, baseObject-&gt;classInfo(vm), ident));
 592     CodeBlock* codeBlock = exec-&gt;codeBlock();
 593     PutPropertySlot slot(baseObject, false, codeBlock-&gt;putByIdContext());
 594     Structure* structure = nullptr;
 595     CommonSlowPaths::putDirectWithReify(vm, exec, baseObject, ident, value, slot, &amp;structure);
 596     RETURN_IF_EXCEPTION(scope, void());
 597 
 598     if (accessType != static_cast&lt;AccessType&gt;(stubInfo-&gt;accessType))
 599         return;
 600 
 601     if (stubInfo-&gt;considerCaching(codeBlock, structure))
 602         repatchPutByID(exec, baseObject, structure, ident, slot, *stubInfo, Direct);
 603 }
 604 
 605 ALWAYS_INLINE static bool isStringOrSymbol(JSValue value)
 606 {
 607     return value.isString() || value.isSymbol();
 608 }
 609 
 610 static void putByVal(CallFrame* callFrame, JSValue baseValue, JSValue subscript, JSValue value, ByValInfo* byValInfo)
 611 {
 612     VM&amp; vm = callFrame-&gt;vm();
 613     auto scope = DECLARE_THROW_SCOPE(vm);
 614     if (LIKELY(subscript.isUInt32())) {
 615         byValInfo-&gt;tookSlowPath = true;
 616         uint32_t i = subscript.asUInt32();
 617         if (baseValue.isObject()) {
 618             JSObject* object = asObject(baseValue);
 619             if (object-&gt;canSetIndexQuickly(i)) {
 620                 object-&gt;setIndexQuickly(vm, i, value);
 621                 return;
 622             }
 623 
 624             // FIXME: This will make us think that in-bounds typed array accesses are actually
 625             // out-of-bounds.
 626             // https://bugs.webkit.org/show_bug.cgi?id=149886
 627             byValInfo-&gt;arrayProfile-&gt;setOutOfBounds();
 628             scope.release();
 629             object-&gt;methodTable(vm)-&gt;putByIndex(object, callFrame, i, value, callFrame-&gt;codeBlock()-&gt;isStrictMode());
 630             return;
 631         }
 632 
 633         scope.release();
 634         baseValue.putByIndex(callFrame, i, value, callFrame-&gt;codeBlock()-&gt;isStrictMode());
 635         return;
 636     }
 637 
 638     auto property = subscript.toPropertyKey(callFrame);
 639     // Don&#39;t put to an object if toString threw an exception.
 640     RETURN_IF_EXCEPTION(scope, void());
 641 
 642     if (byValInfo-&gt;stubInfo &amp;&amp; (!isStringOrSymbol(subscript) || byValInfo-&gt;cachedId != property))
 643         byValInfo-&gt;tookSlowPath = true;
 644 
 645     scope.release();
 646     PutPropertySlot slot(baseValue, callFrame-&gt;codeBlock()-&gt;isStrictMode());
 647     baseValue.putInline(callFrame, property, value, slot);
 648 }
 649 
 650 static void directPutByVal(CallFrame* callFrame, JSObject* baseObject, JSValue subscript, JSValue value, ByValInfo* byValInfo)
 651 {
 652     VM&amp; vm = callFrame-&gt;vm();
 653     auto scope = DECLARE_THROW_SCOPE(vm);
 654     bool isStrictMode = callFrame-&gt;codeBlock()-&gt;isStrictMode();
 655 
 656     if (LIKELY(subscript.isUInt32())) {
 657         // Despite its name, JSValue::isUInt32 will return true only for positive boxed int32_t; all those values are valid array indices.
 658         byValInfo-&gt;tookSlowPath = true;
 659         uint32_t index = subscript.asUInt32();
 660         ASSERT(isIndex(index));
 661 
 662         switch (baseObject-&gt;indexingType()) {
 663         case ALL_INT32_INDEXING_TYPES:
 664         case ALL_DOUBLE_INDEXING_TYPES:
 665         case ALL_CONTIGUOUS_INDEXING_TYPES:
 666         case ALL_ARRAY_STORAGE_INDEXING_TYPES:
 667             if (index &lt; baseObject-&gt;butterfly()-&gt;vectorLength())
 668                 break;
 669             FALLTHROUGH;
 670         default:
 671             byValInfo-&gt;arrayProfile-&gt;setOutOfBounds();
 672             break;
 673         }
 674 
 675         scope.release();
 676         baseObject-&gt;putDirectIndex(callFrame, index, value, 0, isStrictMode ? PutDirectIndexShouldThrow : PutDirectIndexShouldNotThrow);
 677         return;
 678     }
 679 
 680     if (subscript.isDouble()) {
 681         double subscriptAsDouble = subscript.asDouble();
 682         uint32_t subscriptAsUInt32 = static_cast&lt;uint32_t&gt;(subscriptAsDouble);
 683         if (subscriptAsDouble == subscriptAsUInt32 &amp;&amp; isIndex(subscriptAsUInt32)) {
 684             byValInfo-&gt;tookSlowPath = true;
 685             scope.release();
 686             baseObject-&gt;putDirectIndex(callFrame, subscriptAsUInt32, value, 0, isStrictMode ? PutDirectIndexShouldThrow : PutDirectIndexShouldNotThrow);
 687             return;
 688         }
 689     }
 690 
 691     // Don&#39;t put to an object if toString threw an exception.
 692     auto property = subscript.toPropertyKey(callFrame);
 693     RETURN_IF_EXCEPTION(scope, void());
 694 
 695     if (Optional&lt;uint32_t&gt; index = parseIndex(property)) {
 696         byValInfo-&gt;tookSlowPath = true;
 697         scope.release();
 698         baseObject-&gt;putDirectIndex(callFrame, index.value(), value, 0, isStrictMode ? PutDirectIndexShouldThrow : PutDirectIndexShouldNotThrow);
 699         return;
 700     }
 701 
 702     if (byValInfo-&gt;stubInfo &amp;&amp; (!isStringOrSymbol(subscript) || byValInfo-&gt;cachedId != property))
 703         byValInfo-&gt;tookSlowPath = true;
 704 
 705     scope.release();
 706     PutPropertySlot slot(baseObject, isStrictMode);
 707     CommonSlowPaths::putDirectWithReify(vm, callFrame, baseObject, property, value, slot);
 708 }
 709 
 710 enum class OptimizationResult {
 711     NotOptimized,
 712     SeenOnce,
 713     Optimized,
 714     GiveUp,
 715 };
 716 
 717 static OptimizationResult tryPutByValOptimize(ExecState* exec, JSValue baseValue, JSValue subscript, ByValInfo* byValInfo, ReturnAddressPtr returnAddress)
 718 {
 719     // See if it&#39;s worth optimizing at all.
 720     OptimizationResult optimizationResult = OptimizationResult::NotOptimized;
 721 
 722     VM&amp; vm = exec-&gt;vm();
 723 
 724     if (baseValue.isObject() &amp;&amp; isCopyOnWrite(baseValue.getObject()-&gt;indexingMode()))
 725         return OptimizationResult::GiveUp;
 726 
 727     if (baseValue.isObject() &amp;&amp; subscript.isInt32()) {
 728         JSObject* object = asObject(baseValue);
 729 
 730         ASSERT(exec-&gt;bytecodeOffset());
 731         ASSERT(!byValInfo-&gt;stubRoutine);
 732 
 733         Structure* structure = object-&gt;structure(vm);
 734         if (hasOptimizableIndexing(structure)) {
 735             // Attempt to optimize.
 736             JITArrayMode arrayMode = jitArrayModeForStructure(structure);
 737             if (jitArrayModePermitsPut(arrayMode) &amp;&amp; arrayMode != byValInfo-&gt;arrayMode) {
 738                 CodeBlock* codeBlock = exec-&gt;codeBlock();
 739                 ConcurrentJSLocker locker(codeBlock-&gt;m_lock);
 740                 byValInfo-&gt;arrayProfile-&gt;computeUpdatedPrediction(locker, codeBlock, structure);
 741                 JIT::compilePutByVal(locker, &amp;vm, codeBlock, byValInfo, returnAddress, arrayMode);
 742                 optimizationResult = OptimizationResult::Optimized;
 743             }
 744         }
 745 
 746         // If we failed to patch and we have some object that intercepts indexed get, then don&#39;t even wait until 10 times.
 747         if (optimizationResult != OptimizationResult::Optimized &amp;&amp; object-&gt;structure(vm)-&gt;typeInfo().interceptsGetOwnPropertySlotByIndexEvenWhenLengthIsNotZero())
 748             optimizationResult = OptimizationResult::GiveUp;
 749     }
 750 
 751     if (baseValue.isObject() &amp;&amp; isStringOrSymbol(subscript)) {
 752         const Identifier propertyName = subscript.toPropertyKey(exec);
 753         if (subscript.isSymbol() || !parseIndex(propertyName)) {
 754             ASSERT(exec-&gt;bytecodeOffset());
 755             ASSERT(!byValInfo-&gt;stubRoutine);
 756             if (byValInfo-&gt;seen) {
 757                 if (byValInfo-&gt;cachedId == propertyName) {
 758                     JIT::compilePutByValWithCachedId&lt;OpPutByVal&gt;(&amp;vm, exec-&gt;codeBlock(), byValInfo, returnAddress, NotDirect, propertyName);
 759                     optimizationResult = OptimizationResult::Optimized;
 760                 } else {
 761                     // Seem like a generic property access site.
 762                     optimizationResult = OptimizationResult::GiveUp;
 763                 }
 764             } else {
 765                 CodeBlock* codeBlock = exec-&gt;codeBlock();
 766                 ConcurrentJSLocker locker(codeBlock-&gt;m_lock);
 767                 byValInfo-&gt;seen = true;
 768                 byValInfo-&gt;cachedId = propertyName;
 769                 if (subscript.isSymbol())
 770                     byValInfo-&gt;cachedSymbol.set(vm, codeBlock, asSymbol(subscript));
 771                 optimizationResult = OptimizationResult::SeenOnce;
 772             }
 773         }
 774     }
 775 
 776     if (optimizationResult != OptimizationResult::Optimized &amp;&amp; optimizationResult != OptimizationResult::SeenOnce) {
 777         // If we take slow path more than 10 times without patching then make sure we
 778         // never make that mistake again. For cases where we see non-index-intercepting
 779         // objects, this gives 10 iterations worth of opportunity for us to observe
 780         // that the put_by_val may be polymorphic. We count up slowPathCount even if
 781         // the result is GiveUp.
 782         if (++byValInfo-&gt;slowPathCount &gt;= 10)
 783             optimizationResult = OptimizationResult::GiveUp;
 784     }
 785 
 786     return optimizationResult;
 787 }
 788 
 789 void JIT_OPERATION operationPutByValOptimize(ExecState* exec, EncodedJSValue encodedBaseValue, EncodedJSValue encodedSubscript, EncodedJSValue encodedValue, ByValInfo* byValInfo)
 790 {
 791     VM&amp; vm = exec-&gt;vm();
 792     NativeCallFrameTracer tracer(&amp;vm, exec);
 793 
 794     JSValue baseValue = JSValue::decode(encodedBaseValue);
 795     JSValue subscript = JSValue::decode(encodedSubscript);
 796     JSValue value = JSValue::decode(encodedValue);
 797     if (tryPutByValOptimize(exec, baseValue, subscript, byValInfo, ReturnAddressPtr(OUR_RETURN_ADDRESS)) == OptimizationResult::GiveUp) {
 798         // Don&#39;t ever try to optimize.
 799         byValInfo-&gt;tookSlowPath = true;
 800         ctiPatchCallByReturnAddress(ReturnAddressPtr(OUR_RETURN_ADDRESS), operationPutByValGeneric);
 801     }
 802     putByVal(exec, baseValue, subscript, value, byValInfo);
 803 }
 804 
 805 static OptimizationResult tryDirectPutByValOptimize(ExecState* exec, JSObject* object, JSValue subscript, ByValInfo* byValInfo, ReturnAddressPtr returnAddress)
 806 {
 807     // See if it&#39;s worth optimizing at all.
 808     OptimizationResult optimizationResult = OptimizationResult::NotOptimized;
 809 
 810     VM&amp; vm = exec-&gt;vm();
 811 
 812     if (subscript.isInt32()) {
 813         ASSERT(exec-&gt;bytecodeOffset());
 814         ASSERT(!byValInfo-&gt;stubRoutine);
 815 
 816         Structure* structure = object-&gt;structure(vm);
 817         if (hasOptimizableIndexing(structure)) {
 818             // Attempt to optimize.
 819             JITArrayMode arrayMode = jitArrayModeForStructure(structure);
 820             if (jitArrayModePermitsPutDirect(arrayMode) &amp;&amp; arrayMode != byValInfo-&gt;arrayMode) {
 821                 CodeBlock* codeBlock = exec-&gt;codeBlock();
 822                 ConcurrentJSLocker locker(codeBlock-&gt;m_lock);
 823                 byValInfo-&gt;arrayProfile-&gt;computeUpdatedPrediction(locker, codeBlock, structure);
 824 
 825                 JIT::compileDirectPutByVal(locker, &amp;vm, codeBlock, byValInfo, returnAddress, arrayMode);
 826                 optimizationResult = OptimizationResult::Optimized;
 827             }
 828         }
 829 
 830         // If we failed to patch and we have some object that intercepts indexed get, then don&#39;t even wait until 10 times.
 831         if (optimizationResult != OptimizationResult::Optimized &amp;&amp; object-&gt;structure(vm)-&gt;typeInfo().interceptsGetOwnPropertySlotByIndexEvenWhenLengthIsNotZero())
 832             optimizationResult = OptimizationResult::GiveUp;
 833     } else if (isStringOrSymbol(subscript)) {
 834         const Identifier propertyName = subscript.toPropertyKey(exec);
 835         if (subscript.isSymbol() || !parseIndex(propertyName)) {
 836             ASSERT(exec-&gt;bytecodeOffset());
 837             ASSERT(!byValInfo-&gt;stubRoutine);
 838             if (byValInfo-&gt;seen) {
 839                 if (byValInfo-&gt;cachedId == propertyName) {
 840                     JIT::compilePutByValWithCachedId&lt;OpPutByValDirect&gt;(&amp;vm, exec-&gt;codeBlock(), byValInfo, returnAddress, Direct, propertyName);
 841                     optimizationResult = OptimizationResult::Optimized;
 842                 } else {
 843                     // Seem like a generic property access site.
 844                     optimizationResult = OptimizationResult::GiveUp;
 845                 }
 846             } else {
 847                 CodeBlock* codeBlock = exec-&gt;codeBlock();
 848                 ConcurrentJSLocker locker(codeBlock-&gt;m_lock);
 849                 byValInfo-&gt;seen = true;
 850                 byValInfo-&gt;cachedId = propertyName;
 851                 if (subscript.isSymbol())
 852                     byValInfo-&gt;cachedSymbol.set(vm, codeBlock, asSymbol(subscript));
 853                 optimizationResult = OptimizationResult::SeenOnce;
 854             }
 855         }
 856     }
 857 
 858     if (optimizationResult != OptimizationResult::Optimized &amp;&amp; optimizationResult != OptimizationResult::SeenOnce) {
 859         // If we take slow path more than 10 times without patching then make sure we
 860         // never make that mistake again. For cases where we see non-index-intercepting
 861         // objects, this gives 10 iterations worth of opportunity for us to observe
 862         // that the get_by_val may be polymorphic. We count up slowPathCount even if
 863         // the result is GiveUp.
 864         if (++byValInfo-&gt;slowPathCount &gt;= 10)
 865             optimizationResult = OptimizationResult::GiveUp;
 866     }
 867 
 868     return optimizationResult;
 869 }
 870 
 871 void JIT_OPERATION operationDirectPutByValOptimize(ExecState* exec, EncodedJSValue encodedBaseValue, EncodedJSValue encodedSubscript, EncodedJSValue encodedValue, ByValInfo* byValInfo)
 872 {
 873     VM&amp; vm = exec-&gt;vm();
 874     NativeCallFrameTracer tracer(&amp;vm, exec);
 875 
 876     JSValue baseValue = JSValue::decode(encodedBaseValue);
 877     JSValue subscript = JSValue::decode(encodedSubscript);
 878     JSValue value = JSValue::decode(encodedValue);
 879     RELEASE_ASSERT(baseValue.isObject());
 880     JSObject* object = asObject(baseValue);
 881     if (tryDirectPutByValOptimize(exec, object, subscript, byValInfo, ReturnAddressPtr(OUR_RETURN_ADDRESS)) == OptimizationResult::GiveUp) {
 882         // Don&#39;t ever try to optimize.
 883         byValInfo-&gt;tookSlowPath = true;
 884         ctiPatchCallByReturnAddress(ReturnAddressPtr(OUR_RETURN_ADDRESS), operationDirectPutByValGeneric);
 885     }
 886 
 887     directPutByVal(exec, object, subscript, value, byValInfo);
 888 }
 889 
 890 void JIT_OPERATION operationPutByValGeneric(ExecState* exec, EncodedJSValue encodedBaseValue, EncodedJSValue encodedSubscript, EncodedJSValue encodedValue, ByValInfo* byValInfo)
 891 {
 892     VM&amp; vm = exec-&gt;vm();
 893     NativeCallFrameTracer tracer(&amp;vm, exec);
 894 
 895     JSValue baseValue = JSValue::decode(encodedBaseValue);
 896     JSValue subscript = JSValue::decode(encodedSubscript);
 897     JSValue value = JSValue::decode(encodedValue);
 898 
 899     putByVal(exec, baseValue, subscript, value, byValInfo);
 900 }
 901 
 902 
 903 void JIT_OPERATION operationDirectPutByValGeneric(ExecState* exec, EncodedJSValue encodedBaseValue, EncodedJSValue encodedSubscript, EncodedJSValue encodedValue, ByValInfo* byValInfo)
 904 {
 905     VM&amp; vm = exec-&gt;vm();
 906     NativeCallFrameTracer tracer(&amp;vm, exec);
 907 
 908     JSValue baseValue = JSValue::decode(encodedBaseValue);
 909     JSValue subscript = JSValue::decode(encodedSubscript);
 910     JSValue value = JSValue::decode(encodedValue);
 911     RELEASE_ASSERT(baseValue.isObject());
 912     directPutByVal(exec, asObject(baseValue), subscript, value, byValInfo);
 913 }
 914 
 915 EncodedJSValue JIT_OPERATION operationCallEval(ExecState* exec, ExecState* execCallee)
 916 {
 917     VM* vm = &amp;exec-&gt;vm();
 918     auto scope = DECLARE_THROW_SCOPE(*vm);
 919 
 920     execCallee-&gt;setCodeBlock(0);
 921 
 922     if (!isHostFunction(execCallee-&gt;guaranteedJSValueCallee(), globalFuncEval))
 923         return JSValue::encode(JSValue());
 924 
 925     JSValue result = eval(execCallee);
 926     RETURN_IF_EXCEPTION(scope, encodedJSValue());
 927 
 928     return JSValue::encode(result);
 929 }
 930 
 931 static SlowPathReturnType handleHostCall(ExecState* execCallee, JSValue callee, CallLinkInfo* callLinkInfo)
 932 {
 933     ExecState* exec = execCallee-&gt;callerFrame();
 934     VM* vm = &amp;exec-&gt;vm();
 935     auto scope = DECLARE_THROW_SCOPE(*vm);
 936 
 937     execCallee-&gt;setCodeBlock(0);
 938 
 939     if (callLinkInfo-&gt;specializationKind() == CodeForCall) {
 940         CallData callData;
 941         CallType callType = getCallData(*vm, callee, callData);
 942 
 943         ASSERT(callType != CallType::JS);
 944 
 945         if (callType == CallType::Host) {
 946             NativeCallFrameTracer tracer(vm, execCallee);
 947             execCallee-&gt;setCallee(asObject(callee));
 948             vm-&gt;hostCallReturnValue = JSValue::decode(callData.native.function(execCallee));
 949             if (UNLIKELY(scope.exception())) {
 950                 return encodeResult(
 951                     vm-&gt;getCTIStub(throwExceptionFromCallSlowPathGenerator).retaggedCode&lt;JSEntryPtrTag&gt;().executableAddress(),
 952                     reinterpret_cast&lt;void*&gt;(KeepTheFrame));
 953             }
 954 
 955             return encodeResult(
 956                 tagCFunctionPtr&lt;void*, JSEntryPtrTag&gt;(getHostCallReturnValue),
 957                 reinterpret_cast&lt;void*&gt;(callLinkInfo-&gt;callMode() == CallMode::Tail ? ReuseTheFrame : KeepTheFrame));
 958         }
 959 
 960         ASSERT(callType == CallType::None);
 961         throwException(exec, scope, createNotAFunctionError(exec, callee));
 962         return encodeResult(
 963             vm-&gt;getCTIStub(throwExceptionFromCallSlowPathGenerator).retaggedCode&lt;JSEntryPtrTag&gt;().executableAddress(),
 964             reinterpret_cast&lt;void*&gt;(KeepTheFrame));
 965     }
 966 
 967     ASSERT(callLinkInfo-&gt;specializationKind() == CodeForConstruct);
 968 
 969     ConstructData constructData;
 970     ConstructType constructType = getConstructData(*vm, callee, constructData);
 971 
 972     ASSERT(constructType != ConstructType::JS);
 973 
 974     if (constructType == ConstructType::Host) {
 975         NativeCallFrameTracer tracer(vm, execCallee);
 976         execCallee-&gt;setCallee(asObject(callee));
 977         vm-&gt;hostCallReturnValue = JSValue::decode(constructData.native.function(execCallee));
 978         if (UNLIKELY(scope.exception())) {
 979             return encodeResult(
 980                 vm-&gt;getCTIStub(throwExceptionFromCallSlowPathGenerator).retaggedCode&lt;JSEntryPtrTag&gt;().executableAddress(),
 981                 reinterpret_cast&lt;void*&gt;(KeepTheFrame));
 982         }
 983 
 984         return encodeResult(tagCFunctionPtr&lt;void*, JSEntryPtrTag&gt;(getHostCallReturnValue), reinterpret_cast&lt;void*&gt;(KeepTheFrame));
 985     }
 986 
 987     ASSERT(constructType == ConstructType::None);
 988     throwException(exec, scope, createNotAConstructorError(exec, callee));
 989     return encodeResult(
 990         vm-&gt;getCTIStub(throwExceptionFromCallSlowPathGenerator).retaggedCode&lt;JSEntryPtrTag&gt;().executableAddress(),
 991         reinterpret_cast&lt;void*&gt;(KeepTheFrame));
 992 }
 993 
 994 SlowPathReturnType JIT_OPERATION operationLinkCall(ExecState* execCallee, CallLinkInfo* callLinkInfo)
 995 {
 996     ExecState* exec = execCallee-&gt;callerFrame();
 997     VM* vm = &amp;exec-&gt;vm();
 998     auto throwScope = DECLARE_THROW_SCOPE(*vm);
 999 
1000     CodeSpecializationKind kind = callLinkInfo-&gt;specializationKind();
1001     NativeCallFrameTracer tracer(vm, exec);
1002 
1003     RELEASE_ASSERT(!callLinkInfo-&gt;isDirect());
1004 
1005     JSValue calleeAsValue = execCallee-&gt;guaranteedJSValueCallee();
1006     JSCell* calleeAsFunctionCell = getJSFunction(calleeAsValue);
1007     if (!calleeAsFunctionCell) {
1008         if (auto* internalFunction = jsDynamicCast&lt;InternalFunction*&gt;(*vm, calleeAsValue)) {
1009             MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr = vm-&gt;getCTIInternalFunctionTrampolineFor(kind);
1010             RELEASE_ASSERT(!!codePtr);
1011 
1012             if (!callLinkInfo-&gt;seenOnce())
1013                 callLinkInfo-&gt;setSeen();
1014             else
1015                 linkFor(execCallee, *callLinkInfo, nullptr, internalFunction, codePtr);
1016 
1017             void* linkedTarget = codePtr.executableAddress();
1018             return encodeResult(linkedTarget, reinterpret_cast&lt;void*&gt;(callLinkInfo-&gt;callMode() == CallMode::Tail ? ReuseTheFrame : KeepTheFrame));
1019         }
1020         RELEASE_AND_RETURN(throwScope, handleHostCall(execCallee, calleeAsValue, callLinkInfo));
1021     }
1022 
1023     JSFunction* callee = jsCast&lt;JSFunction*&gt;(calleeAsFunctionCell);
1024     JSScope* scope = callee-&gt;scopeUnchecked();
1025     ExecutableBase* executable = callee-&gt;executable();
1026 
1027     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr;
1028     CodeBlock* codeBlock = nullptr;
1029     if (executable-&gt;isHostFunction())
1030         codePtr = executable-&gt;entrypointFor(kind, MustCheckArity);
1031     else {
1032         FunctionExecutable* functionExecutable = static_cast&lt;FunctionExecutable*&gt;(executable);
1033 
1034         auto handleThrowException = [&amp;] () {
1035             void* throwTarget = vm-&gt;getCTIStub(throwExceptionFromCallSlowPathGenerator).retaggedCode&lt;JSEntryPtrTag&gt;().executableAddress();
1036             return encodeResult(throwTarget, reinterpret_cast&lt;void*&gt;(KeepTheFrame));
1037         };
1038 
1039         if (!isCall(kind) &amp;&amp; functionExecutable-&gt;constructAbility() == ConstructAbility::CannotConstruct) {
1040             throwException(exec, throwScope, createNotAConstructorError(exec, callee));
1041             return handleThrowException();
1042         }
1043 
1044         CodeBlock** codeBlockSlot = execCallee-&gt;addressOfCodeBlock();
1045         JSObject* error = functionExecutable-&gt;prepareForExecution&lt;FunctionExecutable&gt;(*vm, callee, scope, kind, *codeBlockSlot);
1046         EXCEPTION_ASSERT(throwScope.exception() == reinterpret_cast&lt;Exception*&gt;(error));
1047         if (error)
1048             return handleThrowException();
1049         codeBlock = *codeBlockSlot;
1050         ArityCheckMode arity;
1051         if (execCallee-&gt;argumentCountIncludingThis() &lt; static_cast&lt;size_t&gt;(codeBlock-&gt;numParameters()) || callLinkInfo-&gt;isVarargs())
1052             arity = MustCheckArity;
1053         else
1054             arity = ArityCheckNotRequired;
1055         codePtr = functionExecutable-&gt;entrypointFor(kind, arity);
1056     }
1057     if (!callLinkInfo-&gt;seenOnce())
1058         callLinkInfo-&gt;setSeen();
1059     else
1060         linkFor(execCallee, *callLinkInfo, codeBlock, callee, codePtr);
1061 
1062     return encodeResult(codePtr.executableAddress(), reinterpret_cast&lt;void*&gt;(callLinkInfo-&gt;callMode() == CallMode::Tail ? ReuseTheFrame : KeepTheFrame));
1063 }
1064 
1065 void JIT_OPERATION operationLinkDirectCall(ExecState* exec, CallLinkInfo* callLinkInfo, JSFunction* callee)
1066 {
1067     VM* vm = &amp;exec-&gt;vm();
1068     auto throwScope = DECLARE_THROW_SCOPE(*vm);
1069 
1070     CodeSpecializationKind kind = callLinkInfo-&gt;specializationKind();
1071     NativeCallFrameTracer tracer(vm, exec);
1072 
1073     RELEASE_ASSERT(callLinkInfo-&gt;isDirect());
1074 
1075     // This would happen if the executable died during GC but the CodeBlock did not die. That should
1076     // not happen because the CodeBlock should have a weak reference to any executable it uses for
1077     // this purpose.
1078     RELEASE_ASSERT(callLinkInfo-&gt;executable());
1079 
1080     // Having a CodeBlock indicates that this is linked. We shouldn&#39;t be taking this path if it&#39;s
1081     // linked.
1082     RELEASE_ASSERT(!callLinkInfo-&gt;codeBlock());
1083 
1084     // We just don&#39;t support this yet.
1085     RELEASE_ASSERT(!callLinkInfo-&gt;isVarargs());
1086 
1087     ExecutableBase* executable = callLinkInfo-&gt;executable();
1088     RELEASE_ASSERT(callee-&gt;executable() == callLinkInfo-&gt;executable());
1089 
1090     JSScope* scope = callee-&gt;scopeUnchecked();
1091 
1092     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr;
1093     CodeBlock* codeBlock = nullptr;
1094     if (executable-&gt;isHostFunction())
1095         codePtr = executable-&gt;entrypointFor(kind, MustCheckArity);
1096     else {
1097         FunctionExecutable* functionExecutable = static_cast&lt;FunctionExecutable*&gt;(executable);
1098 
1099         RELEASE_ASSERT(isCall(kind) || functionExecutable-&gt;constructAbility() != ConstructAbility::CannotConstruct);
1100 
1101         JSObject* error = functionExecutable-&gt;prepareForExecution&lt;FunctionExecutable&gt;(*vm, callee, scope, kind, codeBlock);
1102         EXCEPTION_ASSERT_UNUSED(throwScope, throwScope.exception() == reinterpret_cast&lt;Exception*&gt;(error));
1103         if (error)
1104             return;
1105         unsigned argumentStackSlots = callLinkInfo-&gt;maxNumArguments();
1106         if (argumentStackSlots &lt; static_cast&lt;size_t&gt;(codeBlock-&gt;numParameters()))
1107             codePtr = functionExecutable-&gt;entrypointFor(kind, MustCheckArity);
1108         else
1109             codePtr = functionExecutable-&gt;entrypointFor(kind, ArityCheckNotRequired);
1110     }
1111 
1112     linkDirectFor(exec, *callLinkInfo, codeBlock, codePtr);
1113 }
1114 
1115 inline SlowPathReturnType virtualForWithFunction(
1116     ExecState* execCallee, CallLinkInfo* callLinkInfo, JSCell*&amp; calleeAsFunctionCell)
1117 {
1118     ExecState* exec = execCallee-&gt;callerFrame();
1119     VM* vm = &amp;exec-&gt;vm();
1120     auto throwScope = DECLARE_THROW_SCOPE(*vm);
1121 
1122     CodeSpecializationKind kind = callLinkInfo-&gt;specializationKind();
1123     NativeCallFrameTracer tracer(vm, exec);
1124 
1125     JSValue calleeAsValue = execCallee-&gt;guaranteedJSValueCallee();
1126     calleeAsFunctionCell = getJSFunction(calleeAsValue);
1127     if (UNLIKELY(!calleeAsFunctionCell)) {
1128         if (jsDynamicCast&lt;InternalFunction*&gt;(*vm, calleeAsValue)) {
1129             MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; codePtr = vm-&gt;getCTIInternalFunctionTrampolineFor(kind);
1130             ASSERT(!!codePtr);
1131             return encodeResult(codePtr.executableAddress(), reinterpret_cast&lt;void*&gt;(callLinkInfo-&gt;callMode() == CallMode::Tail ? ReuseTheFrame : KeepTheFrame));
1132         }
1133         RELEASE_AND_RETURN(throwScope, handleHostCall(execCallee, calleeAsValue, callLinkInfo));
1134     }
1135 
1136     JSFunction* function = jsCast&lt;JSFunction*&gt;(calleeAsFunctionCell);
1137     JSScope* scope = function-&gt;scopeUnchecked();
1138     ExecutableBase* executable = function-&gt;executable();
1139     if (UNLIKELY(!executable-&gt;hasJITCodeFor(kind))) {
1140         FunctionExecutable* functionExecutable = static_cast&lt;FunctionExecutable*&gt;(executable);
1141 
1142         if (!isCall(kind) &amp;&amp; functionExecutable-&gt;constructAbility() == ConstructAbility::CannotConstruct) {
1143             throwException(exec, throwScope, createNotAConstructorError(exec, function));
1144             return encodeResult(
1145                 vm-&gt;getCTIStub(throwExceptionFromCallSlowPathGenerator).retaggedCode&lt;JSEntryPtrTag&gt;().executableAddress(),
1146                 reinterpret_cast&lt;void*&gt;(KeepTheFrame));
1147         }
1148 
1149         CodeBlock** codeBlockSlot = execCallee-&gt;addressOfCodeBlock();
1150         JSObject* error = functionExecutable-&gt;prepareForExecution&lt;FunctionExecutable&gt;(*vm, function, scope, kind, *codeBlockSlot);
1151         EXCEPTION_ASSERT(throwScope.exception() == reinterpret_cast&lt;Exception*&gt;(error));
1152         if (error) {
1153             return encodeResult(
1154                 vm-&gt;getCTIStub(throwExceptionFromCallSlowPathGenerator).retaggedCode&lt;JSEntryPtrTag&gt;().executableAddress(),
1155                 reinterpret_cast&lt;void*&gt;(KeepTheFrame));
1156         }
1157     }
1158     return encodeResult(executable-&gt;entrypointFor(
1159         kind, MustCheckArity).executableAddress(),
1160         reinterpret_cast&lt;void*&gt;(callLinkInfo-&gt;callMode() == CallMode::Tail ? ReuseTheFrame : KeepTheFrame));
1161 }
1162 
1163 SlowPathReturnType JIT_OPERATION operationLinkPolymorphicCall(ExecState* execCallee, CallLinkInfo* callLinkInfo)
1164 {
1165     ASSERT(callLinkInfo-&gt;specializationKind() == CodeForCall);
1166     JSCell* calleeAsFunctionCell;
1167     SlowPathReturnType result = virtualForWithFunction(execCallee, callLinkInfo, calleeAsFunctionCell);
1168 
1169     linkPolymorphicCall(execCallee, *callLinkInfo, CallVariant(calleeAsFunctionCell));
1170 
1171     return result;
1172 }
1173 
1174 SlowPathReturnType JIT_OPERATION operationVirtualCall(ExecState* execCallee, CallLinkInfo* callLinkInfo)
1175 {
1176     JSCell* calleeAsFunctionCellIgnored;
1177     return virtualForWithFunction(execCallee, callLinkInfo, calleeAsFunctionCellIgnored);
1178 }
1179 
1180 size_t JIT_OPERATION operationCompareLess(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2)
1181 {
1182     VM* vm = &amp;exec-&gt;vm();
1183     NativeCallFrameTracer tracer(vm, exec);
1184 
1185     return jsLess&lt;true&gt;(exec, JSValue::decode(encodedOp1), JSValue::decode(encodedOp2));
1186 }
1187 
1188 size_t JIT_OPERATION operationCompareLessEq(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2)
1189 {
1190     VM* vm = &amp;exec-&gt;vm();
1191     NativeCallFrameTracer tracer(vm, exec);
1192 
1193     return jsLessEq&lt;true&gt;(exec, JSValue::decode(encodedOp1), JSValue::decode(encodedOp2));
1194 }
1195 
1196 size_t JIT_OPERATION operationCompareGreater(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2)
1197 {
1198     VM* vm = &amp;exec-&gt;vm();
1199     NativeCallFrameTracer tracer(vm, exec);
1200 
1201     return jsLess&lt;false&gt;(exec, JSValue::decode(encodedOp2), JSValue::decode(encodedOp1));
1202 }
1203 
1204 size_t JIT_OPERATION operationCompareGreaterEq(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2)
1205 {
1206     VM* vm = &amp;exec-&gt;vm();
1207     NativeCallFrameTracer tracer(vm, exec);
1208 
1209     return jsLessEq&lt;false&gt;(exec, JSValue::decode(encodedOp2), JSValue::decode(encodedOp1));
1210 }
1211 
1212 size_t JIT_OPERATION operationCompareEq(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2)
1213 {
1214     VM* vm = &amp;exec-&gt;vm();
1215     NativeCallFrameTracer tracer(vm, exec);
1216 
1217     return JSValue::equalSlowCaseInline(exec, JSValue::decode(encodedOp1), JSValue::decode(encodedOp2));
1218 }
1219 
1220 #if USE(JSVALUE64)
1221 EncodedJSValue JIT_OPERATION operationCompareStringEq(ExecState* exec, JSCell* left, JSCell* right)
1222 #else
1223 size_t JIT_OPERATION operationCompareStringEq(ExecState* exec, JSCell* left, JSCell* right)
1224 #endif
1225 {
1226     VM* vm = &amp;exec-&gt;vm();
1227     NativeCallFrameTracer tracer(vm, exec);
1228 
1229     bool result = asString(left)-&gt;equal(exec, asString(right));
1230 #if USE(JSVALUE64)
1231     return JSValue::encode(jsBoolean(result));
1232 #else
1233     return result;
1234 #endif
1235 }
1236 
1237 size_t JIT_OPERATION operationCompareStrictEq(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2)
1238 {
1239     VM* vm = &amp;exec-&gt;vm();
1240     NativeCallFrameTracer tracer(vm, exec);
1241 
1242     JSValue src1 = JSValue::decode(encodedOp1);
1243     JSValue src2 = JSValue::decode(encodedOp2);
1244 
1245     return JSValue::strictEqual(exec, src1, src2);
1246 }
1247 
1248 EncodedJSValue JIT_OPERATION operationNewArrayWithProfile(ExecState* exec, ArrayAllocationProfile* profile, const JSValue* values, int size)
1249 {
1250     VM* vm = &amp;exec-&gt;vm();
1251     NativeCallFrameTracer tracer(vm, exec);
1252     return JSValue::encode(constructArrayNegativeIndexed(exec, profile, values, size));
1253 }
1254 
1255 EncodedJSValue JIT_OPERATION operationNewArrayWithSizeAndProfile(ExecState* exec, ArrayAllocationProfile* profile, EncodedJSValue size)
1256 {
1257     VM* vm = &amp;exec-&gt;vm();
1258     NativeCallFrameTracer tracer(vm, exec);
1259     JSValue sizeValue = JSValue::decode(size);
1260     return JSValue::encode(constructArrayWithSizeQuirk(exec, profile, exec-&gt;lexicalGlobalObject(), sizeValue));
1261 }
1262 
1263 }
1264 
1265 template&lt;typename FunctionType&gt;
1266 static EncodedJSValue operationNewFunctionCommon(ExecState* exec, JSScope* scope, JSCell* functionExecutable, bool isInvalidated)
1267 {
1268     VM&amp; vm = exec-&gt;vm();
1269     ASSERT(functionExecutable-&gt;inherits&lt;FunctionExecutable&gt;(vm));
1270     NativeCallFrameTracer tracer(&amp;vm, exec);
1271     if (isInvalidated)
1272         return JSValue::encode(FunctionType::createWithInvalidatedReallocationWatchpoint(vm, static_cast&lt;FunctionExecutable*&gt;(functionExecutable), scope));
1273     return JSValue::encode(FunctionType::create(vm, static_cast&lt;FunctionExecutable*&gt;(functionExecutable), scope));
1274 }
1275 
1276 extern &quot;C&quot; {
1277 
1278 EncodedJSValue JIT_OPERATION operationNewFunction(ExecState* exec, JSScope* scope, JSCell* functionExecutable)
1279 {
1280     return operationNewFunctionCommon&lt;JSFunction&gt;(exec, scope, functionExecutable, false);
1281 }
1282 
1283 EncodedJSValue JIT_OPERATION operationNewFunctionWithInvalidatedReallocationWatchpoint(ExecState* exec, JSScope* scope, JSCell* functionExecutable)
1284 {
1285     return operationNewFunctionCommon&lt;JSFunction&gt;(exec, scope, functionExecutable, true);
1286 }
1287 
1288 EncodedJSValue JIT_OPERATION operationNewGeneratorFunction(ExecState* exec, JSScope* scope, JSCell* functionExecutable)
1289 {
1290     return operationNewFunctionCommon&lt;JSGeneratorFunction&gt;(exec, scope, functionExecutable, false);
1291 }
1292 
1293 EncodedJSValue JIT_OPERATION operationNewGeneratorFunctionWithInvalidatedReallocationWatchpoint(ExecState* exec, JSScope* scope, JSCell* functionExecutable)
1294 {
1295     return operationNewFunctionCommon&lt;JSGeneratorFunction&gt;(exec, scope, functionExecutable, true);
1296 }
1297 
1298 EncodedJSValue JIT_OPERATION operationNewAsyncFunction(ExecState* exec, JSScope* scope, JSCell* functionExecutable)
1299 {
1300     return operationNewFunctionCommon&lt;JSAsyncFunction&gt;(exec, scope, functionExecutable, false);
1301 }
1302 
1303 EncodedJSValue JIT_OPERATION operationNewAsyncFunctionWithInvalidatedReallocationWatchpoint(ExecState* exec, JSScope* scope, JSCell* functionExecutable)
1304 {
1305     return operationNewFunctionCommon&lt;JSAsyncFunction&gt;(exec, scope, functionExecutable, true);
1306 }
1307 
1308 EncodedJSValue JIT_OPERATION operationNewAsyncGeneratorFunction(ExecState* exec, JSScope* scope, JSCell* functionExecutable)
1309 {
1310     return operationNewFunctionCommon&lt;JSAsyncGeneratorFunction&gt;(exec, scope, functionExecutable, false);
1311 }
1312 
1313 EncodedJSValue JIT_OPERATION operationNewAsyncGeneratorFunctionWithInvalidatedReallocationWatchpoint(ExecState* exec, JSScope* scope, JSCell* functionExecutable)
1314 {
1315     return operationNewFunctionCommon&lt;JSAsyncGeneratorFunction&gt;(exec, scope, functionExecutable, true);
1316 }
1317 
1318 void JIT_OPERATION operationSetFunctionName(ExecState* exec, JSCell* funcCell, EncodedJSValue encodedName)
1319 {
1320     VM* vm = &amp;exec-&gt;vm();
1321     NativeCallFrameTracer tracer(vm, exec);
1322 
1323     JSFunction* func = jsCast&lt;JSFunction*&gt;(funcCell);
1324     JSValue name = JSValue::decode(encodedName);
1325     func-&gt;setFunctionName(exec, name);
1326 }
1327 
1328 JSCell* JIT_OPERATION operationNewObject(ExecState* exec, Structure* structure)
1329 {
1330     VM* vm = &amp;exec-&gt;vm();
1331     NativeCallFrameTracer tracer(vm, exec);
1332 
1333     return constructEmptyObject(exec, structure);
1334 }
1335 
1336 JSCell* JIT_OPERATION operationNewRegexp(ExecState* exec, JSCell* regexpPtr)
1337 {
1338     SuperSamplerScope superSamplerScope(false);
1339     VM&amp; vm = exec-&gt;vm();
1340     NativeCallFrameTracer tracer(&amp;vm, exec);
1341 
1342     RegExp* regexp = static_cast&lt;RegExp*&gt;(regexpPtr);
1343     ASSERT(regexp-&gt;isValid());
1344     return RegExpObject::create(vm, exec-&gt;lexicalGlobalObject()-&gt;regExpStructure(), regexp);
1345 }
1346 
1347 // The only reason for returning an UnusedPtr (instead of void) is so that we can reuse the
1348 // existing DFG slow path generator machinery when creating the slow path for CheckTraps
1349 // in the DFG. If a DFG slow path generator that supports a void return type is added in the
1350 // future, we can switch to using that then.
1351 UnusedPtr JIT_OPERATION operationHandleTraps(ExecState* exec)
1352 {
1353     VM&amp; vm = exec-&gt;vm();
1354     NativeCallFrameTracer tracer(&amp;vm, exec);
1355     ASSERT(vm.needTrapHandling());
1356     vm.handleTraps(exec);
1357     return nullptr;
1358 }
1359 
1360 void JIT_OPERATION operationDebug(ExecState* exec, int32_t debugHookType)
1361 {
1362     VM&amp; vm = exec-&gt;vm();
1363     NativeCallFrameTracer tracer(&amp;vm, exec);
1364 
1365     vm.interpreter-&gt;debug(exec, static_cast&lt;DebugHookType&gt;(debugHookType));
1366 }
1367 
1368 #if ENABLE(DFG_JIT)
1369 static void updateAllPredictionsAndOptimizeAfterWarmUp(CodeBlock* codeBlock)
1370 {
1371     codeBlock-&gt;updateAllPredictions();
1372     codeBlock-&gt;optimizeAfterWarmUp();
1373 }
1374 
1375 SlowPathReturnType JIT_OPERATION operationOptimize(ExecState* exec, uint32_t bytecodeIndex)
1376 {
1377     VM&amp; vm = exec-&gt;vm();
1378     NativeCallFrameTracer tracer(&amp;vm, exec);
1379 
1380     // Defer GC for a while so that it doesn&#39;t run between when we enter into this
1381     // slow path and when we figure out the state of our code block. This prevents
1382     // a number of awkward reentrancy scenarios, including:
1383     //
1384     // - The optimized version of our code block being jettisoned by GC right after
1385     //   we concluded that we wanted to use it, but have not planted it into the JS
1386     //   stack yet.
1387     //
1388     // - An optimized version of our code block being installed just as we decided
1389     //   that it wasn&#39;t ready yet.
1390     //
1391     // Note that jettisoning won&#39;t happen if we already initiated OSR, because in
1392     // that case we would have already planted the optimized code block into the JS
1393     // stack.
1394     DeferGCForAWhile deferGC(vm.heap);
1395 
1396     CodeBlock* codeBlock = exec-&gt;codeBlock();
1397     if (UNLIKELY(codeBlock-&gt;jitType() != JITCode::BaselineJIT)) {
1398         dataLog(&quot;Unexpected code block in Baseline-&gt;DFG tier-up: &quot;, *codeBlock, &quot;\n&quot;);
1399         RELEASE_ASSERT_NOT_REACHED();
1400     }
1401 
1402     if (bytecodeIndex) {
1403         // If we&#39;re attempting to OSR from a loop, assume that this should be
1404         // separately optimized.
1405         codeBlock-&gt;m_shouldAlwaysBeInlined = false;
1406     }
1407 
1408     if (UNLIKELY(Options::verboseOSR())) {
1409         dataLog(
1410             *codeBlock, &quot;: Entered optimize with bytecodeIndex = &quot;, bytecodeIndex,
1411             &quot;, executeCounter = &quot;, codeBlock-&gt;jitExecuteCounter(),
1412             &quot;, optimizationDelayCounter = &quot;, codeBlock-&gt;reoptimizationRetryCounter(),
1413             &quot;, exitCounter = &quot;);
1414         if (codeBlock-&gt;hasOptimizedReplacement())
1415             dataLog(codeBlock-&gt;replacement()-&gt;osrExitCounter());
1416         else
1417             dataLog(&quot;N/A&quot;);
1418         dataLog(&quot;\n&quot;);
1419     }
1420 
1421     if (!codeBlock-&gt;checkIfOptimizationThresholdReached()) {
1422         CODEBLOCK_LOG_EVENT(codeBlock, &quot;delayOptimizeToDFG&quot;, (&quot;counter = &quot;, codeBlock-&gt;jitExecuteCounter()));
1423         codeBlock-&gt;updateAllPredictions();
1424         if (UNLIKELY(Options::verboseOSR()))
1425             dataLog(&quot;Choosing not to optimize &quot;, *codeBlock, &quot; yet, because the threshold hasn&#39;t been reached.\n&quot;);
1426         return encodeResult(0, 0);
1427     }
1428 
1429     Debugger* debugger = codeBlock-&gt;globalObject()-&gt;debugger();
1430     if (UNLIKELY(debugger &amp;&amp; (debugger-&gt;isStepping() || codeBlock-&gt;baselineAlternative()-&gt;hasDebuggerRequests()))) {
1431         CODEBLOCK_LOG_EVENT(codeBlock, &quot;delayOptimizeToDFG&quot;, (&quot;debugger is stepping or has requests&quot;));
1432         updateAllPredictionsAndOptimizeAfterWarmUp(codeBlock);
1433         return encodeResult(0, 0);
1434     }
1435 
1436     if (codeBlock-&gt;m_shouldAlwaysBeInlined) {
1437         CODEBLOCK_LOG_EVENT(codeBlock, &quot;delayOptimizeToDFG&quot;, (&quot;should always be inlined&quot;));
1438         updateAllPredictionsAndOptimizeAfterWarmUp(codeBlock);
1439         if (UNLIKELY(Options::verboseOSR()))
1440             dataLog(&quot;Choosing not to optimize &quot;, *codeBlock, &quot; yet, because m_shouldAlwaysBeInlined == true.\n&quot;);
1441         return encodeResult(0, 0);
1442     }
1443 
1444     // We cannot be in the process of asynchronous compilation and also have an optimized
1445     // replacement.
1446     DFG::Worklist* worklist = DFG::existingGlobalDFGWorklistOrNull();
1447     ASSERT(
1448         !worklist
1449         || !(worklist-&gt;compilationState(DFG::CompilationKey(codeBlock, DFG::DFGMode)) != DFG::Worklist::NotKnown
1450         &amp;&amp; codeBlock-&gt;hasOptimizedReplacement()));
1451 
1452     DFG::Worklist::State worklistState;
1453     if (worklist) {
1454         // The call to DFG::Worklist::completeAllReadyPlansForVM() will complete all ready
1455         // (i.e. compiled) code blocks. But if it completes ours, we also need to know
1456         // what the result was so that we don&#39;t plow ahead and attempt OSR or immediate
1457         // reoptimization. This will have already also set the appropriate JIT execution
1458         // count threshold depending on what happened, so if the compilation was anything
1459         // but successful we just want to return early. See the case for worklistState ==
1460         // DFG::Worklist::Compiled, below.
1461 
1462         // Note that we could have alternatively just called Worklist::compilationState()
1463         // here, and if it returned Compiled, we could have then called
1464         // completeAndScheduleOSR() below. But that would have meant that it could take
1465         // longer for code blocks to be completed: they would only complete when *their*
1466         // execution count trigger fired; but that could take a while since the firing is
1467         // racy. It could also mean that code blocks that never run again after being
1468         // compiled would sit on the worklist until next GC. That&#39;s fine, but it&#39;s
1469         // probably a waste of memory. Our goal here is to complete code blocks as soon as
1470         // possible in order to minimize the chances of us executing baseline code after
1471         // optimized code is already available.
1472         worklistState = worklist-&gt;completeAllReadyPlansForVM(
1473             vm, DFG::CompilationKey(codeBlock, DFG::DFGMode));
1474     } else
1475         worklistState = DFG::Worklist::NotKnown;
1476 
1477     if (worklistState == DFG::Worklist::Compiling) {
1478         CODEBLOCK_LOG_EVENT(codeBlock, &quot;delayOptimizeToDFG&quot;, (&quot;compiling&quot;));
1479         // We cannot be in the process of asynchronous compilation and also have an optimized
1480         // replacement.
1481         RELEASE_ASSERT(!codeBlock-&gt;hasOptimizedReplacement());
1482         codeBlock-&gt;setOptimizationThresholdBasedOnCompilationResult(CompilationDeferred);
1483         return encodeResult(0, 0);
1484     }
1485 
1486     if (worklistState == DFG::Worklist::Compiled) {
1487         // If we don&#39;t have an optimized replacement but we did just get compiled, then
1488         // the compilation failed or was invalidated, in which case the execution count
1489         // thresholds have already been set appropriately by
1490         // CodeBlock::setOptimizationThresholdBasedOnCompilationResult() and we have
1491         // nothing left to do.
1492         if (!codeBlock-&gt;hasOptimizedReplacement()) {
1493             CODEBLOCK_LOG_EVENT(codeBlock, &quot;delayOptimizeToDFG&quot;, (&quot;compiled and failed&quot;));
1494             codeBlock-&gt;updateAllPredictions();
1495             if (UNLIKELY(Options::verboseOSR()))
1496                 dataLog(&quot;Code block &quot;, *codeBlock, &quot; was compiled but it doesn&#39;t have an optimized replacement.\n&quot;);
1497             return encodeResult(0, 0);
1498         }
1499     } else if (codeBlock-&gt;hasOptimizedReplacement()) {
1500         CodeBlock* replacement = codeBlock-&gt;replacement();
1501         if (UNLIKELY(Options::verboseOSR()))
1502             dataLog(&quot;Considering OSR &quot;, codeBlock, &quot; -&gt; &quot;, replacement, &quot;.\n&quot;);
1503         // If we have an optimized replacement, then it must be the case that we entered
1504         // cti_optimize from a loop. That&#39;s because if there&#39;s an optimized replacement,
1505         // then all calls to this function will be relinked to the replacement and so
1506         // the prologue OSR will never fire.
1507 
1508         // This is an interesting threshold check. Consider that a function OSR exits
1509         // in the middle of a loop, while having a relatively low exit count. The exit
1510         // will reset the execution counter to some target threshold, meaning that this
1511         // code won&#39;t be reached until that loop heats up for &gt;=1000 executions. But then
1512         // we do a second check here, to see if we should either reoptimize, or just
1513         // attempt OSR entry. Hence it might even be correct for
1514         // shouldReoptimizeFromLoopNow() to always return true. But we make it do some
1515         // additional checking anyway, to reduce the amount of recompilation thrashing.
1516         if (replacement-&gt;shouldReoptimizeFromLoopNow()) {
1517             CODEBLOCK_LOG_EVENT(codeBlock, &quot;delayOptimizeToDFG&quot;, (&quot;should reoptimize from loop now&quot;));
1518             if (UNLIKELY(Options::verboseOSR())) {
1519                 dataLog(
1520                     &quot;Triggering reoptimization of &quot;, codeBlock,
1521                     &quot;(&quot;, replacement, &quot;) (in loop).\n&quot;);
1522             }
1523             replacement-&gt;jettison(Profiler::JettisonDueToBaselineLoopReoptimizationTrigger, CountReoptimization);
1524             return encodeResult(0, 0);
1525         }
1526     } else {
1527         if (!codeBlock-&gt;shouldOptimizeNow()) {
1528             CODEBLOCK_LOG_EVENT(codeBlock, &quot;delayOptimizeToDFG&quot;, (&quot;insufficient profiling&quot;));
1529             if (UNLIKELY(Options::verboseOSR())) {
1530                 dataLog(
1531                     &quot;Delaying optimization for &quot;, *codeBlock,
1532                     &quot; because of insufficient profiling.\n&quot;);
1533             }
1534             return encodeResult(0, 0);
1535         }
1536 
1537         if (UNLIKELY(Options::verboseOSR()))
1538             dataLog(&quot;Triggering optimized compilation of &quot;, *codeBlock, &quot;\n&quot;);
1539 
1540         unsigned numVarsWithValues;
1541         if (bytecodeIndex)
1542             numVarsWithValues = codeBlock-&gt;numCalleeLocals();
1543         else
1544             numVarsWithValues = 0;
1545         Operands&lt;Optional&lt;JSValue&gt;&gt; mustHandleValues(codeBlock-&gt;numParameters(), numVarsWithValues);
1546         int localsUsedForCalleeSaves = static_cast&lt;int&gt;(CodeBlock::llintBaselineCalleeSaveSpaceAsVirtualRegisters());
1547         for (size_t i = 0; i &lt; mustHandleValues.size(); ++i) {
1548             int operand = mustHandleValues.operandForIndex(i);
1549             if (operandIsLocal(operand) &amp;&amp; VirtualRegister(operand).toLocal() &lt; localsUsedForCalleeSaves)
1550                 continue;
1551             mustHandleValues[i] = exec-&gt;uncheckedR(operand).jsValue();
1552         }
1553 
1554         CodeBlock* replacementCodeBlock = codeBlock-&gt;newReplacement();
1555         CompilationResult result = DFG::compile(
1556             vm, replacementCodeBlock, nullptr, DFG::DFGMode, bytecodeIndex,
1557             mustHandleValues, JITToDFGDeferredCompilationCallback::create());
1558 
1559         if (result != CompilationSuccessful) {
1560             CODEBLOCK_LOG_EVENT(codeBlock, &quot;delayOptimizeToDFG&quot;, (&quot;compilation failed&quot;));
1561             return encodeResult(0, 0);
1562         }
1563     }
1564 
1565     CodeBlock* optimizedCodeBlock = codeBlock-&gt;replacement();
1566     ASSERT(optimizedCodeBlock &amp;&amp; JITCode::isOptimizingJIT(optimizedCodeBlock-&gt;jitType()));
1567 
1568     if (void* dataBuffer = DFG::prepareOSREntry(exec, optimizedCodeBlock, bytecodeIndex)) {
1569         CODEBLOCK_LOG_EVENT(optimizedCodeBlock, &quot;osrEntry&quot;, (&quot;at bc#&quot;, bytecodeIndex));
1570         if (UNLIKELY(Options::verboseOSR())) {
1571             dataLog(
1572                 &quot;Performing OSR &quot;, codeBlock, &quot; -&gt; &quot;, optimizedCodeBlock, &quot;.\n&quot;);
1573         }
1574 
1575         codeBlock-&gt;optimizeSoon();
1576         codeBlock-&gt;unlinkedCodeBlock()-&gt;setDidOptimize(TrueTriState);
1577         void* targetPC = vm.getCTIStub(DFG::osrEntryThunkGenerator).code().executableAddress();
1578         targetPC = retagCodePtr(targetPC, JITThunkPtrTag, bitwise_cast&lt;PtrTag&gt;(exec));
1579         return encodeResult(targetPC, dataBuffer);
1580     }
1581 
1582     if (UNLIKELY(Options::verboseOSR())) {
1583         dataLog(
1584             &quot;Optimizing &quot;, codeBlock, &quot; -&gt; &quot;, codeBlock-&gt;replacement(),
1585             &quot; succeeded, OSR failed, after a delay of &quot;,
1586             codeBlock-&gt;optimizationDelayCounter(), &quot;.\n&quot;);
1587     }
1588 
1589     // Count the OSR failure as a speculation failure. If this happens a lot, then
1590     // reoptimize.
1591     optimizedCodeBlock-&gt;countOSRExit();
1592 
1593     // We are a lot more conservative about triggering reoptimization after OSR failure than
1594     // before it. If we enter the optimize_from_loop trigger with a bucket full of fail
1595     // already, then we really would like to reoptimize immediately. But this case covers
1596     // something else: there weren&#39;t many (or any) speculation failures before, but we just
1597     // failed to enter the speculative code because some variable had the wrong value or
1598     // because the OSR code decided for any spurious reason that it did not want to OSR
1599     // right now. So, we only trigger reoptimization only upon the more conservative (non-loop)
1600     // reoptimization trigger.
1601     if (optimizedCodeBlock-&gt;shouldReoptimizeNow()) {
1602         CODEBLOCK_LOG_EVENT(codeBlock, &quot;delayOptimizeToDFG&quot;, (&quot;should reoptimize now&quot;));
1603         if (UNLIKELY(Options::verboseOSR())) {
1604             dataLog(
1605                 &quot;Triggering reoptimization of &quot;, codeBlock, &quot; -&gt; &quot;,
1606                 codeBlock-&gt;replacement(), &quot; (after OSR fail).\n&quot;);
1607         }
1608         optimizedCodeBlock-&gt;jettison(Profiler::JettisonDueToBaselineLoopReoptimizationTriggerOnOSREntryFail, CountReoptimization);
1609         return encodeResult(0, 0);
1610     }
1611 
1612     // OSR failed this time, but it might succeed next time! Let the code run a bit
1613     // longer and then try again.
1614     codeBlock-&gt;optimizeAfterWarmUp();
1615 
1616     CODEBLOCK_LOG_EVENT(codeBlock, &quot;delayOptimizeToDFG&quot;, (&quot;OSR failed&quot;));
1617     return encodeResult(0, 0);
1618 }
1619 
1620 char* JIT_OPERATION operationTryOSREnterAtCatch(ExecState* exec, uint32_t bytecodeIndex)
1621 {
1622     VM&amp; vm = exec-&gt;vm();
1623     NativeCallFrameTracer tracer(&amp;vm, exec);
1624 
1625     CodeBlock* optimizedReplacement = exec-&gt;codeBlock()-&gt;replacement();
1626     if (UNLIKELY(!optimizedReplacement))
1627         return nullptr;
1628 
1629     switch (optimizedReplacement-&gt;jitType()) {
1630     case JITCode::DFGJIT:
1631     case JITCode::FTLJIT: {
1632         MacroAssemblerCodePtr&lt;ExceptionHandlerPtrTag&gt; entry = DFG::prepareCatchOSREntry(exec, optimizedReplacement, bytecodeIndex);
1633         return entry.executableAddress&lt;char*&gt;();
1634     }
1635     default:
1636         break;
1637     }
1638     return nullptr;
1639 }
1640 
1641 char* JIT_OPERATION operationTryOSREnterAtCatchAndValueProfile(ExecState* exec, uint32_t bytecodeIndex)
1642 {
1643     VM&amp; vm = exec-&gt;vm();
1644     NativeCallFrameTracer tracer(&amp;vm, exec);
1645 
1646     CodeBlock* codeBlock = exec-&gt;codeBlock();
1647     CodeBlock* optimizedReplacement = codeBlock-&gt;replacement();
1648     if (UNLIKELY(!optimizedReplacement))
1649         return nullptr;
1650 
1651     switch (optimizedReplacement-&gt;jitType()) {
1652     case JITCode::DFGJIT:
1653     case JITCode::FTLJIT: {
1654         MacroAssemblerCodePtr&lt;ExceptionHandlerPtrTag&gt; entry = DFG::prepareCatchOSREntry(exec, optimizedReplacement, bytecodeIndex);
1655         return entry.executableAddress&lt;char*&gt;();
1656     }
1657     default:
1658         break;
1659     }
1660 
1661     codeBlock-&gt;ensureCatchLivenessIsComputedForBytecodeOffset(bytecodeIndex);
1662     auto bytecode = codeBlock-&gt;instructions().at(bytecodeIndex)-&gt;as&lt;OpCatch&gt;();
1663     auto&amp; metadata = bytecode.metadata(codeBlock);
1664     metadata.m_buffer-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {
1665         profile.m_profile.m_buckets[0] = JSValue::encode(exec-&gt;uncheckedR(profile.m_operand).jsValue());
1666     });
1667 
1668     return nullptr;
1669 }
1670 
1671 #endif
1672 
1673 void JIT_OPERATION operationPutByIndex(ExecState* exec, EncodedJSValue encodedArrayValue, int32_t index, EncodedJSValue encodedValue)
1674 {
1675     VM&amp; vm = exec-&gt;vm();
1676     NativeCallFrameTracer tracer(&amp;vm, exec);
1677 
1678     JSValue arrayValue = JSValue::decode(encodedArrayValue);
1679     ASSERT(isJSArray(arrayValue));
1680     asArray(arrayValue)-&gt;putDirectIndex(exec, index, JSValue::decode(encodedValue));
1681 }
1682 
1683 enum class AccessorType {
1684     Getter,
1685     Setter
1686 };
1687 
1688 static void putAccessorByVal(ExecState* exec, JSObject* base, JSValue subscript, int32_t attribute, JSObject* accessor, AccessorType accessorType)
1689 {
1690     VM&amp; vm = exec-&gt;vm();
1691     auto scope = DECLARE_THROW_SCOPE(vm);
1692     auto propertyKey = subscript.toPropertyKey(exec);
1693     RETURN_IF_EXCEPTION(scope, void());
1694 
1695     scope.release();
1696     if (accessorType == AccessorType::Getter)
1697         base-&gt;putGetter(exec, propertyKey, accessor, attribute);
1698     else
1699         base-&gt;putSetter(exec, propertyKey, accessor, attribute);
1700 }
1701 
1702 void JIT_OPERATION operationPutGetterById(ExecState* exec, JSCell* object, UniquedStringImpl* uid, int32_t options, JSCell* getter)
1703 {
1704     VM&amp; vm = exec-&gt;vm();
1705     NativeCallFrameTracer tracer(&amp;vm, exec);
1706 
1707     ASSERT(object &amp;&amp; object-&gt;isObject());
1708     JSObject* baseObj = object-&gt;getObject();
1709 
1710     ASSERT(getter-&gt;isObject());
1711     baseObj-&gt;putGetter(exec, uid, getter, options);
1712 }
1713 
1714 void JIT_OPERATION operationPutSetterById(ExecState* exec, JSCell* object, UniquedStringImpl* uid, int32_t options, JSCell* setter)
1715 {
1716     VM&amp; vm = exec-&gt;vm();
1717     NativeCallFrameTracer tracer(&amp;vm, exec);
1718 
1719     ASSERT(object &amp;&amp; object-&gt;isObject());
1720     JSObject* baseObj = object-&gt;getObject();
1721 
1722     ASSERT(setter-&gt;isObject());
1723     baseObj-&gt;putSetter(exec, uid, setter, options);
1724 }
1725 
1726 void JIT_OPERATION operationPutGetterByVal(ExecState* exec, JSCell* base, EncodedJSValue encodedSubscript, int32_t attribute, JSCell* getter)
1727 {
1728     VM&amp; vm = exec-&gt;vm();
1729     NativeCallFrameTracer tracer(&amp;vm, exec);
1730 
1731     putAccessorByVal(exec, asObject(base), JSValue::decode(encodedSubscript), attribute, asObject(getter), AccessorType::Getter);
1732 }
1733 
1734 void JIT_OPERATION operationPutSetterByVal(ExecState* exec, JSCell* base, EncodedJSValue encodedSubscript, int32_t attribute, JSCell* setter)
1735 {
1736     VM&amp; vm = exec-&gt;vm();
1737     NativeCallFrameTracer tracer(&amp;vm, exec);
1738 
1739     putAccessorByVal(exec, asObject(base), JSValue::decode(encodedSubscript), attribute, asObject(setter), AccessorType::Setter);
1740 }
1741 
1742 #if USE(JSVALUE64)
1743 void JIT_OPERATION operationPutGetterSetter(ExecState* exec, JSCell* object, UniquedStringImpl* uid, int32_t attribute, EncodedJSValue encodedGetterValue, EncodedJSValue encodedSetterValue)
1744 {
1745     VM&amp; vm = exec-&gt;vm();
1746     NativeCallFrameTracer tracer(&amp;vm, exec);
1747 
1748     ASSERT(object &amp;&amp; object-&gt;isObject());
1749     JSObject* baseObject = asObject(object);
1750 
1751     JSValue getter = JSValue::decode(encodedGetterValue);
1752     JSValue setter = JSValue::decode(encodedSetterValue);
1753     ASSERT(getter.isObject() || setter.isObject());
1754     GetterSetter* accessor = GetterSetter::create(vm, exec-&gt;lexicalGlobalObject(), getter, setter);
1755     CommonSlowPaths::putDirectAccessorWithReify(vm, exec, baseObject, uid, accessor, attribute);
1756 }
1757 
1758 #else
1759 void JIT_OPERATION operationPutGetterSetter(ExecState* exec, JSCell* object, UniquedStringImpl* uid, int32_t attribute, JSCell* getterCell, JSCell* setterCell)
1760 {
1761     VM&amp; vm = exec-&gt;vm();
1762     NativeCallFrameTracer tracer(&amp;vm, exec);
1763 
1764     ASSERT(object &amp;&amp; object-&gt;isObject());
1765     JSObject* baseObject = asObject(object);
1766 
1767     ASSERT(getterCell || setterCell);
1768     JSObject* getter = getterCell ? getterCell-&gt;getObject() : nullptr;
1769     JSObject* setter = setterCell ? setterCell-&gt;getObject() : nullptr;
1770     GetterSetter* accessor = GetterSetter::create(vm, exec-&gt;lexicalGlobalObject(), getter, setter);
1771     CommonSlowPaths::putDirectAccessorWithReify(vm, exec, baseObject, uid, accessor, attribute);
1772 }
1773 #endif
1774 
1775 void JIT_OPERATION operationPopScope(ExecState* exec, int32_t scopeReg)
1776 {
1777     VM&amp; vm = exec-&gt;vm();
1778     NativeCallFrameTracer tracer(&amp;vm, exec);
1779 
1780     JSScope* scope = exec-&gt;uncheckedR(scopeReg).Register::scope();
1781     exec-&gt;uncheckedR(scopeReg) = scope-&gt;next();
1782 }
1783 
1784 int32_t JIT_OPERATION operationInstanceOfCustom(ExecState* exec, EncodedJSValue encodedValue, JSObject* constructor, EncodedJSValue encodedHasInstance)
1785 {
1786     VM&amp; vm = exec-&gt;vm();
1787     NativeCallFrameTracer tracer(&amp;vm, exec);
1788 
1789     JSValue value = JSValue::decode(encodedValue);
1790     JSValue hasInstanceValue = JSValue::decode(encodedHasInstance);
1791 
1792     ASSERT(hasInstanceValue != exec-&gt;lexicalGlobalObject()-&gt;functionProtoHasInstanceSymbolFunction() || !constructor-&gt;structure(vm)-&gt;typeInfo().implementsDefaultHasInstance());
1793 
1794     if (constructor-&gt;hasInstance(exec, value, hasInstanceValue))
1795         return 1;
1796     return 0;
1797 }
1798 
1799 }
1800 
1801 static JSValue getByVal(ExecState* exec, JSValue baseValue, JSValue subscript, ByValInfo* byValInfo, ReturnAddressPtr returnAddress)
1802 {
1803     VM&amp; vm = exec-&gt;vm();
1804     auto scope = DECLARE_THROW_SCOPE(vm);
1805 
1806     if (LIKELY(baseValue.isCell() &amp;&amp; subscript.isString())) {
1807         Structure&amp; structure = *baseValue.asCell()-&gt;structure(vm);
1808         if (JSCell::canUseFastGetOwnProperty(structure)) {
1809             if (RefPtr&lt;AtomicStringImpl&gt; existingAtomicString = asString(subscript)-&gt;toExistingAtomicString(exec)) {
1810                 if (JSValue result = baseValue.asCell()-&gt;fastGetOwnProperty(vm, structure, existingAtomicString.get())) {
1811                     ASSERT(exec-&gt;bytecodeOffset());
1812                     if (byValInfo-&gt;stubInfo &amp;&amp; byValInfo-&gt;cachedId.impl() != existingAtomicString)
1813                         byValInfo-&gt;tookSlowPath = true;
1814                     return result;
1815                 }
1816             }
1817         }
1818     }
1819 
1820     if (subscript.isUInt32()) {
1821         ASSERT(exec-&gt;bytecodeOffset());
1822         byValInfo-&gt;tookSlowPath = true;
1823 
1824         uint32_t i = subscript.asUInt32();
1825         if (isJSString(baseValue)) {
1826             if (asString(baseValue)-&gt;canGetIndex(i)) {
1827                 ctiPatchCallByReturnAddress(returnAddress, operationGetByValString);
1828                 RELEASE_AND_RETURN(scope, asString(baseValue)-&gt;getIndex(exec, i));
1829             }
1830             byValInfo-&gt;arrayProfile-&gt;setOutOfBounds();
1831         } else if (baseValue.isObject()) {
1832             JSObject* object = asObject(baseValue);
1833             if (object-&gt;canGetIndexQuickly(i))
1834                 return object-&gt;getIndexQuickly(i);
1835 
1836             bool skipMarkingOutOfBounds = false;
1837 
1838             if (object-&gt;indexingType() == ArrayWithContiguous &amp;&amp; i &lt; object-&gt;butterfly()-&gt;publicLength()) {
1839                 // FIXME: expand this to ArrayStorage, Int32, and maybe Double:
1840                 // https://bugs.webkit.org/show_bug.cgi?id=182940
1841                 auto* globalObject = object-&gt;globalObject(vm);
1842                 skipMarkingOutOfBounds = globalObject-&gt;isOriginalArrayStructure(object-&gt;structure(vm)) &amp;&amp; globalObject-&gt;arrayPrototypeChainIsSane();
1843             }
1844 
1845             if (!skipMarkingOutOfBounds &amp;&amp; !CommonSlowPaths::canAccessArgumentIndexQuickly(*object, i)) {
1846                 // FIXME: This will make us think that in-bounds typed array accesses are actually
1847                 // out-of-bounds.
1848                 // https://bugs.webkit.org/show_bug.cgi?id=149886
1849                 byValInfo-&gt;arrayProfile-&gt;setOutOfBounds();
1850             }
1851         }
1852 
1853         RELEASE_AND_RETURN(scope, baseValue.get(exec, i));
1854     }
1855 
1856     baseValue.requireObjectCoercible(exec);
1857     RETURN_IF_EXCEPTION(scope, JSValue());
1858     auto property = subscript.toPropertyKey(exec);
1859     RETURN_IF_EXCEPTION(scope, JSValue());
1860 
1861     ASSERT(exec-&gt;bytecodeOffset());
1862     if (byValInfo-&gt;stubInfo &amp;&amp; (!isStringOrSymbol(subscript) || byValInfo-&gt;cachedId != property))
1863         byValInfo-&gt;tookSlowPath = true;
1864 
1865     RELEASE_AND_RETURN(scope, baseValue.get(exec, property));
1866 }
1867 
1868 static OptimizationResult tryGetByValOptimize(ExecState* exec, JSValue baseValue, JSValue subscript, ByValInfo* byValInfo, ReturnAddressPtr returnAddress)
1869 {
1870     // See if it&#39;s worth optimizing this at all.
1871     OptimizationResult optimizationResult = OptimizationResult::NotOptimized;
1872 
1873     VM&amp; vm = exec-&gt;vm();
1874 
1875     if (baseValue.isObject() &amp;&amp; subscript.isInt32()) {
1876         JSObject* object = asObject(baseValue);
1877 
1878         ASSERT(exec-&gt;bytecodeOffset());
1879         ASSERT(!byValInfo-&gt;stubRoutine);
1880 
1881         if (hasOptimizableIndexing(object-&gt;structure(vm))) {
1882             // Attempt to optimize.
1883             Structure* structure = object-&gt;structure(vm);
1884             JITArrayMode arrayMode = jitArrayModeForStructure(structure);
1885             if (arrayMode != byValInfo-&gt;arrayMode) {
1886                 // If we reached this case, we got an interesting array mode we did not expect when we compiled.
1887                 // Let&#39;s update the profile to do better next time.
1888                 CodeBlock* codeBlock = exec-&gt;codeBlock();
1889                 ConcurrentJSLocker locker(codeBlock-&gt;m_lock);
1890                 byValInfo-&gt;arrayProfile-&gt;computeUpdatedPrediction(locker, codeBlock, structure);
1891 
1892                 JIT::compileGetByVal(locker, &amp;vm, codeBlock, byValInfo, returnAddress, arrayMode);
1893                 optimizationResult = OptimizationResult::Optimized;
1894             }
1895         }
1896 
1897         // If we failed to patch and we have some object that intercepts indexed get, then don&#39;t even wait until 10 times.
1898         if (optimizationResult != OptimizationResult::Optimized &amp;&amp; object-&gt;structure(vm)-&gt;typeInfo().interceptsGetOwnPropertySlotByIndexEvenWhenLengthIsNotZero())
1899             optimizationResult = OptimizationResult::GiveUp;
1900     }
1901 
1902     if (baseValue.isObject() &amp;&amp; isStringOrSymbol(subscript)) {
1903         const Identifier propertyName = subscript.toPropertyKey(exec);
1904         if (subscript.isSymbol() || !parseIndex(propertyName)) {
1905             ASSERT(exec-&gt;bytecodeOffset());
1906             ASSERT(!byValInfo-&gt;stubRoutine);
1907             if (byValInfo-&gt;seen) {
1908                 if (byValInfo-&gt;cachedId == propertyName) {
1909                     JIT::compileGetByValWithCachedId(&amp;vm, exec-&gt;codeBlock(), byValInfo, returnAddress, propertyName);
1910                     optimizationResult = OptimizationResult::Optimized;
1911                 } else {
1912                     // Seem like a generic property access site.
1913                     optimizationResult = OptimizationResult::GiveUp;
1914                 }
1915             } else {
1916                 CodeBlock* codeBlock = exec-&gt;codeBlock();
1917                 ConcurrentJSLocker locker(codeBlock-&gt;m_lock);
1918                 byValInfo-&gt;seen = true;
1919                 byValInfo-&gt;cachedId = propertyName;
1920                 if (subscript.isSymbol())
1921                     byValInfo-&gt;cachedSymbol.set(vm, codeBlock, asSymbol(subscript));
1922                 optimizationResult = OptimizationResult::SeenOnce;
1923             }
1924         }
1925     }
1926 
1927     if (optimizationResult != OptimizationResult::Optimized &amp;&amp; optimizationResult != OptimizationResult::SeenOnce) {
1928         // If we take slow path more than 10 times without patching then make sure we
1929         // never make that mistake again. For cases where we see non-index-intercepting
1930         // objects, this gives 10 iterations worth of opportunity for us to observe
1931         // that the get_by_val may be polymorphic. We count up slowPathCount even if
1932         // the result is GiveUp.
1933         if (++byValInfo-&gt;slowPathCount &gt;= 10)
1934             optimizationResult = OptimizationResult::GiveUp;
1935     }
1936 
1937     return optimizationResult;
1938 }
1939 
1940 extern &quot;C&quot; {
1941 
1942 EncodedJSValue JIT_OPERATION operationGetByValGeneric(ExecState* exec, EncodedJSValue encodedBase, EncodedJSValue encodedSubscript, ByValInfo* byValInfo)
1943 {
1944     VM&amp; vm = exec-&gt;vm();
1945     NativeCallFrameTracer tracer(&amp;vm, exec);
1946     JSValue baseValue = JSValue::decode(encodedBase);
1947     JSValue subscript = JSValue::decode(encodedSubscript);
1948 
1949     JSValue result = getByVal(exec, baseValue, subscript, byValInfo, ReturnAddressPtr(OUR_RETURN_ADDRESS));
1950     return JSValue::encode(result);
1951 }
1952 
1953 EncodedJSValue JIT_OPERATION operationGetByValOptimize(ExecState* exec, EncodedJSValue encodedBase, EncodedJSValue encodedSubscript, ByValInfo* byValInfo)
1954 {
1955     VM&amp; vm = exec-&gt;vm();
1956     NativeCallFrameTracer tracer(&amp;vm, exec);
1957 
1958     JSValue baseValue = JSValue::decode(encodedBase);
1959     JSValue subscript = JSValue::decode(encodedSubscript);
1960     ReturnAddressPtr returnAddress = ReturnAddressPtr(OUR_RETURN_ADDRESS);
1961     if (tryGetByValOptimize(exec, baseValue, subscript, byValInfo, returnAddress) == OptimizationResult::GiveUp) {
1962         // Don&#39;t ever try to optimize.
1963         byValInfo-&gt;tookSlowPath = true;
1964         ctiPatchCallByReturnAddress(returnAddress, operationGetByValGeneric);
1965     }
1966 
1967     return JSValue::encode(getByVal(exec, baseValue, subscript, byValInfo, returnAddress));
1968 }
1969 
1970 EncodedJSValue JIT_OPERATION operationHasIndexedPropertyDefault(ExecState* exec, EncodedJSValue encodedBase, EncodedJSValue encodedSubscript, ByValInfo* byValInfo)
1971 {
1972     VM&amp; vm = exec-&gt;vm();
1973     NativeCallFrameTracer tracer(&amp;vm, exec);
1974     JSValue baseValue = JSValue::decode(encodedBase);
1975     JSValue subscript = JSValue::decode(encodedSubscript);
1976 
1977     ASSERT(baseValue.isObject());
1978     ASSERT(subscript.isUInt32());
1979 
1980     JSObject* object = asObject(baseValue);
1981     bool didOptimize = false;
1982 
1983     ASSERT(exec-&gt;bytecodeOffset());
1984     ASSERT(!byValInfo-&gt;stubRoutine);
1985 
1986     if (hasOptimizableIndexing(object-&gt;structure(vm))) {
1987         // Attempt to optimize.
1988         JITArrayMode arrayMode = jitArrayModeForStructure(object-&gt;structure(vm));
1989         if (arrayMode != byValInfo-&gt;arrayMode) {
1990             JIT::compileHasIndexedProperty(&amp;vm, exec-&gt;codeBlock(), byValInfo, ReturnAddressPtr(OUR_RETURN_ADDRESS), arrayMode);
1991             didOptimize = true;
1992         }
1993     }
1994 
1995     if (!didOptimize) {
1996         // If we take slow path more than 10 times without patching then make sure we
1997         // never make that mistake again. Or, if we failed to patch and we have some object
1998         // that intercepts indexed get, then don&#39;t even wait until 10 times. For cases
1999         // where we see non-index-intercepting objects, this gives 10 iterations worth of
2000         // opportunity for us to observe that the get_by_val may be polymorphic.
2001         if (++byValInfo-&gt;slowPathCount &gt;= 10
2002             || object-&gt;structure(vm)-&gt;typeInfo().interceptsGetOwnPropertySlotByIndexEvenWhenLengthIsNotZero()) {
2003             // Don&#39;t ever try to optimize.
2004             ctiPatchCallByReturnAddress(ReturnAddressPtr(OUR_RETURN_ADDRESS), operationHasIndexedPropertyGeneric);
2005         }
2006     }
2007 
2008     uint32_t index = subscript.asUInt32();
2009     if (object-&gt;canGetIndexQuickly(index))
2010         return JSValue::encode(JSValue(JSValue::JSTrue));
2011 
2012     if (!CommonSlowPaths::canAccessArgumentIndexQuickly(*object, index)) {
2013         // FIXME: This will make us think that in-bounds typed array accesses are actually
2014         // out-of-bounds.
2015         // https://bugs.webkit.org/show_bug.cgi?id=149886
2016         byValInfo-&gt;arrayProfile-&gt;setOutOfBounds();
2017     }
2018     return JSValue::encode(jsBoolean(object-&gt;hasPropertyGeneric(exec, index, PropertySlot::InternalMethodType::GetOwnProperty)));
2019 }
2020 
2021 EncodedJSValue JIT_OPERATION operationHasIndexedPropertyGeneric(ExecState* exec, EncodedJSValue encodedBase, EncodedJSValue encodedSubscript, ByValInfo* byValInfo)
2022 {
2023     VM&amp; vm = exec-&gt;vm();
2024     NativeCallFrameTracer tracer(&amp;vm, exec);
2025     JSValue baseValue = JSValue::decode(encodedBase);
2026     JSValue subscript = JSValue::decode(encodedSubscript);
2027 
2028     ASSERT(baseValue.isObject());
2029     ASSERT(subscript.isUInt32());
2030 
2031     JSObject* object = asObject(baseValue);
2032     uint32_t index = subscript.asUInt32();
2033     if (object-&gt;canGetIndexQuickly(index))
2034         return JSValue::encode(JSValue(JSValue::JSTrue));
2035 
2036     if (!CommonSlowPaths::canAccessArgumentIndexQuickly(*object, index)) {
2037         // FIXME: This will make us think that in-bounds typed array accesses are actually
2038         // out-of-bounds.
2039         // https://bugs.webkit.org/show_bug.cgi?id=149886
2040         byValInfo-&gt;arrayProfile-&gt;setOutOfBounds();
2041     }
2042     return JSValue::encode(jsBoolean(object-&gt;hasPropertyGeneric(exec, subscript.asUInt32(), PropertySlot::InternalMethodType::GetOwnProperty)));
2043 }
2044 
2045 EncodedJSValue JIT_OPERATION operationGetByValString(ExecState* exec, EncodedJSValue encodedBase, EncodedJSValue encodedSubscript, ByValInfo* byValInfo)
2046 {
2047     VM&amp; vm = exec-&gt;vm();
2048     NativeCallFrameTracer tracer(&amp;vm, exec);
2049     auto scope = DECLARE_THROW_SCOPE(vm);
2050     JSValue baseValue = JSValue::decode(encodedBase);
2051     JSValue subscript = JSValue::decode(encodedSubscript);
2052 
2053     JSValue result;
2054     if (LIKELY(subscript.isUInt32())) {
2055         uint32_t i = subscript.asUInt32();
2056         if (isJSString(baseValue) &amp;&amp; asString(baseValue)-&gt;canGetIndex(i))
2057             RELEASE_AND_RETURN(scope, JSValue::encode(asString(baseValue)-&gt;getIndex(exec, i)));
2058 
2059         result = baseValue.get(exec, i);
2060         RETURN_IF_EXCEPTION(scope, encodedJSValue());
2061         if (!isJSString(baseValue)) {
2062             ASSERT(exec-&gt;bytecodeOffset());
2063             auto getByValFunction = byValInfo-&gt;stubRoutine ? operationGetByValGeneric : operationGetByValOptimize;
2064             ctiPatchCallByReturnAddress(ReturnAddressPtr(OUR_RETURN_ADDRESS), getByValFunction);
2065         }
2066     } else {
2067         baseValue.requireObjectCoercible(exec);
2068         RETURN_IF_EXCEPTION(scope, encodedJSValue());
2069         auto property = subscript.toPropertyKey(exec);
2070         RETURN_IF_EXCEPTION(scope, encodedJSValue());
2071         scope.release();
2072         result = baseValue.get(exec, property);
2073     }
2074 
2075     return JSValue::encode(result);
2076 }
2077 
2078 EncodedJSValue JIT_OPERATION operationDeleteByIdJSResult(ExecState* exec, EncodedJSValue base, UniquedStringImpl* uid)
2079 {
2080     return JSValue::encode(jsBoolean(operationDeleteById(exec, base, uid)));
2081 }
2082 
2083 size_t JIT_OPERATION operationDeleteById(ExecState* exec, EncodedJSValue encodedBase, UniquedStringImpl* uid)
2084 {
2085     VM&amp; vm = exec-&gt;vm();
2086     NativeCallFrameTracer tracer(&amp;vm, exec);
2087     auto scope = DECLARE_THROW_SCOPE(vm);
2088 
2089     JSObject* baseObj = JSValue::decode(encodedBase).toObject(exec);
2090     RETURN_IF_EXCEPTION(scope, false);
2091     if (!baseObj)
2092         return false;
2093     bool couldDelete = baseObj-&gt;methodTable(vm)-&gt;deleteProperty(baseObj, exec, Identifier::fromUid(&amp;vm, uid));
2094     RETURN_IF_EXCEPTION(scope, false);
2095     if (!couldDelete &amp;&amp; exec-&gt;codeBlock()-&gt;isStrictMode())
2096         throwTypeError(exec, scope, UnableToDeletePropertyError);
2097     return couldDelete;
2098 }
2099 
2100 EncodedJSValue JIT_OPERATION operationDeleteByValJSResult(ExecState* exec, EncodedJSValue base,  EncodedJSValue key)
2101 {
2102     return JSValue::encode(jsBoolean(operationDeleteByVal(exec, base, key)));
2103 }
2104 
2105 size_t JIT_OPERATION operationDeleteByVal(ExecState* exec, EncodedJSValue encodedBase, EncodedJSValue encodedKey)
2106 {
2107     VM&amp; vm = exec-&gt;vm();
2108     NativeCallFrameTracer tracer(&amp;vm, exec);
2109     auto scope = DECLARE_THROW_SCOPE(vm);
2110 
2111     JSObject* baseObj = JSValue::decode(encodedBase).toObject(exec);
2112     RETURN_IF_EXCEPTION(scope, false);
2113     JSValue key = JSValue::decode(encodedKey);
2114     if (!baseObj)
2115         return false;
2116 
2117     bool couldDelete;
2118     uint32_t index;
2119     if (key.getUInt32(index))
2120         couldDelete = baseObj-&gt;methodTable(vm)-&gt;deletePropertyByIndex(baseObj, exec, index);
2121     else {
2122         Identifier property = key.toPropertyKey(exec);
2123         RETURN_IF_EXCEPTION(scope, false);
2124         couldDelete = baseObj-&gt;methodTable(vm)-&gt;deleteProperty(baseObj, exec, property);
2125     }
2126     RETURN_IF_EXCEPTION(scope, false);
2127     if (!couldDelete &amp;&amp; exec-&gt;codeBlock()-&gt;isStrictMode())
2128         throwTypeError(exec, scope, UnableToDeletePropertyError);
2129     return couldDelete;
2130 }
2131 
2132 JSCell* JIT_OPERATION operationPushWithScope(ExecState* exec, JSCell* currentScopeCell, EncodedJSValue objectValue)
2133 {
2134     VM&amp; vm = exec-&gt;vm();
2135     NativeCallFrameTracer tracer(&amp;vm, exec);
2136     auto scope = DECLARE_THROW_SCOPE(vm);
2137 
2138     JSObject* object = JSValue::decode(objectValue).toObject(exec);
2139     RETURN_IF_EXCEPTION(scope, nullptr);
2140 
2141     JSScope* currentScope = jsCast&lt;JSScope*&gt;(currentScopeCell);
2142 
2143     return JSWithScope::create(vm, exec-&gt;lexicalGlobalObject(), currentScope, object);
2144 }
2145 
2146 JSCell* JIT_OPERATION operationPushWithScopeObject(ExecState* exec, JSCell* currentScopeCell, JSObject* object)
2147 {
2148     VM&amp; vm = exec-&gt;vm();
2149     NativeCallFrameTracer tracer(&amp;vm, exec);
2150     JSScope* currentScope = jsCast&lt;JSScope*&gt;(currentScopeCell);
2151     return JSWithScope::create(vm, exec-&gt;lexicalGlobalObject(), currentScope, object);
2152 }
2153 
2154 EncodedJSValue JIT_OPERATION operationInstanceOf(ExecState* exec, EncodedJSValue encodedValue, EncodedJSValue encodedProto)
2155 {
2156     VM&amp; vm = exec-&gt;vm();
2157     NativeCallFrameTracer tracer(&amp;vm, exec);
2158     JSValue value = JSValue::decode(encodedValue);
2159     JSValue proto = JSValue::decode(encodedProto);
2160 
2161     bool result = JSObject::defaultHasInstance(exec, value, proto);
2162     return JSValue::encode(jsBoolean(result));
2163 }
2164 
2165 EncodedJSValue JIT_OPERATION operationInstanceOfGeneric(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue encodedValue, EncodedJSValue encodedProto)
2166 {
2167     VM&amp; vm = exec-&gt;vm();
2168     NativeCallFrameTracer tracer(&amp;vm, exec);
2169     JSValue value = JSValue::decode(encodedValue);
2170     JSValue proto = JSValue::decode(encodedProto);
2171 
2172     stubInfo-&gt;tookSlowPath = true;
2173 
2174     bool result = JSObject::defaultHasInstance(exec, value, proto);
2175     return JSValue::encode(jsBoolean(result));
2176 }
2177 
2178 EncodedJSValue JIT_OPERATION operationInstanceOfOptimize(ExecState* exec, StructureStubInfo* stubInfo, EncodedJSValue encodedValue, EncodedJSValue encodedProto)
2179 {
2180     VM&amp; vm = exec-&gt;vm();
2181     NativeCallFrameTracer tracer(&amp;vm, exec);
2182     auto scope = DECLARE_THROW_SCOPE(vm);
2183     JSValue value = JSValue::decode(encodedValue);
2184     JSValue proto = JSValue::decode(encodedProto);
2185 
2186     bool result = JSObject::defaultHasInstance(exec, value, proto);
2187     RETURN_IF_EXCEPTION(scope, JSValue::encode(jsUndefined()));
2188 
2189     if (stubInfo-&gt;considerCaching(exec-&gt;codeBlock(), value.structureOrNull()))
2190         repatchInstanceOf(exec, value, proto, *stubInfo, result);
2191 
2192     return JSValue::encode(jsBoolean(result));
2193 }
2194 
2195 int32_t JIT_OPERATION operationSizeFrameForForwardArguments(ExecState* exec, EncodedJSValue, int32_t numUsedStackSlots, int32_t)
2196 {
2197     VM&amp; vm = exec-&gt;vm();
2198     NativeCallFrameTracer tracer(&amp;vm, exec);
2199     return sizeFrameForForwardArguments(exec, vm, numUsedStackSlots);
2200 }
2201 
2202 int32_t JIT_OPERATION operationSizeFrameForVarargs(ExecState* exec, EncodedJSValue encodedArguments, int32_t numUsedStackSlots, int32_t firstVarArgOffset)
2203 {
2204     VM&amp; vm = exec-&gt;vm();
2205     NativeCallFrameTracer tracer(&amp;vm, exec);
2206     JSValue arguments = JSValue::decode(encodedArguments);
2207     return sizeFrameForVarargs(exec, vm, arguments, numUsedStackSlots, firstVarArgOffset);
2208 }
2209 
2210 CallFrame* JIT_OPERATION operationSetupForwardArgumentsFrame(ExecState* exec, CallFrame* newCallFrame, EncodedJSValue, int32_t, int32_t length)
2211 {
2212     VM&amp; vm = exec-&gt;vm();
2213     NativeCallFrameTracer tracer(&amp;vm, exec);
2214     setupForwardArgumentsFrame(exec, newCallFrame, length);
2215     return newCallFrame;
2216 }
2217 
2218 CallFrame* JIT_OPERATION operationSetupVarargsFrame(ExecState* exec, CallFrame* newCallFrame, EncodedJSValue encodedArguments, int32_t firstVarArgOffset, int32_t length)
2219 {
2220     VM&amp; vm = exec-&gt;vm();
2221     NativeCallFrameTracer tracer(&amp;vm, exec);
2222     JSValue arguments = JSValue::decode(encodedArguments);
2223     setupVarargsFrame(exec, newCallFrame, arguments, firstVarArgOffset, length);
2224     return newCallFrame;
2225 }
2226 
2227 char* JIT_OPERATION operationSwitchCharWithUnknownKeyType(ExecState* exec, EncodedJSValue encodedKey, size_t tableIndex)
2228 {
2229     VM&amp; vm = exec-&gt;vm();
2230     NativeCallFrameTracer tracer(&amp;vm, exec);
2231     JSValue key = JSValue::decode(encodedKey);
2232     CodeBlock* codeBlock = exec-&gt;codeBlock();
2233 
2234     SimpleJumpTable&amp; jumpTable = codeBlock-&gt;switchJumpTable(tableIndex);
2235     void* result = jumpTable.ctiDefault.executableAddress();
2236 
2237     if (key.isString()) {
2238         StringImpl* value = asString(key)-&gt;value(exec).impl();
2239         if (value-&gt;length() == 1)
2240             result = jumpTable.ctiForValue((*value)[0]).executableAddress();
2241     }
2242 
2243     assertIsTaggedWith(result, JSSwitchPtrTag);
2244     return reinterpret_cast&lt;char*&gt;(result);
2245 }
2246 
2247 char* JIT_OPERATION operationSwitchImmWithUnknownKeyType(ExecState* exec, EncodedJSValue encodedKey, size_t tableIndex)
2248 {
2249     VM&amp; vm = exec-&gt;vm();
2250     NativeCallFrameTracer tracer(&amp;vm, exec);
2251     JSValue key = JSValue::decode(encodedKey);
2252     CodeBlock* codeBlock = exec-&gt;codeBlock();
2253 
2254     SimpleJumpTable&amp; jumpTable = codeBlock-&gt;switchJumpTable(tableIndex);
2255     void* result;
2256     if (key.isInt32())
2257         result = jumpTable.ctiForValue(key.asInt32()).executableAddress();
2258     else if (key.isDouble() &amp;&amp; key.asDouble() == static_cast&lt;int32_t&gt;(key.asDouble()))
2259         result = jumpTable.ctiForValue(static_cast&lt;int32_t&gt;(key.asDouble())).executableAddress();
2260     else
2261         result = jumpTable.ctiDefault.executableAddress();
2262     assertIsTaggedWith(result, JSSwitchPtrTag);
2263     return reinterpret_cast&lt;char*&gt;(result);
2264 }
2265 
2266 char* JIT_OPERATION operationSwitchStringWithUnknownKeyType(ExecState* exec, EncodedJSValue encodedKey, size_t tableIndex)
2267 {
2268     VM&amp; vm = exec-&gt;vm();
2269     NativeCallFrameTracer tracer(&amp;vm, exec);
2270     JSValue key = JSValue::decode(encodedKey);
2271     CodeBlock* codeBlock = exec-&gt;codeBlock();
2272 
2273     void* result;
2274     StringJumpTable&amp; jumpTable = codeBlock-&gt;stringSwitchJumpTable(tableIndex);
2275 
2276     if (key.isString()) {
2277         StringImpl* value = asString(key)-&gt;value(exec).impl();
2278         result = jumpTable.ctiForValue(value).executableAddress();
2279     } else
2280         result = jumpTable.ctiDefault.executableAddress();
2281 
2282     assertIsTaggedWith(result, JSSwitchPtrTag);
2283     return reinterpret_cast&lt;char*&gt;(result);
2284 }
2285 
2286 EncodedJSValue JIT_OPERATION operationGetFromScope(ExecState* exec, const Instruction* pc)
2287 {
2288     VM&amp; vm = exec-&gt;vm();
2289     NativeCallFrameTracer tracer(&amp;vm, exec);
2290     auto throwScope = DECLARE_THROW_SCOPE(vm);
2291 
2292     CodeBlock* codeBlock = exec-&gt;codeBlock();
2293 
2294     auto bytecode = pc-&gt;as&lt;OpGetFromScope&gt;();
2295     const Identifier&amp; ident = codeBlock-&gt;identifier(bytecode.m_var);
2296     JSObject* scope = jsCast&lt;JSObject*&gt;(exec-&gt;uncheckedR(bytecode.m_scope.offset()).jsValue());
2297     GetPutInfo&amp; getPutInfo = bytecode.metadata(codeBlock).m_getPutInfo;
2298 
2299     // ModuleVar is always converted to ClosureVar for get_from_scope.
2300     ASSERT(getPutInfo.resolveType() != ModuleVar);
2301 
2302     RELEASE_AND_RETURN(throwScope, JSValue::encode(scope-&gt;getPropertySlot(exec, ident, [&amp;] (bool found, PropertySlot&amp; slot) -&gt; JSValue {
2303         if (!found) {
2304             if (getPutInfo.resolveMode() == ThrowIfNotFound)
2305                 throwException(exec, throwScope, createUndefinedVariableError(exec, ident));
2306             return jsUndefined();
2307         }
2308 
2309         JSValue result = JSValue();
2310         if (scope-&gt;isGlobalLexicalEnvironment()) {
2311             // When we can&#39;t statically prove we need a TDZ check, we must perform the check on the slow path.
2312             result = slot.getValue(exec, ident);
2313             if (result == jsTDZValue()) {
2314                 throwException(exec, throwScope, createTDZError(exec));
2315                 return jsUndefined();
2316             }
2317         }
2318 
2319         CommonSlowPaths::tryCacheGetFromScopeGlobal(exec, vm, bytecode, scope, slot, ident);
2320 
2321         if (!result)
2322             return slot.getValue(exec, ident);
2323         return result;
2324     })));
2325 }
2326 
2327 void JIT_OPERATION operationPutToScope(ExecState* exec, const Instruction* pc)
2328 {
2329     VM&amp; vm = exec-&gt;vm();
2330     NativeCallFrameTracer tracer(&amp;vm, exec);
2331     auto throwScope = DECLARE_THROW_SCOPE(vm);
2332 
2333     CodeBlock* codeBlock = exec-&gt;codeBlock();
2334     auto bytecode = pc-&gt;as&lt;OpPutToScope&gt;();
2335     auto&amp; metadata = bytecode.metadata(codeBlock);
2336 
2337     const Identifier&amp; ident = codeBlock-&gt;identifier(bytecode.m_var);
2338     JSObject* scope = jsCast&lt;JSObject*&gt;(exec-&gt;uncheckedR(bytecode.m_scope.offset()).jsValue());
2339     JSValue value = exec-&gt;r(bytecode.m_value.offset()).jsValue();
2340     GetPutInfo&amp; getPutInfo = metadata.m_getPutInfo;
2341 
2342     // ModuleVar does not keep the scope register value alive in DFG.
2343     ASSERT(getPutInfo.resolveType() != ModuleVar);
2344 
2345     if (getPutInfo.resolveType() == LocalClosureVar) {
2346         JSLexicalEnvironment* environment = jsCast&lt;JSLexicalEnvironment*&gt;(scope);
2347         environment-&gt;variableAt(ScopeOffset(metadata.m_operand)).set(vm, environment, value);
2348         if (WatchpointSet* set = metadata.m_watchpointSet)
2349             set-&gt;touch(vm, &quot;Executed op_put_scope&lt;LocalClosureVar&gt;&quot;);
2350         return;
2351     }
2352 
2353     bool hasProperty = scope-&gt;hasProperty(exec, ident);
2354     EXCEPTION_ASSERT(!throwScope.exception() || !hasProperty);
2355     if (hasProperty
2356         &amp;&amp; scope-&gt;isGlobalLexicalEnvironment()
2357         &amp;&amp; !isInitialization(getPutInfo.initializationMode())) {
2358         // When we can&#39;t statically prove we need a TDZ check, we must perform the check on the slow path.
2359         PropertySlot slot(scope, PropertySlot::InternalMethodType::Get);
2360         JSGlobalLexicalEnvironment::getOwnPropertySlot(scope, exec, ident, slot);
2361         if (slot.getValue(exec, ident) == jsTDZValue()) {
2362             throwException(exec, throwScope, createTDZError(exec));
2363             return;
2364         }
2365     }
2366 
2367     if (getPutInfo.resolveMode() == ThrowIfNotFound &amp;&amp; !hasProperty) {
2368         throwException(exec, throwScope, createUndefinedVariableError(exec, ident));
2369         return;
2370     }
2371 
2372     PutPropertySlot slot(scope, codeBlock-&gt;isStrictMode(), PutPropertySlot::UnknownContext, isInitialization(getPutInfo.initializationMode()));
2373     scope-&gt;methodTable(vm)-&gt;put(scope, exec, ident, value, slot);
2374 
2375     RETURN_IF_EXCEPTION(throwScope, void());
2376 
2377     CommonSlowPaths::tryCachePutToScopeGlobal(exec, codeBlock, bytecode, scope, slot, ident);
2378 }
2379 
2380 void JIT_OPERATION operationThrow(ExecState* exec, EncodedJSValue encodedExceptionValue)
2381 {
2382     VM* vm = &amp;exec-&gt;vm();
2383     NativeCallFrameTracer tracer(vm, exec);
2384     auto scope = DECLARE_THROW_SCOPE(*vm);
2385 
2386     JSValue exceptionValue = JSValue::decode(encodedExceptionValue);
2387     throwException(exec, scope, exceptionValue);
2388 
2389     // Results stored out-of-band in vm.targetMachinePCForThrow &amp; vm.callFrameForCatch
2390     genericUnwind(vm, exec);
2391 }
2392 
2393 char* JIT_OPERATION operationReallocateButterflyToHavePropertyStorageWithInitialCapacity(ExecState* exec, JSObject* object)
2394 {
2395     VM&amp; vm = exec-&gt;vm();
2396     NativeCallFrameTracer tracer(&amp;vm, exec);
2397 
2398     ASSERT(!object-&gt;structure(vm)-&gt;outOfLineCapacity());
2399     Butterfly* result = object-&gt;allocateMoreOutOfLineStorage(vm, 0, initialOutOfLineCapacity);
2400     object-&gt;nukeStructureAndSetButterfly(vm, object-&gt;structureID(), result);
2401     return reinterpret_cast&lt;char*&gt;(result);
2402 }
2403 
2404 char* JIT_OPERATION operationReallocateButterflyToGrowPropertyStorage(ExecState* exec, JSObject* object, size_t newSize)
2405 {
2406     VM&amp; vm = exec-&gt;vm();
2407     NativeCallFrameTracer tracer(&amp;vm, exec);
2408 
2409     Butterfly* result = object-&gt;allocateMoreOutOfLineStorage(vm, object-&gt;structure(vm)-&gt;outOfLineCapacity(), newSize);
2410     object-&gt;nukeStructureAndSetButterfly(vm, object-&gt;structureID(), result);
2411     return reinterpret_cast&lt;char*&gt;(result);
2412 }
2413 
2414 void JIT_OPERATION operationOSRWriteBarrier(ExecState* exec, JSCell* cell)
2415 {
2416     VM* vm = &amp;exec-&gt;vm();
2417     NativeCallFrameTracer tracer(vm, exec);
2418     vm-&gt;heap.writeBarrier(cell);
2419 }
2420 
2421 void JIT_OPERATION operationWriteBarrierSlowPath(ExecState* exec, JSCell* cell)
2422 {
2423     VM* vm = &amp;exec-&gt;vm();
2424     NativeCallFrameTracer tracer(vm, exec);
2425     vm-&gt;heap.writeBarrierSlowPath(cell);
2426 }
2427 
2428 void JIT_OPERATION lookupExceptionHandler(VM* vm, ExecState* exec)
2429 {
2430     NativeCallFrameTracer tracer(vm, exec);
2431     genericUnwind(vm, exec);
2432     ASSERT(vm-&gt;targetMachinePCForThrow);
2433 }
2434 
2435 void JIT_OPERATION lookupExceptionHandlerFromCallerFrame(VM* vm, ExecState* exec)
2436 {
2437     ASSERT(exec-&gt;isStackOverflowFrame());
2438     ASSERT(jsCast&lt;ErrorInstance*&gt;(vm-&gt;exceptionForInspection()-&gt;value().asCell())-&gt;isStackOverflowError());
2439     lookupExceptionHandler(vm, exec);
2440 }
2441 
2442 void JIT_OPERATION operationVMHandleException(ExecState* exec)
2443 {
2444     VM* vm = &amp;exec-&gt;vm();
2445     NativeCallFrameTracer tracer(vm, exec);
2446     genericUnwind(vm, exec);
2447 }
2448 
2449 // This function &quot;should&quot; just take the ExecState*, but doing so would make it more difficult
2450 // to call from exception check sites. So, unlike all of our other functions, we allow
2451 // ourselves to play some gnarly ABI tricks just to simplify the calling convention. This is
2452 // particularly safe here since this is never called on the critical path - it&#39;s only for
2453 // testing.
2454 void JIT_OPERATION operationExceptionFuzz(ExecState* exec)
2455 {
2456     VM* vm = &amp;exec-&gt;vm();
2457     NativeCallFrameTracer tracer(vm, exec);
2458     auto scope = DECLARE_THROW_SCOPE(*vm);
2459     UNUSED_PARAM(scope);
2460 #if COMPILER(GCC_COMPATIBLE)
2461     void* returnPC = __builtin_return_address(0);
2462     doExceptionFuzzing(exec, scope, &quot;JITOperations&quot;, returnPC);
2463 #endif // COMPILER(GCC_COMPATIBLE)
2464 }
2465 
2466 ALWAYS_INLINE static EncodedJSValue unprofiledAdd(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2)
2467 {
2468     VM* vm = &amp;exec-&gt;vm();
2469     NativeCallFrameTracer tracer(vm, exec);
2470 
2471     JSValue op1 = JSValue::decode(encodedOp1);
2472     JSValue op2 = JSValue::decode(encodedOp2);
2473 
2474     return JSValue::encode(jsAdd(exec, op1, op2));
2475 }
2476 
2477 ALWAYS_INLINE static EncodedJSValue profiledAdd(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, ArithProfile&amp; arithProfile)
2478 {
2479     VM* vm = &amp;exec-&gt;vm();
2480     NativeCallFrameTracer tracer(vm, exec);
2481 
2482     JSValue op1 = JSValue::decode(encodedOp1);
2483     JSValue op2 = JSValue::decode(encodedOp2);
2484 
2485     arithProfile.observeLHSAndRHS(op1, op2);
2486     JSValue result = jsAdd(exec, op1, op2);
2487     arithProfile.observeResult(result);
2488 
2489     return JSValue::encode(result);
2490 }
2491 
2492 EncodedJSValue JIT_OPERATION operationValueAdd(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2)
2493 {
2494     return unprofiledAdd(exec, encodedOp1, encodedOp2);
2495 }
2496 
2497 EncodedJSValue JIT_OPERATION operationValueAddProfiled(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, ArithProfile* arithProfile)
2498 {
2499     ASSERT(arithProfile);
2500     return profiledAdd(exec, encodedOp1, encodedOp2, *arithProfile);
2501 }
2502 
2503 EncodedJSValue JIT_OPERATION operationValueAddProfiledOptimize(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, JITAddIC* addIC)
2504 {
2505     VM* vm = &amp;exec-&gt;vm();
2506     NativeCallFrameTracer tracer(vm, exec);
2507 
2508     JSValue op1 = JSValue::decode(encodedOp1);
2509     JSValue op2 = JSValue::decode(encodedOp2);
2510 
2511     ArithProfile* arithProfile = addIC-&gt;arithProfile();
2512     ASSERT(arithProfile);
2513     arithProfile-&gt;observeLHSAndRHS(op1, op2);
2514     auto nonOptimizeVariant = operationValueAddProfiledNoOptimize;
2515     addIC-&gt;generateOutOfLine(exec-&gt;codeBlock(), nonOptimizeVariant);
2516 
2517 #if ENABLE(MATH_IC_STATS)
2518     exec-&gt;codeBlock()-&gt;dumpMathICStats();
2519 #endif
2520 
2521     JSValue result = jsAdd(exec, op1, op2);
2522     arithProfile-&gt;observeResult(result);
2523 
2524     return JSValue::encode(result);
2525 }
2526 
2527 EncodedJSValue JIT_OPERATION operationValueAddProfiledNoOptimize(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, JITAddIC* addIC)
2528 {
2529     VM* vm = &amp;exec-&gt;vm();
2530     NativeCallFrameTracer tracer(vm, exec);
2531 
2532     ArithProfile* arithProfile = addIC-&gt;arithProfile();
2533     ASSERT(arithProfile);
2534     return profiledAdd(exec, encodedOp1, encodedOp2, *arithProfile);
2535 }
2536 
2537 EncodedJSValue JIT_OPERATION operationValueAddOptimize(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, JITAddIC* addIC)
2538 {
2539     VM* vm = &amp;exec-&gt;vm();
2540     NativeCallFrameTracer tracer(vm, exec);
2541 
2542     JSValue op1 = JSValue::decode(encodedOp1);
2543     JSValue op2 = JSValue::decode(encodedOp2);
2544 
2545     auto nonOptimizeVariant = operationValueAddNoOptimize;
2546     if (ArithProfile* arithProfile = addIC-&gt;arithProfile())
2547         arithProfile-&gt;observeLHSAndRHS(op1, op2);
2548     addIC-&gt;generateOutOfLine(exec-&gt;codeBlock(), nonOptimizeVariant);
2549 
2550 #if ENABLE(MATH_IC_STATS)
2551     exec-&gt;codeBlock()-&gt;dumpMathICStats();
2552 #endif
2553 
2554     return JSValue::encode(jsAdd(exec, op1, op2));
2555 }
2556 
2557 EncodedJSValue JIT_OPERATION operationValueAddNoOptimize(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, JITAddIC*)
2558 {
2559     VM* vm = &amp;exec-&gt;vm();
2560     NativeCallFrameTracer tracer(vm, exec);
2561 
2562     JSValue op1 = JSValue::decode(encodedOp1);
2563     JSValue op2 = JSValue::decode(encodedOp2);
2564 
2565     JSValue result = jsAdd(exec, op1, op2);
2566 
2567     return JSValue::encode(result);
2568 }
2569 
2570 ALWAYS_INLINE static EncodedJSValue unprofiledMul(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2)
2571 {
2572     JSValue op1 = JSValue::decode(encodedOp1);
2573     JSValue op2 = JSValue::decode(encodedOp2);
2574 
2575     return JSValue::encode(jsMul(exec, op1, op2));
2576 }
2577 
2578 ALWAYS_INLINE static EncodedJSValue profiledMul(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, ArithProfile&amp; arithProfile, bool shouldObserveLHSAndRHSTypes = true)
2579 {
2580     VM&amp; vm = exec-&gt;vm();
2581     auto scope = DECLARE_THROW_SCOPE(vm);
2582     JSValue op1 = JSValue::decode(encodedOp1);
2583     JSValue op2 = JSValue::decode(encodedOp2);
2584 
2585     if (shouldObserveLHSAndRHSTypes)
2586         arithProfile.observeLHSAndRHS(op1, op2);
2587 
2588     JSValue result = jsMul(exec, op1, op2);
2589     RETURN_IF_EXCEPTION(scope, encodedJSValue());
2590     arithProfile.observeResult(result);
2591     return JSValue::encode(result);
2592 }
2593 
2594 EncodedJSValue JIT_OPERATION operationValueMul(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2)
2595 {
2596     VM* vm = &amp;exec-&gt;vm();
2597     NativeCallFrameTracer tracer(vm, exec);
2598 
2599     return unprofiledMul(exec, encodedOp1, encodedOp2);
2600 }
2601 
2602 EncodedJSValue JIT_OPERATION operationValueMulNoOptimize(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, JITMulIC*)
2603 {
2604     VM* vm = &amp;exec-&gt;vm();
2605     NativeCallFrameTracer tracer(vm, exec);
2606 
2607     return unprofiledMul(exec, encodedOp1, encodedOp2);
2608 }
2609 
2610 EncodedJSValue JIT_OPERATION operationValueMulOptimize(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, JITMulIC* mulIC)
2611 {
2612     VM* vm = &amp;exec-&gt;vm();
2613     NativeCallFrameTracer tracer(vm, exec);
2614 
2615     auto nonOptimizeVariant = operationValueMulNoOptimize;
2616     if (ArithProfile* arithProfile = mulIC-&gt;arithProfile())
2617         arithProfile-&gt;observeLHSAndRHS(JSValue::decode(encodedOp1), JSValue::decode(encodedOp2));
2618     mulIC-&gt;generateOutOfLine(exec-&gt;codeBlock(), nonOptimizeVariant);
2619 
2620 #if ENABLE(MATH_IC_STATS)
2621     exec-&gt;codeBlock()-&gt;dumpMathICStats();
2622 #endif
2623 
2624     return unprofiledMul(exec, encodedOp1, encodedOp2);
2625 }
2626 
2627 EncodedJSValue JIT_OPERATION operationValueMulProfiled(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, ArithProfile* arithProfile)
2628 {
2629     VM* vm = &amp;exec-&gt;vm();
2630     NativeCallFrameTracer tracer(vm, exec);
2631 
2632     ASSERT(arithProfile);
2633     return profiledMul(exec, encodedOp1, encodedOp2, *arithProfile);
2634 }
2635 
2636 EncodedJSValue JIT_OPERATION operationValueMulProfiledOptimize(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, JITMulIC* mulIC)
2637 {
2638     VM* vm = &amp;exec-&gt;vm();
2639     NativeCallFrameTracer tracer(vm, exec);
2640 
2641     ArithProfile* arithProfile = mulIC-&gt;arithProfile();
2642     ASSERT(arithProfile);
2643     arithProfile-&gt;observeLHSAndRHS(JSValue::decode(encodedOp1), JSValue::decode(encodedOp2));
2644     auto nonOptimizeVariant = operationValueMulProfiledNoOptimize;
2645     mulIC-&gt;generateOutOfLine(exec-&gt;codeBlock(), nonOptimizeVariant);
2646 
2647 #if ENABLE(MATH_IC_STATS)
2648     exec-&gt;codeBlock()-&gt;dumpMathICStats();
2649 #endif
2650 
2651     return profiledMul(exec, encodedOp1, encodedOp2, *arithProfile, false);
2652 }
2653 
2654 EncodedJSValue JIT_OPERATION operationValueMulProfiledNoOptimize(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, JITMulIC* mulIC)
2655 {
2656     VM* vm = &amp;exec-&gt;vm();
2657     NativeCallFrameTracer tracer(vm, exec);
2658 
2659     ArithProfile* arithProfile = mulIC-&gt;arithProfile();
2660     ASSERT(arithProfile);
2661     return profiledMul(exec, encodedOp1, encodedOp2, *arithProfile);
2662 }
2663 
2664 ALWAYS_INLINE static EncodedJSValue unprofiledNegate(ExecState* exec, EncodedJSValue encodedOperand)
2665 {
2666     VM&amp; vm = exec-&gt;vm();
2667     auto scope = DECLARE_THROW_SCOPE(vm);
2668     NativeCallFrameTracer tracer(&amp;vm, exec);
2669 
2670     JSValue operand = JSValue::decode(encodedOperand);
2671 
2672     JSValue primValue = operand.toPrimitive(exec, PreferNumber);
2673     RETURN_IF_EXCEPTION(scope, encodedJSValue());
2674 
2675     if (primValue.isBigInt())
2676         return JSValue::encode(JSBigInt::unaryMinus(vm, asBigInt(primValue)));
2677 
2678     double number = primValue.toNumber(exec);
2679     RETURN_IF_EXCEPTION(scope, encodedJSValue());
2680     return JSValue::encode(jsNumber(-number));
2681 }
2682 
2683 ALWAYS_INLINE static EncodedJSValue profiledNegate(ExecState* exec, EncodedJSValue encodedOperand, ArithProfile&amp; arithProfile)
2684 {
2685     VM&amp; vm = exec-&gt;vm();
2686     auto scope = DECLARE_THROW_SCOPE(vm);
2687     NativeCallFrameTracer tracer(&amp;vm, exec);
2688 
2689     JSValue operand = JSValue::decode(encodedOperand);
2690     arithProfile.observeLHS(operand);
2691 
2692     JSValue primValue = operand.toPrimitive(exec);
2693     RETURN_IF_EXCEPTION(scope, encodedJSValue());
2694 
2695     if (primValue.isBigInt()) {
2696         JSBigInt* result = JSBigInt::unaryMinus(vm, asBigInt(primValue));
2697         arithProfile.observeResult(result);
2698 
2699         return JSValue::encode(result);
2700     }
2701 
2702     double number = primValue.toNumber(exec);
2703     RETURN_IF_EXCEPTION(scope, encodedJSValue());
2704     JSValue result = jsNumber(-number);
2705     arithProfile.observeResult(result);
2706     return JSValue::encode(result);
2707 }
2708 
2709 EncodedJSValue JIT_OPERATION operationArithNegate(ExecState* exec, EncodedJSValue operand)
2710 {
2711     return unprofiledNegate(exec, operand);
2712 }
2713 
2714 EncodedJSValue JIT_OPERATION operationArithNegateProfiled(ExecState* exec, EncodedJSValue operand, ArithProfile* arithProfile)
2715 {
2716     ASSERT(arithProfile);
2717     return profiledNegate(exec, operand, *arithProfile);
2718 }
2719 
2720 EncodedJSValue JIT_OPERATION operationArithNegateProfiledOptimize(ExecState* exec, EncodedJSValue encodedOperand, JITNegIC* negIC)
2721 {
2722     VM&amp; vm = exec-&gt;vm();
2723     auto scope = DECLARE_THROW_SCOPE(vm);
2724     NativeCallFrameTracer tracer(&amp;vm, exec);
2725 
2726     JSValue operand = JSValue::decode(encodedOperand);
2727 
2728     ArithProfile* arithProfile = negIC-&gt;arithProfile();
2729     ASSERT(arithProfile);
2730     arithProfile-&gt;observeLHS(operand);
2731     negIC-&gt;generateOutOfLine(exec-&gt;codeBlock(), operationArithNegateProfiled);
2732 
2733 #if ENABLE(MATH_IC_STATS)
2734     exec-&gt;codeBlock()-&gt;dumpMathICStats();
2735 #endif
2736 
2737     JSValue primValue = operand.toPrimitive(exec);
2738     RETURN_IF_EXCEPTION(scope, encodedJSValue());
2739 
2740     if (primValue.isBigInt()) {
2741         JSBigInt* result = JSBigInt::unaryMinus(vm, asBigInt(primValue));
2742         arithProfile-&gt;observeResult(result);
2743         return JSValue::encode(result);
2744     }
2745 
2746     double number = primValue.toNumber(exec);
2747     RETURN_IF_EXCEPTION(scope, encodedJSValue());
2748     JSValue result = jsNumber(-number);
2749     arithProfile-&gt;observeResult(result);
2750     return JSValue::encode(result);
2751 }
2752 
2753 EncodedJSValue JIT_OPERATION operationArithNegateOptimize(ExecState* exec, EncodedJSValue encodedOperand, JITNegIC* negIC)
2754 {
2755     VM&amp; vm = exec-&gt;vm();
2756     auto scope = DECLARE_THROW_SCOPE(vm);
2757     NativeCallFrameTracer tracer(&amp;vm, exec);
2758 
2759     JSValue operand = JSValue::decode(encodedOperand);
2760 
2761     if (ArithProfile* arithProfile = negIC-&gt;arithProfile())
2762         arithProfile-&gt;observeLHS(operand);
2763     negIC-&gt;generateOutOfLine(exec-&gt;codeBlock(), operationArithNegate);
2764 
2765 #if ENABLE(MATH_IC_STATS)
2766     exec-&gt;codeBlock()-&gt;dumpMathICStats();
2767 #endif
2768 
2769     JSValue primValue = operand.toPrimitive(exec);
2770     RETURN_IF_EXCEPTION(scope, encodedJSValue());
2771 
2772     if (primValue.isBigInt())
2773         return JSValue::encode(JSBigInt::unaryMinus(vm, asBigInt(primValue)));
2774 
2775     double number = primValue.toNumber(exec);
2776     RETURN_IF_EXCEPTION(scope, encodedJSValue());
2777     return JSValue::encode(jsNumber(-number));
2778 }
2779 
2780 ALWAYS_INLINE static EncodedJSValue unprofiledSub(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2)
2781 {
2782     JSValue op1 = JSValue::decode(encodedOp1);
2783     JSValue op2 = JSValue::decode(encodedOp2);
2784 
2785     return JSValue::encode(jsSub(exec, op1, op2));
2786 }
2787 
2788 ALWAYS_INLINE static EncodedJSValue profiledSub(VM&amp; vm, ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, ArithProfile&amp; arithProfile, bool shouldObserveLHSAndRHSTypes = true)
2789 {
2790     auto scope = DECLARE_THROW_SCOPE(vm);
2791 
2792     JSValue op1 = JSValue::decode(encodedOp1);
2793     JSValue op2 = JSValue::decode(encodedOp2);
2794 
2795     if (shouldObserveLHSAndRHSTypes)
2796         arithProfile.observeLHSAndRHS(op1, op2);
2797 
2798     JSValue result = jsSub(exec, op1, op2);
2799     RETURN_IF_EXCEPTION(scope, encodedJSValue());
2800     arithProfile.observeResult(result);
2801     return JSValue::encode(result);
2802 }
2803 
2804 EncodedJSValue JIT_OPERATION operationValueSub(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2)
2805 {
2806     VM* vm = &amp;exec-&gt;vm();
2807     NativeCallFrameTracer tracer(vm, exec);
2808     return unprofiledSub(exec, encodedOp1, encodedOp2);
2809 }
2810 
2811 EncodedJSValue JIT_OPERATION operationValueSubProfiled(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, ArithProfile* arithProfile)
2812 {
2813     ASSERT(arithProfile);
2814 
2815     VM* vm = &amp;exec-&gt;vm();
2816     NativeCallFrameTracer tracer(vm, exec);
2817 
2818     return profiledSub(*vm, exec, encodedOp1, encodedOp2, *arithProfile);
2819 }
2820 
2821 EncodedJSValue JIT_OPERATION operationValueSubOptimize(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, JITSubIC* subIC)
2822 {
2823     VM* vm = &amp;exec-&gt;vm();
2824     NativeCallFrameTracer tracer(vm, exec);
2825 
2826     auto nonOptimizeVariant = operationValueSubNoOptimize;
2827     if (ArithProfile* arithProfile = subIC-&gt;arithProfile())
2828         arithProfile-&gt;observeLHSAndRHS(JSValue::decode(encodedOp1), JSValue::decode(encodedOp2));
2829     subIC-&gt;generateOutOfLine(exec-&gt;codeBlock(), nonOptimizeVariant);
2830 
2831 #if ENABLE(MATH_IC_STATS)
2832     exec-&gt;codeBlock()-&gt;dumpMathICStats();
2833 #endif
2834 
2835     return unprofiledSub(exec, encodedOp1, encodedOp2);
2836 }
2837 
2838 EncodedJSValue JIT_OPERATION operationValueSubNoOptimize(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, JITSubIC*)
2839 {
2840     VM* vm = &amp;exec-&gt;vm();
2841     NativeCallFrameTracer tracer(vm, exec);
2842 
2843     return unprofiledSub(exec, encodedOp1, encodedOp2);
2844 }
2845 
2846 EncodedJSValue JIT_OPERATION operationValueSubProfiledOptimize(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, JITSubIC* subIC)
2847 {
2848     VM* vm = &amp;exec-&gt;vm();
2849     NativeCallFrameTracer tracer(vm, exec);
2850 
2851     ArithProfile* arithProfile = subIC-&gt;arithProfile();
2852     ASSERT(arithProfile);
2853     arithProfile-&gt;observeLHSAndRHS(JSValue::decode(encodedOp1), JSValue::decode(encodedOp2));
2854     auto nonOptimizeVariant = operationValueSubProfiledNoOptimize;
2855     subIC-&gt;generateOutOfLine(exec-&gt;codeBlock(), nonOptimizeVariant);
2856 
2857 #if ENABLE(MATH_IC_STATS)
2858     exec-&gt;codeBlock()-&gt;dumpMathICStats();
2859 #endif
2860 
2861     return profiledSub(*vm, exec, encodedOp1, encodedOp2, *arithProfile, false);
2862 }
2863 
2864 EncodedJSValue JIT_OPERATION operationValueSubProfiledNoOptimize(ExecState* exec, EncodedJSValue encodedOp1, EncodedJSValue encodedOp2, JITSubIC* subIC)
2865 {
2866     VM* vm = &amp;exec-&gt;vm();
2867     NativeCallFrameTracer tracer(vm, exec);
2868 
2869     ArithProfile* arithProfile = subIC-&gt;arithProfile();
2870     ASSERT(arithProfile);
2871     return profiledSub(*vm, exec, encodedOp1, encodedOp2, *arithProfile);
2872 }
2873 
2874 void JIT_OPERATION operationProcessTypeProfilerLog(ExecState* exec)
2875 {
2876     VM&amp; vm = exec-&gt;vm();
2877     NativeCallFrameTracer tracer(&amp;vm, exec);
2878     vm.typeProfilerLog()-&gt;processLogEntries(vm, &quot;Log Full, called from inside baseline JIT&quot;_s);
2879 }
2880 
2881 void JIT_OPERATION operationProcessShadowChickenLog(ExecState* exec)
2882 {
2883     VM&amp; vm = exec-&gt;vm();
2884     NativeCallFrameTracer tracer(&amp;vm, exec);
2885     RELEASE_ASSERT(vm.shadowChicken());
2886     vm.shadowChicken()-&gt;update(vm, exec);
2887 }
2888 
2889 int32_t JIT_OPERATION operationCheckIfExceptionIsUncatchableAndNotifyProfiler(ExecState* exec)
2890 {
2891     VM&amp; vm = exec-&gt;vm();
2892     NativeCallFrameTracer tracer(&amp;vm, exec);
2893     auto scope = DECLARE_THROW_SCOPE(vm);
2894     RELEASE_ASSERT(!!scope.exception());
2895 
2896     if (isTerminatedExecutionException(vm, scope.exception())) {
2897         genericUnwind(&amp;vm, exec);
2898         return 1;
2899     }
2900     return 0;
2901 }
2902 
2903 } // extern &quot;C&quot;
2904 
2905 } // namespace JSC
2906 
2907 #endif // ENABLE(JIT)
    </pre>
  </body>
</html>