<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JITPropertyAccess.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 
  28 #if ENABLE(JIT)
  29 #include &quot;JIT.h&quot;
  30 
  31 #include &quot;CodeBlock.h&quot;
  32 #include &quot;DirectArguments.h&quot;
  33 #include &quot;GCAwareJITStubRoutine.h&quot;
  34 #include &quot;GetterSetter.h&quot;
  35 #include &quot;InterpreterInlines.h&quot;
  36 #include &quot;JITInlines.h&quot;
  37 #include &quot;JSArray.h&quot;
  38 #include &quot;JSFunction.h&quot;
  39 #include &quot;JSLexicalEnvironment.h&quot;
  40 #include &quot;LinkBuffer.h&quot;
  41 #include &quot;OpcodeInlines.h&quot;
  42 #include &quot;ResultType.h&quot;
  43 #include &quot;ScopedArguments.h&quot;
  44 #include &quot;ScopedArgumentsTable.h&quot;
  45 #include &quot;SlowPathCall.h&quot;
  46 #include &quot;StructureStubInfo.h&quot;
  47 #include &lt;wtf/ScopedLambda.h&gt;
  48 #include &lt;wtf/StringPrintStream.h&gt;
  49 
  50 
  51 namespace JSC {
  52 #if USE(JSVALUE64)
  53 
  54 void JIT::emit_op_get_by_val(const Instruction* currentInstruction)
  55 {
  56     auto bytecode = currentInstruction-&gt;as&lt;OpGetByVal&gt;();
  57     auto&amp; metadata = bytecode.metadata(m_codeBlock);
  58     int dst = bytecode.m_dst.offset();
  59     int base = bytecode.m_base.offset();
  60     int property = bytecode.m_property.offset();
  61     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
  62     ByValInfo* byValInfo = m_codeBlock-&gt;addByValInfo();
  63 
  64     emitGetVirtualRegister(base, regT0);
  65     bool propertyNameIsIntegerConstant = isOperandConstantInt(property);
  66     if (propertyNameIsIntegerConstant)
  67         move(Imm32(getOperandConstantInt(property)), regT1);
  68     else
  69         emitGetVirtualRegister(property, regT1);
  70 
  71     emitJumpSlowCaseIfNotJSCell(regT0, base);
  72 
  73     PatchableJump notIndex;
  74     if (!propertyNameIsIntegerConstant) {
  75         notIndex = emitPatchableJumpIfNotInt(regT1);
  76         addSlowCase(notIndex);
  77 
  78         // This is technically incorrect - we&#39;re zero-extending an int32. On the hot path this doesn&#39;t matter.
  79         // We check the value as if it was a uint32 against the m_vectorLength - which will always fail if
  80         // number was signed since m_vectorLength is always less than intmax (since the total allocation
  81         // size is always less than 4Gb). As such zero extending will have been correct (and extending the value
  82         // to 64-bits is necessary since it&#39;s used in the address calculation). We zero extend rather than sign
  83         // extending since it makes it easier to re-tag the value in the slow case.
  84         zeroExtend32ToPtr(regT1, regT1);
  85     }
  86 
  87     emitArrayProfilingSiteWithCell(regT0, regT2, profile);
  88     and32(TrustedImm32(IndexingShapeMask), regT2);
  89 
  90     PatchableJump badType;
  91     JumpList slowCases;
  92 
  93     JITArrayMode mode = chooseArrayMode(profile);
  94     switch (mode) {
  95     case JITInt32:
  96         slowCases = emitInt32GetByVal(currentInstruction, badType);
  97         break;
  98     case JITDouble:
  99         slowCases = emitDoubleGetByVal(currentInstruction, badType);
 100         break;
 101     case JITContiguous:
 102         slowCases = emitContiguousGetByVal(currentInstruction, badType);
 103         break;
 104     case JITArrayStorage:
 105         slowCases = emitArrayStorageGetByVal(currentInstruction, badType);
 106         break;
 107     default:
 108         CRASH();
 109         break;
 110     }
 111 
 112     addSlowCase(badType);
 113     addSlowCase(slowCases);
 114 
 115     Label done = label();
 116 
 117     if (!ASSERT_DISABLED) {
 118         Jump resultOK = branchIfNotEmpty(regT0);
 119         abortWithReason(JITGetByValResultIsNotEmpty);
 120         resultOK.link(this);
 121     }
 122 
 123     emitValueProfilingSite(metadata);
 124     emitPutVirtualRegister(dst);
 125 
 126     Label nextHotPath = label();
 127 
 128     m_byValCompilationInfo.append(ByValCompilationInfo(byValInfo, m_bytecodeOffset, notIndex, badType, mode, profile, done, nextHotPath));
 129 }
 130 
 131 JITGetByIdGenerator JIT::emitGetByValWithCachedId(ByValInfo* byValInfo, OpGetByVal bytecode, const Identifier&amp; propertyName, Jump&amp; fastDoneCase, Jump&amp; slowDoneCase, JumpList&amp; slowCases)
 132 {
 133     // base: regT0
 134     // property: regT1
 135     // scratch: regT3
 136 
 137     int dst = bytecode.m_dst.offset();
 138 
 139     slowCases.append(branchIfNotCell(regT1));
 140     emitByValIdentifierCheck(byValInfo, regT1, regT3, propertyName, slowCases);
 141 
 142     JITGetByIdGenerator gen(
 143         m_codeBlock, CodeOrigin(m_bytecodeOffset), CallSiteIndex(m_bytecodeOffset), RegisterSet::stubUnavailableRegisters(),
 144         propertyName.impl(), JSValueRegs(regT0), JSValueRegs(regT0), AccessType::Get);
 145     gen.generateFastPath(*this);
 146 
 147     fastDoneCase = jump();
 148 
 149     Label coldPathBegin = label();
 150     gen.slowPathJump().link(this);
 151 
 152     Call call = callOperationWithProfile(bytecode.metadata(m_codeBlock), operationGetByIdOptimize, dst, gen.stubInfo(), regT0, propertyName.impl());
 153     gen.reportSlowPathCall(coldPathBegin, call);
 154     slowDoneCase = jump();
 155 
 156     return gen;
 157 }
 158 
 159 void JIT::emitSlow_op_get_by_val(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 160 {
 161     auto bytecode = currentInstruction-&gt;as&lt;OpGetByVal&gt;();
 162     int dst = bytecode.m_dst.offset();
 163     int base = bytecode.m_base.offset();
 164     int property = bytecode.m_property.offset();
 165     ByValInfo* byValInfo = m_byValCompilationInfo[m_byValInstructionIndex].byValInfo;
 166 
 167     linkSlowCaseIfNotJSCell(iter, base); // base cell check
 168 
 169     if (!isOperandConstantInt(property))
 170         linkSlowCase(iter); // property int32 check
 171     Jump nonCell = jump();
 172     linkSlowCase(iter); // base array check
 173     Jump notString = branchIfNotString(regT0);
 174     emitNakedCall(CodeLocationLabel&lt;NoPtrTag&gt;(m_vm-&gt;getCTIStub(stringGetByValGenerator).retaggedCode&lt;NoPtrTag&gt;()));
 175     Jump failed = branchTest64(Zero, regT0);
 176     emitPutVirtualRegister(dst, regT0);
 177     emitJumpSlowToHot(jump(), currentInstruction-&gt;size());
 178     failed.link(this);
 179     notString.link(this);
 180     nonCell.link(this);
 181 
 182     linkSlowCase(iter); // vector length check
 183     linkSlowCase(iter); // empty value
 184 
 185     Label slowPath = label();
 186 
 187     emitGetVirtualRegister(base, regT0);
 188     emitGetVirtualRegister(property, regT1);
 189     Call call = callOperation(operationGetByValOptimize, dst, regT0, regT1, byValInfo);
 190 
 191     m_byValCompilationInfo[m_byValInstructionIndex].slowPathTarget = slowPath;
 192     m_byValCompilationInfo[m_byValInstructionIndex].returnAddress = call;
 193     m_byValInstructionIndex++;
 194 
 195     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 196 }
 197 
 198 void JIT::emit_op_put_by_val_direct(const Instruction* currentInstruction)
 199 {
 200     emit_op_put_by_val&lt;OpPutByValDirect&gt;(currentInstruction);
 201 }
 202 
 203 template&lt;typename Op&gt;
 204 void JIT::emit_op_put_by_val(const Instruction* currentInstruction)
 205 {
 206     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
 207     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 208     int base = bytecode.m_base.offset();
 209     int property = bytecode.m_property.offset();
 210     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
 211     ByValInfo* byValInfo = m_codeBlock-&gt;addByValInfo();
 212 
 213     emitGetVirtualRegister(base, regT0);
 214     bool propertyNameIsIntegerConstant = isOperandConstantInt(property);
 215     if (propertyNameIsIntegerConstant)
 216         move(Imm32(getOperandConstantInt(property)), regT1);
 217     else
 218         emitGetVirtualRegister(property, regT1);
 219 
 220     emitJumpSlowCaseIfNotJSCell(regT0, base);
 221     PatchableJump notIndex;
 222     if (!propertyNameIsIntegerConstant) {
 223         notIndex = emitPatchableJumpIfNotInt(regT1);
 224         addSlowCase(notIndex);
 225         // See comment in op_get_by_val.
 226         zeroExtend32ToPtr(regT1, regT1);
 227     }
 228     emitArrayProfilingSiteWithCell(regT0, regT2, profile);
 229 
 230     PatchableJump badType;
 231     JumpList slowCases;
 232 
 233     // FIXME: Maybe we should do this inline?
 234     addSlowCase(branchTest32(NonZero, regT2, TrustedImm32(CopyOnWrite)));
 235     and32(TrustedImm32(IndexingShapeMask), regT2);
 236 
 237     JITArrayMode mode = chooseArrayMode(profile);
 238     switch (mode) {
 239     case JITInt32:
 240         slowCases = emitInt32PutByVal(bytecode, badType);
 241         break;
 242     case JITDouble:
 243         slowCases = emitDoublePutByVal(bytecode, badType);
 244         break;
 245     case JITContiguous:
 246         slowCases = emitContiguousPutByVal(bytecode, badType);
 247         break;
 248     case JITArrayStorage:
 249         slowCases = emitArrayStoragePutByVal(bytecode, badType);
 250         break;
 251     default:
 252         CRASH();
 253         break;
 254     }
 255 
 256     addSlowCase(badType);
 257     addSlowCase(slowCases);
 258 
 259     Label done = label();
 260 
 261     m_byValCompilationInfo.append(ByValCompilationInfo(byValInfo, m_bytecodeOffset, notIndex, badType, mode, profile, done, done));
 262 }
 263 
 264 template&lt;typename Op&gt;
 265 JIT::JumpList JIT::emitGenericContiguousPutByVal(Op bytecode, PatchableJump&amp; badType, IndexingType indexingShape)
 266 {
 267     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 268     int value = bytecode.m_value.offset();
 269     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
 270 
 271     JumpList slowCases;
 272 
 273     badType = patchableBranch32(NotEqual, regT2, TrustedImm32(indexingShape));
 274 
 275     loadPtr(Address(regT0, JSObject::butterflyOffset()), regT2);
 276     Jump outOfBounds = branch32(AboveOrEqual, regT1, Address(regT2, Butterfly::offsetOfPublicLength()));
 277 
 278     Label storeResult = label();
 279     emitGetVirtualRegister(value, regT3);
 280     switch (indexingShape) {
 281     case Int32Shape:
 282         slowCases.append(branchIfNotInt32(regT3));
 283         store64(regT3, BaseIndex(regT2, regT1, TimesEight));
 284         break;
 285     case DoubleShape: {
 286         Jump notInt = branchIfNotInt32(regT3);
 287         convertInt32ToDouble(regT3, fpRegT0);
 288         Jump ready = jump();
 289         notInt.link(this);
 290         add64(tagTypeNumberRegister, regT3);
 291         move64ToDouble(regT3, fpRegT0);
 292         slowCases.append(branchIfNaN(fpRegT0));
 293         ready.link(this);
 294         storeDouble(fpRegT0, BaseIndex(regT2, regT1, TimesEight));
 295         break;
 296     }
 297     case ContiguousShape:
 298         store64(regT3, BaseIndex(regT2, regT1, TimesEight));
 299         emitWriteBarrier(bytecode.m_base.offset(), value, ShouldFilterValue);
 300         break;
 301     default:
 302         CRASH();
 303         break;
 304     }
 305 
 306     Jump done = jump();
 307     outOfBounds.link(this);
 308 
 309     slowCases.append(branch32(AboveOrEqual, regT1, Address(regT2, Butterfly::offsetOfVectorLength())));
 310 
 311     emitArrayProfileStoreToHoleSpecialCase(profile);
 312 
 313     add32(TrustedImm32(1), regT1, regT3);
 314     store32(regT3, Address(regT2, Butterfly::offsetOfPublicLength()));
 315     jump().linkTo(storeResult, this);
 316 
 317     done.link(this);
 318 
 319     return slowCases;
 320 }
 321 
 322 template&lt;typename Op&gt;
 323 JIT::JumpList JIT::emitArrayStoragePutByVal(Op bytecode, PatchableJump&amp; badType)
 324 {
 325     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 326     int value = bytecode.m_value.offset();
 327     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
 328 
 329     JumpList slowCases;
 330 
 331     badType = patchableBranch32(NotEqual, regT2, TrustedImm32(ArrayStorageShape));
 332     loadPtr(Address(regT0, JSObject::butterflyOffset()), regT2);
 333     slowCases.append(branch32(AboveOrEqual, regT1, Address(regT2, ArrayStorage::vectorLengthOffset())));
 334 
 335     Jump empty = branchTest64(Zero, BaseIndex(regT2, regT1, TimesEight, ArrayStorage::vectorOffset()));
 336 
 337     Label storeResult(this);
 338     emitGetVirtualRegister(value, regT3);
 339     store64(regT3, BaseIndex(regT2, regT1, TimesEight, ArrayStorage::vectorOffset()));
 340     emitWriteBarrier(bytecode.m_base.offset(), value, ShouldFilterValue);
 341     Jump end = jump();
 342 
 343     empty.link(this);
 344     emitArrayProfileStoreToHoleSpecialCase(profile);
 345     add32(TrustedImm32(1), Address(regT2, ArrayStorage::numValuesInVectorOffset()));
 346     branch32(Below, regT1, Address(regT2, ArrayStorage::lengthOffset())).linkTo(storeResult, this);
 347 
 348     add32(TrustedImm32(1), regT1);
 349     store32(regT1, Address(regT2, ArrayStorage::lengthOffset()));
 350     sub32(TrustedImm32(1), regT1);
 351     jump().linkTo(storeResult, this);
 352 
 353     end.link(this);
 354 
 355     return slowCases;
 356 }
 357 
 358 template&lt;typename Op&gt;
 359 JITPutByIdGenerator JIT::emitPutByValWithCachedId(ByValInfo* byValInfo, Op bytecode, PutKind putKind, const Identifier&amp; propertyName, JumpList&amp; doneCases, JumpList&amp; slowCases)
 360 {
 361     // base: regT0
 362     // property: regT1
 363     // scratch: regT2
 364 
 365     int base = bytecode.m_base.offset();
 366     int value = bytecode.m_value.offset();
 367 
 368     slowCases.append(branchIfNotCell(regT1));
 369     emitByValIdentifierCheck(byValInfo, regT1, regT1, propertyName, slowCases);
 370 
 371     // Write barrier breaks the registers. So after issuing the write barrier,
 372     // reload the registers.
 373     emitGetVirtualRegisters(base, regT0, value, regT1);
 374 
 375     JITPutByIdGenerator gen(
 376         m_codeBlock, CodeOrigin(m_bytecodeOffset), CallSiteIndex(m_bytecodeOffset), RegisterSet::stubUnavailableRegisters(),
 377         JSValueRegs(regT0), JSValueRegs(regT1), regT2, m_codeBlock-&gt;ecmaMode(), putKind);
 378     gen.generateFastPath(*this);
 379     emitWriteBarrier(base, value, ShouldFilterBase);
 380     doneCases.append(jump());
 381 
 382     Label coldPathBegin = label();
 383     gen.slowPathJump().link(this);
 384 
 385     Call call = callOperation(gen.slowPathFunction(), gen.stubInfo(), regT1, regT0, propertyName.impl());
 386     gen.reportSlowPathCall(coldPathBegin, call);
 387     doneCases.append(jump());
 388 
 389     return gen;
 390 }
 391 
 392 void JIT::emitSlow_op_put_by_val(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 393 {
 394     bool isDirect = currentInstruction-&gt;opcodeID() == op_put_by_val_direct;
 395     int base;
 396     int property;
 397     int value;
 398 
 399     auto load = [&amp;](auto bytecode) {
 400         base = bytecode.m_base.offset();
 401         property = bytecode.m_property.offset();
 402         value = bytecode.m_value.offset();
 403     };
 404 
 405     if (isDirect)
 406         load(currentInstruction-&gt;as&lt;OpPutByValDirect&gt;());
 407     else
 408         load(currentInstruction-&gt;as&lt;OpPutByVal&gt;());
 409 
 410     ByValInfo* byValInfo = m_byValCompilationInfo[m_byValInstructionIndex].byValInfo;
 411 
 412     linkAllSlowCases(iter);
 413     Label slowPath = label();
 414 
 415     emitGetVirtualRegister(base, regT0);
 416     emitGetVirtualRegister(property, regT1);
 417     emitGetVirtualRegister(value, regT2);
 418     Call call = callOperation(isDirect ? operationDirectPutByValOptimize : operationPutByValOptimize, regT0, regT1, regT2, byValInfo);
 419 
 420     m_byValCompilationInfo[m_byValInstructionIndex].slowPathTarget = slowPath;
 421     m_byValCompilationInfo[m_byValInstructionIndex].returnAddress = call;
 422     m_byValInstructionIndex++;
 423 }
 424 
 425 void JIT::emit_op_put_getter_by_id(const Instruction* currentInstruction)
 426 {
 427     auto bytecode = currentInstruction-&gt;as&lt;OpPutGetterById&gt;();
 428     emitGetVirtualRegister(bytecode.m_base.offset(), regT0);
 429     int32_t options = bytecode.m_attributes;
 430     emitGetVirtualRegister(bytecode.m_accessor.offset(), regT1);
 431     callOperation(operationPutGetterById, regT0, m_codeBlock-&gt;identifier(bytecode.m_property).impl(), options, regT1);
 432 }
 433 
 434 void JIT::emit_op_put_setter_by_id(const Instruction* currentInstruction)
 435 {
 436     auto bytecode = currentInstruction-&gt;as&lt;OpPutSetterById&gt;();
 437     emitGetVirtualRegister(bytecode.m_base.offset(), regT0);
 438     int32_t options = bytecode.m_attributes;
 439     emitGetVirtualRegister(bytecode.m_accessor.offset(), regT1);
 440     callOperation(operationPutSetterById, regT0, m_codeBlock-&gt;identifier(bytecode.m_property).impl(), options, regT1);
 441 }
 442 
 443 void JIT::emit_op_put_getter_setter_by_id(const Instruction* currentInstruction)
 444 {
 445     auto bytecode = currentInstruction-&gt;as&lt;OpPutGetterSetterById&gt;();
 446     emitGetVirtualRegister(bytecode.m_base.offset(), regT0);
 447     int32_t attribute = bytecode.m_attributes;
 448     emitGetVirtualRegister(bytecode.m_getter.offset(), regT1);
 449     emitGetVirtualRegister(bytecode.m_setter.offset(), regT2);
 450     callOperation(operationPutGetterSetter, regT0, m_codeBlock-&gt;identifier(bytecode.m_property).impl(), attribute, regT1, regT2);
 451 }
 452 
 453 void JIT::emit_op_put_getter_by_val(const Instruction* currentInstruction)
 454 {
 455     auto bytecode = currentInstruction-&gt;as&lt;OpPutGetterByVal&gt;();
 456     emitGetVirtualRegister(bytecode.m_base.offset(), regT0);
 457     emitGetVirtualRegister(bytecode.m_property.offset(), regT1);
 458     int32_t attributes = bytecode.m_attributes;
 459     emitGetVirtualRegister(bytecode.m_accessor, regT2);
 460     callOperation(operationPutGetterByVal, regT0, regT1, attributes, regT2);
 461 }
 462 
 463 void JIT::emit_op_put_setter_by_val(const Instruction* currentInstruction)
 464 {
 465     auto bytecode = currentInstruction-&gt;as&lt;OpPutSetterByVal&gt;();
 466     emitGetVirtualRegister(bytecode.m_base.offset(), regT0);
 467     emitGetVirtualRegister(bytecode.m_property.offset(), regT1);
 468     int32_t attributes = bytecode.m_attributes;
 469     emitGetVirtualRegister(bytecode.m_accessor.offset(), regT2);
 470     callOperation(operationPutSetterByVal, regT0, regT1, attributes, regT2);
 471 }
 472 
 473 void JIT::emit_op_del_by_id(const Instruction* currentInstruction)
 474 {
 475     auto bytecode = currentInstruction-&gt;as&lt;OpDelById&gt;();
 476     int dst = bytecode.m_dst.offset();
 477     int base = bytecode.m_base.offset();
 478     int property = bytecode.m_property;
 479     emitGetVirtualRegister(base, regT0);
 480     callOperation(operationDeleteByIdJSResult, dst, regT0, m_codeBlock-&gt;identifier(property).impl());
 481 }
 482 
 483 void JIT::emit_op_del_by_val(const Instruction* currentInstruction)
 484 {
 485     auto bytecode = currentInstruction-&gt;as&lt;OpDelByVal&gt;();
 486     int dst = bytecode.m_dst.offset();
 487     int base = bytecode.m_base.offset();
 488     int property = bytecode.m_property.offset();
 489     emitGetVirtualRegister(base, regT0);
 490     emitGetVirtualRegister(property, regT1);
 491     callOperation(operationDeleteByValJSResult, dst, regT0, regT1);
 492 }
 493 
 494 void JIT::emit_op_try_get_by_id(const Instruction* currentInstruction)
 495 {
 496     auto bytecode = currentInstruction-&gt;as&lt;OpTryGetById&gt;();
 497     int resultVReg = bytecode.m_dst.offset();
 498     int baseVReg = bytecode.m_base.offset();
 499     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 500 
 501     emitGetVirtualRegister(baseVReg, regT0);
 502 
 503     emitJumpSlowCaseIfNotJSCell(regT0, baseVReg);
 504 
 505     JITGetByIdGenerator gen(
 506         m_codeBlock, CodeOrigin(m_bytecodeOffset), CallSiteIndex(m_bytecodeOffset), RegisterSet::stubUnavailableRegisters(),
 507         ident-&gt;impl(), JSValueRegs(regT0), JSValueRegs(regT0), AccessType::TryGet);
 508     gen.generateFastPath(*this);
 509     addSlowCase(gen.slowPathJump());
 510     m_getByIds.append(gen);
 511 
 512     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 513     emitPutVirtualRegister(resultVReg);
 514 }
 515 
 516 void JIT::emitSlow_op_try_get_by_id(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 517 {
 518     linkAllSlowCases(iter);
 519 
 520     auto bytecode = currentInstruction-&gt;as&lt;OpTryGetById&gt;();
 521     int resultVReg = bytecode.m_dst.offset();
 522     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 523 
 524     JITGetByIdGenerator&amp; gen = m_getByIds[m_getByIdIndex++];
 525 
 526     Label coldPathBegin = label();
 527 
 528     Call call = callOperation(operationTryGetByIdOptimize, resultVReg, gen.stubInfo(), regT0, ident-&gt;impl());
 529 
 530     gen.reportSlowPathCall(coldPathBegin, call);
 531 }
 532 
 533 void JIT::emit_op_get_by_id_direct(const Instruction* currentInstruction)
 534 {
 535     auto bytecode = currentInstruction-&gt;as&lt;OpGetByIdDirect&gt;();
 536     int resultVReg = bytecode.m_dst.offset();
 537     int baseVReg = bytecode.m_base.offset();
 538     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 539 
 540     emitGetVirtualRegister(baseVReg, regT0);
 541 
 542     emitJumpSlowCaseIfNotJSCell(regT0, baseVReg);
 543 
 544     JITGetByIdGenerator gen(
 545         m_codeBlock, CodeOrigin(m_bytecodeOffset), CallSiteIndex(m_bytecodeOffset), RegisterSet::stubUnavailableRegisters(),
 546         ident-&gt;impl(), JSValueRegs(regT0), JSValueRegs(regT0), AccessType::GetDirect);
 547     gen.generateFastPath(*this);
 548     addSlowCase(gen.slowPathJump());
 549     m_getByIds.append(gen);
 550 
 551     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 552     emitPutVirtualRegister(resultVReg);
 553 }
 554 
 555 void JIT::emitSlow_op_get_by_id_direct(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 556 {
 557     linkAllSlowCases(iter);
 558 
 559     auto bytecode = currentInstruction-&gt;as&lt;OpGetByIdDirect&gt;();
 560     int resultVReg = bytecode.m_dst.offset();
 561     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 562 
 563     JITGetByIdGenerator&amp; gen = m_getByIds[m_getByIdIndex++];
 564 
 565     Label coldPathBegin = label();
 566 
 567     Call call = callOperationWithProfile(bytecode.metadata(m_codeBlock), operationGetByIdDirectOptimize, resultVReg, gen.stubInfo(), regT0, ident-&gt;impl());
 568 
 569     gen.reportSlowPathCall(coldPathBegin, call);
 570 }
 571 
 572 void JIT::emit_op_get_by_id(const Instruction* currentInstruction)
 573 {
 574     auto bytecode = currentInstruction-&gt;as&lt;OpGetById&gt;();
 575     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 576     int resultVReg = bytecode.m_dst.offset();
 577     int baseVReg = bytecode.m_base.offset();
 578     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 579 
 580     emitGetVirtualRegister(baseVReg, regT0);
 581 
 582     emitJumpSlowCaseIfNotJSCell(regT0, baseVReg);
 583 
 584     if (*ident == m_vm-&gt;propertyNames-&gt;length &amp;&amp; shouldEmitProfiling()) {
 585         Jump notArrayLengthMode = branch8(NotEqual, AbsoluteAddress(&amp;metadata.m_mode), TrustedImm32(static_cast&lt;uint8_t&gt;(GetByIdMode::ArrayLength)));
 586         emitArrayProfilingSiteWithCell(regT0, regT1, &amp;metadata.m_modeMetadata.arrayLengthMode.arrayProfile);
 587         notArrayLengthMode.link(this);
 588     }
 589 
 590     JITGetByIdGenerator gen(
 591         m_codeBlock, CodeOrigin(m_bytecodeOffset), CallSiteIndex(m_bytecodeOffset), RegisterSet::stubUnavailableRegisters(),
 592         ident-&gt;impl(), JSValueRegs(regT0), JSValueRegs(regT0), AccessType::Get);
 593     gen.generateFastPath(*this);
 594     addSlowCase(gen.slowPathJump());
 595     m_getByIds.append(gen);
 596 
 597     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 598     emitPutVirtualRegister(resultVReg);
 599 }
 600 
 601 void JIT::emit_op_get_by_id_with_this(const Instruction* currentInstruction)
 602 {
 603     auto bytecode = currentInstruction-&gt;as&lt;OpGetByIdWithThis&gt;();
 604     int resultVReg = bytecode.m_dst.offset();
 605     int baseVReg = bytecode.m_base.offset();
 606     int thisVReg = bytecode.m_thisValue.offset();
 607     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 608 
 609     emitGetVirtualRegister(baseVReg, regT0);
 610     emitGetVirtualRegister(thisVReg, regT1);
 611     emitJumpSlowCaseIfNotJSCell(regT0, baseVReg);
 612     emitJumpSlowCaseIfNotJSCell(regT1, thisVReg);
 613 
 614     JITGetByIdWithThisGenerator gen(
 615         m_codeBlock, CodeOrigin(m_bytecodeOffset), CallSiteIndex(m_bytecodeOffset), RegisterSet::stubUnavailableRegisters(),
 616         ident-&gt;impl(), JSValueRegs(regT0), JSValueRegs(regT0), JSValueRegs(regT1), AccessType::GetWithThis);
 617     gen.generateFastPath(*this);
 618     addSlowCase(gen.slowPathJump());
 619     m_getByIdsWithThis.append(gen);
 620 
 621     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
 622     emitPutVirtualRegister(resultVReg);
 623 }
 624 
 625 void JIT::emitSlow_op_get_by_id(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 626 {
 627     linkAllSlowCases(iter);
 628 
 629     auto bytecode = currentInstruction-&gt;as&lt;OpGetById&gt;();
 630     int resultVReg = bytecode.m_dst.offset();
 631     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 632 
 633     JITGetByIdGenerator&amp; gen = m_getByIds[m_getByIdIndex++];
 634 
 635     Label coldPathBegin = label();
 636 
 637     Call call = callOperationWithProfile(bytecode.metadata(m_codeBlock), operationGetByIdOptimize, resultVReg, gen.stubInfo(), regT0, ident-&gt;impl());
 638 
 639     gen.reportSlowPathCall(coldPathBegin, call);
 640 }
 641 
 642 void JIT::emitSlow_op_get_by_id_with_this(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 643 {
 644     linkAllSlowCases(iter);
 645 
 646     auto bytecode = currentInstruction-&gt;as&lt;OpGetByIdWithThis&gt;();
 647     int resultVReg = bytecode.m_dst.offset();
 648     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 649 
 650     JITGetByIdWithThisGenerator&amp; gen = m_getByIdsWithThis[m_getByIdWithThisIndex++];
 651 
 652     Label coldPathBegin = label();
 653 
 654     Call call = callOperationWithProfile(bytecode.metadata(m_codeBlock), operationGetByIdWithThisOptimize, resultVReg, gen.stubInfo(), regT0, regT1, ident-&gt;impl());
 655 
 656     gen.reportSlowPathCall(coldPathBegin, call);
 657 }
 658 
 659 void JIT::emit_op_put_by_id(const Instruction* currentInstruction)
 660 {
 661     auto bytecode = currentInstruction-&gt;as&lt;OpPutById&gt;();
 662     int baseVReg = bytecode.m_base.offset();
 663     int valueVReg = bytecode.m_value.offset();
 664     bool direct = !!(bytecode.m_flags &amp; PutByIdIsDirect);
 665 
 666     // In order to be able to patch both the Structure, and the object offset, we store one pointer,
 667     // to just after the arguments have been loaded into registers &#39;hotPathBegin&#39;, and we generate code
 668     // such that the Structure &amp; offset are always at the same distance from this.
 669 
 670     emitGetVirtualRegisters(baseVReg, regT0, valueVReg, regT1);
 671 
 672     emitJumpSlowCaseIfNotJSCell(regT0, baseVReg);
 673 
 674     JITPutByIdGenerator gen(
 675         m_codeBlock, CodeOrigin(m_bytecodeOffset), CallSiteIndex(m_bytecodeOffset), RegisterSet::stubUnavailableRegisters(),
 676         JSValueRegs(regT0), JSValueRegs(regT1), regT2, m_codeBlock-&gt;ecmaMode(),
 677         direct ? Direct : NotDirect);
 678 
 679     gen.generateFastPath(*this);
 680     addSlowCase(gen.slowPathJump());
 681 
 682     emitWriteBarrier(baseVReg, valueVReg, ShouldFilterBase);
 683 
 684     m_putByIds.append(gen);
 685 }
 686 
 687 void JIT::emitSlow_op_put_by_id(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 688 {
 689     linkAllSlowCases(iter);
 690 
 691     auto bytecode = currentInstruction-&gt;as&lt;OpPutById&gt;();
 692     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 693 
 694     Label coldPathBegin(this);
 695 
 696     JITPutByIdGenerator&amp; gen = m_putByIds[m_putByIdIndex++];
 697 
 698     Call call = callOperation(gen.slowPathFunction(), gen.stubInfo(), regT1, regT0, ident-&gt;impl());
 699 
 700     gen.reportSlowPathCall(coldPathBegin, call);
 701 }
 702 
 703 void JIT::emit_op_in_by_id(const Instruction* currentInstruction)
 704 {
 705     auto bytecode = currentInstruction-&gt;as&lt;OpInById&gt;();
 706     int resultVReg = bytecode.m_dst.offset();
 707     int baseVReg = bytecode.m_base.offset();
 708     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 709 
 710     emitGetVirtualRegister(baseVReg, regT0);
 711 
 712     emitJumpSlowCaseIfNotJSCell(regT0, baseVReg);
 713 
 714     JITInByIdGenerator gen(
 715         m_codeBlock, CodeOrigin(m_bytecodeOffset), CallSiteIndex(m_bytecodeOffset), RegisterSet::stubUnavailableRegisters(),
 716         ident-&gt;impl(), JSValueRegs(regT0), JSValueRegs(regT0));
 717     gen.generateFastPath(*this);
 718     addSlowCase(gen.slowPathJump());
 719     m_inByIds.append(gen);
 720 
 721     emitPutVirtualRegister(resultVReg);
 722 }
 723 
 724 void JIT::emitSlow_op_in_by_id(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
 725 {
 726     linkAllSlowCases(iter);
 727 
 728     auto bytecode = currentInstruction-&gt;as&lt;OpInById&gt;();
 729     int resultVReg = bytecode.m_dst.offset();
 730     const Identifier* ident = &amp;(m_codeBlock-&gt;identifier(bytecode.m_property));
 731 
 732     JITInByIdGenerator&amp; gen = m_inByIds[m_inByIdIndex++];
 733 
 734     Label coldPathBegin = label();
 735 
 736     Call call = callOperation(operationInByIdOptimize, resultVReg, gen.stubInfo(), regT0, ident-&gt;impl());
 737 
 738     gen.reportSlowPathCall(coldPathBegin, call);
 739 }
 740 
 741 void JIT::emitVarInjectionCheck(bool needsVarInjectionChecks)
 742 {
 743     if (!needsVarInjectionChecks)
 744         return;
 745     addSlowCase(branch8(Equal, AbsoluteAddress(m_codeBlock-&gt;globalObject()-&gt;varInjectionWatchpoint()-&gt;addressOfState()), TrustedImm32(IsInvalidated)));
 746 }
 747 
 748 void JIT::emitResolveClosure(int dst, int scope, bool needsVarInjectionChecks, unsigned depth)
 749 {
 750     emitVarInjectionCheck(needsVarInjectionChecks);
 751     emitGetVirtualRegister(scope, regT0);
 752     for (unsigned i = 0; i &lt; depth; ++i)
 753         loadPtr(Address(regT0, JSScope::offsetOfNext()), regT0);
 754     emitPutVirtualRegister(dst);
 755 }
 756 
 757 void JIT::emit_op_resolve_scope(const Instruction* currentInstruction)
 758 {
 759     auto bytecode = currentInstruction-&gt;as&lt;OpResolveScope&gt;();
 760     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 761     int dst = bytecode.m_dst.offset();
 762     int scope = bytecode.m_scope.offset();
 763     ResolveType resolveType = metadata.m_resolveType;
 764     unsigned depth = metadata.m_localScopeDepth;
 765 
 766     auto emitCode = [&amp;] (ResolveType resolveType) {
 767         switch (resolveType) {
 768         case GlobalProperty:
 769         case GlobalPropertyWithVarInjectionChecks: {
 770             JSScope* constantScope = JSScope::constantScopeForCodeBlock(resolveType, m_codeBlock);
 771             RELEASE_ASSERT(constantScope);
 772             emitVarInjectionCheck(needsVarInjectionChecks(resolveType));
 773             load32(&amp;metadata.m_globalLexicalBindingEpoch, regT1);
 774             addSlowCase(branch32(NotEqual, AbsoluteAddress(m_codeBlock-&gt;globalObject()-&gt;addressOfGlobalLexicalBindingEpoch()), regT1));
 775             move(TrustedImmPtr(constantScope), regT0);
 776             emitPutVirtualRegister(dst);
 777             break;
 778         }
 779 
 780         case GlobalVar:
 781         case GlobalVarWithVarInjectionChecks:
 782         case GlobalLexicalVar:
 783         case GlobalLexicalVarWithVarInjectionChecks: {
 784             JSScope* constantScope = JSScope::constantScopeForCodeBlock(resolveType, m_codeBlock);
 785             RELEASE_ASSERT(constantScope);
 786             emitVarInjectionCheck(needsVarInjectionChecks(resolveType));
 787             move(TrustedImmPtr(constantScope), regT0);
 788             emitPutVirtualRegister(dst);
 789             break;
 790         }
 791         case ClosureVar:
 792         case ClosureVarWithVarInjectionChecks:
 793             emitResolveClosure(dst, scope, needsVarInjectionChecks(resolveType), depth);
 794             break;
 795         case ModuleVar:
 796             move(TrustedImmPtr(metadata.m_lexicalEnvironment.get()), regT0);
 797             emitPutVirtualRegister(dst);
 798             break;
 799         case Dynamic:
 800             addSlowCase(jump());
 801             break;
 802         case LocalClosureVar:
 803         case UnresolvedProperty:
 804         case UnresolvedPropertyWithVarInjectionChecks:
 805             RELEASE_ASSERT_NOT_REACHED();
 806         }
 807     };
 808 
 809     switch (resolveType) {
 810     case GlobalProperty:
 811     case GlobalPropertyWithVarInjectionChecks: {
 812         JumpList skipToEnd;
 813         load32(&amp;metadata.m_resolveType, regT0);
 814 
 815         Jump notGlobalProperty = branch32(NotEqual, regT0, TrustedImm32(resolveType));
 816         emitCode(resolveType);
 817         skipToEnd.append(jump());
 818 
 819         notGlobalProperty.link(this);
 820         emitCode(needsVarInjectionChecks(resolveType) ? GlobalLexicalVarWithVarInjectionChecks : GlobalLexicalVar);
 821 
 822         skipToEnd.link(this);
 823         break;
 824     }
 825     case UnresolvedProperty:
 826     case UnresolvedPropertyWithVarInjectionChecks: {
 827         JumpList skipToEnd;
 828         load32(&amp;metadata.m_resolveType, regT0);
 829 
 830         Jump notGlobalProperty = branch32(NotEqual, regT0, TrustedImm32(GlobalProperty));
 831         emitCode(GlobalProperty);
 832         skipToEnd.append(jump());
 833         notGlobalProperty.link(this);
 834 
 835         Jump notGlobalPropertyWithVarInjections = branch32(NotEqual, regT0, TrustedImm32(GlobalPropertyWithVarInjectionChecks));
 836         emitCode(GlobalPropertyWithVarInjectionChecks);
 837         skipToEnd.append(jump());
 838         notGlobalPropertyWithVarInjections.link(this);
 839 
 840         Jump notGlobalLexicalVar = branch32(NotEqual, regT0, TrustedImm32(GlobalLexicalVar));
 841         emitCode(GlobalLexicalVar);
 842         skipToEnd.append(jump());
 843         notGlobalLexicalVar.link(this);
 844 
 845         Jump notGlobalLexicalVarWithVarInjections = branch32(NotEqual, regT0, TrustedImm32(GlobalLexicalVarWithVarInjectionChecks));
 846         emitCode(GlobalLexicalVarWithVarInjectionChecks);
 847         skipToEnd.append(jump());
 848         notGlobalLexicalVarWithVarInjections.link(this);
 849 
 850         addSlowCase(jump());
 851         skipToEnd.link(this);
 852         break;
 853     }
 854 
 855     default:
 856         emitCode(resolveType);
 857         break;
 858     }
 859 }
 860 
 861 void JIT::emitLoadWithStructureCheck(int scope, Structure** structureSlot)
 862 {
 863     loadPtr(structureSlot, regT1);
 864     emitGetVirtualRegister(scope, regT0);
 865     addSlowCase(branchTestPtr(Zero, regT1));
 866     load32(Address(regT1, Structure::structureIDOffset()), regT1);
 867     addSlowCase(branch32(NotEqual, Address(regT0, JSCell::structureIDOffset()), regT1));
 868 }
 869 
 870 void JIT::emitGetVarFromPointer(JSValue* operand, GPRReg reg)
 871 {
 872     loadPtr(operand, reg);
 873 }
 874 
 875 void JIT::emitGetVarFromIndirectPointer(JSValue** operand, GPRReg reg)
 876 {
 877     loadPtr(operand, reg);
 878     loadPtr(reg, reg);
 879 }
 880 
 881 void JIT::emitGetClosureVar(int scope, uintptr_t operand)
 882 {
 883     emitGetVirtualRegister(scope, regT0);
 884     loadPtr(Address(regT0, JSLexicalEnvironment::offsetOfVariables() + operand * sizeof(Register)), regT0);
 885 }
 886 
 887 void JIT::emit_op_get_from_scope(const Instruction* currentInstruction)
 888 {
 889     auto bytecode = currentInstruction-&gt;as&lt;OpGetFromScope&gt;();
 890     auto&amp; metadata = bytecode.metadata(m_codeBlock);
 891     int dst = bytecode.m_dst.offset();
 892     int scope = bytecode.m_scope.offset();
 893     ResolveType resolveType = metadata.m_getPutInfo.resolveType();
 894     Structure** structureSlot = metadata.m_structure.slot();
 895     uintptr_t* operandSlot = reinterpret_cast&lt;uintptr_t*&gt;(&amp;metadata.m_operand);
 896 
 897     auto emitCode = [&amp;] (ResolveType resolveType, bool indirectLoadForOperand) {
 898         switch (resolveType) {
 899         case GlobalProperty:
 900         case GlobalPropertyWithVarInjectionChecks: {
 901             emitLoadWithStructureCheck(scope, structureSlot); // Structure check covers var injection since we don&#39;t cache structures for anything but the GlobalObject. Additionally, resolve_scope handles checking for the var injection.
 902             GPRReg base = regT0;
 903             GPRReg result = regT0;
 904             GPRReg offset = regT1;
 905             GPRReg scratch = regT2;
 906 
 907             jitAssert(scopedLambda&lt;Jump(void)&gt;([&amp;] () -&gt; Jump {
 908                 return branchPtr(Equal, base, TrustedImmPtr(m_codeBlock-&gt;globalObject()));
 909             }));
 910 
 911             load32(operandSlot, offset);
 912             if (!ASSERT_DISABLED) {
 913                 Jump isOutOfLine = branch32(GreaterThanOrEqual, offset, TrustedImm32(firstOutOfLineOffset));
 914                 abortWithReason(JITOffsetIsNotOutOfLine);
 915                 isOutOfLine.link(this);
 916             }
 917             loadPtr(Address(base, JSObject::butterflyOffset()), scratch);
 918             neg32(offset);
 919             signExtend32ToPtr(offset, offset);
 920             load64(BaseIndex(scratch, offset, TimesEight, (firstOutOfLineOffset - 2) * sizeof(EncodedJSValue)), result);
 921             break;
 922         }
 923         case GlobalVar:
 924         case GlobalVarWithVarInjectionChecks:
 925         case GlobalLexicalVar:
 926         case GlobalLexicalVarWithVarInjectionChecks:
 927             emitVarInjectionCheck(needsVarInjectionChecks(resolveType));
 928             if (indirectLoadForOperand)
 929                 emitGetVarFromIndirectPointer(bitwise_cast&lt;JSValue**&gt;(operandSlot), regT0);
 930             else
 931                 emitGetVarFromPointer(bitwise_cast&lt;JSValue*&gt;(*operandSlot), regT0);
 932             if (resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks) // TDZ check.
 933                 addSlowCase(branchIfEmpty(regT0));
 934             break;
 935         case ClosureVar:
 936         case ClosureVarWithVarInjectionChecks:
 937             emitVarInjectionCheck(needsVarInjectionChecks(resolveType));
 938             emitGetClosureVar(scope, *operandSlot);
 939             break;
 940         case Dynamic:
 941             addSlowCase(jump());
 942             break;
 943         case LocalClosureVar:
 944         case ModuleVar:
 945         case UnresolvedProperty:
 946         case UnresolvedPropertyWithVarInjectionChecks:
 947             RELEASE_ASSERT_NOT_REACHED();
 948         }
 949     };
 950 
 951     switch (resolveType) {
 952     case GlobalProperty:
 953     case GlobalPropertyWithVarInjectionChecks: {
 954         JumpList skipToEnd;
 955         load32(&amp;metadata.m_getPutInfo, regT0);
 956         and32(TrustedImm32(GetPutInfo::typeBits), regT0); // Load ResolveType into T0
 957 
 958         Jump isNotGlobalProperty = branch32(NotEqual, regT0, TrustedImm32(resolveType));
 959         emitCode(resolveType, false);
 960         skipToEnd.append(jump());
 961 
 962         isNotGlobalProperty.link(this);
 963         emitCode(needsVarInjectionChecks(resolveType) ? GlobalLexicalVarWithVarInjectionChecks : GlobalLexicalVar, true);
 964 
 965         skipToEnd.link(this);
 966         break;
 967     }
 968     case UnresolvedProperty:
 969     case UnresolvedPropertyWithVarInjectionChecks: {
 970         JumpList skipToEnd;
 971         load32(&amp;metadata.m_getPutInfo, regT0);
 972         and32(TrustedImm32(GetPutInfo::typeBits), regT0); // Load ResolveType into T0
 973 
 974         Jump isGlobalProperty = branch32(Equal, regT0, TrustedImm32(GlobalProperty));
 975         Jump notGlobalPropertyWithVarInjections = branch32(NotEqual, regT0, TrustedImm32(GlobalPropertyWithVarInjectionChecks));
 976         isGlobalProperty.link(this);
 977         emitCode(GlobalProperty, false);
 978         skipToEnd.append(jump());
 979         notGlobalPropertyWithVarInjections.link(this);
 980 
 981         Jump notGlobalLexicalVar = branch32(NotEqual, regT0, TrustedImm32(GlobalLexicalVar));
 982         emitCode(GlobalLexicalVar, true);
 983         skipToEnd.append(jump());
 984         notGlobalLexicalVar.link(this);
 985 
 986         Jump notGlobalLexicalVarWithVarInjections = branch32(NotEqual, regT0, TrustedImm32(GlobalLexicalVarWithVarInjectionChecks));
 987         emitCode(GlobalLexicalVarWithVarInjectionChecks, true);
 988         skipToEnd.append(jump());
 989         notGlobalLexicalVarWithVarInjections.link(this);
 990 
 991         addSlowCase(jump());
 992 
 993         skipToEnd.link(this);
 994         break;
 995     }
 996 
 997     default:
 998         emitCode(resolveType, false);
 999         break;
1000     }
1001     emitPutVirtualRegister(dst);
1002     emitValueProfilingSite(metadata);
1003 }
1004 
1005 void JIT::emitSlow_op_get_from_scope(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1006 {
1007     linkAllSlowCases(iter);
1008 
1009     auto bytecode = currentInstruction-&gt;as&lt;OpGetFromScope&gt;();
1010     int dst = bytecode.m_dst.offset();
1011     callOperationWithProfile(bytecode.metadata(m_codeBlock), operationGetFromScope, dst, currentInstruction);
1012 }
1013 
1014 void JIT::emitPutGlobalVariable(JSValue* operand, int value, WatchpointSet* set)
1015 {
1016     emitGetVirtualRegister(value, regT0);
1017     emitNotifyWrite(set);
1018     storePtr(regT0, operand);
1019 }
1020 void JIT::emitPutGlobalVariableIndirect(JSValue** addressOfOperand, int value, WatchpointSet** indirectWatchpointSet)
1021 {
1022     emitGetVirtualRegister(value, regT0);
1023     loadPtr(indirectWatchpointSet, regT1);
1024     emitNotifyWrite(regT1);
1025     loadPtr(addressOfOperand, regT1);
1026     storePtr(regT0, regT1);
1027 }
1028 
1029 void JIT::emitPutClosureVar(int scope, uintptr_t operand, int value, WatchpointSet* set)
1030 {
1031     emitGetVirtualRegister(value, regT1);
1032     emitGetVirtualRegister(scope, regT0);
1033     emitNotifyWrite(set);
1034     storePtr(regT1, Address(regT0, JSLexicalEnvironment::offsetOfVariables() + operand * sizeof(Register)));
1035 }
1036 
1037 void JIT::emit_op_put_to_scope(const Instruction* currentInstruction)
1038 {
1039     auto bytecode = currentInstruction-&gt;as&lt;OpPutToScope&gt;();
1040     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1041     int scope = bytecode.m_scope.offset();
1042     int value = bytecode.m_value.offset();
1043     GetPutInfo getPutInfo = copiedGetPutInfo(bytecode);
1044     ResolveType resolveType = getPutInfo.resolveType();
1045     Structure** structureSlot = metadata.m_structure.slot();
1046     uintptr_t* operandSlot = reinterpret_cast&lt;uintptr_t*&gt;(&amp;metadata.m_operand);
1047 
1048     auto emitCode = [&amp;] (ResolveType resolveType, bool indirectLoadForOperand) {
1049         switch (resolveType) {
1050         case GlobalProperty:
1051         case GlobalPropertyWithVarInjectionChecks: {
1052             emitLoadWithStructureCheck(scope, structureSlot); // Structure check covers var injection since we don&#39;t cache structures for anything but the GlobalObject. Additionally, resolve_scope handles checking for the var injection.
1053             emitGetVirtualRegister(value, regT2);
1054 
1055             jitAssert(scopedLambda&lt;Jump(void)&gt;([&amp;] () -&gt; Jump {
1056                 return branchPtr(Equal, regT0, TrustedImmPtr(m_codeBlock-&gt;globalObject()));
1057             }));
1058 
1059             loadPtr(Address(regT0, JSObject::butterflyOffset()), regT0);
1060             loadPtr(operandSlot, regT1);
1061             negPtr(regT1);
1062             storePtr(regT2, BaseIndex(regT0, regT1, TimesEight, (firstOutOfLineOffset - 2) * sizeof(EncodedJSValue)));
1063             emitWriteBarrier(m_codeBlock-&gt;globalObject(), value, ShouldFilterValue);
1064             break;
1065         }
1066         case GlobalVar:
1067         case GlobalVarWithVarInjectionChecks:
1068         case GlobalLexicalVar:
1069         case GlobalLexicalVarWithVarInjectionChecks: {
1070             JSScope* constantScope = JSScope::constantScopeForCodeBlock(resolveType, m_codeBlock);
1071             RELEASE_ASSERT(constantScope);
1072             emitVarInjectionCheck(needsVarInjectionChecks(resolveType));
1073             if (!isInitialization(getPutInfo.initializationMode()) &amp;&amp; (resolveType == GlobalLexicalVar || resolveType == GlobalLexicalVarWithVarInjectionChecks)) {
1074                 // We need to do a TDZ check here because we can&#39;t always prove we need to emit TDZ checks statically.
1075                 if (indirectLoadForOperand)
1076                     emitGetVarFromIndirectPointer(bitwise_cast&lt;JSValue**&gt;(operandSlot), regT0);
1077                 else
1078                     emitGetVarFromPointer(bitwise_cast&lt;JSValue*&gt;(*operandSlot), regT0);
1079                 addSlowCase(branchIfEmpty(regT0));
1080             }
1081             if (indirectLoadForOperand)
1082                 emitPutGlobalVariableIndirect(bitwise_cast&lt;JSValue**&gt;(operandSlot), value, &amp;metadata.m_watchpointSet);
1083             else
1084                 emitPutGlobalVariable(bitwise_cast&lt;JSValue*&gt;(*operandSlot), value, metadata.m_watchpointSet);
1085             emitWriteBarrier(constantScope, value, ShouldFilterValue);
1086             break;
1087         }
1088         case LocalClosureVar:
1089         case ClosureVar:
1090         case ClosureVarWithVarInjectionChecks:
1091             emitVarInjectionCheck(needsVarInjectionChecks(resolveType));
1092             emitPutClosureVar(scope, *operandSlot, value, metadata.m_watchpointSet);
1093             emitWriteBarrier(scope, value, ShouldFilterValue);
1094             break;
1095         case ModuleVar:
1096         case Dynamic:
1097             addSlowCase(jump());
1098             break;
1099         case UnresolvedProperty:
1100         case UnresolvedPropertyWithVarInjectionChecks:
1101             RELEASE_ASSERT_NOT_REACHED();
1102             break;
1103         }
1104     };
1105 
1106     switch (resolveType) {
1107     case GlobalProperty:
1108     case GlobalPropertyWithVarInjectionChecks: {
1109         JumpList skipToEnd;
1110         load32(&amp;metadata.m_getPutInfo, regT0);
1111         and32(TrustedImm32(GetPutInfo::typeBits), regT0); // Load ResolveType into T0
1112 
1113         Jump isGlobalProperty = branch32(Equal, regT0, TrustedImm32(resolveType));
1114         Jump isGlobalLexicalVar = branch32(Equal, regT0, TrustedImm32(needsVarInjectionChecks(resolveType) ? GlobalLexicalVarWithVarInjectionChecks : GlobalLexicalVar));
1115         addSlowCase(jump()); // Dynamic, it can happen if we attempt to put a value to already-initialized const binding.
1116 
1117         isGlobalLexicalVar.link(this);
1118         emitCode(needsVarInjectionChecks(resolveType) ? GlobalLexicalVarWithVarInjectionChecks : GlobalLexicalVar, true);
1119         skipToEnd.append(jump());
1120 
1121         isGlobalProperty.link(this);
1122         emitCode(resolveType, false);
1123         skipToEnd.link(this);
1124         break;
1125     }
1126     case UnresolvedProperty:
1127     case UnresolvedPropertyWithVarInjectionChecks: {
1128         JumpList skipToEnd;
1129         load32(&amp;metadata.m_getPutInfo, regT0);
1130         and32(TrustedImm32(GetPutInfo::typeBits), regT0); // Load ResolveType into T0
1131 
1132         Jump isGlobalProperty = branch32(Equal, regT0, TrustedImm32(GlobalProperty));
1133         Jump notGlobalPropertyWithVarInjections = branch32(NotEqual, regT0, TrustedImm32(GlobalPropertyWithVarInjectionChecks));
1134         isGlobalProperty.link(this);
1135         emitCode(GlobalProperty, false);
1136         skipToEnd.append(jump());
1137         notGlobalPropertyWithVarInjections.link(this);
1138 
1139         Jump notGlobalLexicalVar = branch32(NotEqual, regT0, TrustedImm32(GlobalLexicalVar));
1140         emitCode(GlobalLexicalVar, true);
1141         skipToEnd.append(jump());
1142         notGlobalLexicalVar.link(this);
1143 
1144         Jump notGlobalLexicalVarWithVarInjections = branch32(NotEqual, regT0, TrustedImm32(GlobalLexicalVarWithVarInjectionChecks));
1145         emitCode(GlobalLexicalVarWithVarInjectionChecks, true);
1146         skipToEnd.append(jump());
1147         notGlobalLexicalVarWithVarInjections.link(this);
1148 
1149         addSlowCase(jump());
1150 
1151         skipToEnd.link(this);
1152         break;
1153     }
1154 
1155     default:
1156         emitCode(resolveType, false);
1157         break;
1158     }
1159 }
1160 
1161 void JIT::emitSlow_op_put_to_scope(const Instruction* currentInstruction, Vector&lt;SlowCaseEntry&gt;::iterator&amp; iter)
1162 {
1163     linkAllSlowCases(iter);
1164 
1165     auto bytecode = currentInstruction-&gt;as&lt;OpPutToScope&gt;();
1166     ResolveType resolveType = copiedGetPutInfo(bytecode).resolveType();
1167     if (resolveType == ModuleVar) {
1168         JITSlowPathCall slowPathCall(this, currentInstruction, slow_path_throw_strict_mode_readonly_property_write_error);
1169         slowPathCall.call();
1170     } else
1171         callOperation(operationPutToScope, currentInstruction);
1172 }
1173 
1174 void JIT::emit_op_get_from_arguments(const Instruction* currentInstruction)
1175 {
1176     auto bytecode = currentInstruction-&gt;as&lt;OpGetFromArguments&gt;();
1177     int dst = bytecode.m_dst.offset();
1178     int arguments = bytecode.m_arguments.offset();
1179     int index = bytecode.m_index;
1180 
1181     emitGetVirtualRegister(arguments, regT0);
1182     load64(Address(regT0, DirectArguments::storageOffset() + index * sizeof(WriteBarrier&lt;Unknown&gt;)), regT0);
1183     emitValueProfilingSite(bytecode.metadata(m_codeBlock));
1184     emitPutVirtualRegister(dst);
1185 }
1186 
1187 void JIT::emit_op_put_to_arguments(const Instruction* currentInstruction)
1188 {
1189     auto bytecode = currentInstruction-&gt;as&lt;OpPutToArguments&gt;();
1190     int arguments = bytecode.m_arguments.offset();
1191     int index = bytecode.m_index;
1192     int value = bytecode.m_value.offset();
1193 
1194     emitGetVirtualRegister(arguments, regT0);
1195     emitGetVirtualRegister(value, regT1);
1196     store64(regT1, Address(regT0, DirectArguments::storageOffset() + index * sizeof(WriteBarrier&lt;Unknown&gt;)));
1197 
1198     emitWriteBarrier(arguments, value, ShouldFilterValue);
1199 }
1200 
1201 void JIT::emitWriteBarrier(unsigned owner, unsigned value, WriteBarrierMode mode)
1202 {
1203     Jump valueNotCell;
1204     if (mode == ShouldFilterValue || mode == ShouldFilterBaseAndValue) {
1205         emitGetVirtualRegister(value, regT0);
1206         valueNotCell = branchIfNotCell(regT0);
1207     }
1208 
1209     emitGetVirtualRegister(owner, regT0);
1210     Jump ownerNotCell;
1211     if (mode == ShouldFilterBaseAndValue || mode == ShouldFilterBase)
1212         ownerNotCell = branchIfNotCell(regT0);
1213 
1214     Jump ownerIsRememberedOrInEden = barrierBranch(*vm(), regT0, regT1);
1215     callOperation(operationWriteBarrierSlowPath, regT0);
1216     ownerIsRememberedOrInEden.link(this);
1217 
1218     if (mode == ShouldFilterBaseAndValue || mode == ShouldFilterBase)
1219         ownerNotCell.link(this);
1220     if (mode == ShouldFilterValue || mode == ShouldFilterBaseAndValue)
1221         valueNotCell.link(this);
1222 }
1223 
1224 void JIT::emitWriteBarrier(JSCell* owner, unsigned value, WriteBarrierMode mode)
1225 {
1226     emitGetVirtualRegister(value, regT0);
1227     Jump valueNotCell;
1228     if (mode == ShouldFilterValue)
1229         valueNotCell = branchIfNotCell(regT0);
1230 
1231     emitWriteBarrier(owner);
1232 
1233     if (mode == ShouldFilterValue)
1234         valueNotCell.link(this);
1235 }
1236 
1237 #else // USE(JSVALUE64)
1238 
1239 void JIT::emitWriteBarrier(unsigned owner, unsigned value, WriteBarrierMode mode)
1240 {
1241     Jump valueNotCell;
1242     if (mode == ShouldFilterValue || mode == ShouldFilterBaseAndValue) {
1243         emitLoadTag(value, regT0);
1244         valueNotCell = branchIfNotCell(regT0);
1245     }
1246 
1247     emitLoad(owner, regT0, regT1);
1248     Jump ownerNotCell;
1249     if (mode == ShouldFilterBase || mode == ShouldFilterBaseAndValue)
1250         ownerNotCell = branchIfNotCell(regT0);
1251 
1252     Jump ownerIsRememberedOrInEden = barrierBranch(*vm(), regT1, regT2);
1253     callOperation(operationWriteBarrierSlowPath, regT1);
1254     ownerIsRememberedOrInEden.link(this);
1255 
1256     if (mode == ShouldFilterBase || mode == ShouldFilterBaseAndValue)
1257         ownerNotCell.link(this);
1258     if (mode == ShouldFilterValue || mode == ShouldFilterBaseAndValue)
1259         valueNotCell.link(this);
1260 }
1261 
1262 void JIT::emitWriteBarrier(JSCell* owner, unsigned value, WriteBarrierMode mode)
1263 {
1264     Jump valueNotCell;
1265     if (mode == ShouldFilterValue) {
1266         emitLoadTag(value, regT0);
1267         valueNotCell = branchIfNotCell(regT0);
1268     }
1269 
1270     emitWriteBarrier(owner);
1271 
1272     if (mode == ShouldFilterValue)
1273         valueNotCell.link(this);
1274 }
1275 
1276 #endif // USE(JSVALUE64)
1277 
1278 void JIT::emitWriteBarrier(JSCell* owner)
1279 {
1280     Jump ownerIsRememberedOrInEden = barrierBranch(*vm(), owner, regT0);
1281     callOperation(operationWriteBarrierSlowPath, owner);
1282     ownerIsRememberedOrInEden.link(this);
1283 }
1284 
1285 void JIT::emitByValIdentifierCheck(ByValInfo* byValInfo, RegisterID cell, RegisterID scratch, const Identifier&amp; propertyName, JumpList&amp; slowCases)
1286 {
1287     if (propertyName.isSymbol())
1288         slowCases.append(branchPtr(NotEqual, cell, TrustedImmPtr(byValInfo-&gt;cachedSymbol.get())));
1289     else {
1290         slowCases.append(branchIfNotString(cell));
1291         loadPtr(Address(cell, JSString::offsetOfValue()), scratch);
1292         slowCases.append(branchPtr(NotEqual, scratch, TrustedImmPtr(propertyName.impl())));
1293     }
1294 }
1295 
1296 void JIT::privateCompileGetByVal(const ConcurrentJSLocker&amp;, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
1297 {
1298     const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(byValInfo-&gt;bytecodeIndex).ptr();
1299 
1300     PatchableJump badType;
1301     JumpList slowCases;
1302 
1303     switch (arrayMode) {
1304     case JITInt32:
1305         slowCases = emitInt32GetByVal(currentInstruction, badType);
1306         break;
1307     case JITDouble:
1308         slowCases = emitDoubleGetByVal(currentInstruction, badType);
1309         break;
1310     case JITContiguous:
1311         slowCases = emitContiguousGetByVal(currentInstruction, badType);
1312         break;
1313     case JITArrayStorage:
1314         slowCases = emitArrayStorageGetByVal(currentInstruction, badType);
1315         break;
1316     case JITDirectArguments:
1317         slowCases = emitDirectArgumentsGetByVal(currentInstruction, badType);
1318         break;
1319     case JITScopedArguments:
1320         slowCases = emitScopedArgumentsGetByVal(currentInstruction, badType);
1321         break;
1322     default:
1323         TypedArrayType type = typedArrayTypeForJITArrayMode(arrayMode);
1324         if (isInt(type))
1325             slowCases = emitIntTypedArrayGetByVal(currentInstruction, badType, type);
1326         else
1327             slowCases = emitFloatTypedArrayGetByVal(currentInstruction, badType, type);
1328         break;
1329     }
1330 
1331     Jump done = jump();
1332 
1333     LinkBuffer patchBuffer(*this, m_codeBlock);
1334 
1335     patchBuffer.link(badType, byValInfo-&gt;slowPathTarget);
1336     patchBuffer.link(slowCases, byValInfo-&gt;slowPathTarget);
1337 
1338     patchBuffer.link(done, byValInfo-&gt;badTypeDoneTarget);
1339 
1340     byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1341         m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1342         &quot;Baseline get_by_val stub for %s, return point %p&quot;, toCString(*m_codeBlock).data(), returnAddress.value());
1343 
1344     MacroAssembler::repatchJump(byValInfo-&gt;badTypeJump, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(byValInfo-&gt;stubRoutine-&gt;code().code()));
1345     MacroAssembler::repatchCall(CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)), FunctionPtr&lt;OperationPtrTag&gt;(operationGetByValGeneric));
1346 }
1347 
1348 void JIT::privateCompileGetByValWithCachedId(ByValInfo* byValInfo, ReturnAddressPtr returnAddress, const Identifier&amp; propertyName)
1349 {
1350     const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(byValInfo-&gt;bytecodeIndex).ptr();
1351     auto bytecode = currentInstruction-&gt;as&lt;OpGetByVal&gt;();
1352 
1353     Jump fastDoneCase;
1354     Jump slowDoneCase;
1355     JumpList slowCases;
1356 
1357     JITGetByIdGenerator gen = emitGetByValWithCachedId(byValInfo, bytecode, propertyName, fastDoneCase, slowDoneCase, slowCases);
1358 
1359     ConcurrentJSLocker locker(m_codeBlock-&gt;m_lock);
1360     LinkBuffer patchBuffer(*this, m_codeBlock);
1361     patchBuffer.link(slowCases, byValInfo-&gt;slowPathTarget);
1362     patchBuffer.link(fastDoneCase, byValInfo-&gt;badTypeDoneTarget);
1363     patchBuffer.link(slowDoneCase, byValInfo-&gt;badTypeNextHotPathTarget);
1364     if (!m_exceptionChecks.empty())
1365         patchBuffer.link(m_exceptionChecks, byValInfo-&gt;exceptionHandler);
1366 
1367     for (const auto&amp; callSite : m_calls) {
1368         if (callSite.callee)
1369             patchBuffer.link(callSite.from, callSite.callee);
1370     }
1371     gen.finalize(patchBuffer, patchBuffer);
1372 
1373     byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1374         m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1375         &quot;Baseline get_by_val with cached property name &#39;%s&#39; stub for %s, return point %p&quot;, propertyName.impl()-&gt;utf8().data(), toCString(*m_codeBlock).data(), returnAddress.value());
1376     byValInfo-&gt;stubInfo = gen.stubInfo();
1377 
1378     MacroAssembler::repatchJump(byValInfo-&gt;notIndexJump, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(byValInfo-&gt;stubRoutine-&gt;code().code()));
1379     MacroAssembler::repatchCall(CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)), FunctionPtr&lt;OperationPtrTag&gt;(operationGetByValGeneric));
1380 }
1381 
1382 template&lt;typename Op&gt;
1383 void JIT::privateCompilePutByVal(const ConcurrentJSLocker&amp;, ByValInfo* byValInfo, ReturnAddressPtr returnAddress, JITArrayMode arrayMode)
1384 {
1385     const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(byValInfo-&gt;bytecodeIndex).ptr();
1386     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
1387 
1388     PatchableJump badType;
1389     JumpList slowCases;
1390 
1391     bool needsLinkForWriteBarrier = false;
1392 
1393     switch (arrayMode) {
1394     case JITInt32:
1395         slowCases = emitInt32PutByVal(bytecode, badType);
1396         break;
1397     case JITDouble:
1398         slowCases = emitDoublePutByVal(bytecode, badType);
1399         break;
1400     case JITContiguous:
1401         slowCases = emitContiguousPutByVal(bytecode, badType);
1402         needsLinkForWriteBarrier = true;
1403         break;
1404     case JITArrayStorage:
1405         slowCases = emitArrayStoragePutByVal(bytecode, badType);
1406         needsLinkForWriteBarrier = true;
1407         break;
1408     default:
1409         TypedArrayType type = typedArrayTypeForJITArrayMode(arrayMode);
1410         if (isInt(type))
1411             slowCases = emitIntTypedArrayPutByVal(bytecode, badType, type);
1412         else
1413             slowCases = emitFloatTypedArrayPutByVal(bytecode, badType, type);
1414         break;
1415     }
1416 
1417     Jump done = jump();
1418 
1419     LinkBuffer patchBuffer(*this, m_codeBlock);
1420     patchBuffer.link(badType, byValInfo-&gt;slowPathTarget);
1421     patchBuffer.link(slowCases, byValInfo-&gt;slowPathTarget);
1422     patchBuffer.link(done, byValInfo-&gt;badTypeDoneTarget);
1423     if (needsLinkForWriteBarrier) {
1424         ASSERT(removeCodePtrTag(m_calls.last().callee.executableAddress()) == removeCodePtrTag(operationWriteBarrierSlowPath));
1425         patchBuffer.link(m_calls.last().from, m_calls.last().callee);
1426     }
1427 
1428     bool isDirect = currentInstruction-&gt;opcodeID() == op_put_by_val_direct;
1429     if (!isDirect) {
1430         byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1431             m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1432             &quot;Baseline put_by_val stub for %s, return point %p&quot;, toCString(*m_codeBlock).data(), returnAddress.value());
1433 
1434     } else {
1435         byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1436             m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1437             &quot;Baseline put_by_val_direct stub for %s, return point %p&quot;, toCString(*m_codeBlock).data(), returnAddress.value());
1438     }
1439     MacroAssembler::repatchJump(byValInfo-&gt;badTypeJump, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(byValInfo-&gt;stubRoutine-&gt;code().code()));
1440     MacroAssembler::repatchCall(CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)), FunctionPtr&lt;OperationPtrTag&gt;(isDirect ? operationDirectPutByValGeneric : operationPutByValGeneric));
1441 }
1442 
1443 template&lt;typename Op&gt;
1444 void JIT::privateCompilePutByValWithCachedId(ByValInfo* byValInfo, ReturnAddressPtr returnAddress, PutKind putKind, const Identifier&amp; propertyName)
1445 {
1446     ASSERT((putKind == Direct &amp;&amp; Op::opcodeID == op_put_by_val_direct) || (putKind == NotDirect &amp;&amp; Op::opcodeID == op_put_by_val));
1447     const Instruction* currentInstruction = m_codeBlock-&gt;instructions().at(byValInfo-&gt;bytecodeIndex).ptr();
1448     auto bytecode = currentInstruction-&gt;as&lt;Op&gt;();
1449 
1450     JumpList doneCases;
1451     JumpList slowCases;
1452 
1453     JITPutByIdGenerator gen = emitPutByValWithCachedId(byValInfo, bytecode, putKind, propertyName, doneCases, slowCases);
1454 
1455     ConcurrentJSLocker locker(m_codeBlock-&gt;m_lock);
1456     LinkBuffer patchBuffer(*this, m_codeBlock);
1457     patchBuffer.link(slowCases, byValInfo-&gt;slowPathTarget);
1458     patchBuffer.link(doneCases, byValInfo-&gt;badTypeDoneTarget);
1459     if (!m_exceptionChecks.empty())
1460         patchBuffer.link(m_exceptionChecks, byValInfo-&gt;exceptionHandler);
1461 
1462     for (const auto&amp; callSite : m_calls) {
1463         if (callSite.callee)
1464             patchBuffer.link(callSite.from, callSite.callee);
1465     }
1466     gen.finalize(patchBuffer, patchBuffer);
1467 
1468     byValInfo-&gt;stubRoutine = FINALIZE_CODE_FOR_STUB(
1469         m_codeBlock, patchBuffer, JITStubRoutinePtrTag,
1470         &quot;Baseline put_by_val%s with cached property name &#39;%s&#39; stub for %s, return point %p&quot;, (putKind == Direct) ? &quot;_direct&quot; : &quot;&quot;, propertyName.impl()-&gt;utf8().data(), toCString(*m_codeBlock).data(), returnAddress.value());
1471     byValInfo-&gt;stubInfo = gen.stubInfo();
1472 
1473     MacroAssembler::repatchJump(byValInfo-&gt;notIndexJump, CodeLocationLabel&lt;JITStubRoutinePtrTag&gt;(byValInfo-&gt;stubRoutine-&gt;code().code()));
1474     MacroAssembler::repatchCall(CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)), FunctionPtr&lt;OperationPtrTag&gt;(putKind == Direct ? operationDirectPutByValGeneric : operationPutByValGeneric));
1475 }
1476 
1477 JIT::JumpList JIT::emitDoubleLoad(const Instruction*, PatchableJump&amp; badType)
1478 {
1479 #if USE(JSVALUE64)
1480     RegisterID base = regT0;
1481     RegisterID property = regT1;
1482     RegisterID indexing = regT2;
1483     RegisterID scratch = regT3;
1484 #else
1485     RegisterID base = regT0;
1486     RegisterID property = regT2;
1487     RegisterID indexing = regT1;
1488     RegisterID scratch = regT3;
1489 #endif
1490 
1491     JumpList slowCases;
1492 
1493     badType = patchableBranch32(NotEqual, indexing, TrustedImm32(DoubleShape));
1494     loadPtr(Address(base, JSObject::butterflyOffset()), scratch);
1495     slowCases.append(branch32(AboveOrEqual, property, Address(scratch, Butterfly::offsetOfPublicLength())));
1496     loadDouble(BaseIndex(scratch, property, TimesEight), fpRegT0);
1497     slowCases.append(branchIfNaN(fpRegT0));
1498 
1499     return slowCases;
1500 }
1501 
1502 JIT::JumpList JIT::emitContiguousLoad(const Instruction*, PatchableJump&amp; badType, IndexingType expectedShape)
1503 {
1504 #if USE(JSVALUE64)
1505     RegisterID base = regT0;
1506     RegisterID property = regT1;
1507     RegisterID indexing = regT2;
1508     JSValueRegs result = JSValueRegs(regT0);
1509     RegisterID scratch = regT3;
1510 #else
1511     RegisterID base = regT0;
1512     RegisterID property = regT2;
1513     RegisterID indexing = regT1;
1514     JSValueRegs result = JSValueRegs(regT1, regT0);
1515     RegisterID scratch = regT3;
1516 #endif
1517 
1518     JumpList slowCases;
1519 
1520     badType = patchableBranch32(NotEqual, indexing, TrustedImm32(expectedShape));
1521     loadPtr(Address(base, JSObject::butterflyOffset()), scratch);
1522     slowCases.append(branch32(AboveOrEqual, property, Address(scratch, Butterfly::offsetOfPublicLength())));
1523     loadValue(BaseIndex(scratch, property, TimesEight), result);
1524     slowCases.append(branchIfEmpty(result));
1525 
1526     return slowCases;
1527 }
1528 
1529 JIT::JumpList JIT::emitArrayStorageLoad(const Instruction*, PatchableJump&amp; badType)
1530 {
1531 #if USE(JSVALUE64)
1532     RegisterID base = regT0;
1533     RegisterID property = regT1;
1534     RegisterID indexing = regT2;
1535     JSValueRegs result = JSValueRegs(regT0);
1536     RegisterID scratch = regT3;
1537 #else
1538     RegisterID base = regT0;
1539     RegisterID property = regT2;
1540     RegisterID indexing = regT1;
1541     JSValueRegs result = JSValueRegs(regT1, regT0);
1542     RegisterID scratch = regT3;
1543 #endif
1544 
1545     JumpList slowCases;
1546 
1547     add32(TrustedImm32(-ArrayStorageShape), indexing, scratch);
1548     badType = patchableBranch32(Above, scratch, TrustedImm32(SlowPutArrayStorageShape - ArrayStorageShape));
1549 
1550     loadPtr(Address(base, JSObject::butterflyOffset()), scratch);
1551     slowCases.append(branch32(AboveOrEqual, property, Address(scratch, ArrayStorage::vectorLengthOffset())));
1552 
1553     loadValue(BaseIndex(scratch, property, TimesEight, ArrayStorage::vectorOffset()), result);
1554     slowCases.append(branchIfEmpty(result));
1555 
1556     return slowCases;
1557 }
1558 
1559 JIT::JumpList JIT::emitDirectArgumentsGetByVal(const Instruction*, PatchableJump&amp; badType)
1560 {
1561     JumpList slowCases;
1562 
1563 #if USE(JSVALUE64)
1564     RegisterID base = regT0;
1565     RegisterID property = regT1;
1566     JSValueRegs result = JSValueRegs(regT0);
1567     RegisterID scratch = regT3;
1568     RegisterID scratch2 = regT4;
1569 #else
1570     RegisterID base = regT0;
1571     RegisterID property = regT2;
1572     JSValueRegs result = JSValueRegs(regT1, regT0);
1573     RegisterID scratch = regT3;
1574     RegisterID scratch2 = regT4;
1575 #endif
1576 
1577     load8(Address(base, JSCell::typeInfoTypeOffset()), scratch);
1578     badType = patchableBranch32(NotEqual, scratch, TrustedImm32(DirectArgumentsType));
1579 
1580     load32(Address(base, DirectArguments::offsetOfLength()), scratch2);
1581     slowCases.append(branch32(AboveOrEqual, property, scratch2));
1582     slowCases.append(branchTestPtr(NonZero, Address(base, DirectArguments::offsetOfMappedArguments())));
1583 
1584     loadValue(BaseIndex(base, property, TimesEight, DirectArguments::storageOffset()), result);
1585 
1586     return slowCases;
1587 }
1588 
1589 JIT::JumpList JIT::emitScopedArgumentsGetByVal(const Instruction*, PatchableJump&amp; badType)
1590 {
1591     JumpList slowCases;
1592 
1593 #if USE(JSVALUE64)
1594     RegisterID base = regT0;
1595     RegisterID property = regT1;
1596     JSValueRegs result = JSValueRegs(regT0);
1597     RegisterID scratch = regT3;
1598     RegisterID scratch2 = regT4;
1599     RegisterID scratch3 = regT5;
1600 #else
1601     RegisterID base = regT0;
1602     RegisterID property = regT2;
1603     JSValueRegs result = JSValueRegs(regT1, regT0);
1604     RegisterID scratch = regT3;
1605     RegisterID scratch2 = regT4;
1606     RegisterID scratch3 = regT5;
1607 #endif
1608 
1609     load8(Address(base, JSCell::typeInfoTypeOffset()), scratch);
1610     badType = patchableBranch32(NotEqual, scratch, TrustedImm32(ScopedArgumentsType));
1611     loadPtr(Address(base, ScopedArguments::offsetOfStorage()), scratch3);
1612     slowCases.append(branch32(AboveOrEqual, property, Address(scratch3, ScopedArguments::offsetOfTotalLengthInStorage())));
1613 
1614     loadPtr(Address(base, ScopedArguments::offsetOfTable()), scratch);
1615     load32(Address(scratch, ScopedArgumentsTable::offsetOfLength()), scratch2);
1616     Jump overflowCase = branch32(AboveOrEqual, property, scratch2);
1617     loadPtr(Address(base, ScopedArguments::offsetOfScope()), scratch2);
1618     loadPtr(Address(scratch, ScopedArgumentsTable::offsetOfArguments()), scratch);
1619     load32(BaseIndex(scratch, property, TimesFour), scratch);
1620     slowCases.append(branch32(Equal, scratch, TrustedImm32(ScopeOffset::invalidOffset)));
1621     loadValue(BaseIndex(scratch2, scratch, TimesEight, JSLexicalEnvironment::offsetOfVariables()), result);
1622     Jump done = jump();
1623     overflowCase.link(this);
1624     sub32(property, scratch2);
1625     neg32(scratch2);
1626     loadValue(BaseIndex(scratch3, scratch2, TimesEight), result);
1627     slowCases.append(branchIfEmpty(result));
1628     done.link(this);
1629 
1630     load32(Address(scratch3, ScopedArguments::offsetOfTotalLengthInStorage()), scratch);
1631     emitPreparePreciseIndexMask32(property, scratch, scratch2);
1632     andPtr(scratch2, result.payloadGPR());
1633 
1634     return slowCases;
1635 }
1636 
1637 JIT::JumpList JIT::emitIntTypedArrayGetByVal(const Instruction*, PatchableJump&amp; badType, TypedArrayType type)
1638 {
1639     ASSERT(isInt(type));
1640 
1641     // The best way to test the array type is to use the classInfo. We need to do so without
1642     // clobbering the register that holds the indexing type, base, and property.
1643 
1644 #if USE(JSVALUE64)
1645     RegisterID base = regT0;
1646     RegisterID property = regT1;
1647     JSValueRegs result = JSValueRegs(regT0);
1648     RegisterID scratch = regT3;
1649     RegisterID scratch2 = regT4;
1650 #else
1651     RegisterID base = regT0;
1652     RegisterID property = regT2;
1653     JSValueRegs result = JSValueRegs(regT1, regT0);
1654     RegisterID scratch = regT3;
1655     RegisterID scratch2 = regT4;
1656 #endif
1657     RegisterID resultPayload = result.payloadGPR();
1658 
1659     JumpList slowCases;
1660 
1661     load8(Address(base, JSCell::typeInfoTypeOffset()), scratch);
1662     badType = patchableBranch32(NotEqual, scratch, TrustedImm32(typeForTypedArrayType(type)));
1663     slowCases.append(branch32(AboveOrEqual, property, Address(base, JSArrayBufferView::offsetOfLength())));
1664     loadPtr(Address(base, JSArrayBufferView::offsetOfVector()), scratch);
1665     cageConditionally(Gigacage::Primitive, scratch, scratch2);
1666 
1667     switch (elementSize(type)) {
1668     case 1:
1669         if (JSC::isSigned(type))
1670             load8SignedExtendTo32(BaseIndex(scratch, property, TimesOne), resultPayload);
1671         else
1672             load8(BaseIndex(scratch, property, TimesOne), resultPayload);
1673         break;
1674     case 2:
1675         if (JSC::isSigned(type))
1676             load16SignedExtendTo32(BaseIndex(scratch, property, TimesTwo), resultPayload);
1677         else
1678             load16(BaseIndex(scratch, property, TimesTwo), resultPayload);
1679         break;
1680     case 4:
1681         load32(BaseIndex(scratch, property, TimesFour), resultPayload);
1682         break;
1683     default:
1684         CRASH();
1685     }
1686 
1687     Jump done;
1688     if (type == TypeUint32) {
1689         Jump canBeInt = branch32(GreaterThanOrEqual, resultPayload, TrustedImm32(0));
1690 
1691         convertInt32ToDouble(resultPayload, fpRegT0);
1692         addDouble(AbsoluteAddress(&amp;twoToThe32), fpRegT0);
1693         boxDouble(fpRegT0, result);
1694         done = jump();
1695         canBeInt.link(this);
1696     }
1697 
1698     boxInt32(resultPayload, result);
1699     if (done.isSet())
1700         done.link(this);
1701     return slowCases;
1702 }
1703 
1704 JIT::JumpList JIT::emitFloatTypedArrayGetByVal(const Instruction*, PatchableJump&amp; badType, TypedArrayType type)
1705 {
1706     ASSERT(isFloat(type));
1707 
1708 #if USE(JSVALUE64)
1709     RegisterID base = regT0;
1710     RegisterID property = regT1;
1711     JSValueRegs result = JSValueRegs(regT0);
1712     RegisterID scratch = regT3;
1713     RegisterID scratch2 = regT4;
1714 #else
1715     RegisterID base = regT0;
1716     RegisterID property = regT2;
1717     JSValueRegs result = JSValueRegs(regT1, regT0);
1718     RegisterID scratch = regT3;
1719     RegisterID scratch2 = regT4;
1720 #endif
1721 
1722     JumpList slowCases;
1723 
1724     load8(Address(base, JSCell::typeInfoTypeOffset()), scratch);
1725     badType = patchableBranch32(NotEqual, scratch, TrustedImm32(typeForTypedArrayType(type)));
1726     slowCases.append(branch32(AboveOrEqual, property, Address(base, JSArrayBufferView::offsetOfLength())));
1727     loadPtr(Address(base, JSArrayBufferView::offsetOfVector()), scratch);
1728     cageConditionally(Gigacage::Primitive, scratch, scratch2);
1729 
1730     switch (elementSize(type)) {
1731     case 4:
1732         loadFloat(BaseIndex(scratch, property, TimesFour), fpRegT0);
1733         convertFloatToDouble(fpRegT0, fpRegT0);
1734         break;
1735     case 8: {
1736         loadDouble(BaseIndex(scratch, property, TimesEight), fpRegT0);
1737         break;
1738     }
1739     default:
1740         CRASH();
1741     }
1742 
1743     purifyNaN(fpRegT0);
1744 
1745     boxDouble(fpRegT0, result);
1746     return slowCases;
1747 }
1748 
1749 template&lt;typename Op&gt;
1750 JIT::JumpList JIT::emitIntTypedArrayPutByVal(Op bytecode, PatchableJump&amp; badType, TypedArrayType type)
1751 {
1752     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1753     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
1754     ASSERT(isInt(type));
1755 
1756     int value = bytecode.m_value.offset();
1757 
1758 #if USE(JSVALUE64)
1759     RegisterID base = regT0;
1760     RegisterID property = regT1;
1761     RegisterID earlyScratch = regT3;
1762     RegisterID lateScratch = regT2;
1763     RegisterID lateScratch2 = regT4;
1764 #else
1765     RegisterID base = regT0;
1766     RegisterID property = regT2;
1767     RegisterID earlyScratch = regT3;
1768     RegisterID lateScratch = regT1;
1769     RegisterID lateScratch2 = regT4;
1770 #endif
1771 
1772     JumpList slowCases;
1773 
1774     load8(Address(base, JSCell::typeInfoTypeOffset()), earlyScratch);
1775     badType = patchableBranch32(NotEqual, earlyScratch, TrustedImm32(typeForTypedArrayType(type)));
1776     Jump inBounds = branch32(Below, property, Address(base, JSArrayBufferView::offsetOfLength()));
1777     emitArrayProfileOutOfBoundsSpecialCase(profile);
1778     slowCases.append(jump());
1779     inBounds.link(this);
1780 
1781 #if USE(JSVALUE64)
1782     emitGetVirtualRegister(value, earlyScratch);
1783     slowCases.append(branchIfNotInt32(earlyScratch));
1784 #else
1785     emitLoad(value, lateScratch, earlyScratch);
1786     slowCases.append(branchIfNotInt32(lateScratch));
1787 #endif
1788 
1789     // We would be loading this into base as in get_by_val, except that the slow
1790     // path expects the base to be unclobbered.
1791     loadPtr(Address(base, JSArrayBufferView::offsetOfVector()), lateScratch);
1792     cageConditionally(Gigacage::Primitive, lateScratch, lateScratch2);
1793 
1794     if (isClamped(type)) {
1795         ASSERT(elementSize(type) == 1);
1796         ASSERT(!JSC::isSigned(type));
1797         Jump inBounds = branch32(BelowOrEqual, earlyScratch, TrustedImm32(0xff));
1798         Jump tooBig = branch32(GreaterThan, earlyScratch, TrustedImm32(0xff));
1799         xor32(earlyScratch, earlyScratch);
1800         Jump clamped = jump();
1801         tooBig.link(this);
1802         move(TrustedImm32(0xff), earlyScratch);
1803         clamped.link(this);
1804         inBounds.link(this);
1805     }
1806 
1807     switch (elementSize(type)) {
1808     case 1:
1809         store8(earlyScratch, BaseIndex(lateScratch, property, TimesOne));
1810         break;
1811     case 2:
1812         store16(earlyScratch, BaseIndex(lateScratch, property, TimesTwo));
1813         break;
1814     case 4:
1815         store32(earlyScratch, BaseIndex(lateScratch, property, TimesFour));
1816         break;
1817     default:
1818         CRASH();
1819     }
1820 
1821     return slowCases;
1822 }
1823 
1824 template&lt;typename Op&gt;
1825 JIT::JumpList JIT::emitFloatTypedArrayPutByVal(Op bytecode, PatchableJump&amp; badType, TypedArrayType type)
1826 {
1827     auto&amp; metadata = bytecode.metadata(m_codeBlock);
1828     ArrayProfile* profile = &amp;metadata.m_arrayProfile;
1829     ASSERT(isFloat(type));
1830 
1831     int value = bytecode.m_value.offset();
1832 
1833 #if USE(JSVALUE64)
1834     RegisterID base = regT0;
1835     RegisterID property = regT1;
1836     RegisterID earlyScratch = regT3;
1837     RegisterID lateScratch = regT2;
1838     RegisterID lateScratch2 = regT4;
1839 #else
1840     RegisterID base = regT0;
1841     RegisterID property = regT2;
1842     RegisterID earlyScratch = regT3;
1843     RegisterID lateScratch = regT1;
1844     RegisterID lateScratch2 = regT4;
1845 #endif
1846 
1847     JumpList slowCases;
1848 
1849     load8(Address(base, JSCell::typeInfoTypeOffset()), earlyScratch);
1850     badType = patchableBranch32(NotEqual, earlyScratch, TrustedImm32(typeForTypedArrayType(type)));
1851     Jump inBounds = branch32(Below, property, Address(base, JSArrayBufferView::offsetOfLength()));
1852     emitArrayProfileOutOfBoundsSpecialCase(profile);
1853     slowCases.append(jump());
1854     inBounds.link(this);
1855 
1856 #if USE(JSVALUE64)
1857     emitGetVirtualRegister(value, earlyScratch);
1858     Jump doubleCase = branchIfNotInt32(earlyScratch);
1859     convertInt32ToDouble(earlyScratch, fpRegT0);
1860     Jump ready = jump();
1861     doubleCase.link(this);
1862     slowCases.append(branchIfNotNumber(earlyScratch));
1863     add64(tagTypeNumberRegister, earlyScratch);
1864     move64ToDouble(earlyScratch, fpRegT0);
1865     ready.link(this);
1866 #else
1867     emitLoad(value, lateScratch, earlyScratch);
1868     Jump doubleCase = branchIfNotInt32(lateScratch);
1869     convertInt32ToDouble(earlyScratch, fpRegT0);
1870     Jump ready = jump();
1871     doubleCase.link(this);
1872     slowCases.append(branch32(Above, lateScratch, TrustedImm32(JSValue::LowestTag)));
1873     moveIntsToDouble(earlyScratch, lateScratch, fpRegT0, fpRegT1);
1874     ready.link(this);
1875 #endif
1876 
1877     // We would be loading this into base as in get_by_val, except that the slow
1878     // path expects the base to be unclobbered.
1879     loadPtr(Address(base, JSArrayBufferView::offsetOfVector()), lateScratch);
1880     cageConditionally(Gigacage::Primitive, lateScratch, lateScratch2);
1881 
1882     switch (elementSize(type)) {
1883     case 4:
1884         convertDoubleToFloat(fpRegT0, fpRegT0);
1885         storeFloat(fpRegT0, BaseIndex(lateScratch, property, TimesFour));
1886         break;
1887     case 8:
1888         storeDouble(fpRegT0, BaseIndex(lateScratch, property, TimesEight));
1889         break;
1890     default:
1891         CRASH();
1892     }
1893 
1894     return slowCases;
1895 }
1896 
1897 template void JIT::emit_op_put_by_val&lt;OpPutByVal&gt;(const Instruction*);
1898 
1899 } // namespace JSC
1900 
1901 #endif // ENABLE(JIT)
    </pre>
  </body>
</html>