<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGOSRExit.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="DFGOSREntrypointCreationPhase.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGOSRExit.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGOSRExit.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 334     VM&amp; vm = *context.arg&lt;VM*&gt;();
 335     auto scope = DECLARE_THROW_SCOPE(vm);
 336 
 337     ExecState* exec = context.fp&lt;ExecState*&gt;();
 338     ASSERT(&amp;exec-&gt;vm() == &amp;vm);
 339     auto&amp; cpu = context.cpu;
 340 
 341     if (validateDFGDoesGC) {
 342         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
 343         // code running that expects no GC.
 344         vm.heap.setExpectDoesGC(true);
 345     }
 346 
 347     if (vm.callFrameForCatch) {
 348         exec = vm.callFrameForCatch;
 349         context.fp() = exec;
 350     }
 351 
 352     CodeBlock* codeBlock = exec-&gt;codeBlock();
 353     ASSERT(codeBlock);
<span class="line-modified"> 354     ASSERT(codeBlock-&gt;jitType() == JITCode::DFGJIT);</span>
 355 
 356     // It&#39;s sort of preferable that we don&#39;t GC while in here. Anyways, doing so wouldn&#39;t
 357     // really be profitable.
 358     DeferGCForAWhile deferGC(vm.heap);
 359 
 360     uint32_t exitIndex = vm.osrExitIndex;
 361     DFG::JITCode* dfgJITCode = codeBlock-&gt;jitCode()-&gt;dfg();
 362     OSRExit&amp; exit = dfgJITCode-&gt;osrExit[exitIndex];
 363 
 364     ASSERT(!vm.callFrameForCatch || exit.m_kind == GenericUnwind);
 365     EXCEPTION_ASSERT_UNUSED(scope, !!scope.exception() || !exit.isExceptionHandler());
 366 
 367     if (UNLIKELY(!exit.exitState)) {
 368         ExtraInitializationLevel extraInitializationLevel = ExtraInitializationLevel::None;
 369 
 370         // We only need to execute this block once for each OSRExit record. The computed
 371         // results will be cached in the OSRExitState record for use of the rest of the
 372         // exit ramp code.
 373 
 374         // Ensure we have baseline codeBlocks to OSR exit to.
 375         prepareCodeOriginForOSRExit(exec, exit.m_codeOrigin);
 376 
 377         CodeBlock* baselineCodeBlock = codeBlock-&gt;baselineAlternative();
<span class="line-modified"> 378         ASSERT(baselineCodeBlock-&gt;jitType() == JITCode::BaselineJIT);</span>
 379 
 380         SpeculationRecovery* recovery = nullptr;
 381         if (exit.m_recoveryIndex != UINT_MAX) {
 382             recovery = &amp;dfgJITCode-&gt;speculationRecovery[exit.m_recoveryIndex];
 383             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::SpeculationRecovery);
 384         }
 385 
 386         if (UNLIKELY(exit.m_kind == GenericUnwind))
 387             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::Other);
 388 
 389         ArrayProfile* arrayProfile = nullptr;
 390         if (!!exit.m_jsValueSource) {
 391             if (exit.m_valueProfile)
 392                 extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::ValueProfileUpdate);
 393             if (exit.m_kind == BadCache || exit.m_kind == BadIndexingType) {
 394                 CodeOrigin codeOrigin = exit.m_codeOriginForExitProfile;
 395                 CodeBlock* profiledCodeBlock = baselineCodeBlockForOriginAndBaselineCodeBlock(codeOrigin, baselineCodeBlock);
<span class="line-modified"> 396                 arrayProfile = profiledCodeBlock-&gt;getArrayProfile(codeOrigin.bytecodeIndex);</span>
 397                 if (arrayProfile)
 398                     extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::ArrayProfileUpdate);
 399             }
 400         }
 401 
 402         int32_t activeThreshold = baselineCodeBlock-&gt;adjustedCounterValue(Options::thresholdForOptimizeAfterLongWarmUp());
 403         double adjustedThreshold = applyMemoryUsageHeuristicsAndConvertToInt(activeThreshold, baselineCodeBlock);
 404         ASSERT(adjustedThreshold &gt; 0);
 405         adjustedThreshold = BaselineExecutionCounter::clippedThreshold(codeBlock-&gt;globalObject(), adjustedThreshold);
 406 
 407         CodeBlock* codeBlockForExit = baselineCodeBlockForOriginAndBaselineCodeBlock(exit.m_codeOrigin, baselineCodeBlock);
 408         const JITCodeMap&amp; codeMap = codeBlockForExit-&gt;jitCodeMap();
<span class="line-modified"> 409         CodeLocationLabel&lt;JSEntryPtrTag&gt; codeLocation = codeMap.find(exit.m_codeOrigin.bytecodeIndex);</span>
 410         ASSERT(codeLocation);
 411 
 412         void* jumpTarget = codeLocation.executableAddress();
 413 
 414         // Compute the value recoveries.
 415         Operands&lt;ValueRecovery&gt; operands;
 416         Vector&lt;UndefinedOperandSpan&gt; undefinedOperandSpans;
 417         dfgJITCode-&gt;variableEventStream.reconstruct(codeBlock, exit.m_codeOrigin, dfgJITCode-&gt;minifiedDFG, exit.m_streamIndex, operands, &amp;undefinedOperandSpans);
 418         ptrdiff_t stackPointerOffset = -static_cast&lt;ptrdiff_t&gt;(codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;requiredRegisterCountForExit) * sizeof(Register);
 419 
 420         exit.exitState = adoptRef(new OSRExitState(exit, codeBlock, baselineCodeBlock, operands, WTFMove(undefinedOperandSpans), recovery, stackPointerOffset, activeThreshold, adjustedThreshold, jumpTarget, arrayProfile));
 421 
 422         if (UNLIKELY(vm.m_perBytecodeProfiler &amp;&amp; codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation)) {
 423             Profiler::Database&amp; database = *vm.m_perBytecodeProfiler;
 424             Profiler::Compilation* compilation = codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation.get();
 425 
 426             Profiler::OSRExit* profilerExit = compilation-&gt;addOSRExit(
 427                 exitIndex, Profiler::OriginStack(database, codeBlock, exit.m_codeOrigin),
 428                 exit.m_kind, exit.m_kind == UncountableInvalidation);
 429             exit.exitState-&gt;profilerExit = profilerExit;
 430             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::Other);
 431         }
 432 
 433         if (UNLIKELY(Options::printEachOSRExit()))
 434             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::Other);
 435 
 436         exit.exitState-&gt;extraInitializationLevel = extraInitializationLevel;
 437 
 438         if (UNLIKELY(Options::verboseOSR() || Options::verboseDFGOSRExit())) {
 439             dataLogF(&quot;DFG OSR exit #%u (%s, %s) from %s, with operands = %s\n&quot;,
 440                 exitIndex, toCString(exit.m_codeOrigin).data(),
 441                 exitKindToString(exit.m_kind), toCString(*codeBlock).data(),
 442                 toCString(ignoringContext&lt;DumpContext&gt;(operands)).data());
 443         }
 444     }
 445 
 446     OSRExitState&amp; exitState = *exit.exitState.get();
 447     CodeBlock* baselineCodeBlock = exitState.baselineCodeBlock;
<span class="line-modified"> 448     ASSERT(baselineCodeBlock-&gt;jitType() == JITCode::BaselineJIT);</span>
 449 
 450     Operands&lt;ValueRecovery&gt;&amp; operands = exitState.operands;
 451     Vector&lt;UndefinedOperandSpan&gt;&amp; undefinedOperandSpans = exitState.undefinedOperandSpans;
 452 
 453     context.sp() = context.fp&lt;uint8_t*&gt;() + exitState.stackPointerOffset;
 454 
<span class="line-modified"> 455     // The only reason for using this do while look is so we can break out midway when appropriate.</span>
 456     do {
 457         auto extraInitializationLevel = static_cast&lt;ExtraInitializationLevel&gt;(exitState.extraInitializationLevel);
 458 
 459         if (extraInitializationLevel == ExtraInitializationLevel::None)
 460             break;
 461 
 462         // Begin extra initilization level: SpeculationRecovery
 463 
 464         // We need to do speculation recovery first because array profiling and value profiling
 465         // may rely on a value that it recovers. However, that doesn&#39;t mean that it is likely
 466         // to have a recovery value. So, we&#39;ll decorate it as UNLIKELY.
 467         SpeculationRecovery* recovery = exitState.recovery;
 468         if (UNLIKELY(recovery)) {
 469             switch (recovery-&gt;type()) {
 470             case SpeculativeAdd:
 471                 cpu.gpr(recovery-&gt;dest()) = cpu.gpr&lt;uint32_t&gt;(recovery-&gt;dest()) - cpu.gpr&lt;uint32_t&gt;(recovery-&gt;src());
 472 #if USE(JSVALUE64)
 473                 ASSERT(!(cpu.gpr(recovery-&gt;dest()) &gt;&gt; 32));
 474                 cpu.gpr(recovery-&gt;dest()) |= TagTypeNumber;
 475 #endif
 476                 break;
 477 








 478             case SpeculativeAddImmediate:
 479                 cpu.gpr(recovery-&gt;dest()) = (cpu.gpr&lt;uint32_t&gt;(recovery-&gt;dest()) - recovery-&gt;immediate());
 480 #if USE(JSVALUE64)
 481                 ASSERT(!(cpu.gpr(recovery-&gt;dest()) &gt;&gt; 32));
 482                 cpu.gpr(recovery-&gt;dest()) |= TagTypeNumber;
 483 #endif
 484                 break;
 485 
 486             case BooleanSpeculationCheck:
 487 #if USE(JSVALUE64)
 488                 cpu.gpr(recovery-&gt;dest()) = cpu.gpr(recovery-&gt;dest()) ^ ValueFalse;
 489 #endif
 490                 break;
 491 
 492             default:
 493                 break;
 494             }
 495         }
 496         if (extraInitializationLevel &lt;= ExtraInitializationLevel::SpeculationRecovery)
 497             break;
</pre>
<hr />
<pre>
 727     // larger than the number of successes (we want 90% success rate), and if
 728     // there have been a large enough number of failures. If so, we set the
 729     // counter to 0; otherwise we set the counter to
 730     // counterValueForOptimizeAfterWarmUp().
 731 
 732     if (UNLIKELY(codeBlock-&gt;updateOSRExitCounterAndCheckIfNeedToReoptimize(exitState) == CodeBlock::OptimizeAction::ReoptimizeNow))
 733         triggerReoptimizationNow(baselineCodeBlock, codeBlock, &amp;exit);
 734 
 735     reifyInlinedCallFrames(context, baselineCodeBlock, exit);
 736     adjustAndJumpToTarget(context, vm, codeBlock, baselineCodeBlock, exit);
 737 }
 738 
 739 static void reifyInlinedCallFrames(Context&amp; context, CodeBlock* outermostBaselineCodeBlock, const OSRExitBase&amp; exit)
 740 {
 741     auto&amp; cpu = context.cpu;
 742     Frame frame(cpu.fp(), context.stack());
 743 
 744     // FIXME: We shouldn&#39;t leave holes on the stack when performing an OSR exit
 745     // in presence of inlined tail calls.
 746     // https://bugs.webkit.org/show_bug.cgi?id=147511
<span class="line-modified"> 747     ASSERT(outermostBaselineCodeBlock-&gt;jitType() == JITCode::BaselineJIT);</span>
 748     frame.setOperand&lt;CodeBlock*&gt;(CallFrameSlot::codeBlock, outermostBaselineCodeBlock);
 749 
 750     const CodeOrigin* codeOrigin;
<span class="line-modified"> 751     for (codeOrigin = &amp;exit.m_codeOrigin; codeOrigin &amp;&amp; codeOrigin-&gt;inlineCallFrame; codeOrigin = codeOrigin-&gt;inlineCallFrame-&gt;getCallerSkippingTailCalls()) {</span>
<span class="line-modified"> 752         InlineCallFrame* inlineCallFrame = codeOrigin-&gt;inlineCallFrame;</span>
 753         CodeBlock* baselineCodeBlock = baselineCodeBlockForOriginAndBaselineCodeBlock(*codeOrigin, outermostBaselineCodeBlock);
 754         InlineCallFrame::Kind trueCallerCallKind;
 755         CodeOrigin* trueCaller = inlineCallFrame-&gt;getCallerSkippingTailCalls(&amp;trueCallerCallKind);
 756         void* callerFrame = cpu.fp();
 757 
 758         if (!trueCaller) {
 759             ASSERT(inlineCallFrame-&gt;isTail());
 760             void* returnPC = frame.get&lt;void*&gt;(CallFrame::returnPCOffset());
<span class="line-modified"> 761 #if USE(POINTER_PROFILING)</span>
 762             void* oldEntrySP = cpu.fp&lt;uint8_t*&gt;() + sizeof(CallerFrameAndPC);
 763             void* newEntrySP = cpu.fp&lt;uint8_t*&gt;() + inlineCallFrame-&gt;returnPCOffset() + sizeof(void*);
 764             returnPC = retagCodePtr(returnPC, bitwise_cast&lt;PtrTag&gt;(oldEntrySP), bitwise_cast&lt;PtrTag&gt;(newEntrySP));
 765 #endif
 766             frame.set&lt;void*&gt;(inlineCallFrame-&gt;returnPCOffset(), returnPC);
 767             callerFrame = frame.get&lt;void*&gt;(CallFrame::callerFrameOffset());
 768         } else {
 769             CodeBlock* baselineCodeBlockForCaller = baselineCodeBlockForOriginAndBaselineCodeBlock(*trueCaller, outermostBaselineCodeBlock);
<span class="line-modified"> 770             unsigned callBytecodeIndex = trueCaller-&gt;bytecodeIndex;</span>
 771             MacroAssemblerCodePtr&lt;JSInternalPtrTag&gt; jumpTarget;
 772 
 773             switch (trueCallerCallKind) {
 774             case InlineCallFrame::Call:
 775             case InlineCallFrame::Construct:
 776             case InlineCallFrame::CallVarargs:
 777             case InlineCallFrame::ConstructVarargs:
 778             case InlineCallFrame::TailCall:
 779             case InlineCallFrame::TailCallVarargs: {
 780                 CallLinkInfo* callLinkInfo =
 781                     baselineCodeBlockForCaller-&gt;getCallLinkInfoForBytecodeIndex(callBytecodeIndex);
 782                 RELEASE_ASSERT(callLinkInfo);
 783 
 784                 jumpTarget = callLinkInfo-&gt;callReturnLocation();
 785                 break;
 786             }
 787 
 788             case InlineCallFrame::GetterCall:
 789             case InlineCallFrame::SetterCall: {
 790                 StructureStubInfo* stubInfo =
 791                     baselineCodeBlockForCaller-&gt;findStubInfo(CodeOrigin(callBytecodeIndex));
 792                 RELEASE_ASSERT(stubInfo);
 793 
 794                 jumpTarget = stubInfo-&gt;doneLocation();
 795                 break;
 796             }
 797 
 798             default:
 799                 RELEASE_ASSERT_NOT_REACHED();
 800             }
 801 
<span class="line-modified"> 802             if (trueCaller-&gt;inlineCallFrame)</span>
<span class="line-modified"> 803                 callerFrame = cpu.fp&lt;uint8_t*&gt;() + trueCaller-&gt;inlineCallFrame-&gt;stackOffset * sizeof(EncodedJSValue);</span>
 804 
 805             void* targetAddress = jumpTarget.executableAddress();
<span class="line-modified"> 806 #if USE(POINTER_PROFILING)</span>
 807             void* newEntrySP = cpu.fp&lt;uint8_t*&gt;() + inlineCallFrame-&gt;returnPCOffset() + sizeof(void*);
 808             targetAddress = retagCodePtr(targetAddress, JSInternalPtrTag, bitwise_cast&lt;PtrTag&gt;(newEntrySP));
 809 #endif
 810             frame.set&lt;void*&gt;(inlineCallFrame-&gt;returnPCOffset(), targetAddress);
 811         }
 812 
 813         frame.setOperand&lt;void*&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::codeBlock, baselineCodeBlock);
 814 
 815         // Restore the inline call frame&#39;s callee save registers.
 816         // If this inlined frame is a tail call that will return back to the original caller, we need to
 817         // copy the prior contents of the tag registers already saved for the outer frame to this frame.
 818         saveOrCopyCalleeSavesFor(context, baselineCodeBlock, VirtualRegister(inlineCallFrame-&gt;stackOffset), !trueCaller);
 819 
 820         if (!inlineCallFrame-&gt;isVarargs())
 821             frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount, PayloadOffset, inlineCallFrame-&gt;argumentCountIncludingThis);
 822         ASSERT(callerFrame);
 823         frame.set&lt;void*&gt;(inlineCallFrame-&gt;callerFrameOffset(), callerFrame);
 824 #if USE(JSVALUE64)
<span class="line-modified"> 825         uint32_t locationBits = CallSiteIndex(codeOrigin-&gt;bytecodeIndex).bits();</span>
 826         frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount, TagOffset, locationBits);
 827         if (!inlineCallFrame-&gt;isClosureCall)
 828             frame.setOperand(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee, JSValue(inlineCallFrame-&gt;calleeConstant()));
 829 #else // USE(JSVALUE64) // so this is the 32-bit part
<span class="line-modified"> 830         const Instruction* instruction = baselineCodeBlock-&gt;instructions().at(codeOrigin-&gt;bytecodeIndex).ptr();</span>
 831         uint32_t locationBits = CallSiteIndex(instruction).bits();
 832         frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount, TagOffset, locationBits);
 833         frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee, TagOffset, static_cast&lt;uint32_t&gt;(JSValue::CellTag));
 834         if (!inlineCallFrame-&gt;isClosureCall)
 835             frame.setOperand(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee, PayloadOffset, inlineCallFrame-&gt;calleeConstant());
 836 #endif // USE(JSVALUE64) // ending the #else part, so directly above is the 32-bit part
 837     }
 838 
 839     // Don&#39;t need to set the toplevel code origin if we only did inline tail calls
 840     if (codeOrigin) {
 841 #if USE(JSVALUE64)
<span class="line-modified"> 842         uint32_t locationBits = CallSiteIndex(codeOrigin-&gt;bytecodeIndex).bits();</span>
 843 #else
<span class="line-modified"> 844         const Instruction* instruction = outermostBaselineCodeBlock-&gt;instructions().at(codeOrigin-&gt;bytecodeIndex).ptr();</span>
 845         uint32_t locationBits = CallSiteIndex(instruction).bits();
 846 #endif
 847         frame.setOperand&lt;uint32_t&gt;(CallFrameSlot::argumentCount, TagOffset, locationBits);
 848     }
 849 }
 850 
 851 static void adjustAndJumpToTarget(Context&amp; context, VM&amp; vm, CodeBlock* codeBlock, CodeBlock* baselineCodeBlock, OSRExit&amp; exit)
 852 {
 853     OSRExitState* exitState = exit.exitState.get();
 854 
 855     WTF::storeLoadFence(); // The optimizing compiler expects that the OSR exit mechanism will execute this fence.
 856     vm.heap.writeBarrier(baselineCodeBlock);
 857 
 858     // We barrier all inlined frames -- and not just the current inline stack --
 859     // because we don&#39;t know which inlined function owns the value profile that
 860     // we&#39;ll update when we exit. In the case of &quot;f() { a(); b(); }&quot;, if both
 861     // a and b are inlined, we might exit inside b due to a bad value loaded
 862     // from a.
 863     // FIXME: MethodOfGettingAValueProfile should remember which CodeBlock owns
 864     // the value profile.
 865     InlineCallFrameSet* inlineCallFrames = codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;inlineCallFrames.get();
 866     if (inlineCallFrames) {
 867         for (InlineCallFrame* inlineCallFrame : *inlineCallFrames)
 868             vm.heap.writeBarrier(inlineCallFrame-&gt;baselineCodeBlock.get());
 869     }
 870 
<span class="line-modified"> 871     if (exit.m_codeOrigin.inlineCallFrame)</span>
<span class="line-modified"> 872         context.fp() = context.fp&lt;uint8_t*&gt;() + exit.m_codeOrigin.inlineCallFrame-&gt;stackOffset * sizeof(EncodedJSValue);</span>

 873 
 874     void* jumpTarget = exitState-&gt;jumpTarget;
 875     ASSERT(jumpTarget);
 876 
 877     if (exit.isExceptionHandler()) {
 878         // Since we&#39;re jumping to op_catch, we need to set callFrameForCatch.
 879         vm.callFrameForCatch = context.fp&lt;ExecState*&gt;();
 880     }
 881 
 882     vm.topCallFrame = context.fp&lt;ExecState*&gt;();
 883     context.pc() = untagCodePtr&lt;JSEntryPtrTag&gt;(jumpTarget);
 884 }
 885 
 886 static void printOSRExit(Context&amp; context, uint32_t osrExitIndex, const OSRExit&amp; exit)
 887 {
 888     ExecState* exec = context.fp&lt;ExecState*&gt;();
 889     CodeBlock* codeBlock = exec-&gt;codeBlock();
 890     CodeBlock* alternative = codeBlock-&gt;alternative();
 891     ExitKind kind = exit.m_kind;
<span class="line-modified"> 892     unsigned bytecodeOffset = exit.m_codeOrigin.bytecodeIndex;</span>
 893 
 894     dataLog(&quot;Speculation failure in &quot;, *codeBlock);
 895     dataLog(&quot; @ exit #&quot;, osrExitIndex, &quot; (bc#&quot;, bytecodeOffset, &quot;, &quot;, exitKindToString(kind), &quot;) with &quot;);
 896     if (alternative) {
 897         dataLog(
 898             &quot;executeCounter = &quot;, alternative-&gt;jitExecuteCounter(),
 899             &quot;, reoptimizationRetryCounter = &quot;, alternative-&gt;reoptimizationRetryCounter(),
 900             &quot;, optimizationDelayCounter = &quot;, alternative-&gt;optimizationDelayCounter());
 901     } else
 902         dataLog(&quot;no alternative code block (i.e. we&#39;ve been jettisoned)&quot;);
 903     dataLog(&quot;, osrExitCounter = &quot;, codeBlock-&gt;osrExitCounter(), &quot;\n&quot;);
 904     dataLog(&quot;    GPRs at time of exit:&quot;);
 905     for (unsigned i = 0; i &lt; GPRInfo::numberOfRegisters; ++i) {
 906         GPRReg gpr = GPRInfo::toRegister(i);
 907         dataLog(&quot; &quot;, context.gprName(gpr), &quot;:&quot;, RawPointer(context.gpr&lt;void*&gt;(gpr)));
 908     }
 909     dataLog(&quot;\n&quot;);
 910     dataLog(&quot;    FPRs at time of exit:&quot;);
 911     for (unsigned i = 0; i &lt; FPRInfo::numberOfRegisters; ++i) {
 912         FPRReg fpr = FPRInfo::toRegister(i);
</pre>
<hr />
<pre>
 995         switch (recovery.technique()) {
 996         case DirectArgumentsThatWereNotCreated:
 997             jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationCreateDirectArgumentsDuringExit)), GPRInfo::nonArgGPR0);
 998             break;
 999         case ClonedArgumentsThatWereNotCreated:
1000             jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationCreateClonedArgumentsDuringExit)), GPRInfo::nonArgGPR0);
1001             break;
1002         default:
1003             RELEASE_ASSERT_NOT_REACHED();
1004             break;
1005         }
1006         jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
1007         jit.storeCell(GPRInfo::returnValueGPR, AssemblyHelpers::addressFor(operand));
1008 
1009         alreadyAllocatedArguments.add(id, operand);
1010     }
1011 }
1012 
1013 void JIT_OPERATION OSRExit::compileOSRExit(ExecState* exec)
1014 {
<span class="line-modified">1015     VM* vm = &amp;exec-&gt;vm();</span>
<span class="line-modified">1016     auto scope = DECLARE_THROW_SCOPE(*vm);</span>
1017 
1018     if (validateDFGDoesGC) {
1019         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
1020         // code running that expects no GC.
<span class="line-modified">1021         vm-&gt;heap.setExpectDoesGC(true);</span>
1022     }
1023 
<span class="line-modified">1024     if (vm-&gt;callFrameForCatch)</span>
<span class="line-modified">1025         RELEASE_ASSERT(vm-&gt;callFrameForCatch == exec);</span>
1026 
1027     CodeBlock* codeBlock = exec-&gt;codeBlock();
1028     ASSERT(codeBlock);
<span class="line-modified">1029     ASSERT(codeBlock-&gt;jitType() == JITCode::DFGJIT);</span>
1030 
1031     // It&#39;s sort of preferable that we don&#39;t GC while in here. Anyways, doing so wouldn&#39;t
1032     // really be profitable.
<span class="line-modified">1033     DeferGCForAWhile deferGC(vm-&gt;heap);</span>
1034 
<span class="line-modified">1035     uint32_t exitIndex = vm-&gt;osrExitIndex;</span>
1036     OSRExit&amp; exit = codeBlock-&gt;jitCode()-&gt;dfg()-&gt;osrExit[exitIndex];
1037 
<span class="line-modified">1038     ASSERT(!vm-&gt;callFrameForCatch || exit.m_kind == GenericUnwind);</span>
1039     EXCEPTION_ASSERT_UNUSED(scope, !!scope.exception() || !exit.isExceptionHandler());
1040 
1041     prepareCodeOriginForOSRExit(exec, exit.m_codeOrigin);
1042 
1043     // Compute the value recoveries.
1044     Operands&lt;ValueRecovery&gt; operands;
1045     codeBlock-&gt;jitCode()-&gt;dfg()-&gt;variableEventStream.reconstruct(codeBlock, exit.m_codeOrigin, codeBlock-&gt;jitCode()-&gt;dfg()-&gt;minifiedDFG, exit.m_streamIndex, operands);
1046 
1047     SpeculationRecovery* recovery = 0;
1048     if (exit.m_recoveryIndex != UINT_MAX)
1049         recovery = &amp;codeBlock-&gt;jitCode()-&gt;dfg()-&gt;speculationRecovery[exit.m_recoveryIndex];
1050 
1051     {
1052         CCallHelpers jit(codeBlock);
1053 
1054         if (exit.m_kind == GenericUnwind) {
1055             // We are acting as a defacto op_catch because we arrive here from genericUnwind().
1056             // So, we must restore our call frame and stack pointer.
<span class="line-modified">1057             jit.restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm-&gt;topEntryFrame);</span>
<span class="line-modified">1058             jit.loadPtr(vm-&gt;addressOfCallFrameForCatch(), GPRInfo::callFrameRegister);</span>
1059         }
1060         jit.addPtr(
1061             CCallHelpers::TrustedImm32(codeBlock-&gt;stackPointerOffset() * sizeof(Register)),
1062             GPRInfo::callFrameRegister, CCallHelpers::stackPointerRegister);
1063 
1064         jit.jitAssertHasValidCallFrame();
1065 
<span class="line-modified">1066         if (UNLIKELY(vm-&gt;m_perBytecodeProfiler &amp;&amp; codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation)) {</span>
<span class="line-modified">1067             Profiler::Database&amp; database = *vm-&gt;m_perBytecodeProfiler;</span>
1068             Profiler::Compilation* compilation = codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation.get();
1069 
1070             Profiler::OSRExit* profilerExit = compilation-&gt;addOSRExit(
1071                 exitIndex, Profiler::OriginStack(database, codeBlock, exit.m_codeOrigin),
1072                 exit.m_kind, exit.m_kind == UncountableInvalidation);
1073             jit.add64(CCallHelpers::TrustedImm32(1), CCallHelpers::AbsoluteAddress(profilerExit-&gt;counterAddress()));
1074         }
1075 
<span class="line-modified">1076         compileExit(jit, *vm, exit, operands, recovery);</span>
1077 
1078         LinkBuffer patchBuffer(jit, codeBlock);
1079         exit.m_code = FINALIZE_CODE_IF(
1080             shouldDumpDisassembly() || Options::verboseOSR() || Options::verboseDFGOSRExit(),
1081             patchBuffer, OSRExitPtrTag,
1082             &quot;DFG OSR exit #%u (%s, %s) from %s, with operands = %s&quot;,
1083                 exitIndex, toCString(exit.m_codeOrigin).data(),
1084                 exitKindToString(exit.m_kind), toCString(*codeBlock).data(),
1085                 toCString(ignoringContext&lt;DumpContext&gt;(operands)).data());
1086     }
1087 
1088     MacroAssembler::repatchJump(exit.codeLocationForRepatch(), CodeLocationLabel&lt;OSRExitPtrTag&gt;(exit.m_code.code()));
1089 
<span class="line-modified">1090     vm-&gt;osrExitJumpDestination = exit.m_code.code().executableAddress();</span>
1091 }
1092 
1093 void OSRExit::compileExit(CCallHelpers&amp; jit, VM&amp; vm, const OSRExit&amp; exit, const Operands&lt;ValueRecovery&gt;&amp; operands, SpeculationRecovery* recovery)
1094 {
1095     jit.jitAssertTagsInPlace();
1096 
1097     // Pro-forma stuff.
1098     if (Options::printEachOSRExit()) {
1099         SpeculationFailureDebugInfo* debugInfo = new SpeculationFailureDebugInfo;
1100         debugInfo-&gt;codeBlock = jit.codeBlock();
1101         debugInfo-&gt;kind = exit.m_kind;
<span class="line-modified">1102         debugInfo-&gt;bytecodeOffset = exit.m_codeOrigin.bytecodeIndex;</span>
1103 
1104         jit.debugCall(vm, debugOperationPrintSpeculationFailure, debugInfo);
1105     }
1106 
1107     // Perform speculation recovery. This only comes into play when an operation
1108     // starts mutating state before verifying the speculation it has already made.
1109 
1110     if (recovery) {
1111         switch (recovery-&gt;type()) {
1112         case SpeculativeAdd:
1113             jit.sub32(recovery-&gt;src(), recovery-&gt;dest());
1114 #if USE(JSVALUE64)
1115             jit.or64(GPRInfo::tagTypeNumberRegister, recovery-&gt;dest());
1116 #endif
1117             break;
1118 









1119         case SpeculativeAddImmediate:
1120             jit.sub32(AssemblyHelpers::Imm32(recovery-&gt;immediate()), recovery-&gt;dest());
1121 #if USE(JSVALUE64)
1122             jit.or64(GPRInfo::tagTypeNumberRegister, recovery-&gt;dest());
1123 #endif
1124             break;
1125 
1126         case BooleanSpeculationCheck:
1127 #if USE(JSVALUE64)
1128             jit.xor64(AssemblyHelpers::TrustedImm32(static_cast&lt;int32_t&gt;(ValueFalse)), recovery-&gt;dest());
1129 #endif
1130             break;
1131 
1132         default:
1133             break;
1134         }
1135     }
1136 
1137     // Refine some array and/or value profile, if appropriate.
1138 
1139     if (!!exit.m_jsValueSource) {
1140         if (exit.m_kind == BadCache || exit.m_kind == BadIndexingType) {
1141             // If the instruction that this originated from has an array profile, then
1142             // refine it. If it doesn&#39;t, then do nothing. The latter could happen for
1143             // hoisted checks, or checks emitted for operations that didn&#39;t have array
1144             // profiling - either ops that aren&#39;t array accesses at all, or weren&#39;t
1145             // known to be array acceses in the bytecode. The latter case is a FIXME
1146             // while the former case is an outcome of a CheckStructure not knowing why
1147             // it was emitted (could be either due to an inline cache of a property
1148             // property access, or due to an array profile).
1149 
1150             CodeOrigin codeOrigin = exit.m_codeOriginForExitProfile;
<span class="line-modified">1151             if (ArrayProfile* arrayProfile = jit.baselineCodeBlockFor(codeOrigin)-&gt;getArrayProfile(codeOrigin.bytecodeIndex)) {</span>
1152 #if USE(JSVALUE64)
1153                 GPRReg usedRegister;
1154                 if (exit.m_jsValueSource.isAddress())
1155                     usedRegister = exit.m_jsValueSource.base();
1156                 else
1157                     usedRegister = exit.m_jsValueSource.gpr();
1158 #else
1159                 GPRReg usedRegister1;
1160                 GPRReg usedRegister2;
1161                 if (exit.m_jsValueSource.isAddress()) {
1162                     usedRegister1 = exit.m_jsValueSource.base();
1163                     usedRegister2 = InvalidGPRReg;
1164                 } else {
1165                     usedRegister1 = exit.m_jsValueSource.payloadGPR();
1166                     if (exit.m_jsValueSource.hasKnownTag())
1167                         usedRegister2 = InvalidGPRReg;
1168                     else
1169                         usedRegister2 = exit.m_jsValueSource.tagGPR();
1170                 }
1171 #endif
</pre>
<hr />
<pre>
1612     // good job of ensuring (a) and (b). But that doesn&#39;t take care of (d),
1613     // since each speculation failure would reset the execute counter.
1614     // So we check here if the number of speculation failures is significantly
1615     // larger than the number of successes (we want 90% success rate), and if
1616     // there have been a large enough number of failures. If so, we set the
1617     // counter to 0; otherwise we set the counter to
1618     // counterValueForOptimizeAfterWarmUp().
1619 
1620     handleExitCounts(jit, exit);
1621 
1622     // Reify inlined call frames.
1623 
1624     reifyInlinedCallFrames(jit, exit);
1625 
1626     // And finish.
1627     adjustAndJumpToTarget(vm, jit, exit);
1628 }
1629 
1630 void JIT_OPERATION OSRExit::debugOperationPrintSpeculationFailure(ExecState* exec, void* debugInfoRaw, void* scratch)
1631 {
<span class="line-modified">1632     VM* vm = &amp;exec-&gt;vm();</span>
1633     NativeCallFrameTracer tracer(vm, exec);
1634 
1635     SpeculationFailureDebugInfo* debugInfo = static_cast&lt;SpeculationFailureDebugInfo*&gt;(debugInfoRaw);
1636     CodeBlock* codeBlock = debugInfo-&gt;codeBlock;
1637     CodeBlock* alternative = codeBlock-&gt;alternative();
1638     dataLog(&quot;Speculation failure in &quot;, *codeBlock);
<span class="line-modified">1639     dataLog(&quot; @ exit #&quot;, vm-&gt;osrExitIndex, &quot; (bc#&quot;, debugInfo-&gt;bytecodeOffset, &quot;, &quot;, exitKindToString(debugInfo-&gt;kind), &quot;) with &quot;);</span>
1640     if (alternative) {
1641         dataLog(
1642             &quot;executeCounter = &quot;, alternative-&gt;jitExecuteCounter(),
1643             &quot;, reoptimizationRetryCounter = &quot;, alternative-&gt;reoptimizationRetryCounter(),
1644             &quot;, optimizationDelayCounter = &quot;, alternative-&gt;optimizationDelayCounter());
1645     } else
1646         dataLog(&quot;no alternative code block (i.e. we&#39;ve been jettisoned)&quot;);
1647     dataLog(&quot;, osrExitCounter = &quot;, codeBlock-&gt;osrExitCounter(), &quot;\n&quot;);
1648     dataLog(&quot;    GPRs at time of exit:&quot;);
1649     char* scratchPointer = static_cast&lt;char*&gt;(scratch);
1650     for (unsigned i = 0; i &lt; GPRInfo::numberOfRegisters; ++i) {
1651         GPRReg gpr = GPRInfo::toRegister(i);
1652         dataLog(&quot; &quot;, GPRInfo::debugName(gpr), &quot;:&quot;, RawPointer(*reinterpret_cast_ptr&lt;void**&gt;(scratchPointer)));
1653         scratchPointer += sizeof(EncodedJSValue);
1654     }
1655     dataLog(&quot;\n&quot;);
1656     dataLog(&quot;    FPRs at time of exit:&quot;);
1657     for (unsigned i = 0; i &lt; FPRInfo::numberOfRegisters; ++i) {
1658         FPRReg fpr = FPRInfo::toRegister(i);
1659         dataLog(&quot; &quot;, FPRInfo::debugName(fpr), &quot;:&quot;);
</pre>
</td>
<td>
<hr />
<pre>
 334     VM&amp; vm = *context.arg&lt;VM*&gt;();
 335     auto scope = DECLARE_THROW_SCOPE(vm);
 336 
 337     ExecState* exec = context.fp&lt;ExecState*&gt;();
 338     ASSERT(&amp;exec-&gt;vm() == &amp;vm);
 339     auto&amp; cpu = context.cpu;
 340 
 341     if (validateDFGDoesGC) {
 342         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
 343         // code running that expects no GC.
 344         vm.heap.setExpectDoesGC(true);
 345     }
 346 
 347     if (vm.callFrameForCatch) {
 348         exec = vm.callFrameForCatch;
 349         context.fp() = exec;
 350     }
 351 
 352     CodeBlock* codeBlock = exec-&gt;codeBlock();
 353     ASSERT(codeBlock);
<span class="line-modified"> 354     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);</span>
 355 
 356     // It&#39;s sort of preferable that we don&#39;t GC while in here. Anyways, doing so wouldn&#39;t
 357     // really be profitable.
 358     DeferGCForAWhile deferGC(vm.heap);
 359 
 360     uint32_t exitIndex = vm.osrExitIndex;
 361     DFG::JITCode* dfgJITCode = codeBlock-&gt;jitCode()-&gt;dfg();
 362     OSRExit&amp; exit = dfgJITCode-&gt;osrExit[exitIndex];
 363 
 364     ASSERT(!vm.callFrameForCatch || exit.m_kind == GenericUnwind);
 365     EXCEPTION_ASSERT_UNUSED(scope, !!scope.exception() || !exit.isExceptionHandler());
 366 
 367     if (UNLIKELY(!exit.exitState)) {
 368         ExtraInitializationLevel extraInitializationLevel = ExtraInitializationLevel::None;
 369 
 370         // We only need to execute this block once for each OSRExit record. The computed
 371         // results will be cached in the OSRExitState record for use of the rest of the
 372         // exit ramp code.
 373 
 374         // Ensure we have baseline codeBlocks to OSR exit to.
 375         prepareCodeOriginForOSRExit(exec, exit.m_codeOrigin);
 376 
 377         CodeBlock* baselineCodeBlock = codeBlock-&gt;baselineAlternative();
<span class="line-modified"> 378         ASSERT(baselineCodeBlock-&gt;jitType() == JITType::BaselineJIT);</span>
 379 
 380         SpeculationRecovery* recovery = nullptr;
 381         if (exit.m_recoveryIndex != UINT_MAX) {
 382             recovery = &amp;dfgJITCode-&gt;speculationRecovery[exit.m_recoveryIndex];
 383             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::SpeculationRecovery);
 384         }
 385 
 386         if (UNLIKELY(exit.m_kind == GenericUnwind))
 387             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::Other);
 388 
 389         ArrayProfile* arrayProfile = nullptr;
 390         if (!!exit.m_jsValueSource) {
 391             if (exit.m_valueProfile)
 392                 extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::ValueProfileUpdate);
 393             if (exit.m_kind == BadCache || exit.m_kind == BadIndexingType) {
 394                 CodeOrigin codeOrigin = exit.m_codeOriginForExitProfile;
 395                 CodeBlock* profiledCodeBlock = baselineCodeBlockForOriginAndBaselineCodeBlock(codeOrigin, baselineCodeBlock);
<span class="line-modified"> 396                 arrayProfile = profiledCodeBlock-&gt;getArrayProfile(codeOrigin.bytecodeIndex());</span>
 397                 if (arrayProfile)
 398                     extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::ArrayProfileUpdate);
 399             }
 400         }
 401 
 402         int32_t activeThreshold = baselineCodeBlock-&gt;adjustedCounterValue(Options::thresholdForOptimizeAfterLongWarmUp());
 403         double adjustedThreshold = applyMemoryUsageHeuristicsAndConvertToInt(activeThreshold, baselineCodeBlock);
 404         ASSERT(adjustedThreshold &gt; 0);
 405         adjustedThreshold = BaselineExecutionCounter::clippedThreshold(codeBlock-&gt;globalObject(), adjustedThreshold);
 406 
 407         CodeBlock* codeBlockForExit = baselineCodeBlockForOriginAndBaselineCodeBlock(exit.m_codeOrigin, baselineCodeBlock);
 408         const JITCodeMap&amp; codeMap = codeBlockForExit-&gt;jitCodeMap();
<span class="line-modified"> 409         CodeLocationLabel&lt;JSEntryPtrTag&gt; codeLocation = codeMap.find(exit.m_codeOrigin.bytecodeIndex());</span>
 410         ASSERT(codeLocation);
 411 
 412         void* jumpTarget = codeLocation.executableAddress();
 413 
 414         // Compute the value recoveries.
 415         Operands&lt;ValueRecovery&gt; operands;
 416         Vector&lt;UndefinedOperandSpan&gt; undefinedOperandSpans;
 417         dfgJITCode-&gt;variableEventStream.reconstruct(codeBlock, exit.m_codeOrigin, dfgJITCode-&gt;minifiedDFG, exit.m_streamIndex, operands, &amp;undefinedOperandSpans);
 418         ptrdiff_t stackPointerOffset = -static_cast&lt;ptrdiff_t&gt;(codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;requiredRegisterCountForExit) * sizeof(Register);
 419 
 420         exit.exitState = adoptRef(new OSRExitState(exit, codeBlock, baselineCodeBlock, operands, WTFMove(undefinedOperandSpans), recovery, stackPointerOffset, activeThreshold, adjustedThreshold, jumpTarget, arrayProfile));
 421 
 422         if (UNLIKELY(vm.m_perBytecodeProfiler &amp;&amp; codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation)) {
 423             Profiler::Database&amp; database = *vm.m_perBytecodeProfiler;
 424             Profiler::Compilation* compilation = codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation.get();
 425 
 426             Profiler::OSRExit* profilerExit = compilation-&gt;addOSRExit(
 427                 exitIndex, Profiler::OriginStack(database, codeBlock, exit.m_codeOrigin),
 428                 exit.m_kind, exit.m_kind == UncountableInvalidation);
 429             exit.exitState-&gt;profilerExit = profilerExit;
 430             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::Other);
 431         }
 432 
 433         if (UNLIKELY(Options::printEachOSRExit()))
 434             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::Other);
 435 
 436         exit.exitState-&gt;extraInitializationLevel = extraInitializationLevel;
 437 
 438         if (UNLIKELY(Options::verboseOSR() || Options::verboseDFGOSRExit())) {
 439             dataLogF(&quot;DFG OSR exit #%u (%s, %s) from %s, with operands = %s\n&quot;,
 440                 exitIndex, toCString(exit.m_codeOrigin).data(),
 441                 exitKindToString(exit.m_kind), toCString(*codeBlock).data(),
 442                 toCString(ignoringContext&lt;DumpContext&gt;(operands)).data());
 443         }
 444     }
 445 
 446     OSRExitState&amp; exitState = *exit.exitState.get();
 447     CodeBlock* baselineCodeBlock = exitState.baselineCodeBlock;
<span class="line-modified"> 448     ASSERT(baselineCodeBlock-&gt;jitType() == JITType::BaselineJIT);</span>
 449 
 450     Operands&lt;ValueRecovery&gt;&amp; operands = exitState.operands;
 451     Vector&lt;UndefinedOperandSpan&gt;&amp; undefinedOperandSpans = exitState.undefinedOperandSpans;
 452 
 453     context.sp() = context.fp&lt;uint8_t*&gt;() + exitState.stackPointerOffset;
 454 
<span class="line-modified"> 455     // The only reason for using this do while loop is so we can break out midway when appropriate.</span>
 456     do {
 457         auto extraInitializationLevel = static_cast&lt;ExtraInitializationLevel&gt;(exitState.extraInitializationLevel);
 458 
 459         if (extraInitializationLevel == ExtraInitializationLevel::None)
 460             break;
 461 
 462         // Begin extra initilization level: SpeculationRecovery
 463 
 464         // We need to do speculation recovery first because array profiling and value profiling
 465         // may rely on a value that it recovers. However, that doesn&#39;t mean that it is likely
 466         // to have a recovery value. So, we&#39;ll decorate it as UNLIKELY.
 467         SpeculationRecovery* recovery = exitState.recovery;
 468         if (UNLIKELY(recovery)) {
 469             switch (recovery-&gt;type()) {
 470             case SpeculativeAdd:
 471                 cpu.gpr(recovery-&gt;dest()) = cpu.gpr&lt;uint32_t&gt;(recovery-&gt;dest()) - cpu.gpr&lt;uint32_t&gt;(recovery-&gt;src());
 472 #if USE(JSVALUE64)
 473                 ASSERT(!(cpu.gpr(recovery-&gt;dest()) &gt;&gt; 32));
 474                 cpu.gpr(recovery-&gt;dest()) |= TagTypeNumber;
 475 #endif
 476                 break;
 477 
<span class="line-added"> 478             case SpeculativeAddSelf:</span>
<span class="line-added"> 479                 cpu.gpr(recovery-&gt;dest()) = static_cast&lt;uint32_t&gt;(cpu.gpr&lt;int32_t&gt;(recovery-&gt;dest()) &gt;&gt; 1) ^ 0x80000000U;</span>
<span class="line-added"> 480 #if USE(JSVALUE64)</span>
<span class="line-added"> 481                 ASSERT(!(cpu.gpr(recovery-&gt;dest()) &gt;&gt; 32));</span>
<span class="line-added"> 482                 cpu.gpr(recovery-&gt;dest()) |= TagTypeNumber;</span>
<span class="line-added"> 483 #endif</span>
<span class="line-added"> 484                 break;</span>
<span class="line-added"> 485 </span>
 486             case SpeculativeAddImmediate:
 487                 cpu.gpr(recovery-&gt;dest()) = (cpu.gpr&lt;uint32_t&gt;(recovery-&gt;dest()) - recovery-&gt;immediate());
 488 #if USE(JSVALUE64)
 489                 ASSERT(!(cpu.gpr(recovery-&gt;dest()) &gt;&gt; 32));
 490                 cpu.gpr(recovery-&gt;dest()) |= TagTypeNumber;
 491 #endif
 492                 break;
 493 
 494             case BooleanSpeculationCheck:
 495 #if USE(JSVALUE64)
 496                 cpu.gpr(recovery-&gt;dest()) = cpu.gpr(recovery-&gt;dest()) ^ ValueFalse;
 497 #endif
 498                 break;
 499 
 500             default:
 501                 break;
 502             }
 503         }
 504         if (extraInitializationLevel &lt;= ExtraInitializationLevel::SpeculationRecovery)
 505             break;
</pre>
<hr />
<pre>
 735     // larger than the number of successes (we want 90% success rate), and if
 736     // there have been a large enough number of failures. If so, we set the
 737     // counter to 0; otherwise we set the counter to
 738     // counterValueForOptimizeAfterWarmUp().
 739 
 740     if (UNLIKELY(codeBlock-&gt;updateOSRExitCounterAndCheckIfNeedToReoptimize(exitState) == CodeBlock::OptimizeAction::ReoptimizeNow))
 741         triggerReoptimizationNow(baselineCodeBlock, codeBlock, &amp;exit);
 742 
 743     reifyInlinedCallFrames(context, baselineCodeBlock, exit);
 744     adjustAndJumpToTarget(context, vm, codeBlock, baselineCodeBlock, exit);
 745 }
 746 
 747 static void reifyInlinedCallFrames(Context&amp; context, CodeBlock* outermostBaselineCodeBlock, const OSRExitBase&amp; exit)
 748 {
 749     auto&amp; cpu = context.cpu;
 750     Frame frame(cpu.fp(), context.stack());
 751 
 752     // FIXME: We shouldn&#39;t leave holes on the stack when performing an OSR exit
 753     // in presence of inlined tail calls.
 754     // https://bugs.webkit.org/show_bug.cgi?id=147511
<span class="line-modified"> 755     ASSERT(outermostBaselineCodeBlock-&gt;jitType() == JITType::BaselineJIT);</span>
 756     frame.setOperand&lt;CodeBlock*&gt;(CallFrameSlot::codeBlock, outermostBaselineCodeBlock);
 757 
 758     const CodeOrigin* codeOrigin;
<span class="line-modified"> 759     for (codeOrigin = &amp;exit.m_codeOrigin; codeOrigin &amp;&amp; codeOrigin-&gt;inlineCallFrame(); codeOrigin = codeOrigin-&gt;inlineCallFrame()-&gt;getCallerSkippingTailCalls()) {</span>
<span class="line-modified"> 760         InlineCallFrame* inlineCallFrame = codeOrigin-&gt;inlineCallFrame();</span>
 761         CodeBlock* baselineCodeBlock = baselineCodeBlockForOriginAndBaselineCodeBlock(*codeOrigin, outermostBaselineCodeBlock);
 762         InlineCallFrame::Kind trueCallerCallKind;
 763         CodeOrigin* trueCaller = inlineCallFrame-&gt;getCallerSkippingTailCalls(&amp;trueCallerCallKind);
 764         void* callerFrame = cpu.fp();
 765 
 766         if (!trueCaller) {
 767             ASSERT(inlineCallFrame-&gt;isTail());
 768             void* returnPC = frame.get&lt;void*&gt;(CallFrame::returnPCOffset());
<span class="line-modified"> 769 #if CPU(ARM64E)</span>
 770             void* oldEntrySP = cpu.fp&lt;uint8_t*&gt;() + sizeof(CallerFrameAndPC);
 771             void* newEntrySP = cpu.fp&lt;uint8_t*&gt;() + inlineCallFrame-&gt;returnPCOffset() + sizeof(void*);
 772             returnPC = retagCodePtr(returnPC, bitwise_cast&lt;PtrTag&gt;(oldEntrySP), bitwise_cast&lt;PtrTag&gt;(newEntrySP));
 773 #endif
 774             frame.set&lt;void*&gt;(inlineCallFrame-&gt;returnPCOffset(), returnPC);
 775             callerFrame = frame.get&lt;void*&gt;(CallFrame::callerFrameOffset());
 776         } else {
 777             CodeBlock* baselineCodeBlockForCaller = baselineCodeBlockForOriginAndBaselineCodeBlock(*trueCaller, outermostBaselineCodeBlock);
<span class="line-modified"> 778             unsigned callBytecodeIndex = trueCaller-&gt;bytecodeIndex();</span>
 779             MacroAssemblerCodePtr&lt;JSInternalPtrTag&gt; jumpTarget;
 780 
 781             switch (trueCallerCallKind) {
 782             case InlineCallFrame::Call:
 783             case InlineCallFrame::Construct:
 784             case InlineCallFrame::CallVarargs:
 785             case InlineCallFrame::ConstructVarargs:
 786             case InlineCallFrame::TailCall:
 787             case InlineCallFrame::TailCallVarargs: {
 788                 CallLinkInfo* callLinkInfo =
 789                     baselineCodeBlockForCaller-&gt;getCallLinkInfoForBytecodeIndex(callBytecodeIndex);
 790                 RELEASE_ASSERT(callLinkInfo);
 791 
 792                 jumpTarget = callLinkInfo-&gt;callReturnLocation();
 793                 break;
 794             }
 795 
 796             case InlineCallFrame::GetterCall:
 797             case InlineCallFrame::SetterCall: {
 798                 StructureStubInfo* stubInfo =
 799                     baselineCodeBlockForCaller-&gt;findStubInfo(CodeOrigin(callBytecodeIndex));
 800                 RELEASE_ASSERT(stubInfo);
 801 
 802                 jumpTarget = stubInfo-&gt;doneLocation();
 803                 break;
 804             }
 805 
 806             default:
 807                 RELEASE_ASSERT_NOT_REACHED();
 808             }
 809 
<span class="line-modified"> 810             if (trueCaller-&gt;inlineCallFrame())</span>
<span class="line-modified"> 811                 callerFrame = cpu.fp&lt;uint8_t*&gt;() + trueCaller-&gt;inlineCallFrame()-&gt;stackOffset * sizeof(EncodedJSValue);</span>
 812 
 813             void* targetAddress = jumpTarget.executableAddress();
<span class="line-modified"> 814 #if CPU(ARM64E)</span>
 815             void* newEntrySP = cpu.fp&lt;uint8_t*&gt;() + inlineCallFrame-&gt;returnPCOffset() + sizeof(void*);
 816             targetAddress = retagCodePtr(targetAddress, JSInternalPtrTag, bitwise_cast&lt;PtrTag&gt;(newEntrySP));
 817 #endif
 818             frame.set&lt;void*&gt;(inlineCallFrame-&gt;returnPCOffset(), targetAddress);
 819         }
 820 
 821         frame.setOperand&lt;void*&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::codeBlock, baselineCodeBlock);
 822 
 823         // Restore the inline call frame&#39;s callee save registers.
 824         // If this inlined frame is a tail call that will return back to the original caller, we need to
 825         // copy the prior contents of the tag registers already saved for the outer frame to this frame.
 826         saveOrCopyCalleeSavesFor(context, baselineCodeBlock, VirtualRegister(inlineCallFrame-&gt;stackOffset), !trueCaller);
 827 
 828         if (!inlineCallFrame-&gt;isVarargs())
 829             frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount, PayloadOffset, inlineCallFrame-&gt;argumentCountIncludingThis);
 830         ASSERT(callerFrame);
 831         frame.set&lt;void*&gt;(inlineCallFrame-&gt;callerFrameOffset(), callerFrame);
 832 #if USE(JSVALUE64)
<span class="line-modified"> 833         uint32_t locationBits = CallSiteIndex(codeOrigin-&gt;bytecodeIndex()).bits();</span>
 834         frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount, TagOffset, locationBits);
 835         if (!inlineCallFrame-&gt;isClosureCall)
 836             frame.setOperand(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee, JSValue(inlineCallFrame-&gt;calleeConstant()));
 837 #else // USE(JSVALUE64) // so this is the 32-bit part
<span class="line-modified"> 838         const Instruction* instruction = baselineCodeBlock-&gt;instructions().at(codeOrigin-&gt;bytecodeIndex()).ptr();</span>
 839         uint32_t locationBits = CallSiteIndex(instruction).bits();
 840         frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount, TagOffset, locationBits);
 841         frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee, TagOffset, static_cast&lt;uint32_t&gt;(JSValue::CellTag));
 842         if (!inlineCallFrame-&gt;isClosureCall)
 843             frame.setOperand(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee, PayloadOffset, inlineCallFrame-&gt;calleeConstant());
 844 #endif // USE(JSVALUE64) // ending the #else part, so directly above is the 32-bit part
 845     }
 846 
 847     // Don&#39;t need to set the toplevel code origin if we only did inline tail calls
 848     if (codeOrigin) {
 849 #if USE(JSVALUE64)
<span class="line-modified"> 850         uint32_t locationBits = CallSiteIndex(codeOrigin-&gt;bytecodeIndex()).bits();</span>
 851 #else
<span class="line-modified"> 852         const Instruction* instruction = outermostBaselineCodeBlock-&gt;instructions().at(codeOrigin-&gt;bytecodeIndex()).ptr();</span>
 853         uint32_t locationBits = CallSiteIndex(instruction).bits();
 854 #endif
 855         frame.setOperand&lt;uint32_t&gt;(CallFrameSlot::argumentCount, TagOffset, locationBits);
 856     }
 857 }
 858 
 859 static void adjustAndJumpToTarget(Context&amp; context, VM&amp; vm, CodeBlock* codeBlock, CodeBlock* baselineCodeBlock, OSRExit&amp; exit)
 860 {
 861     OSRExitState* exitState = exit.exitState.get();
 862 
 863     WTF::storeLoadFence(); // The optimizing compiler expects that the OSR exit mechanism will execute this fence.
 864     vm.heap.writeBarrier(baselineCodeBlock);
 865 
 866     // We barrier all inlined frames -- and not just the current inline stack --
 867     // because we don&#39;t know which inlined function owns the value profile that
 868     // we&#39;ll update when we exit. In the case of &quot;f() { a(); b(); }&quot;, if both
 869     // a and b are inlined, we might exit inside b due to a bad value loaded
 870     // from a.
 871     // FIXME: MethodOfGettingAValueProfile should remember which CodeBlock owns
 872     // the value profile.
 873     InlineCallFrameSet* inlineCallFrames = codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;inlineCallFrames.get();
 874     if (inlineCallFrames) {
 875         for (InlineCallFrame* inlineCallFrame : *inlineCallFrames)
 876             vm.heap.writeBarrier(inlineCallFrame-&gt;baselineCodeBlock.get());
 877     }
 878 
<span class="line-modified"> 879     auto* exitInlineCallFrame = exit.m_codeOrigin.inlineCallFrame();</span>
<span class="line-modified"> 880     if (exitInlineCallFrame)</span>
<span class="line-added"> 881         context.fp() = context.fp&lt;uint8_t*&gt;() + exitInlineCallFrame-&gt;stackOffset * sizeof(EncodedJSValue);</span>
 882 
 883     void* jumpTarget = exitState-&gt;jumpTarget;
 884     ASSERT(jumpTarget);
 885 
 886     if (exit.isExceptionHandler()) {
 887         // Since we&#39;re jumping to op_catch, we need to set callFrameForCatch.
 888         vm.callFrameForCatch = context.fp&lt;ExecState*&gt;();
 889     }
 890 
 891     vm.topCallFrame = context.fp&lt;ExecState*&gt;();
 892     context.pc() = untagCodePtr&lt;JSEntryPtrTag&gt;(jumpTarget);
 893 }
 894 
 895 static void printOSRExit(Context&amp; context, uint32_t osrExitIndex, const OSRExit&amp; exit)
 896 {
 897     ExecState* exec = context.fp&lt;ExecState*&gt;();
 898     CodeBlock* codeBlock = exec-&gt;codeBlock();
 899     CodeBlock* alternative = codeBlock-&gt;alternative();
 900     ExitKind kind = exit.m_kind;
<span class="line-modified"> 901     unsigned bytecodeOffset = exit.m_codeOrigin.bytecodeIndex();</span>
 902 
 903     dataLog(&quot;Speculation failure in &quot;, *codeBlock);
 904     dataLog(&quot; @ exit #&quot;, osrExitIndex, &quot; (bc#&quot;, bytecodeOffset, &quot;, &quot;, exitKindToString(kind), &quot;) with &quot;);
 905     if (alternative) {
 906         dataLog(
 907             &quot;executeCounter = &quot;, alternative-&gt;jitExecuteCounter(),
 908             &quot;, reoptimizationRetryCounter = &quot;, alternative-&gt;reoptimizationRetryCounter(),
 909             &quot;, optimizationDelayCounter = &quot;, alternative-&gt;optimizationDelayCounter());
 910     } else
 911         dataLog(&quot;no alternative code block (i.e. we&#39;ve been jettisoned)&quot;);
 912     dataLog(&quot;, osrExitCounter = &quot;, codeBlock-&gt;osrExitCounter(), &quot;\n&quot;);
 913     dataLog(&quot;    GPRs at time of exit:&quot;);
 914     for (unsigned i = 0; i &lt; GPRInfo::numberOfRegisters; ++i) {
 915         GPRReg gpr = GPRInfo::toRegister(i);
 916         dataLog(&quot; &quot;, context.gprName(gpr), &quot;:&quot;, RawPointer(context.gpr&lt;void*&gt;(gpr)));
 917     }
 918     dataLog(&quot;\n&quot;);
 919     dataLog(&quot;    FPRs at time of exit:&quot;);
 920     for (unsigned i = 0; i &lt; FPRInfo::numberOfRegisters; ++i) {
 921         FPRReg fpr = FPRInfo::toRegister(i);
</pre>
<hr />
<pre>
1004         switch (recovery.technique()) {
1005         case DirectArgumentsThatWereNotCreated:
1006             jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationCreateDirectArgumentsDuringExit)), GPRInfo::nonArgGPR0);
1007             break;
1008         case ClonedArgumentsThatWereNotCreated:
1009             jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationCreateClonedArgumentsDuringExit)), GPRInfo::nonArgGPR0);
1010             break;
1011         default:
1012             RELEASE_ASSERT_NOT_REACHED();
1013             break;
1014         }
1015         jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
1016         jit.storeCell(GPRInfo::returnValueGPR, AssemblyHelpers::addressFor(operand));
1017 
1018         alreadyAllocatedArguments.add(id, operand);
1019     }
1020 }
1021 
1022 void JIT_OPERATION OSRExit::compileOSRExit(ExecState* exec)
1023 {
<span class="line-modified">1024     VM&amp; vm = exec-&gt;vm();</span>
<span class="line-modified">1025     auto scope = DECLARE_THROW_SCOPE(vm);</span>
1026 
1027     if (validateDFGDoesGC) {
1028         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
1029         // code running that expects no GC.
<span class="line-modified">1030         vm.heap.setExpectDoesGC(true);</span>
1031     }
1032 
<span class="line-modified">1033     if (vm.callFrameForCatch)</span>
<span class="line-modified">1034         RELEASE_ASSERT(vm.callFrameForCatch == exec);</span>
1035 
1036     CodeBlock* codeBlock = exec-&gt;codeBlock();
1037     ASSERT(codeBlock);
<span class="line-modified">1038     ASSERT(codeBlock-&gt;jitType() == JITType::DFGJIT);</span>
1039 
1040     // It&#39;s sort of preferable that we don&#39;t GC while in here. Anyways, doing so wouldn&#39;t
1041     // really be profitable.
<span class="line-modified">1042     DeferGCForAWhile deferGC(vm.heap);</span>
1043 
<span class="line-modified">1044     uint32_t exitIndex = vm.osrExitIndex;</span>
1045     OSRExit&amp; exit = codeBlock-&gt;jitCode()-&gt;dfg()-&gt;osrExit[exitIndex];
1046 
<span class="line-modified">1047     ASSERT(!vm.callFrameForCatch || exit.m_kind == GenericUnwind);</span>
1048     EXCEPTION_ASSERT_UNUSED(scope, !!scope.exception() || !exit.isExceptionHandler());
1049 
1050     prepareCodeOriginForOSRExit(exec, exit.m_codeOrigin);
1051 
1052     // Compute the value recoveries.
1053     Operands&lt;ValueRecovery&gt; operands;
1054     codeBlock-&gt;jitCode()-&gt;dfg()-&gt;variableEventStream.reconstruct(codeBlock, exit.m_codeOrigin, codeBlock-&gt;jitCode()-&gt;dfg()-&gt;minifiedDFG, exit.m_streamIndex, operands);
1055 
1056     SpeculationRecovery* recovery = 0;
1057     if (exit.m_recoveryIndex != UINT_MAX)
1058         recovery = &amp;codeBlock-&gt;jitCode()-&gt;dfg()-&gt;speculationRecovery[exit.m_recoveryIndex];
1059 
1060     {
1061         CCallHelpers jit(codeBlock);
1062 
1063         if (exit.m_kind == GenericUnwind) {
1064             // We are acting as a defacto op_catch because we arrive here from genericUnwind().
1065             // So, we must restore our call frame and stack pointer.
<span class="line-modified">1066             jit.restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm.topEntryFrame);</span>
<span class="line-modified">1067             jit.loadPtr(vm.addressOfCallFrameForCatch(), GPRInfo::callFrameRegister);</span>
1068         }
1069         jit.addPtr(
1070             CCallHelpers::TrustedImm32(codeBlock-&gt;stackPointerOffset() * sizeof(Register)),
1071             GPRInfo::callFrameRegister, CCallHelpers::stackPointerRegister);
1072 
1073         jit.jitAssertHasValidCallFrame();
1074 
<span class="line-modified">1075         if (UNLIKELY(vm.m_perBytecodeProfiler &amp;&amp; codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation)) {</span>
<span class="line-modified">1076             Profiler::Database&amp; database = *vm.m_perBytecodeProfiler;</span>
1077             Profiler::Compilation* compilation = codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation.get();
1078 
1079             Profiler::OSRExit* profilerExit = compilation-&gt;addOSRExit(
1080                 exitIndex, Profiler::OriginStack(database, codeBlock, exit.m_codeOrigin),
1081                 exit.m_kind, exit.m_kind == UncountableInvalidation);
1082             jit.add64(CCallHelpers::TrustedImm32(1), CCallHelpers::AbsoluteAddress(profilerExit-&gt;counterAddress()));
1083         }
1084 
<span class="line-modified">1085         compileExit(jit, vm, exit, operands, recovery);</span>
1086 
1087         LinkBuffer patchBuffer(jit, codeBlock);
1088         exit.m_code = FINALIZE_CODE_IF(
1089             shouldDumpDisassembly() || Options::verboseOSR() || Options::verboseDFGOSRExit(),
1090             patchBuffer, OSRExitPtrTag,
1091             &quot;DFG OSR exit #%u (%s, %s) from %s, with operands = %s&quot;,
1092                 exitIndex, toCString(exit.m_codeOrigin).data(),
1093                 exitKindToString(exit.m_kind), toCString(*codeBlock).data(),
1094                 toCString(ignoringContext&lt;DumpContext&gt;(operands)).data());
1095     }
1096 
1097     MacroAssembler::repatchJump(exit.codeLocationForRepatch(), CodeLocationLabel&lt;OSRExitPtrTag&gt;(exit.m_code.code()));
1098 
<span class="line-modified">1099     vm.osrExitJumpDestination = exit.m_code.code().executableAddress();</span>
1100 }
1101 
1102 void OSRExit::compileExit(CCallHelpers&amp; jit, VM&amp; vm, const OSRExit&amp; exit, const Operands&lt;ValueRecovery&gt;&amp; operands, SpeculationRecovery* recovery)
1103 {
1104     jit.jitAssertTagsInPlace();
1105 
1106     // Pro-forma stuff.
1107     if (Options::printEachOSRExit()) {
1108         SpeculationFailureDebugInfo* debugInfo = new SpeculationFailureDebugInfo;
1109         debugInfo-&gt;codeBlock = jit.codeBlock();
1110         debugInfo-&gt;kind = exit.m_kind;
<span class="line-modified">1111         debugInfo-&gt;bytecodeOffset = exit.m_codeOrigin.bytecodeIndex();</span>
1112 
1113         jit.debugCall(vm, debugOperationPrintSpeculationFailure, debugInfo);
1114     }
1115 
1116     // Perform speculation recovery. This only comes into play when an operation
1117     // starts mutating state before verifying the speculation it has already made.
1118 
1119     if (recovery) {
1120         switch (recovery-&gt;type()) {
1121         case SpeculativeAdd:
1122             jit.sub32(recovery-&gt;src(), recovery-&gt;dest());
1123 #if USE(JSVALUE64)
1124             jit.or64(GPRInfo::tagTypeNumberRegister, recovery-&gt;dest());
1125 #endif
1126             break;
1127 
<span class="line-added">1128         case SpeculativeAddSelf:</span>
<span class="line-added">1129             // If A + A = A (int32_t) overflows, A can be recovered by ((static_cast&lt;int32_t&gt;(A) &gt;&gt; 1) ^ 0x8000000).</span>
<span class="line-added">1130             jit.rshift32(AssemblyHelpers::TrustedImm32(1), recovery-&gt;dest());</span>
<span class="line-added">1131             jit.xor32(AssemblyHelpers::TrustedImm32(0x80000000), recovery-&gt;dest());</span>
<span class="line-added">1132 #if USE(JSVALUE64)</span>
<span class="line-added">1133             jit.or64(GPRInfo::tagTypeNumberRegister, recovery-&gt;dest());</span>
<span class="line-added">1134 #endif</span>
<span class="line-added">1135             break;</span>
<span class="line-added">1136 </span>
1137         case SpeculativeAddImmediate:
1138             jit.sub32(AssemblyHelpers::Imm32(recovery-&gt;immediate()), recovery-&gt;dest());
1139 #if USE(JSVALUE64)
1140             jit.or64(GPRInfo::tagTypeNumberRegister, recovery-&gt;dest());
1141 #endif
1142             break;
1143 
1144         case BooleanSpeculationCheck:
1145 #if USE(JSVALUE64)
1146             jit.xor64(AssemblyHelpers::TrustedImm32(static_cast&lt;int32_t&gt;(ValueFalse)), recovery-&gt;dest());
1147 #endif
1148             break;
1149 
1150         default:
1151             break;
1152         }
1153     }
1154 
1155     // Refine some array and/or value profile, if appropriate.
1156 
1157     if (!!exit.m_jsValueSource) {
1158         if (exit.m_kind == BadCache || exit.m_kind == BadIndexingType) {
1159             // If the instruction that this originated from has an array profile, then
1160             // refine it. If it doesn&#39;t, then do nothing. The latter could happen for
1161             // hoisted checks, or checks emitted for operations that didn&#39;t have array
1162             // profiling - either ops that aren&#39;t array accesses at all, or weren&#39;t
1163             // known to be array acceses in the bytecode. The latter case is a FIXME
1164             // while the former case is an outcome of a CheckStructure not knowing why
1165             // it was emitted (could be either due to an inline cache of a property
1166             // property access, or due to an array profile).
1167 
1168             CodeOrigin codeOrigin = exit.m_codeOriginForExitProfile;
<span class="line-modified">1169             if (ArrayProfile* arrayProfile = jit.baselineCodeBlockFor(codeOrigin)-&gt;getArrayProfile(codeOrigin.bytecodeIndex())) {</span>
1170 #if USE(JSVALUE64)
1171                 GPRReg usedRegister;
1172                 if (exit.m_jsValueSource.isAddress())
1173                     usedRegister = exit.m_jsValueSource.base();
1174                 else
1175                     usedRegister = exit.m_jsValueSource.gpr();
1176 #else
1177                 GPRReg usedRegister1;
1178                 GPRReg usedRegister2;
1179                 if (exit.m_jsValueSource.isAddress()) {
1180                     usedRegister1 = exit.m_jsValueSource.base();
1181                     usedRegister2 = InvalidGPRReg;
1182                 } else {
1183                     usedRegister1 = exit.m_jsValueSource.payloadGPR();
1184                     if (exit.m_jsValueSource.hasKnownTag())
1185                         usedRegister2 = InvalidGPRReg;
1186                     else
1187                         usedRegister2 = exit.m_jsValueSource.tagGPR();
1188                 }
1189 #endif
</pre>
<hr />
<pre>
1630     // good job of ensuring (a) and (b). But that doesn&#39;t take care of (d),
1631     // since each speculation failure would reset the execute counter.
1632     // So we check here if the number of speculation failures is significantly
1633     // larger than the number of successes (we want 90% success rate), and if
1634     // there have been a large enough number of failures. If so, we set the
1635     // counter to 0; otherwise we set the counter to
1636     // counterValueForOptimizeAfterWarmUp().
1637 
1638     handleExitCounts(jit, exit);
1639 
1640     // Reify inlined call frames.
1641 
1642     reifyInlinedCallFrames(jit, exit);
1643 
1644     // And finish.
1645     adjustAndJumpToTarget(vm, jit, exit);
1646 }
1647 
1648 void JIT_OPERATION OSRExit::debugOperationPrintSpeculationFailure(ExecState* exec, void* debugInfoRaw, void* scratch)
1649 {
<span class="line-modified">1650     VM&amp; vm = exec-&gt;vm();</span>
1651     NativeCallFrameTracer tracer(vm, exec);
1652 
1653     SpeculationFailureDebugInfo* debugInfo = static_cast&lt;SpeculationFailureDebugInfo*&gt;(debugInfoRaw);
1654     CodeBlock* codeBlock = debugInfo-&gt;codeBlock;
1655     CodeBlock* alternative = codeBlock-&gt;alternative();
1656     dataLog(&quot;Speculation failure in &quot;, *codeBlock);
<span class="line-modified">1657     dataLog(&quot; @ exit #&quot;, vm.osrExitIndex, &quot; (bc#&quot;, debugInfo-&gt;bytecodeOffset, &quot;, &quot;, exitKindToString(debugInfo-&gt;kind), &quot;) with &quot;);</span>
1658     if (alternative) {
1659         dataLog(
1660             &quot;executeCounter = &quot;, alternative-&gt;jitExecuteCounter(),
1661             &quot;, reoptimizationRetryCounter = &quot;, alternative-&gt;reoptimizationRetryCounter(),
1662             &quot;, optimizationDelayCounter = &quot;, alternative-&gt;optimizationDelayCounter());
1663     } else
1664         dataLog(&quot;no alternative code block (i.e. we&#39;ve been jettisoned)&quot;);
1665     dataLog(&quot;, osrExitCounter = &quot;, codeBlock-&gt;osrExitCounter(), &quot;\n&quot;);
1666     dataLog(&quot;    GPRs at time of exit:&quot;);
1667     char* scratchPointer = static_cast&lt;char*&gt;(scratch);
1668     for (unsigned i = 0; i &lt; GPRInfo::numberOfRegisters; ++i) {
1669         GPRReg gpr = GPRInfo::toRegister(i);
1670         dataLog(&quot; &quot;, GPRInfo::debugName(gpr), &quot;:&quot;, RawPointer(*reinterpret_cast_ptr&lt;void**&gt;(scratchPointer)));
1671         scratchPointer += sizeof(EncodedJSValue);
1672     }
1673     dataLog(&quot;\n&quot;);
1674     dataLog(&quot;    FPRs at time of exit:&quot;);
1675     for (unsigned i = 0; i &lt; FPRInfo::numberOfRegisters; ++i) {
1676         FPRReg fpr = FPRInfo::toRegister(i);
1677         dataLog(&quot; &quot;, FPRInfo::debugName(fpr), &quot;:&quot;);
</pre>
</td>
</tr>
</table>
<center><a href="DFGOSREntrypointCreationPhase.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGOSRExit.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>