<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGOSRExit.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (C) 2011-2019 Apple Inc. All rights reserved.
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  23  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  24  */
  25 
  26 #include &quot;config.h&quot;
  27 #include &quot;DFGOSRExit.h&quot;
  28 
  29 #if ENABLE(DFG_JIT)
  30 
  31 #include &quot;AssemblyHelpers.h&quot;
  32 #include &quot;ClonedArguments.h&quot;
  33 #include &quot;DFGGraph.h&quot;
  34 #include &quot;DFGMayExit.h&quot;
  35 #include &quot;DFGOSRExitCompilerCommon.h&quot;
  36 #include &quot;DFGOSRExitPreparation.h&quot;
  37 #include &quot;DFGOperations.h&quot;
  38 #include &quot;DFGSpeculativeJIT.h&quot;
  39 #include &quot;DirectArguments.h&quot;
  40 #include &quot;FrameTracers.h&quot;
  41 #include &quot;InlineCallFrame.h&quot;
  42 #include &quot;JSCInlines.h&quot;
  43 #include &quot;JSCJSValue.h&quot;
  44 #include &quot;OperandsInlines.h&quot;
  45 #include &quot;ProbeContext.h&quot;
  46 #include &quot;ProbeFrame.h&quot;
  47 
  48 namespace JSC { namespace DFG {
  49 
  50 // Probe based OSR Exit.
  51 
  52 using CPUState = Probe::CPUState;
  53 using Context = Probe::Context;
  54 using Frame = Probe::Frame;
  55 
  56 static void reifyInlinedCallFrames(Probe::Context&amp;, CodeBlock* baselineCodeBlock, const OSRExitBase&amp;);
  57 static void adjustAndJumpToTarget(Probe::Context&amp;, VM&amp;, CodeBlock*, CodeBlock* baselineCodeBlock, OSRExit&amp;);
  58 static void printOSRExit(Context&amp;, uint32_t osrExitIndex, const OSRExit&amp;);
  59 
  60 static JSValue jsValueFor(CPUState&amp; cpu, JSValueSource source)
  61 {
  62     if (source.isAddress()) {
  63         JSValue result;
  64         std::memcpy(&amp;result, cpu.gpr&lt;uint8_t*&gt;(source.base()) + source.offset(), sizeof(JSValue));
  65         return result;
  66     }
  67 #if USE(JSVALUE64)
  68     return JSValue::decode(cpu.gpr&lt;EncodedJSValue&gt;(source.gpr()));
  69 #else
  70     if (source.hasKnownTag())
  71         return JSValue(source.tag(), cpu.gpr&lt;int32_t&gt;(source.payloadGPR()));
  72     return JSValue(cpu.gpr&lt;int32_t&gt;(source.tagGPR()), cpu.gpr&lt;int32_t&gt;(source.payloadGPR()));
  73 #endif
  74 }
  75 
  76 #if NUMBER_OF_CALLEE_SAVES_REGISTERS &gt; 0
  77 
  78 // Based on AssemblyHelpers::emitRestoreCalleeSavesFor().
  79 static void restoreCalleeSavesFor(Context&amp; context, CodeBlock* codeBlock)
  80 {
  81     ASSERT(codeBlock);
  82 
  83     const RegisterAtOffsetList* calleeSaves = codeBlock-&gt;calleeSaveRegisters();
  84     RegisterSet dontRestoreRegisters = RegisterSet(RegisterSet::stackRegisters(), RegisterSet::allFPRs());
  85     unsigned registerCount = calleeSaves-&gt;size();
  86 
  87     UCPURegister* physicalStackFrame = context.fp&lt;UCPURegister*&gt;();
  88     for (unsigned i = 0; i &lt; registerCount; i++) {
  89         RegisterAtOffset entry = calleeSaves-&gt;at(i);
  90         if (dontRestoreRegisters.get(entry.reg()))
  91             continue;
  92         // The callee saved values come from the original stack, not the recovered stack.
  93         // Hence, we read the values directly from the physical stack memory instead of
  94         // going through context.stack().
  95         ASSERT(!(entry.offset() % sizeof(UCPURegister)));
  96         context.gpr(entry.reg().gpr()) = physicalStackFrame[entry.offset() / sizeof(UCPURegister)];
  97     }
  98 }
  99 
 100 // Based on AssemblyHelpers::emitSaveCalleeSavesFor().
 101 static void saveCalleeSavesFor(Context&amp; context, CodeBlock* codeBlock)
 102 {
 103     auto&amp; stack = context.stack();
 104     ASSERT(codeBlock);
 105 
 106     const RegisterAtOffsetList* calleeSaves = codeBlock-&gt;calleeSaveRegisters();
 107     RegisterSet dontSaveRegisters = RegisterSet(RegisterSet::stackRegisters(), RegisterSet::allFPRs());
 108     unsigned registerCount = calleeSaves-&gt;size();
 109 
 110     for (unsigned i = 0; i &lt; registerCount; i++) {
 111         RegisterAtOffset entry = calleeSaves-&gt;at(i);
 112         if (dontSaveRegisters.get(entry.reg()))
 113             continue;
 114         stack.set(context.fp(), entry.offset(), context.gpr&lt;UCPURegister&gt;(entry.reg().gpr()));
 115     }
 116 }
 117 
 118 // Based on AssemblyHelpers::restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer().
 119 static void restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(Context&amp; context)
 120 {
 121     VM&amp; vm = *context.arg&lt;VM*&gt;();
 122 
 123     RegisterAtOffsetList* allCalleeSaves = RegisterSet::vmCalleeSaveRegisterOffsets();
 124     RegisterSet dontRestoreRegisters = RegisterSet::stackRegisters();
 125     unsigned registerCount = allCalleeSaves-&gt;size();
 126 
 127     VMEntryRecord* entryRecord = vmEntryRecord(vm.topEntryFrame);
 128     UCPURegister* calleeSaveBuffer = reinterpret_cast&lt;UCPURegister*&gt;(entryRecord-&gt;calleeSaveRegistersBuffer);
 129 
 130     // Restore all callee saves.
 131     for (unsigned i = 0; i &lt; registerCount; i++) {
 132         RegisterAtOffset entry = allCalleeSaves-&gt;at(i);
 133         if (dontRestoreRegisters.get(entry.reg()))
 134             continue;
 135         size_t uintptrOffset = entry.offset() / sizeof(UCPURegister);
 136         if (entry.reg().isGPR())
 137             context.gpr(entry.reg().gpr()) = calleeSaveBuffer[uintptrOffset];
 138         else {
 139 #if USE(JSVALUE64)
 140             context.fpr(entry.reg().fpr()) = bitwise_cast&lt;double&gt;(calleeSaveBuffer[uintptrOffset]);
 141 #else
 142             // FIXME: &lt;https://webkit.org/b/193275&gt; support callee-saved floating point registers on 32-bit architectures
 143             RELEASE_ASSERT_NOT_REACHED();
 144 #endif
 145         }
 146     }
 147 }
 148 
 149 // Based on AssemblyHelpers::copyCalleeSavesToVMEntryFrameCalleeSavesBuffer().
 150 static void copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(Context&amp; context)
 151 {
 152     VM&amp; vm = *context.arg&lt;VM*&gt;();
 153     auto&amp; stack = context.stack();
 154 
 155     VMEntryRecord* entryRecord = vmEntryRecord(vm.topEntryFrame);
 156     void* calleeSaveBuffer = entryRecord-&gt;calleeSaveRegistersBuffer;
 157 
 158     RegisterAtOffsetList* allCalleeSaves = RegisterSet::vmCalleeSaveRegisterOffsets();
 159     RegisterSet dontCopyRegisters = RegisterSet::stackRegisters();
 160     unsigned registerCount = allCalleeSaves-&gt;size();
 161 
 162     for (unsigned i = 0; i &lt; registerCount; i++) {
 163         RegisterAtOffset entry = allCalleeSaves-&gt;at(i);
 164         if (dontCopyRegisters.get(entry.reg()))
 165             continue;
 166         if (entry.reg().isGPR())
 167             stack.set(calleeSaveBuffer, entry.offset(), context.gpr&lt;UCPURegister&gt;(entry.reg().gpr()));
 168         else {
 169 #if USE(JSVALUE64)
 170             stack.set(calleeSaveBuffer, entry.offset(), context.fpr&lt;UCPURegister&gt;(entry.reg().fpr()));
 171 #else
 172             // FIXME: &lt;https://webkit.org/b/193275&gt; support callee-saved floating point registers on 32-bit architectures
 173             RELEASE_ASSERT_NOT_REACHED();
 174 #endif
 175         }
 176     }
 177 }
 178 
 179 // Based on AssemblyHelpers::emitSaveOrCopyCalleeSavesFor().
 180 static void saveOrCopyCalleeSavesFor(Context&amp; context, CodeBlock* codeBlock, VirtualRegister offsetVirtualRegister, bool wasCalledViaTailCall)
 181 {
 182     Frame frame(context.fp(), context.stack());
 183     ASSERT(codeBlock);
 184 
 185     const RegisterAtOffsetList* calleeSaves = codeBlock-&gt;calleeSaveRegisters();
 186     RegisterSet dontSaveRegisters = RegisterSet(RegisterSet::stackRegisters(), RegisterSet::allFPRs());
 187     unsigned registerCount = calleeSaves-&gt;size();
 188 
 189     RegisterSet baselineCalleeSaves = RegisterSet::llintBaselineCalleeSaveRegisters();
 190 
 191     for (unsigned i = 0; i &lt; registerCount; i++) {
 192         RegisterAtOffset entry = calleeSaves-&gt;at(i);
 193         if (dontSaveRegisters.get(entry.reg()))
 194             continue;
 195 
 196         uintptr_t savedRegisterValue;
 197 
 198         if (wasCalledViaTailCall &amp;&amp; baselineCalleeSaves.get(entry.reg()))
 199             savedRegisterValue = frame.get&lt;uintptr_t&gt;(entry.offset());
 200         else
 201             savedRegisterValue = context.gpr(entry.reg().gpr());
 202 
 203         frame.set(offsetVirtualRegister.offsetInBytes() + entry.offset(), savedRegisterValue);
 204     }
 205 }
 206 #else // not NUMBER_OF_CALLEE_SAVES_REGISTERS &gt; 0
 207 
 208 static void restoreCalleeSavesFor(Context&amp;, CodeBlock*) { }
 209 static void saveCalleeSavesFor(Context&amp;, CodeBlock*) { }
 210 static void restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(Context&amp;) { }
 211 static void copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(Context&amp;) { }
 212 static void saveOrCopyCalleeSavesFor(Context&amp;, CodeBlock*, VirtualRegister, bool) { }
 213 
 214 #endif // NUMBER_OF_CALLEE_SAVES_REGISTERS &gt; 0
 215 
 216 static JSCell* createDirectArgumentsDuringExit(Context&amp; context, CodeBlock* codeBlock, InlineCallFrame* inlineCallFrame, JSFunction* callee, int32_t argumentCount)
 217 {
 218     VM&amp; vm = *context.arg&lt;VM*&gt;();
 219 
 220     ASSERT(vm.heap.isDeferred());
 221 
 222     if (inlineCallFrame)
 223         codeBlock = baselineCodeBlockForInlineCallFrame(inlineCallFrame);
 224 
 225     unsigned length = argumentCount - 1;
 226     unsigned capacity = std::max(length, static_cast&lt;unsigned&gt;(codeBlock-&gt;numParameters() - 1));
 227     DirectArguments* result = DirectArguments::create(
 228         vm, codeBlock-&gt;globalObject()-&gt;directArgumentsStructure(), length, capacity);
 229 
 230     result-&gt;setCallee(vm, callee);
 231 
 232     void* frameBase = context.fp&lt;Register*&gt;() + (inlineCallFrame ? inlineCallFrame-&gt;stackOffset : 0);
 233     Frame frame(frameBase, context.stack());
 234     for (unsigned i = length; i--;)
 235         result-&gt;setIndexQuickly(vm, i, frame.argument(i));
 236 
 237     return result;
 238 }
 239 
 240 static JSCell* createClonedArgumentsDuringExit(Context&amp; context, CodeBlock* codeBlock, InlineCallFrame* inlineCallFrame, JSFunction* callee, int32_t argumentCount)
 241 {
 242     VM&amp; vm = *context.arg&lt;VM*&gt;();
 243     ExecState* exec = context.fp&lt;ExecState*&gt;();
 244 
 245     ASSERT(vm.heap.isDeferred());
 246 
 247     if (inlineCallFrame)
 248         codeBlock = baselineCodeBlockForInlineCallFrame(inlineCallFrame);
 249 
 250     unsigned length = argumentCount - 1;
 251     ClonedArguments* result = ClonedArguments::createEmpty(
 252         vm, codeBlock-&gt;globalObject()-&gt;clonedArgumentsStructure(), callee, length);
 253 
 254     void* frameBase = context.fp&lt;Register*&gt;() + (inlineCallFrame ? inlineCallFrame-&gt;stackOffset : 0);
 255     Frame frame(frameBase, context.stack());
 256     for (unsigned i = length; i--;)
 257         result-&gt;putDirectIndex(exec, i, frame.argument(i));
 258     return result;
 259 }
 260 
 261 static void emitRestoreArguments(Context&amp; context, CodeBlock* codeBlock, DFG::JITCode* dfgJITCode, const Operands&lt;ValueRecovery&gt;&amp; operands)
 262 {
 263     Frame frame(context.fp(), context.stack());
 264 
 265     HashMap&lt;MinifiedID, int&gt; alreadyAllocatedArguments; // Maps phantom arguments node ID to operand.
 266     for (size_t index = 0; index &lt; operands.size(); ++index) {
 267         const ValueRecovery&amp; recovery = operands[index];
 268         int operand = operands.operandForIndex(index);
 269 
 270         if (recovery.technique() != DirectArgumentsThatWereNotCreated
 271             &amp;&amp; recovery.technique() != ClonedArgumentsThatWereNotCreated)
 272             continue;
 273 
 274         MinifiedID id = recovery.nodeID();
 275         auto iter = alreadyAllocatedArguments.find(id);
 276         if (iter != alreadyAllocatedArguments.end()) {
 277             frame.setOperand(operand, frame.operand(iter-&gt;value));
 278             continue;
 279         }
 280 
 281         InlineCallFrame* inlineCallFrame =
 282             dfgJITCode-&gt;minifiedDFG.at(id)-&gt;inlineCallFrame();
 283 
 284         int stackOffset;
 285         if (inlineCallFrame)
 286             stackOffset = inlineCallFrame-&gt;stackOffset;
 287         else
 288             stackOffset = 0;
 289 
 290         JSFunction* callee;
 291         if (!inlineCallFrame || inlineCallFrame-&gt;isClosureCall)
 292             callee = jsCast&lt;JSFunction*&gt;(frame.operand(stackOffset + CallFrameSlot::callee).asCell());
 293         else
 294             callee = jsCast&lt;JSFunction*&gt;(inlineCallFrame-&gt;calleeRecovery.constant().asCell());
 295 
 296         int32_t argumentCount;
 297         if (!inlineCallFrame || inlineCallFrame-&gt;isVarargs())
 298             argumentCount = frame.operand&lt;int32_t&gt;(stackOffset + CallFrameSlot::argumentCount, PayloadOffset);
 299         else
 300             argumentCount = inlineCallFrame-&gt;argumentCountIncludingThis;
 301 
 302         JSCell* argumentsObject;
 303         switch (recovery.technique()) {
 304         case DirectArgumentsThatWereNotCreated:
 305             argumentsObject = createDirectArgumentsDuringExit(context, codeBlock, inlineCallFrame, callee, argumentCount);
 306             break;
 307         case ClonedArgumentsThatWereNotCreated:
 308             argumentsObject = createClonedArgumentsDuringExit(context, codeBlock, inlineCallFrame, callee, argumentCount);
 309             break;
 310         default:
 311             RELEASE_ASSERT_NOT_REACHED();
 312             break;
 313         }
 314         frame.setOperand(operand, JSValue(argumentsObject));
 315 
 316         alreadyAllocatedArguments.add(id, operand);
 317     }
 318 }
 319 
 320 // The following is a list of extra initializations that need to be done in order
 321 // of most likely needed (lower enum value) to least likely needed (higher enum value).
 322 // Each level initialization includes the previous lower enum value (see use of the
 323 // extraInitializationLevel value below).
 324 enum class ExtraInitializationLevel {
 325     None,
 326     SpeculationRecovery,
 327     ValueProfileUpdate,
 328     ArrayProfileUpdate,
 329     Other
 330 };
 331 
 332 void OSRExit::executeOSRExit(Context&amp; context)
 333 {
 334     VM&amp; vm = *context.arg&lt;VM*&gt;();
 335     auto scope = DECLARE_THROW_SCOPE(vm);
 336 
 337     ExecState* exec = context.fp&lt;ExecState*&gt;();
 338     ASSERT(&amp;exec-&gt;vm() == &amp;vm);
 339     auto&amp; cpu = context.cpu;
 340 
 341     if (validateDFGDoesGC) {
 342         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
 343         // code running that expects no GC.
 344         vm.heap.setExpectDoesGC(true);
 345     }
 346 
 347     if (vm.callFrameForCatch) {
 348         exec = vm.callFrameForCatch;
 349         context.fp() = exec;
 350     }
 351 
 352     CodeBlock* codeBlock = exec-&gt;codeBlock();
 353     ASSERT(codeBlock);
<a name="1" id="anc1"></a><span class="line-modified"> 354     ASSERT(codeBlock-&gt;jitType() == JITCode::DFGJIT);</span>
 355 
 356     // It&#39;s sort of preferable that we don&#39;t GC while in here. Anyways, doing so wouldn&#39;t
 357     // really be profitable.
 358     DeferGCForAWhile deferGC(vm.heap);
 359 
 360     uint32_t exitIndex = vm.osrExitIndex;
 361     DFG::JITCode* dfgJITCode = codeBlock-&gt;jitCode()-&gt;dfg();
 362     OSRExit&amp; exit = dfgJITCode-&gt;osrExit[exitIndex];
 363 
 364     ASSERT(!vm.callFrameForCatch || exit.m_kind == GenericUnwind);
 365     EXCEPTION_ASSERT_UNUSED(scope, !!scope.exception() || !exit.isExceptionHandler());
 366 
 367     if (UNLIKELY(!exit.exitState)) {
 368         ExtraInitializationLevel extraInitializationLevel = ExtraInitializationLevel::None;
 369 
 370         // We only need to execute this block once for each OSRExit record. The computed
 371         // results will be cached in the OSRExitState record for use of the rest of the
 372         // exit ramp code.
 373 
 374         // Ensure we have baseline codeBlocks to OSR exit to.
 375         prepareCodeOriginForOSRExit(exec, exit.m_codeOrigin);
 376 
 377         CodeBlock* baselineCodeBlock = codeBlock-&gt;baselineAlternative();
<a name="2" id="anc2"></a><span class="line-modified"> 378         ASSERT(baselineCodeBlock-&gt;jitType() == JITCode::BaselineJIT);</span>
 379 
 380         SpeculationRecovery* recovery = nullptr;
 381         if (exit.m_recoveryIndex != UINT_MAX) {
 382             recovery = &amp;dfgJITCode-&gt;speculationRecovery[exit.m_recoveryIndex];
 383             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::SpeculationRecovery);
 384         }
 385 
 386         if (UNLIKELY(exit.m_kind == GenericUnwind))
 387             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::Other);
 388 
 389         ArrayProfile* arrayProfile = nullptr;
 390         if (!!exit.m_jsValueSource) {
 391             if (exit.m_valueProfile)
 392                 extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::ValueProfileUpdate);
 393             if (exit.m_kind == BadCache || exit.m_kind == BadIndexingType) {
 394                 CodeOrigin codeOrigin = exit.m_codeOriginForExitProfile;
 395                 CodeBlock* profiledCodeBlock = baselineCodeBlockForOriginAndBaselineCodeBlock(codeOrigin, baselineCodeBlock);
<a name="3" id="anc3"></a><span class="line-modified"> 396                 arrayProfile = profiledCodeBlock-&gt;getArrayProfile(codeOrigin.bytecodeIndex);</span>
 397                 if (arrayProfile)
 398                     extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::ArrayProfileUpdate);
 399             }
 400         }
 401 
 402         int32_t activeThreshold = baselineCodeBlock-&gt;adjustedCounterValue(Options::thresholdForOptimizeAfterLongWarmUp());
 403         double adjustedThreshold = applyMemoryUsageHeuristicsAndConvertToInt(activeThreshold, baselineCodeBlock);
 404         ASSERT(adjustedThreshold &gt; 0);
 405         adjustedThreshold = BaselineExecutionCounter::clippedThreshold(codeBlock-&gt;globalObject(), adjustedThreshold);
 406 
 407         CodeBlock* codeBlockForExit = baselineCodeBlockForOriginAndBaselineCodeBlock(exit.m_codeOrigin, baselineCodeBlock);
 408         const JITCodeMap&amp; codeMap = codeBlockForExit-&gt;jitCodeMap();
<a name="4" id="anc4"></a><span class="line-modified"> 409         CodeLocationLabel&lt;JSEntryPtrTag&gt; codeLocation = codeMap.find(exit.m_codeOrigin.bytecodeIndex);</span>
 410         ASSERT(codeLocation);
 411 
 412         void* jumpTarget = codeLocation.executableAddress();
 413 
 414         // Compute the value recoveries.
 415         Operands&lt;ValueRecovery&gt; operands;
 416         Vector&lt;UndefinedOperandSpan&gt; undefinedOperandSpans;
 417         dfgJITCode-&gt;variableEventStream.reconstruct(codeBlock, exit.m_codeOrigin, dfgJITCode-&gt;minifiedDFG, exit.m_streamIndex, operands, &amp;undefinedOperandSpans);
 418         ptrdiff_t stackPointerOffset = -static_cast&lt;ptrdiff_t&gt;(codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;requiredRegisterCountForExit) * sizeof(Register);
 419 
 420         exit.exitState = adoptRef(new OSRExitState(exit, codeBlock, baselineCodeBlock, operands, WTFMove(undefinedOperandSpans), recovery, stackPointerOffset, activeThreshold, adjustedThreshold, jumpTarget, arrayProfile));
 421 
 422         if (UNLIKELY(vm.m_perBytecodeProfiler &amp;&amp; codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation)) {
 423             Profiler::Database&amp; database = *vm.m_perBytecodeProfiler;
 424             Profiler::Compilation* compilation = codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation.get();
 425 
 426             Profiler::OSRExit* profilerExit = compilation-&gt;addOSRExit(
 427                 exitIndex, Profiler::OriginStack(database, codeBlock, exit.m_codeOrigin),
 428                 exit.m_kind, exit.m_kind == UncountableInvalidation);
 429             exit.exitState-&gt;profilerExit = profilerExit;
 430             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::Other);
 431         }
 432 
 433         if (UNLIKELY(Options::printEachOSRExit()))
 434             extraInitializationLevel = std::max(extraInitializationLevel, ExtraInitializationLevel::Other);
 435 
 436         exit.exitState-&gt;extraInitializationLevel = extraInitializationLevel;
 437 
 438         if (UNLIKELY(Options::verboseOSR() || Options::verboseDFGOSRExit())) {
 439             dataLogF(&quot;DFG OSR exit #%u (%s, %s) from %s, with operands = %s\n&quot;,
 440                 exitIndex, toCString(exit.m_codeOrigin).data(),
 441                 exitKindToString(exit.m_kind), toCString(*codeBlock).data(),
 442                 toCString(ignoringContext&lt;DumpContext&gt;(operands)).data());
 443         }
 444     }
 445 
 446     OSRExitState&amp; exitState = *exit.exitState.get();
 447     CodeBlock* baselineCodeBlock = exitState.baselineCodeBlock;
<a name="5" id="anc5"></a><span class="line-modified"> 448     ASSERT(baselineCodeBlock-&gt;jitType() == JITCode::BaselineJIT);</span>
 449 
 450     Operands&lt;ValueRecovery&gt;&amp; operands = exitState.operands;
 451     Vector&lt;UndefinedOperandSpan&gt;&amp; undefinedOperandSpans = exitState.undefinedOperandSpans;
 452 
 453     context.sp() = context.fp&lt;uint8_t*&gt;() + exitState.stackPointerOffset;
 454 
<a name="6" id="anc6"></a><span class="line-modified"> 455     // The only reason for using this do while look is so we can break out midway when appropriate.</span>
 456     do {
 457         auto extraInitializationLevel = static_cast&lt;ExtraInitializationLevel&gt;(exitState.extraInitializationLevel);
 458 
 459         if (extraInitializationLevel == ExtraInitializationLevel::None)
 460             break;
 461 
 462         // Begin extra initilization level: SpeculationRecovery
 463 
 464         // We need to do speculation recovery first because array profiling and value profiling
 465         // may rely on a value that it recovers. However, that doesn&#39;t mean that it is likely
 466         // to have a recovery value. So, we&#39;ll decorate it as UNLIKELY.
 467         SpeculationRecovery* recovery = exitState.recovery;
 468         if (UNLIKELY(recovery)) {
 469             switch (recovery-&gt;type()) {
 470             case SpeculativeAdd:
 471                 cpu.gpr(recovery-&gt;dest()) = cpu.gpr&lt;uint32_t&gt;(recovery-&gt;dest()) - cpu.gpr&lt;uint32_t&gt;(recovery-&gt;src());
 472 #if USE(JSVALUE64)
 473                 ASSERT(!(cpu.gpr(recovery-&gt;dest()) &gt;&gt; 32));
 474                 cpu.gpr(recovery-&gt;dest()) |= TagTypeNumber;
 475 #endif
 476                 break;
 477 
<a name="7" id="anc7"></a>







 478             case SpeculativeAddImmediate:
 479                 cpu.gpr(recovery-&gt;dest()) = (cpu.gpr&lt;uint32_t&gt;(recovery-&gt;dest()) - recovery-&gt;immediate());
 480 #if USE(JSVALUE64)
 481                 ASSERT(!(cpu.gpr(recovery-&gt;dest()) &gt;&gt; 32));
 482                 cpu.gpr(recovery-&gt;dest()) |= TagTypeNumber;
 483 #endif
 484                 break;
 485 
 486             case BooleanSpeculationCheck:
 487 #if USE(JSVALUE64)
 488                 cpu.gpr(recovery-&gt;dest()) = cpu.gpr(recovery-&gt;dest()) ^ ValueFalse;
 489 #endif
 490                 break;
 491 
 492             default:
 493                 break;
 494             }
 495         }
 496         if (extraInitializationLevel &lt;= ExtraInitializationLevel::SpeculationRecovery)
 497             break;
 498 
 499         // Begin extra initilization level: ValueProfileUpdate
 500         JSValue profiledValue;
 501         if (!!exit.m_jsValueSource) {
 502             profiledValue = jsValueFor(cpu, exit.m_jsValueSource);
 503             if (MethodOfGettingAValueProfile profile = exit.m_valueProfile)
 504                 profile.reportValue(profiledValue);
 505         }
 506         if (extraInitializationLevel &lt;= ExtraInitializationLevel::ValueProfileUpdate)
 507             break;
 508 
 509         // Begin extra initilization level: ArrayProfileUpdate
 510         ArrayProfile* arrayProfile = exitState.arrayProfile;
 511         if (arrayProfile) {
 512             ASSERT(!!exit.m_jsValueSource);
 513             ASSERT(exit.m_kind == BadCache || exit.m_kind == BadIndexingType);
 514             Structure* structure = profiledValue.asCell()-&gt;structure(vm);
 515             arrayProfile-&gt;observeStructure(structure);
 516             arrayProfile-&gt;observeArrayMode(arrayModesFromStructure(structure));
 517         }
 518         if (extraInitializationLevel &lt;= ExtraInitializationLevel::ArrayProfileUpdate)
 519             break;
 520 
 521         // Begin Extra initilization level: Other
 522         if (UNLIKELY(exit.m_kind == GenericUnwind)) {
 523             // We are acting as a defacto op_catch because we arrive here from genericUnwind().
 524             // So, we must restore our call frame and stack pointer.
 525             restoreCalleeSavesFromVMEntryFrameCalleeSavesBuffer(context);
 526             ASSERT(context.fp() == vm.callFrameForCatch);
 527         }
 528 
 529         if (exitState.profilerExit)
 530             exitState.profilerExit-&gt;incCount();
 531 
 532         if (UNLIKELY(Options::printEachOSRExit()))
 533             printOSRExit(context, vm.osrExitIndex, exit);
 534 
 535     } while (false); // End extra initialization.
 536 
 537     Frame frame(cpu.fp(), context.stack());
 538     ASSERT(!(context.fp&lt;uintptr_t&gt;() &amp; 0x7));
 539 
 540 #if USE(JSVALUE64)
 541     ASSERT(cpu.gpr(GPRInfo::tagTypeNumberRegister) == TagTypeNumber);
 542     ASSERT(cpu.gpr(GPRInfo::tagMaskRegister) == TagMask);
 543 #endif
 544 
 545     // Do all data format conversions and store the results into the stack.
 546     // Note: we need to recover values before restoring callee save registers below
 547     // because the recovery may rely on values in some of callee save registers.
 548 
 549     int calleeSaveSpaceAsVirtualRegisters = static_cast&lt;int&gt;(baselineCodeBlock-&gt;calleeSaveSpaceAsVirtualRegisters());
 550     size_t numberOfOperands = operands.size();
 551     size_t numUndefinedOperandSpans = undefinedOperandSpans.size();
 552 
 553     size_t nextUndefinedSpanIndex = 0;
 554     size_t nextUndefinedOperandIndex = numberOfOperands;
 555     if (numUndefinedOperandSpans)
 556         nextUndefinedOperandIndex = undefinedOperandSpans[nextUndefinedSpanIndex].firstIndex;
 557 
 558     JSValue undefined = jsUndefined();
 559     for (size_t spanIndex = 0; spanIndex &lt; numUndefinedOperandSpans; ++spanIndex) {
 560         auto&amp; span = undefinedOperandSpans[spanIndex];
 561         int firstOffset = span.minOffset;
 562         int lastOffset = firstOffset + span.numberOfRegisters;
 563 
 564         for (int offset = firstOffset; offset &lt; lastOffset; ++offset)
 565             frame.setOperand(offset, undefined);
 566     }
 567 
 568     for (size_t index = 0; index &lt; numberOfOperands; ++index) {
 569         const ValueRecovery&amp; recovery = operands[index];
 570         VirtualRegister reg = operands.virtualRegisterForIndex(index);
 571 
 572         if (UNLIKELY(index == nextUndefinedOperandIndex)) {
 573             index += undefinedOperandSpans[nextUndefinedSpanIndex++].numberOfRegisters - 1;
 574             if (nextUndefinedSpanIndex &lt; numUndefinedOperandSpans)
 575                 nextUndefinedOperandIndex = undefinedOperandSpans[nextUndefinedSpanIndex].firstIndex;
 576             else
 577                 nextUndefinedOperandIndex = numberOfOperands;
 578             continue;
 579         }
 580 
 581         if (reg.isLocal() &amp;&amp; reg.toLocal() &lt; calleeSaveSpaceAsVirtualRegisters)
 582             continue;
 583 
 584         int operand = reg.offset();
 585 
 586         switch (recovery.technique()) {
 587         case DisplacedInJSStack:
 588             frame.setOperand(operand, exec-&gt;r(recovery.virtualRegister()).asanUnsafeJSValue());
 589             break;
 590 
 591         case InFPR:
 592             frame.setOperand(operand, cpu.fpr&lt;JSValue&gt;(recovery.fpr()));
 593             break;
 594 
 595 #if USE(JSVALUE64)
 596         case InGPR:
 597             frame.setOperand(operand, cpu.gpr&lt;JSValue&gt;(recovery.gpr()));
 598             break;
 599 #else
 600         case InPair:
 601             frame.setOperand(operand, JSValue(cpu.gpr&lt;int32_t&gt;(recovery.tagGPR()), cpu.gpr&lt;int32_t&gt;(recovery.payloadGPR())));
 602             break;
 603 #endif
 604 
 605         case UnboxedCellInGPR:
 606             frame.setOperand(operand, JSValue(cpu.gpr&lt;JSCell*&gt;(recovery.gpr())));
 607             break;
 608 
 609         case CellDisplacedInJSStack:
 610             frame.setOperand(operand, JSValue(exec-&gt;r(recovery.virtualRegister()).asanUnsafeUnboxedCell()));
 611             break;
 612 
 613 #if USE(JSVALUE32_64)
 614         case UnboxedBooleanInGPR:
 615             frame.setOperand(operand, jsBoolean(cpu.gpr&lt;bool&gt;(recovery.gpr())));
 616             break;
 617 #endif
 618 
 619         case BooleanDisplacedInJSStack:
 620 #if USE(JSVALUE64)
 621             frame.setOperand(operand, exec-&gt;r(recovery.virtualRegister()).asanUnsafeJSValue());
 622 #else
 623             frame.setOperand(operand, jsBoolean(exec-&gt;r(recovery.virtualRegister()).asanUnsafeJSValue().payload()));
 624 #endif
 625             break;
 626 
 627         case UnboxedInt32InGPR:
 628             frame.setOperand(operand, JSValue(cpu.gpr&lt;int32_t&gt;(recovery.gpr())));
 629             break;
 630 
 631         case Int32DisplacedInJSStack:
 632             frame.setOperand(operand, JSValue(exec-&gt;r(recovery.virtualRegister()).asanUnsafeUnboxedInt32()));
 633             break;
 634 
 635 #if USE(JSVALUE64)
 636         case UnboxedInt52InGPR:
 637             frame.setOperand(operand, JSValue(cpu.gpr&lt;int64_t&gt;(recovery.gpr()) &gt;&gt; JSValue::int52ShiftAmount));
 638             break;
 639 
 640         case Int52DisplacedInJSStack:
 641             frame.setOperand(operand, JSValue(exec-&gt;r(recovery.virtualRegister()).asanUnsafeUnboxedInt52()));
 642             break;
 643 
 644         case UnboxedStrictInt52InGPR:
 645             frame.setOperand(operand, JSValue(cpu.gpr&lt;int64_t&gt;(recovery.gpr())));
 646             break;
 647 
 648         case StrictInt52DisplacedInJSStack:
 649             frame.setOperand(operand, JSValue(exec-&gt;r(recovery.virtualRegister()).asanUnsafeUnboxedStrictInt52()));
 650             break;
 651 #endif
 652 
 653         case UnboxedDoubleInFPR:
 654             frame.setOperand(operand, JSValue(JSValue::EncodeAsDouble, purifyNaN(cpu.fpr(recovery.fpr()))));
 655             break;
 656 
 657         case DoubleDisplacedInJSStack:
 658             frame.setOperand(operand, JSValue(JSValue::EncodeAsDouble, purifyNaN(exec-&gt;r(recovery.virtualRegister()).asanUnsafeUnboxedDouble())));
 659             break;
 660 
 661         case Constant:
 662             frame.setOperand(operand, recovery.constant());
 663             break;
 664 
 665         case DirectArgumentsThatWereNotCreated:
 666         case ClonedArgumentsThatWereNotCreated:
 667             // Don&#39;t do this, yet.
 668             break;
 669 
 670         default:
 671             RELEASE_ASSERT_NOT_REACHED();
 672             break;
 673         }
 674     }
 675 
 676     // Restore the DFG callee saves and then save the ones the baseline JIT uses.
 677     restoreCalleeSavesFor(context, codeBlock);
 678     saveCalleeSavesFor(context, baselineCodeBlock);
 679 
 680 #if USE(JSVALUE64)
 681     cpu.gpr(GPRInfo::tagTypeNumberRegister) = static_cast&lt;uintptr_t&gt;(TagTypeNumber);
 682     cpu.gpr(GPRInfo::tagMaskRegister) = static_cast&lt;uintptr_t&gt;(TagTypeNumber | TagBitTypeOther);
 683 #endif
 684 
 685     if (exit.isExceptionHandler())
 686         copyCalleeSavesToVMEntryFrameCalleeSavesBuffer(context);
 687 
 688     // Now that things on the stack are recovered, do the arguments recovery. We assume that arguments
 689     // recoveries don&#39;t recursively refer to each other. But, we don&#39;t try to assume that they only
 690     // refer to certain ranges of locals. Hence why we need to do this here, once the stack is sensible.
 691     // Note that we also roughly assume that the arguments might still be materialized outside of its
 692     // inline call frame scope - but for now the DFG wouldn&#39;t do that.
 693 
 694     DFG::emitRestoreArguments(context, codeBlock, dfgJITCode, operands);
 695 
 696     // Adjust the old JIT&#39;s execute counter. Since we are exiting OSR, we know
 697     // that all new calls into this code will go to the new JIT, so the execute
 698     // counter only affects call frames that performed OSR exit and call frames
 699     // that were still executing the old JIT at the time of another call frame&#39;s
 700     // OSR exit. We want to ensure that the following is true:
 701     //
 702     // (a) Code the performs an OSR exit gets a chance to reenter optimized
 703     //     code eventually, since optimized code is faster. But we don&#39;t
 704     //     want to do such reentery too aggressively (see (c) below).
 705     //
 706     // (b) If there is code on the call stack that is still running the old
 707     //     JIT&#39;s code and has never OSR&#39;d, then it should get a chance to
 708     //     perform OSR entry despite the fact that we&#39;ve exited.
 709     //
 710     // (c) Code the performs an OSR exit should not immediately retry OSR
 711     //     entry, since both forms of OSR are expensive. OSR entry is
 712     //     particularly expensive.
 713     //
 714     // (d) Frequent OSR failures, even those that do not result in the code
 715     //     running in a hot loop, result in recompilation getting triggered.
 716     //
 717     // To ensure (c), we&#39;d like to set the execute counter to
 718     // counterValueForOptimizeAfterWarmUp(). This seems like it would endanger
 719     // (a) and (b), since then every OSR exit would delay the opportunity for
 720     // every call frame to perform OSR entry. Essentially, if OSR exit happens
 721     // frequently and the function has few loops, then the counter will never
 722     // become non-negative and OSR entry will never be triggered. OSR entry
 723     // will only happen if a loop gets hot in the old JIT, which does a pretty
 724     // good job of ensuring (a) and (b). But that doesn&#39;t take care of (d),
 725     // since each speculation failure would reset the execute counter.
 726     // So we check here if the number of speculation failures is significantly
 727     // larger than the number of successes (we want 90% success rate), and if
 728     // there have been a large enough number of failures. If so, we set the
 729     // counter to 0; otherwise we set the counter to
 730     // counterValueForOptimizeAfterWarmUp().
 731 
 732     if (UNLIKELY(codeBlock-&gt;updateOSRExitCounterAndCheckIfNeedToReoptimize(exitState) == CodeBlock::OptimizeAction::ReoptimizeNow))
 733         triggerReoptimizationNow(baselineCodeBlock, codeBlock, &amp;exit);
 734 
 735     reifyInlinedCallFrames(context, baselineCodeBlock, exit);
 736     adjustAndJumpToTarget(context, vm, codeBlock, baselineCodeBlock, exit);
 737 }
 738 
 739 static void reifyInlinedCallFrames(Context&amp; context, CodeBlock* outermostBaselineCodeBlock, const OSRExitBase&amp; exit)
 740 {
 741     auto&amp; cpu = context.cpu;
 742     Frame frame(cpu.fp(), context.stack());
 743 
 744     // FIXME: We shouldn&#39;t leave holes on the stack when performing an OSR exit
 745     // in presence of inlined tail calls.
 746     // https://bugs.webkit.org/show_bug.cgi?id=147511
<a name="8" id="anc8"></a><span class="line-modified"> 747     ASSERT(outermostBaselineCodeBlock-&gt;jitType() == JITCode::BaselineJIT);</span>
 748     frame.setOperand&lt;CodeBlock*&gt;(CallFrameSlot::codeBlock, outermostBaselineCodeBlock);
 749 
 750     const CodeOrigin* codeOrigin;
<a name="9" id="anc9"></a><span class="line-modified"> 751     for (codeOrigin = &amp;exit.m_codeOrigin; codeOrigin &amp;&amp; codeOrigin-&gt;inlineCallFrame; codeOrigin = codeOrigin-&gt;inlineCallFrame-&gt;getCallerSkippingTailCalls()) {</span>
<span class="line-modified"> 752         InlineCallFrame* inlineCallFrame = codeOrigin-&gt;inlineCallFrame;</span>
 753         CodeBlock* baselineCodeBlock = baselineCodeBlockForOriginAndBaselineCodeBlock(*codeOrigin, outermostBaselineCodeBlock);
 754         InlineCallFrame::Kind trueCallerCallKind;
 755         CodeOrigin* trueCaller = inlineCallFrame-&gt;getCallerSkippingTailCalls(&amp;trueCallerCallKind);
 756         void* callerFrame = cpu.fp();
 757 
 758         if (!trueCaller) {
 759             ASSERT(inlineCallFrame-&gt;isTail());
 760             void* returnPC = frame.get&lt;void*&gt;(CallFrame::returnPCOffset());
<a name="10" id="anc10"></a><span class="line-modified"> 761 #if USE(POINTER_PROFILING)</span>
 762             void* oldEntrySP = cpu.fp&lt;uint8_t*&gt;() + sizeof(CallerFrameAndPC);
 763             void* newEntrySP = cpu.fp&lt;uint8_t*&gt;() + inlineCallFrame-&gt;returnPCOffset() + sizeof(void*);
 764             returnPC = retagCodePtr(returnPC, bitwise_cast&lt;PtrTag&gt;(oldEntrySP), bitwise_cast&lt;PtrTag&gt;(newEntrySP));
 765 #endif
 766             frame.set&lt;void*&gt;(inlineCallFrame-&gt;returnPCOffset(), returnPC);
 767             callerFrame = frame.get&lt;void*&gt;(CallFrame::callerFrameOffset());
 768         } else {
 769             CodeBlock* baselineCodeBlockForCaller = baselineCodeBlockForOriginAndBaselineCodeBlock(*trueCaller, outermostBaselineCodeBlock);
<a name="11" id="anc11"></a><span class="line-modified"> 770             unsigned callBytecodeIndex = trueCaller-&gt;bytecodeIndex;</span>
 771             MacroAssemblerCodePtr&lt;JSInternalPtrTag&gt; jumpTarget;
 772 
 773             switch (trueCallerCallKind) {
 774             case InlineCallFrame::Call:
 775             case InlineCallFrame::Construct:
 776             case InlineCallFrame::CallVarargs:
 777             case InlineCallFrame::ConstructVarargs:
 778             case InlineCallFrame::TailCall:
 779             case InlineCallFrame::TailCallVarargs: {
 780                 CallLinkInfo* callLinkInfo =
 781                     baselineCodeBlockForCaller-&gt;getCallLinkInfoForBytecodeIndex(callBytecodeIndex);
 782                 RELEASE_ASSERT(callLinkInfo);
 783 
 784                 jumpTarget = callLinkInfo-&gt;callReturnLocation();
 785                 break;
 786             }
 787 
 788             case InlineCallFrame::GetterCall:
 789             case InlineCallFrame::SetterCall: {
 790                 StructureStubInfo* stubInfo =
 791                     baselineCodeBlockForCaller-&gt;findStubInfo(CodeOrigin(callBytecodeIndex));
 792                 RELEASE_ASSERT(stubInfo);
 793 
 794                 jumpTarget = stubInfo-&gt;doneLocation();
 795                 break;
 796             }
 797 
 798             default:
 799                 RELEASE_ASSERT_NOT_REACHED();
 800             }
 801 
<a name="12" id="anc12"></a><span class="line-modified"> 802             if (trueCaller-&gt;inlineCallFrame)</span>
<span class="line-modified"> 803                 callerFrame = cpu.fp&lt;uint8_t*&gt;() + trueCaller-&gt;inlineCallFrame-&gt;stackOffset * sizeof(EncodedJSValue);</span>
 804 
 805             void* targetAddress = jumpTarget.executableAddress();
<a name="13" id="anc13"></a><span class="line-modified"> 806 #if USE(POINTER_PROFILING)</span>
 807             void* newEntrySP = cpu.fp&lt;uint8_t*&gt;() + inlineCallFrame-&gt;returnPCOffset() + sizeof(void*);
 808             targetAddress = retagCodePtr(targetAddress, JSInternalPtrTag, bitwise_cast&lt;PtrTag&gt;(newEntrySP));
 809 #endif
 810             frame.set&lt;void*&gt;(inlineCallFrame-&gt;returnPCOffset(), targetAddress);
 811         }
 812 
 813         frame.setOperand&lt;void*&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::codeBlock, baselineCodeBlock);
 814 
 815         // Restore the inline call frame&#39;s callee save registers.
 816         // If this inlined frame is a tail call that will return back to the original caller, we need to
 817         // copy the prior contents of the tag registers already saved for the outer frame to this frame.
 818         saveOrCopyCalleeSavesFor(context, baselineCodeBlock, VirtualRegister(inlineCallFrame-&gt;stackOffset), !trueCaller);
 819 
 820         if (!inlineCallFrame-&gt;isVarargs())
 821             frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount, PayloadOffset, inlineCallFrame-&gt;argumentCountIncludingThis);
 822         ASSERT(callerFrame);
 823         frame.set&lt;void*&gt;(inlineCallFrame-&gt;callerFrameOffset(), callerFrame);
 824 #if USE(JSVALUE64)
<a name="14" id="anc14"></a><span class="line-modified"> 825         uint32_t locationBits = CallSiteIndex(codeOrigin-&gt;bytecodeIndex).bits();</span>
 826         frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount, TagOffset, locationBits);
 827         if (!inlineCallFrame-&gt;isClosureCall)
 828             frame.setOperand(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee, JSValue(inlineCallFrame-&gt;calleeConstant()));
 829 #else // USE(JSVALUE64) // so this is the 32-bit part
<a name="15" id="anc15"></a><span class="line-modified"> 830         const Instruction* instruction = baselineCodeBlock-&gt;instructions().at(codeOrigin-&gt;bytecodeIndex).ptr();</span>
 831         uint32_t locationBits = CallSiteIndex(instruction).bits();
 832         frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::argumentCount, TagOffset, locationBits);
 833         frame.setOperand&lt;uint32_t&gt;(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee, TagOffset, static_cast&lt;uint32_t&gt;(JSValue::CellTag));
 834         if (!inlineCallFrame-&gt;isClosureCall)
 835             frame.setOperand(inlineCallFrame-&gt;stackOffset + CallFrameSlot::callee, PayloadOffset, inlineCallFrame-&gt;calleeConstant());
 836 #endif // USE(JSVALUE64) // ending the #else part, so directly above is the 32-bit part
 837     }
 838 
 839     // Don&#39;t need to set the toplevel code origin if we only did inline tail calls
 840     if (codeOrigin) {
 841 #if USE(JSVALUE64)
<a name="16" id="anc16"></a><span class="line-modified"> 842         uint32_t locationBits = CallSiteIndex(codeOrigin-&gt;bytecodeIndex).bits();</span>
 843 #else
<a name="17" id="anc17"></a><span class="line-modified"> 844         const Instruction* instruction = outermostBaselineCodeBlock-&gt;instructions().at(codeOrigin-&gt;bytecodeIndex).ptr();</span>
 845         uint32_t locationBits = CallSiteIndex(instruction).bits();
 846 #endif
 847         frame.setOperand&lt;uint32_t&gt;(CallFrameSlot::argumentCount, TagOffset, locationBits);
 848     }
 849 }
 850 
 851 static void adjustAndJumpToTarget(Context&amp; context, VM&amp; vm, CodeBlock* codeBlock, CodeBlock* baselineCodeBlock, OSRExit&amp; exit)
 852 {
 853     OSRExitState* exitState = exit.exitState.get();
 854 
 855     WTF::storeLoadFence(); // The optimizing compiler expects that the OSR exit mechanism will execute this fence.
 856     vm.heap.writeBarrier(baselineCodeBlock);
 857 
 858     // We barrier all inlined frames -- and not just the current inline stack --
 859     // because we don&#39;t know which inlined function owns the value profile that
 860     // we&#39;ll update when we exit. In the case of &quot;f() { a(); b(); }&quot;, if both
 861     // a and b are inlined, we might exit inside b due to a bad value loaded
 862     // from a.
 863     // FIXME: MethodOfGettingAValueProfile should remember which CodeBlock owns
 864     // the value profile.
 865     InlineCallFrameSet* inlineCallFrames = codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;inlineCallFrames.get();
 866     if (inlineCallFrames) {
 867         for (InlineCallFrame* inlineCallFrame : *inlineCallFrames)
 868             vm.heap.writeBarrier(inlineCallFrame-&gt;baselineCodeBlock.get());
 869     }
 870 
<a name="18" id="anc18"></a><span class="line-modified"> 871     if (exit.m_codeOrigin.inlineCallFrame)</span>
<span class="line-modified"> 872         context.fp() = context.fp&lt;uint8_t*&gt;() + exit.m_codeOrigin.inlineCallFrame-&gt;stackOffset * sizeof(EncodedJSValue);</span>

 873 
 874     void* jumpTarget = exitState-&gt;jumpTarget;
 875     ASSERT(jumpTarget);
 876 
 877     if (exit.isExceptionHandler()) {
 878         // Since we&#39;re jumping to op_catch, we need to set callFrameForCatch.
 879         vm.callFrameForCatch = context.fp&lt;ExecState*&gt;();
 880     }
 881 
 882     vm.topCallFrame = context.fp&lt;ExecState*&gt;();
 883     context.pc() = untagCodePtr&lt;JSEntryPtrTag&gt;(jumpTarget);
 884 }
 885 
 886 static void printOSRExit(Context&amp; context, uint32_t osrExitIndex, const OSRExit&amp; exit)
 887 {
 888     ExecState* exec = context.fp&lt;ExecState*&gt;();
 889     CodeBlock* codeBlock = exec-&gt;codeBlock();
 890     CodeBlock* alternative = codeBlock-&gt;alternative();
 891     ExitKind kind = exit.m_kind;
<a name="19" id="anc19"></a><span class="line-modified"> 892     unsigned bytecodeOffset = exit.m_codeOrigin.bytecodeIndex;</span>
 893 
 894     dataLog(&quot;Speculation failure in &quot;, *codeBlock);
 895     dataLog(&quot; @ exit #&quot;, osrExitIndex, &quot; (bc#&quot;, bytecodeOffset, &quot;, &quot;, exitKindToString(kind), &quot;) with &quot;);
 896     if (alternative) {
 897         dataLog(
 898             &quot;executeCounter = &quot;, alternative-&gt;jitExecuteCounter(),
 899             &quot;, reoptimizationRetryCounter = &quot;, alternative-&gt;reoptimizationRetryCounter(),
 900             &quot;, optimizationDelayCounter = &quot;, alternative-&gt;optimizationDelayCounter());
 901     } else
 902         dataLog(&quot;no alternative code block (i.e. we&#39;ve been jettisoned)&quot;);
 903     dataLog(&quot;, osrExitCounter = &quot;, codeBlock-&gt;osrExitCounter(), &quot;\n&quot;);
 904     dataLog(&quot;    GPRs at time of exit:&quot;);
 905     for (unsigned i = 0; i &lt; GPRInfo::numberOfRegisters; ++i) {
 906         GPRReg gpr = GPRInfo::toRegister(i);
 907         dataLog(&quot; &quot;, context.gprName(gpr), &quot;:&quot;, RawPointer(context.gpr&lt;void*&gt;(gpr)));
 908     }
 909     dataLog(&quot;\n&quot;);
 910     dataLog(&quot;    FPRs at time of exit:&quot;);
 911     for (unsigned i = 0; i &lt; FPRInfo::numberOfRegisters; ++i) {
 912         FPRReg fpr = FPRInfo::toRegister(i);
 913         dataLog(&quot; &quot;, context.fprName(fpr), &quot;:&quot;);
 914         uint64_t bits = context.fpr&lt;uint64_t&gt;(fpr);
 915         double value = context.fpr(fpr);
 916         dataLogF(&quot;%llx:%lf&quot;, static_cast&lt;long long&gt;(bits), value);
 917     }
 918     dataLog(&quot;\n&quot;);
 919 }
 920 
 921 // JIT based OSR Exit.
 922 
 923 OSRExit::OSRExit(ExitKind kind, JSValueSource jsValueSource, MethodOfGettingAValueProfile valueProfile, SpeculativeJIT* jit, unsigned streamIndex, unsigned recoveryIndex)
 924     : OSRExitBase(kind, jit-&gt;m_origin.forExit, jit-&gt;m_origin.semantic, jit-&gt;m_origin.wasHoisted)
 925     , m_jsValueSource(jsValueSource)
 926     , m_valueProfile(valueProfile)
 927     , m_recoveryIndex(recoveryIndex)
 928     , m_streamIndex(streamIndex)
 929 {
 930     bool canExit = jit-&gt;m_origin.exitOK;
 931     if (!canExit &amp;&amp; jit-&gt;m_currentNode) {
 932         ExitMode exitMode = mayExit(jit-&gt;m_jit.graph(), jit-&gt;m_currentNode);
 933         canExit = exitMode == ExitMode::Exits || exitMode == ExitMode::ExitsForExceptions;
 934     }
 935     DFG_ASSERT(jit-&gt;m_jit.graph(), jit-&gt;m_currentNode, canExit);
 936 }
 937 
 938 CodeLocationJump&lt;JSInternalPtrTag&gt; OSRExit::codeLocationForRepatch() const
 939 {
 940     return CodeLocationJump&lt;JSInternalPtrTag&gt;(m_patchableJumpLocation);
 941 }
 942 
 943 void OSRExit::emitRestoreArguments(CCallHelpers&amp; jit, const Operands&lt;ValueRecovery&gt;&amp; operands)
 944 {
 945     HashMap&lt;MinifiedID, int&gt; alreadyAllocatedArguments; // Maps phantom arguments node ID to operand.
 946     for (size_t index = 0; index &lt; operands.size(); ++index) {
 947         const ValueRecovery&amp; recovery = operands[index];
 948         int operand = operands.operandForIndex(index);
 949 
 950         if (recovery.technique() != DirectArgumentsThatWereNotCreated
 951             &amp;&amp; recovery.technique() != ClonedArgumentsThatWereNotCreated)
 952             continue;
 953 
 954         MinifiedID id = recovery.nodeID();
 955         auto iter = alreadyAllocatedArguments.find(id);
 956         if (iter != alreadyAllocatedArguments.end()) {
 957             JSValueRegs regs = JSValueRegs::withTwoAvailableRegs(GPRInfo::regT0, GPRInfo::regT1);
 958             jit.loadValue(CCallHelpers::addressFor(iter-&gt;value), regs);
 959             jit.storeValue(regs, CCallHelpers::addressFor(operand));
 960             continue;
 961         }
 962 
 963         InlineCallFrame* inlineCallFrame =
 964             jit.codeBlock()-&gt;jitCode()-&gt;dfg()-&gt;minifiedDFG.at(id)-&gt;inlineCallFrame();
 965 
 966         int stackOffset;
 967         if (inlineCallFrame)
 968             stackOffset = inlineCallFrame-&gt;stackOffset;
 969         else
 970             stackOffset = 0;
 971 
 972         if (!inlineCallFrame || inlineCallFrame-&gt;isClosureCall) {
 973             jit.loadPtr(
 974                 AssemblyHelpers::addressFor(stackOffset + CallFrameSlot::callee),
 975                 GPRInfo::regT0);
 976         } else {
 977             jit.move(
 978                 AssemblyHelpers::TrustedImmPtr(inlineCallFrame-&gt;calleeRecovery.constant().asCell()),
 979                 GPRInfo::regT0);
 980         }
 981 
 982         if (!inlineCallFrame || inlineCallFrame-&gt;isVarargs()) {
 983             jit.load32(
 984                 AssemblyHelpers::payloadFor(stackOffset + CallFrameSlot::argumentCount),
 985                 GPRInfo::regT1);
 986         } else {
 987             jit.move(
 988                 AssemblyHelpers::TrustedImm32(inlineCallFrame-&gt;argumentCountIncludingThis),
 989                 GPRInfo::regT1);
 990         }
 991 
 992         static_assert(std::is_same&lt;decltype(operationCreateDirectArgumentsDuringExit), decltype(operationCreateClonedArgumentsDuringExit)&gt;::value, &quot;We assume these functions have the same signature below.&quot;);
 993         jit.setupArguments&lt;decltype(operationCreateDirectArgumentsDuringExit)&gt;(
 994             AssemblyHelpers::TrustedImmPtr(inlineCallFrame), GPRInfo::regT0, GPRInfo::regT1);
 995         switch (recovery.technique()) {
 996         case DirectArgumentsThatWereNotCreated:
 997             jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationCreateDirectArgumentsDuringExit)), GPRInfo::nonArgGPR0);
 998             break;
 999         case ClonedArgumentsThatWereNotCreated:
1000             jit.move(AssemblyHelpers::TrustedImmPtr(tagCFunctionPtr&lt;OperationPtrTag&gt;(operationCreateClonedArgumentsDuringExit)), GPRInfo::nonArgGPR0);
1001             break;
1002         default:
1003             RELEASE_ASSERT_NOT_REACHED();
1004             break;
1005         }
1006         jit.call(GPRInfo::nonArgGPR0, OperationPtrTag);
1007         jit.storeCell(GPRInfo::returnValueGPR, AssemblyHelpers::addressFor(operand));
1008 
1009         alreadyAllocatedArguments.add(id, operand);
1010     }
1011 }
1012 
1013 void JIT_OPERATION OSRExit::compileOSRExit(ExecState* exec)
1014 {
<a name="20" id="anc20"></a><span class="line-modified">1015     VM* vm = &amp;exec-&gt;vm();</span>
<span class="line-modified">1016     auto scope = DECLARE_THROW_SCOPE(*vm);</span>
1017 
1018     if (validateDFGDoesGC) {
1019         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
1020         // code running that expects no GC.
<a name="21" id="anc21"></a><span class="line-modified">1021         vm-&gt;heap.setExpectDoesGC(true);</span>
1022     }
1023 
<a name="22" id="anc22"></a><span class="line-modified">1024     if (vm-&gt;callFrameForCatch)</span>
<span class="line-modified">1025         RELEASE_ASSERT(vm-&gt;callFrameForCatch == exec);</span>
1026 
1027     CodeBlock* codeBlock = exec-&gt;codeBlock();
1028     ASSERT(codeBlock);
<a name="23" id="anc23"></a><span class="line-modified">1029     ASSERT(codeBlock-&gt;jitType() == JITCode::DFGJIT);</span>
1030 
1031     // It&#39;s sort of preferable that we don&#39;t GC while in here. Anyways, doing so wouldn&#39;t
1032     // really be profitable.
<a name="24" id="anc24"></a><span class="line-modified">1033     DeferGCForAWhile deferGC(vm-&gt;heap);</span>
1034 
<a name="25" id="anc25"></a><span class="line-modified">1035     uint32_t exitIndex = vm-&gt;osrExitIndex;</span>
1036     OSRExit&amp; exit = codeBlock-&gt;jitCode()-&gt;dfg()-&gt;osrExit[exitIndex];
1037 
<a name="26" id="anc26"></a><span class="line-modified">1038     ASSERT(!vm-&gt;callFrameForCatch || exit.m_kind == GenericUnwind);</span>
1039     EXCEPTION_ASSERT_UNUSED(scope, !!scope.exception() || !exit.isExceptionHandler());
1040 
1041     prepareCodeOriginForOSRExit(exec, exit.m_codeOrigin);
1042 
1043     // Compute the value recoveries.
1044     Operands&lt;ValueRecovery&gt; operands;
1045     codeBlock-&gt;jitCode()-&gt;dfg()-&gt;variableEventStream.reconstruct(codeBlock, exit.m_codeOrigin, codeBlock-&gt;jitCode()-&gt;dfg()-&gt;minifiedDFG, exit.m_streamIndex, operands);
1046 
1047     SpeculationRecovery* recovery = 0;
1048     if (exit.m_recoveryIndex != UINT_MAX)
1049         recovery = &amp;codeBlock-&gt;jitCode()-&gt;dfg()-&gt;speculationRecovery[exit.m_recoveryIndex];
1050 
1051     {
1052         CCallHelpers jit(codeBlock);
1053 
1054         if (exit.m_kind == GenericUnwind) {
1055             // We are acting as a defacto op_catch because we arrive here from genericUnwind().
1056             // So, we must restore our call frame and stack pointer.
<a name="27" id="anc27"></a><span class="line-modified">1057             jit.restoreCalleeSavesFromEntryFrameCalleeSavesBuffer(vm-&gt;topEntryFrame);</span>
<span class="line-modified">1058             jit.loadPtr(vm-&gt;addressOfCallFrameForCatch(), GPRInfo::callFrameRegister);</span>
1059         }
1060         jit.addPtr(
1061             CCallHelpers::TrustedImm32(codeBlock-&gt;stackPointerOffset() * sizeof(Register)),
1062             GPRInfo::callFrameRegister, CCallHelpers::stackPointerRegister);
1063 
1064         jit.jitAssertHasValidCallFrame();
1065 
<a name="28" id="anc28"></a><span class="line-modified">1066         if (UNLIKELY(vm-&gt;m_perBytecodeProfiler &amp;&amp; codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation)) {</span>
<span class="line-modified">1067             Profiler::Database&amp; database = *vm-&gt;m_perBytecodeProfiler;</span>
1068             Profiler::Compilation* compilation = codeBlock-&gt;jitCode()-&gt;dfgCommon()-&gt;compilation.get();
1069 
1070             Profiler::OSRExit* profilerExit = compilation-&gt;addOSRExit(
1071                 exitIndex, Profiler::OriginStack(database, codeBlock, exit.m_codeOrigin),
1072                 exit.m_kind, exit.m_kind == UncountableInvalidation);
1073             jit.add64(CCallHelpers::TrustedImm32(1), CCallHelpers::AbsoluteAddress(profilerExit-&gt;counterAddress()));
1074         }
1075 
<a name="29" id="anc29"></a><span class="line-modified">1076         compileExit(jit, *vm, exit, operands, recovery);</span>
1077 
1078         LinkBuffer patchBuffer(jit, codeBlock);
1079         exit.m_code = FINALIZE_CODE_IF(
1080             shouldDumpDisassembly() || Options::verboseOSR() || Options::verboseDFGOSRExit(),
1081             patchBuffer, OSRExitPtrTag,
1082             &quot;DFG OSR exit #%u (%s, %s) from %s, with operands = %s&quot;,
1083                 exitIndex, toCString(exit.m_codeOrigin).data(),
1084                 exitKindToString(exit.m_kind), toCString(*codeBlock).data(),
1085                 toCString(ignoringContext&lt;DumpContext&gt;(operands)).data());
1086     }
1087 
1088     MacroAssembler::repatchJump(exit.codeLocationForRepatch(), CodeLocationLabel&lt;OSRExitPtrTag&gt;(exit.m_code.code()));
1089 
<a name="30" id="anc30"></a><span class="line-modified">1090     vm-&gt;osrExitJumpDestination = exit.m_code.code().executableAddress();</span>
1091 }
1092 
1093 void OSRExit::compileExit(CCallHelpers&amp; jit, VM&amp; vm, const OSRExit&amp; exit, const Operands&lt;ValueRecovery&gt;&amp; operands, SpeculationRecovery* recovery)
1094 {
1095     jit.jitAssertTagsInPlace();
1096 
1097     // Pro-forma stuff.
1098     if (Options::printEachOSRExit()) {
1099         SpeculationFailureDebugInfo* debugInfo = new SpeculationFailureDebugInfo;
1100         debugInfo-&gt;codeBlock = jit.codeBlock();
1101         debugInfo-&gt;kind = exit.m_kind;
<a name="31" id="anc31"></a><span class="line-modified">1102         debugInfo-&gt;bytecodeOffset = exit.m_codeOrigin.bytecodeIndex;</span>
1103 
1104         jit.debugCall(vm, debugOperationPrintSpeculationFailure, debugInfo);
1105     }
1106 
1107     // Perform speculation recovery. This only comes into play when an operation
1108     // starts mutating state before verifying the speculation it has already made.
1109 
1110     if (recovery) {
1111         switch (recovery-&gt;type()) {
1112         case SpeculativeAdd:
1113             jit.sub32(recovery-&gt;src(), recovery-&gt;dest());
1114 #if USE(JSVALUE64)
1115             jit.or64(GPRInfo::tagTypeNumberRegister, recovery-&gt;dest());
1116 #endif
1117             break;
1118 
<a name="32" id="anc32"></a>








1119         case SpeculativeAddImmediate:
1120             jit.sub32(AssemblyHelpers::Imm32(recovery-&gt;immediate()), recovery-&gt;dest());
1121 #if USE(JSVALUE64)
1122             jit.or64(GPRInfo::tagTypeNumberRegister, recovery-&gt;dest());
1123 #endif
1124             break;
1125 
1126         case BooleanSpeculationCheck:
1127 #if USE(JSVALUE64)
1128             jit.xor64(AssemblyHelpers::TrustedImm32(static_cast&lt;int32_t&gt;(ValueFalse)), recovery-&gt;dest());
1129 #endif
1130             break;
1131 
1132         default:
1133             break;
1134         }
1135     }
1136 
1137     // Refine some array and/or value profile, if appropriate.
1138 
1139     if (!!exit.m_jsValueSource) {
1140         if (exit.m_kind == BadCache || exit.m_kind == BadIndexingType) {
1141             // If the instruction that this originated from has an array profile, then
1142             // refine it. If it doesn&#39;t, then do nothing. The latter could happen for
1143             // hoisted checks, or checks emitted for operations that didn&#39;t have array
1144             // profiling - either ops that aren&#39;t array accesses at all, or weren&#39;t
1145             // known to be array acceses in the bytecode. The latter case is a FIXME
1146             // while the former case is an outcome of a CheckStructure not knowing why
1147             // it was emitted (could be either due to an inline cache of a property
1148             // property access, or due to an array profile).
1149 
1150             CodeOrigin codeOrigin = exit.m_codeOriginForExitProfile;
<a name="33" id="anc33"></a><span class="line-modified">1151             if (ArrayProfile* arrayProfile = jit.baselineCodeBlockFor(codeOrigin)-&gt;getArrayProfile(codeOrigin.bytecodeIndex)) {</span>
1152 #if USE(JSVALUE64)
1153                 GPRReg usedRegister;
1154                 if (exit.m_jsValueSource.isAddress())
1155                     usedRegister = exit.m_jsValueSource.base();
1156                 else
1157                     usedRegister = exit.m_jsValueSource.gpr();
1158 #else
1159                 GPRReg usedRegister1;
1160                 GPRReg usedRegister2;
1161                 if (exit.m_jsValueSource.isAddress()) {
1162                     usedRegister1 = exit.m_jsValueSource.base();
1163                     usedRegister2 = InvalidGPRReg;
1164                 } else {
1165                     usedRegister1 = exit.m_jsValueSource.payloadGPR();
1166                     if (exit.m_jsValueSource.hasKnownTag())
1167                         usedRegister2 = InvalidGPRReg;
1168                     else
1169                         usedRegister2 = exit.m_jsValueSource.tagGPR();
1170                 }
1171 #endif
1172 
1173                 GPRReg scratch1;
1174                 GPRReg scratch2;
1175 #if USE(JSVALUE64)
1176                 scratch1 = AssemblyHelpers::selectScratchGPR(usedRegister);
1177                 scratch2 = AssemblyHelpers::selectScratchGPR(usedRegister, scratch1);
1178 #else
1179                 scratch1 = AssemblyHelpers::selectScratchGPR(usedRegister1, usedRegister2);
1180                 scratch2 = AssemblyHelpers::selectScratchGPR(usedRegister1, usedRegister2, scratch1);
1181 #endif
1182 
1183                 if (isARM64()) {
1184                     jit.pushToSave(scratch1);
1185                     jit.pushToSave(scratch2);
1186                 } else {
1187                     jit.push(scratch1);
1188                     jit.push(scratch2);
1189                 }
1190 
1191                 GPRReg value;
1192                 if (exit.m_jsValueSource.isAddress()) {
1193                     value = scratch1;
1194                     jit.loadPtr(AssemblyHelpers::Address(exit.m_jsValueSource.asAddress()), value);
1195                 } else
1196                     value = exit.m_jsValueSource.payloadGPR();
1197 
1198                 jit.load32(AssemblyHelpers::Address(value, JSCell::structureIDOffset()), scratch1);
1199                 jit.store32(scratch1, arrayProfile-&gt;addressOfLastSeenStructureID());
1200 
1201                 jit.load8(AssemblyHelpers::Address(value, JSCell::typeInfoTypeOffset()), scratch2);
1202                 jit.sub32(AssemblyHelpers::TrustedImm32(FirstTypedArrayType), scratch2);
1203                 auto notTypedArray = jit.branch32(MacroAssembler::AboveOrEqual, scratch2, AssemblyHelpers::TrustedImm32(NumberOfTypedArrayTypesExcludingDataView));
1204                 jit.move(AssemblyHelpers::TrustedImmPtr(typedArrayModes), scratch1);
1205                 jit.load32(AssemblyHelpers::BaseIndex(scratch1, scratch2, AssemblyHelpers::TimesFour), scratch2);
1206                 auto storeArrayModes = jit.jump();
1207 
1208                 notTypedArray.link(&amp;jit);
1209 #if USE(JSVALUE64)
1210                 jit.load8(AssemblyHelpers::Address(value, JSCell::indexingTypeAndMiscOffset()), scratch1);
1211 #else
1212                 jit.load8(AssemblyHelpers::Address(scratch1, Structure::indexingModeIncludingHistoryOffset()), scratch1);
1213 #endif
1214                 jit.and32(AssemblyHelpers::TrustedImm32(IndexingModeMask), scratch1);
1215                 jit.move(AssemblyHelpers::TrustedImm32(1), scratch2);
1216                 jit.lshift32(scratch1, scratch2);
1217                 storeArrayModes.link(&amp;jit);
1218                 jit.or32(scratch2, AssemblyHelpers::AbsoluteAddress(arrayProfile-&gt;addressOfArrayModes()));
1219 
1220                 if (isARM64()) {
1221                     jit.popToRestore(scratch2);
1222                     jit.popToRestore(scratch1);
1223                 } else {
1224                     jit.pop(scratch2);
1225                     jit.pop(scratch1);
1226                 }
1227             }
1228         }
1229 
1230         if (MethodOfGettingAValueProfile profile = exit.m_valueProfile) {
1231 #if USE(JSVALUE64)
1232             if (exit.m_jsValueSource.isAddress()) {
1233                 // We can&#39;t be sure that we have a spare register. So use the tagTypeNumberRegister,
1234                 // since we know how to restore it.
1235                 jit.load64(AssemblyHelpers::Address(exit.m_jsValueSource.asAddress()), GPRInfo::tagTypeNumberRegister);
1236                 profile.emitReportValue(jit, JSValueRegs(GPRInfo::tagTypeNumberRegister));
1237                 jit.move(AssemblyHelpers::TrustedImm64(TagTypeNumber), GPRInfo::tagTypeNumberRegister);
1238             } else
1239                 profile.emitReportValue(jit, JSValueRegs(exit.m_jsValueSource.gpr()));
1240 #else // not USE(JSVALUE64)
1241             if (exit.m_jsValueSource.isAddress()) {
1242                 // Save a register so we can use it.
1243                 GPRReg scratchPayload = AssemblyHelpers::selectScratchGPR(exit.m_jsValueSource.base());
1244                 GPRReg scratchTag = AssemblyHelpers::selectScratchGPR(exit.m_jsValueSource.base(), scratchPayload);
1245                 jit.pushToSave(scratchPayload);
1246                 jit.pushToSave(scratchTag);
1247 
1248                 JSValueRegs scratch(scratchTag, scratchPayload);
1249 
1250                 jit.loadValue(exit.m_jsValueSource.asAddress(), scratch);
1251                 profile.emitReportValue(jit, scratch);
1252 
1253                 jit.popToRestore(scratchTag);
1254                 jit.popToRestore(scratchPayload);
1255             } else if (exit.m_jsValueSource.hasKnownTag()) {
1256                 GPRReg scratchTag = AssemblyHelpers::selectScratchGPR(exit.m_jsValueSource.payloadGPR());
1257                 jit.pushToSave(scratchTag);
1258                 jit.move(AssemblyHelpers::TrustedImm32(exit.m_jsValueSource.tag()), scratchTag);
1259                 JSValueRegs value(scratchTag, exit.m_jsValueSource.payloadGPR());
1260                 profile.emitReportValue(jit, value);
1261                 jit.popToRestore(scratchTag);
1262             } else
1263                 profile.emitReportValue(jit, exit.m_jsValueSource.regs());
1264 #endif // USE(JSVALUE64)
1265         }
1266     }
1267 
1268     // What follows is an intentionally simple OSR exit implementation that generates
1269     // fairly poor code but is very easy to hack. In particular, it dumps all state that
1270     // needs conversion into a scratch buffer so that in step 6, where we actually do the
1271     // conversions, we know that all temp registers are free to use and the variable is
1272     // definitely in a well-known spot in the scratch buffer regardless of whether it had
1273     // originally been in a register or spilled. This allows us to decouple &quot;where was
1274     // the variable&quot; from &quot;how was it represented&quot;. Consider that the
1275     // Int32DisplacedInJSStack recovery: it tells us that the value is in a
1276     // particular place and that that place holds an unboxed int32. We have two different
1277     // places that a value could be (displaced, register) and a bunch of different
1278     // ways of representing a value. The number of recoveries is two * a bunch. The code
1279     // below means that we have to have two + a bunch cases rather than two * a bunch.
1280     // Once we have loaded the value from wherever it was, the reboxing is the same
1281     // regardless of its location. Likewise, before we do the reboxing, the way we get to
1282     // the value (i.e. where we load it from) is the same regardless of its type. Because
1283     // the code below always dumps everything into a scratch buffer first, the two
1284     // questions become orthogonal, which simplifies adding new types and adding new
1285     // locations.
1286     //
1287     // This raises the question: does using such a suboptimal implementation of OSR exit,
1288     // where we always emit code to dump all state into a scratch buffer only to then
1289     // dump it right back into the stack, hurt us in any way? The asnwer is that OSR exits
1290     // are rare. Our tiering strategy ensures this. This is because if an OSR exit is
1291     // taken more than ~100 times, we jettison the DFG code block along with all of its
1292     // exits. It is impossible for an OSR exit - i.e. the code we compile below - to
1293     // execute frequently enough for the codegen to matter that much. It probably matters
1294     // enough that we don&#39;t want to turn this into some super-slow function call, but so
1295     // long as we&#39;re generating straight-line code, that code can be pretty bad. Also
1296     // because we tend to exit only along one OSR exit from any DFG code block - that&#39;s an
1297     // empirical result that we&#39;re extremely confident about - the code size of this
1298     // doesn&#39;t matter much. Hence any attempt to optimize the codegen here is just purely
1299     // harmful to the system: it probably won&#39;t reduce either net memory usage or net
1300     // execution time. It will only prevent us from cleanly decoupling &quot;where was the
1301     // variable&quot; from &quot;how was it represented&quot;, which will make it more difficult to add
1302     // features in the future and it will make it harder to reason about bugs.
1303 
1304     // Save all state from GPRs into the scratch buffer.
1305 
1306     ScratchBuffer* scratchBuffer = vm.scratchBufferForSize(sizeof(EncodedJSValue) * operands.size());
1307     EncodedJSValue* scratch = scratchBuffer ? static_cast&lt;EncodedJSValue*&gt;(scratchBuffer-&gt;dataBuffer()) : 0;
1308 
1309     for (size_t index = 0; index &lt; operands.size(); ++index) {
1310         const ValueRecovery&amp; recovery = operands[index];
1311 
1312         switch (recovery.technique()) {
1313         case UnboxedInt32InGPR:
1314         case UnboxedCellInGPR:
1315 #if USE(JSVALUE64)
1316         case InGPR:
1317         case UnboxedInt52InGPR:
1318         case UnboxedStrictInt52InGPR:
1319             jit.store64(recovery.gpr(), scratch + index);
1320             break;
1321 #else
1322         case UnboxedBooleanInGPR:
1323             jit.store32(
1324                 recovery.gpr(),
1325                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload);
1326             break;
1327 
1328         case InPair:
1329             jit.store32(
1330                 recovery.tagGPR(),
1331                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.tag);
1332             jit.store32(
1333                 recovery.payloadGPR(),
1334                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload);
1335             break;
1336 #endif
1337 
1338         default:
1339             break;
1340         }
1341     }
1342 
1343     // And voila, all GPRs are free to reuse.
1344 
1345     // Save all state from FPRs into the scratch buffer.
1346 
1347     for (size_t index = 0; index &lt; operands.size(); ++index) {
1348         const ValueRecovery&amp; recovery = operands[index];
1349 
1350         switch (recovery.technique()) {
1351         case UnboxedDoubleInFPR:
1352         case InFPR:
1353             jit.move(AssemblyHelpers::TrustedImmPtr(scratch + index), GPRInfo::regT0);
1354             jit.storeDouble(recovery.fpr(), MacroAssembler::Address(GPRInfo::regT0));
1355             break;
1356 
1357         default:
1358             break;
1359         }
1360     }
1361 
1362     // Now, all FPRs are also free.
1363 
1364     // Save all state from the stack into the scratch buffer. For simplicity we
1365     // do this even for state that&#39;s already in the right place on the stack.
1366     // It makes things simpler later.
1367 
1368     for (size_t index = 0; index &lt; operands.size(); ++index) {
1369         const ValueRecovery&amp; recovery = operands[index];
1370 
1371         switch (recovery.technique()) {
1372         case DisplacedInJSStack:
1373         case CellDisplacedInJSStack:
1374         case BooleanDisplacedInJSStack:
1375         case Int32DisplacedInJSStack:
1376         case DoubleDisplacedInJSStack:
1377 #if USE(JSVALUE64)
1378         case Int52DisplacedInJSStack:
1379         case StrictInt52DisplacedInJSStack:
1380             jit.load64(AssemblyHelpers::addressFor(recovery.virtualRegister()), GPRInfo::regT0);
1381             jit.store64(GPRInfo::regT0, scratch + index);
1382             break;
1383 #else
1384             jit.load32(
1385                 AssemblyHelpers::tagFor(recovery.virtualRegister()),
1386                 GPRInfo::regT0);
1387             jit.load32(
1388                 AssemblyHelpers::payloadFor(recovery.virtualRegister()),
1389                 GPRInfo::regT1);
1390             jit.store32(
1391                 GPRInfo::regT0,
1392                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.tag);
1393             jit.store32(
1394                 GPRInfo::regT1,
1395                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload);
1396             break;
1397 #endif
1398 
1399         default:
1400             break;
1401         }
1402     }
1403 
1404     if (validateDFGDoesGC) {
1405         // We&#39;re about to exit optimized code. So, there&#39;s no longer any optimized
1406         // code running that expects no GC. We need to set this before arguments
1407         // materialization below (see emitRestoreArguments()).
1408 
1409         // Even though we set Heap::m_expectDoesGC in compileOSRExit(), we also need
1410         // to set it here because compileOSRExit() is only called on the first time
1411         // we exit from this site, but all subsequent exits will take this compiled
1412         // ramp without calling compileOSRExit() first.
1413         jit.store8(CCallHelpers::TrustedImm32(true), vm.heap.addressOfExpectDoesGC());
1414     }
1415 
1416     // Need to ensure that the stack pointer accounts for the worst-case stack usage at exit. This
1417     // could toast some stack that the DFG used. We need to do it before storing to stack offsets
1418     // used by baseline.
1419     jit.addPtr(
1420         CCallHelpers::TrustedImm32(
1421             -jit.codeBlock()-&gt;jitCode()-&gt;dfgCommon()-&gt;requiredRegisterCountForExit * sizeof(Register)),
1422         CCallHelpers::framePointerRegister, CCallHelpers::stackPointerRegister);
1423 
1424     // Restore the DFG callee saves and then save the ones the baseline JIT uses.
1425     jit.emitRestoreCalleeSaves();
1426     jit.emitSaveCalleeSavesFor(jit.baselineCodeBlock());
1427 
1428     // The tag registers are needed to materialize recoveries below.
1429     jit.emitMaterializeTagCheckRegisters();
1430 
1431     if (exit.isExceptionHandler())
1432         jit.copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm.topEntryFrame);
1433 
1434     // Do all data format conversions and store the results into the stack.
1435 
1436     for (size_t index = 0; index &lt; operands.size(); ++index) {
1437         const ValueRecovery&amp; recovery = operands[index];
1438         VirtualRegister reg = operands.virtualRegisterForIndex(index);
1439 
1440         if (reg.isLocal() &amp;&amp; reg.toLocal() &lt; static_cast&lt;int&gt;(jit.baselineCodeBlock()-&gt;calleeSaveSpaceAsVirtualRegisters()))
1441             continue;
1442 
1443         int operand = reg.offset();
1444 
1445         switch (recovery.technique()) {
1446         case DisplacedInJSStack:
1447         case InFPR:
1448 #if USE(JSVALUE64)
1449         case InGPR:
1450         case UnboxedCellInGPR:
1451         case CellDisplacedInJSStack:
1452         case BooleanDisplacedInJSStack:
1453             jit.load64(scratch + index, GPRInfo::regT0);
1454             jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand));
1455             break;
1456 #else // not USE(JSVALUE64)
1457         case InPair:
1458             jit.load32(
1459                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.tag,
1460                 GPRInfo::regT0);
1461             jit.load32(
1462                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload,
1463                 GPRInfo::regT1);
1464             jit.store32(
1465                 GPRInfo::regT0,
1466                 AssemblyHelpers::tagFor(operand));
1467             jit.store32(
1468                 GPRInfo::regT1,
1469                 AssemblyHelpers::payloadFor(operand));
1470             break;
1471 
1472         case UnboxedCellInGPR:
1473         case CellDisplacedInJSStack:
1474             jit.load32(
1475                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload,
1476                 GPRInfo::regT0);
1477             jit.store32(
1478                 AssemblyHelpers::TrustedImm32(JSValue::CellTag),
1479                 AssemblyHelpers::tagFor(operand));
1480             jit.store32(
1481                 GPRInfo::regT0,
1482                 AssemblyHelpers::payloadFor(operand));
1483             break;
1484 
1485         case UnboxedBooleanInGPR:
1486         case BooleanDisplacedInJSStack:
1487             jit.load32(
1488                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload,
1489                 GPRInfo::regT0);
1490             jit.store32(
1491                 AssemblyHelpers::TrustedImm32(JSValue::BooleanTag),
1492                 AssemblyHelpers::tagFor(operand));
1493             jit.store32(
1494                 GPRInfo::regT0,
1495                 AssemblyHelpers::payloadFor(operand));
1496             break;
1497 #endif // USE(JSVALUE64)
1498 
1499         case UnboxedInt32InGPR:
1500         case Int32DisplacedInJSStack:
1501 #if USE(JSVALUE64)
1502             jit.load64(scratch + index, GPRInfo::regT0);
1503             jit.zeroExtend32ToPtr(GPRInfo::regT0, GPRInfo::regT0);
1504             jit.or64(GPRInfo::tagTypeNumberRegister, GPRInfo::regT0);
1505             jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand));
1506 #else
1507             jit.load32(
1508                 &amp;bitwise_cast&lt;EncodedValueDescriptor*&gt;(scratch + index)-&gt;asBits.payload,
1509                 GPRInfo::regT0);
1510             jit.store32(
1511                 AssemblyHelpers::TrustedImm32(JSValue::Int32Tag),
1512                 AssemblyHelpers::tagFor(operand));
1513             jit.store32(
1514                 GPRInfo::regT0,
1515                 AssemblyHelpers::payloadFor(operand));
1516 #endif
1517             break;
1518 
1519 #if USE(JSVALUE64)
1520         case UnboxedInt52InGPR:
1521         case Int52DisplacedInJSStack:
1522             jit.load64(scratch + index, GPRInfo::regT0);
1523             jit.rshift64(
1524                 AssemblyHelpers::TrustedImm32(JSValue::int52ShiftAmount), GPRInfo::regT0);
1525             jit.boxInt52(GPRInfo::regT0, GPRInfo::regT0, GPRInfo::regT1, FPRInfo::fpRegT0);
1526             jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand));
1527             break;
1528 
1529         case UnboxedStrictInt52InGPR:
1530         case StrictInt52DisplacedInJSStack:
1531             jit.load64(scratch + index, GPRInfo::regT0);
1532             jit.boxInt52(GPRInfo::regT0, GPRInfo::regT0, GPRInfo::regT1, FPRInfo::fpRegT0);
1533             jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand));
1534             break;
1535 #endif
1536 
1537         case UnboxedDoubleInFPR:
1538         case DoubleDisplacedInJSStack:
1539             jit.move(AssemblyHelpers::TrustedImmPtr(scratch + index), GPRInfo::regT0);
1540             jit.loadDouble(MacroAssembler::Address(GPRInfo::regT0), FPRInfo::fpRegT0);
1541             jit.purifyNaN(FPRInfo::fpRegT0);
1542 #if USE(JSVALUE64)
1543             jit.boxDouble(FPRInfo::fpRegT0, GPRInfo::regT0);
1544             jit.store64(GPRInfo::regT0, AssemblyHelpers::addressFor(operand));
1545 #else
1546             jit.storeDouble(FPRInfo::fpRegT0, AssemblyHelpers::addressFor(operand));
1547 #endif
1548             break;
1549 
1550         case Constant:
1551 #if USE(JSVALUE64)
1552             jit.store64(
1553                 AssemblyHelpers::TrustedImm64(JSValue::encode(recovery.constant())),
1554                 AssemblyHelpers::addressFor(operand));
1555 #else
1556             jit.store32(
1557                 AssemblyHelpers::TrustedImm32(recovery.constant().tag()),
1558                 AssemblyHelpers::tagFor(operand));
1559             jit.store32(
1560                 AssemblyHelpers::TrustedImm32(recovery.constant().payload()),
1561                 AssemblyHelpers::payloadFor(operand));
1562 #endif
1563             break;
1564 
1565         case DirectArgumentsThatWereNotCreated:
1566         case ClonedArgumentsThatWereNotCreated:
1567             // Don&#39;t do this, yet.
1568             break;
1569 
1570         default:
1571             RELEASE_ASSERT_NOT_REACHED();
1572             break;
1573         }
1574     }
1575 
1576     // Now that things on the stack are recovered, do the arguments recovery. We assume that arguments
1577     // recoveries don&#39;t recursively refer to each other. But, we don&#39;t try to assume that they only
1578     // refer to certain ranges of locals. Hence why we need to do this here, once the stack is sensible.
1579     // Note that we also roughly assume that the arguments might still be materialized outside of its
1580     // inline call frame scope - but for now the DFG wouldn&#39;t do that.
1581 
1582     emitRestoreArguments(jit, operands);
1583 
1584     // Adjust the old JIT&#39;s execute counter. Since we are exiting OSR, we know
1585     // that all new calls into this code will go to the new JIT, so the execute
1586     // counter only affects call frames that performed OSR exit and call frames
1587     // that were still executing the old JIT at the time of another call frame&#39;s
1588     // OSR exit. We want to ensure that the following is true:
1589     //
1590     // (a) Code the performs an OSR exit gets a chance to reenter optimized
1591     //     code eventually, since optimized code is faster. But we don&#39;t
1592     //     want to do such reentery too aggressively (see (c) below).
1593     //
1594     // (b) If there is code on the call stack that is still running the old
1595     //     JIT&#39;s code and has never OSR&#39;d, then it should get a chance to
1596     //     perform OSR entry despite the fact that we&#39;ve exited.
1597     //
1598     // (c) Code the performs an OSR exit should not immediately retry OSR
1599     //     entry, since both forms of OSR are expensive. OSR entry is
1600     //     particularly expensive.
1601     //
1602     // (d) Frequent OSR failures, even those that do not result in the code
1603     //     running in a hot loop, result in recompilation getting triggered.
1604     //
1605     // To ensure (c), we&#39;d like to set the execute counter to
1606     // counterValueForOptimizeAfterWarmUp(). This seems like it would endanger
1607     // (a) and (b), since then every OSR exit would delay the opportunity for
1608     // every call frame to perform OSR entry. Essentially, if OSR exit happens
1609     // frequently and the function has few loops, then the counter will never
1610     // become non-negative and OSR entry will never be triggered. OSR entry
1611     // will only happen if a loop gets hot in the old JIT, which does a pretty
1612     // good job of ensuring (a) and (b). But that doesn&#39;t take care of (d),
1613     // since each speculation failure would reset the execute counter.
1614     // So we check here if the number of speculation failures is significantly
1615     // larger than the number of successes (we want 90% success rate), and if
1616     // there have been a large enough number of failures. If so, we set the
1617     // counter to 0; otherwise we set the counter to
1618     // counterValueForOptimizeAfterWarmUp().
1619 
1620     handleExitCounts(jit, exit);
1621 
1622     // Reify inlined call frames.
1623 
1624     reifyInlinedCallFrames(jit, exit);
1625 
1626     // And finish.
1627     adjustAndJumpToTarget(vm, jit, exit);
1628 }
1629 
1630 void JIT_OPERATION OSRExit::debugOperationPrintSpeculationFailure(ExecState* exec, void* debugInfoRaw, void* scratch)
1631 {
<a name="34" id="anc34"></a><span class="line-modified">1632     VM* vm = &amp;exec-&gt;vm();</span>
1633     NativeCallFrameTracer tracer(vm, exec);
1634 
1635     SpeculationFailureDebugInfo* debugInfo = static_cast&lt;SpeculationFailureDebugInfo*&gt;(debugInfoRaw);
1636     CodeBlock* codeBlock = debugInfo-&gt;codeBlock;
1637     CodeBlock* alternative = codeBlock-&gt;alternative();
1638     dataLog(&quot;Speculation failure in &quot;, *codeBlock);
<a name="35" id="anc35"></a><span class="line-modified">1639     dataLog(&quot; @ exit #&quot;, vm-&gt;osrExitIndex, &quot; (bc#&quot;, debugInfo-&gt;bytecodeOffset, &quot;, &quot;, exitKindToString(debugInfo-&gt;kind), &quot;) with &quot;);</span>
1640     if (alternative) {
1641         dataLog(
1642             &quot;executeCounter = &quot;, alternative-&gt;jitExecuteCounter(),
1643             &quot;, reoptimizationRetryCounter = &quot;, alternative-&gt;reoptimizationRetryCounter(),
1644             &quot;, optimizationDelayCounter = &quot;, alternative-&gt;optimizationDelayCounter());
1645     } else
1646         dataLog(&quot;no alternative code block (i.e. we&#39;ve been jettisoned)&quot;);
1647     dataLog(&quot;, osrExitCounter = &quot;, codeBlock-&gt;osrExitCounter(), &quot;\n&quot;);
1648     dataLog(&quot;    GPRs at time of exit:&quot;);
1649     char* scratchPointer = static_cast&lt;char*&gt;(scratch);
1650     for (unsigned i = 0; i &lt; GPRInfo::numberOfRegisters; ++i) {
1651         GPRReg gpr = GPRInfo::toRegister(i);
1652         dataLog(&quot; &quot;, GPRInfo::debugName(gpr), &quot;:&quot;, RawPointer(*reinterpret_cast_ptr&lt;void**&gt;(scratchPointer)));
1653         scratchPointer += sizeof(EncodedJSValue);
1654     }
1655     dataLog(&quot;\n&quot;);
1656     dataLog(&quot;    FPRs at time of exit:&quot;);
1657     for (unsigned i = 0; i &lt; FPRInfo::numberOfRegisters; ++i) {
1658         FPRReg fpr = FPRInfo::toRegister(i);
1659         dataLog(&quot; &quot;, FPRInfo::debugName(fpr), &quot;:&quot;);
1660         uint64_t bits = *reinterpret_cast_ptr&lt;uint64_t*&gt;(scratchPointer);
1661         double value = *reinterpret_cast_ptr&lt;double*&gt;(scratchPointer);
1662         dataLogF(&quot;%llx:%lf&quot;, static_cast&lt;long long&gt;(bits), value);
1663         scratchPointer += sizeof(EncodedJSValue);
1664     }
1665     dataLog(&quot;\n&quot;);
1666 }
1667 
1668 } } // namespace JSC::DFG
1669 
1670 #endif // ENABLE(DFG_JIT)
<a name="36" id="anc36"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="36" type="hidden" />
</body>
</html>