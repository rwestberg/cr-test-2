<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old modules/javafx.web/src/main/native/Source/JavaScriptCore/heap/Heap.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  *  Copyright (C) 1999-2000 Harri Porten (porten@kde.org)
  3  *  Copyright (C) 2001 Peter Kelly (pmk@post.com)
  4  *  Copyright (C) 2003-2019 Apple Inc. All rights reserved.
  5  *
  6  *  This library is free software; you can redistribute it and/or
  7  *  modify it under the terms of the GNU Lesser General Public
  8  *  License as published by the Free Software Foundation; either
  9  *  version 2 of the License, or (at your option) any later version.
 10  *
 11  *  This library is distributed in the hope that it will be useful,
 12  *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 13  *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 14  *  Lesser General Public License for more details.
 15  *
 16  *  You should have received a copy of the GNU Lesser General Public
 17  *  License along with this library; if not, write to the Free Software
 18  *  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
 19  *
 20  */
 21 
 22 #pragma once
 23 
 24 #include &quot;ArrayBuffer.h&quot;
 25 #include &quot;CellState.h&quot;
 26 #include &quot;CollectionScope.h&quot;
 27 #include &quot;CollectorPhase.h&quot;
 28 #include &quot;DeleteAllCodeEffort.h&quot;
 29 #include &quot;GCConductor.h&quot;
 30 #include &quot;GCIncomingRefCountedSet.h&quot;
 31 #include &quot;GCRequest.h&quot;
 32 #include &quot;HandleSet.h&quot;
 33 #include &quot;HeapFinalizerCallback.h&quot;
 34 #include &quot;HeapObserver.h&quot;
 35 #include &quot;MarkedBlock.h&quot;
 36 #include &quot;MarkedSpace.h&quot;
 37 #include &quot;MutatorState.h&quot;
 38 #include &quot;Options.h&quot;
 39 #include &quot;StructureIDTable.h&quot;
 40 #include &quot;Synchronousness.h&quot;
 41 #include &quot;WeakHandleOwner.h&quot;
 42 #include &lt;wtf/AutomaticThread.h&gt;
 43 #include &lt;wtf/ConcurrentPtrHashSet.h&gt;
 44 #include &lt;wtf/Deque.h&gt;
 45 #include &lt;wtf/HashCountedSet.h&gt;
 46 #include &lt;wtf/HashSet.h&gt;
 47 #include &lt;wtf/Markable.h&gt;
 48 #include &lt;wtf/ParallelHelperPool.h&gt;
 49 #include &lt;wtf/Threading.h&gt;
 50 
 51 namespace JSC {
 52 
 53 class CodeBlock;
 54 class CodeBlockSet;
 55 class CollectingScope;
 56 class ConservativeRoots;
 57 class GCDeferralContext;
 58 class EdenGCActivityCallback;
 59 class FullGCActivityCallback;
 60 class GCActivityCallback;
 61 class GCAwareJITStubRoutine;
 62 class Heap;
 63 class HeapProfiler;
 64 class HeapVerifier;
 65 class IncrementalSweeper;
 66 class JITStubRoutine;
 67 class JITStubRoutineSet;
 68 class JSCell;
 69 class JSImmutableButterfly;
 70 class JSValue;
 71 class LLIntOffsetsExtractor;
 72 class MachineThreads;
 73 class MarkStackArray;
 74 class MarkStackMergingConstraint;
 75 class BlockDirectory;
 76 class MarkedArgumentBuffer;
 77 class MarkingConstraint;
 78 class MarkingConstraintSet;
 79 class MutatorScheduler;
 80 class RunningScope;
 81 class SlotVisitor;
 82 class SpaceTimeMutatorScheduler;
 83 class StopIfNecessaryTimer;
 84 class SweepingScope;
 85 class VM;
 86 class WeakGCMapBase;
 87 struct CurrentThreadState;
 88 
 89 #if USE(GLIB)
 90 class JSCGLibWrapperObject;
 91 #endif
 92 
 93 namespace DFG {
 94 class SpeculativeJIT;
 95 class Worklist;
 96 }
 97 
 98 #if !ASSERT_DISABLED
 99 #define ENABLE_DFG_DOES_GC_VALIDATION 1
100 #else
101 #define ENABLE_DFG_DOES_GC_VALIDATION 0
102 #endif
103 constexpr bool validateDFGDoesGC = ENABLE_DFG_DOES_GC_VALIDATION;
104 
105 typedef HashCountedSet&lt;JSCell*&gt; ProtectCountSet;
106 typedef HashCountedSet&lt;const char*&gt; TypeCountSet;
107 
108 enum HeapType { SmallHeap, LargeHeap };
109 
110 class HeapUtil;
111 
112 class Heap {
113     WTF_MAKE_NONCOPYABLE(Heap);
114 public:
115     friend class JIT;
116     friend class DFG::SpeculativeJIT;
117     static Heap* heap(const JSValue); // 0 for immediate values
118     static Heap* heap(const HeapCell*);
119 
120     // This constant determines how many blocks we iterate between checks of our
121     // deadline when calling Heap::isPagedOut. Decreasing it will cause us to detect
122     // overstepping our deadline more quickly, while increasing it will cause
123     // our scan to run faster.
124     static const unsigned s_timeCheckResolution = 16;
125 
126     static bool isMarked(const void*);
127     static bool testAndSetMarked(HeapVersion, const void*);
128 
129     static size_t cellSize(const void*);
130 
131     void writeBarrier(const JSCell* from);
132     void writeBarrier(const JSCell* from, JSValue to);
133     void writeBarrier(const JSCell* from, JSCell* to);
134 
135     void writeBarrierWithoutFence(const JSCell* from);
136 
137     void mutatorFence();
138 
139     // Take this if you know that from-&gt;cellState() &lt; barrierThreshold.
140     JS_EXPORT_PRIVATE void writeBarrierSlowPath(const JSCell* from);
141 
142     Heap(VM*, HeapType);
143     ~Heap();
144     void lastChanceToFinalize();
145     void releaseDelayedReleasedObjects();
146 
147     VM* vm() const;
148 
149     MarkedSpace&amp; objectSpace() { return m_objectSpace; }
150     MachineThreads&amp; machineThreads() { return *m_machineThreads; }
151 
152     SlotVisitor&amp; collectorSlotVisitor() { return *m_collectorSlotVisitor; }
153 
154     JS_EXPORT_PRIVATE GCActivityCallback* fullActivityCallback();
155     JS_EXPORT_PRIVATE GCActivityCallback* edenActivityCallback();
156     JS_EXPORT_PRIVATE void setGarbageCollectionTimerEnabled(bool);
157 
158     JS_EXPORT_PRIVATE IncrementalSweeper&amp; sweeper();
159 
160     void addObserver(HeapObserver* observer) { m_observers.append(observer); }
161     void removeObserver(HeapObserver* observer) { m_observers.removeFirst(observer); }
162 
163     MutatorState mutatorState() const { return m_mutatorState; }
164     Optional&lt;CollectionScope&gt; collectionScope() const { return m_collectionScope; }
165     bool hasHeapAccess() const;
166     bool worldIsStopped() const;
167     bool worldIsRunning() const { return !worldIsStopped(); }
168 
169     // We&#39;re always busy on the collection threads. On the main thread, this returns true if we&#39;re
170     // helping heap.
171     JS_EXPORT_PRIVATE bool isCurrentThreadBusy();
172 
173     typedef void (*Finalizer)(JSCell*);
174     JS_EXPORT_PRIVATE void addFinalizer(JSCell*, Finalizer);
175 
176     void notifyIsSafeToCollect();
177     bool isSafeToCollect() const { return m_isSafeToCollect; }
178 
179     bool isShuttingDown() const { return m_isShuttingDown; }
180 
181     JS_EXPORT_PRIVATE bool isHeapSnapshotting() const;
182 
183     JS_EXPORT_PRIVATE void sweepSynchronously();
184 
185     bool shouldCollectHeuristic();
186 
187     // Queue up a collection. Returns immediately. This will not queue a collection if a collection
188     // of equal or greater strength exists. Full collections are stronger than WTF::nullopt collections
189     // and WTF::nullopt collections are stronger than Eden collections. WTF::nullopt means that the GC can
190     // choose Eden or Full. This implies that if you request a GC while that GC is ongoing, nothing
191     // will happen.
192     JS_EXPORT_PRIVATE void collectAsync(GCRequest = GCRequest());
193 
194     // Queue up a collection and wait for it to complete. This won&#39;t return until you get your own
195     // complete collection. For example, if there was an ongoing asynchronous collection at the time
196     // you called this, then this would wait for that one to complete and then trigger your
197     // collection and then return. In weird cases, there could be multiple GC requests in the backlog
198     // and this will wait for that backlog before running its GC and returning.
199     JS_EXPORT_PRIVATE void collectSync(GCRequest = GCRequest());
200 
201     JS_EXPORT_PRIVATE void collect(Synchronousness, GCRequest = GCRequest());
202 
203     // Like collect(), but in the case of Async this will stopIfNecessary() and in the case of
204     // Sync this will sweep synchronously.
205     JS_EXPORT_PRIVATE void collectNow(Synchronousness, GCRequest = GCRequest());
206 
207     JS_EXPORT_PRIVATE void collectNowFullIfNotDoneRecently(Synchronousness);
208 
209     void collectIfNecessaryOrDefer(GCDeferralContext* = nullptr);
210 
211     void completeAllJITPlans();
212 
213     // Use this API to report non-GC memory referenced by GC objects. Be sure to
214     // call both of these functions: Calling only one may trigger catastropic
215     // memory growth.
216     void reportExtraMemoryAllocated(size_t);
217     JS_EXPORT_PRIVATE void reportExtraMemoryVisited(size_t);
218 
219 #if ENABLE(RESOURCE_USAGE)
220     // Use this API to report the subset of extra memory that lives outside this process.
221     JS_EXPORT_PRIVATE void reportExternalMemoryVisited(size_t);
222     size_t externalMemorySize() { return m_externalMemorySize; }
223 #endif
224 
225     // Use this API to report non-GC memory if you can&#39;t use the better API above.
226     void deprecatedReportExtraMemory(size_t);
227 
228     JS_EXPORT_PRIVATE void reportAbandonedObjectGraph();
229 
230     JS_EXPORT_PRIVATE void protect(JSValue);
231     JS_EXPORT_PRIVATE bool unprotect(JSValue); // True when the protect count drops to 0.
232 
233     JS_EXPORT_PRIVATE size_t extraMemorySize(); // Non-GC memory referenced by GC objects.
234     JS_EXPORT_PRIVATE size_t size();
235     JS_EXPORT_PRIVATE size_t capacity();
236     JS_EXPORT_PRIVATE size_t objectCount();
237     JS_EXPORT_PRIVATE size_t globalObjectCount();
238     JS_EXPORT_PRIVATE size_t protectedObjectCount();
239     JS_EXPORT_PRIVATE size_t protectedGlobalObjectCount();
240     JS_EXPORT_PRIVATE std::unique_ptr&lt;TypeCountSet&gt; protectedObjectTypeCounts();
241     JS_EXPORT_PRIVATE std::unique_ptr&lt;TypeCountSet&gt; objectTypeCounts();
242 
243     HashSet&lt;MarkedArgumentBuffer*&gt;&amp; markListSet();
244 
245     template&lt;typename Functor&gt; void forEachProtectedCell(const Functor&amp;);
246     template&lt;typename Functor&gt; void forEachCodeBlock(const Functor&amp;);
247     template&lt;typename Functor&gt; void forEachCodeBlockIgnoringJITPlans(const AbstractLocker&amp; codeBlockSetLocker, const Functor&amp;);
248 
249     HandleSet* handleSet() { return &amp;m_handleSet; }
250 
251     void willStartIterating();
252     void didFinishIterating();
253 
254     Seconds lastFullGCLength() const { return m_lastFullGCLength; }
255     Seconds lastEdenGCLength() const { return m_lastEdenGCLength; }
256     void increaseLastFullGCLength(Seconds amount) { m_lastFullGCLength += amount; }
257 
258     size_t sizeBeforeLastEdenCollection() const { return m_sizeBeforeLastEdenCollect; }
259     size_t sizeAfterLastEdenCollection() const { return m_sizeAfterLastEdenCollect; }
260     size_t sizeBeforeLastFullCollection() const { return m_sizeBeforeLastFullCollect; }
261     size_t sizeAfterLastFullCollection() const { return m_sizeAfterLastFullCollect; }
262 
263     void deleteAllCodeBlocks(DeleteAllCodeEffort);
264     void deleteAllUnlinkedCodeBlocks(DeleteAllCodeEffort);
265 
266     void didAllocate(size_t);
267     bool isPagedOut(MonotonicTime deadline);
268 
269     const JITStubRoutineSet&amp; jitStubRoutines() { return *m_jitStubRoutines; }
270 
271     void addReference(JSCell*, ArrayBuffer*);
272 
273     bool isDeferred() const { return !!m_deferralDepth; }
274 
275     StructureIDTable&amp; structureIDTable() { return m_structureIDTable; }
276 
277     CodeBlockSet&amp; codeBlockSet() { return *m_codeBlocks; }
278 
279 #if USE(FOUNDATION)
280     template&lt;typename T&gt; void releaseSoon(RetainPtr&lt;T&gt;&amp;&amp;);
281 #endif
282 #if USE(GLIB)
283     void releaseSoon(std::unique_ptr&lt;JSCGLibWrapperObject&gt;&amp;&amp;);
284 #endif
285 
286     JS_EXPORT_PRIVATE void registerWeakGCMap(WeakGCMapBase* weakGCMap);
287     JS_EXPORT_PRIVATE void unregisterWeakGCMap(WeakGCMapBase* weakGCMap);
288 
289     void addLogicallyEmptyWeakBlock(WeakBlock*);
290 
291 #if ENABLE(RESOURCE_USAGE)
292     size_t blockBytesAllocated() const { return m_blockBytesAllocated; }
293 #endif
294 
295     void didAllocateBlock(size_t capacity);
296     void didFreeBlock(size_t capacity);
297 
298     bool mutatorShouldBeFenced() const { return m_mutatorShouldBeFenced; }
299     const bool* addressOfMutatorShouldBeFenced() const { return &amp;m_mutatorShouldBeFenced; }
300 
301     unsigned barrierThreshold() const { return m_barrierThreshold; }
302     const unsigned* addressOfBarrierThreshold() const { return &amp;m_barrierThreshold; }
303 
304 #if ENABLE(DFG_DOES_GC_VALIDATION)
305     bool expectDoesGC() const { return m_expectDoesGC; }
306     void setExpectDoesGC(bool value) { m_expectDoesGC = value; }
307     bool* addressOfExpectDoesGC() { return &amp;m_expectDoesGC; }
308 #else
309     bool expectDoesGC() const { UNREACHABLE_FOR_PLATFORM(); return true; }
310     void setExpectDoesGC(bool) { UNREACHABLE_FOR_PLATFORM(); }
311     bool* addressOfExpectDoesGC() { UNREACHABLE_FOR_PLATFORM(); return nullptr; }
312 #endif
313 
314     // If true, the GC believes that the mutator is currently messing with the heap. We call this
315     // &quot;having heap access&quot;. The GC may block if the mutator is in this state. If false, the GC may
316     // currently be doing things to the heap that make the heap unsafe to access for the mutator.
317     bool hasAccess() const;
318 
319     // If the mutator does not currently have heap access, this function will acquire it. If the GC
320     // is currently using the lack of heap access to do dangerous things to the heap then this
321     // function will block, waiting for the GC to finish. It&#39;s not valid to call this if the mutator
322     // already has heap access. The mutator is required to precisely track whether or not it has
323     // heap access.
324     //
325     // It&#39;s totally fine to acquireAccess() upon VM instantiation and keep it that way. This is how
326     // WebCore uses us. For most other clients, JSLock does acquireAccess()/releaseAccess() for you.
327     void acquireAccess();
328 
329     // Releases heap access. If the GC is blocking waiting to do bad things to the heap, it will be
330     // allowed to run now.
331     //
332     // Ordinarily, you should use the ReleaseHeapAccessScope to release and then reacquire heap
333     // access. You should do this anytime you&#39;re about do perform a blocking operation, like waiting
334     // on the ParkingLot.
335     void releaseAccess();
336 
337     // This is like a super optimized way of saying:
338     //
339     //     releaseAccess()
340     //     acquireAccess()
341     //
342     // The fast path is an inlined relaxed load and branch. The slow path will block the mutator if
343     // the GC wants to do bad things to the heap.
344     //
345     // All allocations logically call this. As an optimization to improve GC progress, you can call
346     // this anywhere that you can afford a load-branch and where an object allocation would have been
347     // safe.
348     //
349     // The GC will also push a stopIfNecessary() event onto the runloop of the thread that
350     // instantiated the VM whenever it wants the mutator to stop. This means that if you never block
351     // but instead use the runloop to wait for events, then you could safely run in a mode where the
352     // mutator has permanent heap access (like the DOM does). If you have good event handling
353     // discipline (i.e. you don&#39;t block the runloop) then you can be sure that stopIfNecessary() will
354     // already be called for you at the right times.
355     void stopIfNecessary();
356 
357     // This gives the conn to the collector.
358     void relinquishConn();
359 
360     bool mayNeedToStop();
361 
362     void performIncrement(size_t bytes);
363 
364     // This is a much stronger kind of stopping of the collector, and it may require waiting for a
365     // while. This is meant to be a legacy API for clients of collectAllGarbage that expect that there
366     // is no GC before or after that function call. After calling this, you are free to start GCs
367     // yourself but you can be sure that none are running.
368     //
369     // This both prevents new collections from being started asynchronously and waits for any
370     // outstanding collections to complete.
371     void preventCollection();
372     void allowCollection();
373 
374     uint64_t mutatorExecutionVersion() const { return m_mutatorExecutionVersion; }
375     uint64_t phaseVersion() const { return m_phaseVersion; }
376 
377     JS_EXPORT_PRIVATE void addMarkingConstraint(std::unique_ptr&lt;MarkingConstraint&gt;);
378 
379     size_t numOpaqueRoots() const { return m_opaqueRoots.size(); }
380 
381     HeapVerifier* verifier() const { return m_verifier.get(); }
382 
383     void addHeapFinalizerCallback(const HeapFinalizerCallback&amp;);
384     void removeHeapFinalizerCallback(const HeapFinalizerCallback&amp;);
385 
386     void runTaskInParallel(RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt;);
387 
388     template&lt;typename Func&gt;
389     void runFunctionInParallel(const Func&amp; func)
390     {
391         runTaskInParallel(createSharedTask&lt;void(SlotVisitor&amp;)&gt;(func));
392     }
393 
394     template&lt;typename Func&gt;
395     void forEachSlotVisitor(const Func&amp;);
396     unsigned numberOfSlotVisitors();
397 
398     Seconds totalGCTime() const { return m_totalGCTime; }
399 
400     HashMap&lt;JSImmutableButterfly*, JSString*&gt; immutableButterflyToStringCache;
401 
402 private:
403     friend class AllocatingScope;
404     friend class CodeBlock;
405     friend class CollectingScope;
406     friend class DeferGC;
407     friend class DeferGCForAWhile;
408     friend class GCAwareJITStubRoutine;
409     friend class GCLogging;
410     friend class GCThread;
411     friend class HandleSet;
412     friend class HeapUtil;
413     friend class HeapVerifier;
414     friend class JITStubRoutine;
415     friend class LLIntOffsetsExtractor;
416     friend class MarkStackMergingConstraint;
417     friend class MarkedSpace;
418     friend class BlockDirectory;
419     friend class MarkedBlock;
420     friend class RunningScope;
421     friend class SlotVisitor;
422     friend class SpaceTimeMutatorScheduler;
423     friend class StochasticSpaceTimeMutatorScheduler;
424     friend class SweepingScope;
425     friend class IncrementalSweeper;
426     friend class VM;
427     friend class WeakSet;
428 
429     class Thread;
430     friend class Thread;
431 
432     static const size_t minExtraMemory = 256;
433 
434     class FinalizerOwner : public WeakHandleOwner {
435         void finalize(Handle&lt;Unknown&gt;, void* context) override;
436     };
437 
438     JS_EXPORT_PRIVATE bool isValidAllocation(size_t);
439     JS_EXPORT_PRIVATE void reportExtraMemoryAllocatedSlowCase(size_t);
440     JS_EXPORT_PRIVATE void deprecatedReportExtraMemorySlowCase(size_t);
441 
442     bool shouldCollectInCollectorThread(const AbstractLocker&amp;);
443     void collectInCollectorThread();
444 
445     void checkConn(GCConductor);
446 
447     enum class RunCurrentPhaseResult {
448         Finished,
449         Continue,
450         NeedCurrentThreadState
451     };
452     RunCurrentPhaseResult runCurrentPhase(GCConductor, CurrentThreadState*);
453 
454     // Returns true if we should keep doing things.
455     bool runNotRunningPhase(GCConductor);
456     bool runBeginPhase(GCConductor);
457     bool runFixpointPhase(GCConductor);
458     bool runConcurrentPhase(GCConductor);
459     bool runReloopPhase(GCConductor);
460     bool runEndPhase(GCConductor);
461     bool changePhase(GCConductor, CollectorPhase);
462     bool finishChangingPhase(GCConductor);
463 
464     void collectInMutatorThread();
465 
466     void stopThePeriphery(GCConductor);
467     void resumeThePeriphery();
468 
469     // Returns true if the mutator is stopped, false if the mutator has the conn now.
470     bool stopTheMutator();
471     void resumeTheMutator();
472 
473     JS_EXPORT_PRIVATE void stopIfNecessarySlow();
474     bool stopIfNecessarySlow(unsigned extraStateBits);
475 
476     template&lt;typename Func&gt;
477     void waitForCollector(const Func&amp;);
478 
479     JS_EXPORT_PRIVATE void acquireAccessSlow();
480     JS_EXPORT_PRIVATE void releaseAccessSlow();
481 
482     bool handleGCDidJIT(unsigned);
483     void handleGCDidJIT();
484 
485     bool handleNeedFinalize(unsigned);
486     void handleNeedFinalize();
487 
488     bool relinquishConn(unsigned);
489     void finishRelinquishingConn();
490 
491     void setGCDidJIT();
492     void setNeedFinalize();
493     void waitWhileNeedFinalize();
494 
495     void setMutatorWaiting();
496     void clearMutatorWaiting();
497     void notifyThreadStopping(const AbstractLocker&amp;);
498 
499     typedef uint64_t Ticket;
500     Ticket requestCollection(GCRequest);
501     void waitForCollection(Ticket);
502 
503     void suspendCompilerThreads();
504     void willStartCollection();
505     void prepareForMarking();
506 
507     void gatherStackRoots(ConservativeRoots&amp;);
508     void gatherJSStackRoots(ConservativeRoots&amp;);
509     void gatherScratchBufferRoots(ConservativeRoots&amp;);
510     void beginMarking();
511     void visitCompilerWorklistWeakReferences();
512     void removeDeadCompilerWorklistEntries();
513     void updateObjectCounts();
514     void endMarking();
515 
516     void reapWeakHandles();
517     void pruneStaleEntriesFromWeakGCMaps();
518     void sweepArrayBuffers();
519     void snapshotUnswept();
520     void deleteSourceProviderCaches();
521     void notifyIncrementalSweeper();
522     void harvestWeakReferences();
523 
524     template&lt;typename CellType, typename CellSet&gt;
525     void finalizeMarkedUnconditionalFinalizers(CellSet&amp;);
526 
527     void finalizeUnconditionalFinalizers();
528 
529     void deleteUnmarkedCompiledCode();
530     JS_EXPORT_PRIVATE void addToRememberedSet(const JSCell*);
531     void updateAllocationLimits();
532     void didFinishCollection();
533     void resumeCompilerThreads();
534     void gatherExtraHeapSnapshotData(HeapProfiler&amp;);
535     void removeDeadHeapSnapshotNodes(HeapProfiler&amp;);
536     void finalize();
537     void sweepInFinalize();
538 
539     void sweepAllLogicallyEmptyWeakBlocks();
540     bool sweepNextLogicallyEmptyWeakBlock();
541 
542     bool shouldDoFullCollection();
543 
544     void incrementDeferralDepth();
545     void decrementDeferralDepth();
546     void decrementDeferralDepthAndGCIfNeeded();
547     JS_EXPORT_PRIVATE void decrementDeferralDepthAndGCIfNeededSlow();
548 
549     size_t visitCount();
550     size_t bytesVisited();
551 
552     void forEachCodeBlockImpl(const ScopedLambda&lt;void(CodeBlock*)&gt;&amp;);
553     void forEachCodeBlockIgnoringJITPlansImpl(const AbstractLocker&amp; codeBlockSetLocker, const ScopedLambda&lt;void(CodeBlock*)&gt;&amp;);
554 
555     void setMutatorShouldBeFenced(bool value);
556 
557     void addCoreConstraints();
558 
559     enum class MemoryThresholdCallType {
560         Cached,
561         Direct
562     };
563 
564     bool overCriticalMemoryThreshold(MemoryThresholdCallType memoryThresholdCallType = MemoryThresholdCallType::Cached);
565 
566     template&lt;typename Func&gt;
567     void iterateExecutingAndCompilingCodeBlocks(const Func&amp;);
568 
569     template&lt;typename Func&gt;
570     void iterateExecutingAndCompilingCodeBlocksWithoutHoldingLocks(const Func&amp;);
571 
572     void assertMarkStacksEmpty();
573 
574     void setBonusVisitorTask(RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt;);
575 
576     static bool useGenerationalGC();
577     static bool shouldSweepSynchronously();
578 
579     const HeapType m_heapType;
580     MutatorState m_mutatorState { MutatorState::Running };
581     const size_t m_ramSize;
582     const size_t m_minBytesPerCycle;
583     size_t m_sizeAfterLastCollect { 0 };
584     size_t m_sizeAfterLastFullCollect { 0 };
585     size_t m_sizeBeforeLastFullCollect { 0 };
586     size_t m_sizeAfterLastEdenCollect { 0 };
587     size_t m_sizeBeforeLastEdenCollect { 0 };
588 
589     size_t m_bytesAllocatedThisCycle { 0 };
590     size_t m_bytesAbandonedSinceLastFullCollect { 0 };
591     size_t m_maxEdenSize;
592     size_t m_maxEdenSizeWhenCritical;
593     size_t m_maxHeapSize;
594     size_t m_totalBytesVisited { 0 };
595     size_t m_totalBytesVisitedThisCycle { 0 };
596     double m_incrementBalance { 0 };
597 
598     bool m_shouldDoFullCollection { false };
599     Markable&lt;CollectionScope, EnumMarkableTraits&lt;CollectionScope&gt;&gt; m_collectionScope;
600     Markable&lt;CollectionScope, EnumMarkableTraits&lt;CollectionScope&gt;&gt; m_lastCollectionScope;
601     Lock m_raceMarkStackLock;
602 #if ENABLE(DFG_DOES_GC_VALIDATION)
603     bool m_expectDoesGC { true };
604 #endif
605 
606     StructureIDTable m_structureIDTable;
607     MarkedSpace m_objectSpace;
608     GCIncomingRefCountedSet&lt;ArrayBuffer&gt; m_arrayBuffers;
609     size_t m_extraMemorySize { 0 };
610     size_t m_deprecatedExtraMemorySize { 0 };
611 
612     HashSet&lt;const JSCell*&gt; m_copyingRememberedSet;
613 
614     ProtectCountSet m_protectedValues;
615     std::unique_ptr&lt;HashSet&lt;MarkedArgumentBuffer*&gt;&gt; m_markListSet;
616 
617     std::unique_ptr&lt;MachineThreads&gt; m_machineThreads;
618 
619     std::unique_ptr&lt;SlotVisitor&gt; m_collectorSlotVisitor;
620     std::unique_ptr&lt;SlotVisitor&gt; m_mutatorSlotVisitor;
621     std::unique_ptr&lt;MarkStackArray&gt; m_mutatorMarkStack;
622     std::unique_ptr&lt;MarkStackArray&gt; m_raceMarkStack;
623     std::unique_ptr&lt;MarkingConstraintSet&gt; m_constraintSet;
624 
625     // We pool the slot visitors used by parallel marking threads. It&#39;s useful to be able to
626     // enumerate over them, and it&#39;s useful to have them cache some small amount of memory from
627     // one GC to the next. GC marking threads claim these at the start of marking, and return
628     // them at the end.
629     Vector&lt;std::unique_ptr&lt;SlotVisitor&gt;&gt; m_parallelSlotVisitors;
630     Vector&lt;SlotVisitor*&gt; m_availableParallelSlotVisitors;
631 
632     HandleSet m_handleSet;
633     std::unique_ptr&lt;CodeBlockSet&gt; m_codeBlocks;
634     std::unique_ptr&lt;JITStubRoutineSet&gt; m_jitStubRoutines;
635     FinalizerOwner m_finalizerOwner;
636 
637     Lock m_parallelSlotVisitorLock;
638     bool m_isSafeToCollect { false };
639     bool m_isShuttingDown { false };
640     bool m_mutatorShouldBeFenced { Options::forceFencedBarrier() };
641 
642     unsigned m_barrierThreshold { Options::forceFencedBarrier() ? tautologicalThreshold : blackThreshold };
643 
644     VM* m_vm;
645     Seconds m_lastFullGCLength { 10_ms };
646     Seconds m_lastEdenGCLength { 10_ms };
647 
648     Vector&lt;WeakBlock*&gt; m_logicallyEmptyWeakBlocks;
649     size_t m_indexOfNextLogicallyEmptyWeakBlockToSweep { WTF::notFound };
650 
651     RefPtr&lt;FullGCActivityCallback&gt; m_fullActivityCallback;
652     RefPtr&lt;GCActivityCallback&gt; m_edenActivityCallback;
653     Ref&lt;IncrementalSweeper&gt; m_sweeper;
654     Ref&lt;StopIfNecessaryTimer&gt; m_stopIfNecessaryTimer;
655 
656     Vector&lt;HeapObserver*&gt; m_observers;
657 
658     Vector&lt;HeapFinalizerCallback&gt; m_heapFinalizerCallbacks;
659 
660     std::unique_ptr&lt;HeapVerifier&gt; m_verifier;
661 
662 #if USE(FOUNDATION)
663     Vector&lt;RetainPtr&lt;CFTypeRef&gt;&gt; m_delayedReleaseObjects;
664     unsigned m_delayedReleaseRecursionCount { 0 };
665 #endif
666 #if USE(GLIB)
667     Vector&lt;std::unique_ptr&lt;JSCGLibWrapperObject&gt;&gt; m_delayedReleaseObjects;
668     unsigned m_delayedReleaseRecursionCount { 0 };
669 #endif
670     unsigned m_deferralDepth { 0 };
671 
672     HashSet&lt;WeakGCMapBase*&gt; m_weakGCMaps;
673 
674     std::unique_ptr&lt;MarkStackArray&gt; m_sharedCollectorMarkStack;
675     std::unique_ptr&lt;MarkStackArray&gt; m_sharedMutatorMarkStack;
676     unsigned m_numberOfActiveParallelMarkers { 0 };
677     unsigned m_numberOfWaitingParallelMarkers { 0 };
678 
679     ConcurrentPtrHashSet m_opaqueRoots;
680     static const size_t s_blockFragmentLength = 32;
681 
682     ParallelHelperClient m_helperClient;
683     RefPtr&lt;SharedTask&lt;void(SlotVisitor&amp;)&gt;&gt; m_bonusVisitorTask;
684 
685 #if ENABLE(RESOURCE_USAGE)
686     size_t m_blockBytesAllocated { 0 };
687     size_t m_externalMemorySize { 0 };
688 #endif
689 
690     std::unique_ptr&lt;MutatorScheduler&gt; m_scheduler;
691 
692     static const unsigned mutatorHasConnBit = 1u &lt;&lt; 0u; // Must also be protected by threadLock.
693     static const unsigned stoppedBit = 1u &lt;&lt; 1u; // Only set when !hasAccessBit
694     static const unsigned hasAccessBit = 1u &lt;&lt; 2u;
695     static const unsigned gcDidJITBit = 1u &lt;&lt; 3u; // Set when the GC did some JITing, so on resume we need to cpuid.
696     static const unsigned needFinalizeBit = 1u &lt;&lt; 4u;
697     static const unsigned mutatorWaitingBit = 1u &lt;&lt; 5u; // Allows the mutator to use this as a condition variable.
698     Atomic&lt;unsigned&gt; m_worldState;
699     bool m_worldIsStopped { false };
700     Lock m_visitRaceLock;
701     Lock m_markingMutex;
702     Condition m_markingConditionVariable;
703 
704     MonotonicTime m_beforeGC;
705     MonotonicTime m_afterGC;
706     MonotonicTime m_stopTime;
707 
708     Deque&lt;GCRequest&gt; m_requests;
709     GCRequest m_currentRequest;
710     Ticket m_lastServedTicket { 0 };
711     Ticket m_lastGrantedTicket { 0 };
712 
713     CollectorPhase m_lastPhase { CollectorPhase::NotRunning };
714     CollectorPhase m_currentPhase { CollectorPhase::NotRunning };
715     CollectorPhase m_nextPhase { CollectorPhase::NotRunning };
716     bool m_threadShouldStop { false };
717     bool m_threadIsStopping { false };
718     bool m_mutatorDidRun { true };
719     bool m_didDeferGCWork { false };
720     bool m_shouldStopCollectingContinuously { false };
721 
722     uint64_t m_mutatorExecutionVersion { 0 };
723     uint64_t m_phaseVersion { 0 };
724     Box&lt;Lock&gt; m_threadLock;
725     Ref&lt;AutomaticThreadCondition&gt; m_threadCondition; // The mutator must not wait on this. It would cause a deadlock.
726     RefPtr&lt;AutomaticThread&gt; m_thread;
727 
728     RefPtr&lt;WTF::Thread&gt; m_collectContinuouslyThread { nullptr };
729 
730     MonotonicTime m_lastGCStartTime;
731     MonotonicTime m_lastGCEndTime;
732     MonotonicTime m_currentGCStartTime;
733     Seconds m_totalGCTime;
734 
735     uintptr_t m_barriersExecuted { 0 };
736 
737     CurrentThreadState* m_currentThreadState { nullptr };
738     WTF::Thread* m_currentThread { nullptr }; // It&#39;s OK if this becomes a dangling pointer.
739 
740 #if PLATFORM(IOS_FAMILY)
741     unsigned m_precentAvailableMemoryCachedCallCount;
742     bool m_overCriticalMemoryThreshold;
743 #endif
744 
745     bool m_parallelMarkersShouldExit { false };
746     Lock m_collectContinuouslyLock;
747     Condition m_collectContinuouslyCondition;
748 };
749 
750 } // namespace JSC
    </pre>
  </body>
</html>