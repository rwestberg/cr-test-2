<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/CodeBlock.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="CodeBlock.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CodeBlockInlines.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/CodeBlock.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  98 
  99 struct ArithProfile;
 100 struct OpCatch;
 101 
 102 enum ReoptimizationMode { DontCountReoptimization, CountReoptimization };
 103 
 104 class CodeBlock : public JSCell {
 105     typedef JSCell Base;
 106     friend class BytecodeLivenessAnalysis;
 107     friend class JIT;
 108     friend class LLIntOffsetsExtractor;
 109 
 110 public:
 111 
 112     enum CopyParsedBlockTag { CopyParsedBlock };
 113 
 114     static const unsigned StructureFlags = Base::StructureFlags | StructureIsImmortal;
 115     static const bool needsDestruction = true;
 116 
 117     template&lt;typename, SubspaceAccess&gt;
<span class="line-modified"> 118     static void subspaceFor(VM&amp;) { }</span>
 119 
 120     DECLARE_INFO;
 121 
 122 protected:
<span class="line-modified"> 123     CodeBlock(VM*, Structure*, CopyParsedBlockTag, CodeBlock&amp; other);</span>
<span class="line-modified"> 124     CodeBlock(VM*, Structure*, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock*, JSScope*);</span>
 125 
 126     void finishCreation(VM&amp;, CopyParsedBlockTag, CodeBlock&amp; other);
 127     bool finishCreation(VM&amp;, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock*, JSScope*);
 128 
 129     void finishCreationCommon(VM&amp;);
 130 
 131     WriteBarrier&lt;JSGlobalObject&gt; m_globalObject;
 132 
 133 public:
 134     JS_EXPORT_PRIVATE ~CodeBlock();
 135 
 136     UnlinkedCodeBlock* unlinkedCodeBlock() const { return m_unlinkedCode.get(); }
 137 
 138     CString inferredName() const;
 139     CodeBlockHash hash() const;
 140     bool hasHash() const;
 141     bool isSafeToComputeHash() const;
 142     CString hashAsStringIfPossible() const;
 143     CString sourceCodeForTools() const; // Not quite the actual source we parsed; this will do things like prefix the source for a function with a reified signature.
 144     CString sourceCodeOnOneLine() const; // As sourceCodeForTools(), but replaces all whitespace runs with a single space.
<span class="line-modified"> 145     void dumpAssumingJITType(PrintStream&amp;, JITCode::JITType) const;</span>
 146     JS_EXPORT_PRIVATE void dump(PrintStream&amp;) const;
 147 


 148     int numParameters() const { return m_numParameters; }
 149     void setNumParameters(int newValue);
 150 
 151     int numberOfArgumentsToSkip() const { return m_numberOfArgumentsToSkip; }
 152 
 153     int numCalleeLocals() const { return m_numCalleeLocals; }
 154 
 155     int numVars() const { return m_numVars; }
 156 
 157     int* addressOfNumParameters() { return &amp;m_numParameters; }
 158     static ptrdiff_t offsetOfNumParameters() { return OBJECT_OFFSETOF(CodeBlock, m_numParameters); }
 159 
 160     CodeBlock* alternative() const { return static_cast&lt;CodeBlock*&gt;(m_alternative.get()); }
 161     void setAlternative(VM&amp;, CodeBlock*);
 162 
 163     template &lt;typename Functor&gt; void forEachRelatedCodeBlock(Functor&amp;&amp; functor)
 164     {
 165         Functor f(std::forward&lt;Functor&gt;(functor));
 166         Vector&lt;CodeBlock*, 4&gt; codeBlocks;
 167         codeBlocks.append(this);
</pre>
<hr />
<pre>
 244     unsigned columnNumberForBytecodeOffset(unsigned bytecodeOffset);
 245     void expressionRangeForBytecodeOffset(unsigned bytecodeOffset, int&amp; divot,
 246         int&amp; startOffset, int&amp; endOffset, unsigned&amp; line, unsigned&amp; column) const;
 247 
 248     Optional&lt;unsigned&gt; bytecodeOffsetFromCallSiteIndex(CallSiteIndex);
 249 
 250     void getICStatusMap(const ConcurrentJSLocker&amp;, ICStatusMap&amp; result);
 251     void getICStatusMap(ICStatusMap&amp; result);
 252 
 253 #if ENABLE(JIT)
 254     struct JITData {
 255         WTF_MAKE_STRUCT_FAST_ALLOCATED;
 256 
 257         Bag&lt;StructureStubInfo&gt; m_stubInfos;
 258         Bag&lt;JITAddIC&gt; m_addICs;
 259         Bag&lt;JITMulIC&gt; m_mulICs;
 260         Bag&lt;JITNegIC&gt; m_negICs;
 261         Bag&lt;JITSubIC&gt; m_subICs;
 262         Bag&lt;ByValInfo&gt; m_byValInfos;
 263         Bag&lt;CallLinkInfo&gt; m_callLinkInfos;
<span class="line-modified"> 264         SentinelLinkedList&lt;CallLinkInfo, BasicRawSentinelNode&lt;CallLinkInfo&gt;&gt; m_incomingCalls;</span>
<span class="line-modified"> 265         SentinelLinkedList&lt;PolymorphicCallNode, BasicRawSentinelNode&lt;PolymorphicCallNode&gt;&gt; m_incomingPolymorphicCalls;</span>
 266         SegmentedVector&lt;RareCaseProfile, 8&gt; m_rareCaseProfiles;
 267         std::unique_ptr&lt;PCToCodeOriginMap&gt; m_pcToCodeOriginMap;
 268         std::unique_ptr&lt;RegisterAtOffsetList&gt; m_calleeSaveRegisters;
 269         JITCodeMap m_jitCodeMap;
 270     };
 271 
 272     JITData&amp; ensureJITData(const ConcurrentJSLocker&amp; locker)
 273     {
 274         if (LIKELY(m_jitData))
 275             return *m_jitData;
 276         return ensureJITDataSlow(locker);
 277     }
 278     JITData&amp; ensureJITDataSlow(const ConcurrentJSLocker&amp;);
 279 
<span class="line-modified"> 280     JITAddIC* addJITAddIC(ArithProfile*, const Instruction*);</span>
<span class="line-modified"> 281     JITMulIC* addJITMulIC(ArithProfile*, const Instruction*);</span>
<span class="line-modified"> 282     JITNegIC* addJITNegIC(ArithProfile*, const Instruction*);</span>
<span class="line-modified"> 283     JITSubIC* addJITSubIC(ArithProfile*, const Instruction*);</span>
 284 
 285     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITAddGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 286     JITAddIC* addMathIC(ArithProfile* profile, const Instruction* instruction) { return addJITAddIC(profile, instruction); }</span>
 287 
 288     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITMulGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 289     JITMulIC* addMathIC(ArithProfile* profile, const Instruction* instruction) { return addJITMulIC(profile, instruction); }</span>
 290 
 291     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITNegGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 292     JITNegIC* addMathIC(ArithProfile* profile, const Instruction* instruction) { return addJITNegIC(profile, instruction); }</span>
 293 
 294     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITSubGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 295     JITSubIC* addMathIC(ArithProfile* profile, const Instruction* instruction) { return addJITSubIC(profile, instruction); }</span>
 296 
 297     StructureStubInfo* addStubInfo(AccessType);
 298 
 299     // O(n) operation. Use getStubInfoMap() unless you really only intend to get one
 300     // stub info.
 301     StructureStubInfo* findStubInfo(CodeOrigin);
 302 
 303     ByValInfo* addByValInfo();
 304 
 305     CallLinkInfo* addCallLinkInfo();
 306 
 307     // This is a slow function call used primarily for compiling OSR exits in the case
 308     // that there had been inlining. Chances are if you want to use this, you&#39;re really
 309     // looking for a CallLinkInfoMap to amortize the cost of calling this.
 310     CallLinkInfo* getCallLinkInfoForBytecodeIndex(unsigned bytecodeIndex);
 311 
 312     void setJITCodeMap(JITCodeMap&amp;&amp; jitCodeMap)
 313     {
 314         ConcurrentJSLocker locker(m_lock);
 315         ensureJITData(locker).m_jitCodeMap = WTFMove(jitCodeMap);
</pre>
<hr />
<pre>
 367 
 368     const Instruction* outOfLineJumpTarget(const Instruction* pc);
 369     int outOfLineJumpOffset(const Instruction* pc);
 370     int outOfLineJumpOffset(const InstructionStream::Ref&amp; instruction)
 371     {
 372         return outOfLineJumpOffset(instruction.ptr());
 373     }
 374 
 375     inline unsigned bytecodeOffset(const Instruction* returnAddress)
 376     {
 377         const auto* instructionsBegin = instructions().at(0).ptr();
 378         const auto* instructionsEnd = reinterpret_cast&lt;const Instruction*&gt;(reinterpret_cast&lt;uintptr_t&gt;(instructionsBegin) + instructions().size());
 379         RELEASE_ASSERT(returnAddress &gt;= instructionsBegin &amp;&amp; returnAddress &lt; instructionsEnd);
 380         return returnAddress - instructionsBegin;
 381     }
 382 
 383     const InstructionStream&amp; instructions() const { return m_unlinkedCode-&gt;instructions(); }
 384 
 385     size_t predictedMachineCodeSize();
 386 
<span class="line-modified"> 387     unsigned instructionCount() const { return m_instructionCount; }</span>

 388 
 389     // Exactly equivalent to codeBlock-&gt;ownerExecutable()-&gt;newReplacementCodeBlockFor(codeBlock-&gt;specializationKind())
 390     CodeBlock* newReplacement();
 391 
 392     void setJITCode(Ref&lt;JITCode&gt;&amp;&amp; code)
 393     {
 394         ASSERT(heap()-&gt;isDeferred());
<span class="line-modified"> 395         heap()-&gt;reportExtraMemoryAllocated(code-&gt;size());</span>


 396         ConcurrentJSLocker locker(m_lock);
 397         WTF::storeStoreFence(); // This is probably not needed because the lock will also do something similar, but it&#39;s good to be paranoid.
 398         m_jitCode = WTFMove(code);
 399     }

 400     RefPtr&lt;JITCode&gt; jitCode() { return m_jitCode; }
 401     static ptrdiff_t jitCodeOffset() { return OBJECT_OFFSETOF(CodeBlock, m_jitCode); }
<span class="line-modified"> 402     JITCode::JITType jitType() const</span>
 403     {
 404         JITCode* jitCode = m_jitCode.get();
 405         WTF::loadLoadFence();
<span class="line-modified"> 406         JITCode::JITType result = JITCode::jitTypeFor(jitCode);</span>
 407         WTF::loadLoadFence(); // This probably isn&#39;t needed. Oh well, paranoia is good.
 408         return result;
 409     }
 410 
 411     bool hasBaselineJITProfiling() const
 412     {
<span class="line-modified"> 413         return jitType() == JITCode::BaselineJIT;</span>
 414     }
 415 
 416 #if ENABLE(JIT)
 417     CodeBlock* replacement();
 418 
 419     DFG::CapabilityLevel computeCapabilityLevel();
 420     DFG::CapabilityLevel capabilityLevel();
 421     DFG::CapabilityLevel capabilityLevelState() { return static_cast&lt;DFG::CapabilityLevel&gt;(m_capabilityLevelState); }
 422 
<span class="line-modified"> 423     bool hasOptimizedReplacement(JITCode::JITType typeToReplace);</span>
 424     bool hasOptimizedReplacement(); // the typeToReplace is my JITType
 425 #endif
 426 
 427     void jettison(Profiler::JettisonReason, ReoptimizationMode = DontCountReoptimization, const FireDetail* = nullptr);
 428 
 429     ScriptExecutable* ownerExecutable() const { return m_ownerExecutable.get(); }
 430 
 431     ExecutableToCodeBlockEdge* ownerEdge() const { return m_ownerEdge.get(); }
 432 
<span class="line-modified"> 433     VM* vm() const { return m_vm; }</span>
 434 
 435     VirtualRegister thisRegister() const { return m_unlinkedCode-&gt;thisRegister(); }
 436 
 437     bool usesEval() const { return m_unlinkedCode-&gt;usesEval(); }
 438 
 439     void setScopeRegister(VirtualRegister scopeRegister)
 440     {
 441         ASSERT(scopeRegister.isLocal() || !scopeRegister.isValid());
 442         m_scopeRegister = scopeRegister;
 443     }
 444 
 445     VirtualRegister scopeRegister() const
 446     {
 447         return m_scopeRegister;
 448     }
 449 
 450     PutPropertySlot::Context putByIdContext() const
 451     {
 452         if (codeType() == EvalCode)
 453             return PutPropertySlot::PutByIdEval;
 454         return PutPropertySlot::PutById;
 455     }
 456 
 457     const SourceCode&amp; source() const { return m_ownerExecutable-&gt;source(); }
 458     unsigned sourceOffset() const { return m_ownerExecutable-&gt;source().startOffset(); }
 459     unsigned firstLineColumnOffset() const { return m_ownerExecutable-&gt;startColumn(); }
 460 
 461     size_t numberOfJumpTargets() const { return m_unlinkedCode-&gt;numberOfJumpTargets(); }
 462     unsigned jumpTarget(int index) const { return m_unlinkedCode-&gt;jumpTarget(index); }
 463 
 464     String nameForRegister(VirtualRegister);
 465 
 466     unsigned numberOfArgumentValueProfiles()
 467     {
 468         ASSERT(m_numParameters &gt;= 0);
<span class="line-modified"> 469         ASSERT(m_argumentValueProfiles.size() == static_cast&lt;unsigned&gt;(m_numParameters) || !vm()-&gt;canUseJIT());</span>
 470         return m_argumentValueProfiles.size();
 471     }
 472 
 473     ValueProfile&amp; valueProfileForArgument(unsigned argumentIndex)
 474     {
<span class="line-modified"> 475         ASSERT(vm()-&gt;canUseJIT()); // This is only called from the various JIT compilers or places that first check numberOfArgumentValueProfiles before calling this.</span>
 476         ValueProfile&amp; result = m_argumentValueProfiles[argumentIndex];
<span class="line-removed"> 477         ASSERT(result.m_bytecodeOffset == -1);</span>
 478         return result;
 479     }
 480 
 481     ValueProfile&amp; valueProfileForBytecodeOffset(int bytecodeOffset);
 482     SpeculatedType valueProfilePredictionForBytecodeOffset(const ConcurrentJSLocker&amp;, int bytecodeOffset);
 483 
 484     template&lt;typename Functor&gt; void forEachValueProfile(const Functor&amp;);
 485     template&lt;typename Functor&gt; void forEachArrayProfile(const Functor&amp;);
 486     template&lt;typename Functor&gt; void forEachArrayAllocationProfile(const Functor&amp;);
 487     template&lt;typename Functor&gt; void forEachObjectAllocationProfile(const Functor&amp;);
 488     template&lt;typename Functor&gt; void forEachLLIntCallLinkInfo(const Functor&amp;);
 489 
 490     ArithProfile* arithProfileForBytecodeOffset(InstructionStream::Offset bytecodeOffset);
 491     ArithProfile* arithProfileForPC(const Instruction*);
 492 
 493     bool couldTakeSpecialFastCase(InstructionStream::Offset bytecodeOffset);
 494 
 495     ArrayProfile* getArrayProfile(const ConcurrentJSLocker&amp;, unsigned bytecodeOffset);
 496     ArrayProfile* getArrayProfile(unsigned bytecodeOffset);
 497 
</pre>
<hr />
<pre>
 576     JSGlobalObject* globalObjectFor(CodeOrigin);
 577 
 578     BytecodeLivenessAnalysis&amp; livenessAnalysis()
 579     {
 580         return m_unlinkedCode-&gt;livenessAnalysis(this);
 581     }
 582 
 583     void validate();
 584 
 585     // Jump Tables
 586 
 587     size_t numberOfSwitchJumpTables() const { return m_rareData ? m_rareData-&gt;m_switchJumpTables.size() : 0; }
 588     SimpleJumpTable&amp; addSwitchJumpTable() { createRareDataIfNecessary(); m_rareData-&gt;m_switchJumpTables.append(SimpleJumpTable()); return m_rareData-&gt;m_switchJumpTables.last(); }
 589     SimpleJumpTable&amp; switchJumpTable(int tableIndex) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_switchJumpTables[tableIndex]; }
 590     void clearSwitchJumpTables()
 591     {
 592         if (!m_rareData)
 593             return;
 594         m_rareData-&gt;m_switchJumpTables.clear();
 595     }







 596 
 597     size_t numberOfStringSwitchJumpTables() const { return m_rareData ? m_rareData-&gt;m_stringSwitchJumpTables.size() : 0; }
 598     StringJumpTable&amp; addStringSwitchJumpTable() { createRareDataIfNecessary(); m_rareData-&gt;m_stringSwitchJumpTables.append(StringJumpTable()); return m_rareData-&gt;m_stringSwitchJumpTables.last(); }
 599     StringJumpTable&amp; stringSwitchJumpTable(int tableIndex) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_stringSwitchJumpTables[tableIndex]; }
 600 
 601     DirectEvalCodeCache&amp; directEvalCodeCache() { createRareDataIfNecessary(); return m_rareData-&gt;m_directEvalCodeCache; }
 602 
 603     enum ShrinkMode {
 604         // Shrink prior to generating machine code that may point directly into vectors.
 605         EarlyShrink,
 606 
 607         // Shrink after generating machine code, and after possibly creating new vectors
 608         // and appending to others. At this time it is not safe to shrink certain vectors
 609         // because we would have generated machine code that references them directly.
 610         LateShrink
 611     };
 612     void shrinkToFit(ShrinkMode);
 613 
 614     // Functions for controlling when JITting kicks in, in a mixed mode
 615     // execution world.
 616 
 617     bool checkIfJITThresholdReached()
 618     {
 619         return m_llintExecuteCounter.checkIfThresholdCrossedAndSet(this);
 620     }
 621 
 622     void dontJITAnytimeSoon()
 623     {
 624         m_llintExecuteCounter.deferIndefinitely();
 625     }
 626 
 627     int32_t thresholdForJIT(int32_t threshold);
 628     void jitAfterWarmUp();
 629     void jitSoon();
 630 
 631     const BaselineExecutionCounter&amp; llintExecuteCounter() const
 632     {
 633         return m_llintExecuteCounter;
 634     }
 635 
<span class="line-modified"> 636     typedef HashMap&lt;std::tuple&lt;Structure*, const Instruction*&gt;, Bag&lt;LLIntPrototypeLoadAdaptiveStructureWatchpoint&gt;&gt; StructureWatchpointMap;</span>
 637     StructureWatchpointMap&amp; llintGetByIdWatchpointMap() { return m_llintGetByIdWatchpointMap; }
 638 
 639     // Functions for controlling when tiered compilation kicks in. This
 640     // controls both when the optimizing compiler is invoked and when OSR
 641     // entry happens. Two triggers exist: the loop trigger and the return
 642     // trigger. In either case, when an addition to m_jitExecuteCounter
 643     // causes it to become non-negative, the optimizing compiler is
 644     // invoked. This includes a fast check to see if this CodeBlock has
 645     // already been optimized (i.e. replacement() returns a CodeBlock
 646     // that was optimized with a higher tier JIT than this one). In the
 647     // case of the loop trigger, if the optimized compilation succeeds
 648     // (or has already succeeded in the past) then OSR is attempted to
 649     // redirect program flow into the optimized code.
 650 
 651     // These functions are called from within the optimization triggers,
 652     // and are used as a single point at which we define the heuristics
 653     // for how much warm-up is mandated before the next optimization
 654     // trigger files. All CodeBlocks start out with optimizeAfterWarmUp(),
 655     // as this is called from the CodeBlock constructor.
 656 
</pre>
<hr />
<pre>
 886     void finalizeBaselineJITInlineCaches();
 887 #endif
 888 #if ENABLE(DFG_JIT)
 889     void tallyFrequentExitSites();
 890 #else
 891     void tallyFrequentExitSites() { }
 892 #endif
 893 
 894 private:
 895     friend class CodeBlockSet;
 896     friend class ExecutableToCodeBlockEdge;
 897 
 898     BytecodeLivenessAnalysis&amp; livenessAnalysisSlow();
 899 
 900     CodeBlock* specialOSREntryBlockOrNull();
 901 
 902     void noticeIncomingCall(ExecState* callerFrame);
 903 
 904     double optimizationThresholdScalingFactor();
 905 
<span class="line-modified"> 906     void updateAllPredictionsAndCountLiveness(unsigned&amp; numberOfLiveNonArgumentValueProfiles, unsigned&amp; numberOfSamplesInProfiles);</span>
 907 
 908     void setConstantIdentifierSetRegisters(VM&amp;, const Vector&lt;ConstantIdentifierSetEntry&gt;&amp; constants);
 909 
<span class="line-modified"> 910     void setConstantRegisters(const Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants, const Vector&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation);</span>
 911 
 912     void replaceConstant(int index, JSValue value)
 913     {
 914         ASSERT(isConstantRegisterIndex(index) &amp;&amp; static_cast&lt;size_t&gt;(index - FirstConstantRegisterIndex) &lt; m_constantRegisters.size());
 915         m_constantRegisters[index - FirstConstantRegisterIndex].set(*m_vm, this, value);
 916     }
 917 
 918     bool shouldVisitStrongly(const ConcurrentJSLocker&amp;);
<span class="line-modified"> 919     bool shouldJettisonDueToWeakReference();</span>
 920     bool shouldJettisonDueToOldAge(const ConcurrentJSLocker&amp;);
 921 
 922     void propagateTransitions(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 923     void determineLiveness(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 924 
 925     void stronglyVisitStrongReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 926     void stronglyVisitWeakReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 927     void visitOSRExitTargets(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 928 
 929     unsigned numberOfNonArgumentValueProfiles() { return m_numberOfNonArgumentValueProfiles; }
 930     unsigned totalNumberOfValueProfiles() { return numberOfArgumentValueProfiles() + numberOfNonArgumentValueProfiles(); }
 931     ValueProfile* tryGetValueProfileForBytecodeOffset(int bytecodeOffset);
 932 
 933     Seconds timeSinceCreation()
 934     {
 935         return MonotonicTime::now() - m_creationTime;
 936     }
 937 
 938     void createRareDataIfNecessary()
 939     {
 940         if (!m_rareData) {
<span class="line-modified"> 941             auto rareData = std::make_unique&lt;RareData&gt;();</span>
 942             WTF::storeStoreFence(); // m_catchProfiles can be touched from compiler threads.
 943             m_rareData = WTFMove(rareData);
 944         }
 945     }
 946 
 947     void insertBasicBlockBoundariesForControlFlowProfiler();
 948     void ensureCatchLivenessIsComputedForBytecodeOffsetSlow(const OpCatch&amp;, InstructionStream::Offset);
 949 
 950     int m_numCalleeLocals;
 951     int m_numVars;
 952     int m_numParameters;
 953     int m_numberOfArgumentsToSkip { 0 };
 954     unsigned m_numberOfNonArgumentValueProfiles { 0 };
 955     union {
 956         unsigned m_debuggerRequests;
 957         struct {
 958             unsigned m_hasDebuggerStatement : 1;
 959             unsigned m_steppingMode : 1;
 960             unsigned m_numBreakpoints : 30;
 961         };
 962     };
<span class="line-modified"> 963     unsigned m_instructionCount { 0 };</span>
 964     VirtualRegister m_scopeRegister;
 965     mutable CodeBlockHash m_hash;
 966 
 967     WriteBarrier&lt;UnlinkedCodeBlock&gt; m_unlinkedCode;
 968     WriteBarrier&lt;ScriptExecutable&gt; m_ownerExecutable;
 969     WriteBarrier&lt;ExecutableToCodeBlockEdge&gt; m_ownerEdge;


 970     VM* m_vm;
 971 
 972     const void* m_instructionsRawPointer { nullptr };
<span class="line-modified"> 973     SentinelLinkedList&lt;LLIntCallLinkInfo, BasicRawSentinelNode&lt;LLIntCallLinkInfo&gt;&gt; m_incomingLLIntCalls;</span>
 974     StructureWatchpointMap m_llintGetByIdWatchpointMap;
 975     RefPtr&lt;JITCode&gt; m_jitCode;
 976 #if ENABLE(JIT)
 977     std::unique_ptr&lt;JITData&gt; m_jitData;
 978 #endif
 979 #if ENABLE(DFG_JIT)
 980     // This is relevant to non-DFG code blocks that serve as the profiled code block
 981     // for DFG code blocks.
 982     CompressedLazyOperandValueProfileHolder m_lazyOperandValueProfiles;
 983 #endif
 984     RefCountedArray&lt;ValueProfile&gt; m_argumentValueProfiles;
 985 
 986     // Constant Pool
 987     COMPILE_ASSERT(sizeof(Register) == sizeof(WriteBarrier&lt;Unknown&gt;), Register_must_be_same_size_as_WriteBarrier_Unknown);
 988     // TODO: This could just be a pointer to m_unlinkedCodeBlock&#39;s data, but the DFG mutates
 989     // it, so we&#39;re stuck with it for now.
 990     Vector&lt;WriteBarrier&lt;Unknown&gt;&gt; m_constantRegisters;
 991     Vector&lt;SourceCodeRepresentation&gt; m_constantsSourceCodeRepresentation;
 992     RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt; m_functionDecls;
 993     RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt; m_functionExprs;
 994 
 995     WriteBarrier&lt;CodeBlock&gt; m_alternative;
 996 
 997     BaselineExecutionCounter m_llintExecuteCounter;
 998 
 999     BaselineExecutionCounter m_jitExecuteCounter;
1000     uint32_t m_osrExitCounter;
1001 
1002     uint16_t m_optimizationDelayCounter;
1003     uint16_t m_reoptimizationRetryCounter;
1004 
1005     RefPtr&lt;MetadataTable&gt; m_metadata;
1006 
1007     MonotonicTime m_creationTime;

1008 
1009     std::unique_ptr&lt;RareData&gt; m_rareData;
1010 };
1011 
1012 inline Register&amp; ExecState::r(int index)
1013 {
1014     CodeBlock* codeBlock = this-&gt;codeBlock();
1015     if (codeBlock-&gt;isConstantRegisterIndex(index))
1016         return *reinterpret_cast&lt;Register*&gt;(&amp;codeBlock-&gt;constantRegister(index));
1017     return this[index];
1018 }
1019 
1020 inline Register&amp; ExecState::r(VirtualRegister reg)
1021 {
1022     return r(reg.offset());
1023 }
1024 
1025 inline Register&amp; ExecState::uncheckedR(int index)
1026 {
1027     RELEASE_ASSERT(index &lt; FirstConstantRegisterIndex);
1028     return this[index];
1029 }
1030 
1031 inline Register&amp; ExecState::uncheckedR(VirtualRegister reg)
1032 {
1033     return uncheckedR(reg.offset());
1034 }
1035 
1036 template &lt;typename ExecutableType&gt;
<span class="line-modified">1037 JSObject* ScriptExecutable::prepareForExecution(VM&amp; vm, JSFunction* function, JSScope* scope, CodeSpecializationKind kind, CodeBlock*&amp; resultCodeBlock)</span>
1038 {
1039     if (hasJITCodeFor(kind)) {
1040         if (std::is_same&lt;ExecutableType, EvalExecutable&gt;::value)
1041             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;EvalExecutable*&gt;(this)-&gt;codeBlock());
1042         else if (std::is_same&lt;ExecutableType, ProgramExecutable&gt;::value)
1043             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;ProgramExecutable*&gt;(this)-&gt;codeBlock());
1044         else if (std::is_same&lt;ExecutableType, ModuleProgramExecutable&gt;::value)
1045             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;ModuleProgramExecutable*&gt;(this)-&gt;codeBlock());
1046         else if (std::is_same&lt;ExecutableType, FunctionExecutable&gt;::value)
1047             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;FunctionExecutable*&gt;(this)-&gt;codeBlockFor(kind));
1048         else
1049             RELEASE_ASSERT_NOT_REACHED();
1050         return nullptr;
1051     }
1052     return prepareForExecutionImpl(vm, function, scope, kind, resultCodeBlock);
1053 }
1054 
1055 #define CODEBLOCK_LOG_EVENT(codeBlock, summary, details) \
<span class="line-modified">1056     (codeBlock-&gt;vm()-&gt;logEvent(codeBlock, summary, [&amp;] () { return toCString details; }))</span>



1057 
1058 
1059 void setPrinter(Printer::PrintRecord&amp;, CodeBlock*);
1060 
1061 } // namespace JSC
1062 
1063 namespace WTF {
1064 
1065 JS_EXPORT_PRIVATE void printInternal(PrintStream&amp;, JSC::CodeBlock*);
1066 
1067 } // namespace WTF
</pre>
</td>
<td>
<hr />
<pre>
  98 
  99 struct ArithProfile;
 100 struct OpCatch;
 101 
 102 enum ReoptimizationMode { DontCountReoptimization, CountReoptimization };
 103 
 104 class CodeBlock : public JSCell {
 105     typedef JSCell Base;
 106     friend class BytecodeLivenessAnalysis;
 107     friend class JIT;
 108     friend class LLIntOffsetsExtractor;
 109 
 110 public:
 111 
 112     enum CopyParsedBlockTag { CopyParsedBlock };
 113 
 114     static const unsigned StructureFlags = Base::StructureFlags | StructureIsImmortal;
 115     static const bool needsDestruction = true;
 116 
 117     template&lt;typename, SubspaceAccess&gt;
<span class="line-modified"> 118     static IsoSubspace* subspaceFor(VM&amp;) { return nullptr; }</span>
 119 
 120     DECLARE_INFO;
 121 
 122 protected:
<span class="line-modified"> 123     CodeBlock(VM&amp;, Structure*, CopyParsedBlockTag, CodeBlock&amp; other);</span>
<span class="line-modified"> 124     CodeBlock(VM&amp;, Structure*, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock*, JSScope*);</span>
 125 
 126     void finishCreation(VM&amp;, CopyParsedBlockTag, CodeBlock&amp; other);
 127     bool finishCreation(VM&amp;, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock*, JSScope*);
 128 
 129     void finishCreationCommon(VM&amp;);
 130 
 131     WriteBarrier&lt;JSGlobalObject&gt; m_globalObject;
 132 
 133 public:
 134     JS_EXPORT_PRIVATE ~CodeBlock();
 135 
 136     UnlinkedCodeBlock* unlinkedCodeBlock() const { return m_unlinkedCode.get(); }
 137 
 138     CString inferredName() const;
 139     CodeBlockHash hash() const;
 140     bool hasHash() const;
 141     bool isSafeToComputeHash() const;
 142     CString hashAsStringIfPossible() const;
 143     CString sourceCodeForTools() const; // Not quite the actual source we parsed; this will do things like prefix the source for a function with a reified signature.
 144     CString sourceCodeOnOneLine() const; // As sourceCodeForTools(), but replaces all whitespace runs with a single space.
<span class="line-modified"> 145     void dumpAssumingJITType(PrintStream&amp;, JITType) const;</span>
 146     JS_EXPORT_PRIVATE void dump(PrintStream&amp;) const;
 147 
<span class="line-added"> 148     MetadataTable* metadataTable() const { return m_metadata.get(); }</span>
<span class="line-added"> 149 </span>
 150     int numParameters() const { return m_numParameters; }
 151     void setNumParameters(int newValue);
 152 
 153     int numberOfArgumentsToSkip() const { return m_numberOfArgumentsToSkip; }
 154 
 155     int numCalleeLocals() const { return m_numCalleeLocals; }
 156 
 157     int numVars() const { return m_numVars; }
 158 
 159     int* addressOfNumParameters() { return &amp;m_numParameters; }
 160     static ptrdiff_t offsetOfNumParameters() { return OBJECT_OFFSETOF(CodeBlock, m_numParameters); }
 161 
 162     CodeBlock* alternative() const { return static_cast&lt;CodeBlock*&gt;(m_alternative.get()); }
 163     void setAlternative(VM&amp;, CodeBlock*);
 164 
 165     template &lt;typename Functor&gt; void forEachRelatedCodeBlock(Functor&amp;&amp; functor)
 166     {
 167         Functor f(std::forward&lt;Functor&gt;(functor));
 168         Vector&lt;CodeBlock*, 4&gt; codeBlocks;
 169         codeBlocks.append(this);
</pre>
<hr />
<pre>
 246     unsigned columnNumberForBytecodeOffset(unsigned bytecodeOffset);
 247     void expressionRangeForBytecodeOffset(unsigned bytecodeOffset, int&amp; divot,
 248         int&amp; startOffset, int&amp; endOffset, unsigned&amp; line, unsigned&amp; column) const;
 249 
 250     Optional&lt;unsigned&gt; bytecodeOffsetFromCallSiteIndex(CallSiteIndex);
 251 
 252     void getICStatusMap(const ConcurrentJSLocker&amp;, ICStatusMap&amp; result);
 253     void getICStatusMap(ICStatusMap&amp; result);
 254 
 255 #if ENABLE(JIT)
 256     struct JITData {
 257         WTF_MAKE_STRUCT_FAST_ALLOCATED;
 258 
 259         Bag&lt;StructureStubInfo&gt; m_stubInfos;
 260         Bag&lt;JITAddIC&gt; m_addICs;
 261         Bag&lt;JITMulIC&gt; m_mulICs;
 262         Bag&lt;JITNegIC&gt; m_negICs;
 263         Bag&lt;JITSubIC&gt; m_subICs;
 264         Bag&lt;ByValInfo&gt; m_byValInfos;
 265         Bag&lt;CallLinkInfo&gt; m_callLinkInfos;
<span class="line-modified"> 266         SentinelLinkedList&lt;CallLinkInfo, PackedRawSentinelNode&lt;CallLinkInfo&gt;&gt; m_incomingCalls;</span>
<span class="line-modified"> 267         SentinelLinkedList&lt;PolymorphicCallNode, PackedRawSentinelNode&lt;PolymorphicCallNode&gt;&gt; m_incomingPolymorphicCalls;</span>
 268         SegmentedVector&lt;RareCaseProfile, 8&gt; m_rareCaseProfiles;
 269         std::unique_ptr&lt;PCToCodeOriginMap&gt; m_pcToCodeOriginMap;
 270         std::unique_ptr&lt;RegisterAtOffsetList&gt; m_calleeSaveRegisters;
 271         JITCodeMap m_jitCodeMap;
 272     };
 273 
 274     JITData&amp; ensureJITData(const ConcurrentJSLocker&amp; locker)
 275     {
 276         if (LIKELY(m_jitData))
 277             return *m_jitData;
 278         return ensureJITDataSlow(locker);
 279     }
 280     JITData&amp; ensureJITDataSlow(const ConcurrentJSLocker&amp;);
 281 
<span class="line-modified"> 282     JITAddIC* addJITAddIC(ArithProfile*);</span>
<span class="line-modified"> 283     JITMulIC* addJITMulIC(ArithProfile*);</span>
<span class="line-modified"> 284     JITNegIC* addJITNegIC(ArithProfile*);</span>
<span class="line-modified"> 285     JITSubIC* addJITSubIC(ArithProfile*);</span>
 286 
 287     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITAddGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 288     JITAddIC* addMathIC(ArithProfile* profile) { return addJITAddIC(profile); }</span>
 289 
 290     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITMulGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 291     JITMulIC* addMathIC(ArithProfile* profile) { return addJITMulIC(profile); }</span>
 292 
 293     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITNegGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 294     JITNegIC* addMathIC(ArithProfile* profile) { return addJITNegIC(profile); }</span>
 295 
 296     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITSubGenerator&gt;::value&gt;::type&gt;
<span class="line-modified"> 297     JITSubIC* addMathIC(ArithProfile* profile) { return addJITSubIC(profile); }</span>
 298 
 299     StructureStubInfo* addStubInfo(AccessType);
 300 
 301     // O(n) operation. Use getStubInfoMap() unless you really only intend to get one
 302     // stub info.
 303     StructureStubInfo* findStubInfo(CodeOrigin);
 304 
 305     ByValInfo* addByValInfo();
 306 
 307     CallLinkInfo* addCallLinkInfo();
 308 
 309     // This is a slow function call used primarily for compiling OSR exits in the case
 310     // that there had been inlining. Chances are if you want to use this, you&#39;re really
 311     // looking for a CallLinkInfoMap to amortize the cost of calling this.
 312     CallLinkInfo* getCallLinkInfoForBytecodeIndex(unsigned bytecodeIndex);
 313 
 314     void setJITCodeMap(JITCodeMap&amp;&amp; jitCodeMap)
 315     {
 316         ConcurrentJSLocker locker(m_lock);
 317         ensureJITData(locker).m_jitCodeMap = WTFMove(jitCodeMap);
</pre>
<hr />
<pre>
 369 
 370     const Instruction* outOfLineJumpTarget(const Instruction* pc);
 371     int outOfLineJumpOffset(const Instruction* pc);
 372     int outOfLineJumpOffset(const InstructionStream::Ref&amp; instruction)
 373     {
 374         return outOfLineJumpOffset(instruction.ptr());
 375     }
 376 
 377     inline unsigned bytecodeOffset(const Instruction* returnAddress)
 378     {
 379         const auto* instructionsBegin = instructions().at(0).ptr();
 380         const auto* instructionsEnd = reinterpret_cast&lt;const Instruction*&gt;(reinterpret_cast&lt;uintptr_t&gt;(instructionsBegin) + instructions().size());
 381         RELEASE_ASSERT(returnAddress &gt;= instructionsBegin &amp;&amp; returnAddress &lt; instructionsEnd);
 382         return returnAddress - instructionsBegin;
 383     }
 384 
 385     const InstructionStream&amp; instructions() const { return m_unlinkedCode-&gt;instructions(); }
 386 
 387     size_t predictedMachineCodeSize();
 388 
<span class="line-modified"> 389     unsigned instructionsSize() const { return instructions().size(); }</span>
<span class="line-added"> 390     unsigned bytecodeCost() const { return m_bytecodeCost; }</span>
 391 
 392     // Exactly equivalent to codeBlock-&gt;ownerExecutable()-&gt;newReplacementCodeBlockFor(codeBlock-&gt;specializationKind())
 393     CodeBlock* newReplacement();
 394 
 395     void setJITCode(Ref&lt;JITCode&gt;&amp;&amp; code)
 396     {
 397         ASSERT(heap()-&gt;isDeferred());
<span class="line-modified"> 398         if (!code-&gt;isShared())</span>
<span class="line-added"> 399             heap()-&gt;reportExtraMemoryAllocated(code-&gt;size());</span>
<span class="line-added"> 400 </span>
 401         ConcurrentJSLocker locker(m_lock);
 402         WTF::storeStoreFence(); // This is probably not needed because the lock will also do something similar, but it&#39;s good to be paranoid.
 403         m_jitCode = WTFMove(code);
 404     }
<span class="line-added"> 405 </span>
 406     RefPtr&lt;JITCode&gt; jitCode() { return m_jitCode; }
 407     static ptrdiff_t jitCodeOffset() { return OBJECT_OFFSETOF(CodeBlock, m_jitCode); }
<span class="line-modified"> 408     JITType jitType() const</span>
 409     {
 410         JITCode* jitCode = m_jitCode.get();
 411         WTF::loadLoadFence();
<span class="line-modified"> 412         JITType result = JITCode::jitTypeFor(jitCode);</span>
 413         WTF::loadLoadFence(); // This probably isn&#39;t needed. Oh well, paranoia is good.
 414         return result;
 415     }
 416 
 417     bool hasBaselineJITProfiling() const
 418     {
<span class="line-modified"> 419         return jitType() == JITType::BaselineJIT;</span>
 420     }
 421 
 422 #if ENABLE(JIT)
 423     CodeBlock* replacement();
 424 
 425     DFG::CapabilityLevel computeCapabilityLevel();
 426     DFG::CapabilityLevel capabilityLevel();
 427     DFG::CapabilityLevel capabilityLevelState() { return static_cast&lt;DFG::CapabilityLevel&gt;(m_capabilityLevelState); }
 428 
<span class="line-modified"> 429     bool hasOptimizedReplacement(JITType typeToReplace);</span>
 430     bool hasOptimizedReplacement(); // the typeToReplace is my JITType
 431 #endif
 432 
 433     void jettison(Profiler::JettisonReason, ReoptimizationMode = DontCountReoptimization, const FireDetail* = nullptr);
 434 
 435     ScriptExecutable* ownerExecutable() const { return m_ownerExecutable.get(); }
 436 
 437     ExecutableToCodeBlockEdge* ownerEdge() const { return m_ownerEdge.get(); }
 438 
<span class="line-modified"> 439     VM&amp; vm() const { return *m_vm; }</span>
 440 
 441     VirtualRegister thisRegister() const { return m_unlinkedCode-&gt;thisRegister(); }
 442 
 443     bool usesEval() const { return m_unlinkedCode-&gt;usesEval(); }
 444 
 445     void setScopeRegister(VirtualRegister scopeRegister)
 446     {
 447         ASSERT(scopeRegister.isLocal() || !scopeRegister.isValid());
 448         m_scopeRegister = scopeRegister;
 449     }
 450 
 451     VirtualRegister scopeRegister() const
 452     {
 453         return m_scopeRegister;
 454     }
 455 
 456     PutPropertySlot::Context putByIdContext() const
 457     {
 458         if (codeType() == EvalCode)
 459             return PutPropertySlot::PutByIdEval;
 460         return PutPropertySlot::PutById;
 461     }
 462 
 463     const SourceCode&amp; source() const { return m_ownerExecutable-&gt;source(); }
 464     unsigned sourceOffset() const { return m_ownerExecutable-&gt;source().startOffset(); }
 465     unsigned firstLineColumnOffset() const { return m_ownerExecutable-&gt;startColumn(); }
 466 
 467     size_t numberOfJumpTargets() const { return m_unlinkedCode-&gt;numberOfJumpTargets(); }
 468     unsigned jumpTarget(int index) const { return m_unlinkedCode-&gt;jumpTarget(index); }
 469 
 470     String nameForRegister(VirtualRegister);
 471 
 472     unsigned numberOfArgumentValueProfiles()
 473     {
 474         ASSERT(m_numParameters &gt;= 0);
<span class="line-modified"> 475         ASSERT(m_argumentValueProfiles.size() == static_cast&lt;unsigned&gt;(m_numParameters) || !vm().canUseJIT());</span>
 476         return m_argumentValueProfiles.size();
 477     }
 478 
 479     ValueProfile&amp; valueProfileForArgument(unsigned argumentIndex)
 480     {
<span class="line-modified"> 481         ASSERT(vm().canUseJIT()); // This is only called from the various JIT compilers or places that first check numberOfArgumentValueProfiles before calling this.</span>
 482         ValueProfile&amp; result = m_argumentValueProfiles[argumentIndex];

 483         return result;
 484     }
 485 
 486     ValueProfile&amp; valueProfileForBytecodeOffset(int bytecodeOffset);
 487     SpeculatedType valueProfilePredictionForBytecodeOffset(const ConcurrentJSLocker&amp;, int bytecodeOffset);
 488 
 489     template&lt;typename Functor&gt; void forEachValueProfile(const Functor&amp;);
 490     template&lt;typename Functor&gt; void forEachArrayProfile(const Functor&amp;);
 491     template&lt;typename Functor&gt; void forEachArrayAllocationProfile(const Functor&amp;);
 492     template&lt;typename Functor&gt; void forEachObjectAllocationProfile(const Functor&amp;);
 493     template&lt;typename Functor&gt; void forEachLLIntCallLinkInfo(const Functor&amp;);
 494 
 495     ArithProfile* arithProfileForBytecodeOffset(InstructionStream::Offset bytecodeOffset);
 496     ArithProfile* arithProfileForPC(const Instruction*);
 497 
 498     bool couldTakeSpecialFastCase(InstructionStream::Offset bytecodeOffset);
 499 
 500     ArrayProfile* getArrayProfile(const ConcurrentJSLocker&amp;, unsigned bytecodeOffset);
 501     ArrayProfile* getArrayProfile(unsigned bytecodeOffset);
 502 
</pre>
<hr />
<pre>
 581     JSGlobalObject* globalObjectFor(CodeOrigin);
 582 
 583     BytecodeLivenessAnalysis&amp; livenessAnalysis()
 584     {
 585         return m_unlinkedCode-&gt;livenessAnalysis(this);
 586     }
 587 
 588     void validate();
 589 
 590     // Jump Tables
 591 
 592     size_t numberOfSwitchJumpTables() const { return m_rareData ? m_rareData-&gt;m_switchJumpTables.size() : 0; }
 593     SimpleJumpTable&amp; addSwitchJumpTable() { createRareDataIfNecessary(); m_rareData-&gt;m_switchJumpTables.append(SimpleJumpTable()); return m_rareData-&gt;m_switchJumpTables.last(); }
 594     SimpleJumpTable&amp; switchJumpTable(int tableIndex) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_switchJumpTables[tableIndex]; }
 595     void clearSwitchJumpTables()
 596     {
 597         if (!m_rareData)
 598             return;
 599         m_rareData-&gt;m_switchJumpTables.clear();
 600     }
<span class="line-added"> 601 #if ENABLE(DFG_JIT)</span>
<span class="line-added"> 602     void addSwitchJumpTableFromProfiledCodeBlock(SimpleJumpTable&amp; profiled)</span>
<span class="line-added"> 603     {</span>
<span class="line-added"> 604         createRareDataIfNecessary();</span>
<span class="line-added"> 605         m_rareData-&gt;m_switchJumpTables.append(profiled.cloneNonJITPart());</span>
<span class="line-added"> 606     }</span>
<span class="line-added"> 607 #endif</span>
 608 
 609     size_t numberOfStringSwitchJumpTables() const { return m_rareData ? m_rareData-&gt;m_stringSwitchJumpTables.size() : 0; }
 610     StringJumpTable&amp; addStringSwitchJumpTable() { createRareDataIfNecessary(); m_rareData-&gt;m_stringSwitchJumpTables.append(StringJumpTable()); return m_rareData-&gt;m_stringSwitchJumpTables.last(); }
 611     StringJumpTable&amp; stringSwitchJumpTable(int tableIndex) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_stringSwitchJumpTables[tableIndex]; }
 612 
 613     DirectEvalCodeCache&amp; directEvalCodeCache() { createRareDataIfNecessary(); return m_rareData-&gt;m_directEvalCodeCache; }
 614 
 615     enum ShrinkMode {
 616         // Shrink prior to generating machine code that may point directly into vectors.
 617         EarlyShrink,
 618 
 619         // Shrink after generating machine code, and after possibly creating new vectors
 620         // and appending to others. At this time it is not safe to shrink certain vectors
 621         // because we would have generated machine code that references them directly.
 622         LateShrink
 623     };
 624     void shrinkToFit(ShrinkMode);
 625 
 626     // Functions for controlling when JITting kicks in, in a mixed mode
 627     // execution world.
 628 
 629     bool checkIfJITThresholdReached()
 630     {
 631         return m_llintExecuteCounter.checkIfThresholdCrossedAndSet(this);
 632     }
 633 
 634     void dontJITAnytimeSoon()
 635     {
 636         m_llintExecuteCounter.deferIndefinitely();
 637     }
 638 
 639     int32_t thresholdForJIT(int32_t threshold);
 640     void jitAfterWarmUp();
 641     void jitSoon();
 642 
 643     const BaselineExecutionCounter&amp; llintExecuteCounter() const
 644     {
 645         return m_llintExecuteCounter;
 646     }
 647 
<span class="line-modified"> 648     typedef HashMap&lt;std::tuple&lt;StructureID, unsigned&gt;, Vector&lt;LLIntPrototypeLoadAdaptiveStructureWatchpoint&gt;&gt; StructureWatchpointMap;</span>
 649     StructureWatchpointMap&amp; llintGetByIdWatchpointMap() { return m_llintGetByIdWatchpointMap; }
 650 
 651     // Functions for controlling when tiered compilation kicks in. This
 652     // controls both when the optimizing compiler is invoked and when OSR
 653     // entry happens. Two triggers exist: the loop trigger and the return
 654     // trigger. In either case, when an addition to m_jitExecuteCounter
 655     // causes it to become non-negative, the optimizing compiler is
 656     // invoked. This includes a fast check to see if this CodeBlock has
 657     // already been optimized (i.e. replacement() returns a CodeBlock
 658     // that was optimized with a higher tier JIT than this one). In the
 659     // case of the loop trigger, if the optimized compilation succeeds
 660     // (or has already succeeded in the past) then OSR is attempted to
 661     // redirect program flow into the optimized code.
 662 
 663     // These functions are called from within the optimization triggers,
 664     // and are used as a single point at which we define the heuristics
 665     // for how much warm-up is mandated before the next optimization
 666     // trigger files. All CodeBlocks start out with optimizeAfterWarmUp(),
 667     // as this is called from the CodeBlock constructor.
 668 
</pre>
<hr />
<pre>
 898     void finalizeBaselineJITInlineCaches();
 899 #endif
 900 #if ENABLE(DFG_JIT)
 901     void tallyFrequentExitSites();
 902 #else
 903     void tallyFrequentExitSites() { }
 904 #endif
 905 
 906 private:
 907     friend class CodeBlockSet;
 908     friend class ExecutableToCodeBlockEdge;
 909 
 910     BytecodeLivenessAnalysis&amp; livenessAnalysisSlow();
 911 
 912     CodeBlock* specialOSREntryBlockOrNull();
 913 
 914     void noticeIncomingCall(ExecState* callerFrame);
 915 
 916     double optimizationThresholdScalingFactor();
 917 
<span class="line-modified"> 918     void updateAllValueProfilePredictionsAndCountLiveness(unsigned&amp; numberOfLiveNonArgumentValueProfiles, unsigned&amp; numberOfSamplesInProfiles);</span>
 919 
 920     void setConstantIdentifierSetRegisters(VM&amp;, const Vector&lt;ConstantIdentifierSetEntry&gt;&amp; constants);
 921 
<span class="line-modified"> 922     void setConstantRegisters(const Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants, const Vector&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation, ScriptExecutable* topLevelExecutable);</span>
 923 
 924     void replaceConstant(int index, JSValue value)
 925     {
 926         ASSERT(isConstantRegisterIndex(index) &amp;&amp; static_cast&lt;size_t&gt;(index - FirstConstantRegisterIndex) &lt; m_constantRegisters.size());
 927         m_constantRegisters[index - FirstConstantRegisterIndex].set(*m_vm, this, value);
 928     }
 929 
 930     bool shouldVisitStrongly(const ConcurrentJSLocker&amp;);
<span class="line-modified"> 931     bool shouldJettisonDueToWeakReference(VM&amp;);</span>
 932     bool shouldJettisonDueToOldAge(const ConcurrentJSLocker&amp;);
 933 
 934     void propagateTransitions(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 935     void determineLiveness(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 936 
 937     void stronglyVisitStrongReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 938     void stronglyVisitWeakReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 939     void visitOSRExitTargets(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 940 
 941     unsigned numberOfNonArgumentValueProfiles() { return m_numberOfNonArgumentValueProfiles; }
 942     unsigned totalNumberOfValueProfiles() { return numberOfArgumentValueProfiles() + numberOfNonArgumentValueProfiles(); }
 943     ValueProfile* tryGetValueProfileForBytecodeOffset(int bytecodeOffset);
 944 
 945     Seconds timeSinceCreation()
 946     {
 947         return MonotonicTime::now() - m_creationTime;
 948     }
 949 
 950     void createRareDataIfNecessary()
 951     {
 952         if (!m_rareData) {
<span class="line-modified"> 953             auto rareData = makeUnique&lt;RareData&gt;();</span>
 954             WTF::storeStoreFence(); // m_catchProfiles can be touched from compiler threads.
 955             m_rareData = WTFMove(rareData);
 956         }
 957     }
 958 
 959     void insertBasicBlockBoundariesForControlFlowProfiler();
 960     void ensureCatchLivenessIsComputedForBytecodeOffsetSlow(const OpCatch&amp;, InstructionStream::Offset);
 961 
 962     int m_numCalleeLocals;
 963     int m_numVars;
 964     int m_numParameters;
 965     int m_numberOfArgumentsToSkip { 0 };
 966     unsigned m_numberOfNonArgumentValueProfiles { 0 };
 967     union {
 968         unsigned m_debuggerRequests;
 969         struct {
 970             unsigned m_hasDebuggerStatement : 1;
 971             unsigned m_steppingMode : 1;
 972             unsigned m_numBreakpoints : 30;
 973         };
 974     };
<span class="line-modified"> 975     unsigned m_bytecodeCost { 0 };</span>
 976     VirtualRegister m_scopeRegister;
 977     mutable CodeBlockHash m_hash;
 978 
 979     WriteBarrier&lt;UnlinkedCodeBlock&gt; m_unlinkedCode;
 980     WriteBarrier&lt;ScriptExecutable&gt; m_ownerExecutable;
 981     WriteBarrier&lt;ExecutableToCodeBlockEdge&gt; m_ownerEdge;
<span class="line-added"> 982     // m_vm must be a pointer (instead of a reference) because the JSCLLIntOffsetsExtractor</span>
<span class="line-added"> 983     // cannot handle it being a reference.</span>
 984     VM* m_vm;
 985 
 986     const void* m_instructionsRawPointer { nullptr };
<span class="line-modified"> 987     SentinelLinkedList&lt;LLIntCallLinkInfo, PackedRawSentinelNode&lt;LLIntCallLinkInfo&gt;&gt; m_incomingLLIntCalls;</span>
 988     StructureWatchpointMap m_llintGetByIdWatchpointMap;
 989     RefPtr&lt;JITCode&gt; m_jitCode;
 990 #if ENABLE(JIT)
 991     std::unique_ptr&lt;JITData&gt; m_jitData;
 992 #endif
 993 #if ENABLE(DFG_JIT)
 994     // This is relevant to non-DFG code blocks that serve as the profiled code block
 995     // for DFG code blocks.
 996     CompressedLazyOperandValueProfileHolder m_lazyOperandValueProfiles;
 997 #endif
 998     RefCountedArray&lt;ValueProfile&gt; m_argumentValueProfiles;
 999 
1000     // Constant Pool
1001     COMPILE_ASSERT(sizeof(Register) == sizeof(WriteBarrier&lt;Unknown&gt;), Register_must_be_same_size_as_WriteBarrier_Unknown);
1002     // TODO: This could just be a pointer to m_unlinkedCodeBlock&#39;s data, but the DFG mutates
1003     // it, so we&#39;re stuck with it for now.
1004     Vector&lt;WriteBarrier&lt;Unknown&gt;&gt; m_constantRegisters;
1005     Vector&lt;SourceCodeRepresentation&gt; m_constantsSourceCodeRepresentation;
1006     RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt; m_functionDecls;
1007     RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt; m_functionExprs;
1008 
1009     WriteBarrier&lt;CodeBlock&gt; m_alternative;
1010 
1011     BaselineExecutionCounter m_llintExecuteCounter;
1012 
1013     BaselineExecutionCounter m_jitExecuteCounter;
1014     uint32_t m_osrExitCounter;
1015 
1016     uint16_t m_optimizationDelayCounter;
1017     uint16_t m_reoptimizationRetryCounter;
1018 
1019     RefPtr&lt;MetadataTable&gt; m_metadata;
1020 
1021     MonotonicTime m_creationTime;
<span class="line-added">1022     double m_previousCounter { 0 };</span>
1023 
1024     std::unique_ptr&lt;RareData&gt; m_rareData;
1025 };
1026 
1027 inline Register&amp; ExecState::r(int index)
1028 {
1029     CodeBlock* codeBlock = this-&gt;codeBlock();
1030     if (codeBlock-&gt;isConstantRegisterIndex(index))
1031         return *reinterpret_cast&lt;Register*&gt;(&amp;codeBlock-&gt;constantRegister(index));
1032     return this[index];
1033 }
1034 
1035 inline Register&amp; ExecState::r(VirtualRegister reg)
1036 {
1037     return r(reg.offset());
1038 }
1039 
1040 inline Register&amp; ExecState::uncheckedR(int index)
1041 {
1042     RELEASE_ASSERT(index &lt; FirstConstantRegisterIndex);
1043     return this[index];
1044 }
1045 
1046 inline Register&amp; ExecState::uncheckedR(VirtualRegister reg)
1047 {
1048     return uncheckedR(reg.offset());
1049 }
1050 
1051 template &lt;typename ExecutableType&gt;
<span class="line-modified">1052 Exception* ScriptExecutable::prepareForExecution(VM&amp; vm, JSFunction* function, JSScope* scope, CodeSpecializationKind kind, CodeBlock*&amp; resultCodeBlock)</span>
1053 {
1054     if (hasJITCodeFor(kind)) {
1055         if (std::is_same&lt;ExecutableType, EvalExecutable&gt;::value)
1056             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;EvalExecutable*&gt;(this)-&gt;codeBlock());
1057         else if (std::is_same&lt;ExecutableType, ProgramExecutable&gt;::value)
1058             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;ProgramExecutable*&gt;(this)-&gt;codeBlock());
1059         else if (std::is_same&lt;ExecutableType, ModuleProgramExecutable&gt;::value)
1060             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;ModuleProgramExecutable*&gt;(this)-&gt;codeBlock());
1061         else if (std::is_same&lt;ExecutableType, FunctionExecutable&gt;::value)
1062             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;FunctionExecutable*&gt;(this)-&gt;codeBlockFor(kind));
1063         else
1064             RELEASE_ASSERT_NOT_REACHED();
1065         return nullptr;
1066     }
1067     return prepareForExecutionImpl(vm, function, scope, kind, resultCodeBlock);
1068 }
1069 
1070 #define CODEBLOCK_LOG_EVENT(codeBlock, summary, details) \
<span class="line-modified">1071     do { \</span>
<span class="line-added">1072         if (codeBlock) \</span>
<span class="line-added">1073             (codeBlock-&gt;vm().logEvent(codeBlock, summary, [&amp;] () { return toCString details; })); \</span>
<span class="line-added">1074     } while (0)</span>
1075 
1076 
1077 void setPrinter(Printer::PrintRecord&amp;, CodeBlock*);
1078 
1079 } // namespace JSC
1080 
1081 namespace WTF {
1082 
1083 JS_EXPORT_PRIVATE void printInternal(PrintStream&amp;, JSC::CodeBlock*);
1084 
1085 } // namespace WTF
</pre>
</td>
</tr>
</table>
<center><a href="CodeBlock.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CodeBlockInlines.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>