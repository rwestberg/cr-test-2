<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/CodeBlock.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="CallVariant.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CodeBlock.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/CodeBlock.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  91 #include &lt;wtf/CommaPrinter.h&gt;
  92 #include &lt;wtf/Forward.h&gt;
  93 #include &lt;wtf/SimpleStats.h&gt;
  94 #include &lt;wtf/StringPrintStream.h&gt;
  95 #include &lt;wtf/text/StringConcatenateNumbers.h&gt;
  96 #include &lt;wtf/text/UniquedStringImpl.h&gt;
  97 
  98 #if ENABLE(ASSEMBLER)
  99 #include &quot;RegisterAtOffsetList.h&quot;
 100 #endif
 101 
 102 #if ENABLE(DFG_JIT)
 103 #include &quot;DFGOperations.h&quot;
 104 #endif
 105 
 106 #if ENABLE(FTL_JIT)
 107 #include &quot;FTLJITCode.h&quot;
 108 #endif
 109 
 110 namespace JSC {
<span class="line-removed"> 111 namespace CodeBlockInternal {</span>
<span class="line-removed"> 112 static constexpr bool verbose = false;</span>
<span class="line-removed"> 113 } // namespace CodeBlockInternal</span>
 114 
 115 const ClassInfo CodeBlock::s_info = {
 116     &quot;CodeBlock&quot;, nullptr, nullptr, nullptr,
 117     CREATE_METHOD_TABLE(CodeBlock)
 118 };
 119 
 120 CString CodeBlock::inferredName() const
 121 {
 122     switch (codeType()) {
 123     case GlobalCode:
 124         return &quot;&lt;global&gt;&quot;;
 125     case EvalCode:
 126         return &quot;&lt;eval&gt;&quot;;
 127     case FunctionCode:
<span class="line-modified"> 128         return jsCast&lt;FunctionExecutable*&gt;(ownerExecutable())-&gt;inferredName().utf8();</span>
 129     case ModuleCode:
 130         return &quot;&lt;module&gt;&quot;;
 131     default:
 132         CRASH();
 133         return CString(&quot;&quot;, 0);
 134     }
 135 }
 136 
 137 bool CodeBlock::hasHash() const
 138 {
 139     return !!m_hash;
 140 }
 141 
 142 bool CodeBlock::isSafeToComputeHash() const
 143 {
 144     return !isCompilationThread();
 145 }
 146 
 147 CodeBlockHash CodeBlock::hash() const
 148 {
</pre>
<hr />
<pre>
 166     int delta = linkedStartOffset - unlinkedStartOffset;
 167     unsigned rangeStart = delta + unlinked-&gt;unlinkedFunctionNameStart();
 168     unsigned rangeEnd = delta + unlinked-&gt;startOffset() + unlinked-&gt;sourceLength();
 169     return toCString(
 170         &quot;function &quot;,
 171         provider-&gt;source().substring(rangeStart, rangeEnd - rangeStart).utf8());
 172 }
 173 
 174 CString CodeBlock::sourceCodeOnOneLine() const
 175 {
 176     return reduceWhitespace(sourceCodeForTools());
 177 }
 178 
 179 CString CodeBlock::hashAsStringIfPossible() const
 180 {
 181     if (hasHash() || isSafeToComputeHash())
 182         return toCString(hash());
 183     return &quot;&lt;no-hash&gt;&quot;;
 184 }
 185 
<span class="line-modified"> 186 void CodeBlock::dumpAssumingJITType(PrintStream&amp; out, JITCode::JITType jitType) const</span>
 187 {
 188     out.print(inferredName(), &quot;#&quot;, hashAsStringIfPossible());
 189     out.print(&quot;:[&quot;, RawPointer(this), &quot;-&gt;&quot;);
 190     if (!!m_alternative)
 191         out.print(RawPointer(alternative()), &quot;-&gt;&quot;);
 192     out.print(RawPointer(ownerExecutable()), &quot;, &quot;, jitType, codeType());
 193 
 194     if (codeType() == FunctionCode)
 195         out.print(specializationKind());
<span class="line-modified"> 196     out.print(&quot;, &quot;, instructionCount());</span>
<span class="line-modified"> 197     if (this-&gt;jitType() == JITCode::BaselineJIT &amp;&amp; m_shouldAlwaysBeInlined)</span>
 198         out.print(&quot; (ShouldAlwaysBeInlined)&quot;);
 199     if (ownerExecutable()-&gt;neverInline())
 200         out.print(&quot; (NeverInline)&quot;);
 201     if (ownerExecutable()-&gt;neverOptimize())
 202         out.print(&quot; (NeverOptimize)&quot;);
 203     else if (ownerExecutable()-&gt;neverFTLOptimize())
 204         out.print(&quot; (NeverFTLOptimize)&quot;);
 205     if (ownerExecutable()-&gt;didTryToEnterInLoop())
 206         out.print(&quot; (DidTryToEnterInLoop)&quot;);
 207     if (ownerExecutable()-&gt;isStrictMode())
 208         out.print(&quot; (StrictMode)&quot;);
 209     if (m_didFailJITCompilation)
 210         out.print(&quot; (JITFail)&quot;);
<span class="line-modified"> 211     if (this-&gt;jitType() == JITCode::BaselineJIT &amp;&amp; m_didFailFTLCompilation)</span>
 212         out.print(&quot; (FTLFail)&quot;);
<span class="line-modified"> 213     if (this-&gt;jitType() == JITCode::BaselineJIT &amp;&amp; m_hasBeenCompiledWithFTL)</span>
 214         out.print(&quot; (HadFTLReplacement)&quot;);
 215     out.print(&quot;]&quot;);
 216 }
 217 
 218 void CodeBlock::dump(PrintStream&amp; out) const
 219 {
 220     dumpAssumingJITType(out, jitType());
 221 }
 222 
 223 void CodeBlock::dumpSource()
 224 {
 225     dumpSource(WTF::dataFile());
 226 }
 227 
 228 void CodeBlock::dumpSource(PrintStream&amp; out)
 229 {
 230     ScriptExecutable* executable = ownerExecutable();
 231     if (executable-&gt;isFunctionExecutable()) {
 232         FunctionExecutable* functionExecutable = reinterpret_cast&lt;FunctionExecutable*&gt;(executable);
 233         StringView source = functionExecutable-&gt;source().provider()-&gt;getRange(
 234             functionExecutable-&gt;parametersStartOffset(),
<span class="line-modified"> 235             functionExecutable-&gt;typeProfilingEndOffset(*vm()) + 1); // Type profiling end offset is the character before the &#39;}&#39;.</span>
 236 
 237         out.print(&quot;function &quot;, inferredName(), source);
 238         return;
 239     }
 240     out.print(executable-&gt;source().view());
 241 }
 242 
 243 void CodeBlock::dumpBytecode()
 244 {
 245     dumpBytecode(WTF::dataFile());
 246 }
 247 
 248 void CodeBlock::dumpBytecode(PrintStream&amp; out)
 249 {
 250     ICStatusMap statusMap;
 251     getICStatusMap(statusMap);
 252     BytecodeDumper&lt;CodeBlock&gt;::dumpBlock(this, instructions(), out, statusMap);
 253 }
 254 
 255 void CodeBlock::dumpBytecode(PrintStream&amp; out, const InstructionStream::Ref&amp; it, const ICStatusMap&amp; statusMap)
</pre>
<hr />
<pre>
 268 class PutToScopeFireDetail : public FireDetail {
 269 public:
 270     PutToScopeFireDetail(CodeBlock* codeBlock, const Identifier&amp; ident)
 271         : m_codeBlock(codeBlock)
 272         , m_ident(ident)
 273     {
 274     }
 275 
 276     void dump(PrintStream&amp; out) const override
 277     {
 278         out.print(&quot;Linking put_to_scope in &quot;, FunctionExecutableDump(jsCast&lt;FunctionExecutable*&gt;(m_codeBlock-&gt;ownerExecutable())), &quot; for &quot;, m_ident);
 279     }
 280 
 281 private:
 282     CodeBlock* m_codeBlock;
 283     const Identifier&amp; m_ident;
 284 };
 285 
 286 } // anonymous namespace
 287 
<span class="line-modified"> 288 CodeBlock::CodeBlock(VM* vm, Structure* structure, CopyParsedBlockTag, CodeBlock&amp; other)</span>
<span class="line-modified"> 289     : JSCell(*vm, structure)</span>
 290     , m_globalObject(other.m_globalObject)
 291     , m_shouldAlwaysBeInlined(true)
 292 #if ENABLE(JIT)
 293     , m_capabilityLevelState(DFG::CapabilityLevelNotSet)
 294 #endif
 295     , m_didFailJITCompilation(false)
 296     , m_didFailFTLCompilation(false)
 297     , m_hasBeenCompiledWithFTL(false)
 298     , m_numCalleeLocals(other.m_numCalleeLocals)
 299     , m_numVars(other.m_numVars)
 300     , m_numberOfArgumentsToSkip(other.m_numberOfArgumentsToSkip)
 301     , m_hasDebuggerStatement(false)
 302     , m_steppingMode(SteppingModeDisabled)
 303     , m_numBreakpoints(0)
<span class="line-modified"> 304     , m_instructionCount(other.m_instructionCount)</span>
 305     , m_scopeRegister(other.m_scopeRegister)
 306     , m_hash(other.m_hash)
<span class="line-modified"> 307     , m_unlinkedCode(*other.vm(), this, other.m_unlinkedCode.get())</span>
<span class="line-modified"> 308     , m_ownerExecutable(*other.vm(), this, other.m_ownerExecutable.get())</span>
 309     , m_vm(other.m_vm)
 310     , m_instructionsRawPointer(other.m_instructionsRawPointer)
 311     , m_constantRegisters(other.m_constantRegisters)
 312     , m_constantsSourceCodeRepresentation(other.m_constantsSourceCodeRepresentation)
 313     , m_functionDecls(other.m_functionDecls)
 314     , m_functionExprs(other.m_functionExprs)
 315     , m_osrExitCounter(0)
 316     , m_optimizationDelayCounter(0)
 317     , m_reoptimizationRetryCounter(0)
 318     , m_metadata(other.m_metadata)
 319     , m_creationTime(MonotonicTime::now())
 320 {
 321     ASSERT(heap()-&gt;isDeferred());
 322     ASSERT(m_scopeRegister.isLocal());
 323 
 324     ASSERT(source().provider());
 325     setNumParameters(other.numParameters());
 326 
<span class="line-modified"> 327     vm-&gt;heap.codeBlockSet().add(this);</span>
 328 }
 329 
 330 void CodeBlock::finishCreation(VM&amp; vm, CopyParsedBlockTag, CodeBlock&amp; other)
 331 {
 332     Base::finishCreation(vm);
 333     finishCreationCommon(vm);
 334 
 335     optimizeAfterWarmUp();
 336     jitAfterWarmUp();
 337 
 338     if (other.m_rareData) {
 339         createRareDataIfNecessary();
 340 
 341         m_rareData-&gt;m_exceptionHandlers = other.m_rareData-&gt;m_exceptionHandlers;
 342         m_rareData-&gt;m_switchJumpTables = other.m_rareData-&gt;m_switchJumpTables;
 343         m_rareData-&gt;m_stringSwitchJumpTables = other.m_rareData-&gt;m_stringSwitchJumpTables;
 344     }
 345 }
 346 
<span class="line-modified"> 347 CodeBlock::CodeBlock(VM* vm, Structure* structure, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock* unlinkedCodeBlock, JSScope* scope)</span>
<span class="line-modified"> 348     : JSCell(*vm, structure)</span>
<span class="line-modified"> 349     , m_globalObject(*vm, this, scope-&gt;globalObject(*vm))</span>
 350     , m_shouldAlwaysBeInlined(true)
 351 #if ENABLE(JIT)
 352     , m_capabilityLevelState(DFG::CapabilityLevelNotSet)
 353 #endif
 354     , m_didFailJITCompilation(false)
 355     , m_didFailFTLCompilation(false)
 356     , m_hasBeenCompiledWithFTL(false)
 357     , m_numCalleeLocals(unlinkedCodeBlock-&gt;numCalleeLocals())
 358     , m_numVars(unlinkedCodeBlock-&gt;numVars())
 359     , m_hasDebuggerStatement(false)
 360     , m_steppingMode(SteppingModeDisabled)
 361     , m_numBreakpoints(0)
 362     , m_scopeRegister(unlinkedCodeBlock-&gt;scopeRegister())
<span class="line-modified"> 363     , m_unlinkedCode(*vm, this, unlinkedCodeBlock)</span>
<span class="line-modified"> 364     , m_ownerExecutable(*vm, this, ownerExecutable)</span>
<span class="line-modified"> 365     , m_vm(vm)</span>
 366     , m_instructionsRawPointer(unlinkedCodeBlock-&gt;instructions().rawPointer())
 367     , m_osrExitCounter(0)
 368     , m_optimizationDelayCounter(0)
 369     , m_reoptimizationRetryCounter(0)
 370     , m_metadata(unlinkedCodeBlock-&gt;metadata().link())
 371     , m_creationTime(MonotonicTime::now())
 372 {
 373     ASSERT(heap()-&gt;isDeferred());
 374     ASSERT(m_scopeRegister.isLocal());
 375 
 376     ASSERT(source().provider());
 377     setNumParameters(unlinkedCodeBlock-&gt;numParameters());
 378 
<span class="line-modified"> 379     vm-&gt;heap.codeBlockSet().add(this);</span>
 380 }
 381 
 382 // The main purpose of this function is to generate linked bytecode from unlinked bytecode. The process
 383 // of linking is taking an abstract representation of bytecode and tying it to a GlobalObject and scope
 384 // chain. For example, this process allows us to cache the depth of lexical environment reads that reach
 385 // outside of this CodeBlock&#39;s compilation unit. It also allows us to generate particular constants that
 386 // we can&#39;t generate during unlinked bytecode generation. This process is not allowed to generate control
 387 // flow or introduce new locals. The reason for this is we rely on liveness analysis to be the same for
 388 // all the CodeBlocks of an UnlinkedCodeBlock. We rely on this fact by caching the liveness analysis
 389 // inside UnlinkedCodeBlock.
 390 bool CodeBlock::finishCreation(VM&amp; vm, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock* unlinkedCodeBlock,
 391     JSScope* scope)
 392 {
 393     Base::finishCreation(vm);
 394     finishCreationCommon(vm);
 395 
 396     auto throwScope = DECLARE_THROW_SCOPE(vm);
 397 
<span class="line-modified"> 398     if (vm.typeProfiler() || vm.controlFlowProfiler())</span>
 399         vm.functionHasExecutedCache()-&gt;removeUnexecutedRange(ownerExecutable-&gt;sourceID(), ownerExecutable-&gt;typeProfilingStartOffset(vm), ownerExecutable-&gt;typeProfilingEndOffset(vm));
 400 
<span class="line-modified"> 401     setConstantRegisters(unlinkedCodeBlock-&gt;constantRegisters(), unlinkedCodeBlock-&gt;constantsSourceCodeRepresentation());</span>

 402     RETURN_IF_EXCEPTION(throwScope, false);
 403 
 404     for (unsigned i = 0; i &lt; LinkTimeConstantCount; i++) {
 405         LinkTimeConstant type = static_cast&lt;LinkTimeConstant&gt;(i);
 406         if (unsigned registerIndex = unlinkedCodeBlock-&gt;registerIndexForLinkTimeConstant(type))
 407             m_constantRegisters[registerIndex].set(vm, this, m_globalObject-&gt;jsCellForLinkTimeConstant(type));
 408     }
 409 
 410     // We already have the cloned symbol table for the module environment since we need to instantiate
 411     // the module environments before linking the code block. We replace the stored symbol table with the already cloned one.
 412     if (UnlinkedModuleProgramCodeBlock* unlinkedModuleProgramCodeBlock = jsDynamicCast&lt;UnlinkedModuleProgramCodeBlock*&gt;(vm, unlinkedCodeBlock)) {
 413         SymbolTable* clonedSymbolTable = jsCast&lt;ModuleProgramExecutable*&gt;(ownerExecutable)-&gt;moduleEnvironmentSymbolTable();
<span class="line-modified"> 414         if (vm.typeProfiler()) {</span>
 415             ConcurrentJSLocker locker(clonedSymbolTable-&gt;m_lock);
 416             clonedSymbolTable-&gt;prepareForTypeProfiling(locker);
 417         }
 418         replaceConstant(unlinkedModuleProgramCodeBlock-&gt;moduleEnvironmentSymbolTableConstantRegisterOffset(), clonedSymbolTable);
 419     }
 420 
<span class="line-modified"> 421     bool shouldUpdateFunctionHasExecutedCache = vm.typeProfiler() || vm.controlFlowProfiler();</span>
 422     m_functionDecls = RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt;(unlinkedCodeBlock-&gt;numberOfFunctionDecls());
 423     for (size_t count = unlinkedCodeBlock-&gt;numberOfFunctionDecls(), i = 0; i &lt; count; ++i) {
 424         UnlinkedFunctionExecutable* unlinkedExecutable = unlinkedCodeBlock-&gt;functionDecl(i);
 425         if (shouldUpdateFunctionHasExecutedCache)
 426             vm.functionHasExecutedCache()-&gt;insertUnexecutedRange(ownerExecutable-&gt;sourceID(), unlinkedExecutable-&gt;typeProfilingStartOffset(), unlinkedExecutable-&gt;typeProfilingEndOffset());
<span class="line-modified"> 427         m_functionDecls[i].set(vm, this, unlinkedExecutable-&gt;link(vm, ownerExecutable-&gt;source()));</span>
 428     }
 429 
 430     m_functionExprs = RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt;(unlinkedCodeBlock-&gt;numberOfFunctionExprs());
 431     for (size_t count = unlinkedCodeBlock-&gt;numberOfFunctionExprs(), i = 0; i &lt; count; ++i) {
 432         UnlinkedFunctionExecutable* unlinkedExecutable = unlinkedCodeBlock-&gt;functionExpr(i);
 433         if (shouldUpdateFunctionHasExecutedCache)
 434             vm.functionHasExecutedCache()-&gt;insertUnexecutedRange(ownerExecutable-&gt;sourceID(), unlinkedExecutable-&gt;typeProfilingStartOffset(), unlinkedExecutable-&gt;typeProfilingEndOffset());
<span class="line-modified"> 435         m_functionExprs[i].set(vm, this, unlinkedExecutable-&gt;link(vm, ownerExecutable-&gt;source()));</span>
 436     }
 437 
 438     if (unlinkedCodeBlock-&gt;hasRareData()) {
 439         createRareDataIfNecessary();
 440 
 441         setConstantIdentifierSetRegisters(vm, unlinkedCodeBlock-&gt;constantIdentifierSets());
 442         RETURN_IF_EXCEPTION(throwScope, false);
 443 
 444         if (size_t count = unlinkedCodeBlock-&gt;numberOfExceptionHandlers()) {
 445             m_rareData-&gt;m_exceptionHandlers.resizeToFit(count);
 446             for (size_t i = 0; i &lt; count; i++) {
 447                 const UnlinkedHandlerInfo&amp; unlinkedHandler = unlinkedCodeBlock-&gt;exceptionHandler(i);
 448                 HandlerInfo&amp; handler = m_rareData-&gt;m_exceptionHandlers[i];
 449 #if ENABLE(JIT)
<span class="line-modified"> 450                 MacroAssemblerCodePtr&lt;BytecodePtrTag&gt; codePtr = instructions().at(unlinkedHandler.target)-&gt;isWide()</span>
<span class="line-modified"> 451                     ? LLInt::getWideCodePtr&lt;BytecodePtrTag&gt;(op_catch)</span>
<span class="line-modified"> 452                     : LLInt::getCodePtr&lt;BytecodePtrTag&gt;(op_catch);</span>





 453                 handler.initialize(unlinkedHandler, CodeLocationLabel&lt;ExceptionHandlerPtrTag&gt;(codePtr.retagged&lt;ExceptionHandlerPtrTag&gt;()));
 454 #else
 455                 handler.initialize(unlinkedHandler);
 456 #endif
 457             }
 458         }
 459 
 460         if (size_t count = unlinkedCodeBlock-&gt;numberOfStringSwitchJumpTables()) {
 461             m_rareData-&gt;m_stringSwitchJumpTables.grow(count);
 462             for (size_t i = 0; i &lt; count; i++) {
 463                 UnlinkedStringJumpTable::StringOffsetTable::iterator ptr = unlinkedCodeBlock-&gt;stringSwitchJumpTable(i).offsetTable.begin();
 464                 UnlinkedStringJumpTable::StringOffsetTable::iterator end = unlinkedCodeBlock-&gt;stringSwitchJumpTable(i).offsetTable.end();
 465                 for (; ptr != end; ++ptr) {
 466                     OffsetLocation offset;
 467                     offset.branchOffset = ptr-&gt;value.branchOffset;
 468                     m_rareData-&gt;m_stringSwitchJumpTables[i].offsetTable.add(ptr-&gt;key, offset);
 469                 }
 470             }
 471         }
 472 
 473         if (size_t count = unlinkedCodeBlock-&gt;numberOfSwitchJumpTables()) {
 474             m_rareData-&gt;m_switchJumpTables.grow(count);
 475             for (size_t i = 0; i &lt; count; i++) {
 476                 UnlinkedSimpleJumpTable&amp; sourceTable = unlinkedCodeBlock-&gt;switchJumpTable(i);
 477                 SimpleJumpTable&amp; destTable = m_rareData-&gt;m_switchJumpTables[i];
 478                 destTable.branchOffsets = sourceTable.branchOffsets;
 479                 destTable.min = sourceTable.min;
 480             }
 481         }
 482     }
 483 
 484     // Bookkeep the strongly referenced module environments.
 485     HashSet&lt;JSModuleEnvironment*&gt; stronglyReferencedModuleEnvironments;
 486 
<span class="line-modified"> 487     auto link_profile = [&amp;](const auto&amp; instruction, auto /*bytecode*/, auto&amp; metadata) {</span>
 488         m_numberOfNonArgumentValueProfiles++;
<span class="line-removed"> 489         metadata.m_profile.m_bytecodeOffset = instruction.offset();</span>
<span class="line-removed"> 490     };</span>
<span class="line-removed"> 491 </span>
<span class="line-removed"> 492     auto link_arrayProfile = [&amp;](const auto&amp; instruction, auto /*bytecode*/, auto&amp; metadata) {</span>
<span class="line-removed"> 493         metadata.m_arrayProfile.m_bytecodeOffset = instruction.offset();</span>
 494     };
 495 
 496     auto link_objectAllocationProfile = [&amp;](const auto&amp; /*instruction*/, auto bytecode, auto&amp; metadata) {
 497         metadata.m_objectAllocationProfile.initializeProfile(vm, m_globalObject.get(), this, m_globalObject-&gt;objectPrototype(), bytecode.m_inlineCapacity);
 498     };
 499 
 500     auto link_arrayAllocationProfile = [&amp;](const auto&amp; /*instruction*/, auto bytecode, auto&amp; metadata) {
 501         metadata.m_arrayAllocationProfile.initializeIndexingMode(bytecode.m_recommendedIndexingType);
 502     };
 503 
<span class="line-removed"> 504     auto link_hitCountForLLIntCaching = [&amp;](const auto&amp; /*instruction*/, auto /*bytecode*/, auto&amp; metadata) {</span>
<span class="line-removed"> 505         metadata.m_hitCountForLLIntCaching = Options::prototypeHitCountForLLIntCaching();</span>
<span class="line-removed"> 506     };</span>
<span class="line-removed"> 507 </span>
 508 #define LINK_FIELD(__field) \
 509     WTF_LAZY_JOIN(link_, __field)(instruction, bytecode, metadata);
 510 
 511 #define INITIALIZE_METADATA(__op) \
 512     auto bytecode = instruction-&gt;as&lt;__op&gt;(); \
 513     auto&amp; metadata = bytecode.metadata(this); \
 514     new (&amp;metadata) __op::Metadata { bytecode }; \
 515 
 516 #define CASE(__op) case __op::opcodeID
 517 
 518 #define LINK(...) \
 519     CASE(WTF_LAZY_FIRST(__VA_ARGS__)): { \
 520         INITIALIZE_METADATA(WTF_LAZY_FIRST(__VA_ARGS__)) \
 521         WTF_LAZY_HAS_REST(__VA_ARGS__)({ \
 522             WTF_LAZY_FOR_EACH_TERM(LINK_FIELD,  WTF_LAZY_REST_(__VA_ARGS__)) \
 523         }) \
 524         break; \
 525     }
 526 
 527     const InstructionStream&amp; instructionStream = instructions();
 528     for (const auto&amp; instruction : instructionStream) {
 529         OpcodeID opcodeID = instruction-&gt;opcodeID();
<span class="line-modified"> 530         m_instructionCount += opcodeLengths[opcodeID];</span>
 531         switch (opcodeID) {
<span class="line-modified"> 532         LINK(OpHasIndexedProperty, arrayProfile)</span>
 533 
<span class="line-modified"> 534         LINK(OpCallVarargs, arrayProfile, profile)</span>
<span class="line-modified"> 535         LINK(OpTailCallVarargs, arrayProfile, profile)</span>
<span class="line-modified"> 536         LINK(OpTailCallForwardArguments, arrayProfile, profile)</span>
<span class="line-modified"> 537         LINK(OpConstructVarargs, arrayProfile, profile)</span>
<span class="line-modified"> 538         LINK(OpGetByVal, arrayProfile, profile)</span>
 539 
 540         LINK(OpGetDirectPname, profile)
 541         LINK(OpGetByIdWithThis, profile)
 542         LINK(OpTryGetById, profile)
 543         LINK(OpGetByIdDirect, profile)
 544         LINK(OpGetByValWithThis, profile)
 545         LINK(OpGetFromArguments, profile)
 546         LINK(OpToNumber, profile)
 547         LINK(OpToObject, profile)
 548         LINK(OpGetArgument, profile)
 549         LINK(OpToThis, profile)
 550         LINK(OpBitand, profile)
 551         LINK(OpBitor, profile)
 552         LINK(OpBitnot, profile)
 553         LINK(OpBitxor, profile)

 554 
<span class="line-modified"> 555         LINK(OpGetById, profile, hitCountForLLIntCaching)</span>
 556 
<span class="line-modified"> 557         LINK(OpCall, profile, arrayProfile)</span>
<span class="line-modified"> 558         LINK(OpTailCall, profile, arrayProfile)</span>
<span class="line-modified"> 559         LINK(OpCallEval, profile, arrayProfile)</span>
<span class="line-modified"> 560         LINK(OpConstruct, profile, arrayProfile)</span>
 561 
<span class="line-modified"> 562         LINK(OpInByVal, arrayProfile)</span>
<span class="line-modified"> 563         LINK(OpPutByVal, arrayProfile)</span>
<span class="line-modified"> 564         LINK(OpPutByValDirect, arrayProfile)</span>
 565 
 566         LINK(OpNewArray)
 567         LINK(OpNewArrayWithSize)
 568         LINK(OpNewArrayBuffer, arrayAllocationProfile)
 569 
 570         LINK(OpNewObject, objectAllocationProfile)
 571 
 572         LINK(OpPutById)
 573         LINK(OpCreateThis)
 574 
 575         LINK(OpAdd)
 576         LINK(OpMul)
 577         LINK(OpDiv)
 578         LINK(OpSub)
 579 
 580         LINK(OpNegate)
 581 
 582         LINK(OpJneqPtr)
 583 
 584         LINK(OpCatch)
</pre>
<hr />
<pre>
 591             RELEASE_ASSERT(bytecode.m_resolveType != LocalClosureVar);
 592 
 593             ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), bytecode.m_localScopeDepth, scope, ident, Get, bytecode.m_resolveType, InitializationMode::NotInitialization);
 594             RETURN_IF_EXCEPTION(throwScope, false);
 595 
 596             metadata.m_resolveType = op.type;
 597             metadata.m_localScopeDepth = op.depth;
 598             if (op.lexicalEnvironment) {
 599                 if (op.type == ModuleVar) {
 600                     // Keep the linked module environment strongly referenced.
 601                     if (stronglyReferencedModuleEnvironments.add(jsCast&lt;JSModuleEnvironment*&gt;(op.lexicalEnvironment)).isNewEntry)
 602                         addConstant(op.lexicalEnvironment);
 603                     metadata.m_lexicalEnvironment.set(vm, this, op.lexicalEnvironment);
 604                 } else
 605                     metadata.m_symbolTable.set(vm, this, op.lexicalEnvironment-&gt;symbolTable());
 606             } else if (JSScope* constantScope = JSScope::constantScopeForCodeBlock(op.type, this)) {
 607                 metadata.m_constantScope.set(vm, this, constantScope);
 608                 if (op.type == GlobalProperty || op.type == GlobalPropertyWithVarInjectionChecks)
 609                     metadata.m_globalLexicalBindingEpoch = m_globalObject-&gt;globalLexicalBindingEpoch();
 610             } else
<span class="line-modified"> 611                 metadata.m_globalObject = nullptr;</span>
 612             break;
 613         }
 614 
 615         case op_get_from_scope: {
 616             INITIALIZE_METADATA(OpGetFromScope)
 617 
 618             link_profile(instruction, bytecode, metadata);
 619             metadata.m_watchpointSet = nullptr;
 620 
 621             ASSERT(!isInitialization(bytecode.m_getPutInfo.initializationMode()));
 622             if (bytecode.m_getPutInfo.resolveType() == LocalClosureVar) {
 623                 metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), ClosureVar, bytecode.m_getPutInfo.initializationMode());
 624                 break;
 625             }
 626 
 627             const Identifier&amp; ident = identifier(bytecode.m_var);
 628             ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), bytecode.m_localScopeDepth, scope, ident, Get, bytecode.m_getPutInfo.resolveType(), InitializationMode::NotInitialization);
 629             RETURN_IF_EXCEPTION(throwScope, false);
 630 
 631             metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), op.type, bytecode.m_getPutInfo.initializationMode());
 632             if (op.type == ModuleVar)
 633                 metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), ClosureVar, bytecode.m_getPutInfo.initializationMode());
 634             if (op.type == GlobalVar || op.type == GlobalVarWithVarInjectionChecks || op.type == GlobalLexicalVar || op.type == GlobalLexicalVarWithVarInjectionChecks)
 635                 metadata.m_watchpointSet = op.watchpointSet;
 636             else if (op.structure)
 637                 metadata.m_structure.set(vm, this, op.structure);
 638             metadata.m_operand = op.operand;
 639             break;
 640         }
 641 
 642         case op_put_to_scope: {
 643             INITIALIZE_METADATA(OpPutToScope)
 644 
 645             if (bytecode.m_getPutInfo.resolveType() == LocalClosureVar) {
 646                 // Only do watching if the property we&#39;re putting to is not anonymous.
 647                 if (bytecode.m_var != UINT_MAX) {
<span class="line-modified"> 648                     SymbolTable* symbolTable = jsCast&lt;SymbolTable*&gt;(getConstant(bytecode.m_symbolTableOrScopeDepth));</span>
 649                     const Identifier&amp; ident = identifier(bytecode.m_var);
 650                     ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 651                     auto iter = symbolTable-&gt;find(locker, ident.impl());
 652                     ASSERT(iter != symbolTable-&gt;end(locker));
 653                     iter-&gt;value.prepareToWatch();
 654                     metadata.m_watchpointSet = iter-&gt;value.watchpointSet();
 655                 } else
 656                     metadata.m_watchpointSet = nullptr;
 657                 break;
 658             }
 659 
 660             const Identifier&amp; ident = identifier(bytecode.m_var);
 661             metadata.m_watchpointSet = nullptr;
<span class="line-modified"> 662             ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), bytecode.m_symbolTableOrScopeDepth, scope, ident, Put, bytecode.m_getPutInfo.resolveType(), bytecode.m_getPutInfo.initializationMode());</span>
 663             RETURN_IF_EXCEPTION(throwScope, false);
 664 
 665             metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), op.type, bytecode.m_getPutInfo.initializationMode());
 666             if (op.type == GlobalVar || op.type == GlobalVarWithVarInjectionChecks || op.type == GlobalLexicalVar || op.type == GlobalLexicalVarWithVarInjectionChecks)
 667                 metadata.m_watchpointSet = op.watchpointSet;
 668             else if (op.type == ClosureVar || op.type == ClosureVarWithVarInjectionChecks) {
 669                 if (op.watchpointSet)
 670                     op.watchpointSet-&gt;invalidate(vm, PutToScopeFireDetail(this, ident));
 671             } else if (op.structure)
 672                 metadata.m_structure.set(vm, this, op.structure);
 673             metadata.m_operand = op.operand;
 674             break;
 675         }
 676 
 677         case op_profile_type: {
<span class="line-modified"> 678             RELEASE_ASSERT(vm.typeProfiler());</span>
 679 
 680             INITIALIZE_METADATA(OpProfileType)
 681 
 682             size_t instructionOffset = instruction.offset() + instruction-&gt;size() - 1;
 683             unsigned divotStart, divotEnd;
 684             GlobalVariableID globalVariableID = 0;
 685             RefPtr&lt;TypeSet&gt; globalTypeSet;
 686             bool shouldAnalyze = m_unlinkedCode-&gt;typeProfilerExpressionInfoForBytecodeOffset(instructionOffset, divotStart, divotEnd);
 687             SymbolTable* symbolTable = nullptr;
 688 
 689             switch (bytecode.m_flag) {
 690             case ProfileTypeBytecodeClosureVar: {
 691                 const Identifier&amp; ident = identifier(bytecode.m_identifier);
<span class="line-modified"> 692                 unsigned localScopeDepth = bytecode.m_symbolTableOrScopeDepth;</span>
 693                 // Even though type profiling may be profiling either a Get or a Put, we can always claim a Get because
 694                 // we&#39;re abstractly &quot;read&quot;ing from a JSScope.
 695                 ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), localScopeDepth, scope, ident, Get, bytecode.m_resolveType, InitializationMode::NotInitialization);
 696                 RETURN_IF_EXCEPTION(throwScope, false);
 697 
 698                 if (op.type == ClosureVar || op.type == ModuleVar)
 699                     symbolTable = op.lexicalEnvironment-&gt;symbolTable();
 700                 else if (op.type == GlobalVar)
 701                     symbolTable = m_globalObject.get()-&gt;symbolTable();
 702 
 703                 UniquedStringImpl* impl = (op.type == ModuleVar) ? op.importedName.get() : ident.impl();
 704                 if (symbolTable) {
 705                     ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 706                     // If our parent scope was created while profiling was disabled, it will not have prepared for profiling yet.
 707                     symbolTable-&gt;prepareForTypeProfiling(locker);
 708                     globalVariableID = symbolTable-&gt;uniqueIDForVariable(locker, impl, vm);
 709                     globalTypeSet = symbolTable-&gt;globalTypeSetForVariable(locker, impl, vm);
 710                 } else
 711                     globalVariableID = TypeProfilerNoGlobalIDExists;
 712 
 713                 break;
 714             }
 715             case ProfileTypeBytecodeLocallyResolved: {
<span class="line-modified"> 716                 int symbolTableIndex = bytecode.m_symbolTableOrScopeDepth;</span>
 717                 SymbolTable* symbolTable = jsCast&lt;SymbolTable*&gt;(getConstant(symbolTableIndex));
 718                 const Identifier&amp; ident = identifier(bytecode.m_identifier);
 719                 ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 720                 // If our parent scope was created while profiling was disabled, it will not have prepared for profiling yet.
 721                 globalVariableID = symbolTable-&gt;uniqueIDForVariable(locker, ident.impl(), vm);
 722                 globalTypeSet = symbolTable-&gt;globalTypeSetForVariable(locker, ident.impl(), vm);
 723 
 724                 break;
 725             }
 726             case ProfileTypeBytecodeDoesNotHaveGlobalID:
 727             case ProfileTypeBytecodeFunctionArgument: {
 728                 globalVariableID = TypeProfilerNoGlobalIDExists;
 729                 break;
 730             }
 731             case ProfileTypeBytecodeFunctionReturnStatement: {
 732                 RELEASE_ASSERT(ownerExecutable-&gt;isFunctionExecutable());
 733                 globalTypeSet = jsCast&lt;FunctionExecutable*&gt;(ownerExecutable)-&gt;returnStatementTypeSet();
 734                 globalVariableID = TypeProfilerReturnStatement;
 735                 if (!shouldAnalyze) {
 736                     // Because a return statement can be added implicitly to return undefined at the end of a function,
</pre>
<hr />
<pre>
 767         }
 768 
 769         case op_create_rest: {
 770             int numberOfArgumentsToSkip = instruction-&gt;as&lt;OpCreateRest&gt;().m_numParametersToSkip;
 771             ASSERT_UNUSED(numberOfArgumentsToSkip, numberOfArgumentsToSkip &gt;= 0);
 772             // This is used when rematerializing the rest parameter during OSR exit in the FTL JIT.&quot;);
 773             m_numberOfArgumentsToSkip = numberOfArgumentsToSkip;
 774             break;
 775         }
 776 
 777         default:
 778             break;
 779         }
 780     }
 781 
 782 #undef CASE
 783 #undef INITIALIZE_METADATA
 784 #undef LINK_FIELD
 785 #undef LINK
 786 
<span class="line-modified"> 787     if (vm.controlFlowProfiler())</span>
 788         insertBasicBlockBoundariesForControlFlowProfiler();
 789 
 790     // Set optimization thresholds only after instructions is initialized, since these
 791     // rely on the instruction count (and are in theory permitted to also inspect the
 792     // instruction stream to more accurate assess the cost of tier-up).
 793     optimizeAfterWarmUp();
 794     jitAfterWarmUp();
 795 
 796     // If the concurrent thread will want the code block&#39;s hash, then compute it here
 797     // synchronously.
 798     if (Options::alwaysComputeHash())
 799         hash();
 800 
 801     if (Options::dumpGeneratedBytecodes())
 802         dumpBytecode();
 803 
 804     if (m_metadata)
 805         vm.heap.reportExtraMemoryAllocated(m_metadata-&gt;sizeInBytes());
 806 
 807     return true;
 808 }
 809 
 810 void CodeBlock::finishCreationCommon(VM&amp; vm)
 811 {
 812     m_ownerEdge.set(vm, this, ExecutableToCodeBlockEdge::create(vm, this));
 813 }
 814 
 815 CodeBlock::~CodeBlock()
 816 {
 817     VM&amp; vm = *m_vm;
 818 




















 819     vm.heap.codeBlockSet().remove(this);
 820 
 821     if (UNLIKELY(vm.m_perBytecodeProfiler))
 822         vm.m_perBytecodeProfiler-&gt;notifyDestruction(this);
 823 
 824     if (!vm.heap.isShuttingDown() &amp;&amp; unlinkedCodeBlock()-&gt;didOptimize() == MixedTriState)
 825         unlinkedCodeBlock()-&gt;setDidOptimize(FalseTriState);
 826 
 827 #if ENABLE(VERBOSE_VALUE_PROFILE)
 828     dumpValueProfiles();
 829 #endif
 830 
 831     // We may be destroyed before any CodeBlocks that refer to us are destroyed.
 832     // Consider that two CodeBlocks become unreachable at the same time. There
 833     // is no guarantee about the order in which the CodeBlocks are destroyed.
 834     // So, if we don&#39;t remove incoming calls, and get destroyed before the
 835     // CodeBlock(s) that have calls into us, then the CallLinkInfo vector&#39;s
 836     // destructor will try to remove nodes from our (no longer valid) linked list.
 837     unlinkIncomingCalls();
 838 
</pre>
<hr />
<pre>
 848         }
 849     }
 850 #endif // ENABLE(JIT)
 851 }
 852 
 853 void CodeBlock::setConstantIdentifierSetRegisters(VM&amp; vm, const Vector&lt;ConstantIdentifierSetEntry&gt;&amp; constants)
 854 {
 855     auto scope = DECLARE_THROW_SCOPE(vm);
 856     JSGlobalObject* globalObject = m_globalObject.get();
 857     ExecState* exec = globalObject-&gt;globalExec();
 858 
 859     for (const auto&amp; entry : constants) {
 860         const IdentifierSet&amp; set = entry.first;
 861 
 862         Structure* setStructure = globalObject-&gt;setStructure();
 863         RETURN_IF_EXCEPTION(scope, void());
 864         JSSet* jsSet = JSSet::create(exec, vm, setStructure, set.size());
 865         RETURN_IF_EXCEPTION(scope, void());
 866 
 867         for (auto setEntry : set) {
<span class="line-modified"> 868             JSString* jsString = jsOwnedString(&amp;vm, setEntry.get());</span>
 869             jsSet-&gt;add(exec, jsString);
 870             RETURN_IF_EXCEPTION(scope, void());
 871         }
 872         m_constantRegisters[entry.second].set(vm, this, jsSet);
 873     }
 874 }
 875 
<span class="line-modified"> 876 void CodeBlock::setConstantRegisters(const Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants, const Vector&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation)</span>
 877 {
 878     VM&amp; vm = *m_vm;
 879     auto scope = DECLARE_THROW_SCOPE(vm);
 880     JSGlobalObject* globalObject = m_globalObject.get();
 881     ExecState* exec = globalObject-&gt;globalExec();
 882 
 883     ASSERT(constants.size() == constantsSourceCodeRepresentation.size());
 884     size_t count = constants.size();
 885     m_constantRegisters.resizeToFit(count);
<span class="line-removed"> 886     bool hasTypeProfiler = !!vm.typeProfiler();</span>
 887     for (size_t i = 0; i &lt; count; i++) {
 888         JSValue constant = constants[i].get();
 889 
 890         if (!constant.isEmpty()) {
 891             if (constant.isCell()) {
 892                 JSCell* cell = constant.asCell();
 893                 if (SymbolTable* symbolTable = jsDynamicCast&lt;SymbolTable*&gt;(vm, cell)) {
<span class="line-modified"> 894                     if (hasTypeProfiler) {</span>
 895                         ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 896                         symbolTable-&gt;prepareForTypeProfiling(locker);
 897                     }
 898 
 899                     SymbolTable* clone = symbolTable-&gt;cloneScopePart(vm);
 900                     if (wasCompiledWithDebuggingOpcodes())
 901                         clone-&gt;setRareDataCodeBlock(this);
 902 
 903                     constant = clone;
 904                 } else if (auto* descriptor = jsDynamicCast&lt;JSTemplateObjectDescriptor*&gt;(vm, cell)) {
<span class="line-modified"> 905                     auto* templateObject = descriptor-&gt;createTemplateObject(exec);</span>
 906                     RETURN_IF_EXCEPTION(scope, void());
 907                     constant = templateObject;
 908                 }
 909             }
 910         }
 911 
 912         m_constantRegisters[i].set(vm, this, constant);
 913     }
 914 
 915     m_constantsSourceCodeRepresentation = constantsSourceCodeRepresentation;
 916 }
 917 
 918 void CodeBlock::setAlternative(VM&amp; vm, CodeBlock* alternative)
 919 {
 920     RELEASE_ASSERT(alternative);
 921     RELEASE_ASSERT(alternative-&gt;jitCode());
 922     m_alternative.set(vm, this, alternative);
 923 }
 924 
 925 void CodeBlock::setNumParameters(int newValue)
 926 {
 927     m_numParameters = newValue;
 928 
<span class="line-modified"> 929     m_argumentValueProfiles = RefCountedArray&lt;ValueProfile&gt;(vm()-&gt;canUseJIT() ? newValue : 0);</span>
 930 }
 931 
 932 CodeBlock* CodeBlock::specialOSREntryBlockOrNull()
 933 {
 934 #if ENABLE(FTL_JIT)
<span class="line-modified"> 935     if (jitType() != JITCode::DFGJIT)</span>
 936         return 0;
 937     DFG::JITCode* jitCode = m_jitCode-&gt;dfg();
 938     return jitCode-&gt;osrEntryBlock();
 939 #else // ENABLE(FTL_JIT)
 940     return 0;
 941 #endif // ENABLE(FTL_JIT)
 942 }
 943 
 944 size_t CodeBlock::estimatedSize(JSCell* cell, VM&amp; vm)
 945 {
 946     CodeBlock* thisObject = jsCast&lt;CodeBlock*&gt;(cell);
 947     size_t extraMemoryAllocated = 0;
 948     if (thisObject-&gt;m_metadata)
 949         extraMemoryAllocated += thisObject-&gt;m_metadata-&gt;sizeInBytes();
<span class="line-modified"> 950     if (thisObject-&gt;m_jitCode)</span>
<span class="line-modified"> 951         extraMemoryAllocated += thisObject-&gt;m_jitCode-&gt;size();</span>

 952     return Base::estimatedSize(cell, vm) + extraMemoryAllocated;
 953 }
 954 
 955 void CodeBlock::visitChildren(JSCell* cell, SlotVisitor&amp; visitor)
 956 {
 957     CodeBlock* thisObject = jsCast&lt;CodeBlock*&gt;(cell);
 958     ASSERT_GC_OBJECT_INHERITS(thisObject, info());
 959     Base::visitChildren(cell, visitor);
 960     visitor.append(thisObject-&gt;m_ownerEdge);
 961     thisObject-&gt;visitChildren(visitor);
 962 }
 963 
 964 void CodeBlock::visitChildren(SlotVisitor&amp; visitor)
 965 {
 966     ConcurrentJSLocker locker(m_lock);
 967     if (CodeBlock* otherBlock = specialOSREntryBlockOrNull())
 968         visitor.appendUnbarriered(otherBlock);
 969 
 970     size_t extraMemory = 0;
 971     if (m_metadata)
 972         extraMemory += m_metadata-&gt;sizeInBytes();
<span class="line-modified"> 973     if (m_jitCode)</span>
 974         extraMemory += m_jitCode-&gt;size();
 975     visitor.reportExtraMemoryVisited(extraMemory);
 976 
 977     stronglyVisitStrongReferences(locker, visitor);
 978     stronglyVisitWeakReferences(locker, visitor);
 979 
 980     VM::SpaceAndSet::setFor(*subspace()).add(this);
 981 }
 982 
 983 bool CodeBlock::shouldVisitStrongly(const ConcurrentJSLocker&amp; locker)
 984 {
 985     if (Options::forceCodeBlockLiveness())
 986         return true;
 987 
 988     if (shouldJettisonDueToOldAge(locker))
 989         return false;
 990 
 991     // Interpreter and Baseline JIT CodeBlocks don&#39;t need to be jettisoned when
 992     // their weak references go stale. So if a basline JIT CodeBlock gets
 993     // scanned, we can assume that this means that it&#39;s live.
 994     if (!JITCode::isOptimizingJIT(jitType()))
 995         return true;
 996 
 997     return false;
 998 }
 999 
<span class="line-modified">1000 bool CodeBlock::shouldJettisonDueToWeakReference()</span>
1001 {
1002     if (!JITCode::isOptimizingJIT(jitType()))
1003         return false;
<span class="line-modified">1004     return !Heap::isMarked(this);</span>
1005 }
1006 
<span class="line-modified">1007 static Seconds timeToLive(JITCode::JITType jitType)</span>
1008 {
1009     if (UNLIKELY(Options::useEagerCodeBlockJettisonTiming())) {
1010         switch (jitType) {
<span class="line-modified">1011         case JITCode::InterpreterThunk:</span>
1012             return 10_ms;
<span class="line-modified">1013         case JITCode::BaselineJIT:</span>
1014             return 30_ms;
<span class="line-modified">1015         case JITCode::DFGJIT:</span>
1016             return 40_ms;
<span class="line-modified">1017         case JITCode::FTLJIT:</span>
1018             return 120_ms;
1019         default:
1020             return Seconds::infinity();
1021         }
1022     }
1023 
1024     switch (jitType) {
<span class="line-modified">1025     case JITCode::InterpreterThunk:</span>
1026         return 5_s;
<span class="line-modified">1027     case JITCode::BaselineJIT:</span>
1028         // Effectively 10 additional seconds, since BaselineJIT and
1029         // InterpreterThunk share a CodeBlock.
1030         return 15_s;
<span class="line-modified">1031     case JITCode::DFGJIT:</span>
1032         return 20_s;
<span class="line-modified">1033     case JITCode::FTLJIT:</span>
1034         return 60_s;
1035     default:
1036         return Seconds::infinity();
1037     }
1038 }
1039 
1040 bool CodeBlock::shouldJettisonDueToOldAge(const ConcurrentJSLocker&amp;)
1041 {
<span class="line-modified">1042     if (Heap::isMarked(this))</span>
1043         return false;
1044 
1045     if (UNLIKELY(Options::forceCodeBlockToJettisonDueToOldAge()))
1046         return true;
1047 
1048     if (timeSinceCreation() &lt; timeToLive(jitType()))
1049         return false;
1050 
1051     return true;
1052 }
1053 
1054 #if ENABLE(DFG_JIT)
<span class="line-modified">1055 static bool shouldMarkTransition(DFG::WeakReferenceTransition&amp; transition)</span>
1056 {
<span class="line-modified">1057     if (transition.m_codeOrigin &amp;&amp; !Heap::isMarked(transition.m_codeOrigin.get()))</span>
1058         return false;
1059 
<span class="line-modified">1060     if (!Heap::isMarked(transition.m_from.get()))</span>
1061         return false;
1062 
1063     return true;
1064 }
1065 #endif // ENABLE(DFG_JIT)
1066 
1067 void CodeBlock::propagateTransitions(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1068 {
1069     UNUSED_PARAM(visitor);
1070 
1071     VM&amp; vm = *m_vm;
1072 
<span class="line-modified">1073     if (jitType() == JITCode::InterpreterThunk) {</span>
1074         const Vector&lt;InstructionStream::Offset&gt;&amp; propertyAccessInstructions = m_unlinkedCode-&gt;propertyAccessInstructions();
1075         const InstructionStream&amp; instructionStream = instructions();
1076         for (size_t i = 0; i &lt; propertyAccessInstructions.size(); ++i) {
1077             auto instruction = instructionStream.at(propertyAccessInstructions[i]);
1078             if (instruction-&gt;is&lt;OpPutById&gt;()) {
1079                 auto&amp; metadata = instruction-&gt;as&lt;OpPutById&gt;().metadata(this);
1080                 StructureID oldStructureID = metadata.m_oldStructureID;
1081                 StructureID newStructureID = metadata.m_newStructureID;
1082                 if (!oldStructureID || !newStructureID)
1083                     continue;
1084                 Structure* oldStructure =
1085                     vm.heap.structureIDTable().get(oldStructureID);
1086                 Structure* newStructure =
1087                     vm.heap.structureIDTable().get(newStructureID);
<span class="line-modified">1088                 if (Heap::isMarked(oldStructure))</span>
1089                     visitor.appendUnbarriered(newStructure);
1090                 continue;
1091             }
1092         }
1093     }
1094 
1095 #if ENABLE(JIT)
1096     if (JITCode::isJIT(jitType())) {
1097         if (auto* jitData = m_jitData.get()) {
1098             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1099                 stubInfo-&gt;propagateTransitions(visitor);
1100         }
1101     }
1102 #endif // ENABLE(JIT)
1103 
1104 #if ENABLE(DFG_JIT)
1105     if (JITCode::isOptimizingJIT(jitType())) {
1106         DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1107 
1108         dfgCommon-&gt;recordedStatuses.markIfCheap(visitor);
1109 
1110         for (auto&amp; weakReference : dfgCommon-&gt;weakStructureReferences)
1111             weakReference-&gt;markIfCheap(visitor);
1112 
1113         for (auto&amp; transition : dfgCommon-&gt;transitions) {
<span class="line-modified">1114             if (shouldMarkTransition(transition)) {</span>
1115                 // If the following three things are live, then the target of the
1116                 // transition is also live:
1117                 //
1118                 // - This code block. We know it&#39;s live already because otherwise
1119                 //   we wouldn&#39;t be scanning ourselves.
1120                 //
1121                 // - The code origin of the transition. Transitions may arise from
1122                 //   code that was inlined. They are not relevant if the user&#39;s
1123                 //   object that is required for the inlinee to run is no longer
1124                 //   live.
1125                 //
1126                 // - The source of the transition. The transition checks if some
1127                 //   heap location holds the source, and if so, stores the target.
1128                 //   Hence the source must be live for the transition to be live.
1129                 //
1130                 // We also short-circuit the liveness if the structure is harmless
1131                 // to mark (i.e. its global object and prototype are both already
1132                 // live).
1133 
1134                 visitor.append(transition.m_to);
1135             }
1136         }
1137     }
1138 #endif // ENABLE(DFG_JIT)
1139 }
1140 
1141 void CodeBlock::determineLiveness(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1142 {
1143     UNUSED_PARAM(visitor);
1144 
1145 #if ENABLE(DFG_JIT)
<span class="line-modified">1146     if (Heap::isMarked(this))</span>

1147         return;
1148 
1149     // In rare and weird cases, this could be called on a baseline CodeBlock. One that I found was
1150     // that we might decide that the CodeBlock should be jettisoned due to old age, so the
1151     // isMarked check doesn&#39;t protect us.
1152     if (!JITCode::isOptimizingJIT(jitType()))
1153         return;
1154 
1155     DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1156     // Now check all of our weak references. If all of them are live, then we
1157     // have proved liveness and so we scan our strong references. If at end of
1158     // GC we still have not proved liveness, then this code block is toast.
1159     bool allAreLiveSoFar = true;
1160     for (unsigned i = 0; i &lt; dfgCommon-&gt;weakReferences.size(); ++i) {
1161         JSCell* reference = dfgCommon-&gt;weakReferences[i].get();
<span class="line-modified">1162         ASSERT(!jsDynamicCast&lt;CodeBlock*&gt;(*reference-&gt;vm(), reference));</span>
<span class="line-modified">1163         if (!Heap::isMarked(reference)) {</span>
1164             allAreLiveSoFar = false;
1165             break;
1166         }
1167     }
1168     if (allAreLiveSoFar) {
1169         for (unsigned i = 0; i &lt; dfgCommon-&gt;weakStructureReferences.size(); ++i) {
<span class="line-modified">1170             if (!Heap::isMarked(dfgCommon-&gt;weakStructureReferences[i].get())) {</span>
1171                 allAreLiveSoFar = false;
1172                 break;
1173             }
1174         }
1175     }
1176 
1177     // If some weak references are dead, then this fixpoint iteration was
1178     // unsuccessful.
1179     if (!allAreLiveSoFar)
1180         return;
1181 
1182     // All weak references are live. Record this information so we don&#39;t
1183     // come back here again, and scan the strong references.
1184     visitor.appendUnbarriered(this);
1185 #endif // ENABLE(DFG_JIT)
1186 }
1187 
1188 void CodeBlock::finalizeLLIntInlineCaches()
1189 {
1190     VM&amp; vm = *m_vm;
1191     const Vector&lt;InstructionStream::Offset&gt;&amp; propertyAccessInstructions = m_unlinkedCode-&gt;propertyAccessInstructions();
1192 
<span class="line-modified">1193     auto handleGetPutFromScope = [](auto&amp; metadata) {</span>
1194         GetPutInfo getPutInfo = metadata.m_getPutInfo;
1195         if (getPutInfo.resolveType() == GlobalVar || getPutInfo.resolveType() == GlobalVarWithVarInjectionChecks
1196             || getPutInfo.resolveType() == LocalClosureVar || getPutInfo.resolveType() == GlobalLexicalVar || getPutInfo.resolveType() == GlobalLexicalVarWithVarInjectionChecks)
1197             return;
1198         WriteBarrierBase&lt;Structure&gt;&amp; structure = metadata.m_structure;
<span class="line-modified">1199         if (!structure || Heap::isMarked(structure.get()))</span>
1200             return;
1201         if (Options::verboseOSR())
1202             dataLogF(&quot;Clearing scope access with structure %p.\n&quot;, structure.get());
1203         structure.clear();
1204     };
1205 
1206     const InstructionStream&amp; instructionStream = instructions();
1207     for (size_t size = propertyAccessInstructions.size(), i = 0; i &lt; size; ++i) {
1208         const auto curInstruction = instructionStream.at(propertyAccessInstructions[i]);
1209         switch (curInstruction-&gt;opcodeID()) {
1210         case op_get_by_id: {
1211             auto&amp; metadata = curInstruction-&gt;as&lt;OpGetById&gt;().metadata(this);
<span class="line-modified">1212             if (metadata.m_mode != GetByIdMode::Default)</span>
1213                 break;
1214             StructureID oldStructureID = metadata.m_modeMetadata.defaultMode.structureID;
<span class="line-modified">1215             if (!oldStructureID || Heap::isMarked(vm.heap.structureIDTable().get(oldStructureID)))</span>
1216                 break;
1217             if (Options::verboseOSR())
1218                 dataLogF(&quot;Clearing LLInt property access.\n&quot;);
1219             LLIntPrototypeLoadAdaptiveStructureWatchpoint::clearLLIntGetByIdCache(metadata);
1220             break;
1221         }
1222         case op_get_by_id_direct: {
1223             auto&amp; metadata = curInstruction-&gt;as&lt;OpGetByIdDirect&gt;().metadata(this);
1224             StructureID oldStructureID = metadata.m_structureID;
<span class="line-modified">1225             if (!oldStructureID || Heap::isMarked(vm.heap.structureIDTable().get(oldStructureID)))</span>
1226                 break;
1227             if (Options::verboseOSR())
1228                 dataLogF(&quot;Clearing LLInt property access.\n&quot;);
1229             metadata.m_structureID = 0;
1230             metadata.m_offset = 0;
1231             break;
1232         }
1233         case op_put_by_id: {
1234             auto&amp; metadata = curInstruction-&gt;as&lt;OpPutById&gt;().metadata(this);
1235             StructureID oldStructureID = metadata.m_oldStructureID;
1236             StructureID newStructureID = metadata.m_newStructureID;
1237             StructureChain* chain = metadata.m_structureChain.get();
<span class="line-modified">1238             if ((!oldStructureID || Heap::isMarked(vm.heap.structureIDTable().get(oldStructureID)))</span>
<span class="line-modified">1239                 &amp;&amp; (!newStructureID || Heap::isMarked(vm.heap.structureIDTable().get(newStructureID)))</span>
<span class="line-modified">1240                 &amp;&amp; (!chain || Heap::isMarked(chain)))</span>
1241                 break;
1242             if (Options::verboseOSR())
1243                 dataLogF(&quot;Clearing LLInt put transition.\n&quot;);
1244             metadata.m_oldStructureID = 0;
1245             metadata.m_offset = 0;
1246             metadata.m_newStructureID = 0;
1247             metadata.m_structureChain.clear();
1248             break;
1249         }
1250         // FIXME: https://bugs.webkit.org/show_bug.cgi?id=166418
1251         // We need to add optimizations for op_resolve_scope_for_hoisting_func_decl_in_eval to do link time scope resolution.
1252         case op_resolve_scope_for_hoisting_func_decl_in_eval:
1253             break;
1254         case op_to_this: {
1255             auto&amp; metadata = curInstruction-&gt;as&lt;OpToThis&gt;().metadata(this);
<span class="line-modified">1256             if (!metadata.m_cachedStructure || Heap::isMarked(metadata.m_cachedStructure.get()))</span>
1257                 break;
<span class="line-modified">1258             if (Options::verboseOSR())</span>
<span class="line-modified">1259                 dataLogF(&quot;Clearing LLInt to_this with structure %p.\n&quot;, metadata.m_cachedStructure.get());</span>
<span class="line-modified">1260             metadata.m_cachedStructure.clear();</span>


1261             metadata.m_toThisStatus = merge(metadata.m_toThisStatus, ToThisClearedByGC);
1262             break;
1263         }
1264         case op_create_this: {
1265             auto&amp; metadata = curInstruction-&gt;as&lt;OpCreateThis&gt;().metadata(this);
1266             auto&amp; cacheWriteBarrier = metadata.m_cachedCallee;
1267             if (!cacheWriteBarrier || cacheWriteBarrier.unvalidatedGet() == JSCell::seenMultipleCalleeObjects())
1268                 break;
1269             JSCell* cachedFunction = cacheWriteBarrier.get();
<span class="line-modified">1270             if (Heap::isMarked(cachedFunction))</span>
1271                 break;
1272             if (Options::verboseOSR())
1273                 dataLogF(&quot;Clearing LLInt create_this with cached callee %p.\n&quot;, cachedFunction);
1274             cacheWriteBarrier.clear();
1275             break;
1276         }
1277         case op_resolve_scope: {
1278             // Right now this isn&#39;t strictly necessary. Any symbol tables that this will refer to
1279             // are for outer functions, and we refer to those functions strongly, and they refer
1280             // to the symbol table strongly. But it&#39;s nice to be on the safe side.
1281             auto&amp; metadata = curInstruction-&gt;as&lt;OpResolveScope&gt;().metadata(this);
1282             WriteBarrierBase&lt;SymbolTable&gt;&amp; symbolTable = metadata.m_symbolTable;
<span class="line-modified">1283             if (!symbolTable || Heap::isMarked(symbolTable.get()))</span>
1284                 break;
1285             if (Options::verboseOSR())
1286                 dataLogF(&quot;Clearing dead symbolTable %p.\n&quot;, symbolTable.get());
1287             symbolTable.clear();
1288             break;
1289         }
1290         case op_get_from_scope:
1291             handleGetPutFromScope(curInstruction-&gt;as&lt;OpGetFromScope&gt;().metadata(this));
1292             break;
1293         case op_put_to_scope:
1294             handleGetPutFromScope(curInstruction-&gt;as&lt;OpPutToScope&gt;().metadata(this));
1295             break;
1296         default:
1297             OpcodeID opcodeID = curInstruction-&gt;opcodeID();
1298             ASSERT_WITH_MESSAGE_UNUSED(opcodeID, false, &quot;Unhandled opcode in CodeBlock::finalizeUnconditionally, %s(%d) at bc %u&quot;, opcodeNames[opcodeID], opcodeID, propertyAccessInstructions[i]);
1299         }
1300     }
1301 
1302     // We can&#39;t just remove all the sets when we clear the caches since we might have created a watchpoint set
1303     // then cleared the cache without GCing in between.
1304     m_llintGetByIdWatchpointMap.removeIf([&amp;] (const StructureWatchpointMap::KeyValuePairType&amp; pair) -&gt; bool {
1305         auto clear = [&amp;] () {
<span class="line-modified">1306             const Instruction* instruction = std::get&lt;1&gt;(pair.key);</span>
1307             OpcodeID opcode = instruction-&gt;opcodeID();
1308             if (opcode == op_get_by_id) {
1309                 if (Options::verboseOSR())
1310                     dataLogF(&quot;Clearing LLInt property access.\n&quot;);
1311                 LLIntPrototypeLoadAdaptiveStructureWatchpoint::clearLLIntGetByIdCache(instruction-&gt;as&lt;OpGetById&gt;().metadata(this));
1312             }
1313             return true;
1314         };
1315 
<span class="line-modified">1316         if (!Heap::isMarked(std::get&lt;0&gt;(pair.key)))</span>
1317             return clear();
1318 
<span class="line-modified">1319         for (const LLIntPrototypeLoadAdaptiveStructureWatchpoint* watchpoint : pair.value) {</span>
<span class="line-modified">1320             if (!watchpoint-&gt;key().isStillLive())</span>
1321                 return clear();
1322         }
1323 
1324         return false;
1325     });
1326 
1327     forEachLLIntCallLinkInfo([&amp;](LLIntCallLinkInfo&amp; callLinkInfo) {
<span class="line-modified">1328         if (callLinkInfo.isLinked() &amp;&amp; !Heap::isMarked(callLinkInfo.callee.get())) {</span>
1329             if (Options::verboseOSR())
1330                 dataLog(&quot;Clearing LLInt call from &quot;, *this, &quot;\n&quot;);
1331             callLinkInfo.unlink();
1332         }
<span class="line-modified">1333         if (!!callLinkInfo.lastSeenCallee &amp;&amp; !Heap::isMarked(callLinkInfo.lastSeenCallee.get()))</span>
<span class="line-modified">1334             callLinkInfo.lastSeenCallee.clear();</span>
1335     });
1336 }
1337 
1338 #if ENABLE(JIT)
1339 CodeBlock::JITData&amp; CodeBlock::ensureJITDataSlow(const ConcurrentJSLocker&amp;)
1340 {
1341     ASSERT(!m_jitData);
<span class="line-modified">1342     m_jitData = std::make_unique&lt;JITData&gt;();</span>
1343     return *m_jitData;
1344 }
1345 
1346 void CodeBlock::finalizeBaselineJITInlineCaches()
1347 {
1348     if (auto* jitData = m_jitData.get()) {
1349         for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos)
<span class="line-modified">1350             callLinkInfo-&gt;visitWeak(*vm());</span>
1351 
1352         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1353             stubInfo-&gt;visitWeakReferences(this);
1354     }
1355 }
1356 #endif
1357 
<span class="line-modified">1358 void CodeBlock::finalizeUnconditionally(VM&amp;)</span>
1359 {


1360     updateAllPredictions();
1361 
1362     if (JITCode::couldBeInterpreted(jitType()))
1363         finalizeLLIntInlineCaches();
1364 
1365 #if ENABLE(JIT)
1366     if (!!jitCode())
1367         finalizeBaselineJITInlineCaches();
1368 #endif
1369 
1370 #if ENABLE(DFG_JIT)
1371     if (JITCode::isOptimizingJIT(jitType())) {
1372         DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
<span class="line-modified">1373         dfgCommon-&gt;recordedStatuses.finalize();</span>
1374     }
1375 #endif // ENABLE(DFG_JIT)
1376 



































1377     VM::SpaceAndSet::setFor(*subspace()).remove(this);
1378 }
1379 
1380 void CodeBlock::destroy(JSCell* cell)
1381 {
1382     static_cast&lt;CodeBlock*&gt;(cell)-&gt;~CodeBlock();
1383 }
1384 
1385 void CodeBlock::getICStatusMap(const ConcurrentJSLocker&amp;, ICStatusMap&amp; result)
1386 {
1387 #if ENABLE(JIT)
1388     if (JITCode::isJIT(jitType())) {
1389         if (auto* jitData = m_jitData.get()) {
1390             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1391                 result.add(stubInfo-&gt;codeOrigin, ICStatus()).iterator-&gt;value.stubInfo = stubInfo;
1392             for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos)
1393                 result.add(callLinkInfo-&gt;codeOrigin(), ICStatus()).iterator-&gt;value.callLinkInfo = callLinkInfo;
1394             for (ByValInfo* byValInfo : jitData-&gt;m_byValInfos)
1395                 result.add(CodeOrigin(byValInfo-&gt;bytecodeIndex), ICStatus()).iterator-&gt;value.byValInfo = byValInfo;
1396         }
</pre>
<hr />
<pre>
1409 #endif
1410     }
1411 #else
1412     UNUSED_PARAM(result);
1413 #endif
1414 }
1415 
1416 void CodeBlock::getICStatusMap(ICStatusMap&amp; result)
1417 {
1418     ConcurrentJSLocker locker(m_lock);
1419     getICStatusMap(locker, result);
1420 }
1421 
1422 #if ENABLE(JIT)
1423 StructureStubInfo* CodeBlock::addStubInfo(AccessType accessType)
1424 {
1425     ConcurrentJSLocker locker(m_lock);
1426     return ensureJITData(locker).m_stubInfos.add(accessType);
1427 }
1428 
<span class="line-modified">1429 JITAddIC* CodeBlock::addJITAddIC(ArithProfile* arithProfile, const Instruction* instruction)</span>
1430 {
1431     ConcurrentJSLocker locker(m_lock);
<span class="line-modified">1432     return ensureJITData(locker).m_addICs.add(arithProfile, instruction);</span>
1433 }
1434 
<span class="line-modified">1435 JITMulIC* CodeBlock::addJITMulIC(ArithProfile* arithProfile, const Instruction* instruction)</span>
1436 {
1437     ConcurrentJSLocker locker(m_lock);
<span class="line-modified">1438     return ensureJITData(locker).m_mulICs.add(arithProfile, instruction);</span>
1439 }
1440 
<span class="line-modified">1441 JITSubIC* CodeBlock::addJITSubIC(ArithProfile* arithProfile, const Instruction* instruction)</span>
1442 {
1443     ConcurrentJSLocker locker(m_lock);
<span class="line-modified">1444     return ensureJITData(locker).m_subICs.add(arithProfile, instruction);</span>
1445 }
1446 
<span class="line-modified">1447 JITNegIC* CodeBlock::addJITNegIC(ArithProfile* arithProfile, const Instruction* instruction)</span>
1448 {
1449     ConcurrentJSLocker locker(m_lock);
<span class="line-modified">1450     return ensureJITData(locker).m_negICs.add(arithProfile, instruction);</span>
1451 }
1452 
1453 StructureStubInfo* CodeBlock::findStubInfo(CodeOrigin codeOrigin)
1454 {
1455     ConcurrentJSLocker locker(m_lock);
1456     if (auto* jitData = m_jitData.get()) {
1457         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
1458             if (stubInfo-&gt;codeOrigin == codeOrigin)
1459                 return stubInfo;
1460         }
1461     }
1462     return nullptr;
1463 }
1464 
1465 ByValInfo* CodeBlock::addByValInfo()
1466 {
1467     ConcurrentJSLocker locker(m_lock);
1468     return ensureJITData(locker).m_byValInfos.add();
1469 }
1470 
</pre>
<hr />
<pre>
1498 {
1499     if (auto* jitData = m_jitData.get()) {
1500         return tryBinarySearch&lt;RareCaseProfile, int&gt;(
1501             jitData-&gt;m_rareCaseProfiles, jitData-&gt;m_rareCaseProfiles.size(), bytecodeOffset,
1502             getRareCaseProfileBytecodeOffset);
1503     }
1504     return nullptr;
1505 }
1506 
1507 unsigned CodeBlock::rareCaseProfileCountForBytecodeOffset(const ConcurrentJSLocker&amp; locker, int bytecodeOffset)
1508 {
1509     RareCaseProfile* profile = rareCaseProfileForBytecodeOffset(locker, bytecodeOffset);
1510     if (profile)
1511         return profile-&gt;m_counter;
1512     return 0;
1513 }
1514 
1515 void CodeBlock::setCalleeSaveRegisters(RegisterSet calleeSaveRegisters)
1516 {
1517     ConcurrentJSLocker locker(m_lock);
<span class="line-modified">1518     ensureJITData(locker).m_calleeSaveRegisters = std::make_unique&lt;RegisterAtOffsetList&gt;(calleeSaveRegisters);</span>
1519 }
1520 
1521 void CodeBlock::setCalleeSaveRegisters(std::unique_ptr&lt;RegisterAtOffsetList&gt; registerAtOffsetList)
1522 {
1523     ConcurrentJSLocker locker(m_lock);
1524     ensureJITData(locker).m_calleeSaveRegisters = WTFMove(registerAtOffsetList);
1525 }
1526 
1527 void CodeBlock::resetJITData()
1528 {
1529     RELEASE_ASSERT(!JITCode::isJIT(jitType()));
1530     ConcurrentJSLocker locker(m_lock);
1531 
1532     if (auto* jitData = m_jitData.get()) {
1533         // We can clear these because no other thread will have references to any stub infos, call
1534         // link infos, or by val infos if we don&#39;t have JIT code. Attempts to query these data
1535         // structures using the concurrent API (getICStatusMap and friends) will return nothing if we
1536         // don&#39;t have JIT code.
1537         jitData-&gt;m_stubInfos.clear();
1538         jitData-&gt;m_callLinkInfos.clear();
</pre>
<hr />
<pre>
1612         visitor.append(transition.m_to);
1613     }
1614 
1615     for (auto&amp; weakReference : dfgCommon-&gt;weakReferences)
1616         visitor.append(weakReference);
1617 
1618     for (auto&amp; weakStructureReference : dfgCommon-&gt;weakStructureReferences)
1619         visitor.append(weakStructureReference);
1620 
1621     dfgCommon-&gt;livenessHasBeenProved = true;
1622 #endif
1623 }
1624 
1625 CodeBlock* CodeBlock::baselineAlternative()
1626 {
1627 #if ENABLE(JIT)
1628     CodeBlock* result = this;
1629     while (result-&gt;alternative())
1630         result = result-&gt;alternative();
1631     RELEASE_ASSERT(result);
<span class="line-modified">1632     RELEASE_ASSERT(JITCode::isBaselineCode(result-&gt;jitType()) || result-&gt;jitType() == JITCode::None);</span>
1633     return result;
1634 #else
1635     return this;
1636 #endif
1637 }
1638 
1639 CodeBlock* CodeBlock::baselineVersion()
1640 {
1641 #if ENABLE(JIT)
<span class="line-modified">1642     JITCode::JITType selfJITType = jitType();</span>
1643     if (JITCode::isBaselineCode(selfJITType))
1644         return this;
1645     CodeBlock* result = replacement();
1646     if (!result) {
1647         if (JITCode::isOptimizingJIT(selfJITType)) {
1648             // The replacement can be null if we&#39;ve had a memory clean up and the executable
1649             // has been purged of its codeBlocks (see ExecutableBase::clearCode()). Regardless,
1650             // the current codeBlock is still live on the stack, and as an optimizing JIT
1651             // codeBlock, it will keep its baselineAlternative() alive for us to fetch below.
1652             result = this;
1653         } else {
1654             // This can happen if we&#39;re creating the original CodeBlock for an executable.
1655             // Assume that we&#39;re the baseline CodeBlock.
<span class="line-modified">1656             RELEASE_ASSERT(selfJITType == JITCode::None);</span>
1657             return this;
1658         }
1659     }
1660     result = result-&gt;baselineAlternative();
1661     ASSERT(result);
1662     return result;
1663 #else
1664     return this;
1665 #endif
1666 }
1667 
1668 #if ENABLE(JIT)
<span class="line-modified">1669 bool CodeBlock::hasOptimizedReplacement(JITCode::JITType typeToReplace)</span>
1670 {
1671     CodeBlock* replacement = this-&gt;replacement();
1672     return replacement &amp;&amp; JITCode::isHigherTier(replacement-&gt;jitType(), typeToReplace);
1673 }
1674 
1675 bool CodeBlock::hasOptimizedReplacement()
1676 {
1677     return hasOptimizedReplacement(jitType());
1678 }
1679 #endif
1680 
1681 HandlerInfo* CodeBlock::handlerForBytecodeOffset(unsigned bytecodeOffset, RequiredHandler requiredHandler)
1682 {
1683     RELEASE_ASSERT(bytecodeOffset &lt; instructions().size());
1684     return handlerForIndex(bytecodeOffset, requiredHandler);
1685 }
1686 
1687 HandlerInfo* CodeBlock::handlerForIndex(unsigned index, RequiredHandler requiredHandler)
1688 {
1689     if (!m_rareData)
</pre>
<hr />
<pre>
1738 void CodeBlock::ensureCatchLivenessIsComputedForBytecodeOffsetSlow(const OpCatch&amp; op, InstructionStream::Offset bytecodeOffset)
1739 {
1740     BytecodeLivenessAnalysis&amp; bytecodeLiveness = livenessAnalysis();
1741 
1742     // We get the live-out set of variables at op_catch, not the live-in. This
1743     // is because the variables that the op_catch defines might be dead, and
1744     // we can avoid profiling them and extracting them when doing OSR entry
1745     // into the DFG.
1746 
1747     auto nextOffset = instructions().at(bytecodeOffset).next().offset();
1748     FastBitVector liveLocals = bytecodeLiveness.getLivenessInfoAtBytecodeOffset(this, nextOffset);
1749     Vector&lt;VirtualRegister&gt; liveOperands;
1750     liveOperands.reserveInitialCapacity(liveLocals.bitCount());
1751     liveLocals.forEachSetBit([&amp;] (unsigned liveLocal) {
1752         liveOperands.append(virtualRegisterForLocal(liveLocal));
1753     });
1754 
1755     for (int i = 0; i &lt; numParameters(); ++i)
1756         liveOperands.append(virtualRegisterForArgument(i));
1757 
<span class="line-modified">1758     auto profiles = std::make_unique&lt;ValueProfileAndOperandBuffer&gt;(liveOperands.size());</span>
1759     RELEASE_ASSERT(profiles-&gt;m_size == liveOperands.size());
1760     for (unsigned i = 0; i &lt; profiles-&gt;m_size; ++i)
1761         profiles-&gt;m_buffer.get()[i].m_operand = liveOperands[i].offset();
1762 
1763     createRareDataIfNecessary();
1764 
1765     // The compiler thread will read this pointer value and then proceed to dereference it
1766     // if it is not null. We need to make sure all above stores happen before this store so
1767     // the compiler thread reads fully initialized data.
1768     WTF::storeStoreFence();
1769 
1770     op.metadata(this).m_buffer = profiles.get();
1771     {
1772         ConcurrentJSLocker locker(m_lock);
1773         m_rareData-&gt;m_catchProfiles.append(WTFMove(profiles));
1774     }
1775 }
1776 
1777 void CodeBlock::removeExceptionHandlerForCallSite(DisposableCallSiteIndex callSiteIndex)
1778 {
</pre>
<hr />
<pre>
1864     noticeIncomingCall(callerFrame);
1865     {
1866         ConcurrentJSLocker locker(m_lock);
1867         ensureJITData(locker).m_incomingPolymorphicCalls.push(incoming);
1868     }
1869 }
1870 #endif // ENABLE(JIT)
1871 
1872 void CodeBlock::unlinkIncomingCalls()
1873 {
1874     while (m_incomingLLIntCalls.begin() != m_incomingLLIntCalls.end())
1875         m_incomingLLIntCalls.begin()-&gt;unlink();
1876 #if ENABLE(JIT)
1877     JITData* jitData = nullptr;
1878     {
1879         ConcurrentJSLocker locker(m_lock);
1880         jitData = m_jitData.get();
1881     }
1882     if (jitData) {
1883         while (jitData-&gt;m_incomingCalls.begin() != jitData-&gt;m_incomingCalls.end())
<span class="line-modified">1884             jitData-&gt;m_incomingCalls.begin()-&gt;unlink(*vm());</span>
1885         while (jitData-&gt;m_incomingPolymorphicCalls.begin() != jitData-&gt;m_incomingPolymorphicCalls.end())
<span class="line-modified">1886             jitData-&gt;m_incomingPolymorphicCalls.begin()-&gt;unlink(*vm());</span>
1887     }
1888 #endif // ENABLE(JIT)
1889 }
1890 
1891 void CodeBlock::linkIncomingCall(ExecState* callerFrame, LLIntCallLinkInfo* incoming)
1892 {
1893     noticeIncomingCall(callerFrame);
1894     m_incomingLLIntCalls.push(incoming);
1895 }
1896 
1897 CodeBlock* CodeBlock::newReplacement()
1898 {
1899     return ownerExecutable()-&gt;newReplacementCodeBlockFor(specializationKind());
1900 }
1901 
1902 #if ENABLE(JIT)
1903 CodeBlock* CodeBlock::replacement()
1904 {
<span class="line-modified">1905     const ClassInfo* classInfo = this-&gt;classInfo(*vm());</span>
1906 
1907     if (classInfo == FunctionCodeBlock::info())
1908         return jsCast&lt;FunctionExecutable*&gt;(ownerExecutable())-&gt;codeBlockFor(isConstructor() ? CodeForConstruct : CodeForCall);
1909 
1910     if (classInfo == EvalCodeBlock::info())
1911         return jsCast&lt;EvalExecutable*&gt;(ownerExecutable())-&gt;codeBlock();
1912 
1913     if (classInfo == ProgramCodeBlock::info())
1914         return jsCast&lt;ProgramExecutable*&gt;(ownerExecutable())-&gt;codeBlock();
1915 
1916     if (classInfo == ModuleProgramCodeBlock::info())
1917         return jsCast&lt;ModuleProgramExecutable*&gt;(ownerExecutable())-&gt;codeBlock();
1918 
1919     RELEASE_ASSERT_NOT_REACHED();
1920     return nullptr;
1921 }
1922 
1923 DFG::CapabilityLevel CodeBlock::computeCapabilityLevel()
1924 {
<span class="line-modified">1925     const ClassInfo* classInfo = this-&gt;classInfo(*vm());</span>
1926 
1927     if (classInfo == FunctionCodeBlock::info()) {
1928         if (isConstructor())
1929             return DFG::functionForConstructCapabilityLevel(this);
1930         return DFG::functionForCallCapabilityLevel(this);
1931     }
1932 
1933     if (classInfo == EvalCodeBlock::info())
1934         return DFG::evalCapabilityLevel(this);
1935 
1936     if (classInfo == ProgramCodeBlock::info())
1937         return DFG::programCapabilityLevel(this);
1938 
1939     if (classInfo == ModuleProgramCodeBlock::info())
1940         return DFG::programCapabilityLevel(this);
1941 
1942     RELEASE_ASSERT_NOT_REACHED();
1943     return DFG::CannotCompile;
1944 }
1945 
1946 #endif // ENABLE(JIT)
1947 
1948 void CodeBlock::jettison(Profiler::JettisonReason reason, ReoptimizationMode mode, const FireDetail* detail)
1949 {
1950 #if !ENABLE(DFG_JIT)
1951     UNUSED_PARAM(mode);
1952     UNUSED_PARAM(detail);
1953 #endif
1954 
<span class="line-modified">1955     CODEBLOCK_LOG_EVENT(this, &quot;jettison&quot;, (&quot;due to &quot;, reason, &quot;, counting = &quot;, mode == CountReoptimization, &quot;, detail = &quot;, pointerDump(detail)));</span>



1956 
1957     RELEASE_ASSERT(reason != Profiler::NotJettisoned);
1958 
1959 #if ENABLE(DFG_JIT)
1960     if (DFG::shouldDumpDisassembly()) {
1961         dataLog(&quot;Jettisoning &quot;, *this);
1962         if (mode == CountReoptimization)
1963             dataLog(&quot; and counting reoptimization&quot;);
1964         dataLog(&quot; due to &quot;, reason);
1965         if (detail)
1966             dataLog(&quot;, &quot;, *detail);
1967         dataLog(&quot;.\n&quot;);
1968     }
1969 
1970     if (reason == Profiler::JettisonDueToWeakReference) {
1971         if (DFG::shouldDumpDisassembly()) {
1972             dataLog(*this, &quot; will be jettisoned because of the following dead references:\n&quot;);
1973             DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1974             for (auto&amp; transition : dfgCommon-&gt;transitions) {
1975                 JSCell* origin = transition.m_codeOrigin.get();
1976                 JSCell* from = transition.m_from.get();
1977                 JSCell* to = transition.m_to.get();
<span class="line-modified">1978                 if ((!origin || Heap::isMarked(origin)) &amp;&amp; Heap::isMarked(from))</span>
1979                     continue;
1980                 dataLog(&quot;    Transition under &quot;, RawPointer(origin), &quot;, &quot;, RawPointer(from), &quot; -&gt; &quot;, RawPointer(to), &quot;.\n&quot;);
1981             }
1982             for (unsigned i = 0; i &lt; dfgCommon-&gt;weakReferences.size(); ++i) {
1983                 JSCell* weak = dfgCommon-&gt;weakReferences[i].get();
<span class="line-modified">1984                 if (Heap::isMarked(weak))</span>
1985                     continue;
1986                 dataLog(&quot;    Weak reference &quot;, RawPointer(weak), &quot;.\n&quot;);
1987             }
1988         }
1989     }
1990 #endif // ENABLE(DFG_JIT)
1991 
<span class="line-removed">1992     VM&amp; vm = *m_vm;</span>
1993     DeferGCForAWhile deferGC(*heap());
1994 
1995     // We want to accomplish two things here:
1996     // 1) Make sure that if this CodeBlock is on the stack right now, then if we return to it
1997     //    we should OSR exit at the top of the next bytecode instruction after the return.
1998     // 2) Make sure that if we call the owner executable, then we shouldn&#39;t call this CodeBlock.
1999 
2000 #if ENABLE(DFG_JIT)
2001     if (JITCode::isOptimizingJIT(jitType()))
2002         jitCode()-&gt;dfgCommon()-&gt;clearWatchpoints();
2003 
2004     if (reason != Profiler::JettisonDueToOldAge) {
2005         Profiler::Compilation* compilation = jitCode()-&gt;dfgCommon()-&gt;compilation.get();
2006         if (UNLIKELY(compilation))
2007             compilation-&gt;setJettisonReason(reason, detail);
2008 
2009         // This accomplishes (1), and does its own book-keeping about whether it has already happened.
2010         if (!jitCode()-&gt;dfgCommon()-&gt;invalidate()) {
2011             // We&#39;ve already been invalidated.
<span class="line-modified">2012             RELEASE_ASSERT(this != replacement() || (vm.heap.isCurrentThreadBusy() &amp;&amp; !Heap::isMarked(ownerExecutable())));</span>
2013             return;
2014         }
2015     }
2016 
2017     if (DFG::shouldDumpDisassembly())
2018         dataLog(&quot;    Did invalidate &quot;, *this, &quot;\n&quot;);
2019 
2020     // Count the reoptimization if that&#39;s what the user wanted.
2021     if (mode == CountReoptimization) {
2022         // FIXME: Maybe this should call alternative().
2023         // https://bugs.webkit.org/show_bug.cgi?id=123677
2024         baselineAlternative()-&gt;countReoptimization();
2025         if (DFG::shouldDumpDisassembly())
2026             dataLog(&quot;    Did count reoptimization for &quot;, *this, &quot;\n&quot;);
2027     }
2028 
2029     if (this != replacement()) {
2030         // This means that we were never the entrypoint. This can happen for OSR entry code
2031         // blocks.
2032         return;
2033     }
2034 
2035     if (alternative())
2036         alternative()-&gt;optimizeAfterWarmUp();
2037 
2038     if (reason != Profiler::JettisonDueToOldAge &amp;&amp; reason != Profiler::JettisonDueToVMTraps)
2039         tallyFrequentExitSites();
2040 #endif // ENABLE(DFG_JIT)
2041 
2042     // Jettison can happen during GC. We don&#39;t want to install code to a dead executable
2043     // because that would add a dead object to the remembered set.
<span class="line-modified">2044     if (vm.heap.isCurrentThreadBusy() &amp;&amp; !Heap::isMarked(ownerExecutable()))</span>
2045         return;
2046 










2047     // This accomplishes (2).
2048     ownerExecutable()-&gt;installCode(vm, alternative(), codeType(), specializationKind());
2049 
2050 #if ENABLE(DFG_JIT)
2051     if (DFG::shouldDumpDisassembly())
2052         dataLog(&quot;    Did install baseline version of &quot;, *this, &quot;\n&quot;);
2053 #endif // ENABLE(DFG_JIT)
2054 }
2055 
2056 JSGlobalObject* CodeBlock::globalObjectFor(CodeOrigin codeOrigin)
2057 {
<span class="line-modified">2058     if (!codeOrigin.inlineCallFrame)</span>

2059         return globalObject();
<span class="line-modified">2060     return codeOrigin.inlineCallFrame-&gt;baselineCodeBlock-&gt;globalObject();</span>
2061 }
2062 
2063 class RecursionCheckFunctor {
2064 public:
2065     RecursionCheckFunctor(CallFrame* startCallFrame, CodeBlock* codeBlock, unsigned depthToCheck)
2066         : m_startCallFrame(startCallFrame)
2067         , m_codeBlock(codeBlock)
2068         , m_depthToCheck(depthToCheck)
2069         , m_foundStartCallFrame(false)
2070         , m_didRecurse(false)
2071     { }
2072 
2073     StackVisitor::Status operator()(StackVisitor&amp; visitor) const
2074     {
2075         CallFrame* currentCallFrame = visitor-&gt;callFrame();
2076 
2077         if (currentCallFrame == m_startCallFrame)
2078             m_foundStartCallFrame = true;
2079 
2080         if (m_foundStartCallFrame) {
</pre>
<hr />
<pre>
2117             dataLog(&quot;    Clearing SABI because caller is native.\n&quot;);
2118         return;
2119     }
2120 
2121     if (!hasBaselineJITProfiling())
2122         return;
2123 
2124     if (!DFG::mightInlineFunction(this))
2125         return;
2126 
2127     if (!canInline(capabilityLevelState()))
2128         return;
2129 
2130     if (!DFG::isSmallEnoughToInlineCodeInto(callerCodeBlock)) {
2131         m_shouldAlwaysBeInlined = false;
2132         if (Options::verboseCallLink())
2133             dataLog(&quot;    Clearing SABI because caller is too large.\n&quot;);
2134         return;
2135     }
2136 
<span class="line-modified">2137     if (callerCodeBlock-&gt;jitType() == JITCode::InterpreterThunk) {</span>
2138         // If the caller is still in the interpreter, then we can&#39;t expect inlining to
2139         // happen anytime soon. Assume it&#39;s profitable to optimize it separately. This
2140         // ensures that a function is SABI only if it is called no more frequently than
2141         // any of its callers.
2142         m_shouldAlwaysBeInlined = false;
2143         if (Options::verboseCallLink())
2144             dataLog(&quot;    Clearing SABI because caller is in LLInt.\n&quot;);
2145         return;
2146     }
2147 
2148     if (JITCode::isOptimizingJIT(callerCodeBlock-&gt;jitType())) {
2149         m_shouldAlwaysBeInlined = false;
2150         if (Options::verboseCallLink())
2151             dataLog(&quot;    Clearing SABI bcause caller was already optimized.\n&quot;);
2152         return;
2153     }
2154 
2155     if (callerCodeBlock-&gt;codeType() != FunctionCode) {
2156         // If the caller is either eval or global code, assume that that won&#39;t be
2157         // optimized anytime soon. For eval code this is particularly true since we
2158         // delay eval optimization by a *lot*.
2159         m_shouldAlwaysBeInlined = false;
2160         if (Options::verboseCallLink())
2161             dataLog(&quot;    Clearing SABI because caller is not a function.\n&quot;);
2162         return;
2163     }
2164 
2165     // Recursive calls won&#39;t be inlined.
2166     RecursionCheckFunctor functor(callerFrame, this, Options::maximumInliningDepth());
<span class="line-modified">2167     vm()-&gt;topCallFrame-&gt;iterate(functor);</span>
2168 
2169     if (functor.didRecurse()) {
2170         if (Options::verboseCallLink())
2171             dataLog(&quot;    Clearing SABI because recursion was detected.\n&quot;);
2172         m_shouldAlwaysBeInlined = false;
2173         return;
2174     }
2175 
2176     if (callerCodeBlock-&gt;capabilityLevelState() == DFG::CapabilityLevelNotSet) {
2177         dataLog(&quot;In call from &quot;, FullCodeOrigin(callerCodeBlock, callerFrame-&gt;codeOrigin()), &quot; to &quot;, *this, &quot;: caller&#39;s DFG capability level is not set.\n&quot;);
2178         CRASH();
2179     }
2180 
2181     if (canCompile(callerCodeBlock-&gt;capabilityLevelState()))
2182         return;
2183 
2184     if (Options::verboseCallLink())
2185         dataLog(&quot;    Clearing SABI because the caller is not a DFG candidate.\n&quot;);
2186 
2187     m_shouldAlwaysBeInlined = false;
</pre>
<hr />
<pre>
2305     // count 320, which got compiled early as it should have been) and one where they were
2306     // totally wrong (code block in 3d-cube with instruction count 1268, which was expensive
2307     // to compile and didn&#39;t run often enough to warrant compilation in my opinion), and
2308     // then threw in additional data points that represented my own guess of what our
2309     // heuristics should do for some round-numbered examples.
2310     //
2311     // The expression to which I decided to fit the data arose because I started with an
2312     // affine function, and then did two things: put the linear part in an Abs to ensure
2313     // that the fit didn&#39;t end up choosing a negative value of c (which would result in
2314     // the function turning over and going negative for large x) and I threw in a Sqrt
2315     // term because Sqrt represents my intution that the function should be more sensitive
2316     // to small changes in small values of x, but less sensitive when x gets large.
2317 
2318     // Note that the current fit essentially eliminates the linear portion of the
2319     // expression (c == 0.0).
2320     const double a = 0.061504;
2321     const double b = 1.02406;
2322     const double c = 0.0;
2323     const double d = 0.825914;
2324 
<span class="line-modified">2325     double instructionCount = this-&gt;instructionCount();</span>
2326 
<span class="line-modified">2327     ASSERT(instructionCount); // Make sure this is called only after we have an instruction stream; otherwise it&#39;ll just return the value of d, which makes no sense.</span>
2328 
<span class="line-modified">2329     double result = d + a * sqrt(instructionCount + b) + c * instructionCount;</span>
2330 
2331     result *= codeTypeThresholdMultiplier();
2332 
2333     if (Options::verboseOSR()) {
2334         dataLog(
<span class="line-modified">2335             *this, &quot;: instruction count is &quot;, instructionCount,</span>
2336             &quot;, scaling execution counter by &quot;, result, &quot; * &quot;, codeTypeThresholdMultiplier(),
2337             &quot;\n&quot;);
2338     }
2339     return result;
2340 }
2341 
2342 static int32_t clipThreshold(double threshold)
2343 {
2344     if (threshold &lt; 1.0)
2345         return 1;
2346 
2347     if (threshold &gt; static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::max()))
2348         return std::numeric_limits&lt;int32_t&gt;::max();
2349 
2350     return static_cast&lt;int32_t&gt;(threshold);
2351 }
2352 
2353 int32_t CodeBlock::adjustedCounterValue(int32_t desiredThreshold)
2354 {
2355     return clipThreshold(
</pre>
<hr />
<pre>
2385         return OptimizeAction::None;
2386     }
2387 
2388     exit.m_count++;
2389     m_osrExitCounter++;
2390 
2391     CodeBlock* baselineCodeBlock = exitState.baselineCodeBlock;
2392     ASSERT(baselineCodeBlock == baselineAlternative());
2393     if (UNLIKELY(baselineCodeBlock-&gt;jitExecuteCounter().hasCrossedThreshold()))
2394         return OptimizeAction::ReoptimizeNow;
2395 
2396     // We want to figure out if there&#39;s a possibility that we&#39;re in a loop. For the outermost
2397     // code block in the inline stack, we handle this appropriately by having the loop OSR trigger
2398     // check the exit count of the replacement of the CodeBlock from which we are OSRing. The
2399     // problem is the inlined functions, which might also have loops, but whose baseline versions
2400     // don&#39;t know where to look for the exit count. Figure out if those loops are severe enough
2401     // that we had tried to OSR enter. If so, then we should use the loop reoptimization trigger.
2402     // Otherwise, we should use the normal reoptimization trigger.
2403 
2404     bool didTryToEnterInLoop = false;
<span class="line-modified">2405     for (InlineCallFrame* inlineCallFrame = exit.m_codeOrigin.inlineCallFrame; inlineCallFrame; inlineCallFrame = inlineCallFrame-&gt;directCaller.inlineCallFrame) {</span>
2406         if (inlineCallFrame-&gt;baselineCodeBlock-&gt;ownerExecutable()-&gt;didTryToEnterInLoop()) {
2407             didTryToEnterInLoop = true;
2408             break;
2409         }
2410     }
2411 
2412     uint32_t exitCountThreshold = didTryToEnterInLoop
2413         ? exitCountThresholdForReoptimizationFromLoop()
2414         : exitCountThresholdForReoptimization();
2415 
2416     if (m_osrExitCounter &gt; exitCountThreshold)
2417         return OptimizeAction::ReoptimizeNow;
2418 
2419     // Too few fails. Adjust the execution counter such that the target is to only optimize after a while.
2420     baselineCodeBlock-&gt;m_jitExecuteCounter.setNewThresholdForOSRExit(exitState.activeThreshold, exitState.memoryUsageAdjustedThreshold);
2421     return OptimizeAction::None;
2422 }
2423 #endif
2424 
2425 void CodeBlock::optimizeNextInvocation()
</pre>
<hr />
<pre>
2459 void CodeBlock::optimizeSoon()
2460 {
2461     if (Options::verboseOSR())
2462         dataLog(*this, &quot;: Optimizing soon.\n&quot;);
2463 #if ENABLE(DFG_JIT)
2464     m_jitExecuteCounter.setNewThreshold(
2465         adjustedCounterValue(Options::thresholdForOptimizeSoon()), this);
2466 #endif
2467 }
2468 
2469 void CodeBlock::forceOptimizationSlowPathConcurrently()
2470 {
2471     if (Options::verboseOSR())
2472         dataLog(*this, &quot;: Forcing slow path concurrently.\n&quot;);
2473     m_jitExecuteCounter.forceSlowPathConcurrently();
2474 }
2475 
2476 #if ENABLE(DFG_JIT)
2477 void CodeBlock::setOptimizationThresholdBasedOnCompilationResult(CompilationResult result)
2478 {
<span class="line-modified">2479     JITCode::JITType type = jitType();</span>
<span class="line-modified">2480     if (type != JITCode::BaselineJIT) {</span>
2481         dataLog(*this, &quot;: expected to have baseline code but have &quot;, type, &quot;\n&quot;);
<span class="line-modified">2482         CRASH_WITH_INFO(bitwise_cast&lt;uintptr_t&gt;(jitCode().get()), type);</span>
2483     }
2484 
2485     CodeBlock* replacement = this-&gt;replacement();
2486     bool hasReplacement = (replacement &amp;&amp; replacement != this);
2487     if ((result == CompilationSuccessful) != hasReplacement) {
2488         dataLog(*this, &quot;: we have result = &quot;, result, &quot; but &quot;);
2489         if (replacement == this)
2490             dataLog(&quot;we are our own replacement.\n&quot;);
2491         else
2492             dataLog(&quot;our replacement is &quot;, pointerDump(replacement), &quot;\n&quot;);
2493         RELEASE_ASSERT_NOT_REACHED();
2494     }
2495 
2496     switch (result) {
2497     case CompilationSuccessful:
2498         RELEASE_ASSERT(replacement &amp;&amp; JITCode::isOptimizingJIT(replacement-&gt;jitType()));
2499         optimizeNextInvocation();
2500         return;
2501     case CompilationFailed:
2502         dontOptimizeAnytimeSoon();
</pre>
<hr />
<pre>
2545 uint32_t CodeBlock::exitCountThresholdForReoptimizationFromLoop()
2546 {
2547     return adjustedExitCountThreshold(Options::osrExitCountForReoptimizationFromLoop() * codeTypeThresholdMultiplier());
2548 }
2549 
2550 bool CodeBlock::shouldReoptimizeNow()
2551 {
2552     return osrExitCounter() &gt;= exitCountThresholdForReoptimization();
2553 }
2554 
2555 bool CodeBlock::shouldReoptimizeFromLoopNow()
2556 {
2557     return osrExitCounter() &gt;= exitCountThresholdForReoptimizationFromLoop();
2558 }
2559 #endif
2560 
2561 ArrayProfile* CodeBlock::getArrayProfile(const ConcurrentJSLocker&amp;, unsigned bytecodeOffset)
2562 {
2563     auto instruction = instructions().at(bytecodeOffset);
2564     switch (instruction-&gt;opcodeID()) {
<span class="line-modified">2565 #define CASE(Op) \</span>
2566     case Op::opcodeID: \
2567         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_arrayProfile;
2568 
<span class="line-modified">2569     FOR_EACH_OPCODE_WITH_ARRAY_PROFILE(CASE)</span>
<span class="line-modified">2570 #undef CASE</span>







2571 
2572     case OpGetById::opcodeID: {
2573         auto bytecode = instruction-&gt;as&lt;OpGetById&gt;();
2574         auto&amp; metadata = bytecode.metadata(this);
<span class="line-modified">2575         if (metadata.m_mode == GetByIdMode::ArrayLength)</span>
2576             return &amp;metadata.m_modeMetadata.arrayLengthMode.arrayProfile;
2577         break;
2578     }
2579     default:
2580         break;
2581     }
2582 
2583     return nullptr;
2584 }
2585 
2586 ArrayProfile* CodeBlock::getArrayProfile(unsigned bytecodeOffset)
2587 {
2588     ConcurrentJSLocker locker(m_lock);
2589     return getArrayProfile(locker, bytecodeOffset);
2590 }
2591 
2592 #if ENABLE(DFG_JIT)
2593 Vector&lt;CodeOrigin, 0, UnsafeVectorOverflow&gt;&amp; CodeBlock::codeOrigins()
2594 {
2595     return m_jitCode-&gt;dfgCommon()-&gt;codeOrigins;
2596 }
2597 
2598 size_t CodeBlock::numberOfDFGIdentifiers() const
2599 {
2600     if (!JITCode::isOptimizingJIT(jitType()))
2601         return 0;
2602 
2603     return m_jitCode-&gt;dfgCommon()-&gt;dfgIdentifiers.size();
2604 }
2605 
2606 const Identifier&amp; CodeBlock::identifier(int index) const
2607 {
2608     size_t unlinkedIdentifiers = m_unlinkedCode-&gt;numberOfIdentifiers();
2609     if (static_cast&lt;unsigned&gt;(index) &lt; unlinkedIdentifiers)
2610         return m_unlinkedCode-&gt;identifier(index);
2611     ASSERT(JITCode::isOptimizingJIT(jitType()));
2612     return m_jitCode-&gt;dfgCommon()-&gt;dfgIdentifiers[index - unlinkedIdentifiers];
2613 }
2614 #endif // ENABLE(DFG_JIT)
2615 
<span class="line-modified">2616 void CodeBlock::updateAllPredictionsAndCountLiveness(unsigned&amp; numberOfLiveNonArgumentValueProfiles, unsigned&amp; numberOfSamplesInProfiles)</span>
2617 {
2618     ConcurrentJSLocker locker(m_lock);
2619 
2620     numberOfLiveNonArgumentValueProfiles = 0;
2621     numberOfSamplesInProfiles = 0; // If this divided by ValueProfile::numberOfBuckets equals numberOfValueProfiles() then value profiles are full.
2622 
<span class="line-modified">2623     forEachValueProfile([&amp;](ValueProfile&amp; profile) {</span>
2624         unsigned numSamples = profile.totalNumberOfSamples();

2625         if (numSamples &gt; ValueProfile::numberOfBuckets)
2626             numSamples = ValueProfile::numberOfBuckets; // We don&#39;t want profiles that are extremely hot to be given more weight.
2627         numberOfSamplesInProfiles += numSamples;
<span class="line-modified">2628         if (profile.m_bytecodeOffset &lt; 0) {</span>
2629             profile.computeUpdatedPrediction(locker);
2630             return;
2631         }
<span class="line-modified">2632         if (profile.numberOfSamples() || profile.m_prediction != SpecNone)</span>
2633             numberOfLiveNonArgumentValueProfiles++;
2634         profile.computeUpdatedPrediction(locker);
2635     });
2636 
2637     if (auto* rareData = m_rareData.get()) {
2638         for (auto&amp; profileBucket : rareData-&gt;m_catchProfiles) {
2639             profileBucket-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {
<span class="line-modified">2640                 profile.m_profile.computeUpdatedPrediction(locker);</span>
2641             });
2642         }
2643     }
2644 
2645 #if ENABLE(DFG_JIT)
2646     lazyOperandValueProfiles(locker).computeUpdatedPredictions(locker);
2647 #endif
2648 }
2649 
2650 void CodeBlock::updateAllValueProfilePredictions()
2651 {
2652     unsigned ignoredValue1, ignoredValue2;
<span class="line-modified">2653     updateAllPredictionsAndCountLiveness(ignoredValue1, ignoredValue2);</span>
2654 }
2655 
2656 void CodeBlock::updateAllArrayPredictions()
2657 {
2658     ConcurrentJSLocker locker(m_lock);
2659 
2660     forEachArrayProfile([&amp;](ArrayProfile&amp; profile) {
2661         profile.computeUpdatedPrediction(locker, this);
2662     });
2663 
2664     forEachArrayAllocationProfile([&amp;](ArrayAllocationProfile&amp; profile) {
2665         profile.updateProfile();
2666     });
2667 }
2668 
2669 void CodeBlock::updateAllPredictions()
2670 {
2671     updateAllValueProfilePredictions();
2672     updateAllArrayPredictions();
2673 }
2674 
2675 bool CodeBlock::shouldOptimizeNow()
2676 {
2677     if (Options::verboseOSR())
2678         dataLog(&quot;Considering optimizing &quot;, *this, &quot;...\n&quot;);
2679 
2680     if (m_optimizationDelayCounter &gt;= Options::maximumOptimizationDelay())
2681         return true;
2682 
2683     updateAllArrayPredictions();
2684 
2685     unsigned numberOfLiveNonArgumentValueProfiles;
2686     unsigned numberOfSamplesInProfiles;
<span class="line-modified">2687     updateAllPredictionsAndCountLiveness(numberOfLiveNonArgumentValueProfiles, numberOfSamplesInProfiles);</span>
2688 
2689     if (Options::verboseOSR()) {
2690         dataLogF(
2691             &quot;Profile hotness: %lf (%u / %u), %lf (%u / %u)\n&quot;,
2692             (double)numberOfLiveNonArgumentValueProfiles / numberOfNonArgumentValueProfiles(),
2693             numberOfLiveNonArgumentValueProfiles, numberOfNonArgumentValueProfiles(),
2694             (double)numberOfSamplesInProfiles / ValueProfile::numberOfBuckets / numberOfNonArgumentValueProfiles(),
2695             numberOfSamplesInProfiles, ValueProfile::numberOfBuckets * numberOfNonArgumentValueProfiles());
2696     }
2697 
2698     if ((!numberOfNonArgumentValueProfiles() || (double)numberOfLiveNonArgumentValueProfiles / numberOfNonArgumentValueProfiles() &gt;= Options::desiredProfileLivenessRate())
2699         &amp;&amp; (!totalNumberOfValueProfiles() || (double)numberOfSamplesInProfiles / ValueProfile::numberOfBuckets / totalNumberOfValueProfiles() &gt;= Options::desiredProfileFullnessRate())
2700         &amp;&amp; static_cast&lt;unsigned&gt;(m_optimizationDelayCounter) + 1 &gt;= Options::minimumOptimizationDelay())
2701         return true;
2702 
2703     ASSERT(m_optimizationDelayCounter &lt; std::numeric_limits&lt;uint8_t&gt;::max());
2704     m_optimizationDelayCounter++;
2705     optimizeAfterWarmUp();
2706     return false;
2707 }
2708 
2709 #if ENABLE(DFG_JIT)
2710 void CodeBlock::tallyFrequentExitSites()
2711 {
2712     ASSERT(JITCode::isOptimizingJIT(jitType()));
<span class="line-modified">2713     ASSERT(alternative()-&gt;jitType() == JITCode::BaselineJIT);</span>
2714 
2715     CodeBlock* profiledBlock = alternative();
2716 
2717     switch (jitType()) {
<span class="line-modified">2718     case JITCode::DFGJIT: {</span>
2719         DFG::JITCode* jitCode = m_jitCode-&gt;dfg();
2720         for (auto&amp; exit : jitCode-&gt;osrExit)
2721             exit.considerAddingAsFrequentExitSite(profiledBlock);
2722         break;
2723     }
2724 
2725 #if ENABLE(FTL_JIT)
<span class="line-modified">2726     case JITCode::FTLJIT: {</span>
2727         // There is no easy way to avoid duplicating this code since the FTL::JITCode::osrExit
2728         // vector contains a totally different type, that just so happens to behave like
2729         // DFG::JITCode::osrExit.
2730         FTL::JITCode* jitCode = m_jitCode-&gt;ftl();
2731         for (unsigned i = 0; i &lt; jitCode-&gt;osrExit.size(); ++i) {
2732             FTL::OSRExit&amp; exit = jitCode-&gt;osrExit[i];
2733             exit.considerAddingAsFrequentExitSite(profiledBlock);
2734         }
2735         break;
2736     }
2737 #endif
2738 
2739     default:
2740         RELEASE_ASSERT_NOT_REACHED();
2741         break;
2742     }
2743 }
2744 #endif // ENABLE(DFG_JIT)
2745 
2746 void CodeBlock::notifyLexicalBindingUpdate()
</pre>
<hr />
<pre>
2770             ResolveType originalResolveType = metadata.m_resolveType;
2771             if (originalResolveType == GlobalProperty || originalResolveType == GlobalPropertyWithVarInjectionChecks) {
2772                 const Identifier&amp; ident = identifier(bytecode.m_var);
2773                 if (isShadowed(ident.impl()))
2774                     metadata.m_globalLexicalBindingEpoch = 0;
2775                 else
2776                     metadata.m_globalLexicalBindingEpoch = globalObject-&gt;globalLexicalBindingEpoch();
2777             }
2778             break;
2779         }
2780         default:
2781             break;
2782         }
2783     }
2784 }
2785 
2786 #if ENABLE(VERBOSE_VALUE_PROFILE)
2787 void CodeBlock::dumpValueProfiles()
2788 {
2789     dataLog(&quot;ValueProfile for &quot;, *this, &quot;:\n&quot;);
<span class="line-modified">2790     forEachValueProfile([](ValueProfile&amp; profile) {</span>
<span class="line-modified">2791         if (profile.m_bytecodeOffset &lt; 0) {</span>
<span class="line-modified">2792             ASSERT(profile.m_bytecodeOffset == -1);</span>
<span class="line-modified">2793             dataLogF(&quot;   arg = %u: &quot;, i);</span>
<span class="line-modified">2794         } else</span>
<span class="line-removed">2795             dataLogF(&quot;   bc = %d: &quot;, profile.m_bytecodeOffset);</span>
2796         if (!profile.numberOfSamples() &amp;&amp; profile.m_prediction == SpecNone) {
2797             dataLogF(&quot;&lt;empty&gt;\n&quot;);
2798             continue;
2799         }
2800         profile.dump(WTF::dataFile());
2801         dataLogF(&quot;\n&quot;);
2802     });
2803     dataLog(&quot;RareCaseProfile for &quot;, *this, &quot;:\n&quot;);
2804     if (auto* jitData = m_jitData.get()) {
2805         for (RareCaseProfile* profile : jitData-&gt;m_rareCaseProfiles)
2806             dataLogF(&quot;   bc = %d: %u\n&quot;, profile-&gt;m_bytecodeOffset, profile-&gt;m_counter);
2807     }
2808 }
2809 #endif // ENABLE(VERBOSE_VALUE_PROFILE)
2810 
2811 unsigned CodeBlock::frameRegisterCount()
2812 {
2813     switch (jitType()) {
<span class="line-modified">2814     case JITCode::InterpreterThunk:</span>
2815         return LLInt::frameRegisterCountFor(this);
2816 
2817 #if ENABLE(JIT)
<span class="line-modified">2818     case JITCode::BaselineJIT:</span>
2819         return JIT::frameRegisterCountFor(this);
2820 #endif // ENABLE(JIT)
2821 
2822 #if ENABLE(DFG_JIT)
<span class="line-modified">2823     case JITCode::DFGJIT:</span>
<span class="line-modified">2824     case JITCode::FTLJIT:</span>
2825         return jitCode()-&gt;dfgCommon()-&gt;frameRegisterCount;
2826 #endif // ENABLE(DFG_JIT)
2827 
2828     default:
2829         RELEASE_ASSERT_NOT_REACHED();
2830         return 0;
2831     }
2832 }
2833 
2834 int CodeBlock::stackPointerOffset()
2835 {
2836     return virtualRegisterForLocal(frameRegisterCount() - 1).offset();
2837 }
2838 
2839 size_t CodeBlock::predictedMachineCodeSize()
2840 {
2841     VM* vm = m_vm;
2842     // This will be called from CodeBlock::CodeBlock before either m_vm or the
2843     // instructions have been initialized. It&#39;s OK to return 0 because what will really
2844     // matter is the recomputation of this value when the slow path is triggered.
2845     if (!vm)
2846         return 0;
2847 
2848     if (!*vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT)
2849         return 0; // It&#39;s as good of a prediction as we&#39;ll get.
2850 
2851     // Be conservative: return a size that will be an overestimation 84% of the time.
2852     double multiplier = vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;mean() +
2853         vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;standardDeviation();
2854 
2855     // Be paranoid: silently reject bogus multipiers. Silently doing the &quot;wrong&quot; thing
2856     // here is OK, since this whole method is just a heuristic.
2857     if (multiplier &lt; 0 || multiplier &gt; 1000)
2858         return 0;
2859 
<span class="line-modified">2860     double doubleResult = multiplier * instructionCount();</span>
2861 
2862     // Be even more paranoid: silently reject values that won&#39;t fit into a size_t. If
2863     // the function is so huge that we can&#39;t even fit it into virtual memory then we
2864     // should probably have some other guards in place to prevent us from even getting
2865     // to this point.
2866     if (doubleResult &gt; std::numeric_limits&lt;size_t&gt;::max())
2867         return 0;
2868 
2869     return static_cast&lt;size_t&gt;(doubleResult);
2870 }
2871 
2872 String CodeBlock::nameForRegister(VirtualRegister virtualRegister)
2873 {
2874     for (auto&amp; constantRegister : m_constantRegisters) {
2875         if (constantRegister.get().isEmpty())
2876             continue;
<span class="line-modified">2877         if (SymbolTable* symbolTable = jsDynamicCast&lt;SymbolTable*&gt;(*vm(), constantRegister.get())) {</span>
2878             ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
2879             auto end = symbolTable-&gt;end(locker);
2880             for (auto ptr = symbolTable-&gt;begin(locker); ptr != end; ++ptr) {
2881                 if (ptr-&gt;value.varOffset() == VarOffset(virtualRegister)) {
2882                     // FIXME: This won&#39;t work from the compilation thread.
2883                     // https://bugs.webkit.org/show_bug.cgi?id=115300
2884                     return ptr-&gt;key.get();
2885                 }
2886             }
2887         }
2888     }
2889     if (virtualRegister == thisRegister())
2890         return &quot;this&quot;_s;
2891     if (virtualRegister.isArgument())
2892         return makeString(&quot;arguments[&quot;, pad(&#39; &#39;, 3, virtualRegister.toArgument()), &#39;]&#39;);
2893 
2894     return emptyString();
2895 }
2896 
2897 ValueProfile* CodeBlock::tryGetValueProfileForBytecodeOffset(int bytecodeOffset)
</pre>
<hr />
<pre>
3083         // The following check allows for the same textual JavaScript basic block to have its bytecode emitted more
3084         // than once and still play nice with the control flow profiler. When basicBlockStartOffset is larger than
3085         // basicBlockEndOffset, it indicates that the bytecode generator has emitted code for the same AST node
3086         // more than once (for example: ForInNode, Finally blocks in TryNode, etc). Though these are different
3087         // basic blocks at the bytecode level, they are generated from the same textual basic block in the JavaScript
3088         // program. The condition:
3089         // (basicBlockEndOffset &lt; basicBlockStartOffset)
3090         // is encountered when op_profile_control_flow lies across the boundary of these duplicated bytecode basic
3091         // blocks and the textual offset goes from the end of the duplicated block back to the beginning. These
3092         // ranges are dummy ranges and are ignored. The duplicated bytecode basic blocks point to the same
3093         // internal data structure, so if any of them execute, it will record the same textual basic block in the
3094         // JavaScript program as executing.
3095         // At the bytecode level, this situation looks like:
3096         // j: op_profile_control_flow (from j-&gt;k, we have basicBlockEndOffset &lt; basicBlockStartOffset)
3097         // ...
3098         // k: op_profile_control_flow (we want to skip over the j-&gt;k block and start fresh at offset k as the start of a new basic block k-&gt;m).
3099         // ...
3100         // m: op_profile_control_flow
3101         if (basicBlockEndOffset &lt; basicBlockStartOffset) {
3102             RELEASE_ASSERT(i + 1 &lt; offsetsLength); // We should never encounter dummy blocks at the end of a CodeBlock.
<span class="line-modified">3103             metadata.m_basicBlockLocation = vm()-&gt;controlFlowProfiler()-&gt;dummyBasicBlock();</span>
3104             continue;
3105         }
3106 
<span class="line-modified">3107         BasicBlockLocation* basicBlockLocation = vm()-&gt;controlFlowProfiler()-&gt;getBasicBlockLocation(ownerExecutable()-&gt;sourceID(), basicBlockStartOffset, basicBlockEndOffset);</span>
3108 
3109         // Find all functions that are enclosed within the range: [basicBlockStartOffset, basicBlockEndOffset]
3110         // and insert these functions&#39; start/end offsets as gaps in the current BasicBlockLocation.
3111         // This is necessary because in the original source text of a JavaScript program,
3112         // function literals form new basic blocks boundaries, but they aren&#39;t represented
3113         // inside the CodeBlock&#39;s instruction stream.
3114         auto insertFunctionGaps = [basicBlockLocation, basicBlockStartOffset, basicBlockEndOffset] (const WriteBarrier&lt;FunctionExecutable&gt;&amp; functionExecutable) {
3115             const UnlinkedFunctionExecutable* executable = functionExecutable-&gt;unlinkedExecutable();
3116             int functionStart = executable-&gt;typeProfilingStartOffset();
3117             int functionEnd = executable-&gt;typeProfilingEndOffset();
3118             if (functionStart &gt;= basicBlockStartOffset &amp;&amp; functionEnd &lt;= basicBlockEndOffset)
3119                 basicBlockLocation-&gt;insertGap(functionStart, functionEnd);
3120         };
3121 
3122         for (const WriteBarrier&lt;FunctionExecutable&gt;&amp; executable : m_functionDecls)
3123             insertFunctionGaps(executable);
3124         for (const WriteBarrier&lt;FunctionExecutable&gt;&amp; executable : m_functionExprs)
3125             insertFunctionGaps(executable);
3126 
3127         metadata.m_basicBlockLocation = basicBlockLocation;
</pre>
<hr />
<pre>
3145                     return codeOrigin;
3146             }
3147 
3148             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
3149                 if (stubInfo-&gt;containsPC(pc))
3150                     return Optional&lt;CodeOrigin&gt;(stubInfo-&gt;codeOrigin);
3151             }
3152         }
3153     }
3154 
3155     if (Optional&lt;CodeOrigin&gt; codeOrigin = m_jitCode-&gt;findPC(this, pc))
3156         return codeOrigin;
3157 
3158     return WTF::nullopt;
3159 }
3160 #endif // ENABLE(JIT)
3161 
3162 Optional&lt;unsigned&gt; CodeBlock::bytecodeOffsetFromCallSiteIndex(CallSiteIndex callSiteIndex)
3163 {
3164     Optional&lt;unsigned&gt; bytecodeOffset;
<span class="line-modified">3165     JITCode::JITType jitType = this-&gt;jitType();</span>
<span class="line-modified">3166     if (jitType == JITCode::InterpreterThunk || jitType == JITCode::BaselineJIT) {</span>
3167 #if USE(JSVALUE64)
3168         bytecodeOffset = callSiteIndex.bits();
3169 #else
3170         Instruction* instruction = bitwise_cast&lt;Instruction*&gt;(callSiteIndex.bits());
3171         bytecodeOffset = this-&gt;bytecodeOffset(instruction);
3172 #endif
<span class="line-modified">3173     } else if (jitType == JITCode::DFGJIT || jitType == JITCode::FTLJIT) {</span>
3174 #if ENABLE(DFG_JIT)
3175         RELEASE_ASSERT(canGetCodeOrigin(callSiteIndex));
3176         CodeOrigin origin = codeOrigin(callSiteIndex);
<span class="line-modified">3177         bytecodeOffset = origin.bytecodeIndex;</span>
3178 #else
3179         RELEASE_ASSERT_NOT_REACHED();
3180 #endif
3181     }
3182 
3183     return bytecodeOffset;
3184 }
3185 
3186 int32_t CodeBlock::thresholdForJIT(int32_t threshold)
3187 {
3188     switch (unlinkedCodeBlock()-&gt;didOptimize()) {
3189     case MixedTriState:
3190         return threshold;
3191     case FalseTriState:
3192         return threshold * 4;
3193     case TrueTriState:
3194         return threshold / 2;
3195     }
3196     ASSERT_NOT_REACHED();
3197     return threshold;
</pre>
</td>
<td>
<hr />
<pre>
  91 #include &lt;wtf/CommaPrinter.h&gt;
  92 #include &lt;wtf/Forward.h&gt;
  93 #include &lt;wtf/SimpleStats.h&gt;
  94 #include &lt;wtf/StringPrintStream.h&gt;
  95 #include &lt;wtf/text/StringConcatenateNumbers.h&gt;
  96 #include &lt;wtf/text/UniquedStringImpl.h&gt;
  97 
  98 #if ENABLE(ASSEMBLER)
  99 #include &quot;RegisterAtOffsetList.h&quot;
 100 #endif
 101 
 102 #if ENABLE(DFG_JIT)
 103 #include &quot;DFGOperations.h&quot;
 104 #endif
 105 
 106 #if ENABLE(FTL_JIT)
 107 #include &quot;FTLJITCode.h&quot;
 108 #endif
 109 
 110 namespace JSC {



 111 
 112 const ClassInfo CodeBlock::s_info = {
 113     &quot;CodeBlock&quot;, nullptr, nullptr, nullptr,
 114     CREATE_METHOD_TABLE(CodeBlock)
 115 };
 116 
 117 CString CodeBlock::inferredName() const
 118 {
 119     switch (codeType()) {
 120     case GlobalCode:
 121         return &quot;&lt;global&gt;&quot;;
 122     case EvalCode:
 123         return &quot;&lt;eval&gt;&quot;;
 124     case FunctionCode:
<span class="line-modified"> 125         return jsCast&lt;FunctionExecutable*&gt;(ownerExecutable())-&gt;ecmaName().utf8();</span>
 126     case ModuleCode:
 127         return &quot;&lt;module&gt;&quot;;
 128     default:
 129         CRASH();
 130         return CString(&quot;&quot;, 0);
 131     }
 132 }
 133 
 134 bool CodeBlock::hasHash() const
 135 {
 136     return !!m_hash;
 137 }
 138 
 139 bool CodeBlock::isSafeToComputeHash() const
 140 {
 141     return !isCompilationThread();
 142 }
 143 
 144 CodeBlockHash CodeBlock::hash() const
 145 {
</pre>
<hr />
<pre>
 163     int delta = linkedStartOffset - unlinkedStartOffset;
 164     unsigned rangeStart = delta + unlinked-&gt;unlinkedFunctionNameStart();
 165     unsigned rangeEnd = delta + unlinked-&gt;startOffset() + unlinked-&gt;sourceLength();
 166     return toCString(
 167         &quot;function &quot;,
 168         provider-&gt;source().substring(rangeStart, rangeEnd - rangeStart).utf8());
 169 }
 170 
 171 CString CodeBlock::sourceCodeOnOneLine() const
 172 {
 173     return reduceWhitespace(sourceCodeForTools());
 174 }
 175 
 176 CString CodeBlock::hashAsStringIfPossible() const
 177 {
 178     if (hasHash() || isSafeToComputeHash())
 179         return toCString(hash());
 180     return &quot;&lt;no-hash&gt;&quot;;
 181 }
 182 
<span class="line-modified"> 183 void CodeBlock::dumpAssumingJITType(PrintStream&amp; out, JITType jitType) const</span>
 184 {
 185     out.print(inferredName(), &quot;#&quot;, hashAsStringIfPossible());
 186     out.print(&quot;:[&quot;, RawPointer(this), &quot;-&gt;&quot;);
 187     if (!!m_alternative)
 188         out.print(RawPointer(alternative()), &quot;-&gt;&quot;);
 189     out.print(RawPointer(ownerExecutable()), &quot;, &quot;, jitType, codeType());
 190 
 191     if (codeType() == FunctionCode)
 192         out.print(specializationKind());
<span class="line-modified"> 193     out.print(&quot;, &quot;, instructionsSize());</span>
<span class="line-modified"> 194     if (this-&gt;jitType() == JITType::BaselineJIT &amp;&amp; m_shouldAlwaysBeInlined)</span>
 195         out.print(&quot; (ShouldAlwaysBeInlined)&quot;);
 196     if (ownerExecutable()-&gt;neverInline())
 197         out.print(&quot; (NeverInline)&quot;);
 198     if (ownerExecutable()-&gt;neverOptimize())
 199         out.print(&quot; (NeverOptimize)&quot;);
 200     else if (ownerExecutable()-&gt;neverFTLOptimize())
 201         out.print(&quot; (NeverFTLOptimize)&quot;);
 202     if (ownerExecutable()-&gt;didTryToEnterInLoop())
 203         out.print(&quot; (DidTryToEnterInLoop)&quot;);
 204     if (ownerExecutable()-&gt;isStrictMode())
 205         out.print(&quot; (StrictMode)&quot;);
 206     if (m_didFailJITCompilation)
 207         out.print(&quot; (JITFail)&quot;);
<span class="line-modified"> 208     if (this-&gt;jitType() == JITType::BaselineJIT &amp;&amp; m_didFailFTLCompilation)</span>
 209         out.print(&quot; (FTLFail)&quot;);
<span class="line-modified"> 210     if (this-&gt;jitType() == JITType::BaselineJIT &amp;&amp; m_hasBeenCompiledWithFTL)</span>
 211         out.print(&quot; (HadFTLReplacement)&quot;);
 212     out.print(&quot;]&quot;);
 213 }
 214 
 215 void CodeBlock::dump(PrintStream&amp; out) const
 216 {
 217     dumpAssumingJITType(out, jitType());
 218 }
 219 
 220 void CodeBlock::dumpSource()
 221 {
 222     dumpSource(WTF::dataFile());
 223 }
 224 
 225 void CodeBlock::dumpSource(PrintStream&amp; out)
 226 {
 227     ScriptExecutable* executable = ownerExecutable();
 228     if (executable-&gt;isFunctionExecutable()) {
 229         FunctionExecutable* functionExecutable = reinterpret_cast&lt;FunctionExecutable*&gt;(executable);
 230         StringView source = functionExecutable-&gt;source().provider()-&gt;getRange(
 231             functionExecutable-&gt;parametersStartOffset(),
<span class="line-modified"> 232             functionExecutable-&gt;typeProfilingEndOffset(vm()) + 1); // Type profiling end offset is the character before the &#39;}&#39;.</span>
 233 
 234         out.print(&quot;function &quot;, inferredName(), source);
 235         return;
 236     }
 237     out.print(executable-&gt;source().view());
 238 }
 239 
 240 void CodeBlock::dumpBytecode()
 241 {
 242     dumpBytecode(WTF::dataFile());
 243 }
 244 
 245 void CodeBlock::dumpBytecode(PrintStream&amp; out)
 246 {
 247     ICStatusMap statusMap;
 248     getICStatusMap(statusMap);
 249     BytecodeDumper&lt;CodeBlock&gt;::dumpBlock(this, instructions(), out, statusMap);
 250 }
 251 
 252 void CodeBlock::dumpBytecode(PrintStream&amp; out, const InstructionStream::Ref&amp; it, const ICStatusMap&amp; statusMap)
</pre>
<hr />
<pre>
 265 class PutToScopeFireDetail : public FireDetail {
 266 public:
 267     PutToScopeFireDetail(CodeBlock* codeBlock, const Identifier&amp; ident)
 268         : m_codeBlock(codeBlock)
 269         , m_ident(ident)
 270     {
 271     }
 272 
 273     void dump(PrintStream&amp; out) const override
 274     {
 275         out.print(&quot;Linking put_to_scope in &quot;, FunctionExecutableDump(jsCast&lt;FunctionExecutable*&gt;(m_codeBlock-&gt;ownerExecutable())), &quot; for &quot;, m_ident);
 276     }
 277 
 278 private:
 279     CodeBlock* m_codeBlock;
 280     const Identifier&amp; m_ident;
 281 };
 282 
 283 } // anonymous namespace
 284 
<span class="line-modified"> 285 CodeBlock::CodeBlock(VM&amp; vm, Structure* structure, CopyParsedBlockTag, CodeBlock&amp; other)</span>
<span class="line-modified"> 286     : JSCell(vm, structure)</span>
 287     , m_globalObject(other.m_globalObject)
 288     , m_shouldAlwaysBeInlined(true)
 289 #if ENABLE(JIT)
 290     , m_capabilityLevelState(DFG::CapabilityLevelNotSet)
 291 #endif
 292     , m_didFailJITCompilation(false)
 293     , m_didFailFTLCompilation(false)
 294     , m_hasBeenCompiledWithFTL(false)
 295     , m_numCalleeLocals(other.m_numCalleeLocals)
 296     , m_numVars(other.m_numVars)
 297     , m_numberOfArgumentsToSkip(other.m_numberOfArgumentsToSkip)
 298     , m_hasDebuggerStatement(false)
 299     , m_steppingMode(SteppingModeDisabled)
 300     , m_numBreakpoints(0)
<span class="line-modified"> 301     , m_bytecodeCost(other.m_bytecodeCost)</span>
 302     , m_scopeRegister(other.m_scopeRegister)
 303     , m_hash(other.m_hash)
<span class="line-modified"> 304     , m_unlinkedCode(other.vm(), this, other.m_unlinkedCode.get())</span>
<span class="line-modified"> 305     , m_ownerExecutable(other.vm(), this, other.m_ownerExecutable.get())</span>
 306     , m_vm(other.m_vm)
 307     , m_instructionsRawPointer(other.m_instructionsRawPointer)
 308     , m_constantRegisters(other.m_constantRegisters)
 309     , m_constantsSourceCodeRepresentation(other.m_constantsSourceCodeRepresentation)
 310     , m_functionDecls(other.m_functionDecls)
 311     , m_functionExprs(other.m_functionExprs)
 312     , m_osrExitCounter(0)
 313     , m_optimizationDelayCounter(0)
 314     , m_reoptimizationRetryCounter(0)
 315     , m_metadata(other.m_metadata)
 316     , m_creationTime(MonotonicTime::now())
 317 {
 318     ASSERT(heap()-&gt;isDeferred());
 319     ASSERT(m_scopeRegister.isLocal());
 320 
 321     ASSERT(source().provider());
 322     setNumParameters(other.numParameters());
 323 
<span class="line-modified"> 324     vm.heap.codeBlockSet().add(this);</span>
 325 }
 326 
 327 void CodeBlock::finishCreation(VM&amp; vm, CopyParsedBlockTag, CodeBlock&amp; other)
 328 {
 329     Base::finishCreation(vm);
 330     finishCreationCommon(vm);
 331 
 332     optimizeAfterWarmUp();
 333     jitAfterWarmUp();
 334 
 335     if (other.m_rareData) {
 336         createRareDataIfNecessary();
 337 
 338         m_rareData-&gt;m_exceptionHandlers = other.m_rareData-&gt;m_exceptionHandlers;
 339         m_rareData-&gt;m_switchJumpTables = other.m_rareData-&gt;m_switchJumpTables;
 340         m_rareData-&gt;m_stringSwitchJumpTables = other.m_rareData-&gt;m_stringSwitchJumpTables;
 341     }
 342 }
 343 
<span class="line-modified"> 344 CodeBlock::CodeBlock(VM&amp; vm, Structure* structure, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock* unlinkedCodeBlock, JSScope* scope)</span>
<span class="line-modified"> 345     : JSCell(vm, structure)</span>
<span class="line-modified"> 346     , m_globalObject(vm, this, scope-&gt;globalObject(vm))</span>
 347     , m_shouldAlwaysBeInlined(true)
 348 #if ENABLE(JIT)
 349     , m_capabilityLevelState(DFG::CapabilityLevelNotSet)
 350 #endif
 351     , m_didFailJITCompilation(false)
 352     , m_didFailFTLCompilation(false)
 353     , m_hasBeenCompiledWithFTL(false)
 354     , m_numCalleeLocals(unlinkedCodeBlock-&gt;numCalleeLocals())
 355     , m_numVars(unlinkedCodeBlock-&gt;numVars())
 356     , m_hasDebuggerStatement(false)
 357     , m_steppingMode(SteppingModeDisabled)
 358     , m_numBreakpoints(0)
 359     , m_scopeRegister(unlinkedCodeBlock-&gt;scopeRegister())
<span class="line-modified"> 360     , m_unlinkedCode(vm, this, unlinkedCodeBlock)</span>
<span class="line-modified"> 361     , m_ownerExecutable(vm, this, ownerExecutable)</span>
<span class="line-modified"> 362     , m_vm(&amp;vm)</span>
 363     , m_instructionsRawPointer(unlinkedCodeBlock-&gt;instructions().rawPointer())
 364     , m_osrExitCounter(0)
 365     , m_optimizationDelayCounter(0)
 366     , m_reoptimizationRetryCounter(0)
 367     , m_metadata(unlinkedCodeBlock-&gt;metadata().link())
 368     , m_creationTime(MonotonicTime::now())
 369 {
 370     ASSERT(heap()-&gt;isDeferred());
 371     ASSERT(m_scopeRegister.isLocal());
 372 
 373     ASSERT(source().provider());
 374     setNumParameters(unlinkedCodeBlock-&gt;numParameters());
 375 
<span class="line-modified"> 376     vm.heap.codeBlockSet().add(this);</span>
 377 }
 378 
 379 // The main purpose of this function is to generate linked bytecode from unlinked bytecode. The process
 380 // of linking is taking an abstract representation of bytecode and tying it to a GlobalObject and scope
 381 // chain. For example, this process allows us to cache the depth of lexical environment reads that reach
 382 // outside of this CodeBlock&#39;s compilation unit. It also allows us to generate particular constants that
 383 // we can&#39;t generate during unlinked bytecode generation. This process is not allowed to generate control
 384 // flow or introduce new locals. The reason for this is we rely on liveness analysis to be the same for
 385 // all the CodeBlocks of an UnlinkedCodeBlock. We rely on this fact by caching the liveness analysis
 386 // inside UnlinkedCodeBlock.
 387 bool CodeBlock::finishCreation(VM&amp; vm, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock* unlinkedCodeBlock,
 388     JSScope* scope)
 389 {
 390     Base::finishCreation(vm);
 391     finishCreationCommon(vm);
 392 
 393     auto throwScope = DECLARE_THROW_SCOPE(vm);
 394 
<span class="line-modified"> 395     if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes() || m_unlinkedCode-&gt;wasCompiledWithControlFlowProfilerOpcodes())</span>
 396         vm.functionHasExecutedCache()-&gt;removeUnexecutedRange(ownerExecutable-&gt;sourceID(), ownerExecutable-&gt;typeProfilingStartOffset(vm), ownerExecutable-&gt;typeProfilingEndOffset(vm));
 397 
<span class="line-modified"> 398     ScriptExecutable* topLevelExecutable = ownerExecutable-&gt;topLevelExecutable();</span>
<span class="line-added"> 399     setConstantRegisters(unlinkedCodeBlock-&gt;constantRegisters(), unlinkedCodeBlock-&gt;constantsSourceCodeRepresentation(), topLevelExecutable);</span>
 400     RETURN_IF_EXCEPTION(throwScope, false);
 401 
 402     for (unsigned i = 0; i &lt; LinkTimeConstantCount; i++) {
 403         LinkTimeConstant type = static_cast&lt;LinkTimeConstant&gt;(i);
 404         if (unsigned registerIndex = unlinkedCodeBlock-&gt;registerIndexForLinkTimeConstant(type))
 405             m_constantRegisters[registerIndex].set(vm, this, m_globalObject-&gt;jsCellForLinkTimeConstant(type));
 406     }
 407 
 408     // We already have the cloned symbol table for the module environment since we need to instantiate
 409     // the module environments before linking the code block. We replace the stored symbol table with the already cloned one.
 410     if (UnlinkedModuleProgramCodeBlock* unlinkedModuleProgramCodeBlock = jsDynamicCast&lt;UnlinkedModuleProgramCodeBlock*&gt;(vm, unlinkedCodeBlock)) {
 411         SymbolTable* clonedSymbolTable = jsCast&lt;ModuleProgramExecutable*&gt;(ownerExecutable)-&gt;moduleEnvironmentSymbolTable();
<span class="line-modified"> 412         if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes()) {</span>
 413             ConcurrentJSLocker locker(clonedSymbolTable-&gt;m_lock);
 414             clonedSymbolTable-&gt;prepareForTypeProfiling(locker);
 415         }
 416         replaceConstant(unlinkedModuleProgramCodeBlock-&gt;moduleEnvironmentSymbolTableConstantRegisterOffset(), clonedSymbolTable);
 417     }
 418 
<span class="line-modified"> 419     bool shouldUpdateFunctionHasExecutedCache = m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes() || m_unlinkedCode-&gt;wasCompiledWithControlFlowProfilerOpcodes();</span>
 420     m_functionDecls = RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt;(unlinkedCodeBlock-&gt;numberOfFunctionDecls());
 421     for (size_t count = unlinkedCodeBlock-&gt;numberOfFunctionDecls(), i = 0; i &lt; count; ++i) {
 422         UnlinkedFunctionExecutable* unlinkedExecutable = unlinkedCodeBlock-&gt;functionDecl(i);
 423         if (shouldUpdateFunctionHasExecutedCache)
 424             vm.functionHasExecutedCache()-&gt;insertUnexecutedRange(ownerExecutable-&gt;sourceID(), unlinkedExecutable-&gt;typeProfilingStartOffset(), unlinkedExecutable-&gt;typeProfilingEndOffset());
<span class="line-modified"> 425         m_functionDecls[i].set(vm, this, unlinkedExecutable-&gt;link(vm, topLevelExecutable, ownerExecutable-&gt;source()));</span>
 426     }
 427 
 428     m_functionExprs = RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt;(unlinkedCodeBlock-&gt;numberOfFunctionExprs());
 429     for (size_t count = unlinkedCodeBlock-&gt;numberOfFunctionExprs(), i = 0; i &lt; count; ++i) {
 430         UnlinkedFunctionExecutable* unlinkedExecutable = unlinkedCodeBlock-&gt;functionExpr(i);
 431         if (shouldUpdateFunctionHasExecutedCache)
 432             vm.functionHasExecutedCache()-&gt;insertUnexecutedRange(ownerExecutable-&gt;sourceID(), unlinkedExecutable-&gt;typeProfilingStartOffset(), unlinkedExecutable-&gt;typeProfilingEndOffset());
<span class="line-modified"> 433         m_functionExprs[i].set(vm, this, unlinkedExecutable-&gt;link(vm, topLevelExecutable, ownerExecutable-&gt;source()));</span>
 434     }
 435 
 436     if (unlinkedCodeBlock-&gt;hasRareData()) {
 437         createRareDataIfNecessary();
 438 
 439         setConstantIdentifierSetRegisters(vm, unlinkedCodeBlock-&gt;constantIdentifierSets());
 440         RETURN_IF_EXCEPTION(throwScope, false);
 441 
 442         if (size_t count = unlinkedCodeBlock-&gt;numberOfExceptionHandlers()) {
 443             m_rareData-&gt;m_exceptionHandlers.resizeToFit(count);
 444             for (size_t i = 0; i &lt; count; i++) {
 445                 const UnlinkedHandlerInfo&amp; unlinkedHandler = unlinkedCodeBlock-&gt;exceptionHandler(i);
 446                 HandlerInfo&amp; handler = m_rareData-&gt;m_exceptionHandlers[i];
 447 #if ENABLE(JIT)
<span class="line-modified"> 448                 auto instruction = instructions().at(unlinkedHandler.target);</span>
<span class="line-modified"> 449                 MacroAssemblerCodePtr&lt;BytecodePtrTag&gt; codePtr;</span>
<span class="line-modified"> 450                 if (instruction-&gt;isWide32())</span>
<span class="line-added"> 451                     codePtr = LLInt::getWide32CodePtr&lt;BytecodePtrTag&gt;(op_catch);</span>
<span class="line-added"> 452                 else if (instruction-&gt;isWide16())</span>
<span class="line-added"> 453                     codePtr = LLInt::getWide16CodePtr&lt;BytecodePtrTag&gt;(op_catch);</span>
<span class="line-added"> 454                 else</span>
<span class="line-added"> 455                     codePtr = LLInt::getCodePtr&lt;BytecodePtrTag&gt;(op_catch);</span>
 456                 handler.initialize(unlinkedHandler, CodeLocationLabel&lt;ExceptionHandlerPtrTag&gt;(codePtr.retagged&lt;ExceptionHandlerPtrTag&gt;()));
 457 #else
 458                 handler.initialize(unlinkedHandler);
 459 #endif
 460             }
 461         }
 462 
 463         if (size_t count = unlinkedCodeBlock-&gt;numberOfStringSwitchJumpTables()) {
 464             m_rareData-&gt;m_stringSwitchJumpTables.grow(count);
 465             for (size_t i = 0; i &lt; count; i++) {
 466                 UnlinkedStringJumpTable::StringOffsetTable::iterator ptr = unlinkedCodeBlock-&gt;stringSwitchJumpTable(i).offsetTable.begin();
 467                 UnlinkedStringJumpTable::StringOffsetTable::iterator end = unlinkedCodeBlock-&gt;stringSwitchJumpTable(i).offsetTable.end();
 468                 for (; ptr != end; ++ptr) {
 469                     OffsetLocation offset;
 470                     offset.branchOffset = ptr-&gt;value.branchOffset;
 471                     m_rareData-&gt;m_stringSwitchJumpTables[i].offsetTable.add(ptr-&gt;key, offset);
 472                 }
 473             }
 474         }
 475 
 476         if (size_t count = unlinkedCodeBlock-&gt;numberOfSwitchJumpTables()) {
 477             m_rareData-&gt;m_switchJumpTables.grow(count);
 478             for (size_t i = 0; i &lt; count; i++) {
 479                 UnlinkedSimpleJumpTable&amp; sourceTable = unlinkedCodeBlock-&gt;switchJumpTable(i);
 480                 SimpleJumpTable&amp; destTable = m_rareData-&gt;m_switchJumpTables[i];
 481                 destTable.branchOffsets = sourceTable.branchOffsets;
 482                 destTable.min = sourceTable.min;
 483             }
 484         }
 485     }
 486 
 487     // Bookkeep the strongly referenced module environments.
 488     HashSet&lt;JSModuleEnvironment*&gt; stronglyReferencedModuleEnvironments;
 489 
<span class="line-modified"> 490     auto link_profile = [&amp;](const auto&amp; /*instruction*/, auto /*bytecode*/, auto&amp; /*metadata*/) {</span>
 491         m_numberOfNonArgumentValueProfiles++;





 492     };
 493 
 494     auto link_objectAllocationProfile = [&amp;](const auto&amp; /*instruction*/, auto bytecode, auto&amp; metadata) {
 495         metadata.m_objectAllocationProfile.initializeProfile(vm, m_globalObject.get(), this, m_globalObject-&gt;objectPrototype(), bytecode.m_inlineCapacity);
 496     };
 497 
 498     auto link_arrayAllocationProfile = [&amp;](const auto&amp; /*instruction*/, auto bytecode, auto&amp; metadata) {
 499         metadata.m_arrayAllocationProfile.initializeIndexingMode(bytecode.m_recommendedIndexingType);
 500     };
 501 




 502 #define LINK_FIELD(__field) \
 503     WTF_LAZY_JOIN(link_, __field)(instruction, bytecode, metadata);
 504 
 505 #define INITIALIZE_METADATA(__op) \
 506     auto bytecode = instruction-&gt;as&lt;__op&gt;(); \
 507     auto&amp; metadata = bytecode.metadata(this); \
 508     new (&amp;metadata) __op::Metadata { bytecode }; \
 509 
 510 #define CASE(__op) case __op::opcodeID
 511 
 512 #define LINK(...) \
 513     CASE(WTF_LAZY_FIRST(__VA_ARGS__)): { \
 514         INITIALIZE_METADATA(WTF_LAZY_FIRST(__VA_ARGS__)) \
 515         WTF_LAZY_HAS_REST(__VA_ARGS__)({ \
 516             WTF_LAZY_FOR_EACH_TERM(LINK_FIELD,  WTF_LAZY_REST_(__VA_ARGS__)) \
 517         }) \
 518         break; \
 519     }
 520 
 521     const InstructionStream&amp; instructionStream = instructions();
 522     for (const auto&amp; instruction : instructionStream) {
 523         OpcodeID opcodeID = instruction-&gt;opcodeID();
<span class="line-modified"> 524         m_bytecodeCost += opcodeLengths[opcodeID];</span>
 525         switch (opcodeID) {
<span class="line-modified"> 526         LINK(OpHasIndexedProperty)</span>
 527 
<span class="line-modified"> 528         LINK(OpCallVarargs, profile)</span>
<span class="line-modified"> 529         LINK(OpTailCallVarargs, profile)</span>
<span class="line-modified"> 530         LINK(OpTailCallForwardArguments, profile)</span>
<span class="line-modified"> 531         LINK(OpConstructVarargs, profile)</span>
<span class="line-modified"> 532         LINK(OpGetByVal, profile)</span>
 533 
 534         LINK(OpGetDirectPname, profile)
 535         LINK(OpGetByIdWithThis, profile)
 536         LINK(OpTryGetById, profile)
 537         LINK(OpGetByIdDirect, profile)
 538         LINK(OpGetByValWithThis, profile)
 539         LINK(OpGetFromArguments, profile)
 540         LINK(OpToNumber, profile)
 541         LINK(OpToObject, profile)
 542         LINK(OpGetArgument, profile)
 543         LINK(OpToThis, profile)
 544         LINK(OpBitand, profile)
 545         LINK(OpBitor, profile)
 546         LINK(OpBitnot, profile)
 547         LINK(OpBitxor, profile)
<span class="line-added"> 548         LINK(OpLshift, profile)</span>
 549 
<span class="line-modified"> 550         LINK(OpGetById, profile)</span>
 551 
<span class="line-modified"> 552         LINK(OpCall, profile)</span>
<span class="line-modified"> 553         LINK(OpTailCall, profile)</span>
<span class="line-modified"> 554         LINK(OpCallEval, profile)</span>
<span class="line-modified"> 555         LINK(OpConstruct, profile)</span>
 556 
<span class="line-modified"> 557         LINK(OpInByVal)</span>
<span class="line-modified"> 558         LINK(OpPutByVal)</span>
<span class="line-modified"> 559         LINK(OpPutByValDirect)</span>
 560 
 561         LINK(OpNewArray)
 562         LINK(OpNewArrayWithSize)
 563         LINK(OpNewArrayBuffer, arrayAllocationProfile)
 564 
 565         LINK(OpNewObject, objectAllocationProfile)
 566 
 567         LINK(OpPutById)
 568         LINK(OpCreateThis)
 569 
 570         LINK(OpAdd)
 571         LINK(OpMul)
 572         LINK(OpDiv)
 573         LINK(OpSub)
 574 
 575         LINK(OpNegate)
 576 
 577         LINK(OpJneqPtr)
 578 
 579         LINK(OpCatch)
</pre>
<hr />
<pre>
 586             RELEASE_ASSERT(bytecode.m_resolveType != LocalClosureVar);
 587 
 588             ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), bytecode.m_localScopeDepth, scope, ident, Get, bytecode.m_resolveType, InitializationMode::NotInitialization);
 589             RETURN_IF_EXCEPTION(throwScope, false);
 590 
 591             metadata.m_resolveType = op.type;
 592             metadata.m_localScopeDepth = op.depth;
 593             if (op.lexicalEnvironment) {
 594                 if (op.type == ModuleVar) {
 595                     // Keep the linked module environment strongly referenced.
 596                     if (stronglyReferencedModuleEnvironments.add(jsCast&lt;JSModuleEnvironment*&gt;(op.lexicalEnvironment)).isNewEntry)
 597                         addConstant(op.lexicalEnvironment);
 598                     metadata.m_lexicalEnvironment.set(vm, this, op.lexicalEnvironment);
 599                 } else
 600                     metadata.m_symbolTable.set(vm, this, op.lexicalEnvironment-&gt;symbolTable());
 601             } else if (JSScope* constantScope = JSScope::constantScopeForCodeBlock(op.type, this)) {
 602                 metadata.m_constantScope.set(vm, this, constantScope);
 603                 if (op.type == GlobalProperty || op.type == GlobalPropertyWithVarInjectionChecks)
 604                     metadata.m_globalLexicalBindingEpoch = m_globalObject-&gt;globalLexicalBindingEpoch();
 605             } else
<span class="line-modified"> 606                 metadata.m_globalObject.clear();</span>
 607             break;
 608         }
 609 
 610         case op_get_from_scope: {
 611             INITIALIZE_METADATA(OpGetFromScope)
 612 
 613             link_profile(instruction, bytecode, metadata);
 614             metadata.m_watchpointSet = nullptr;
 615 
 616             ASSERT(!isInitialization(bytecode.m_getPutInfo.initializationMode()));
 617             if (bytecode.m_getPutInfo.resolveType() == LocalClosureVar) {
 618                 metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), ClosureVar, bytecode.m_getPutInfo.initializationMode());
 619                 break;
 620             }
 621 
 622             const Identifier&amp; ident = identifier(bytecode.m_var);
 623             ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), bytecode.m_localScopeDepth, scope, ident, Get, bytecode.m_getPutInfo.resolveType(), InitializationMode::NotInitialization);
 624             RETURN_IF_EXCEPTION(throwScope, false);
 625 
 626             metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), op.type, bytecode.m_getPutInfo.initializationMode());
 627             if (op.type == ModuleVar)
 628                 metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), ClosureVar, bytecode.m_getPutInfo.initializationMode());
 629             if (op.type == GlobalVar || op.type == GlobalVarWithVarInjectionChecks || op.type == GlobalLexicalVar || op.type == GlobalLexicalVarWithVarInjectionChecks)
 630                 metadata.m_watchpointSet = op.watchpointSet;
 631             else if (op.structure)
 632                 metadata.m_structure.set(vm, this, op.structure);
 633             metadata.m_operand = op.operand;
 634             break;
 635         }
 636 
 637         case op_put_to_scope: {
 638             INITIALIZE_METADATA(OpPutToScope)
 639 
 640             if (bytecode.m_getPutInfo.resolveType() == LocalClosureVar) {
 641                 // Only do watching if the property we&#39;re putting to is not anonymous.
 642                 if (bytecode.m_var != UINT_MAX) {
<span class="line-modified"> 643                     SymbolTable* symbolTable = jsCast&lt;SymbolTable*&gt;(getConstant(bytecode.m_symbolTableOrScopeDepth.symbolTable().offset()));</span>
 644                     const Identifier&amp; ident = identifier(bytecode.m_var);
 645                     ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 646                     auto iter = symbolTable-&gt;find(locker, ident.impl());
 647                     ASSERT(iter != symbolTable-&gt;end(locker));
 648                     iter-&gt;value.prepareToWatch();
 649                     metadata.m_watchpointSet = iter-&gt;value.watchpointSet();
 650                 } else
 651                     metadata.m_watchpointSet = nullptr;
 652                 break;
 653             }
 654 
 655             const Identifier&amp; ident = identifier(bytecode.m_var);
 656             metadata.m_watchpointSet = nullptr;
<span class="line-modified"> 657             ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), bytecode.m_symbolTableOrScopeDepth.scopeDepth(), scope, ident, Put, bytecode.m_getPutInfo.resolveType(), bytecode.m_getPutInfo.initializationMode());</span>
 658             RETURN_IF_EXCEPTION(throwScope, false);
 659 
 660             metadata.m_getPutInfo = GetPutInfo(bytecode.m_getPutInfo.resolveMode(), op.type, bytecode.m_getPutInfo.initializationMode());
 661             if (op.type == GlobalVar || op.type == GlobalVarWithVarInjectionChecks || op.type == GlobalLexicalVar || op.type == GlobalLexicalVarWithVarInjectionChecks)
 662                 metadata.m_watchpointSet = op.watchpointSet;
 663             else if (op.type == ClosureVar || op.type == ClosureVarWithVarInjectionChecks) {
 664                 if (op.watchpointSet)
 665                     op.watchpointSet-&gt;invalidate(vm, PutToScopeFireDetail(this, ident));
 666             } else if (op.structure)
 667                 metadata.m_structure.set(vm, this, op.structure);
 668             metadata.m_operand = op.operand;
 669             break;
 670         }
 671 
 672         case op_profile_type: {
<span class="line-modified"> 673             RELEASE_ASSERT(m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes());</span>
 674 
 675             INITIALIZE_METADATA(OpProfileType)
 676 
 677             size_t instructionOffset = instruction.offset() + instruction-&gt;size() - 1;
 678             unsigned divotStart, divotEnd;
 679             GlobalVariableID globalVariableID = 0;
 680             RefPtr&lt;TypeSet&gt; globalTypeSet;
 681             bool shouldAnalyze = m_unlinkedCode-&gt;typeProfilerExpressionInfoForBytecodeOffset(instructionOffset, divotStart, divotEnd);
 682             SymbolTable* symbolTable = nullptr;
 683 
 684             switch (bytecode.m_flag) {
 685             case ProfileTypeBytecodeClosureVar: {
 686                 const Identifier&amp; ident = identifier(bytecode.m_identifier);
<span class="line-modified"> 687                 unsigned localScopeDepth = bytecode.m_symbolTableOrScopeDepth.scopeDepth();</span>
 688                 // Even though type profiling may be profiling either a Get or a Put, we can always claim a Get because
 689                 // we&#39;re abstractly &quot;read&quot;ing from a JSScope.
 690                 ResolveOp op = JSScope::abstractResolve(m_globalObject-&gt;globalExec(), localScopeDepth, scope, ident, Get, bytecode.m_resolveType, InitializationMode::NotInitialization);
 691                 RETURN_IF_EXCEPTION(throwScope, false);
 692 
 693                 if (op.type == ClosureVar || op.type == ModuleVar)
 694                     symbolTable = op.lexicalEnvironment-&gt;symbolTable();
 695                 else if (op.type == GlobalVar)
 696                     symbolTable = m_globalObject.get()-&gt;symbolTable();
 697 
 698                 UniquedStringImpl* impl = (op.type == ModuleVar) ? op.importedName.get() : ident.impl();
 699                 if (symbolTable) {
 700                     ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 701                     // If our parent scope was created while profiling was disabled, it will not have prepared for profiling yet.
 702                     symbolTable-&gt;prepareForTypeProfiling(locker);
 703                     globalVariableID = symbolTable-&gt;uniqueIDForVariable(locker, impl, vm);
 704                     globalTypeSet = symbolTable-&gt;globalTypeSetForVariable(locker, impl, vm);
 705                 } else
 706                     globalVariableID = TypeProfilerNoGlobalIDExists;
 707 
 708                 break;
 709             }
 710             case ProfileTypeBytecodeLocallyResolved: {
<span class="line-modified"> 711                 int symbolTableIndex = bytecode.m_symbolTableOrScopeDepth.symbolTable().offset();</span>
 712                 SymbolTable* symbolTable = jsCast&lt;SymbolTable*&gt;(getConstant(symbolTableIndex));
 713                 const Identifier&amp; ident = identifier(bytecode.m_identifier);
 714                 ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 715                 // If our parent scope was created while profiling was disabled, it will not have prepared for profiling yet.
 716                 globalVariableID = symbolTable-&gt;uniqueIDForVariable(locker, ident.impl(), vm);
 717                 globalTypeSet = symbolTable-&gt;globalTypeSetForVariable(locker, ident.impl(), vm);
 718 
 719                 break;
 720             }
 721             case ProfileTypeBytecodeDoesNotHaveGlobalID:
 722             case ProfileTypeBytecodeFunctionArgument: {
 723                 globalVariableID = TypeProfilerNoGlobalIDExists;
 724                 break;
 725             }
 726             case ProfileTypeBytecodeFunctionReturnStatement: {
 727                 RELEASE_ASSERT(ownerExecutable-&gt;isFunctionExecutable());
 728                 globalTypeSet = jsCast&lt;FunctionExecutable*&gt;(ownerExecutable)-&gt;returnStatementTypeSet();
 729                 globalVariableID = TypeProfilerReturnStatement;
 730                 if (!shouldAnalyze) {
 731                     // Because a return statement can be added implicitly to return undefined at the end of a function,
</pre>
<hr />
<pre>
 762         }
 763 
 764         case op_create_rest: {
 765             int numberOfArgumentsToSkip = instruction-&gt;as&lt;OpCreateRest&gt;().m_numParametersToSkip;
 766             ASSERT_UNUSED(numberOfArgumentsToSkip, numberOfArgumentsToSkip &gt;= 0);
 767             // This is used when rematerializing the rest parameter during OSR exit in the FTL JIT.&quot;);
 768             m_numberOfArgumentsToSkip = numberOfArgumentsToSkip;
 769             break;
 770         }
 771 
 772         default:
 773             break;
 774         }
 775     }
 776 
 777 #undef CASE
 778 #undef INITIALIZE_METADATA
 779 #undef LINK_FIELD
 780 #undef LINK
 781 
<span class="line-modified"> 782     if (m_unlinkedCode-&gt;wasCompiledWithControlFlowProfilerOpcodes())</span>
 783         insertBasicBlockBoundariesForControlFlowProfiler();
 784 
 785     // Set optimization thresholds only after instructions is initialized, since these
 786     // rely on the instruction count (and are in theory permitted to also inspect the
 787     // instruction stream to more accurate assess the cost of tier-up).
 788     optimizeAfterWarmUp();
 789     jitAfterWarmUp();
 790 
 791     // If the concurrent thread will want the code block&#39;s hash, then compute it here
 792     // synchronously.
 793     if (Options::alwaysComputeHash())
 794         hash();
 795 
 796     if (Options::dumpGeneratedBytecodes())
 797         dumpBytecode();
 798 
 799     if (m_metadata)
 800         vm.heap.reportExtraMemoryAllocated(m_metadata-&gt;sizeInBytes());
 801 
 802     return true;
 803 }
 804 
 805 void CodeBlock::finishCreationCommon(VM&amp; vm)
 806 {
 807     m_ownerEdge.set(vm, this, ExecutableToCodeBlockEdge::create(vm, this));
 808 }
 809 
 810 CodeBlock::~CodeBlock()
 811 {
 812     VM&amp; vm = *m_vm;
 813 
<span class="line-added"> 814 #if ENABLE(DFG_JIT)</span>
<span class="line-added"> 815     // The JITCode (and its corresponding DFG::CommonData) may outlive the CodeBlock by</span>
<span class="line-added"> 816     // a short amount of time after the CodeBlock is destructed. For example, the</span>
<span class="line-added"> 817     // Interpreter::execute methods will ref JITCode before invoking it. This can</span>
<span class="line-added"> 818     // result in the JITCode having a non-zero refCount when its owner CodeBlock is</span>
<span class="line-added"> 819     // destructed.</span>
<span class="line-added"> 820     //</span>
<span class="line-added"> 821     // Hence, we cannot rely on DFG::CommonData destruction to clear these now invalid</span>
<span class="line-added"> 822     // watchpoints in a timely manner. We&#39;ll ensure they are cleared here eagerly.</span>
<span class="line-added"> 823     //</span>
<span class="line-added"> 824     // We only need to do this for a DFG/FTL CodeBlock because only these will have a</span>
<span class="line-added"> 825     // DFG:CommonData. Hence, the LLInt and Baseline will not have any of these watchpoints.</span>
<span class="line-added"> 826     //</span>
<span class="line-added"> 827     // Note also that the LLIntPrototypeLoadAdaptiveStructureWatchpoint is also related</span>
<span class="line-added"> 828     // to the CodeBlock. However, its lifecycle is tied directly to the CodeBlock, and</span>
<span class="line-added"> 829     // will be automatically cleared when the CodeBlock destructs.</span>
<span class="line-added"> 830 </span>
<span class="line-added"> 831     if (JITCode::isOptimizingJIT(jitType()))</span>
<span class="line-added"> 832         jitCode()-&gt;dfgCommon()-&gt;clearWatchpoints();</span>
<span class="line-added"> 833 #endif</span>
 834     vm.heap.codeBlockSet().remove(this);
 835 
 836     if (UNLIKELY(vm.m_perBytecodeProfiler))
 837         vm.m_perBytecodeProfiler-&gt;notifyDestruction(this);
 838 
 839     if (!vm.heap.isShuttingDown() &amp;&amp; unlinkedCodeBlock()-&gt;didOptimize() == MixedTriState)
 840         unlinkedCodeBlock()-&gt;setDidOptimize(FalseTriState);
 841 
 842 #if ENABLE(VERBOSE_VALUE_PROFILE)
 843     dumpValueProfiles();
 844 #endif
 845 
 846     // We may be destroyed before any CodeBlocks that refer to us are destroyed.
 847     // Consider that two CodeBlocks become unreachable at the same time. There
 848     // is no guarantee about the order in which the CodeBlocks are destroyed.
 849     // So, if we don&#39;t remove incoming calls, and get destroyed before the
 850     // CodeBlock(s) that have calls into us, then the CallLinkInfo vector&#39;s
 851     // destructor will try to remove nodes from our (no longer valid) linked list.
 852     unlinkIncomingCalls();
 853 
</pre>
<hr />
<pre>
 863         }
 864     }
 865 #endif // ENABLE(JIT)
 866 }
 867 
 868 void CodeBlock::setConstantIdentifierSetRegisters(VM&amp; vm, const Vector&lt;ConstantIdentifierSetEntry&gt;&amp; constants)
 869 {
 870     auto scope = DECLARE_THROW_SCOPE(vm);
 871     JSGlobalObject* globalObject = m_globalObject.get();
 872     ExecState* exec = globalObject-&gt;globalExec();
 873 
 874     for (const auto&amp; entry : constants) {
 875         const IdentifierSet&amp; set = entry.first;
 876 
 877         Structure* setStructure = globalObject-&gt;setStructure();
 878         RETURN_IF_EXCEPTION(scope, void());
 879         JSSet* jsSet = JSSet::create(exec, vm, setStructure, set.size());
 880         RETURN_IF_EXCEPTION(scope, void());
 881 
 882         for (auto setEntry : set) {
<span class="line-modified"> 883             JSString* jsString = jsOwnedString(vm, setEntry.get());</span>
 884             jsSet-&gt;add(exec, jsString);
 885             RETURN_IF_EXCEPTION(scope, void());
 886         }
 887         m_constantRegisters[entry.second].set(vm, this, jsSet);
 888     }
 889 }
 890 
<span class="line-modified"> 891 void CodeBlock::setConstantRegisters(const Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants, const Vector&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation, ScriptExecutable* topLevelExecutable)</span>
 892 {
 893     VM&amp; vm = *m_vm;
 894     auto scope = DECLARE_THROW_SCOPE(vm);
 895     JSGlobalObject* globalObject = m_globalObject.get();
 896     ExecState* exec = globalObject-&gt;globalExec();
 897 
 898     ASSERT(constants.size() == constantsSourceCodeRepresentation.size());
 899     size_t count = constants.size();
 900     m_constantRegisters.resizeToFit(count);

 901     for (size_t i = 0; i &lt; count; i++) {
 902         JSValue constant = constants[i].get();
 903 
 904         if (!constant.isEmpty()) {
 905             if (constant.isCell()) {
 906                 JSCell* cell = constant.asCell();
 907                 if (SymbolTable* symbolTable = jsDynamicCast&lt;SymbolTable*&gt;(vm, cell)) {
<span class="line-modified"> 908                     if (m_unlinkedCode-&gt;wasCompiledWithTypeProfilerOpcodes()) {</span>
 909                         ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
 910                         symbolTable-&gt;prepareForTypeProfiling(locker);
 911                     }
 912 
 913                     SymbolTable* clone = symbolTable-&gt;cloneScopePart(vm);
 914                     if (wasCompiledWithDebuggingOpcodes())
 915                         clone-&gt;setRareDataCodeBlock(this);
 916 
 917                     constant = clone;
 918                 } else if (auto* descriptor = jsDynamicCast&lt;JSTemplateObjectDescriptor*&gt;(vm, cell)) {
<span class="line-modified"> 919                     auto* templateObject = topLevelExecutable-&gt;createTemplateObject(exec, descriptor);</span>
 920                     RETURN_IF_EXCEPTION(scope, void());
 921                     constant = templateObject;
 922                 }
 923             }
 924         }
 925 
 926         m_constantRegisters[i].set(vm, this, constant);
 927     }
 928 
 929     m_constantsSourceCodeRepresentation = constantsSourceCodeRepresentation;
 930 }
 931 
 932 void CodeBlock::setAlternative(VM&amp; vm, CodeBlock* alternative)
 933 {
 934     RELEASE_ASSERT(alternative);
 935     RELEASE_ASSERT(alternative-&gt;jitCode());
 936     m_alternative.set(vm, this, alternative);
 937 }
 938 
 939 void CodeBlock::setNumParameters(int newValue)
 940 {
 941     m_numParameters = newValue;
 942 
<span class="line-modified"> 943     m_argumentValueProfiles = RefCountedArray&lt;ValueProfile&gt;(vm().canUseJIT() ? newValue : 0);</span>
 944 }
 945 
 946 CodeBlock* CodeBlock::specialOSREntryBlockOrNull()
 947 {
 948 #if ENABLE(FTL_JIT)
<span class="line-modified"> 949     if (jitType() != JITType::DFGJIT)</span>
 950         return 0;
 951     DFG::JITCode* jitCode = m_jitCode-&gt;dfg();
 952     return jitCode-&gt;osrEntryBlock();
 953 #else // ENABLE(FTL_JIT)
 954     return 0;
 955 #endif // ENABLE(FTL_JIT)
 956 }
 957 
 958 size_t CodeBlock::estimatedSize(JSCell* cell, VM&amp; vm)
 959 {
 960     CodeBlock* thisObject = jsCast&lt;CodeBlock*&gt;(cell);
 961     size_t extraMemoryAllocated = 0;
 962     if (thisObject-&gt;m_metadata)
 963         extraMemoryAllocated += thisObject-&gt;m_metadata-&gt;sizeInBytes();
<span class="line-modified"> 964     RefPtr&lt;JITCode&gt; jitCode = thisObject-&gt;m_jitCode;</span>
<span class="line-modified"> 965     if (jitCode &amp;&amp; !jitCode-&gt;isShared())</span>
<span class="line-added"> 966         extraMemoryAllocated += jitCode-&gt;size();</span>
 967     return Base::estimatedSize(cell, vm) + extraMemoryAllocated;
 968 }
 969 
 970 void CodeBlock::visitChildren(JSCell* cell, SlotVisitor&amp; visitor)
 971 {
 972     CodeBlock* thisObject = jsCast&lt;CodeBlock*&gt;(cell);
 973     ASSERT_GC_OBJECT_INHERITS(thisObject, info());
 974     Base::visitChildren(cell, visitor);
 975     visitor.append(thisObject-&gt;m_ownerEdge);
 976     thisObject-&gt;visitChildren(visitor);
 977 }
 978 
 979 void CodeBlock::visitChildren(SlotVisitor&amp; visitor)
 980 {
 981     ConcurrentJSLocker locker(m_lock);
 982     if (CodeBlock* otherBlock = specialOSREntryBlockOrNull())
 983         visitor.appendUnbarriered(otherBlock);
 984 
 985     size_t extraMemory = 0;
 986     if (m_metadata)
 987         extraMemory += m_metadata-&gt;sizeInBytes();
<span class="line-modified"> 988     if (m_jitCode &amp;&amp; !m_jitCode-&gt;isShared())</span>
 989         extraMemory += m_jitCode-&gt;size();
 990     visitor.reportExtraMemoryVisited(extraMemory);
 991 
 992     stronglyVisitStrongReferences(locker, visitor);
 993     stronglyVisitWeakReferences(locker, visitor);
 994 
 995     VM::SpaceAndSet::setFor(*subspace()).add(this);
 996 }
 997 
 998 bool CodeBlock::shouldVisitStrongly(const ConcurrentJSLocker&amp; locker)
 999 {
1000     if (Options::forceCodeBlockLiveness())
1001         return true;
1002 
1003     if (shouldJettisonDueToOldAge(locker))
1004         return false;
1005 
1006     // Interpreter and Baseline JIT CodeBlocks don&#39;t need to be jettisoned when
1007     // their weak references go stale. So if a basline JIT CodeBlock gets
1008     // scanned, we can assume that this means that it&#39;s live.
1009     if (!JITCode::isOptimizingJIT(jitType()))
1010         return true;
1011 
1012     return false;
1013 }
1014 
<span class="line-modified">1015 bool CodeBlock::shouldJettisonDueToWeakReference(VM&amp; vm)</span>
1016 {
1017     if (!JITCode::isOptimizingJIT(jitType()))
1018         return false;
<span class="line-modified">1019     return !vm.heap.isMarked(this);</span>
1020 }
1021 
<span class="line-modified">1022 static Seconds timeToLive(JITType jitType)</span>
1023 {
1024     if (UNLIKELY(Options::useEagerCodeBlockJettisonTiming())) {
1025         switch (jitType) {
<span class="line-modified">1026         case JITType::InterpreterThunk:</span>
1027             return 10_ms;
<span class="line-modified">1028         case JITType::BaselineJIT:</span>
1029             return 30_ms;
<span class="line-modified">1030         case JITType::DFGJIT:</span>
1031             return 40_ms;
<span class="line-modified">1032         case JITType::FTLJIT:</span>
1033             return 120_ms;
1034         default:
1035             return Seconds::infinity();
1036         }
1037     }
1038 
1039     switch (jitType) {
<span class="line-modified">1040     case JITType::InterpreterThunk:</span>
1041         return 5_s;
<span class="line-modified">1042     case JITType::BaselineJIT:</span>
1043         // Effectively 10 additional seconds, since BaselineJIT and
1044         // InterpreterThunk share a CodeBlock.
1045         return 15_s;
<span class="line-modified">1046     case JITType::DFGJIT:</span>
1047         return 20_s;
<span class="line-modified">1048     case JITType::FTLJIT:</span>
1049         return 60_s;
1050     default:
1051         return Seconds::infinity();
1052     }
1053 }
1054 
1055 bool CodeBlock::shouldJettisonDueToOldAge(const ConcurrentJSLocker&amp;)
1056 {
<span class="line-modified">1057     if (m_vm-&gt;heap.isMarked(this))</span>
1058         return false;
1059 
1060     if (UNLIKELY(Options::forceCodeBlockToJettisonDueToOldAge()))
1061         return true;
1062 
1063     if (timeSinceCreation() &lt; timeToLive(jitType()))
1064         return false;
1065 
1066     return true;
1067 }
1068 
1069 #if ENABLE(DFG_JIT)
<span class="line-modified">1070 static bool shouldMarkTransition(VM&amp; vm, DFG::WeakReferenceTransition&amp; transition)</span>
1071 {
<span class="line-modified">1072     if (transition.m_codeOrigin &amp;&amp; !vm.heap.isMarked(transition.m_codeOrigin.get()))</span>
1073         return false;
1074 
<span class="line-modified">1075     if (!vm.heap.isMarked(transition.m_from.get()))</span>
1076         return false;
1077 
1078     return true;
1079 }
1080 #endif // ENABLE(DFG_JIT)
1081 
1082 void CodeBlock::propagateTransitions(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1083 {
1084     UNUSED_PARAM(visitor);
1085 
1086     VM&amp; vm = *m_vm;
1087 
<span class="line-modified">1088     if (jitType() == JITType::InterpreterThunk) {</span>
1089         const Vector&lt;InstructionStream::Offset&gt;&amp; propertyAccessInstructions = m_unlinkedCode-&gt;propertyAccessInstructions();
1090         const InstructionStream&amp; instructionStream = instructions();
1091         for (size_t i = 0; i &lt; propertyAccessInstructions.size(); ++i) {
1092             auto instruction = instructionStream.at(propertyAccessInstructions[i]);
1093             if (instruction-&gt;is&lt;OpPutById&gt;()) {
1094                 auto&amp; metadata = instruction-&gt;as&lt;OpPutById&gt;().metadata(this);
1095                 StructureID oldStructureID = metadata.m_oldStructureID;
1096                 StructureID newStructureID = metadata.m_newStructureID;
1097                 if (!oldStructureID || !newStructureID)
1098                     continue;
1099                 Structure* oldStructure =
1100                     vm.heap.structureIDTable().get(oldStructureID);
1101                 Structure* newStructure =
1102                     vm.heap.structureIDTable().get(newStructureID);
<span class="line-modified">1103                 if (vm.heap.isMarked(oldStructure))</span>
1104                     visitor.appendUnbarriered(newStructure);
1105                 continue;
1106             }
1107         }
1108     }
1109 
1110 #if ENABLE(JIT)
1111     if (JITCode::isJIT(jitType())) {
1112         if (auto* jitData = m_jitData.get()) {
1113             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1114                 stubInfo-&gt;propagateTransitions(visitor);
1115         }
1116     }
1117 #endif // ENABLE(JIT)
1118 
1119 #if ENABLE(DFG_JIT)
1120     if (JITCode::isOptimizingJIT(jitType())) {
1121         DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1122 
1123         dfgCommon-&gt;recordedStatuses.markIfCheap(visitor);
1124 
1125         for (auto&amp; weakReference : dfgCommon-&gt;weakStructureReferences)
1126             weakReference-&gt;markIfCheap(visitor);
1127 
1128         for (auto&amp; transition : dfgCommon-&gt;transitions) {
<span class="line-modified">1129             if (shouldMarkTransition(vm, transition)) {</span>
1130                 // If the following three things are live, then the target of the
1131                 // transition is also live:
1132                 //
1133                 // - This code block. We know it&#39;s live already because otherwise
1134                 //   we wouldn&#39;t be scanning ourselves.
1135                 //
1136                 // - The code origin of the transition. Transitions may arise from
1137                 //   code that was inlined. They are not relevant if the user&#39;s
1138                 //   object that is required for the inlinee to run is no longer
1139                 //   live.
1140                 //
1141                 // - The source of the transition. The transition checks if some
1142                 //   heap location holds the source, and if so, stores the target.
1143                 //   Hence the source must be live for the transition to be live.
1144                 //
1145                 // We also short-circuit the liveness if the structure is harmless
1146                 // to mark (i.e. its global object and prototype are both already
1147                 // live).
1148 
1149                 visitor.append(transition.m_to);
1150             }
1151         }
1152     }
1153 #endif // ENABLE(DFG_JIT)
1154 }
1155 
1156 void CodeBlock::determineLiveness(const ConcurrentJSLocker&amp;, SlotVisitor&amp; visitor)
1157 {
1158     UNUSED_PARAM(visitor);
1159 
1160 #if ENABLE(DFG_JIT)
<span class="line-modified">1161     VM&amp; vm = *m_vm;</span>
<span class="line-added">1162     if (vm.heap.isMarked(this))</span>
1163         return;
1164 
1165     // In rare and weird cases, this could be called on a baseline CodeBlock. One that I found was
1166     // that we might decide that the CodeBlock should be jettisoned due to old age, so the
1167     // isMarked check doesn&#39;t protect us.
1168     if (!JITCode::isOptimizingJIT(jitType()))
1169         return;
1170 
1171     DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
1172     // Now check all of our weak references. If all of them are live, then we
1173     // have proved liveness and so we scan our strong references. If at end of
1174     // GC we still have not proved liveness, then this code block is toast.
1175     bool allAreLiveSoFar = true;
1176     for (unsigned i = 0; i &lt; dfgCommon-&gt;weakReferences.size(); ++i) {
1177         JSCell* reference = dfgCommon-&gt;weakReferences[i].get();
<span class="line-modified">1178         ASSERT(!jsDynamicCast&lt;CodeBlock*&gt;(vm, reference));</span>
<span class="line-modified">1179         if (!vm.heap.isMarked(reference)) {</span>
1180             allAreLiveSoFar = false;
1181             break;
1182         }
1183     }
1184     if (allAreLiveSoFar) {
1185         for (unsigned i = 0; i &lt; dfgCommon-&gt;weakStructureReferences.size(); ++i) {
<span class="line-modified">1186             if (!vm.heap.isMarked(dfgCommon-&gt;weakStructureReferences[i].get())) {</span>
1187                 allAreLiveSoFar = false;
1188                 break;
1189             }
1190         }
1191     }
1192 
1193     // If some weak references are dead, then this fixpoint iteration was
1194     // unsuccessful.
1195     if (!allAreLiveSoFar)
1196         return;
1197 
1198     // All weak references are live. Record this information so we don&#39;t
1199     // come back here again, and scan the strong references.
1200     visitor.appendUnbarriered(this);
1201 #endif // ENABLE(DFG_JIT)
1202 }
1203 
1204 void CodeBlock::finalizeLLIntInlineCaches()
1205 {
1206     VM&amp; vm = *m_vm;
1207     const Vector&lt;InstructionStream::Offset&gt;&amp; propertyAccessInstructions = m_unlinkedCode-&gt;propertyAccessInstructions();
1208 
<span class="line-modified">1209     auto handleGetPutFromScope = [&amp;] (auto&amp; metadata) {</span>
1210         GetPutInfo getPutInfo = metadata.m_getPutInfo;
1211         if (getPutInfo.resolveType() == GlobalVar || getPutInfo.resolveType() == GlobalVarWithVarInjectionChecks
1212             || getPutInfo.resolveType() == LocalClosureVar || getPutInfo.resolveType() == GlobalLexicalVar || getPutInfo.resolveType() == GlobalLexicalVarWithVarInjectionChecks)
1213             return;
1214         WriteBarrierBase&lt;Structure&gt;&amp; structure = metadata.m_structure;
<span class="line-modified">1215         if (!structure || vm.heap.isMarked(structure.get()))</span>
1216             return;
1217         if (Options::verboseOSR())
1218             dataLogF(&quot;Clearing scope access with structure %p.\n&quot;, structure.get());
1219         structure.clear();
1220     };
1221 
1222     const InstructionStream&amp; instructionStream = instructions();
1223     for (size_t size = propertyAccessInstructions.size(), i = 0; i &lt; size; ++i) {
1224         const auto curInstruction = instructionStream.at(propertyAccessInstructions[i]);
1225         switch (curInstruction-&gt;opcodeID()) {
1226         case op_get_by_id: {
1227             auto&amp; metadata = curInstruction-&gt;as&lt;OpGetById&gt;().metadata(this);
<span class="line-modified">1228             if (metadata.m_modeMetadata.mode != GetByIdMode::Default)</span>
1229                 break;
1230             StructureID oldStructureID = metadata.m_modeMetadata.defaultMode.structureID;
<span class="line-modified">1231             if (!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))</span>
1232                 break;
1233             if (Options::verboseOSR())
1234                 dataLogF(&quot;Clearing LLInt property access.\n&quot;);
1235             LLIntPrototypeLoadAdaptiveStructureWatchpoint::clearLLIntGetByIdCache(metadata);
1236             break;
1237         }
1238         case op_get_by_id_direct: {
1239             auto&amp; metadata = curInstruction-&gt;as&lt;OpGetByIdDirect&gt;().metadata(this);
1240             StructureID oldStructureID = metadata.m_structureID;
<span class="line-modified">1241             if (!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))</span>
1242                 break;
1243             if (Options::verboseOSR())
1244                 dataLogF(&quot;Clearing LLInt property access.\n&quot;);
1245             metadata.m_structureID = 0;
1246             metadata.m_offset = 0;
1247             break;
1248         }
1249         case op_put_by_id: {
1250             auto&amp; metadata = curInstruction-&gt;as&lt;OpPutById&gt;().metadata(this);
1251             StructureID oldStructureID = metadata.m_oldStructureID;
1252             StructureID newStructureID = metadata.m_newStructureID;
1253             StructureChain* chain = metadata.m_structureChain.get();
<span class="line-modified">1254             if ((!oldStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(oldStructureID)))</span>
<span class="line-modified">1255                 &amp;&amp; (!newStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(newStructureID)))</span>
<span class="line-modified">1256                 &amp;&amp; (!chain || vm.heap.isMarked(chain)))</span>
1257                 break;
1258             if (Options::verboseOSR())
1259                 dataLogF(&quot;Clearing LLInt put transition.\n&quot;);
1260             metadata.m_oldStructureID = 0;
1261             metadata.m_offset = 0;
1262             metadata.m_newStructureID = 0;
1263             metadata.m_structureChain.clear();
1264             break;
1265         }
1266         // FIXME: https://bugs.webkit.org/show_bug.cgi?id=166418
1267         // We need to add optimizations for op_resolve_scope_for_hoisting_func_decl_in_eval to do link time scope resolution.
1268         case op_resolve_scope_for_hoisting_func_decl_in_eval:
1269             break;
1270         case op_to_this: {
1271             auto&amp; metadata = curInstruction-&gt;as&lt;OpToThis&gt;().metadata(this);
<span class="line-modified">1272             if (!metadata.m_cachedStructureID || vm.heap.isMarked(vm.heap.structureIDTable().get(metadata.m_cachedStructureID)))</span>
1273                 break;
<span class="line-modified">1274             if (Options::verboseOSR()) {</span>
<span class="line-modified">1275                 Structure* structure = vm.heap.structureIDTable().get(metadata.m_cachedStructureID);</span>
<span class="line-modified">1276                 dataLogF(&quot;Clearing LLInt to_this with structure %p.\n&quot;, structure);</span>
<span class="line-added">1277             }</span>
<span class="line-added">1278             metadata.m_cachedStructureID = 0;</span>
1279             metadata.m_toThisStatus = merge(metadata.m_toThisStatus, ToThisClearedByGC);
1280             break;
1281         }
1282         case op_create_this: {
1283             auto&amp; metadata = curInstruction-&gt;as&lt;OpCreateThis&gt;().metadata(this);
1284             auto&amp; cacheWriteBarrier = metadata.m_cachedCallee;
1285             if (!cacheWriteBarrier || cacheWriteBarrier.unvalidatedGet() == JSCell::seenMultipleCalleeObjects())
1286                 break;
1287             JSCell* cachedFunction = cacheWriteBarrier.get();
<span class="line-modified">1288             if (vm.heap.isMarked(cachedFunction))</span>
1289                 break;
1290             if (Options::verboseOSR())
1291                 dataLogF(&quot;Clearing LLInt create_this with cached callee %p.\n&quot;, cachedFunction);
1292             cacheWriteBarrier.clear();
1293             break;
1294         }
1295         case op_resolve_scope: {
1296             // Right now this isn&#39;t strictly necessary. Any symbol tables that this will refer to
1297             // are for outer functions, and we refer to those functions strongly, and they refer
1298             // to the symbol table strongly. But it&#39;s nice to be on the safe side.
1299             auto&amp; metadata = curInstruction-&gt;as&lt;OpResolveScope&gt;().metadata(this);
1300             WriteBarrierBase&lt;SymbolTable&gt;&amp; symbolTable = metadata.m_symbolTable;
<span class="line-modified">1301             if (!symbolTable || vm.heap.isMarked(symbolTable.get()))</span>
1302                 break;
1303             if (Options::verboseOSR())
1304                 dataLogF(&quot;Clearing dead symbolTable %p.\n&quot;, symbolTable.get());
1305             symbolTable.clear();
1306             break;
1307         }
1308         case op_get_from_scope:
1309             handleGetPutFromScope(curInstruction-&gt;as&lt;OpGetFromScope&gt;().metadata(this));
1310             break;
1311         case op_put_to_scope:
1312             handleGetPutFromScope(curInstruction-&gt;as&lt;OpPutToScope&gt;().metadata(this));
1313             break;
1314         default:
1315             OpcodeID opcodeID = curInstruction-&gt;opcodeID();
1316             ASSERT_WITH_MESSAGE_UNUSED(opcodeID, false, &quot;Unhandled opcode in CodeBlock::finalizeUnconditionally, %s(%d) at bc %u&quot;, opcodeNames[opcodeID], opcodeID, propertyAccessInstructions[i]);
1317         }
1318     }
1319 
1320     // We can&#39;t just remove all the sets when we clear the caches since we might have created a watchpoint set
1321     // then cleared the cache without GCing in between.
1322     m_llintGetByIdWatchpointMap.removeIf([&amp;] (const StructureWatchpointMap::KeyValuePairType&amp; pair) -&gt; bool {
1323         auto clear = [&amp;] () {
<span class="line-modified">1324             auto&amp; instruction = instructions().at(std::get&lt;1&gt;(pair.key));</span>
1325             OpcodeID opcode = instruction-&gt;opcodeID();
1326             if (opcode == op_get_by_id) {
1327                 if (Options::verboseOSR())
1328                     dataLogF(&quot;Clearing LLInt property access.\n&quot;);
1329                 LLIntPrototypeLoadAdaptiveStructureWatchpoint::clearLLIntGetByIdCache(instruction-&gt;as&lt;OpGetById&gt;().metadata(this));
1330             }
1331             return true;
1332         };
1333 
<span class="line-modified">1334         if (!vm.heap.isMarked(vm.heap.structureIDTable().get(std::get&lt;0&gt;(pair.key))))</span>
1335             return clear();
1336 
<span class="line-modified">1337         for (const LLIntPrototypeLoadAdaptiveStructureWatchpoint&amp; watchpoint : pair.value) {</span>
<span class="line-modified">1338             if (!watchpoint.key().isStillLive(vm))</span>
1339                 return clear();
1340         }
1341 
1342         return false;
1343     });
1344 
1345     forEachLLIntCallLinkInfo([&amp;](LLIntCallLinkInfo&amp; callLinkInfo) {
<span class="line-modified">1346         if (callLinkInfo.isLinked() &amp;&amp; !vm.heap.isMarked(callLinkInfo.callee())) {</span>
1347             if (Options::verboseOSR())
1348                 dataLog(&quot;Clearing LLInt call from &quot;, *this, &quot;\n&quot;);
1349             callLinkInfo.unlink();
1350         }
<span class="line-modified">1351         if (callLinkInfo.lastSeenCallee() &amp;&amp; !vm.heap.isMarked(callLinkInfo.lastSeenCallee()))</span>
<span class="line-modified">1352             callLinkInfo.clearLastSeenCallee();</span>
1353     });
1354 }
1355 
1356 #if ENABLE(JIT)
1357 CodeBlock::JITData&amp; CodeBlock::ensureJITDataSlow(const ConcurrentJSLocker&amp;)
1358 {
1359     ASSERT(!m_jitData);
<span class="line-modified">1360     m_jitData = makeUnique&lt;JITData&gt;();</span>
1361     return *m_jitData;
1362 }
1363 
1364 void CodeBlock::finalizeBaselineJITInlineCaches()
1365 {
1366     if (auto* jitData = m_jitData.get()) {
1367         for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos)
<span class="line-modified">1368             callLinkInfo-&gt;visitWeak(vm());</span>
1369 
1370         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1371             stubInfo-&gt;visitWeakReferences(this);
1372     }
1373 }
1374 #endif
1375 
<span class="line-modified">1376 void CodeBlock::finalizeUnconditionally(VM&amp; vm)</span>
1377 {
<span class="line-added">1378     UNUSED_PARAM(vm);</span>
<span class="line-added">1379 </span>
1380     updateAllPredictions();
1381 
1382     if (JITCode::couldBeInterpreted(jitType()))
1383         finalizeLLIntInlineCaches();
1384 
1385 #if ENABLE(JIT)
1386     if (!!jitCode())
1387         finalizeBaselineJITInlineCaches();
1388 #endif
1389 
1390 #if ENABLE(DFG_JIT)
1391     if (JITCode::isOptimizingJIT(jitType())) {
1392         DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
<span class="line-modified">1393         dfgCommon-&gt;recordedStatuses.finalize(vm);</span>
1394     }
1395 #endif // ENABLE(DFG_JIT)
1396 
<span class="line-added">1397     auto updateActivity = [&amp;] {</span>
<span class="line-added">1398         if (!VM::useUnlinkedCodeBlockJettisoning())</span>
<span class="line-added">1399             return;</span>
<span class="line-added">1400         JITCode* jitCode = m_jitCode.get();</span>
<span class="line-added">1401         double count = 0;</span>
<span class="line-added">1402         bool alwaysActive = false;</span>
<span class="line-added">1403         switch (JITCode::jitTypeFor(jitCode)) {</span>
<span class="line-added">1404         case JITType::None:</span>
<span class="line-added">1405         case JITType::HostCallThunk:</span>
<span class="line-added">1406             return;</span>
<span class="line-added">1407         case JITType::InterpreterThunk:</span>
<span class="line-added">1408             count = m_llintExecuteCounter.count();</span>
<span class="line-added">1409             break;</span>
<span class="line-added">1410         case JITType::BaselineJIT:</span>
<span class="line-added">1411             count = m_jitExecuteCounter.count();</span>
<span class="line-added">1412             break;</span>
<span class="line-added">1413         case JITType::DFGJIT:</span>
<span class="line-added">1414 #if ENABLE(FTL_JIT)</span>
<span class="line-added">1415             count = static_cast&lt;DFG::JITCode*&gt;(jitCode)-&gt;tierUpCounter.count();</span>
<span class="line-added">1416 #else</span>
<span class="line-added">1417             alwaysActive = true;</span>
<span class="line-added">1418 #endif</span>
<span class="line-added">1419             break;</span>
<span class="line-added">1420         case JITType::FTLJIT:</span>
<span class="line-added">1421             alwaysActive = true;</span>
<span class="line-added">1422             break;</span>
<span class="line-added">1423         }</span>
<span class="line-added">1424         if (alwaysActive || m_previousCounter &lt; count) {</span>
<span class="line-added">1425             // CodeBlock is active right now, so resetting UnlinkedCodeBlock&#39;s age.</span>
<span class="line-added">1426             m_unlinkedCode-&gt;resetAge();</span>
<span class="line-added">1427         }</span>
<span class="line-added">1428         m_previousCounter = count;</span>
<span class="line-added">1429     };</span>
<span class="line-added">1430     updateActivity();</span>
<span class="line-added">1431 </span>
1432     VM::SpaceAndSet::setFor(*subspace()).remove(this);
1433 }
1434 
1435 void CodeBlock::destroy(JSCell* cell)
1436 {
1437     static_cast&lt;CodeBlock*&gt;(cell)-&gt;~CodeBlock();
1438 }
1439 
1440 void CodeBlock::getICStatusMap(const ConcurrentJSLocker&amp;, ICStatusMap&amp; result)
1441 {
1442 #if ENABLE(JIT)
1443     if (JITCode::isJIT(jitType())) {
1444         if (auto* jitData = m_jitData.get()) {
1445             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos)
1446                 result.add(stubInfo-&gt;codeOrigin, ICStatus()).iterator-&gt;value.stubInfo = stubInfo;
1447             for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos)
1448                 result.add(callLinkInfo-&gt;codeOrigin(), ICStatus()).iterator-&gt;value.callLinkInfo = callLinkInfo;
1449             for (ByValInfo* byValInfo : jitData-&gt;m_byValInfos)
1450                 result.add(CodeOrigin(byValInfo-&gt;bytecodeIndex), ICStatus()).iterator-&gt;value.byValInfo = byValInfo;
1451         }
</pre>
<hr />
<pre>
1464 #endif
1465     }
1466 #else
1467     UNUSED_PARAM(result);
1468 #endif
1469 }
1470 
1471 void CodeBlock::getICStatusMap(ICStatusMap&amp; result)
1472 {
1473     ConcurrentJSLocker locker(m_lock);
1474     getICStatusMap(locker, result);
1475 }
1476 
1477 #if ENABLE(JIT)
1478 StructureStubInfo* CodeBlock::addStubInfo(AccessType accessType)
1479 {
1480     ConcurrentJSLocker locker(m_lock);
1481     return ensureJITData(locker).m_stubInfos.add(accessType);
1482 }
1483 
<span class="line-modified">1484 JITAddIC* CodeBlock::addJITAddIC(ArithProfile* arithProfile)</span>
1485 {
1486     ConcurrentJSLocker locker(m_lock);
<span class="line-modified">1487     return ensureJITData(locker).m_addICs.add(arithProfile);</span>
1488 }
1489 
<span class="line-modified">1490 JITMulIC* CodeBlock::addJITMulIC(ArithProfile* arithProfile)</span>
1491 {
1492     ConcurrentJSLocker locker(m_lock);
<span class="line-modified">1493     return ensureJITData(locker).m_mulICs.add(arithProfile);</span>
1494 }
1495 
<span class="line-modified">1496 JITSubIC* CodeBlock::addJITSubIC(ArithProfile* arithProfile)</span>
1497 {
1498     ConcurrentJSLocker locker(m_lock);
<span class="line-modified">1499     return ensureJITData(locker).m_subICs.add(arithProfile);</span>
1500 }
1501 
<span class="line-modified">1502 JITNegIC* CodeBlock::addJITNegIC(ArithProfile* arithProfile)</span>
1503 {
1504     ConcurrentJSLocker locker(m_lock);
<span class="line-modified">1505     return ensureJITData(locker).m_negICs.add(arithProfile);</span>
1506 }
1507 
1508 StructureStubInfo* CodeBlock::findStubInfo(CodeOrigin codeOrigin)
1509 {
1510     ConcurrentJSLocker locker(m_lock);
1511     if (auto* jitData = m_jitData.get()) {
1512         for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
1513             if (stubInfo-&gt;codeOrigin == codeOrigin)
1514                 return stubInfo;
1515         }
1516     }
1517     return nullptr;
1518 }
1519 
1520 ByValInfo* CodeBlock::addByValInfo()
1521 {
1522     ConcurrentJSLocker locker(m_lock);
1523     return ensureJITData(locker).m_byValInfos.add();
1524 }
1525 
</pre>
<hr />
<pre>
1553 {
1554     if (auto* jitData = m_jitData.get()) {
1555         return tryBinarySearch&lt;RareCaseProfile, int&gt;(
1556             jitData-&gt;m_rareCaseProfiles, jitData-&gt;m_rareCaseProfiles.size(), bytecodeOffset,
1557             getRareCaseProfileBytecodeOffset);
1558     }
1559     return nullptr;
1560 }
1561 
1562 unsigned CodeBlock::rareCaseProfileCountForBytecodeOffset(const ConcurrentJSLocker&amp; locker, int bytecodeOffset)
1563 {
1564     RareCaseProfile* profile = rareCaseProfileForBytecodeOffset(locker, bytecodeOffset);
1565     if (profile)
1566         return profile-&gt;m_counter;
1567     return 0;
1568 }
1569 
1570 void CodeBlock::setCalleeSaveRegisters(RegisterSet calleeSaveRegisters)
1571 {
1572     ConcurrentJSLocker locker(m_lock);
<span class="line-modified">1573     ensureJITData(locker).m_calleeSaveRegisters = makeUnique&lt;RegisterAtOffsetList&gt;(calleeSaveRegisters);</span>
1574 }
1575 
1576 void CodeBlock::setCalleeSaveRegisters(std::unique_ptr&lt;RegisterAtOffsetList&gt; registerAtOffsetList)
1577 {
1578     ConcurrentJSLocker locker(m_lock);
1579     ensureJITData(locker).m_calleeSaveRegisters = WTFMove(registerAtOffsetList);
1580 }
1581 
1582 void CodeBlock::resetJITData()
1583 {
1584     RELEASE_ASSERT(!JITCode::isJIT(jitType()));
1585     ConcurrentJSLocker locker(m_lock);
1586 
1587     if (auto* jitData = m_jitData.get()) {
1588         // We can clear these because no other thread will have references to any stub infos, call
1589         // link infos, or by val infos if we don&#39;t have JIT code. Attempts to query these data
1590         // structures using the concurrent API (getICStatusMap and friends) will return nothing if we
1591         // don&#39;t have JIT code.
1592         jitData-&gt;m_stubInfos.clear();
1593         jitData-&gt;m_callLinkInfos.clear();
</pre>
<hr />
<pre>
1667         visitor.append(transition.m_to);
1668     }
1669 
1670     for (auto&amp; weakReference : dfgCommon-&gt;weakReferences)
1671         visitor.append(weakReference);
1672 
1673     for (auto&amp; weakStructureReference : dfgCommon-&gt;weakStructureReferences)
1674         visitor.append(weakStructureReference);
1675 
1676     dfgCommon-&gt;livenessHasBeenProved = true;
1677 #endif
1678 }
1679 
1680 CodeBlock* CodeBlock::baselineAlternative()
1681 {
1682 #if ENABLE(JIT)
1683     CodeBlock* result = this;
1684     while (result-&gt;alternative())
1685         result = result-&gt;alternative();
1686     RELEASE_ASSERT(result);
<span class="line-modified">1687     RELEASE_ASSERT(JITCode::isBaselineCode(result-&gt;jitType()) || result-&gt;jitType() == JITType::None);</span>
1688     return result;
1689 #else
1690     return this;
1691 #endif
1692 }
1693 
1694 CodeBlock* CodeBlock::baselineVersion()
1695 {
1696 #if ENABLE(JIT)
<span class="line-modified">1697     JITType selfJITType = jitType();</span>
1698     if (JITCode::isBaselineCode(selfJITType))
1699         return this;
1700     CodeBlock* result = replacement();
1701     if (!result) {
1702         if (JITCode::isOptimizingJIT(selfJITType)) {
1703             // The replacement can be null if we&#39;ve had a memory clean up and the executable
1704             // has been purged of its codeBlocks (see ExecutableBase::clearCode()). Regardless,
1705             // the current codeBlock is still live on the stack, and as an optimizing JIT
1706             // codeBlock, it will keep its baselineAlternative() alive for us to fetch below.
1707             result = this;
1708         } else {
1709             // This can happen if we&#39;re creating the original CodeBlock for an executable.
1710             // Assume that we&#39;re the baseline CodeBlock.
<span class="line-modified">1711             RELEASE_ASSERT(selfJITType == JITType::None);</span>
1712             return this;
1713         }
1714     }
1715     result = result-&gt;baselineAlternative();
1716     ASSERT(result);
1717     return result;
1718 #else
1719     return this;
1720 #endif
1721 }
1722 
1723 #if ENABLE(JIT)
<span class="line-modified">1724 bool CodeBlock::hasOptimizedReplacement(JITType typeToReplace)</span>
1725 {
1726     CodeBlock* replacement = this-&gt;replacement();
1727     return replacement &amp;&amp; JITCode::isHigherTier(replacement-&gt;jitType(), typeToReplace);
1728 }
1729 
1730 bool CodeBlock::hasOptimizedReplacement()
1731 {
1732     return hasOptimizedReplacement(jitType());
1733 }
1734 #endif
1735 
1736 HandlerInfo* CodeBlock::handlerForBytecodeOffset(unsigned bytecodeOffset, RequiredHandler requiredHandler)
1737 {
1738     RELEASE_ASSERT(bytecodeOffset &lt; instructions().size());
1739     return handlerForIndex(bytecodeOffset, requiredHandler);
1740 }
1741 
1742 HandlerInfo* CodeBlock::handlerForIndex(unsigned index, RequiredHandler requiredHandler)
1743 {
1744     if (!m_rareData)
</pre>
<hr />
<pre>
1793 void CodeBlock::ensureCatchLivenessIsComputedForBytecodeOffsetSlow(const OpCatch&amp; op, InstructionStream::Offset bytecodeOffset)
1794 {
1795     BytecodeLivenessAnalysis&amp; bytecodeLiveness = livenessAnalysis();
1796 
1797     // We get the live-out set of variables at op_catch, not the live-in. This
1798     // is because the variables that the op_catch defines might be dead, and
1799     // we can avoid profiling them and extracting them when doing OSR entry
1800     // into the DFG.
1801 
1802     auto nextOffset = instructions().at(bytecodeOffset).next().offset();
1803     FastBitVector liveLocals = bytecodeLiveness.getLivenessInfoAtBytecodeOffset(this, nextOffset);
1804     Vector&lt;VirtualRegister&gt; liveOperands;
1805     liveOperands.reserveInitialCapacity(liveLocals.bitCount());
1806     liveLocals.forEachSetBit([&amp;] (unsigned liveLocal) {
1807         liveOperands.append(virtualRegisterForLocal(liveLocal));
1808     });
1809 
1810     for (int i = 0; i &lt; numParameters(); ++i)
1811         liveOperands.append(virtualRegisterForArgument(i));
1812 
<span class="line-modified">1813     auto profiles = makeUnique&lt;ValueProfileAndOperandBuffer&gt;(liveOperands.size());</span>
1814     RELEASE_ASSERT(profiles-&gt;m_size == liveOperands.size());
1815     for (unsigned i = 0; i &lt; profiles-&gt;m_size; ++i)
1816         profiles-&gt;m_buffer.get()[i].m_operand = liveOperands[i].offset();
1817 
1818     createRareDataIfNecessary();
1819 
1820     // The compiler thread will read this pointer value and then proceed to dereference it
1821     // if it is not null. We need to make sure all above stores happen before this store so
1822     // the compiler thread reads fully initialized data.
1823     WTF::storeStoreFence();
1824 
1825     op.metadata(this).m_buffer = profiles.get();
1826     {
1827         ConcurrentJSLocker locker(m_lock);
1828         m_rareData-&gt;m_catchProfiles.append(WTFMove(profiles));
1829     }
1830 }
1831 
1832 void CodeBlock::removeExceptionHandlerForCallSite(DisposableCallSiteIndex callSiteIndex)
1833 {
</pre>
<hr />
<pre>
1919     noticeIncomingCall(callerFrame);
1920     {
1921         ConcurrentJSLocker locker(m_lock);
1922         ensureJITData(locker).m_incomingPolymorphicCalls.push(incoming);
1923     }
1924 }
1925 #endif // ENABLE(JIT)
1926 
1927 void CodeBlock::unlinkIncomingCalls()
1928 {
1929     while (m_incomingLLIntCalls.begin() != m_incomingLLIntCalls.end())
1930         m_incomingLLIntCalls.begin()-&gt;unlink();
1931 #if ENABLE(JIT)
1932     JITData* jitData = nullptr;
1933     {
1934         ConcurrentJSLocker locker(m_lock);
1935         jitData = m_jitData.get();
1936     }
1937     if (jitData) {
1938         while (jitData-&gt;m_incomingCalls.begin() != jitData-&gt;m_incomingCalls.end())
<span class="line-modified">1939             jitData-&gt;m_incomingCalls.begin()-&gt;unlink(vm());</span>
1940         while (jitData-&gt;m_incomingPolymorphicCalls.begin() != jitData-&gt;m_incomingPolymorphicCalls.end())
<span class="line-modified">1941             jitData-&gt;m_incomingPolymorphicCalls.begin()-&gt;unlink(vm());</span>
1942     }
1943 #endif // ENABLE(JIT)
1944 }
1945 
1946 void CodeBlock::linkIncomingCall(ExecState* callerFrame, LLIntCallLinkInfo* incoming)
1947 {
1948     noticeIncomingCall(callerFrame);
1949     m_incomingLLIntCalls.push(incoming);
1950 }
1951 
1952 CodeBlock* CodeBlock::newReplacement()
1953 {
1954     return ownerExecutable()-&gt;newReplacementCodeBlockFor(specializationKind());
1955 }
1956 
1957 #if ENABLE(JIT)
1958 CodeBlock* CodeBlock::replacement()
1959 {
<span class="line-modified">1960     const ClassInfo* classInfo = this-&gt;classInfo(vm());</span>
1961 
1962     if (classInfo == FunctionCodeBlock::info())
1963         return jsCast&lt;FunctionExecutable*&gt;(ownerExecutable())-&gt;codeBlockFor(isConstructor() ? CodeForConstruct : CodeForCall);
1964 
1965     if (classInfo == EvalCodeBlock::info())
1966         return jsCast&lt;EvalExecutable*&gt;(ownerExecutable())-&gt;codeBlock();
1967 
1968     if (classInfo == ProgramCodeBlock::info())
1969         return jsCast&lt;ProgramExecutable*&gt;(ownerExecutable())-&gt;codeBlock();
1970 
1971     if (classInfo == ModuleProgramCodeBlock::info())
1972         return jsCast&lt;ModuleProgramExecutable*&gt;(ownerExecutable())-&gt;codeBlock();
1973 
1974     RELEASE_ASSERT_NOT_REACHED();
1975     return nullptr;
1976 }
1977 
1978 DFG::CapabilityLevel CodeBlock::computeCapabilityLevel()
1979 {
<span class="line-modified">1980     const ClassInfo* classInfo = this-&gt;classInfo(vm());</span>
1981 
1982     if (classInfo == FunctionCodeBlock::info()) {
1983         if (isConstructor())
1984             return DFG::functionForConstructCapabilityLevel(this);
1985         return DFG::functionForCallCapabilityLevel(this);
1986     }
1987 
1988     if (classInfo == EvalCodeBlock::info())
1989         return DFG::evalCapabilityLevel(this);
1990 
1991     if (classInfo == ProgramCodeBlock::info())
1992         return DFG::programCapabilityLevel(this);
1993 
1994     if (classInfo == ModuleProgramCodeBlock::info())
1995         return DFG::programCapabilityLevel(this);
1996 
1997     RELEASE_ASSERT_NOT_REACHED();
1998     return DFG::CannotCompile;
1999 }
2000 
2001 #endif // ENABLE(JIT)
2002 
2003 void CodeBlock::jettison(Profiler::JettisonReason reason, ReoptimizationMode mode, const FireDetail* detail)
2004 {
2005 #if !ENABLE(DFG_JIT)
2006     UNUSED_PARAM(mode);
2007     UNUSED_PARAM(detail);
2008 #endif
2009 
<span class="line-modified">2010     VM&amp; vm = *m_vm;</span>
<span class="line-added">2011 </span>
<span class="line-added">2012     CodeBlock* codeBlock = this; // Placate GCC for use in CODEBLOCK_LOG_EVENT  (does not like this).</span>
<span class="line-added">2013     CODEBLOCK_LOG_EVENT(codeBlock, &quot;jettison&quot;, (&quot;due to &quot;, reason, &quot;, counting = &quot;, mode == CountReoptimization, &quot;, detail = &quot;, pointerDump(detail)));</span>
2014 
2015     RELEASE_ASSERT(reason != Profiler::NotJettisoned);
2016 
2017 #if ENABLE(DFG_JIT)
2018     if (DFG::shouldDumpDisassembly()) {
2019         dataLog(&quot;Jettisoning &quot;, *this);
2020         if (mode == CountReoptimization)
2021             dataLog(&quot; and counting reoptimization&quot;);
2022         dataLog(&quot; due to &quot;, reason);
2023         if (detail)
2024             dataLog(&quot;, &quot;, *detail);
2025         dataLog(&quot;.\n&quot;);
2026     }
2027 
2028     if (reason == Profiler::JettisonDueToWeakReference) {
2029         if (DFG::shouldDumpDisassembly()) {
2030             dataLog(*this, &quot; will be jettisoned because of the following dead references:\n&quot;);
2031             DFG::CommonData* dfgCommon = m_jitCode-&gt;dfgCommon();
2032             for (auto&amp; transition : dfgCommon-&gt;transitions) {
2033                 JSCell* origin = transition.m_codeOrigin.get();
2034                 JSCell* from = transition.m_from.get();
2035                 JSCell* to = transition.m_to.get();
<span class="line-modified">2036                 if ((!origin || vm.heap.isMarked(origin)) &amp;&amp; vm.heap.isMarked(from))</span>
2037                     continue;
2038                 dataLog(&quot;    Transition under &quot;, RawPointer(origin), &quot;, &quot;, RawPointer(from), &quot; -&gt; &quot;, RawPointer(to), &quot;.\n&quot;);
2039             }
2040             for (unsigned i = 0; i &lt; dfgCommon-&gt;weakReferences.size(); ++i) {
2041                 JSCell* weak = dfgCommon-&gt;weakReferences[i].get();
<span class="line-modified">2042                 if (vm.heap.isMarked(weak))</span>
2043                     continue;
2044                 dataLog(&quot;    Weak reference &quot;, RawPointer(weak), &quot;.\n&quot;);
2045             }
2046         }
2047     }
2048 #endif // ENABLE(DFG_JIT)
2049 

2050     DeferGCForAWhile deferGC(*heap());
2051 
2052     // We want to accomplish two things here:
2053     // 1) Make sure that if this CodeBlock is on the stack right now, then if we return to it
2054     //    we should OSR exit at the top of the next bytecode instruction after the return.
2055     // 2) Make sure that if we call the owner executable, then we shouldn&#39;t call this CodeBlock.
2056 
2057 #if ENABLE(DFG_JIT)
2058     if (JITCode::isOptimizingJIT(jitType()))
2059         jitCode()-&gt;dfgCommon()-&gt;clearWatchpoints();
2060 
2061     if (reason != Profiler::JettisonDueToOldAge) {
2062         Profiler::Compilation* compilation = jitCode()-&gt;dfgCommon()-&gt;compilation.get();
2063         if (UNLIKELY(compilation))
2064             compilation-&gt;setJettisonReason(reason, detail);
2065 
2066         // This accomplishes (1), and does its own book-keeping about whether it has already happened.
2067         if (!jitCode()-&gt;dfgCommon()-&gt;invalidate()) {
2068             // We&#39;ve already been invalidated.
<span class="line-modified">2069             RELEASE_ASSERT(this != replacement() || (vm.heap.isCurrentThreadBusy() &amp;&amp; !vm.heap.isMarked(ownerExecutable())));</span>
2070             return;
2071         }
2072     }
2073 
2074     if (DFG::shouldDumpDisassembly())
2075         dataLog(&quot;    Did invalidate &quot;, *this, &quot;\n&quot;);
2076 
2077     // Count the reoptimization if that&#39;s what the user wanted.
2078     if (mode == CountReoptimization) {
2079         // FIXME: Maybe this should call alternative().
2080         // https://bugs.webkit.org/show_bug.cgi?id=123677
2081         baselineAlternative()-&gt;countReoptimization();
2082         if (DFG::shouldDumpDisassembly())
2083             dataLog(&quot;    Did count reoptimization for &quot;, *this, &quot;\n&quot;);
2084     }
2085 
2086     if (this != replacement()) {
2087         // This means that we were never the entrypoint. This can happen for OSR entry code
2088         // blocks.
2089         return;
2090     }
2091 
2092     if (alternative())
2093         alternative()-&gt;optimizeAfterWarmUp();
2094 
2095     if (reason != Profiler::JettisonDueToOldAge &amp;&amp; reason != Profiler::JettisonDueToVMTraps)
2096         tallyFrequentExitSites();
2097 #endif // ENABLE(DFG_JIT)
2098 
2099     // Jettison can happen during GC. We don&#39;t want to install code to a dead executable
2100     // because that would add a dead object to the remembered set.
<span class="line-modified">2101     if (vm.heap.isCurrentThreadBusy() &amp;&amp; !vm.heap.isMarked(ownerExecutable()))</span>
2102         return;
2103 
<span class="line-added">2104 #if ENABLE(JIT)</span>
<span class="line-added">2105     {</span>
<span class="line-added">2106         ConcurrentJSLocker locker(m_lock);</span>
<span class="line-added">2107         if (JITData* jitData = m_jitData.get()) {</span>
<span class="line-added">2108             for (CallLinkInfo* callLinkInfo : jitData-&gt;m_callLinkInfos)</span>
<span class="line-added">2109                 callLinkInfo-&gt;setClearedByJettison();</span>
<span class="line-added">2110         }</span>
<span class="line-added">2111     }</span>
<span class="line-added">2112 #endif</span>
<span class="line-added">2113 </span>
2114     // This accomplishes (2).
2115     ownerExecutable()-&gt;installCode(vm, alternative(), codeType(), specializationKind());
2116 
2117 #if ENABLE(DFG_JIT)
2118     if (DFG::shouldDumpDisassembly())
2119         dataLog(&quot;    Did install baseline version of &quot;, *this, &quot;\n&quot;);
2120 #endif // ENABLE(DFG_JIT)
2121 }
2122 
2123 JSGlobalObject* CodeBlock::globalObjectFor(CodeOrigin codeOrigin)
2124 {
<span class="line-modified">2125     auto* inlineCallFrame = codeOrigin.inlineCallFrame();</span>
<span class="line-added">2126     if (!inlineCallFrame)</span>
2127         return globalObject();
<span class="line-modified">2128     return inlineCallFrame-&gt;baselineCodeBlock-&gt;globalObject();</span>
2129 }
2130 
2131 class RecursionCheckFunctor {
2132 public:
2133     RecursionCheckFunctor(CallFrame* startCallFrame, CodeBlock* codeBlock, unsigned depthToCheck)
2134         : m_startCallFrame(startCallFrame)
2135         , m_codeBlock(codeBlock)
2136         , m_depthToCheck(depthToCheck)
2137         , m_foundStartCallFrame(false)
2138         , m_didRecurse(false)
2139     { }
2140 
2141     StackVisitor::Status operator()(StackVisitor&amp; visitor) const
2142     {
2143         CallFrame* currentCallFrame = visitor-&gt;callFrame();
2144 
2145         if (currentCallFrame == m_startCallFrame)
2146             m_foundStartCallFrame = true;
2147 
2148         if (m_foundStartCallFrame) {
</pre>
<hr />
<pre>
2185             dataLog(&quot;    Clearing SABI because caller is native.\n&quot;);
2186         return;
2187     }
2188 
2189     if (!hasBaselineJITProfiling())
2190         return;
2191 
2192     if (!DFG::mightInlineFunction(this))
2193         return;
2194 
2195     if (!canInline(capabilityLevelState()))
2196         return;
2197 
2198     if (!DFG::isSmallEnoughToInlineCodeInto(callerCodeBlock)) {
2199         m_shouldAlwaysBeInlined = false;
2200         if (Options::verboseCallLink())
2201             dataLog(&quot;    Clearing SABI because caller is too large.\n&quot;);
2202         return;
2203     }
2204 
<span class="line-modified">2205     if (callerCodeBlock-&gt;jitType() == JITType::InterpreterThunk) {</span>
2206         // If the caller is still in the interpreter, then we can&#39;t expect inlining to
2207         // happen anytime soon. Assume it&#39;s profitable to optimize it separately. This
2208         // ensures that a function is SABI only if it is called no more frequently than
2209         // any of its callers.
2210         m_shouldAlwaysBeInlined = false;
2211         if (Options::verboseCallLink())
2212             dataLog(&quot;    Clearing SABI because caller is in LLInt.\n&quot;);
2213         return;
2214     }
2215 
2216     if (JITCode::isOptimizingJIT(callerCodeBlock-&gt;jitType())) {
2217         m_shouldAlwaysBeInlined = false;
2218         if (Options::verboseCallLink())
2219             dataLog(&quot;    Clearing SABI bcause caller was already optimized.\n&quot;);
2220         return;
2221     }
2222 
2223     if (callerCodeBlock-&gt;codeType() != FunctionCode) {
2224         // If the caller is either eval or global code, assume that that won&#39;t be
2225         // optimized anytime soon. For eval code this is particularly true since we
2226         // delay eval optimization by a *lot*.
2227         m_shouldAlwaysBeInlined = false;
2228         if (Options::verboseCallLink())
2229             dataLog(&quot;    Clearing SABI because caller is not a function.\n&quot;);
2230         return;
2231     }
2232 
2233     // Recursive calls won&#39;t be inlined.
2234     RecursionCheckFunctor functor(callerFrame, this, Options::maximumInliningDepth());
<span class="line-modified">2235     vm().topCallFrame-&gt;iterate(functor);</span>
2236 
2237     if (functor.didRecurse()) {
2238         if (Options::verboseCallLink())
2239             dataLog(&quot;    Clearing SABI because recursion was detected.\n&quot;);
2240         m_shouldAlwaysBeInlined = false;
2241         return;
2242     }
2243 
2244     if (callerCodeBlock-&gt;capabilityLevelState() == DFG::CapabilityLevelNotSet) {
2245         dataLog(&quot;In call from &quot;, FullCodeOrigin(callerCodeBlock, callerFrame-&gt;codeOrigin()), &quot; to &quot;, *this, &quot;: caller&#39;s DFG capability level is not set.\n&quot;);
2246         CRASH();
2247     }
2248 
2249     if (canCompile(callerCodeBlock-&gt;capabilityLevelState()))
2250         return;
2251 
2252     if (Options::verboseCallLink())
2253         dataLog(&quot;    Clearing SABI because the caller is not a DFG candidate.\n&quot;);
2254 
2255     m_shouldAlwaysBeInlined = false;
</pre>
<hr />
<pre>
2373     // count 320, which got compiled early as it should have been) and one where they were
2374     // totally wrong (code block in 3d-cube with instruction count 1268, which was expensive
2375     // to compile and didn&#39;t run often enough to warrant compilation in my opinion), and
2376     // then threw in additional data points that represented my own guess of what our
2377     // heuristics should do for some round-numbered examples.
2378     //
2379     // The expression to which I decided to fit the data arose because I started with an
2380     // affine function, and then did two things: put the linear part in an Abs to ensure
2381     // that the fit didn&#39;t end up choosing a negative value of c (which would result in
2382     // the function turning over and going negative for large x) and I threw in a Sqrt
2383     // term because Sqrt represents my intution that the function should be more sensitive
2384     // to small changes in small values of x, but less sensitive when x gets large.
2385 
2386     // Note that the current fit essentially eliminates the linear portion of the
2387     // expression (c == 0.0).
2388     const double a = 0.061504;
2389     const double b = 1.02406;
2390     const double c = 0.0;
2391     const double d = 0.825914;
2392 
<span class="line-modified">2393     double bytecodeCost = this-&gt;bytecodeCost();</span>
2394 
<span class="line-modified">2395     ASSERT(bytecodeCost); // Make sure this is called only after we have an instruction stream; otherwise it&#39;ll just return the value of d, which makes no sense.</span>
2396 
<span class="line-modified">2397     double result = d + a * sqrt(bytecodeCost + b) + c * bytecodeCost;</span>
2398 
2399     result *= codeTypeThresholdMultiplier();
2400 
2401     if (Options::verboseOSR()) {
2402         dataLog(
<span class="line-modified">2403             *this, &quot;: bytecode cost is &quot;, bytecodeCost,</span>
2404             &quot;, scaling execution counter by &quot;, result, &quot; * &quot;, codeTypeThresholdMultiplier(),
2405             &quot;\n&quot;);
2406     }
2407     return result;
2408 }
2409 
2410 static int32_t clipThreshold(double threshold)
2411 {
2412     if (threshold &lt; 1.0)
2413         return 1;
2414 
2415     if (threshold &gt; static_cast&lt;double&gt;(std::numeric_limits&lt;int32_t&gt;::max()))
2416         return std::numeric_limits&lt;int32_t&gt;::max();
2417 
2418     return static_cast&lt;int32_t&gt;(threshold);
2419 }
2420 
2421 int32_t CodeBlock::adjustedCounterValue(int32_t desiredThreshold)
2422 {
2423     return clipThreshold(
</pre>
<hr />
<pre>
2453         return OptimizeAction::None;
2454     }
2455 
2456     exit.m_count++;
2457     m_osrExitCounter++;
2458 
2459     CodeBlock* baselineCodeBlock = exitState.baselineCodeBlock;
2460     ASSERT(baselineCodeBlock == baselineAlternative());
2461     if (UNLIKELY(baselineCodeBlock-&gt;jitExecuteCounter().hasCrossedThreshold()))
2462         return OptimizeAction::ReoptimizeNow;
2463 
2464     // We want to figure out if there&#39;s a possibility that we&#39;re in a loop. For the outermost
2465     // code block in the inline stack, we handle this appropriately by having the loop OSR trigger
2466     // check the exit count of the replacement of the CodeBlock from which we are OSRing. The
2467     // problem is the inlined functions, which might also have loops, but whose baseline versions
2468     // don&#39;t know where to look for the exit count. Figure out if those loops are severe enough
2469     // that we had tried to OSR enter. If so, then we should use the loop reoptimization trigger.
2470     // Otherwise, we should use the normal reoptimization trigger.
2471 
2472     bool didTryToEnterInLoop = false;
<span class="line-modified">2473     for (InlineCallFrame* inlineCallFrame = exit.m_codeOrigin.inlineCallFrame(); inlineCallFrame; inlineCallFrame = inlineCallFrame-&gt;directCaller.inlineCallFrame()) {</span>
2474         if (inlineCallFrame-&gt;baselineCodeBlock-&gt;ownerExecutable()-&gt;didTryToEnterInLoop()) {
2475             didTryToEnterInLoop = true;
2476             break;
2477         }
2478     }
2479 
2480     uint32_t exitCountThreshold = didTryToEnterInLoop
2481         ? exitCountThresholdForReoptimizationFromLoop()
2482         : exitCountThresholdForReoptimization();
2483 
2484     if (m_osrExitCounter &gt; exitCountThreshold)
2485         return OptimizeAction::ReoptimizeNow;
2486 
2487     // Too few fails. Adjust the execution counter such that the target is to only optimize after a while.
2488     baselineCodeBlock-&gt;m_jitExecuteCounter.setNewThresholdForOSRExit(exitState.activeThreshold, exitState.memoryUsageAdjustedThreshold);
2489     return OptimizeAction::None;
2490 }
2491 #endif
2492 
2493 void CodeBlock::optimizeNextInvocation()
</pre>
<hr />
<pre>
2527 void CodeBlock::optimizeSoon()
2528 {
2529     if (Options::verboseOSR())
2530         dataLog(*this, &quot;: Optimizing soon.\n&quot;);
2531 #if ENABLE(DFG_JIT)
2532     m_jitExecuteCounter.setNewThreshold(
2533         adjustedCounterValue(Options::thresholdForOptimizeSoon()), this);
2534 #endif
2535 }
2536 
2537 void CodeBlock::forceOptimizationSlowPathConcurrently()
2538 {
2539     if (Options::verboseOSR())
2540         dataLog(*this, &quot;: Forcing slow path concurrently.\n&quot;);
2541     m_jitExecuteCounter.forceSlowPathConcurrently();
2542 }
2543 
2544 #if ENABLE(DFG_JIT)
2545 void CodeBlock::setOptimizationThresholdBasedOnCompilationResult(CompilationResult result)
2546 {
<span class="line-modified">2547     JITType type = jitType();</span>
<span class="line-modified">2548     if (type != JITType::BaselineJIT) {</span>
2549         dataLog(*this, &quot;: expected to have baseline code but have &quot;, type, &quot;\n&quot;);
<span class="line-modified">2550         CRASH_WITH_INFO(bitwise_cast&lt;uintptr_t&gt;(jitCode().get()), static_cast&lt;uint8_t&gt;(type));</span>
2551     }
2552 
2553     CodeBlock* replacement = this-&gt;replacement();
2554     bool hasReplacement = (replacement &amp;&amp; replacement != this);
2555     if ((result == CompilationSuccessful) != hasReplacement) {
2556         dataLog(*this, &quot;: we have result = &quot;, result, &quot; but &quot;);
2557         if (replacement == this)
2558             dataLog(&quot;we are our own replacement.\n&quot;);
2559         else
2560             dataLog(&quot;our replacement is &quot;, pointerDump(replacement), &quot;\n&quot;);
2561         RELEASE_ASSERT_NOT_REACHED();
2562     }
2563 
2564     switch (result) {
2565     case CompilationSuccessful:
2566         RELEASE_ASSERT(replacement &amp;&amp; JITCode::isOptimizingJIT(replacement-&gt;jitType()));
2567         optimizeNextInvocation();
2568         return;
2569     case CompilationFailed:
2570         dontOptimizeAnytimeSoon();
</pre>
<hr />
<pre>
2613 uint32_t CodeBlock::exitCountThresholdForReoptimizationFromLoop()
2614 {
2615     return adjustedExitCountThreshold(Options::osrExitCountForReoptimizationFromLoop() * codeTypeThresholdMultiplier());
2616 }
2617 
2618 bool CodeBlock::shouldReoptimizeNow()
2619 {
2620     return osrExitCounter() &gt;= exitCountThresholdForReoptimization();
2621 }
2622 
2623 bool CodeBlock::shouldReoptimizeFromLoopNow()
2624 {
2625     return osrExitCounter() &gt;= exitCountThresholdForReoptimizationFromLoop();
2626 }
2627 #endif
2628 
2629 ArrayProfile* CodeBlock::getArrayProfile(const ConcurrentJSLocker&amp;, unsigned bytecodeOffset)
2630 {
2631     auto instruction = instructions().at(bytecodeOffset);
2632     switch (instruction-&gt;opcodeID()) {
<span class="line-modified">2633 #define CASE1(Op) \</span>
2634     case Op::opcodeID: \
2635         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_arrayProfile;
2636 
<span class="line-modified">2637 #define CASE2(Op) \</span>
<span class="line-modified">2638     case Op::opcodeID: \</span>
<span class="line-added">2639         return &amp;instruction-&gt;as&lt;Op&gt;().metadata(this).m_callLinkInfo.m_arrayProfile;</span>
<span class="line-added">2640 </span>
<span class="line-added">2641     FOR_EACH_OPCODE_WITH_ARRAY_PROFILE(CASE1)</span>
<span class="line-added">2642     FOR_EACH_OPCODE_WITH_LLINT_CALL_LINK_INFO(CASE2)</span>
<span class="line-added">2643 </span>
<span class="line-added">2644 #undef CASE1</span>
<span class="line-added">2645 #undef CASE2</span>
2646 
2647     case OpGetById::opcodeID: {
2648         auto bytecode = instruction-&gt;as&lt;OpGetById&gt;();
2649         auto&amp; metadata = bytecode.metadata(this);
<span class="line-modified">2650         if (metadata.m_modeMetadata.mode == GetByIdMode::ArrayLength)</span>
2651             return &amp;metadata.m_modeMetadata.arrayLengthMode.arrayProfile;
2652         break;
2653     }
2654     default:
2655         break;
2656     }
2657 
2658     return nullptr;
2659 }
2660 
2661 ArrayProfile* CodeBlock::getArrayProfile(unsigned bytecodeOffset)
2662 {
2663     ConcurrentJSLocker locker(m_lock);
2664     return getArrayProfile(locker, bytecodeOffset);
2665 }
2666 
2667 #if ENABLE(DFG_JIT)
2668 Vector&lt;CodeOrigin, 0, UnsafeVectorOverflow&gt;&amp; CodeBlock::codeOrigins()
2669 {
2670     return m_jitCode-&gt;dfgCommon()-&gt;codeOrigins;
2671 }
2672 
2673 size_t CodeBlock::numberOfDFGIdentifiers() const
2674 {
2675     if (!JITCode::isOptimizingJIT(jitType()))
2676         return 0;
2677 
2678     return m_jitCode-&gt;dfgCommon()-&gt;dfgIdentifiers.size();
2679 }
2680 
2681 const Identifier&amp; CodeBlock::identifier(int index) const
2682 {
2683     size_t unlinkedIdentifiers = m_unlinkedCode-&gt;numberOfIdentifiers();
2684     if (static_cast&lt;unsigned&gt;(index) &lt; unlinkedIdentifiers)
2685         return m_unlinkedCode-&gt;identifier(index);
2686     ASSERT(JITCode::isOptimizingJIT(jitType()));
2687     return m_jitCode-&gt;dfgCommon()-&gt;dfgIdentifiers[index - unlinkedIdentifiers];
2688 }
2689 #endif // ENABLE(DFG_JIT)
2690 
<span class="line-modified">2691 void CodeBlock::updateAllValueProfilePredictionsAndCountLiveness(unsigned&amp; numberOfLiveNonArgumentValueProfiles, unsigned&amp; numberOfSamplesInProfiles)</span>
2692 {
2693     ConcurrentJSLocker locker(m_lock);
2694 
2695     numberOfLiveNonArgumentValueProfiles = 0;
2696     numberOfSamplesInProfiles = 0; // If this divided by ValueProfile::numberOfBuckets equals numberOfValueProfiles() then value profiles are full.
2697 
<span class="line-modified">2698     forEachValueProfile([&amp;](ValueProfile&amp; profile, bool isArgument) {</span>
2699         unsigned numSamples = profile.totalNumberOfSamples();
<span class="line-added">2700         static_assert(ValueProfile::numberOfBuckets == 1);</span>
2701         if (numSamples &gt; ValueProfile::numberOfBuckets)
2702             numSamples = ValueProfile::numberOfBuckets; // We don&#39;t want profiles that are extremely hot to be given more weight.
2703         numberOfSamplesInProfiles += numSamples;
<span class="line-modified">2704         if (isArgument) {</span>
2705             profile.computeUpdatedPrediction(locker);
2706             return;
2707         }
<span class="line-modified">2708         if (profile.numberOfSamples() || profile.isSampledBefore())</span>
2709             numberOfLiveNonArgumentValueProfiles++;
2710         profile.computeUpdatedPrediction(locker);
2711     });
2712 
2713     if (auto* rareData = m_rareData.get()) {
2714         for (auto&amp; profileBucket : rareData-&gt;m_catchProfiles) {
2715             profileBucket-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {
<span class="line-modified">2716                 profile.computeUpdatedPrediction(locker);</span>
2717             });
2718         }
2719     }
2720 
2721 #if ENABLE(DFG_JIT)
2722     lazyOperandValueProfiles(locker).computeUpdatedPredictions(locker);
2723 #endif
2724 }
2725 
2726 void CodeBlock::updateAllValueProfilePredictions()
2727 {
2728     unsigned ignoredValue1, ignoredValue2;
<span class="line-modified">2729     updateAllValueProfilePredictionsAndCountLiveness(ignoredValue1, ignoredValue2);</span>
2730 }
2731 
2732 void CodeBlock::updateAllArrayPredictions()
2733 {
2734     ConcurrentJSLocker locker(m_lock);
2735 
2736     forEachArrayProfile([&amp;](ArrayProfile&amp; profile) {
2737         profile.computeUpdatedPrediction(locker, this);
2738     });
2739 
2740     forEachArrayAllocationProfile([&amp;](ArrayAllocationProfile&amp; profile) {
2741         profile.updateProfile();
2742     });
2743 }
2744 
2745 void CodeBlock::updateAllPredictions()
2746 {
2747     updateAllValueProfilePredictions();
2748     updateAllArrayPredictions();
2749 }
2750 
2751 bool CodeBlock::shouldOptimizeNow()
2752 {
2753     if (Options::verboseOSR())
2754         dataLog(&quot;Considering optimizing &quot;, *this, &quot;...\n&quot;);
2755 
2756     if (m_optimizationDelayCounter &gt;= Options::maximumOptimizationDelay())
2757         return true;
2758 
2759     updateAllArrayPredictions();
2760 
2761     unsigned numberOfLiveNonArgumentValueProfiles;
2762     unsigned numberOfSamplesInProfiles;
<span class="line-modified">2763     updateAllValueProfilePredictionsAndCountLiveness(numberOfLiveNonArgumentValueProfiles, numberOfSamplesInProfiles);</span>
2764 
2765     if (Options::verboseOSR()) {
2766         dataLogF(
2767             &quot;Profile hotness: %lf (%u / %u), %lf (%u / %u)\n&quot;,
2768             (double)numberOfLiveNonArgumentValueProfiles / numberOfNonArgumentValueProfiles(),
2769             numberOfLiveNonArgumentValueProfiles, numberOfNonArgumentValueProfiles(),
2770             (double)numberOfSamplesInProfiles / ValueProfile::numberOfBuckets / numberOfNonArgumentValueProfiles(),
2771             numberOfSamplesInProfiles, ValueProfile::numberOfBuckets * numberOfNonArgumentValueProfiles());
2772     }
2773 
2774     if ((!numberOfNonArgumentValueProfiles() || (double)numberOfLiveNonArgumentValueProfiles / numberOfNonArgumentValueProfiles() &gt;= Options::desiredProfileLivenessRate())
2775         &amp;&amp; (!totalNumberOfValueProfiles() || (double)numberOfSamplesInProfiles / ValueProfile::numberOfBuckets / totalNumberOfValueProfiles() &gt;= Options::desiredProfileFullnessRate())
2776         &amp;&amp; static_cast&lt;unsigned&gt;(m_optimizationDelayCounter) + 1 &gt;= Options::minimumOptimizationDelay())
2777         return true;
2778 
2779     ASSERT(m_optimizationDelayCounter &lt; std::numeric_limits&lt;uint8_t&gt;::max());
2780     m_optimizationDelayCounter++;
2781     optimizeAfterWarmUp();
2782     return false;
2783 }
2784 
2785 #if ENABLE(DFG_JIT)
2786 void CodeBlock::tallyFrequentExitSites()
2787 {
2788     ASSERT(JITCode::isOptimizingJIT(jitType()));
<span class="line-modified">2789     ASSERT(alternative()-&gt;jitType() == JITType::BaselineJIT);</span>
2790 
2791     CodeBlock* profiledBlock = alternative();
2792 
2793     switch (jitType()) {
<span class="line-modified">2794     case JITType::DFGJIT: {</span>
2795         DFG::JITCode* jitCode = m_jitCode-&gt;dfg();
2796         for (auto&amp; exit : jitCode-&gt;osrExit)
2797             exit.considerAddingAsFrequentExitSite(profiledBlock);
2798         break;
2799     }
2800 
2801 #if ENABLE(FTL_JIT)
<span class="line-modified">2802     case JITType::FTLJIT: {</span>
2803         // There is no easy way to avoid duplicating this code since the FTL::JITCode::osrExit
2804         // vector contains a totally different type, that just so happens to behave like
2805         // DFG::JITCode::osrExit.
2806         FTL::JITCode* jitCode = m_jitCode-&gt;ftl();
2807         for (unsigned i = 0; i &lt; jitCode-&gt;osrExit.size(); ++i) {
2808             FTL::OSRExit&amp; exit = jitCode-&gt;osrExit[i];
2809             exit.considerAddingAsFrequentExitSite(profiledBlock);
2810         }
2811         break;
2812     }
2813 #endif
2814 
2815     default:
2816         RELEASE_ASSERT_NOT_REACHED();
2817         break;
2818     }
2819 }
2820 #endif // ENABLE(DFG_JIT)
2821 
2822 void CodeBlock::notifyLexicalBindingUpdate()
</pre>
<hr />
<pre>
2846             ResolveType originalResolveType = metadata.m_resolveType;
2847             if (originalResolveType == GlobalProperty || originalResolveType == GlobalPropertyWithVarInjectionChecks) {
2848                 const Identifier&amp; ident = identifier(bytecode.m_var);
2849                 if (isShadowed(ident.impl()))
2850                     metadata.m_globalLexicalBindingEpoch = 0;
2851                 else
2852                     metadata.m_globalLexicalBindingEpoch = globalObject-&gt;globalLexicalBindingEpoch();
2853             }
2854             break;
2855         }
2856         default:
2857             break;
2858         }
2859     }
2860 }
2861 
2862 #if ENABLE(VERBOSE_VALUE_PROFILE)
2863 void CodeBlock::dumpValueProfiles()
2864 {
2865     dataLog(&quot;ValueProfile for &quot;, *this, &quot;:\n&quot;);
<span class="line-modified">2866     forEachValueProfile([](ValueProfile&amp; profile, bool isArgument) {</span>
<span class="line-modified">2867         if (isArgument)</span>
<span class="line-modified">2868             dataLogF(&quot;   arg: &quot;);</span>
<span class="line-modified">2869         else</span>
<span class="line-modified">2870             dataLogF(&quot;   bc: &quot;);</span>

2871         if (!profile.numberOfSamples() &amp;&amp; profile.m_prediction == SpecNone) {
2872             dataLogF(&quot;&lt;empty&gt;\n&quot;);
2873             continue;
2874         }
2875         profile.dump(WTF::dataFile());
2876         dataLogF(&quot;\n&quot;);
2877     });
2878     dataLog(&quot;RareCaseProfile for &quot;, *this, &quot;:\n&quot;);
2879     if (auto* jitData = m_jitData.get()) {
2880         for (RareCaseProfile* profile : jitData-&gt;m_rareCaseProfiles)
2881             dataLogF(&quot;   bc = %d: %u\n&quot;, profile-&gt;m_bytecodeOffset, profile-&gt;m_counter);
2882     }
2883 }
2884 #endif // ENABLE(VERBOSE_VALUE_PROFILE)
2885 
2886 unsigned CodeBlock::frameRegisterCount()
2887 {
2888     switch (jitType()) {
<span class="line-modified">2889     case JITType::InterpreterThunk:</span>
2890         return LLInt::frameRegisterCountFor(this);
2891 
2892 #if ENABLE(JIT)
<span class="line-modified">2893     case JITType::BaselineJIT:</span>
2894         return JIT::frameRegisterCountFor(this);
2895 #endif // ENABLE(JIT)
2896 
2897 #if ENABLE(DFG_JIT)
<span class="line-modified">2898     case JITType::DFGJIT:</span>
<span class="line-modified">2899     case JITType::FTLJIT:</span>
2900         return jitCode()-&gt;dfgCommon()-&gt;frameRegisterCount;
2901 #endif // ENABLE(DFG_JIT)
2902 
2903     default:
2904         RELEASE_ASSERT_NOT_REACHED();
2905         return 0;
2906     }
2907 }
2908 
2909 int CodeBlock::stackPointerOffset()
2910 {
2911     return virtualRegisterForLocal(frameRegisterCount() - 1).offset();
2912 }
2913 
2914 size_t CodeBlock::predictedMachineCodeSize()
2915 {
2916     VM* vm = m_vm;
2917     // This will be called from CodeBlock::CodeBlock before either m_vm or the
2918     // instructions have been initialized. It&#39;s OK to return 0 because what will really
2919     // matter is the recomputation of this value when the slow path is triggered.
2920     if (!vm)
2921         return 0;
2922 
2923     if (!*vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT)
2924         return 0; // It&#39;s as good of a prediction as we&#39;ll get.
2925 
2926     // Be conservative: return a size that will be an overestimation 84% of the time.
2927     double multiplier = vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;mean() +
2928         vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;standardDeviation();
2929 
2930     // Be paranoid: silently reject bogus multipiers. Silently doing the &quot;wrong&quot; thing
2931     // here is OK, since this whole method is just a heuristic.
2932     if (multiplier &lt; 0 || multiplier &gt; 1000)
2933         return 0;
2934 
<span class="line-modified">2935     double doubleResult = multiplier * bytecodeCost();</span>
2936 
2937     // Be even more paranoid: silently reject values that won&#39;t fit into a size_t. If
2938     // the function is so huge that we can&#39;t even fit it into virtual memory then we
2939     // should probably have some other guards in place to prevent us from even getting
2940     // to this point.
2941     if (doubleResult &gt; std::numeric_limits&lt;size_t&gt;::max())
2942         return 0;
2943 
2944     return static_cast&lt;size_t&gt;(doubleResult);
2945 }
2946 
2947 String CodeBlock::nameForRegister(VirtualRegister virtualRegister)
2948 {
2949     for (auto&amp; constantRegister : m_constantRegisters) {
2950         if (constantRegister.get().isEmpty())
2951             continue;
<span class="line-modified">2952         if (SymbolTable* symbolTable = jsDynamicCast&lt;SymbolTable*&gt;(vm(), constantRegister.get())) {</span>
2953             ConcurrentJSLocker locker(symbolTable-&gt;m_lock);
2954             auto end = symbolTable-&gt;end(locker);
2955             for (auto ptr = symbolTable-&gt;begin(locker); ptr != end; ++ptr) {
2956                 if (ptr-&gt;value.varOffset() == VarOffset(virtualRegister)) {
2957                     // FIXME: This won&#39;t work from the compilation thread.
2958                     // https://bugs.webkit.org/show_bug.cgi?id=115300
2959                     return ptr-&gt;key.get();
2960                 }
2961             }
2962         }
2963     }
2964     if (virtualRegister == thisRegister())
2965         return &quot;this&quot;_s;
2966     if (virtualRegister.isArgument())
2967         return makeString(&quot;arguments[&quot;, pad(&#39; &#39;, 3, virtualRegister.toArgument()), &#39;]&#39;);
2968 
2969     return emptyString();
2970 }
2971 
2972 ValueProfile* CodeBlock::tryGetValueProfileForBytecodeOffset(int bytecodeOffset)
</pre>
<hr />
<pre>
3158         // The following check allows for the same textual JavaScript basic block to have its bytecode emitted more
3159         // than once and still play nice with the control flow profiler. When basicBlockStartOffset is larger than
3160         // basicBlockEndOffset, it indicates that the bytecode generator has emitted code for the same AST node
3161         // more than once (for example: ForInNode, Finally blocks in TryNode, etc). Though these are different
3162         // basic blocks at the bytecode level, they are generated from the same textual basic block in the JavaScript
3163         // program. The condition:
3164         // (basicBlockEndOffset &lt; basicBlockStartOffset)
3165         // is encountered when op_profile_control_flow lies across the boundary of these duplicated bytecode basic
3166         // blocks and the textual offset goes from the end of the duplicated block back to the beginning. These
3167         // ranges are dummy ranges and are ignored. The duplicated bytecode basic blocks point to the same
3168         // internal data structure, so if any of them execute, it will record the same textual basic block in the
3169         // JavaScript program as executing.
3170         // At the bytecode level, this situation looks like:
3171         // j: op_profile_control_flow (from j-&gt;k, we have basicBlockEndOffset &lt; basicBlockStartOffset)
3172         // ...
3173         // k: op_profile_control_flow (we want to skip over the j-&gt;k block and start fresh at offset k as the start of a new basic block k-&gt;m).
3174         // ...
3175         // m: op_profile_control_flow
3176         if (basicBlockEndOffset &lt; basicBlockStartOffset) {
3177             RELEASE_ASSERT(i + 1 &lt; offsetsLength); // We should never encounter dummy blocks at the end of a CodeBlock.
<span class="line-modified">3178             metadata.m_basicBlockLocation = vm().controlFlowProfiler()-&gt;dummyBasicBlock();</span>
3179             continue;
3180         }
3181 
<span class="line-modified">3182         BasicBlockLocation* basicBlockLocation = vm().controlFlowProfiler()-&gt;getBasicBlockLocation(ownerExecutable()-&gt;sourceID(), basicBlockStartOffset, basicBlockEndOffset);</span>
3183 
3184         // Find all functions that are enclosed within the range: [basicBlockStartOffset, basicBlockEndOffset]
3185         // and insert these functions&#39; start/end offsets as gaps in the current BasicBlockLocation.
3186         // This is necessary because in the original source text of a JavaScript program,
3187         // function literals form new basic blocks boundaries, but they aren&#39;t represented
3188         // inside the CodeBlock&#39;s instruction stream.
3189         auto insertFunctionGaps = [basicBlockLocation, basicBlockStartOffset, basicBlockEndOffset] (const WriteBarrier&lt;FunctionExecutable&gt;&amp; functionExecutable) {
3190             const UnlinkedFunctionExecutable* executable = functionExecutable-&gt;unlinkedExecutable();
3191             int functionStart = executable-&gt;typeProfilingStartOffset();
3192             int functionEnd = executable-&gt;typeProfilingEndOffset();
3193             if (functionStart &gt;= basicBlockStartOffset &amp;&amp; functionEnd &lt;= basicBlockEndOffset)
3194                 basicBlockLocation-&gt;insertGap(functionStart, functionEnd);
3195         };
3196 
3197         for (const WriteBarrier&lt;FunctionExecutable&gt;&amp; executable : m_functionDecls)
3198             insertFunctionGaps(executable);
3199         for (const WriteBarrier&lt;FunctionExecutable&gt;&amp; executable : m_functionExprs)
3200             insertFunctionGaps(executable);
3201 
3202         metadata.m_basicBlockLocation = basicBlockLocation;
</pre>
<hr />
<pre>
3220                     return codeOrigin;
3221             }
3222 
3223             for (StructureStubInfo* stubInfo : jitData-&gt;m_stubInfos) {
3224                 if (stubInfo-&gt;containsPC(pc))
3225                     return Optional&lt;CodeOrigin&gt;(stubInfo-&gt;codeOrigin);
3226             }
3227         }
3228     }
3229 
3230     if (Optional&lt;CodeOrigin&gt; codeOrigin = m_jitCode-&gt;findPC(this, pc))
3231         return codeOrigin;
3232 
3233     return WTF::nullopt;
3234 }
3235 #endif // ENABLE(JIT)
3236 
3237 Optional&lt;unsigned&gt; CodeBlock::bytecodeOffsetFromCallSiteIndex(CallSiteIndex callSiteIndex)
3238 {
3239     Optional&lt;unsigned&gt; bytecodeOffset;
<span class="line-modified">3240     JITType jitType = this-&gt;jitType();</span>
<span class="line-modified">3241     if (jitType == JITType::InterpreterThunk || jitType == JITType::BaselineJIT) {</span>
3242 #if USE(JSVALUE64)
3243         bytecodeOffset = callSiteIndex.bits();
3244 #else
3245         Instruction* instruction = bitwise_cast&lt;Instruction*&gt;(callSiteIndex.bits());
3246         bytecodeOffset = this-&gt;bytecodeOffset(instruction);
3247 #endif
<span class="line-modified">3248     } else if (jitType == JITType::DFGJIT || jitType == JITType::FTLJIT) {</span>
3249 #if ENABLE(DFG_JIT)
3250         RELEASE_ASSERT(canGetCodeOrigin(callSiteIndex));
3251         CodeOrigin origin = codeOrigin(callSiteIndex);
<span class="line-modified">3252         bytecodeOffset = origin.bytecodeIndex();</span>
3253 #else
3254         RELEASE_ASSERT_NOT_REACHED();
3255 #endif
3256     }
3257 
3258     return bytecodeOffset;
3259 }
3260 
3261 int32_t CodeBlock::thresholdForJIT(int32_t threshold)
3262 {
3263     switch (unlinkedCodeBlock()-&gt;didOptimize()) {
3264     case MixedTriState:
3265         return threshold;
3266     case FalseTriState:
3267         return threshold * 4;
3268     case TrueTriState:
3269         return threshold / 2;
3270     }
3271     ASSERT_NOT_REACHED();
3272     return threshold;
</pre>
</td>
</tr>
</table>
<center><a href="CallVariant.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="CodeBlock.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>