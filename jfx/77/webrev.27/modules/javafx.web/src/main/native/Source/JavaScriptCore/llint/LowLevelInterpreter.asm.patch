diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter.asm b/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter.asm
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter.asm
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter.asm
@@ -1,6 +1,6 @@
-# Copyright (C) 2011-2019 Apple Inc. All rights reserved.
+# Copyrsght (C) 2011-2019 Apple Inc. All rights reserved.
 #
 # Redistribution and use in source and binary forms, with or without
 # modification, are permitted provided that the following conditions
 # are met:
 # 1. Redistributions of source code must retain the above copyright
@@ -74,11 +74,11 @@
 #  - lr is defined on non-X86 architectures (ARM64, ARM64E, ARMv7, MIPS and CLOOP)
 #  and holds the return PC
 #
 #  - pc holds the (native) program counter on 32-bits ARM architectures (ARMv7)
 #
-#  - t0, t1, t2, t3, t4 and optionally t5 are temporary registers that can get trashed on
+#  - t0, t1, t2, t3, t4, and optionally t5, t6, and t7 are temporary registers that can get trashed on
 #  calls, and are pairwise distinct registers. t4 holds the JS program counter, so use
 #  with caution in opcodes (actually, don't use it in opcodes at all, except as PC).
 #
 #  - r0 and r1 are the platform's customary return registers, and thus are
 #  two distinct registers
@@ -179,10 +179,13 @@
 const ArgumentCount = Callee + SlotSize
 const ThisArgumentOffset = ArgumentCount + SlotSize
 const FirstArgumentOffset = ThisArgumentOffset + SlotSize
 const CallFrameHeaderSize = ThisArgumentOffset
 
+const MetadataOffsetTable16Offset = 0
+const MetadataOffsetTable32Offset = constexpr UnlinkedMetadataTable::s_offset16TableSize
+
 # Some value representation constants.
 if JSVALUE64
     const TagBitTypeOther = constexpr TagBitTypeOther
     const TagBitBool      = constexpr TagBitBool
     const TagBitUndefined = constexpr TagBitUndefined
@@ -213,11 +216,11 @@
 
 const maxFrameExtentForSlowPathCall = constexpr maxFrameExtentForSlowPathCall
 
 if X86_64 or X86_64_WIN or ARM64 or ARM64E
     const CalleeSaveSpaceAsVirtualRegisters = 4
-elsif C_LOOP
+elsif C_LOOP or C_LOOP_WIN
     const CalleeSaveSpaceAsVirtualRegisters = 1
 elsif ARMv7
     const CalleeSaveSpaceAsVirtualRegisters = 1
 elsif MIPS
     const CalleeSaveSpaceAsVirtualRegisters = 1
@@ -272,20 +275,20 @@
     elsif X86_64_WIN
         const metadataTable = csr3
         const PB = csr4
         const tagTypeNumber = csr5
         const tagMask = csr6
-    elsif C_LOOP
+    elsif C_LOOP or C_LOOP_WIN
         const PB = csr0
         const tagTypeNumber = csr1
         const tagMask = csr2
         const metadataTable = csr3
     end
 
 else
     const PC = t4 # When changing this, make sure LLIntPC is up to date in LLIntPCRanges.h
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         const metadataTable = csr3
     elsif ARMv7
         const metadataTable = csr0
     elsif MIPS
         const metadataTable = csr0
@@ -306,39 +309,50 @@
 macro dispatchOp(size, opcodeName)
     macro dispatchNarrow()
         dispatch(constexpr %opcodeName%_length)
     end
 
-    macro dispatchWide()
+    macro dispatchWide16()
+        dispatch(constexpr %opcodeName%_length * 2 + 1)
+    end
+
+    macro dispatchWide32()
         dispatch(constexpr %opcodeName%_length * 4 + 1)
     end
 
-    size(dispatchNarrow, dispatchWide, macro (dispatch) dispatch() end)
+    size(dispatchNarrow, dispatchWide16, dispatchWide32, macro (dispatch) dispatch() end)
 end
 
 macro getu(size, opcodeStruct, fieldName, dst)
-    size(getuOperandNarrow, getuOperandWide, macro (getu)
+    size(getuOperandNarrow, getuOperandWide16, getuOperandWide32, macro (getu)
         getu(opcodeStruct, fieldName, dst)
     end)
 end
 
 macro get(size, opcodeStruct, fieldName, dst)
-    size(getOperandNarrow, getOperandWide, macro (get)
+    size(getOperandNarrow, getOperandWide16, getOperandWide32, macro (get)
         get(opcodeStruct, fieldName, dst)
     end)
 end
 
-macro narrow(narrowFn, wideFn, k)
+macro narrow(narrowFn, wide16Fn, wide32Fn, k)
     k(narrowFn)
 end
 
-macro wide(narrowFn, wideFn, k)
-    k(wideFn)
+macro wide16(narrowFn, wide16Fn, wide32Fn, k)
+    k(wide16Fn)
+end
+
+macro wide32(narrowFn, wide16Fn, wide32Fn, k)
+    k(wide32Fn)
 end
 
 macro metadata(size, opcode, dst, scratch)
-    loadi constexpr %opcode%::opcodeID * 4[metadataTable], dst # offset = metadataTable<unsigned*>[opcodeID]
+    loadh (constexpr %opcode%::opcodeID * 2 + MetadataOffsetTable16Offset)[metadataTable], dst # offset = metadataTable<uint16_t*>[opcodeID]
+    btinz dst, .setUpOffset
+    loadi (constexpr %opcode%::opcodeID * 4 + MetadataOffsetTable32Offset)[metadataTable], dst # offset = metadataTable<uint32_t*>[opcodeID]
+.setUpOffset:
     getu(size, opcode, m_metadataID, scratch) # scratch = bytecode.m_metadataID
     muli sizeof %opcode%::Metadata, scratch # scratch *= sizeof(Op::Metadata)
     addi scratch, dst # offset += scratch
     addp metadataTable, dst # return &metadataTable[offset]
 end
@@ -354,18 +368,28 @@
 macro commonOp(label, prologue, fn)
 _%label%:
     prologue()
     fn(narrow)
 
-_%label%_wide:
+# FIXME: We cannot enable wide16 bytecode in Windows CLoop. With MSVC, as CLoop::execute gets larger code
+# size, CLoop::execute gets higher stack height requirement. This makes CLoop::execute takes 160KB stack
+# per call, causes stack overflow error easily. For now, we disable wide16 optimization for Windows CLoop.
+# https://bugs.webkit.org/show_bug.cgi?id=198283
+if not C_LOOP_WIN
+_%label%_wide16:
     prologue()
-    fn(wide)
+    fn(wide16)
+end
+
+_%label%_wide32:
+    prologue()
+    fn(wide32)
 end
 
 macro op(l, fn)
-    commonOp(l, macro () end, macro (unused)
-        fn()
+    commonOp(l, macro () end, macro (size)
+        size(fn, macro() end, macro() end, macro(gen) gen() end)
     end)
 end
 
 macro llintOp(opcodeName, opcodeStruct, fn)
     commonOp(llint_%opcodeName%, traceExecution, macro(size)
@@ -467,12 +491,13 @@
 # Type flags constants.
 const MasqueradesAsUndefined = constexpr MasqueradesAsUndefined
 const ImplementsDefaultHasInstance = constexpr ImplementsDefaultHasInstance
 
 # Bytecode operand constants.
-const FirstConstantRegisterIndexNarrow = 16
-const FirstConstantRegisterIndexWide = constexpr FirstConstantRegisterIndex
+const FirstConstantRegisterIndexNarrow = constexpr FirstConstantRegisterIndex8
+const FirstConstantRegisterIndexWide16 = constexpr FirstConstantRegisterIndex16
+const FirstConstantRegisterIndexWide32 = constexpr FirstConstantRegisterIndex
 
 # Code type constants.
 const GlobalCode = constexpr GlobalCode
 const EvalCode = constexpr EvalCode
 const FunctionCode = constexpr FunctionCode
@@ -514,11 +539,11 @@
 const VectorBufferOffset = Vector::m_buffer
 const VectorSizeOffset = Vector::m_size
 
 # Some common utilities.
 macro crash()
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         cloopCrash
     else
         call _llint_crash
     end
 end
@@ -597,13 +622,13 @@
     end
 end
 
 macro checkStackPointerAlignment(tempReg, location)
     if ASSERT_ENABLED
-        if ARM64 or ARM64E or C_LOOP
+        if ARM64 or ARM64E or C_LOOP or C_LOOP_WIN
             # ARM64 and ARM64E will check for us!
-            # C_LOOP does not need the alignment, and can use a little perf
+            # C_LOOP or C_LOOP_WIN does not need the alignment, and can use a little perf
             # improvement from avoiding useless work.
         else
             if ARMv7
                 # ARM can't do logical ops with the sp as a source
                 move sp, tempReg
@@ -617,11 +642,11 @@
         .stackPointerOkay:
         end
     end
 end
 
-if C_LOOP or ARM64 or ARM64E or X86_64 or X86_64_WIN
+if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN
     const CalleeSaveRegisterCount = 0
 elsif ARMv7
     const CalleeSaveRegisterCount = 7
 elsif MIPS
     const CalleeSaveRegisterCount = 2
@@ -634,11 +659,11 @@
 # VMEntryTotalFrameSize includes the space for struct VMEntryRecord and the
 # callee save registers rounded up to keep the stack aligned
 const VMEntryTotalFrameSize = (CalleeRegisterSaveSize + sizeof VMEntryRecord + StackAlignment - 1) & ~StackAlignmentMask
 
 macro pushCalleeSaves()
-    if C_LOOP or ARM64 or ARM64E or X86_64 or X86_64_WIN
+    if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN
     elsif ARMv7
         emit "push {r4-r6, r8-r11}"
     elsif MIPS
         emit "addiu $sp, $sp, -8"
         emit "sw $s0, 0($sp)" # csr0/metaData
@@ -655,11 +680,11 @@
         emit "push ebx"
     end
 end
 
 macro popCalleeSaves()
-    if C_LOOP or ARM64 or ARM64E or X86_64 or X86_64_WIN
+    if C_LOOP or C_LOOP_WIN or ARM64 or ARM64E or X86_64 or X86_64_WIN
     elsif ARMv7
         emit "pop {r4-r6, r8-r11}"
     elsif MIPS
         emit "lw $s0, 0($sp)"
         emit "lw $s4, 4($sp)"
@@ -674,11 +699,11 @@
         emit "pop esi"
     end
 end
 
 macro preserveCallerPCAndCFR()
-    if C_LOOP or ARMv7 or MIPS
+    if C_LOOP or C_LOOP_WIN or ARMv7 or MIPS
         push lr
         push cfr
     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
         push cfr
     elsif ARM64 or ARM64E
@@ -689,11 +714,11 @@
     move sp, cfr
 end
 
 macro restoreCallerPCAndCFR()
     move cfr, sp
-    if C_LOOP or ARMv7 or MIPS
+    if C_LOOP or C_LOOP_WIN or ARMv7 or MIPS
         pop cfr
         pop lr
     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
         pop cfr
     elsif ARM64 or ARM64E
@@ -701,11 +726,11 @@
     end
 end
 
 macro preserveCalleeSavesUsedByLLInt()
     subp CalleeSaveSpaceStackAligned, sp
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         storep metadataTable, -PtrSize[cfr]
     elsif ARMv7 or MIPS
         storep metadataTable, -4[cfr]
     elsif ARM64 or ARM64E
         emit "stp x27, x28, [x29, #-16]"
@@ -724,11 +749,11 @@
         storep csr3, -32[cfr]
     end
 end
 
 macro restoreCalleeSavesUsedByLLInt()
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         loadp -PtrSize[cfr], metadataTable
     elsif ARMv7 or MIPS
         loadp -4[cfr], metadataTable
     elsif ARM64 or ARM64E
         emit "ldp x25, x26, [x29, #-32]"
@@ -835,12 +860,12 @@
         end
     end
 end
 
 macro preserveReturnAddressAfterCall(destinationRegister)
-    if C_LOOP or ARMv7 or ARM64 or ARM64E or MIPS
-        # In C_LOOP case, we're only preserving the bytecode vPC.
+    if C_LOOP or C_LOOP_WIN or ARMv7 or ARM64 or ARM64E or MIPS
+        # In C_LOOP or C_LOOP_WIN case, we're only preserving the bytecode vPC.
         move lr, destinationRegister
     elsif X86 or X86_WIN or X86_64 or X86_64_WIN
         pop destinationRegister
     else
         error
@@ -851,11 +876,11 @@
     tagReturnAddress sp
     if X86 or X86_WIN or X86_64 or X86_64_WIN
         push cfr
     elsif ARM64 or ARM64E
         push cfr, lr
-    elsif C_LOOP or ARMv7 or MIPS
+    elsif C_LOOP or C_LOOP_WIN or ARMv7 or MIPS
         push lr
         push cfr
     end
     move sp, cfr
 end
@@ -863,11 +888,11 @@
 macro functionEpilogue()
     if X86 or X86_WIN or X86_64 or X86_64_WIN
         pop cfr
     elsif ARM64 or ARM64E
         pop lr, cfr
-    elsif C_LOOP or ARMv7 or MIPS
+    elsif C_LOOP or C_LOOP_WIN or ARMv7 or MIPS
         pop cfr
         pop lr
     end
 end
 
@@ -897,11 +922,11 @@
         callSlowPath(_llint_trace)
     end
 end
 
 macro callTargetFunction(size, opcodeStruct, dispatch, callee, callPtrTag)
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         cloopCallJSFunction callee
     else
         call callee, callPtrTag
     end
     restoreStackPointerAfterCall()
@@ -935,22 +960,22 @@
     # We assume < 2^28 arguments
     muli SlotSize, temp2
     addi StackAlignment - 1 + CallFrameHeaderSize, temp2
     andi ~StackAlignmentMask, temp2
 
-    if ARMv7 or ARM64 or ARM64E or C_LOOP or MIPS
+    if ARMv7 or ARM64 or ARM64E or C_LOOP or C_LOOP_WIN or MIPS
         addp CallerFrameAndPCSize, sp
         subi CallerFrameAndPCSize, temp2
         loadp CallerFrameAndPC::returnPC[cfr], lr
     else
         addp PtrSize, sp
         subi PtrSize, temp2
         loadp PtrSize[cfr], temp3
         storep temp3, [sp]
     end
 
-    if POINTER_PROFILING
+    if ARM64E
         addp 16, cfr, temp3
         untagReturnAddress temp3
     end
 
     subp temp2, temp1
@@ -1019,11 +1044,11 @@
             callSlowPath(_llint_replace)
         end)
 end
 
 macro assertNotConstant(size, index)
-    size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide, macro (FirstConstantRegisterIndex)
+    size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide16, FirstConstantRegisterIndexWide32, macro (FirstConstantRegisterIndex)
         assert(macro (ok) bilt index, FirstConstantRegisterIndex, ok end)
     end)
 end
 
 macro functionForCallCodeBlockGetter(targetRegister)
@@ -1071,11 +1096,11 @@
         subp maxFrameExtentForSlowPathCall, sp
         callSlowPath(traceSlowPath)
         addp maxFrameExtentForSlowPathCall, sp
     end
     codeBlockGetter(t1)
-    if not C_LOOP
+    if not (C_LOOP or C_LOOP_WIN)
         baddis 5, CodeBlock::m_llintExecuteCounter + BaselineExecutionCounter::m_counter[t1], .continue
         if JSVALUE64
             move cfr, a0
             move PC, a1
             cCall2(osrSlowPath)
@@ -1121,11 +1146,11 @@
     # Get new sp in t0 and check stack height.
     getFrameRegisterSizeForCodeBlock(t1, t0)
     subp cfr, t0, t0
     bpa t0, cfr, .needStackCheck
     loadp CodeBlock::m_vm[t1], t2
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         bpbeq VM::m_cloopStackLimit[t2], t0, .stackHeightOK
     else
         bpbeq VM::m_softStackLimit[t2], t0, .stackHeightOK
     end
 
@@ -1224,29 +1249,29 @@
 
 # stub to call into JavaScript or Native functions
 # EncodedJSValue vmEntryToJavaScript(void* code, VM* vm, ProtoCallFrame* protoFrame)
 # EncodedJSValue vmEntryToNativeFunction(void* code, VM* vm, ProtoCallFrame* protoFrame)
 
-if C_LOOP
+if C_LOOP or C_LOOP_WIN
     _llint_vm_entry_to_javascript:
 else
     global _vmEntryToJavaScript
     _vmEntryToJavaScript:
 end
     doVMEntry(makeJavaScriptCall)
 
 
-if C_LOOP
+if C_LOOP or C_LOOP_WIN
     _llint_vm_entry_to_native:
 else
     global _vmEntryToNative
     _vmEntryToNative:
 end
     doVMEntry(makeHostFunctionCall)
 
 
-if not C_LOOP
+if not (C_LOOP or C_LOOP_WIN)
     # void sanitizeStackForVMImpl(VM* vm)
     global _sanitizeStackForVMImpl
     _sanitizeStackForVMImpl:
         tagReturnAddress sp
         # We need three non-aliased caller-save registers. We are guaranteed
@@ -1282,11 +1307,11 @@
 
         vmEntryRecord(a0, r0)
         ret
 end
 
-if C_LOOP
+if C_LOOP or C_LOOP_WIN
     # Dummy entry point the C Loop uses to initialize.
     _llint_entry:
         crash()
 else
     macro initPCRelative(pcBase)
@@ -1304,45 +1329,53 @@
             setcallreg pcBase # needed to set $t9 to the right value for the .cpload created by the label.
         _relativePCBase:
         end
 end
 
-# The PC base is in t2, as this is what _llint_entry leaves behind through
-# initPCRelative(t2)
+# The PC base is in t3, as this is what _llint_entry leaves behind through
+# initPCRelative(t3)
 macro setEntryAddress(index, label)
     setEntryAddressCommon(index, label, a0)
 end
 
-macro setEntryAddressWide(index, label)
+macro setEntryAddressWide16(index, label)
      setEntryAddressCommon(index, label, a1)
 end
 
+macro setEntryAddressWide32(index, label)
+     setEntryAddressCommon(index, label, a2)
+end
+
 macro setEntryAddressCommon(index, label, map)
-    if X86_64 or X86_64_WIN
-        leap (label - _relativePCBase)[t2], t3
-        move index, t4
-        storep t3, [map, t4, 8]
+    if X86_64
+        leap (label - _relativePCBase)[t3], t4
+        move index, t5
+        storep t4, [map, t5, 8]
+    elsif X86_64_WIN
+        leap (label - _relativePCBase)[t3], t4
+        move index, t0
+        storep t4, [map, t0, 8]
     elsif X86 or X86_WIN
-        leap (label - _relativePCBase)[t2], t3
-        move index, t4
-        storep t3, [map, t4, 4]
+        leap (label - _relativePCBase)[t3], t4
+        move index, t5
+        storep t4, [map, t5, 4]
     elsif ARM64 or ARM64E
-        pcrtoaddr label, t2
+        pcrtoaddr label, t3
         move index, t4
-        storep t2, [map, t4, PtrSize]
+        storep t3, [map, t4, PtrSize]
     elsif ARMv7
         mvlbl (label - _relativePCBase), t4
-        addp t4, t2, t4
-        move index, t3
-        storep t4, [map, t3, 4]
+        addp t4, t3, t4
+        move index, t5
+        storep t4, [map, t5, 4]
     elsif MIPS
         la label, t4
         la _relativePCBase, t3
         subp t3, t4
-        addp t4, t2, t4
-        move index, t3
-        storep t4, [map, t3, 4]
+        addp t4, t3, t4
+        move index, t5
+        storep t4, [map, t5, 4]
     end
 end
 
 global _llint_entry
 # Entry point for the llint to initialize.
@@ -1350,30 +1383,40 @@
     functionPrologue()
     pushCalleeSaves()
     if X86 or X86_WIN
         loadp 20[sp], a0
         loadp 24[sp], a1
+        loadp 28[sp], a2
     end
 
-    initPCRelative(t2)
+    initPCRelative(t3)
 
     # Include generated bytecode initialization file.
     include InitBytecodes
 
     popCalleeSaves()
     functionEpilogue()
     ret
 end
 
-_llint_op_wide:
-    nextInstructionWide()
+_llint_op_wide16:
+    nextInstructionWide16()
 
-_llint_op_wide_wide:
+_llint_op_wide32:
+    nextInstructionWide32()
+
+macro noWide(label)
+_llint_%label%_wide16:
     crash()
 
-_llint_op_enter_wide:
+_llint_%label%_wide32:
     crash()
+end
+
+noWide(op_wide16)
+noWide(op_wide32)
+noWide(op_enter)
 
 op(llint_program_prologue, macro ()
     prologue(notFunctionCodeBlockGetter, notFunctionCodeBlockSetter, _llint_entry_osr, _llint_trace_prologue)
     dispatch(0)
 end)
@@ -1626,27 +1669,20 @@
 preOp(dec, OpDec,
     macro (value, slow) bsubio 1, value, slow end)
 
 
 llintOp(op_loop_hint, OpLoopHint, macro (unused, unused, dispatch)
-    checkSwitchToJITForLoop()
-    dispatch()
-end)
-
-
-llintOp(op_check_traps, OpCheckTraps, macro (unused, unused, dispatch)
+    # CheckTraps.
     loadp CodeBlock[cfr], t1
     loadp CodeBlock::m_vm[t1], t1
-    loadb VM::m_traps+VMTraps::m_needTrapHandling[t1], t0
-    btpnz t0, .handleTraps
+    btbnz VM::m_traps + VMTraps::m_needTrapHandling[t1], .handleTraps
 .afterHandlingTraps:
+    checkSwitchToJITForLoop()
     dispatch()
 .handleTraps:
-    callTrapHandler(.throwHandler)
+    callTrapHandler(_llint_throw_from_slow_path_trampoline)
     jmp .afterHandlingTraps
-.throwHandler:
-    jmp _llint_throw_from_slow_path_trampoline
 end)
 
 
 # Returns the packet pointer in t0.
 macro acquireShadowChickenPacket(slow)
@@ -1770,27 +1806,33 @@
         OpCallEval,
         macro () dispatchOp(narrow, op_call_eval) end,
         _llint_slow_path_call_eval,
         prepareForRegularCall)
 
-_llint_op_call_eval_wide:
+_llint_op_call_eval_wide16:
     slowPathForCall(
-        wide,
+        wide16,
         OpCallEval,
-        macro () dispatchOp(wide, op_call_eval) end,
-        _llint_slow_path_call_eval_wide,
+        macro () dispatchOp(wide16, op_call_eval) end,
+        _llint_slow_path_call_eval_wide16,
+        prepareForRegularCall)
+
+_llint_op_call_eval_wide32:
+    slowPathForCall(
+        wide32,
+        OpCallEval,
+        macro () dispatchOp(wide32, op_call_eval) end,
+        _llint_slow_path_call_eval_wide32,
         prepareForRegularCall)
 
-_llint_generic_return_point:
-    dispatchAfterCall(narrow, OpCallEval, macro ()
-        dispatchOp(narrow, op_call_eval)
-    end)
 
-_llint_generic_return_point_wide:
-    dispatchAfterCall(wide, OpCallEval, macro()
-        dispatchOp(wide, op_call_eval)
+commonOp(llint_generic_return_point, macro () end, macro (size)
+    dispatchAfterCall(size, OpCallEval, macro ()
+        dispatchOp(size, op_call_eval)
     end)
+end)
+
 
 llintOp(op_identity_with_profile, OpIdentityWithProfile, macro (unused, unused, dispatch)
     dispatch()
 end)
 
@@ -1798,10 +1840,15 @@
 llintOp(op_yield, OpYield, macro (unused, unused, unused)
     notSupported()
 end)
 
 
+llintOp(op_create_generator_frame_environment, OpYield, macro (unused, unused, unused)
+    notSupported()
+end)
+
+
 llintOp(op_debug, OpDebug, macro (unused, unused, dispatch)
     loadp CodeBlock[cfr], t0
     loadi CodeBlock::m_debuggerRequests[t0], t0
     btiz t0, .opDebugDone
     callSlowPath(_llint_slow_path_debug)
