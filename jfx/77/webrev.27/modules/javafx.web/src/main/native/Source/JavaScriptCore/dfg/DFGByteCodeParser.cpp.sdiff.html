<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGByteCodeParser.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="DFGBasicBlock.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGCFAPhase.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/dfg/DFGByteCodeParser.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 250     // Calls check() for those conditions that aren&#39;t the slot base, and calls load() for the slot
 251     // base. Does a combination of watchpoint registration and check emission to guard the
 252     // conditions, and emits code to load the value from the slot base. Returns a node containing
 253     // the loaded value. Returns null if any of the conditions were no longer checkable.
 254     GetByOffsetMethod planLoad(const ObjectPropertyConditionSet&amp;);
 255     Node* load(SpeculatedType, const ObjectPropertyConditionSet&amp;, NodeType = GetByOffset);
 256 
 257     void prepareToParseBlock();
 258     void clearCaches();
 259 
 260     // Parse a single basic block of bytecode instructions.
 261     void parseBlock(unsigned limit);
 262     // Link block successors.
 263     void linkBlock(BasicBlock*, Vector&lt;BasicBlock*&gt;&amp; possibleTargets);
 264     void linkBlocks(Vector&lt;BasicBlock*&gt;&amp; unlinkedBlocks, Vector&lt;BasicBlock*&gt;&amp; possibleTargets);
 265 
 266     VariableAccessData* newVariableAccessData(VirtualRegister operand)
 267     {
 268         ASSERT(!operand.isConstant());
 269 
<span class="line-modified"> 270         m_graph.m_variableAccessData.append(VariableAccessData(operand));</span>
 271         return &amp;m_graph.m_variableAccessData.last();
 272     }
 273 
 274     // Get/Set the operands/result of a bytecode instruction.
 275     Node* getDirect(VirtualRegister operand)
 276     {
 277         ASSERT(!operand.isConstant());
 278 
 279         // Is this an argument?
 280         if (operand.isArgument())
 281             return getArgument(operand);
 282 
 283         // Must be a local.
 284         return getLocal(operand);
 285     }
 286 
 287     Node* get(VirtualRegister operand)
 288     {
 289         if (operand.isConstant()) {
 290             unsigned constantIndex = operand.toConstantIndex();
</pre>
<hr />
<pre>
 304                     constantNode = addToGraph(DoubleConstant, OpInfo(m_graph.freezeStrong(jsDoubleNumber(value.asNumber()))));
 305                 else
 306                     constantNode = addToGraph(JSConstant, OpInfo(m_graph.freezeStrong(value)));
 307                 m_constants[constantIndex] = constantNode;
 308             }
 309             ASSERT(m_constants[constantIndex]);
 310             return m_constants[constantIndex];
 311         }
 312 
 313         if (inlineCallFrame()) {
 314             if (!inlineCallFrame()-&gt;isClosureCall) {
 315                 JSFunction* callee = inlineCallFrame()-&gt;calleeConstant();
 316                 if (operand.offset() == CallFrameSlot::callee)
 317                     return weakJSConstant(callee);
 318             }
 319         } else if (operand.offset() == CallFrameSlot::callee) {
 320             // We have to do some constant-folding here because this enables CreateThis folding. Note
 321             // that we don&#39;t have such watchpoint-based folding for inlined uses of Callee, since in that
 322             // case if the function is a singleton then we already know it.
 323             if (FunctionExecutable* executable = jsDynamicCast&lt;FunctionExecutable*&gt;(*m_vm, m_codeBlock-&gt;ownerExecutable())) {
<span class="line-modified"> 324                 InferredValue* singleton = executable-&gt;singletonFunction();</span>
<span class="line-modified"> 325                 if (JSValue value = singleton-&gt;inferredValue()) {</span>
<span class="line-removed"> 326                     m_graph.watchpoints().addLazily(singleton);</span>
<span class="line-removed"> 327                     JSFunction* function = jsCast&lt;JSFunction*&gt;(value);</span>
 328                     return weakJSConstant(function);
 329                 }
 330             }
 331             return addToGraph(GetCallee);
 332         }
 333 
 334         return getDirect(m_inlineStackTop-&gt;remapOperand(operand));
 335     }
 336 
 337     enum SetMode {
 338         // A normal set which follows a two-phase commit that spans code origins. During
 339         // the current code origin it issues a MovHint, and at the start of the next
 340         // code origin there will be a SetLocal. If the local needs flushing, the second
 341         // SetLocal will be preceded with a Flush.
 342         NormalSet,
 343 
 344         // A set where the SetLocal happens immediately and there is still a Flush. This
 345         // is relevant when assigning to a local in tricky situations for the delayed
 346         // SetLocal logic but where we know that we have not performed any side effects
 347         // within this code origin. This is a safe replacement for NormalSet anytime we
</pre>
<hr />
<pre>
 368         }
 369 
 370         return delayed.execute(this);
 371     }
 372 
 373     void processSetLocalQueue()
 374     {
 375         for (unsigned i = 0; i &lt; m_setLocalQueue.size(); ++i)
 376             m_setLocalQueue[i].execute(this);
 377         m_setLocalQueue.shrink(0);
 378     }
 379 
 380     Node* set(VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 381     {
 382         return setDirect(m_inlineStackTop-&gt;remapOperand(operand), value, setMode);
 383     }
 384 
 385     Node* injectLazyOperandSpeculation(Node* node)
 386     {
 387         ASSERT(node-&gt;op() == GetLocal);
<span class="line-modified"> 388         ASSERT(node-&gt;origin.semantic.bytecodeIndex == m_currentIndex);</span>
 389         ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
 390         LazyOperandValueProfileKey key(m_currentIndex, node-&gt;local());
 391         SpeculatedType prediction = m_inlineStackTop-&gt;m_lazyOperands.prediction(locker, key);
 392         node-&gt;variableAccessData()-&gt;predict(prediction);
 393         return node;
 394     }
 395 
 396     // Used in implementing get/set, above, where the operand is a local variable.
 397     Node* getLocal(VirtualRegister operand)
 398     {
 399         unsigned local = operand.toLocal();
 400 
 401         Node* node = m_currentBlock-&gt;variablesAtTail.local(local);
 402 
 403         // This has two goals: 1) link together variable access datas, and 2)
 404         // try to avoid creating redundant GetLocals. (1) is required for
 405         // correctness - no other phase will ensure that block-local variable
 406         // access data unification is done correctly. (2) is purely opportunistic
 407         // and is meant as an compile-time optimization only.
 408 
</pre>
<hr />
<pre>
 425         node = injectLazyOperandSpeculation(addToGraph(GetLocal, OpInfo(variable)));
 426         m_currentBlock-&gt;variablesAtTail.local(local) = node;
 427         return node;
 428     }
 429     Node* setLocal(const CodeOrigin&amp; semanticOrigin, VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 430     {
 431         SetForScope&lt;CodeOrigin&gt; originChange(m_currentSemanticOrigin, semanticOrigin);
 432 
 433         unsigned local = operand.toLocal();
 434 
 435         if (setMode != ImmediateNakedSet) {
 436             ArgumentPosition* argumentPosition = findArgumentPositionForLocal(operand);
 437             if (argumentPosition)
 438                 flushDirect(operand, argumentPosition);
 439             else if (m_graph.needsScopeRegister() &amp;&amp; operand == m_codeBlock-&gt;scopeRegister())
 440                 flush(operand);
 441         }
 442 
 443         VariableAccessData* variableAccessData = newVariableAccessData(operand);
 444         variableAccessData-&gt;mergeStructureCheckHoistingFailed(
<span class="line-modified"> 445             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex, BadCache));</span>
 446         variableAccessData-&gt;mergeCheckArrayHoistingFailed(
<span class="line-modified"> 447             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex, BadIndexingType));</span>
 448         Node* node = addToGraph(SetLocal, OpInfo(variableAccessData), value);
 449         m_currentBlock-&gt;variablesAtTail.local(local) = node;
 450         return node;
 451     }
 452 
 453     // Used in implementing get/set, above, where the operand is an argument.
 454     Node* getArgument(VirtualRegister operand)
 455     {
 456         unsigned argument = operand.toArgument();
 457         ASSERT(argument &lt; m_numArguments);
 458 
 459         Node* node = m_currentBlock-&gt;variablesAtTail.argument(argument);
 460 
 461         VariableAccessData* variable;
 462 
 463         if (node) {
 464             variable = node-&gt;variableAccessData();
 465 
 466             switch (node-&gt;op()) {
 467             case GetLocal:
</pre>
<hr />
<pre>
 481     Node* setArgument(const CodeOrigin&amp; semanticOrigin, VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 482     {
 483         SetForScope&lt;CodeOrigin&gt; originChange(m_currentSemanticOrigin, semanticOrigin);
 484 
 485         unsigned argument = operand.toArgument();
 486         ASSERT(argument &lt; m_numArguments);
 487 
 488         VariableAccessData* variableAccessData = newVariableAccessData(operand);
 489 
 490         // Always flush arguments, except for &#39;this&#39;. If &#39;this&#39; is created by us,
 491         // then make sure that it&#39;s never unboxed.
 492         if (argument || m_graph.needsFlushedThis()) {
 493             if (setMode != ImmediateNakedSet)
 494                 flushDirect(operand);
 495         }
 496 
 497         if (!argument &amp;&amp; m_codeBlock-&gt;specializationKind() == CodeForConstruct)
 498             variableAccessData-&gt;mergeShouldNeverUnbox(true);
 499 
 500         variableAccessData-&gt;mergeStructureCheckHoistingFailed(
<span class="line-modified"> 501             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex, BadCache));</span>
 502         variableAccessData-&gt;mergeCheckArrayHoistingFailed(
<span class="line-modified"> 503             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex, BadIndexingType));</span>
 504         Node* node = addToGraph(SetLocal, OpInfo(variableAccessData), value);
 505         m_currentBlock-&gt;variablesAtTail.argument(argument) = node;
 506         return node;
 507     }
 508 
 509     ArgumentPosition* findArgumentPositionForArgument(int argument)
 510     {
 511         InlineStackEntry* stack = m_inlineStackTop;
 512         while (stack-&gt;m_inlineCallFrame)
 513             stack = stack-&gt;m_caller;
 514         return stack-&gt;m_argumentPositions[argument];
 515     }
 516 
 517     ArgumentPosition* findArgumentPositionForLocal(VirtualRegister operand)
 518     {
 519         for (InlineStackEntry* stack = m_inlineStackTop; ; stack = stack-&gt;m_caller) {
 520             InlineCallFrame* inlineCallFrame = stack-&gt;m_inlineCallFrame;
 521             if (!inlineCallFrame)
 522                 break;
 523             if (operand.offset() &lt; static_cast&lt;int&gt;(inlineCallFrame-&gt;stackOffset + CallFrame::headerSizeInRegisters))
</pre>
<hr />
<pre>
 546             numArguments = inlineCallFrame-&gt;argumentsWithFixup.size();
 547             if (inlineCallFrame-&gt;isClosureCall)
 548                 addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, VirtualRegister(CallFrameSlot::callee)));
 549             if (inlineCallFrame-&gt;isVarargs())
 550                 addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, VirtualRegister(CallFrameSlot::argumentCount)));
 551         } else
 552             numArguments = m_graph.baselineCodeBlockFor(inlineCallFrame)-&gt;numParameters();
 553 
 554         for (unsigned argument = numArguments; argument--;)
 555             addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, virtualRegisterForArgument(argument)));
 556 
 557         if (m_graph.needsScopeRegister())
 558             addFlushDirect(nullptr, m_graph.m_codeBlock-&gt;scopeRegister());
 559     }
 560 
 561     template&lt;typename AddFlushDirectFunc, typename AddPhantomLocalDirectFunc&gt;
 562     void flushForTerminalImpl(CodeOrigin origin, const AddFlushDirectFunc&amp; addFlushDirect, const AddPhantomLocalDirectFunc&amp; addPhantomLocalDirect)
 563     {
 564         origin.walkUpInlineStack(
 565             [&amp;] (CodeOrigin origin) {
<span class="line-modified"> 566                 unsigned bytecodeIndex = origin.bytecodeIndex;</span>
<span class="line-modified"> 567                 InlineCallFrame* inlineCallFrame = origin.inlineCallFrame;</span>
 568                 flushImpl(inlineCallFrame, addFlushDirect);
 569 
 570                 CodeBlock* codeBlock = m_graph.baselineCodeBlockFor(inlineCallFrame);
 571                 FullBytecodeLiveness&amp; fullLiveness = m_graph.livenessFor(codeBlock);
 572                 const FastBitVector&amp; livenessAtBytecode = fullLiveness.getLiveness(bytecodeIndex);
 573 
 574                 for (unsigned local = codeBlock-&gt;numCalleeLocals(); local--;) {
 575                     if (livenessAtBytecode[local])
 576                         addPhantomLocalDirect(inlineCallFrame, remapOperand(inlineCallFrame, virtualRegisterForLocal(local)));
 577                 }
 578             });
 579     }
 580 
 581     void flush(VirtualRegister operand)
 582     {
 583         flushDirect(m_inlineStackTop-&gt;remapOperand(operand));
 584     }
 585 
 586     void flushDirect(VirtualRegister operand)
 587     {
</pre>
<hr />
<pre>
 815 
 816         Node* call = addCallWithoutSettingResult(
 817             op, OpInfo(signature), callee, argCount, registerOffset, OpInfo(prediction));
 818         if (result.isValid())
 819             set(result, call);
 820         return call;
 821     }
 822 
 823     Node* cellConstantWithStructureCheck(JSCell* object, Structure* structure)
 824     {
 825         // FIXME: This should route to emitPropertyCheck, not the other way around. But currently,
 826         // this gets no profit from using emitPropertyCheck() since we&#39;ll non-adaptively watch the
 827         // object&#39;s structure as soon as we make it a weakJSCosntant.
 828         Node* objectNode = weakJSConstant(object);
 829         addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(structure)), objectNode);
 830         return objectNode;
 831     }
 832 
 833     SpeculatedType getPredictionWithoutOSRExit(unsigned bytecodeIndex)
 834     {
<span class="line-modified"> 835         SpeculatedType prediction;</span>
 836         {
<span class="line-modified"> 837             ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);</span>
<span class="line-modified"> 838             prediction = m_inlineStackTop-&gt;m_profiledBlock-&gt;valueProfilePredictionForBytecodeOffset(locker, bytecodeIndex);</span>
<span class="line-modified"> 839         }</span>







 840 

 841         if (prediction != SpecNone)
 842             return prediction;
 843 
 844         // If we have no information about the values this
 845         // node generates, we check if by any chance it is
 846         // a tail call opcode. In that case, we walk up the
 847         // inline frames to find a call higher in the call
 848         // chain and use its prediction. If we only have
 849         // inlined tail call frames, we use SpecFullTop
 850         // to avoid a spurious OSR exit.
 851         auto instruction = m_inlineStackTop-&gt;m_profiledBlock-&gt;instructions().at(bytecodeIndex);
 852         OpcodeID opcodeID = instruction-&gt;opcodeID();
 853 
 854         switch (opcodeID) {
 855         case op_tail_call:
 856         case op_tail_call_varargs:
 857         case op_tail_call_forward_arguments: {
 858             // Things should be more permissive to us returning BOTTOM instead of TOP here.
 859             // Currently, this will cause us to Force OSR exit. This is bad because returning
 860             // TOP will cause anything that transitively touches this speculated type to
 861             // also become TOP during prediction propagation.
 862             // https://bugs.webkit.org/show_bug.cgi?id=164337
 863             if (!inlineCallFrame())
 864                 return SpecFullTop;
 865 
 866             CodeOrigin* codeOrigin = inlineCallFrame()-&gt;getCallerSkippingTailCalls();
 867             if (!codeOrigin)
 868                 return SpecFullTop;
 869 
 870             InlineStackEntry* stack = m_inlineStackTop;
<span class="line-modified"> 871             while (stack-&gt;m_inlineCallFrame != codeOrigin-&gt;inlineCallFrame)</span>
 872                 stack = stack-&gt;m_caller;
 873 
<span class="line-modified"> 874             bytecodeIndex = codeOrigin-&gt;bytecodeIndex;</span>
<span class="line-removed"> 875             CodeBlock* profiledBlock = stack-&gt;m_profiledBlock;</span>
<span class="line-removed"> 876             ConcurrentJSLocker locker(profiledBlock-&gt;m_lock);</span>
<span class="line-removed"> 877             return profiledBlock-&gt;valueProfilePredictionForBytecodeOffset(locker, bytecodeIndex);</span>
 878         }
 879 
 880         default:
 881             return SpecNone;
 882         }
 883 
 884         RELEASE_ASSERT_NOT_REACHED();
 885         return SpecNone;
 886     }
 887 
 888     SpeculatedType getPrediction(unsigned bytecodeIndex)
 889     {
 890         SpeculatedType prediction = getPredictionWithoutOSRExit(bytecodeIndex);
 891 
 892         if (prediction == SpecNone) {
 893             // We have no information about what values this node generates. Give up
 894             // on executing this code, since we&#39;re likely to do more damage than good.
 895             addToGraph(ForceOSRExit);
 896         }
 897 
</pre>
<hr />
<pre>
 913         CodeBlock* codeBlock = m_inlineStackTop-&gt;m_profiledBlock;
 914         ArrayProfile* profile = codeBlock-&gt;getArrayProfile(codeBlock-&gt;bytecodeOffset(m_currentInstruction));
 915         return getArrayMode(*profile, action);
 916     }
 917 
 918     ArrayMode getArrayMode(ArrayProfile&amp; profile, Array::Action action)
 919     {
 920         ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
 921         profile.computeUpdatedPrediction(locker, m_inlineStackTop-&gt;m_profiledBlock);
 922         bool makeSafe = profile.outOfBounds(locker);
 923         return ArrayMode::fromObserved(locker, &amp;profile, action, makeSafe);
 924     }
 925 
 926     Node* makeSafe(Node* node)
 927     {
 928         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
 929             node-&gt;mergeFlags(NodeMayOverflowInt32InDFG);
 930         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
 931             node-&gt;mergeFlags(NodeMayNegZeroInDFG);
 932 
<span class="line-modified"> 933         if (!isX86() &amp;&amp; node-&gt;op() == ArithMod)</span>
 934             return node;
 935 
 936         {
 937             ArithProfile* arithProfile = m_inlineStackTop-&gt;m_profiledBlock-&gt;arithProfileForBytecodeOffset(m_currentIndex);
 938             if (arithProfile) {
 939                 switch (node-&gt;op()) {
 940                 case ArithAdd:
 941                 case ArithSub:
 942                 case ValueAdd:
 943                     if (arithProfile-&gt;didObserveDouble())
 944                         node-&gt;mergeFlags(NodeMayHaveDoubleResult);
 945                     if (arithProfile-&gt;didObserveNonNumeric())
 946                         node-&gt;mergeFlags(NodeMayHaveNonNumericResult);
 947                     if (arithProfile-&gt;didObserveBigInt())
 948                         node-&gt;mergeFlags(NodeMayHaveBigIntResult);
 949                     break;
 950 
 951                 case ValueMul:
 952                 case ArithMul: {
 953                     if (arithProfile-&gt;didObserveInt52Overflow())
</pre>
<hr />
<pre>
 974                         node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline);
 975                     if (arithProfile-&gt;didObserveNonNumeric())
 976                         node-&gt;mergeFlags(NodeMayHaveNonNumericResult);
 977                     if (arithProfile-&gt;didObserveBigInt())
 978                         node-&gt;mergeFlags(NodeMayHaveBigIntResult);
 979                     break;
 980                 }
 981 
 982                 default:
 983                     break;
 984                 }
 985             }
 986         }
 987 
 988         if (m_inlineStackTop-&gt;m_profiledBlock-&gt;likelyToTakeSlowCase(m_currentIndex)) {
 989             switch (node-&gt;op()) {
 990             case UInt32ToNumber:
 991             case ArithAdd:
 992             case ArithSub:
 993             case ValueAdd:

 994             case ArithMod: // for ArithMod &quot;MayOverflow&quot; means we tried to divide by zero, or we saw double.
 995                 node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline);
 996                 break;
 997 
 998             default:
 999                 break;
1000             }
1001         }
1002 
1003         return node;
1004     }
1005 
1006     Node* makeDivSafe(Node* node)
1007     {
1008         ASSERT(node-&gt;op() == ArithDiv || node-&gt;op() == ValueDiv);
1009 
1010         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
1011             node-&gt;mergeFlags(NodeMayOverflowInt32InDFG);
1012         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
1013             node-&gt;mergeFlags(NodeMayNegZeroInDFG);
</pre>
<hr />
<pre>
1420         } else
1421             addToGraph(SetCallee, callTargetNode);
1422 
1423         // We must set the arguments to the right values
1424         if (!stackEntry-&gt;m_inlineCallFrame)
1425             addToGraph(SetArgumentCountIncludingThis, OpInfo(argumentCountIncludingThis));
1426         int argIndex = 0;
1427         for (; argIndex &lt; argumentCountIncludingThis; ++argIndex) {
1428             Node* value = get(virtualRegisterForArgument(argIndex, registerOffset));
1429             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForArgument(argIndex)), value, NormalSet);
1430         }
1431         Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
1432         for (; argIndex &lt; stackEntry-&gt;m_codeBlock-&gt;numParameters(); ++argIndex)
1433             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForArgument(argIndex)), undefined, NormalSet);
1434 
1435         // We must repeat the work of op_enter here as we will jump right after it.
1436         // We jump right after it and not before it, because of some invariant saying that a CFG root cannot have predecessors in the IR.
1437         for (int i = 0; i &lt; stackEntry-&gt;m_codeBlock-&gt;numVars(); ++i)
1438             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForLocal(i)), undefined, NormalSet);
1439 
<span class="line-removed">1440         // We want to emit the SetLocals with an exit origin that points to the place we are jumping to.</span>
1441         unsigned oldIndex = m_currentIndex;
1442         auto oldStackTop = m_inlineStackTop;


1443         m_inlineStackTop = stackEntry;





1444         m_currentIndex = opcodeLengths[op_enter];
1445         m_exitOK = true;
1446         processSetLocalQueue();
1447         m_currentIndex = oldIndex;
1448         m_inlineStackTop = oldStackTop;
1449         m_exitOK = false;
1450 
1451         BasicBlock** entryBlockPtr = tryBinarySearch&lt;BasicBlock*, unsigned&gt;(stackEntry-&gt;m_blockLinkingTargets, stackEntry-&gt;m_blockLinkingTargets.size(), opcodeLengths[op_enter], getBytecodeBeginForBlock);
1452         RELEASE_ASSERT(entryBlockPtr);
1453         addJumpTo(*entryBlockPtr);
1454         return true;
1455         // It would be unsound to jump over a non-tail call: the &quot;tail&quot; call is not really a tail call in that case.
1456     } while (stackEntry-&gt;m_inlineCallFrame &amp;&amp; stackEntry-&gt;m_inlineCallFrame-&gt;kind == InlineCallFrame::TailCall &amp;&amp; (stackEntry = stackEntry-&gt;m_caller));
1457 
1458     // The tail call was not recursive
1459     return false;
1460 }
1461 
1462 unsigned ByteCodeParser::inliningCost(CallVariant callee, int argumentCountIncludingThis, InlineCallFrame::Kind kind)
1463 {
</pre>
<hr />
<pre>
1537 
1538     for (InlineStackEntry* entry = m_inlineStackTop; entry; entry = entry-&gt;m_caller) {
1539         ++depth;
1540         if (depth &gt;= Options::maximumInliningDepth()) {
1541             VERBOSE_LOG(&quot;    Failing because depth exceeded.\n&quot;);
1542             return UINT_MAX;
1543         }
1544 
1545         if (entry-&gt;executable() == executable) {
1546             ++recursion;
1547             if (recursion &gt;= Options::maximumInliningRecursion()) {
1548                 VERBOSE_LOG(&quot;    Failing because recursion detected.\n&quot;);
1549                 return UINT_MAX;
1550             }
1551         }
1552     }
1553 
1554     VERBOSE_LOG(&quot;    Inlining should be possible.\n&quot;);
1555 
1556     // It might be possible to inline.
<span class="line-modified">1557     return codeBlock-&gt;instructionCount();</span>
1558 }
1559 
1560 template&lt;typename ChecksFunctor&gt;
1561 void ByteCodeParser::inlineCall(Node* callTargetNode, VirtualRegister result, CallVariant callee, int registerOffset, int argumentCountIncludingThis, InlineCallFrame::Kind kind, BasicBlock* continuationBlock, const ChecksFunctor&amp; insertChecks)
1562 {
1563     const Instruction* savedCurrentInstruction = m_currentInstruction;
1564     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1565 
<span class="line-removed">1566     ASSERT(inliningCost(callee, argumentCountIncludingThis, kind) != UINT_MAX);</span>
<span class="line-removed">1567 </span>
1568     CodeBlock* codeBlock = callee.functionExecutable()-&gt;baselineCodeBlockFor(specializationKind);
1569     insertChecks(codeBlock);
1570 
1571     // FIXME: Don&#39;t flush constants!
1572 
1573     // arityFixupCount and numberOfStackPaddingSlots are different. While arityFixupCount does not consider about stack alignment,
1574     // numberOfStackPaddingSlots consider alignment. Consider the following case,
1575     //
1576     // before: [ ... ][arg0][header]
1577     // after:  [ ... ][ext ][arg1][arg0][header]
1578     //
1579     // In the above case, arityFixupCount is 1. But numberOfStackPaddingSlots is 2 because the stack needs to be aligned.
1580     // We insert extra slots to align stack.
1581     int arityFixupCount = std::max&lt;int&gt;(codeBlock-&gt;numParameters() - argumentCountIncludingThis, 0);
1582     int numberOfStackPaddingSlots = CommonSlowPaths::numberOfStackPaddingSlots(codeBlock, argumentCountIncludingThis);
1583     ASSERT(!(numberOfStackPaddingSlots % stackAlignmentRegisters()));
1584     int registerOffsetAfterFixup = registerOffset - numberOfStackPaddingSlots;
1585 
1586     int inlineCallFrameStart = m_inlineStackTop-&gt;remapOperand(VirtualRegister(registerOffsetAfterFixup)).offset() + CallFrame::headerSizeInRegisters;
1587 
1588     ensureLocals(
1589         VirtualRegister(inlineCallFrameStart).toLocal() + 1 +
1590         CallFrame::headerSizeInRegisters + codeBlock-&gt;numCalleeLocals());
1591 
1592     size_t argumentPositionStart = m_graph.m_argumentPositions.size();
1593 
1594     if (result.isValid())
1595         result = m_inlineStackTop-&gt;remapOperand(result);
1596 
1597     VariableAccessData* calleeVariable = nullptr;
1598     if (callee.isClosureCall()) {
1599         Node* calleeSet = set(
1600             VirtualRegister(registerOffsetAfterFixup + CallFrameSlot::callee), callTargetNode, ImmediateNakedSet);
1601 
1602         calleeVariable = calleeSet-&gt;variableAccessData();
1603         calleeVariable-&gt;mergeShouldNeverUnbox(true);
1604     }
1605 
















































1606     if (arityFixupCount) {
1607         // Note: we do arity fixup in two phases:
1608         // 1. We get all the values we need and MovHint them to the expected locals.
<span class="line-modified">1609         // 2. We SetLocal them inside the callee&#39;s CodeOrigin. This way, if we exit, the callee&#39;s</span>
1610         //    frame is already set up. If any SetLocal exits, we have a valid exit state.
1611         //    This is required because if we didn&#39;t do this in two phases, we may exit in
<span class="line-modified">1612         //    the middle of arity fixup from the caller&#39;s CodeOrigin. This is unsound because if</span>
<span class="line-modified">1613         //    we did the SetLocals in the caller&#39;s frame, the memcpy may clobber needed parts</span>
<span class="line-modified">1614         //    of the frame right before exiting. For example, consider if we need to pad two args:</span>
1615         //    [arg3][arg2][arg1][arg0]
1616         //    [fix ][fix ][arg3][arg2][arg1][arg0]
1617         //    We memcpy starting from arg0 in the direction of arg3. If we were to exit at a type check
<span class="line-modified">1618         //    for arg3&#39;s SetLocal in the caller&#39;s CodeOrigin, we&#39;d exit with a frame like so:</span>
1619         //    [arg3][arg2][arg1][arg2][arg1][arg0]
<span class="line-modified">1620         //    And the caller would then just end up thinking its argument are:</span>
<span class="line-modified">1621         //    [arg3][arg2][arg1][arg2]</span>

1622         //    which is incorrect.
1623 
1624         Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
1625         // The stack needs to be aligned due to the JS calling convention. Thus, we have a hole if the count of arguments is not aligned.
1626         // We call this hole &quot;extra slot&quot;. Consider the following case, the number of arguments is 2. If this argument
1627         // count does not fulfill the stack alignment requirement, we already inserted extra slots.
1628         //
1629         // before: [ ... ][ext ][arg1][arg0][header]
1630         //
1631         // In the above case, one extra slot is inserted. If the code&#39;s parameter count is 3, we will fixup arguments.
1632         // At that time, we can simply use this extra slots. So the fixuped stack is the following.
1633         //
1634         // before: [ ... ][ext ][arg1][arg0][header]
1635         // after:  [ ... ][arg2][arg1][arg0][header]
1636         //
1637         // In such cases, we do not need to move frames.
1638         if (registerOffsetAfterFixup != registerOffset) {
1639             for (int index = 0; index &lt; argumentCountIncludingThis; ++index) {
<span class="line-modified">1640                 Node* value = get(virtualRegisterForArgument(index, registerOffset));</span>
<span class="line-modified">1641                 VirtualRegister argumentToSet = m_inlineStackTop-&gt;remapOperand(virtualRegisterForArgument(index, registerOffsetAfterFixup));</span>

1642                 addToGraph(MovHint, OpInfo(argumentToSet.offset()), value);
1643                 m_setLocalQueue.append(DelayedSetLocal { currentCodeOrigin(), argumentToSet, value, ImmediateNakedSet });
1644             }
1645         }
1646         for (int index = 0; index &lt; arityFixupCount; ++index) {
<span class="line-modified">1647             VirtualRegister argumentToSet = m_inlineStackTop-&gt;remapOperand(virtualRegisterForArgument(argumentCountIncludingThis + index, registerOffsetAfterFixup));</span>
1648             addToGraph(MovHint, OpInfo(argumentToSet.offset()), undefined);
1649             m_setLocalQueue.append(DelayedSetLocal { currentCodeOrigin(), argumentToSet, undefined, ImmediateNakedSet });
1650         }
1651 
1652         // At this point, it&#39;s OK to OSR exit because we finished setting up
<span class="line-modified">1653         // our callee&#39;s frame. We emit an ExitOK below from the callee&#39;s CodeOrigin.</span>
1654     }
1655 
<span class="line-removed">1656     InlineStackEntry inlineStackEntry(this, codeBlock, codeBlock, callee.function(), result,</span>
<span class="line-removed">1657         (VirtualRegister)inlineCallFrameStart, argumentCountIncludingThis, kind, continuationBlock);</span>
<span class="line-removed">1658 </span>
<span class="line-removed">1659     // This is where the actual inlining really happens.</span>
<span class="line-removed">1660     unsigned oldIndex = m_currentIndex;</span>
<span class="line-removed">1661     m_currentIndex = 0;</span>
<span class="line-removed">1662 </span>
1663     // At this point, it&#39;s again OK to OSR exit.
1664     m_exitOK = true;
1665     addToGraph(ExitOK);
1666 
1667     processSetLocalQueue();
1668 
1669     InlineVariableData inlineVariableData;
1670     inlineVariableData.inlineCallFrame = m_inlineStackTop-&gt;m_inlineCallFrame;
1671     inlineVariableData.argumentPositionStart = argumentPositionStart;
1672     inlineVariableData.calleeVariable = 0;
1673 
1674     RELEASE_ASSERT(
1675         m_inlineStackTop-&gt;m_inlineCallFrame-&gt;isClosureCall
1676         == callee.isClosureCall());
1677     if (callee.isClosureCall()) {
1678         RELEASE_ASSERT(calleeVariable);
1679         inlineVariableData.calleeVariable = calleeVariable;
1680     }
1681 
1682     m_graph.m_inlineVariableData.append(inlineVariableData);
</pre>
<hr />
<pre>
1807     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1808     if (inliningCost(callVariant, maxNumArguments, kind) &gt; getInliningBalance(callLinkStatus, specializationKind)) {
1809         VERBOSE_LOG(&quot;Bailing inlining: inlining cost too high.\n&quot;);
1810         return false;
1811     }
1812 
1813     int registerOffset = firstFreeReg + 1;
1814     registerOffset -= maxNumArguments; // includes &quot;this&quot;
1815     registerOffset -= CallFrame::headerSizeInRegisters;
1816     registerOffset = -WTF::roundUpToMultipleOf(stackAlignmentRegisters(), -registerOffset);
1817 
1818     auto insertChecks = [&amp;] (CodeBlock* codeBlock) {
1819         emitFunctionChecks(callVariant, callTargetNode, thisArgument);
1820 
1821         int remappedRegisterOffset =
1822         m_inlineStackTop-&gt;remapOperand(VirtualRegister(registerOffset)).offset();
1823 
1824         ensureLocals(VirtualRegister(remappedRegisterOffset).toLocal());
1825 
1826         int argumentStart = registerOffset + CallFrame::headerSizeInRegisters;
<span class="line-modified">1827         int remappedArgumentStart =</span>
<span class="line-removed">1828         m_inlineStackTop-&gt;remapOperand(VirtualRegister(argumentStart)).offset();</span>
1829 
1830         LoadVarargsData* data = m_graph.m_loadVarargsData.add();
1831         data-&gt;start = VirtualRegister(remappedArgumentStart + 1);
1832         data-&gt;count = VirtualRegister(remappedRegisterOffset + CallFrameSlot::argumentCount);
1833         data-&gt;offset = argumentsOffset;
1834         data-&gt;limit = maxNumArguments;
1835         data-&gt;mandatoryMinimum = mandatoryMinimum;
1836 
1837         if (callOp == TailCallForwardVarargs)
1838             addToGraph(ForwardVarargs, OpInfo(data));
1839         else
1840             addToGraph(LoadVarargs, OpInfo(data), get(argumentsArgument));
1841 
1842         // LoadVarargs may OSR exit. Hence, we need to keep alive callTargetNode, thisArgument
1843         // and argumentsArgument for the baseline JIT. However, we only need a Phantom for
1844         // callTargetNode because the other 2 are still in use and alive at this point.
1845         addToGraph(Phantom, callTargetNode);
1846 
1847         // In DFG IR before SSA, we cannot insert control flow between after the
<span class="line-modified">1848         // LoadVarargs and the last SetArgument. This isn&#39;t a problem once we get to DFG</span>
1849         // SSA. Fortunately, we also have other reasons for not inserting control flow
1850         // before SSA.
1851 
1852         VariableAccessData* countVariable = newVariableAccessData(VirtualRegister(remappedRegisterOffset + CallFrameSlot::argumentCount));
1853         // This is pretty lame, but it will force the count to be flushed as an int. This doesn&#39;t
<span class="line-modified">1854         // matter very much, since our use of a SetArgument and Flushes for this local slot is</span>
1855         // mostly just a formality.
1856         countVariable-&gt;predict(SpecInt32Only);
1857         countVariable-&gt;mergeIsProfitableToUnbox(true);
<span class="line-modified">1858         Node* setArgumentCount = addToGraph(SetArgument, OpInfo(countVariable));</span>
1859         m_currentBlock-&gt;variablesAtTail.setOperand(countVariable-&gt;local(), setArgumentCount);
1860 
1861         set(VirtualRegister(argumentStart), get(thisArgument), ImmediateNakedSet);

1862         for (unsigned argument = 1; argument &lt; maxNumArguments; ++argument) {
1863             VariableAccessData* variable = newVariableAccessData(VirtualRegister(remappedArgumentStart + argument));
1864             variable-&gt;mergeShouldNeverUnbox(true); // We currently have nowhere to put the type check on the LoadVarargs. LoadVarargs is effectful, so after it finishes, we cannot exit.
1865 
1866             // For a while it had been my intention to do things like this inside the
1867             // prediction injection phase. But in this case it&#39;s really best to do it here,
1868             // because it&#39;s here that we have access to the variable access datas for the
1869             // inlining we&#39;re about to do.
1870             //
1871             // Something else that&#39;s interesting here is that we&#39;d really love to get
1872             // predictions from the arguments loaded at the callsite, rather than the
1873             // arguments received inside the callee. But that probably won&#39;t matter for most
1874             // calls.
1875             if (codeBlock &amp;&amp; argument &lt; static_cast&lt;unsigned&gt;(codeBlock-&gt;numParameters())) {
1876                 ConcurrentJSLocker locker(codeBlock-&gt;m_lock);
1877                 ValueProfile&amp; profile = codeBlock-&gt;valueProfileForArgument(argument);
1878                 variable-&gt;predict(profile.computeUpdatedPrediction(locker));
1879             }
1880 
<span class="line-modified">1881             Node* setArgument = addToGraph(SetArgument, OpInfo(variable));</span>
1882             m_currentBlock-&gt;variablesAtTail.setOperand(variable-&gt;local(), setArgument);

1883         }
1884     };
1885 
1886     // Intrinsics and internal functions can only be inlined if we&#39;re not doing varargs. This is because
1887     // we currently don&#39;t have any way of getting profiling information for arguments to non-JS varargs
1888     // calls. The prediction propagator won&#39;t be of any help because LoadVarargs obscures the data flow,
1889     // and there are no callsite value profiles and native function won&#39;t have callee value profiles for
1890     // those arguments. Even worse, if the intrinsic decides to exit, it won&#39;t really have anywhere to
1891     // exit to: LoadVarargs is effectful and it&#39;s part of the op_call_varargs, so we can&#39;t exit without
1892     // calling LoadVarargs twice.
1893     inlineCall(callTargetNode, result, callVariant, registerOffset, maxNumArguments, kind, nullptr, insertChecks);
1894 

1895     VERBOSE_LOG(&quot;Successful inlining (varargs, monomorphic).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1896     return true;
1897 }
1898 
1899 unsigned ByteCodeParser::getInliningBalance(const CallLinkStatus&amp; callLinkStatus, CodeSpecializationKind specializationKind)
1900 {
<span class="line-modified">1901     unsigned inliningBalance = Options::maximumFunctionForCallInlineCandidateInstructionCount();</span>
1902     if (specializationKind == CodeForConstruct)
<span class="line-modified">1903         inliningBalance = std::min(inliningBalance, Options::maximumFunctionForConstructInlineCandidateInstructionCount());</span>
1904     if (callLinkStatus.isClosureCall())
<span class="line-modified">1905         inliningBalance = std::min(inliningBalance, Options::maximumFunctionForClosureCallInlineCandidateInstructionCount());</span>
1906     return inliningBalance;
1907 }
1908 
1909 ByteCodeParser::CallOptimizationResult ByteCodeParser::handleInlining(
1910     Node* callTargetNode, VirtualRegister result, const CallLinkStatus&amp; callLinkStatus,
1911     int registerOffset, VirtualRegister thisArgument,
1912     int argumentCountIncludingThis,
1913     unsigned nextOffset, NodeType callOp, InlineCallFrame::Kind kind, SpeculatedType prediction)
1914 {
1915     VERBOSE_LOG(&quot;Handling inlining...\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1916 
1917     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1918     unsigned inliningBalance = getInliningBalance(callLinkStatus, specializationKind);
1919 
1920     // First check if we can avoid creating control flow. Our inliner does some CFG
1921     // simplification on the fly and this helps reduce compile times, but we can only leverage
1922     // this in cases where we don&#39;t need control flow diamonds to check the callee.
1923     if (!callLinkStatus.couldTakeSlowPath() &amp;&amp; callLinkStatus.size() == 1) {
1924         return handleCallVariant(
1925             callTargetNode, result, callLinkStatus[0], registerOffset, thisArgument,
</pre>
<hr />
<pre>
2040         VERBOSE_LOG(&quot;Finished optimizing &quot;, callLinkStatus[i], &quot; at &quot;, currentCodeOrigin(), &quot;.\n&quot;);
2041     }
2042 
2043     // Slow path block
2044     m_currentBlock = allocateUntargetableBlock();
2045     m_currentIndex = oldOffset;
2046     m_exitOK = true;
2047     data.fallThrough = BranchTarget(m_currentBlock);
2048     prepareToParseBlock();
2049     Node* myCallTargetNode = getDirect(calleeReg);
2050     if (couldTakeSlowPath) {
2051         addCall(
2052             result, callOp, nullptr, myCallTargetNode, argumentCountIncludingThis,
2053             registerOffset, prediction);
2054         VERBOSE_LOG(&quot;We added a call in the slow path\n&quot;);
2055     } else {
2056         addToGraph(CheckBadCell);
2057         addToGraph(Phantom, myCallTargetNode);
2058         emitArgumentPhantoms(registerOffset, argumentCountIncludingThis);
2059 
<span class="line-modified">2060         set(result, addToGraph(BottomValue));</span>

2061         VERBOSE_LOG(&quot;couldTakeSlowPath was false\n&quot;);
2062     }
2063 
2064     m_currentIndex = nextOffset;
2065     m_exitOK = true; // Origin changed, so it&#39;s fine to exit again.
2066     processSetLocalQueue();
2067 
2068     if (Node* terminal = m_currentBlock-&gt;terminal())
2069         ASSERT_UNUSED(terminal, terminal-&gt;op() == TailCall || terminal-&gt;op() == TailCallVarargs || terminal-&gt;op() == TailCallForwardVarargs);
2070     else {
2071         addJumpTo(continuationBlock);
2072     }
2073 
2074     prepareToParseBlock();
2075 
2076     m_currentIndex = oldOffset;
2077     m_currentBlock = continuationBlock;
2078     m_exitOK = true;
2079 
2080     VERBOSE_LOG(&quot;Done inlining (hard).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
</pre>
<hr />
<pre>
2279                 return false;
2280 
2281             ArrayMode arrayMode = getArrayMode(Array::Read);
2282             if (!arrayMode.isJSArray())
2283                 return false;
2284 
2285             if (!arrayMode.isJSArrayWithOriginalStructure())
2286                 return false;
2287 
2288             switch (arrayMode.type()) {
2289             case Array::Double:
2290             case Array::Int32:
2291             case Array::Contiguous: {
2292                 JSGlobalObject* globalObject = m_graph.globalObjectFor(currentNodeOrigin().semantic);
2293 
2294                 Structure* arrayPrototypeStructure = globalObject-&gt;arrayPrototype()-&gt;structure(*m_vm);
2295                 Structure* objectPrototypeStructure = globalObject-&gt;objectPrototype()-&gt;structure(*m_vm);
2296 
2297                 // FIXME: We could easily relax the Array/Object.prototype transition as long as we OSR exitted if we saw a hole.
2298                 // https://bugs.webkit.org/show_bug.cgi?id=173171
<span class="line-modified">2299                 if (globalObject-&gt;arraySpeciesWatchpoint().state() == IsWatched</span>
2300                     &amp;&amp; globalObject-&gt;havingABadTimeWatchpoint()-&gt;isStillValid()
2301                     &amp;&amp; arrayPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2302                     &amp;&amp; objectPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2303                     &amp;&amp; globalObject-&gt;arrayPrototypeChainIsSane()) {
2304 
<span class="line-modified">2305                     m_graph.watchpoints().addLazily(globalObject-&gt;arraySpeciesWatchpoint());</span>
2306                     m_graph.watchpoints().addLazily(globalObject-&gt;havingABadTimeWatchpoint());
2307                     m_graph.registerAndWatchStructureTransition(arrayPrototypeStructure);
2308                     m_graph.registerAndWatchStructureTransition(objectPrototypeStructure);
2309 
2310                     insertChecks();
2311 
2312                     Node* array = get(virtualRegisterForArgument(0, registerOffset));
2313                     // We do a few things here to prove that we aren&#39;t skipping doing side-effects in an observable way:
2314                     // 1. We ensure that the &quot;constructor&quot; property hasn&#39;t been changed (because the observable
2315                     // effects of slice require that we perform a Get(array, &quot;constructor&quot;) and we can skip
2316                     // that if we&#39;re an original array structure. (We can relax this in the future by using
2317                     // TryGetById and CheckCell).
2318                     //
2319                     // 2. We check that the array we&#39;re calling slice on has the same global object as the lexical
2320                     // global object that this code is running in. This requirement is necessary because we setup the
2321                     // watchpoints above on the lexical global object. This means that code that calls slice on
2322                     // arrays produced by other global objects won&#39;t get this optimization. We could relax this
2323                     // requirement in the future by checking that the watchpoint hasn&#39;t fired at runtime in the code
2324                     // we generate instead of registering it as a watchpoint that would invalidate the compilation.
2325                     //
</pre>
<hr />
<pre>
3172                 byteSize = 4;
3173                 op = DataViewGetFloat;
3174                 break;
3175             case DataViewGetFloat64:
3176                 byteSize = 8;
3177                 op = DataViewGetFloat;
3178                 break;
3179             default:
3180                 RELEASE_ASSERT_NOT_REACHED();
3181             }
3182 
3183             TriState isLittleEndian = MixedTriState;
3184             Node* littleEndianChild = nullptr;
3185             if (byteSize &gt; 1) {
3186                 if (argumentCountIncludingThis &lt; 3)
3187                     isLittleEndian = FalseTriState;
3188                 else {
3189                     littleEndianChild = get(virtualRegisterForArgument(2, registerOffset));
3190                     if (littleEndianChild-&gt;hasConstant()) {
3191                         JSValue constant = littleEndianChild-&gt;constant()-&gt;value();
<span class="line-modified">3192                         isLittleEndian = constant.pureToBoolean();</span>
<span class="line-modified">3193                         if (isLittleEndian != MixedTriState)</span>
<span class="line-modified">3194                             littleEndianChild = nullptr;</span>


3195                     } else
3196                         isLittleEndian = MixedTriState;
3197                 }
3198             }
3199 
3200             DataViewData data { };
3201             data.isLittleEndian = isLittleEndian;
3202             data.isSigned = isSigned;
3203             data.byteSize = byteSize;
3204 
3205             setResult(
3206                 addToGraph(op, OpInfo(data.asQuadWord), OpInfo(prediction), get(virtualRegisterForArgument(0, registerOffset)), get(virtualRegisterForArgument(1, registerOffset)), littleEndianChild));
3207             return true;
3208         }
3209 
3210         case DataViewSetInt8:
3211         case DataViewSetUint8:
3212         case DataViewSetInt16:
3213         case DataViewSetUint16:
3214         case DataViewSetInt32:
</pre>
<hr />
<pre>
3255                 isFloatingPoint = true;
3256                 byteSize = 4;
3257                 break;
3258             case DataViewSetFloat64:
3259                 isFloatingPoint = true;
3260                 byteSize = 8;
3261                 break;
3262             default:
3263                 RELEASE_ASSERT_NOT_REACHED();
3264             }
3265 
3266             TriState isLittleEndian = MixedTriState;
3267             Node* littleEndianChild = nullptr;
3268             if (byteSize &gt; 1) {
3269                 if (argumentCountIncludingThis &lt; 4)
3270                     isLittleEndian = FalseTriState;
3271                 else {
3272                     littleEndianChild = get(virtualRegisterForArgument(3, registerOffset));
3273                     if (littleEndianChild-&gt;hasConstant()) {
3274                         JSValue constant = littleEndianChild-&gt;constant()-&gt;value();
<span class="line-modified">3275                         isLittleEndian = constant.pureToBoolean();</span>
<span class="line-modified">3276                         if (isLittleEndian != MixedTriState)</span>
<span class="line-modified">3277                             littleEndianChild = nullptr;</span>


3278                     } else
3279                         isLittleEndian = MixedTriState;
3280                 }
3281             }
3282 
3283             DataViewData data { };
3284             data.isLittleEndian = isLittleEndian;
3285             data.isSigned = isSigned;
3286             data.byteSize = byteSize;
3287             data.isFloatingPoint = isFloatingPoint;
3288 
3289             addVarArgChild(get(virtualRegisterForArgument(0, registerOffset)));
3290             addVarArgChild(get(virtualRegisterForArgument(1, registerOffset)));
3291             addVarArgChild(get(virtualRegisterForArgument(2, registerOffset)));
3292             addVarArgChild(littleEndianChild);
3293 
3294             addToGraph(Node::VarArg, DataViewSet, OpInfo(data.asQuadWord), OpInfo());
3295             setResult(addToGraph(JSConstant, OpInfo(m_constantUndefined)));
3296             return true;
3297         }
</pre>
<hr />
<pre>
3437         Array::Type arrayType = toArrayType(type);
3438         size_t logSize = logElementSize(type);
3439 
3440         variant.structureSet().forEach([&amp;] (Structure* structure) {
3441             TypedArrayType curType = structure-&gt;classInfo()-&gt;typedArrayStorageType;
3442             ASSERT(logSize == logElementSize(curType));
3443             arrayType = refineTypedArrayType(arrayType, curType);
3444             ASSERT(arrayType != Array::Generic);
3445         });
3446 
3447         Node* lengthNode = addToGraph(GetArrayLength, OpInfo(ArrayMode(arrayType, Array::Read).asWord()), thisNode);
3448 
3449         if (!logSize) {
3450             set(result, lengthNode);
3451             return true;
3452         }
3453 
3454         // We can use a BitLShift here because typed arrays will never have a byteLength
3455         // that overflows int32.
3456         Node* shiftNode = jsConstant(jsNumber(logSize));
<span class="line-modified">3457         set(result, addToGraph(BitLShift, lengthNode, shiftNode));</span>
3458 
3459         return true;
3460     }
3461 
3462     case TypedArrayLengthIntrinsic: {
3463         insertChecks();
3464 
3465         TypedArrayType type = (*variant.structureSet().begin())-&gt;classInfo()-&gt;typedArrayStorageType;
3466         Array::Type arrayType = toArrayType(type);
3467 
3468         variant.structureSet().forEach([&amp;] (Structure* structure) {
3469             TypedArrayType curType = structure-&gt;classInfo()-&gt;typedArrayStorageType;
3470             arrayType = refineTypedArrayType(arrayType, curType);
3471             ASSERT(arrayType != Array::Generic);
3472         });
3473 
3474         set(result, addToGraph(GetArrayLength, OpInfo(ArrayMode(arrayType, Array::Read).asWord()), thisNode));
3475 
3476         return true;
3477 
</pre>
<hr />
<pre>
4348     int registerOffset = virtualRegisterForLocal(
4349         m_inlineStackTop-&gt;m_profiledBlock-&gt;numCalleeLocals() - 1).offset();
4350     registerOffset -= numberOfParameters;
4351     registerOffset -= CallFrame::headerSizeInRegisters;
4352 
4353     // Get the alignment right.
4354     registerOffset = -WTF::roundUpToMultipleOf(
4355         stackAlignmentRegisters(),
4356         -registerOffset);
4357 
4358     ensureLocals(
4359         m_inlineStackTop-&gt;remapOperand(
4360             VirtualRegister(registerOffset)).toLocal());
4361 
4362     // Issue SetLocals. This has two effects:
4363     // 1) That&#39;s how handleCall() sees the arguments.
4364     // 2) If we inline then this ensures that the arguments are flushed so that if you use
4365     //    the dreaded arguments object on the getter, the right things happen. Well, sort of -
4366     //    since we only really care about &#39;this&#39; in this case. But we&#39;re not going to take that
4367     //    shortcut.
<span class="line-modified">4368     int nextRegister = registerOffset + CallFrame::headerSizeInRegisters;</span>
<span class="line-removed">4369     set(VirtualRegister(nextRegister++), base, ImmediateNakedSet);</span>
4370 
4371     // We&#39;ve set some locals, but they are not user-visible. It&#39;s still OK to exit from here.
4372     m_exitOK = true;
4373     addToGraph(ExitOK);
4374 
4375     handleCall(
4376         destination, Call, InlineCallFrame::GetterCall, instructionSize,
4377         getter, numberOfParameters - 1, registerOffset, *variant.callLinkStatus(), prediction);
4378 }
4379 
4380 void ByteCodeParser::emitPutById(
4381     Node* base, unsigned identifierNumber, Node* value, const PutByIdStatus&amp; putByIdStatus, bool isDirect)
4382 {
4383     if (isDirect)
4384         addToGraph(PutByIdDirect, OpInfo(identifierNumber), base, value);
4385     else
4386         addToGraph(putByIdStatus.makesCalls() ? PutByIdFlush : PutById, OpInfo(identifierNumber), base, value);
4387 }
4388 
4389 void ByteCodeParser::handlePutById(
</pre>
<hr />
<pre>
4532         unsigned numberOfParameters = 0;
4533         numberOfParameters++; // The &#39;this&#39; argument.
4534         numberOfParameters++; // The new value.
4535         numberOfParameters++; // True return PC.
4536 
4537         // Start with a register offset that corresponds to the last in-use register.
4538         int registerOffset = virtualRegisterForLocal(
4539             m_inlineStackTop-&gt;m_profiledBlock-&gt;numCalleeLocals() - 1).offset();
4540         registerOffset -= numberOfParameters;
4541         registerOffset -= CallFrame::headerSizeInRegisters;
4542 
4543         // Get the alignment right.
4544         registerOffset = -WTF::roundUpToMultipleOf(
4545             stackAlignmentRegisters(),
4546             -registerOffset);
4547 
4548         ensureLocals(
4549             m_inlineStackTop-&gt;remapOperand(
4550                 VirtualRegister(registerOffset)).toLocal());
4551 
<span class="line-modified">4552         int nextRegister = registerOffset + CallFrame::headerSizeInRegisters;</span>
<span class="line-modified">4553         set(VirtualRegister(nextRegister++), base, ImmediateNakedSet);</span>
<span class="line-removed">4554         set(VirtualRegister(nextRegister++), value, ImmediateNakedSet);</span>
4555 
4556         // We&#39;ve set some locals, but they are not user-visible. It&#39;s still OK to exit from here.
4557         m_exitOK = true;
4558         addToGraph(ExitOK);
4559 
4560         handleCall(
4561             VirtualRegister(), Call, InlineCallFrame::SetterCall,
4562             instructionSize, setter, numberOfParameters - 1, registerOffset,
4563             *variant.callLinkStatus(), SpecOther);
4564         return;
4565     }
4566 
4567     default: {
4568         emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4569         return;
4570     } }
4571 }
4572 
4573 void ByteCodeParser::prepareToParseBlock()
4574 {
</pre>
<hr />
<pre>
4648             default: break; \
4649             } \
4650         } \
4651         LAST_OPCODE_LINKED(name); \
4652     } while (false)
4653 
4654 void ByteCodeParser::parseBlock(unsigned limit)
4655 {
4656     auto&amp; instructions = m_inlineStackTop-&gt;m_codeBlock-&gt;instructions();
4657     unsigned blockBegin = m_currentIndex;
4658 
4659     // If we are the first basic block, introduce markers for arguments. This allows
4660     // us to track if a use of an argument may use the actual argument passed, as
4661     // opposed to using a value we set explicitly.
4662     if (m_currentBlock == m_graph.block(0) &amp;&amp; !inlineCallFrame()) {
4663         auto addResult = m_graph.m_rootToArguments.add(m_currentBlock, ArgumentsVector());
4664         RELEASE_ASSERT(addResult.isNewEntry);
4665         ArgumentsVector&amp; entrypointArguments = addResult.iterator-&gt;value;
4666         entrypointArguments.resize(m_numArguments);
4667 
<span class="line-modified">4668         // We will emit SetArgument nodes. They don&#39;t exit, but we&#39;re at the top of an op_enter so</span>
4669         // exitOK = true.
4670         m_exitOK = true;
4671         for (unsigned argument = 0; argument &lt; m_numArguments; ++argument) {
4672             VariableAccessData* variable = newVariableAccessData(
4673                 virtualRegisterForArgument(argument));
4674             variable-&gt;mergeStructureCheckHoistingFailed(
4675                 m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache));
4676             variable-&gt;mergeCheckArrayHoistingFailed(
4677                 m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIndexingType));
4678 
<span class="line-modified">4679             Node* setArgument = addToGraph(SetArgument, OpInfo(variable));</span>
4680             entrypointArguments[argument] = setArgument;
4681             m_currentBlock-&gt;variablesAtTail.setArgumentFirstTime(argument, setArgument);
4682         }
4683     }
4684 
4685     CodeBlock* codeBlock = m_inlineStackTop-&gt;m_codeBlock;
4686 
4687     auto jumpTarget = [&amp;](int target) {
4688         if (target)
4689             return target;
4690         return codeBlock-&gt;outOfLineJumpOffset(m_currentInstruction);
4691     };
4692 
4693     while (true) {
4694         // We&#39;re staring at a new bytecode instruction. So we once again have a place that we can exit
4695         // to.
4696         m_exitOK = true;
4697 
4698         processSetLocalQueue();
4699 
</pre>
<hr />
<pre>
4712             return;
4713         }
4714 
4715         // Switch on the current bytecode opcode.
4716         const Instruction* currentInstruction = instructions.at(m_currentIndex).ptr();
4717         m_currentInstruction = currentInstruction; // Some methods want to use this, and we&#39;d rather not thread it through calls.
4718         OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
4719 
4720         VERBOSE_LOG(&quot;    parsing &quot;, currentCodeOrigin(), &quot;: &quot;, opcodeID, &quot;\n&quot;);
4721 
4722         if (UNLIKELY(m_graph.compilation())) {
4723             addToGraph(CountExecution, OpInfo(m_graph.compilation()-&gt;executionCounterFor(
4724                 Profiler::OriginStack(*m_vm-&gt;m_perBytecodeProfiler, m_codeBlock, currentCodeOrigin()))));
4725         }
4726 
4727         switch (opcodeID) {
4728 
4729         // === Function entry opcodes ===
4730 
4731         case op_enter: {

4732             Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
4733             // Initialize all locals to undefined.
4734             for (int i = 0; i &lt; m_inlineStackTop-&gt;m_codeBlock-&gt;numVars(); ++i)
4735                 set(virtualRegisterForLocal(i), undefined, ImmediateNakedSet);
<span class="line-removed">4736 </span>
4737             NEXT_OPCODE(op_enter);
4738         }
4739 
4740         case op_to_this: {
4741             Node* op1 = getThis();
4742             auto&amp; metadata = currentInstruction-&gt;as&lt;OpToThis&gt;().metadata(codeBlock);
<span class="line-modified">4743             Structure* cachedStructure = metadata.m_cachedStructure.get();</span>



4744             if (metadata.m_toThisStatus != ToThisOK
4745                 || !cachedStructure
4746                 || cachedStructure-&gt;classInfo()-&gt;methodTable.toThis != JSObject::info()-&gt;methodTable.toThis
4747                 || m_inlineStackTop-&gt;m_profiledBlock-&gt;couldTakeSlowCase(m_currentIndex)
4748                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache)
4749                 || (op1-&gt;op() == GetLocal &amp;&amp; op1-&gt;variableAccessData()-&gt;structureCheckHoistingFailed())) {
4750                 setThis(addToGraph(ToThis, OpInfo(), OpInfo(getPrediction()), op1));
4751             } else {
4752                 addToGraph(
4753                     CheckStructure,
4754                     OpInfo(m_graph.addStructureSet(cachedStructure)),
4755                     op1);
4756             }
4757             NEXT_OPCODE(op_to_this);
4758         }
4759 
4760         case op_create_this: {
4761             auto bytecode = currentInstruction-&gt;as&lt;OpCreateThis&gt;();
4762             Node* callee = get(VirtualRegister(bytecode.m_callee));
4763 
</pre>
<hr />
<pre>
4810                     addToGraph(CreateThis, OpInfo(bytecode.m_inlineCapacity), callee));
4811             }
4812             NEXT_OPCODE(op_create_this);
4813         }
4814 
4815         case op_new_object: {
4816             auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
4817             set(bytecode.m_dst,
4818                 addToGraph(NewObject,
4819                     OpInfo(m_graph.registerStructure(bytecode.metadata(codeBlock).m_objectAllocationProfile.structure()))));
4820             NEXT_OPCODE(op_new_object);
4821         }
4822 
4823         case op_new_array: {
4824             auto bytecode = currentInstruction-&gt;as&lt;OpNewArray&gt;();
4825             int startOperand = bytecode.m_argv.offset();
4826             int numOperands = bytecode.m_argc;
4827             ArrayAllocationProfile&amp; profile = bytecode.metadata(codeBlock).m_arrayAllocationProfile;
4828             for (int operandIdx = startOperand; operandIdx &gt; startOperand - numOperands; --operandIdx)
4829                 addVarArgChild(get(VirtualRegister(operandIdx)));
<span class="line-modified">4830             unsigned vectorLengthHint = std::max&lt;unsigned&gt;(profile.vectorLengthHint(), numOperands);</span>
<span class="line-modified">4831             set(bytecode.m_dst, addToGraph(Node::VarArg, NewArray, OpInfo(profile.selectIndexingType()), OpInfo(vectorLengthHint)));</span>
4832             NEXT_OPCODE(op_new_array);
4833         }
4834 
4835         case op_new_array_with_spread: {
4836             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSpread&gt;();
4837             int startOperand = bytecode.m_argv.offset();
4838             int numOperands = bytecode.m_argc;
4839             const BitVector&amp; bitVector = m_inlineStackTop-&gt;m_profiledBlock-&gt;unlinkedCodeBlock()-&gt;bitVector(bytecode.m_bitVector);
4840             for (int operandIdx = startOperand; operandIdx &gt; startOperand - numOperands; --operandIdx)
4841                 addVarArgChild(get(VirtualRegister(operandIdx)));
4842 
4843             BitVector* copy = m_graph.m_bitVectors.add(bitVector);
4844             ASSERT(*copy == bitVector);
4845 
4846             set(bytecode.m_dst,
4847                 addToGraph(Node::VarArg, NewArrayWithSpread, OpInfo(copy)));
4848             NEXT_OPCODE(op_new_array_with_spread);
4849         }
4850 
4851         case op_spread: {
4852             auto bytecode = currentInstruction-&gt;as&lt;OpSpread&gt;();
4853             set(bytecode.m_dst,
4854                 addToGraph(Spread, get(bytecode.m_argument)));
4855             NEXT_OPCODE(op_spread);
4856         }
4857 
4858         case op_new_array_with_size: {
4859             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSize&gt;();
4860             ArrayAllocationProfile&amp; profile = bytecode.metadata(codeBlock).m_arrayAllocationProfile;
<span class="line-modified">4861             set(bytecode.m_dst, addToGraph(NewArrayWithSize, OpInfo(profile.selectIndexingType()), get(bytecode.m_length)));</span>
4862             NEXT_OPCODE(op_new_array_with_size);
4863         }
4864 
4865         case op_new_array_buffer: {
4866             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayBuffer&gt;();
4867             // Unfortunately, we can&#39;t allocate a new JSImmutableButterfly if the profile tells us new information because we
4868             // cannot allocate from compilation threads.
4869             WTF::loadLoadFence();
4870             FrozenValue* frozen = get(VirtualRegister(bytecode.m_immutableButterfly))-&gt;constant();
4871             WTF::loadLoadFence();
4872             JSImmutableButterfly* immutableButterfly = frozen-&gt;cast&lt;JSImmutableButterfly*&gt;();
4873             NewArrayBufferData data { };
4874             data.indexingMode = immutableButterfly-&gt;indexingMode();
4875             data.vectorLengthHint = immutableButterfly-&gt;toButterfly()-&gt;vectorLength();
4876 
4877             set(VirtualRegister(bytecode.m_dst), addToGraph(NewArrayBuffer, OpInfo(frozen), OpInfo(data.asQuadWord)));
4878             NEXT_OPCODE(op_new_array_buffer);
4879         }
4880 
4881         case op_new_regexp: {
</pre>
<hr />
<pre>
4901                 length = jsConstant(restLength);
4902             } else
4903                 length = addToGraph(GetRestLength, OpInfo(bytecode.m_numParametersToSkip));
4904             set(bytecode.m_dst, length);
4905             NEXT_OPCODE(op_get_rest_length);
4906         }
4907 
4908         case op_create_rest: {
4909             auto bytecode = currentInstruction-&gt;as&lt;OpCreateRest&gt;();
4910             noticeArgumentsUse();
4911             Node* arrayLength = get(bytecode.m_arraySize);
4912             set(bytecode.m_dst,
4913                 addToGraph(CreateRest, OpInfo(bytecode.m_numParametersToSkip), arrayLength));
4914             NEXT_OPCODE(op_create_rest);
4915         }
4916 
4917         // === Bitwise operations ===
4918 
4919         case op_bitnot: {
4920             auto bytecode = currentInstruction-&gt;as&lt;OpBitnot&gt;();

4921             Node* op1 = get(bytecode.m_operand);
<span class="line-modified">4922             set(bytecode.m_dst, addToGraph(ArithBitNot, op1));</span>



4923             NEXT_OPCODE(op_bitnot);
4924         }
4925 
4926         case op_bitand: {
4927             auto bytecode = currentInstruction-&gt;as&lt;OpBitand&gt;();
4928             SpeculatedType prediction = getPrediction();
4929             Node* op1 = get(bytecode.m_lhs);
4930             Node* op2 = get(bytecode.m_rhs);
4931             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
4932                 set(bytecode.m_dst, addToGraph(ArithBitAnd, op1, op2));
4933             else
4934                 set(bytecode.m_dst, addToGraph(ValueBitAnd, OpInfo(), OpInfo(prediction), op1, op2));
4935             NEXT_OPCODE(op_bitand);
4936         }
4937 
4938         case op_bitor: {
4939             auto bytecode = currentInstruction-&gt;as&lt;OpBitor&gt;();
4940             SpeculatedType prediction = getPrediction();
4941             Node* op1 = get(bytecode.m_lhs);
4942             Node* op2 = get(bytecode.m_rhs);
</pre>
<hr />
<pre>
4954             Node* op2 = get(bytecode.m_rhs);
4955             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
4956                 set(bytecode.m_dst, addToGraph(ArithBitXor, op1, op2));
4957             else
4958                 set(bytecode.m_dst, addToGraph(ValueBitXor, OpInfo(), OpInfo(prediction), op1, op2));
4959             NEXT_OPCODE(op_bitxor);
4960         }
4961 
4962         case op_rshift: {
4963             auto bytecode = currentInstruction-&gt;as&lt;OpRshift&gt;();
4964             Node* op1 = get(bytecode.m_lhs);
4965             Node* op2 = get(bytecode.m_rhs);
4966             set(bytecode.m_dst, addToGraph(BitRShift, op1, op2));
4967             NEXT_OPCODE(op_rshift);
4968         }
4969 
4970         case op_lshift: {
4971             auto bytecode = currentInstruction-&gt;as&lt;OpLshift&gt;();
4972             Node* op1 = get(bytecode.m_lhs);
4973             Node* op2 = get(bytecode.m_rhs);
<span class="line-modified">4974             set(bytecode.m_dst, addToGraph(BitLShift, op1, op2));</span>





4975             NEXT_OPCODE(op_lshift);
4976         }
4977 
4978         case op_urshift: {
4979             auto bytecode = currentInstruction-&gt;as&lt;OpUrshift&gt;();
4980             Node* op1 = get(bytecode.m_lhs);
4981             Node* op2 = get(bytecode.m_rhs);
4982             set(bytecode.m_dst, addToGraph(BitURShift, op1, op2));
4983             NEXT_OPCODE(op_urshift);
4984         }
4985 
4986         case op_unsigned: {
4987             auto bytecode = currentInstruction-&gt;as&lt;OpUnsigned&gt;();
4988             set(bytecode.m_dst, makeSafe(addToGraph(UInt32ToNumber, get(bytecode.m_operand))));
4989             NEXT_OPCODE(op_unsigned);
4990         }
4991 
4992         // === Increment/Decrement opcodes ===
4993 
4994         case op_inc: {
</pre>
<hr />
<pre>
5038                 set(bytecode.m_dst, makeSafe(addToGraph(ValueNegate, op1)));
5039             NEXT_OPCODE(op_negate);
5040         }
5041 
5042         case op_mul: {
5043             // Multiply requires that the inputs are not truncated, unfortunately.
5044             auto bytecode = currentInstruction-&gt;as&lt;OpMul&gt;();
5045             Node* op1 = get(bytecode.m_lhs);
5046             Node* op2 = get(bytecode.m_rhs);
5047             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5048                 set(bytecode.m_dst, makeSafe(addToGraph(ArithMul, op1, op2)));
5049             else
5050                 set(bytecode.m_dst, makeSafe(addToGraph(ValueMul, op1, op2)));
5051             NEXT_OPCODE(op_mul);
5052         }
5053 
5054         case op_mod: {
5055             auto bytecode = currentInstruction-&gt;as&lt;OpMod&gt;();
5056             Node* op1 = get(bytecode.m_lhs);
5057             Node* op2 = get(bytecode.m_rhs);
<span class="line-modified">5058             set(bytecode.m_dst, makeSafe(addToGraph(ArithMod, op1, op2)));</span>



5059             NEXT_OPCODE(op_mod);
5060         }
5061 
5062         case op_pow: {
<span class="line-removed">5063             // FIXME: ArithPow(Untyped, Untyped) should be supported as the same to ArithMul, ArithSub etc.</span>
<span class="line-removed">5064             // https://bugs.webkit.org/show_bug.cgi?id=160012</span>
5065             auto bytecode = currentInstruction-&gt;as&lt;OpPow&gt;();
5066             Node* op1 = get(bytecode.m_lhs);
5067             Node* op2 = get(bytecode.m_rhs);
<span class="line-modified">5068             set(bytecode.m_dst, addToGraph(ArithPow, op1, op2));</span>



5069             NEXT_OPCODE(op_pow);
5070         }
5071 
5072         case op_div: {
5073             auto bytecode = currentInstruction-&gt;as&lt;OpDiv&gt;();
5074             Node* op1 = get(bytecode.m_lhs);
5075             Node* op2 = get(bytecode.m_rhs);
5076             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5077                 set(bytecode.m_dst, makeDivSafe(addToGraph(ArithDiv, op1, op2)));
5078             else
5079                 set(bytecode.m_dst, makeDivSafe(addToGraph(ValueDiv, op1, op2)));
5080             NEXT_OPCODE(op_div);
5081         }
5082 
5083         // === Misc operations ===
5084 
5085         case op_debug: {
5086             // This is a nop in the DFG/FTL because when we set a breakpoint in the debugger,
5087             // we will jettison all optimized CodeBlocks that contains the breakpoint.
5088             addToGraph(Check); // We add a nop here so that basic block linking doesn&#39;t break.
</pre>
<hr />
<pre>
5375             Node* op2 = get(bytecode.m_rhs);
5376             Node* invertedResult;
5377             invertedResult = addToGraph(CompareStrictEq, op1, op2);
5378             set(bytecode.m_dst, addToGraph(LogicalNot, invertedResult));
5379             NEXT_OPCODE(op_nstricteq);
5380         }
5381 
5382         // === Property access operations ===
5383 
5384         case op_get_by_val: {
5385             auto bytecode = currentInstruction-&gt;as&lt;OpGetByVal&gt;();
5386             SpeculatedType prediction = getPredictionWithoutOSRExit();
5387 
5388             Node* base = get(bytecode.m_base);
5389             Node* property = get(bytecode.m_property);
5390             bool compiledAsGetById = false;
5391             GetByIdStatus getByIdStatus;
5392             unsigned identifierNumber = 0;
5393             {
5394                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
<span class="line-modified">5395                 ByValInfo* byValInfo = m_inlineStackTop-&gt;m_baselineMap.get(CodeOrigin(currentCodeOrigin().bytecodeIndex)).byValInfo;</span>
5396                 // FIXME: When the bytecode is not compiled in the baseline JIT, byValInfo becomes null.
5397                 // At that time, there is no information.
5398                 if (byValInfo
5399                     &amp;&amp; byValInfo-&gt;stubInfo
5400                     &amp;&amp; !byValInfo-&gt;tookSlowPath
5401                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIdent)
5402                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType)
5403                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
5404                     compiledAsGetById = true;
5405                     identifierNumber = m_graph.identifiers().ensure(byValInfo-&gt;cachedId.impl());
5406                     UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
5407 
5408                     if (Symbol* symbol = byValInfo-&gt;cachedSymbol.get()) {
5409                         FrozenValue* frozen = m_graph.freezeStrong(symbol);
5410                         addToGraph(CheckCell, OpInfo(frozen), property);
5411                     } else {
5412                         ASSERT(!uid-&gt;isSymbol());
5413                         addToGraph(CheckStringIdent, OpInfo(uid), property);
5414                     }
5415 
</pre>
<hr />
<pre>
5649         case op_jeq_null: {
5650             auto bytecode = currentInstruction-&gt;as&lt;OpJeqNull&gt;();
5651             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5652             Node* value = get(bytecode.m_value);
5653             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
5654             Node* condition = addToGraph(CompareEq, value, nullConstant);
5655             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5656             LAST_OPCODE(op_jeq_null);
5657         }
5658 
5659         case op_jneq_null: {
5660             auto bytecode = currentInstruction-&gt;as&lt;OpJneqNull&gt;();
5661             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5662             Node* value = get(bytecode.m_value);
5663             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
5664             Node* condition = addToGraph(CompareEq, value, nullConstant);
5665             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5666             LAST_OPCODE(op_jneq_null);
5667         }
5668 


















5669         case op_jless: {
5670             auto bytecode = currentInstruction-&gt;as&lt;OpJless&gt;();
5671             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5672             Node* op1 = get(bytecode.m_lhs);
5673             Node* op2 = get(bytecode.m_rhs);
5674             Node* condition = addToGraph(CompareLess, op1, op2);
5675             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5676             LAST_OPCODE(op_jless);
5677         }
5678 
5679         case op_jlesseq: {
5680             auto bytecode = currentInstruction-&gt;as&lt;OpJlesseq&gt;();
5681             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5682             Node* op1 = get(bytecode.m_lhs);
5683             Node* op2 = get(bytecode.m_rhs);
5684             Node* condition = addToGraph(CompareLessEq, op1, op2);
5685             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5686             LAST_OPCODE(op_jlesseq);
5687         }
5688 
</pre>
<hr />
<pre>
5932 
5933             ValueProfileAndOperandBuffer* buffer = bytecode.metadata(codeBlock).m_buffer;
5934 
5935             if (!buffer) {
5936                 NEXT_OPCODE(op_catch); // This catch has yet to execute. Note: this load can be racy with the main thread.
5937             }
5938 
5939             // We&#39;re now committed to compiling this as an entrypoint.
5940             m_currentBlock-&gt;isCatchEntrypoint = true;
5941             m_graph.m_roots.append(m_currentBlock);
5942 
5943             Vector&lt;SpeculatedType&gt; argumentPredictions(m_numArguments);
5944             Vector&lt;SpeculatedType&gt; localPredictions;
5945             HashSet&lt;unsigned, WTF::IntHash&lt;unsigned&gt;, WTF::UnsignedWithZeroKeyHashTraits&lt;unsigned&gt;&gt; seenArguments;
5946 
5947             {
5948                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
5949 
5950                 buffer-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {
5951                     VirtualRegister operand(profile.m_operand);
<span class="line-modified">5952                     SpeculatedType prediction = profile.m_profile.computeUpdatedPrediction(locker);</span>
5953                     if (operand.isLocal())
5954                         localPredictions.append(prediction);
5955                     else {
5956                         RELEASE_ASSERT(operand.isArgument());
5957                         RELEASE_ASSERT(static_cast&lt;uint32_t&gt;(operand.toArgument()) &lt; argumentPredictions.size());
5958                         if (validationEnabled())
5959                             seenArguments.add(operand.toArgument());
5960                         argumentPredictions[operand.toArgument()] = prediction;
5961                     }
5962                 });
5963 
5964                 if (validationEnabled()) {
5965                     for (unsigned argument = 0; argument &lt; m_numArguments; ++argument)
5966                         RELEASE_ASSERT(seenArguments.contains(argument));
5967                 }
5968             }
5969 
5970             Vector&lt;std::pair&lt;VirtualRegister, Node*&gt;&gt; localsToSet;
5971             localsToSet.reserveInitialCapacity(buffer-&gt;m_size); // Note: This will reserve more than the number of locals we see below because the buffer includes arguments.
5972 
</pre>
<hr />
<pre>
5981                     return;
5982                 ASSERT(operand.isLocal());
5983                 Node* value = addToGraph(ExtractCatchLocal, OpInfo(numberOfLocals), OpInfo(localPredictions[numberOfLocals]));
5984                 ++numberOfLocals;
5985                 addToGraph(MovHint, OpInfo(profile.m_operand), value);
5986                 localsToSet.uncheckedAppend(std::make_pair(operand, value));
5987             });
5988             if (numberOfLocals)
5989                 addToGraph(ClearCatchLocals);
5990 
5991             if (!m_graph.m_maxLocalsForCatchOSREntry)
5992                 m_graph.m_maxLocalsForCatchOSREntry = 0;
5993             m_graph.m_maxLocalsForCatchOSREntry = std::max(numberOfLocals, *m_graph.m_maxLocalsForCatchOSREntry);
5994 
5995             // We could not exit before this point in the program because we would not know how to do value
5996             // recovery for live locals. The above IR sets up the necessary state so we can recover values
5997             // during OSR exit.
5998             //
5999             // The nodes that follow here all exit to the following bytecode instruction, not
6000             // the op_catch. Exiting to op_catch is reserved for when an exception is thrown.
<span class="line-modified">6001             // The SetArgument nodes that follow below may exit because we may hoist type checks</span>
6002             // to them. The SetLocal nodes that follow below may exit because we may choose
6003             // a flush format that speculates on the type of the local.
6004             m_exitOK = true;
6005             addToGraph(ExitOK);
6006 
6007             {
6008                 auto addResult = m_graph.m_rootToArguments.add(m_currentBlock, ArgumentsVector());
6009                 RELEASE_ASSERT(addResult.isNewEntry);
6010                 ArgumentsVector&amp; entrypointArguments = addResult.iterator-&gt;value;
6011                 entrypointArguments.resize(m_numArguments);
6012 
6013                 unsigned exitBytecodeIndex = m_currentIndex + currentInstruction-&gt;size();
6014 
6015                 for (unsigned argument = 0; argument &lt; argumentPredictions.size(); ++argument) {
6016                     VariableAccessData* variable = newVariableAccessData(virtualRegisterForArgument(argument));
6017                     variable-&gt;predict(argumentPredictions[argument]);
6018 
6019                     variable-&gt;mergeStructureCheckHoistingFailed(
6020                         m_inlineStackTop-&gt;m_exitProfile.hasExitSite(exitBytecodeIndex, BadCache));
6021                     variable-&gt;mergeCheckArrayHoistingFailed(
6022                         m_inlineStackTop-&gt;m_exitProfile.hasExitSite(exitBytecodeIndex, BadIndexingType));
6023 
<span class="line-modified">6024                     Node* setArgument = addToGraph(SetArgument, OpInfo(variable));</span>
<span class="line-modified">6025                     setArgument-&gt;origin.forExit.bytecodeIndex = exitBytecodeIndex;</span>
6026                     m_currentBlock-&gt;variablesAtTail.setArgumentFirstTime(argument, setArgument);
6027                     entrypointArguments[argument] = setArgument;
6028                 }
6029             }
6030 
6031             for (const std::pair&lt;VirtualRegister, Node*&gt;&amp; pair : localsToSet) {
6032                 DelayedSetLocal delayed { currentCodeOrigin(), pair.first, pair.second, ImmediateNakedSet };
6033                 m_setLocalQueue.append(delayed);
6034             }
6035 
6036             NEXT_OPCODE(op_catch);
6037         }
6038 
6039         case op_call:
6040             handleCall&lt;OpCall&gt;(currentInstruction, Call, CallMode::Regular);
6041             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6042             NEXT_OPCODE(op_call);
6043 
6044         case op_tail_call: {
6045             flushForReturn();
</pre>
<hr />
<pre>
6193                 RELEASE_ASSERT(constantScope == JSScope::constantScopeForCodeBlock(resolveType, m_inlineStackTop-&gt;m_codeBlock));
6194                 set(bytecode.m_dst, weakJSConstant(constantScope));
6195                 addToGraph(Phantom, get(bytecode.m_scope));
6196                 break;
6197             }
6198             case ModuleVar: {
6199                 // Since the value of the &quot;scope&quot; virtual register is not used in LLInt / baseline op_resolve_scope with ModuleVar,
6200                 // we need not to keep it alive by the Phantom node.
6201                 // Module environment is already strongly referenced by the CodeBlock.
6202                 set(bytecode.m_dst, weakJSConstant(lexicalEnvironment));
6203                 break;
6204             }
6205             case LocalClosureVar:
6206             case ClosureVar:
6207             case ClosureVarWithVarInjectionChecks: {
6208                 Node* localBase = get(bytecode.m_scope);
6209                 addToGraph(Phantom, localBase); // OSR exit cannot handle resolve_scope on a DCE&#39;d scope.
6210 
6211                 // We have various forms of constant folding here. This is necessary to avoid
6212                 // spurious recompiles in dead-but-foldable code.

6213                 if (symbolTable) {
<span class="line-modified">6214                     InferredValue* singleton = symbolTable-&gt;singletonScope();</span>
<span class="line-modified">6215                     if (JSValue value = singleton-&gt;inferredValue()) {</span>
<span class="line-modified">6216                         m_graph.watchpoints().addLazily(singleton);</span>
<span class="line-removed">6217                         set(bytecode.m_dst, weakJSConstant(value));</span>
6218                         break;
6219                     }
6220                 }
6221                 if (JSScope* scope = localBase-&gt;dynamicCastConstant&lt;JSScope*&gt;(*m_vm)) {
6222                     for (unsigned n = depth; n--;)
6223                         scope = scope-&gt;next();
6224                     set(bytecode.m_dst, weakJSConstant(scope));
6225                     break;
6226                 }
6227                 for (unsigned n = depth; n--;)
6228                     localBase = addToGraph(SkipScope, localBase);
6229                 set(bytecode.m_dst, localBase);
6230                 break;
6231             }
6232             case UnresolvedProperty:
6233             case UnresolvedPropertyWithVarInjectionChecks: {
6234                 addToGraph(Phantom, get(bytecode.m_scope));
6235                 addToGraph(ForceOSRExit);
6236                 set(bytecode.m_dst, addToGraph(JSConstant, OpInfo(m_constantNull)));
6237                 break;
6238             }
6239             case Dynamic:
6240                 RELEASE_ASSERT_NOT_REACHED();
6241                 break;
6242             }
6243             NEXT_OPCODE(op_resolve_scope);
6244         }
6245         case op_resolve_scope_for_hoisting_func_decl_in_eval: {
6246             auto bytecode = currentInstruction-&gt;as&lt;OpResolveScopeForHoistingFuncDeclInEval&gt;();
6247             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];
<span class="line-removed">6248 </span>
6249             set(bytecode.m_dst, addToGraph(ResolveScopeForHoistingFuncDeclInEval, OpInfo(identifierNumber), get(bytecode.m_scope)));
6250 
6251             NEXT_OPCODE(op_resolve_scope_for_hoisting_func_decl_in_eval);
6252         }
6253 
6254         case op_get_from_scope: {
6255             auto bytecode = currentInstruction-&gt;as&lt;OpGetFromScope&gt;();
6256             auto&amp; metadata = bytecode.metadata(codeBlock);
6257             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_var];
6258             UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
6259 
6260             ResolveType resolveType;
6261             GetPutInfo getPutInfo(0);
6262             Structure* structure = 0;
6263             WatchpointSet* watchpoints = 0;
6264             uintptr_t operand;
6265             {
6266                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
6267                 getPutInfo = metadata.m_getPutInfo;
6268                 resolveType = getPutInfo.resolveType();
</pre>
<hr />
<pre>
6540             case UnresolvedPropertyWithVarInjectionChecks:
6541                 RELEASE_ASSERT_NOT_REACHED();
6542                 break;
6543             }
6544             NEXT_OPCODE(op_put_to_scope);
6545         }
6546 
6547         case op_loop_hint: {
6548             // Baseline-&gt;DFG OSR jumps between loop hints. The DFG assumes that Baseline-&gt;DFG
6549             // OSR can only happen at basic block boundaries. Assert that these two statements
6550             // are compatible.
6551             RELEASE_ASSERT(m_currentIndex == blockBegin);
6552 
6553             // We never do OSR into an inlined code block. That could not happen, since OSR
6554             // looks up the code block that is the replacement for the baseline JIT code
6555             // block. Hence, machine code block = true code block = not inline code block.
6556             if (!m_inlineStackTop-&gt;m_caller)
6557                 m_currentBlock-&gt;isOSRTarget = true;
6558 
6559             addToGraph(LoopHint);
<span class="line-removed">6560             NEXT_OPCODE(op_loop_hint);</span>
<span class="line-removed">6561         }</span>
<span class="line-removed">6562 </span>
<span class="line-removed">6563         case op_check_traps: {</span>
6564             addToGraph(Options::usePollingTraps() ? CheckTraps : InvalidationPoint);
<span class="line-modified">6565             NEXT_OPCODE(op_check_traps);</span>
6566         }
6567 
6568         case op_nop: {
6569             addToGraph(Check); // We add a nop here so that basic block linking doesn&#39;t break.
6570             NEXT_OPCODE(op_nop);
6571         }
6572 
6573         case op_super_sampler_begin: {
6574             addToGraph(SuperSamplerBegin);
6575             NEXT_OPCODE(op_super_sampler_begin);
6576         }
6577 
6578         case op_super_sampler_end: {
6579             addToGraph(SuperSamplerEnd);
6580             NEXT_OPCODE(op_super_sampler_end);
6581         }
6582 
6583         case op_create_lexical_environment: {
6584             auto bytecode = currentInstruction-&gt;as&lt;OpCreateLexicalEnvironment&gt;();
6585             ASSERT(bytecode.m_symbolTable.isConstant() &amp;&amp; bytecode.m_initialValue.isConstant());
</pre>
<hr />
<pre>
7016         m_inlineCallFrame-&gt;argumentCountIncludingThis = argumentCountIncludingThis;
7017         if (callee) {
7018             m_inlineCallFrame-&gt;calleeRecovery = ValueRecovery::constant(callee);
7019             m_inlineCallFrame-&gt;isClosureCall = false;
7020         } else
7021             m_inlineCallFrame-&gt;isClosureCall = true;
7022         m_inlineCallFrame-&gt;directCaller = byteCodeParser-&gt;currentCodeOrigin();
7023         m_inlineCallFrame-&gt;argumentsWithFixup.resizeToFit(argumentCountIncludingThisWithFixup); // Set the number of arguments including this, but don&#39;t configure the value recoveries, yet.
7024         m_inlineCallFrame-&gt;kind = kind;
7025 
7026         m_identifierRemap.resize(codeBlock-&gt;numberOfIdentifiers());
7027         m_switchRemap.resize(codeBlock-&gt;numberOfSwitchJumpTables());
7028 
7029         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfIdentifiers(); ++i) {
7030             UniquedStringImpl* rep = codeBlock-&gt;identifier(i).impl();
7031             unsigned index = byteCodeParser-&gt;m_graph.identifiers().ensure(rep);
7032             m_identifierRemap[i] = index;
7033         }
7034         for (unsigned i = 0; i &lt; codeBlock-&gt;numberOfSwitchJumpTables(); ++i) {
7035             m_switchRemap[i] = byteCodeParser-&gt;m_codeBlock-&gt;numberOfSwitchJumpTables();
<span class="line-modified">7036             byteCodeParser-&gt;m_codeBlock-&gt;addSwitchJumpTable() = codeBlock-&gt;switchJumpTable(i);</span>
7037         }
7038     } else {
7039         // Machine code block case.
7040         ASSERT(codeBlock == byteCodeParser-&gt;m_codeBlock);
7041         ASSERT(!callee);
7042         ASSERT(!returnValueVR.isValid());
7043         ASSERT(!inlineCallFrameStart.isValid());
7044 
7045         m_inlineCallFrame = 0;
7046 
7047         m_identifierRemap.resize(codeBlock-&gt;numberOfIdentifiers());
7048         m_switchRemap.resize(codeBlock-&gt;numberOfSwitchJumpTables());
7049         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfIdentifiers(); ++i)
7050             m_identifierRemap[i] = i;
7051         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfSwitchJumpTables(); ++i)
7052             m_switchRemap[i] = i;
7053     }
7054 
7055     m_argumentPositions.resize(argumentCountIncludingThisWithFixup);
7056     for (int i = 0; i &lt; argumentCountIncludingThisWithFixup; ++i) {
</pre>
<hr />
<pre>
7067 {
7068     m_byteCodeParser-&gt;m_inlineStackTop = m_caller;
7069     RELEASE_ASSERT(m_byteCodeParser-&gt;m_icContextStack.last() == &amp;m_optimizedContext);
7070     m_byteCodeParser-&gt;m_icContextStack.removeLast();
7071 }
7072 
7073 void ByteCodeParser::parseCodeBlock()
7074 {
7075     clearCaches();
7076 
7077     CodeBlock* codeBlock = m_inlineStackTop-&gt;m_codeBlock;
7078 
7079     if (UNLIKELY(m_graph.compilation())) {
7080         m_graph.compilation()-&gt;addProfiledBytecodes(
7081             *m_vm-&gt;m_perBytecodeProfiler, m_inlineStackTop-&gt;m_profiledBlock);
7082     }
7083 
7084     if (UNLIKELY(Options::dumpSourceAtDFGTime())) {
7085         Vector&lt;DeferredSourceDump&gt;&amp; deferredSourceDump = m_graph.m_plan.callback()-&gt;ensureDeferredSourceDump();
7086         if (inlineCallFrame()) {
<span class="line-modified">7087             DeferredSourceDump dump(codeBlock-&gt;baselineVersion(), m_codeBlock, JITCode::DFGJIT, inlineCallFrame()-&gt;directCaller.bytecodeIndex);</span>
7088             deferredSourceDump.append(dump);
7089         } else
7090             deferredSourceDump.append(DeferredSourceDump(codeBlock-&gt;baselineVersion()));
7091     }
7092 
7093     if (Options::dumpBytecodeAtDFGTime()) {
7094         dataLog(&quot;Parsing &quot;, *codeBlock);
7095         if (inlineCallFrame()) {
7096             dataLog(
<span class="line-modified">7097                 &quot; for inlining at &quot;, CodeBlockWithJITType(m_codeBlock, JITCode::DFGJIT),</span>
7098                 &quot; &quot;, inlineCallFrame()-&gt;directCaller);
7099         }
7100         dataLog(
7101             &quot;, isStrictMode = &quot;, codeBlock-&gt;ownerExecutable()-&gt;isStrictMode(), &quot;\n&quot;);
7102         codeBlock-&gt;baselineVersion()-&gt;dumpBytecode();
7103     }
7104 
7105     Vector&lt;InstructionStream::Offset, 32&gt; jumpTargets;
7106     computePreciseJumpTargets(codeBlock, jumpTargets);
7107     if (Options::dumpBytecodeAtDFGTime()) {
7108         dataLog(&quot;Jump targets: &quot;);
7109         CommaPrinter comma;
7110         for (unsigned i = 0; i &lt; jumpTargets.size(); ++i)
7111             dataLog(comma, jumpTargets[i]);
7112         dataLog(&quot;\n&quot;);
7113     }
7114 
7115     for (unsigned jumpTargetIndex = 0; jumpTargetIndex &lt;= jumpTargets.size(); ++jumpTargetIndex) {
7116         // The maximum bytecode offset to go into the current basicblock is either the next jump target, or the end of the instructions.
7117         unsigned limit = jumpTargetIndex &lt; jumpTargets.size() ? jumpTargets[jumpTargetIndex] : codeBlock-&gt;instructions().size();
</pre>
<hr />
<pre>
7155 
7156     // Should have reached the end of the instructions.
7157     ASSERT(m_currentIndex == codeBlock-&gt;instructions().size());
7158 
7159     VERBOSE_LOG(&quot;Done parsing &quot;, *codeBlock, &quot; (fell off end)\n&quot;);
7160 }
7161 
7162 template &lt;typename Bytecode&gt;
7163 void ByteCodeParser::handlePutByVal(Bytecode bytecode, unsigned instructionSize)
7164 {
7165     Node* base = get(bytecode.m_base);
7166     Node* property = get(bytecode.m_property);
7167     Node* value = get(bytecode.m_value);
7168     bool isDirect = Bytecode::opcodeID == op_put_by_val_direct;
7169     bool compiledAsPutById = false;
7170     {
7171         unsigned identifierNumber = std::numeric_limits&lt;unsigned&gt;::max();
7172         PutByIdStatus putByIdStatus;
7173         {
7174             ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
<span class="line-modified">7175             ByValInfo* byValInfo = m_inlineStackTop-&gt;m_baselineMap.get(CodeOrigin(currentCodeOrigin().bytecodeIndex)).byValInfo;</span>
7176             // FIXME: When the bytecode is not compiled in the baseline JIT, byValInfo becomes null.
7177             // At that time, there is no information.
7178             if (byValInfo
7179                 &amp;&amp; byValInfo-&gt;stubInfo
7180                 &amp;&amp; !byValInfo-&gt;tookSlowPath
7181                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIdent)
7182                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType)
7183                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
7184                 compiledAsPutById = true;
7185                 identifierNumber = m_graph.identifiers().ensure(byValInfo-&gt;cachedId.impl());
7186                 UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
7187 
7188                 if (Symbol* symbol = byValInfo-&gt;cachedSymbol.get()) {
7189                     FrozenValue* frozen = m_graph.freezeStrong(symbol);
7190                     addToGraph(CheckCell, OpInfo(frozen), property);
7191                 } else {
7192                     ASSERT(!uid-&gt;isSymbol());
7193                     addToGraph(CheckStringIdent, OpInfo(uid), property);
7194                 }
7195 
</pre>
<hr />
<pre>
7309             } while (changed);
7310         }
7311 
7312         InsertionSet insertionSet(m_graph);
7313         Operands&lt;VariableAccessData*&gt; mapping(OperandsLike, m_graph.block(0)-&gt;variablesAtHead);
7314 
7315         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
7316             if (blocksToIgnore.contains(block))
7317                 continue;
7318 
7319             mapping.fill(nullptr);
7320             if (validationEnabled()) {
7321                 // Verify that it&#39;s correct to fill mapping with nullptr.
7322                 for (unsigned i = 0; i &lt; block-&gt;variablesAtHead.size(); ++i) {
7323                     Node* node = block-&gt;variablesAtHead.at(i);
7324                     RELEASE_ASSERT(!node);
7325                 }
7326             }
7327 
7328             for (unsigned nodeIndex = 0; nodeIndex &lt; block-&gt;size(); ++nodeIndex) {
<span class="line-modified">7329                 Node* node = block-&gt;at(nodeIndex);</span>




7330 
<span class="line-modified">7331                 if (node-&gt;hasVariableAccessData(m_graph))</span>
<span class="line-modified">7332                     mapping.operand(node-&gt;local()) = node-&gt;variableAccessData();</span>




7333 
<span class="line-modified">7334                 if (node-&gt;op() == ForceOSRExit) {</span>
<span class="line-removed">7335                     NodeOrigin endOrigin = node-&gt;origin.withExitOK(true);</span>
7336 

7337                     if (validationEnabled()) {
7338                         // This verifies that we don&#39;t need to change any of the successors&#39;s predecessor
7339                         // list after planting the Unreachable below. At this point in the bytecode
7340                         // parser, we haven&#39;t linked up the predecessor lists yet.
7341                         for (BasicBlock* successor : block-&gt;successors())
7342                             RELEASE_ASSERT(successor-&gt;predecessors.isEmpty());
7343                     }
7344 
<span class="line-removed">7345                     block-&gt;resize(nodeIndex + 1);</span>
<span class="line-removed">7346 </span>
<span class="line-removed">7347                     insertionSet.insertNode(block-&gt;size(), SpecNone, ExitOK, endOrigin);</span>
<span class="line-removed">7348 </span>
7349                     auto insertLivenessPreservingOp = [&amp;] (InlineCallFrame* inlineCallFrame, NodeType op, VirtualRegister operand) {
7350                         VariableAccessData* variable = mapping.operand(operand);
7351                         if (!variable) {
7352                             variable = newVariableAccessData(operand);
7353                             mapping.operand(operand) = variable;
7354                         }
7355 
7356                         VirtualRegister argument = operand - (inlineCallFrame ? inlineCallFrame-&gt;stackOffset : 0);
7357                         if (argument.isArgument() &amp;&amp; !argument.isHeader()) {
7358                             const Vector&lt;ArgumentPosition*&gt;&amp; arguments = m_inlineCallFrameToArgumentPositions.get(inlineCallFrame);
7359                             arguments[argument.toArgument()]-&gt;addVariable(variable);
<span class="line-modified">7360                         } insertionSet.insertNode(block-&gt;size(), SpecNone, op, endOrigin, OpInfo(variable));</span>

7361                     };
7362                     auto addFlushDirect = [&amp;] (InlineCallFrame* inlineCallFrame, VirtualRegister operand) {
7363                         insertLivenessPreservingOp(inlineCallFrame, Flush, operand);
7364                     };
7365                     auto addPhantomLocalDirect = [&amp;] (InlineCallFrame* inlineCallFrame, VirtualRegister operand) {
7366                         insertLivenessPreservingOp(inlineCallFrame, PhantomLocal, operand);
7367                     };
<span class="line-modified">7368                     flushForTerminalImpl(endOrigin.semantic, addFlushDirect, addPhantomLocalDirect);</span>

7369 
<span class="line-modified">7370                     insertionSet.insertNode(block-&gt;size(), SpecNone, Unreachable, endOrigin);</span>
<span class="line-modified">7371                     insertionSet.execute(block);</span>
<span class="line-modified">7372                     break;</span>

























7373                 }







7374             }
7375         }
7376     } else if (validationEnabled()) {
7377         // Ensure our bookkeeping for ForceOSRExit nodes is working.
7378         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
7379             for (Node* node : *block)
7380                 RELEASE_ASSERT(node-&gt;op() != ForceOSRExit);
7381         }
7382     }
7383 
7384     m_graph.determineReachability();
7385     m_graph.killUnreachableBlocks();
7386 
7387     for (BlockIndex blockIndex = m_graph.numBlocks(); blockIndex--;) {
7388         BasicBlock* block = m_graph.block(blockIndex);
7389         if (!block)
7390             continue;
7391         ASSERT(block-&gt;variablesAtHead.numberOfLocals() == m_graph.block(0)-&gt;variablesAtHead.numberOfLocals());
7392         ASSERT(block-&gt;variablesAtHead.numberOfArguments() == m_graph.block(0)-&gt;variablesAtHead.numberOfArguments());
7393         ASSERT(block-&gt;variablesAtTail.numberOfLocals() == m_graph.block(0)-&gt;variablesAtHead.numberOfLocals());
</pre>
</td>
<td>
<hr />
<pre>
 250     // Calls check() for those conditions that aren&#39;t the slot base, and calls load() for the slot
 251     // base. Does a combination of watchpoint registration and check emission to guard the
 252     // conditions, and emits code to load the value from the slot base. Returns a node containing
 253     // the loaded value. Returns null if any of the conditions were no longer checkable.
 254     GetByOffsetMethod planLoad(const ObjectPropertyConditionSet&amp;);
 255     Node* load(SpeculatedType, const ObjectPropertyConditionSet&amp;, NodeType = GetByOffset);
 256 
 257     void prepareToParseBlock();
 258     void clearCaches();
 259 
 260     // Parse a single basic block of bytecode instructions.
 261     void parseBlock(unsigned limit);
 262     // Link block successors.
 263     void linkBlock(BasicBlock*, Vector&lt;BasicBlock*&gt;&amp; possibleTargets);
 264     void linkBlocks(Vector&lt;BasicBlock*&gt;&amp; unlinkedBlocks, Vector&lt;BasicBlock*&gt;&amp; possibleTargets);
 265 
 266     VariableAccessData* newVariableAccessData(VirtualRegister operand)
 267     {
 268         ASSERT(!operand.isConstant());
 269 
<span class="line-modified"> 270         m_graph.m_variableAccessData.append(operand);</span>
 271         return &amp;m_graph.m_variableAccessData.last();
 272     }
 273 
 274     // Get/Set the operands/result of a bytecode instruction.
 275     Node* getDirect(VirtualRegister operand)
 276     {
 277         ASSERT(!operand.isConstant());
 278 
 279         // Is this an argument?
 280         if (operand.isArgument())
 281             return getArgument(operand);
 282 
 283         // Must be a local.
 284         return getLocal(operand);
 285     }
 286 
 287     Node* get(VirtualRegister operand)
 288     {
 289         if (operand.isConstant()) {
 290             unsigned constantIndex = operand.toConstantIndex();
</pre>
<hr />
<pre>
 304                     constantNode = addToGraph(DoubleConstant, OpInfo(m_graph.freezeStrong(jsDoubleNumber(value.asNumber()))));
 305                 else
 306                     constantNode = addToGraph(JSConstant, OpInfo(m_graph.freezeStrong(value)));
 307                 m_constants[constantIndex] = constantNode;
 308             }
 309             ASSERT(m_constants[constantIndex]);
 310             return m_constants[constantIndex];
 311         }
 312 
 313         if (inlineCallFrame()) {
 314             if (!inlineCallFrame()-&gt;isClosureCall) {
 315                 JSFunction* callee = inlineCallFrame()-&gt;calleeConstant();
 316                 if (operand.offset() == CallFrameSlot::callee)
 317                     return weakJSConstant(callee);
 318             }
 319         } else if (operand.offset() == CallFrameSlot::callee) {
 320             // We have to do some constant-folding here because this enables CreateThis folding. Note
 321             // that we don&#39;t have such watchpoint-based folding for inlined uses of Callee, since in that
 322             // case if the function is a singleton then we already know it.
 323             if (FunctionExecutable* executable = jsDynamicCast&lt;FunctionExecutable*&gt;(*m_vm, m_codeBlock-&gt;ownerExecutable())) {
<span class="line-modified"> 324                 if (JSFunction* function = executable-&gt;singleton().inferredValue()) {</span>
<span class="line-modified"> 325                     m_graph.watchpoints().addLazily(executable);</span>


 326                     return weakJSConstant(function);
 327                 }
 328             }
 329             return addToGraph(GetCallee);
 330         }
 331 
 332         return getDirect(m_inlineStackTop-&gt;remapOperand(operand));
 333     }
 334 
 335     enum SetMode {
 336         // A normal set which follows a two-phase commit that spans code origins. During
 337         // the current code origin it issues a MovHint, and at the start of the next
 338         // code origin there will be a SetLocal. If the local needs flushing, the second
 339         // SetLocal will be preceded with a Flush.
 340         NormalSet,
 341 
 342         // A set where the SetLocal happens immediately and there is still a Flush. This
 343         // is relevant when assigning to a local in tricky situations for the delayed
 344         // SetLocal logic but where we know that we have not performed any side effects
 345         // within this code origin. This is a safe replacement for NormalSet anytime we
</pre>
<hr />
<pre>
 366         }
 367 
 368         return delayed.execute(this);
 369     }
 370 
 371     void processSetLocalQueue()
 372     {
 373         for (unsigned i = 0; i &lt; m_setLocalQueue.size(); ++i)
 374             m_setLocalQueue[i].execute(this);
 375         m_setLocalQueue.shrink(0);
 376     }
 377 
 378     Node* set(VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 379     {
 380         return setDirect(m_inlineStackTop-&gt;remapOperand(operand), value, setMode);
 381     }
 382 
 383     Node* injectLazyOperandSpeculation(Node* node)
 384     {
 385         ASSERT(node-&gt;op() == GetLocal);
<span class="line-modified"> 386         ASSERT(node-&gt;origin.semantic.bytecodeIndex() == m_currentIndex);</span>
 387         ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
 388         LazyOperandValueProfileKey key(m_currentIndex, node-&gt;local());
 389         SpeculatedType prediction = m_inlineStackTop-&gt;m_lazyOperands.prediction(locker, key);
 390         node-&gt;variableAccessData()-&gt;predict(prediction);
 391         return node;
 392     }
 393 
 394     // Used in implementing get/set, above, where the operand is a local variable.
 395     Node* getLocal(VirtualRegister operand)
 396     {
 397         unsigned local = operand.toLocal();
 398 
 399         Node* node = m_currentBlock-&gt;variablesAtTail.local(local);
 400 
 401         // This has two goals: 1) link together variable access datas, and 2)
 402         // try to avoid creating redundant GetLocals. (1) is required for
 403         // correctness - no other phase will ensure that block-local variable
 404         // access data unification is done correctly. (2) is purely opportunistic
 405         // and is meant as an compile-time optimization only.
 406 
</pre>
<hr />
<pre>
 423         node = injectLazyOperandSpeculation(addToGraph(GetLocal, OpInfo(variable)));
 424         m_currentBlock-&gt;variablesAtTail.local(local) = node;
 425         return node;
 426     }
 427     Node* setLocal(const CodeOrigin&amp; semanticOrigin, VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 428     {
 429         SetForScope&lt;CodeOrigin&gt; originChange(m_currentSemanticOrigin, semanticOrigin);
 430 
 431         unsigned local = operand.toLocal();
 432 
 433         if (setMode != ImmediateNakedSet) {
 434             ArgumentPosition* argumentPosition = findArgumentPositionForLocal(operand);
 435             if (argumentPosition)
 436                 flushDirect(operand, argumentPosition);
 437             else if (m_graph.needsScopeRegister() &amp;&amp; operand == m_codeBlock-&gt;scopeRegister())
 438                 flush(operand);
 439         }
 440 
 441         VariableAccessData* variableAccessData = newVariableAccessData(operand);
 442         variableAccessData-&gt;mergeStructureCheckHoistingFailed(
<span class="line-modified"> 443             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex(), BadCache));</span>
 444         variableAccessData-&gt;mergeCheckArrayHoistingFailed(
<span class="line-modified"> 445             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex(), BadIndexingType));</span>
 446         Node* node = addToGraph(SetLocal, OpInfo(variableAccessData), value);
 447         m_currentBlock-&gt;variablesAtTail.local(local) = node;
 448         return node;
 449     }
 450 
 451     // Used in implementing get/set, above, where the operand is an argument.
 452     Node* getArgument(VirtualRegister operand)
 453     {
 454         unsigned argument = operand.toArgument();
 455         ASSERT(argument &lt; m_numArguments);
 456 
 457         Node* node = m_currentBlock-&gt;variablesAtTail.argument(argument);
 458 
 459         VariableAccessData* variable;
 460 
 461         if (node) {
 462             variable = node-&gt;variableAccessData();
 463 
 464             switch (node-&gt;op()) {
 465             case GetLocal:
</pre>
<hr />
<pre>
 479     Node* setArgument(const CodeOrigin&amp; semanticOrigin, VirtualRegister operand, Node* value, SetMode setMode = NormalSet)
 480     {
 481         SetForScope&lt;CodeOrigin&gt; originChange(m_currentSemanticOrigin, semanticOrigin);
 482 
 483         unsigned argument = operand.toArgument();
 484         ASSERT(argument &lt; m_numArguments);
 485 
 486         VariableAccessData* variableAccessData = newVariableAccessData(operand);
 487 
 488         // Always flush arguments, except for &#39;this&#39;. If &#39;this&#39; is created by us,
 489         // then make sure that it&#39;s never unboxed.
 490         if (argument || m_graph.needsFlushedThis()) {
 491             if (setMode != ImmediateNakedSet)
 492                 flushDirect(operand);
 493         }
 494 
 495         if (!argument &amp;&amp; m_codeBlock-&gt;specializationKind() == CodeForConstruct)
 496             variableAccessData-&gt;mergeShouldNeverUnbox(true);
 497 
 498         variableAccessData-&gt;mergeStructureCheckHoistingFailed(
<span class="line-modified"> 499             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex(), BadCache));</span>
 500         variableAccessData-&gt;mergeCheckArrayHoistingFailed(
<span class="line-modified"> 501             m_inlineStackTop-&gt;m_exitProfile.hasExitSite(semanticOrigin.bytecodeIndex(), BadIndexingType));</span>
 502         Node* node = addToGraph(SetLocal, OpInfo(variableAccessData), value);
 503         m_currentBlock-&gt;variablesAtTail.argument(argument) = node;
 504         return node;
 505     }
 506 
 507     ArgumentPosition* findArgumentPositionForArgument(int argument)
 508     {
 509         InlineStackEntry* stack = m_inlineStackTop;
 510         while (stack-&gt;m_inlineCallFrame)
 511             stack = stack-&gt;m_caller;
 512         return stack-&gt;m_argumentPositions[argument];
 513     }
 514 
 515     ArgumentPosition* findArgumentPositionForLocal(VirtualRegister operand)
 516     {
 517         for (InlineStackEntry* stack = m_inlineStackTop; ; stack = stack-&gt;m_caller) {
 518             InlineCallFrame* inlineCallFrame = stack-&gt;m_inlineCallFrame;
 519             if (!inlineCallFrame)
 520                 break;
 521             if (operand.offset() &lt; static_cast&lt;int&gt;(inlineCallFrame-&gt;stackOffset + CallFrame::headerSizeInRegisters))
</pre>
<hr />
<pre>
 544             numArguments = inlineCallFrame-&gt;argumentsWithFixup.size();
 545             if (inlineCallFrame-&gt;isClosureCall)
 546                 addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, VirtualRegister(CallFrameSlot::callee)));
 547             if (inlineCallFrame-&gt;isVarargs())
 548                 addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, VirtualRegister(CallFrameSlot::argumentCount)));
 549         } else
 550             numArguments = m_graph.baselineCodeBlockFor(inlineCallFrame)-&gt;numParameters();
 551 
 552         for (unsigned argument = numArguments; argument--;)
 553             addFlushDirect(inlineCallFrame, remapOperand(inlineCallFrame, virtualRegisterForArgument(argument)));
 554 
 555         if (m_graph.needsScopeRegister())
 556             addFlushDirect(nullptr, m_graph.m_codeBlock-&gt;scopeRegister());
 557     }
 558 
 559     template&lt;typename AddFlushDirectFunc, typename AddPhantomLocalDirectFunc&gt;
 560     void flushForTerminalImpl(CodeOrigin origin, const AddFlushDirectFunc&amp; addFlushDirect, const AddPhantomLocalDirectFunc&amp; addPhantomLocalDirect)
 561     {
 562         origin.walkUpInlineStack(
 563             [&amp;] (CodeOrigin origin) {
<span class="line-modified"> 564                 unsigned bytecodeIndex = origin.bytecodeIndex();</span>
<span class="line-modified"> 565                 InlineCallFrame* inlineCallFrame = origin.inlineCallFrame();</span>
 566                 flushImpl(inlineCallFrame, addFlushDirect);
 567 
 568                 CodeBlock* codeBlock = m_graph.baselineCodeBlockFor(inlineCallFrame);
 569                 FullBytecodeLiveness&amp; fullLiveness = m_graph.livenessFor(codeBlock);
 570                 const FastBitVector&amp; livenessAtBytecode = fullLiveness.getLiveness(bytecodeIndex);
 571 
 572                 for (unsigned local = codeBlock-&gt;numCalleeLocals(); local--;) {
 573                     if (livenessAtBytecode[local])
 574                         addPhantomLocalDirect(inlineCallFrame, remapOperand(inlineCallFrame, virtualRegisterForLocal(local)));
 575                 }
 576             });
 577     }
 578 
 579     void flush(VirtualRegister operand)
 580     {
 581         flushDirect(m_inlineStackTop-&gt;remapOperand(operand));
 582     }
 583 
 584     void flushDirect(VirtualRegister operand)
 585     {
</pre>
<hr />
<pre>
 813 
 814         Node* call = addCallWithoutSettingResult(
 815             op, OpInfo(signature), callee, argCount, registerOffset, OpInfo(prediction));
 816         if (result.isValid())
 817             set(result, call);
 818         return call;
 819     }
 820 
 821     Node* cellConstantWithStructureCheck(JSCell* object, Structure* structure)
 822     {
 823         // FIXME: This should route to emitPropertyCheck, not the other way around. But currently,
 824         // this gets no profit from using emitPropertyCheck() since we&#39;ll non-adaptively watch the
 825         // object&#39;s structure as soon as we make it a weakJSCosntant.
 826         Node* objectNode = weakJSConstant(object);
 827         addToGraph(CheckStructure, OpInfo(m_graph.addStructureSet(structure)), objectNode);
 828         return objectNode;
 829     }
 830 
 831     SpeculatedType getPredictionWithoutOSRExit(unsigned bytecodeIndex)
 832     {
<span class="line-modified"> 833         auto getValueProfilePredictionFromForCodeBlockAndBytecodeOffset = [&amp;] (CodeBlock* codeBlock, const CodeOrigin&amp; codeOrigin)</span>
 834         {
<span class="line-modified"> 835             SpeculatedType prediction;</span>
<span class="line-modified"> 836             {</span>
<span class="line-modified"> 837                 ConcurrentJSLocker locker(codeBlock-&gt;m_lock);</span>
<span class="line-added"> 838                 prediction = codeBlock-&gt;valueProfilePredictionForBytecodeOffset(locker, codeOrigin.bytecodeIndex());</span>
<span class="line-added"> 839             }</span>
<span class="line-added"> 840             auto* fuzzerAgent = m_vm-&gt;fuzzerAgent();</span>
<span class="line-added"> 841             if (UNLIKELY(fuzzerAgent))</span>
<span class="line-added"> 842                 return fuzzerAgent-&gt;getPrediction(codeBlock, codeOrigin, prediction) &amp; SpecBytecodeTop;</span>
<span class="line-added"> 843             return prediction;</span>
<span class="line-added"> 844         };</span>
 845 
<span class="line-added"> 846         SpeculatedType prediction = getValueProfilePredictionFromForCodeBlockAndBytecodeOffset(m_inlineStackTop-&gt;m_profiledBlock, CodeOrigin(bytecodeIndex, inlineCallFrame()));</span>
 847         if (prediction != SpecNone)
 848             return prediction;
 849 
 850         // If we have no information about the values this
 851         // node generates, we check if by any chance it is
 852         // a tail call opcode. In that case, we walk up the
 853         // inline frames to find a call higher in the call
 854         // chain and use its prediction. If we only have
 855         // inlined tail call frames, we use SpecFullTop
 856         // to avoid a spurious OSR exit.
 857         auto instruction = m_inlineStackTop-&gt;m_profiledBlock-&gt;instructions().at(bytecodeIndex);
 858         OpcodeID opcodeID = instruction-&gt;opcodeID();
 859 
 860         switch (opcodeID) {
 861         case op_tail_call:
 862         case op_tail_call_varargs:
 863         case op_tail_call_forward_arguments: {
 864             // Things should be more permissive to us returning BOTTOM instead of TOP here.
 865             // Currently, this will cause us to Force OSR exit. This is bad because returning
 866             // TOP will cause anything that transitively touches this speculated type to
 867             // also become TOP during prediction propagation.
 868             // https://bugs.webkit.org/show_bug.cgi?id=164337
 869             if (!inlineCallFrame())
 870                 return SpecFullTop;
 871 
 872             CodeOrigin* codeOrigin = inlineCallFrame()-&gt;getCallerSkippingTailCalls();
 873             if (!codeOrigin)
 874                 return SpecFullTop;
 875 
 876             InlineStackEntry* stack = m_inlineStackTop;
<span class="line-modified"> 877             while (stack-&gt;m_inlineCallFrame != codeOrigin-&gt;inlineCallFrame())</span>
 878                 stack = stack-&gt;m_caller;
 879 
<span class="line-modified"> 880             return getValueProfilePredictionFromForCodeBlockAndBytecodeOffset(stack-&gt;m_profiledBlock, *codeOrigin);</span>



 881         }
 882 
 883         default:
 884             return SpecNone;
 885         }
 886 
 887         RELEASE_ASSERT_NOT_REACHED();
 888         return SpecNone;
 889     }
 890 
 891     SpeculatedType getPrediction(unsigned bytecodeIndex)
 892     {
 893         SpeculatedType prediction = getPredictionWithoutOSRExit(bytecodeIndex);
 894 
 895         if (prediction == SpecNone) {
 896             // We have no information about what values this node generates. Give up
 897             // on executing this code, since we&#39;re likely to do more damage than good.
 898             addToGraph(ForceOSRExit);
 899         }
 900 
</pre>
<hr />
<pre>
 916         CodeBlock* codeBlock = m_inlineStackTop-&gt;m_profiledBlock;
 917         ArrayProfile* profile = codeBlock-&gt;getArrayProfile(codeBlock-&gt;bytecodeOffset(m_currentInstruction));
 918         return getArrayMode(*profile, action);
 919     }
 920 
 921     ArrayMode getArrayMode(ArrayProfile&amp; profile, Array::Action action)
 922     {
 923         ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
 924         profile.computeUpdatedPrediction(locker, m_inlineStackTop-&gt;m_profiledBlock);
 925         bool makeSafe = profile.outOfBounds(locker);
 926         return ArrayMode::fromObserved(locker, &amp;profile, action, makeSafe);
 927     }
 928 
 929     Node* makeSafe(Node* node)
 930     {
 931         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
 932             node-&gt;mergeFlags(NodeMayOverflowInt32InDFG);
 933         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
 934             node-&gt;mergeFlags(NodeMayNegZeroInDFG);
 935 
<span class="line-modified"> 936         if (!isX86() &amp;&amp; (node-&gt;op() == ArithMod || node-&gt;op() == ValueMod))</span>
 937             return node;
 938 
 939         {
 940             ArithProfile* arithProfile = m_inlineStackTop-&gt;m_profiledBlock-&gt;arithProfileForBytecodeOffset(m_currentIndex);
 941             if (arithProfile) {
 942                 switch (node-&gt;op()) {
 943                 case ArithAdd:
 944                 case ArithSub:
 945                 case ValueAdd:
 946                     if (arithProfile-&gt;didObserveDouble())
 947                         node-&gt;mergeFlags(NodeMayHaveDoubleResult);
 948                     if (arithProfile-&gt;didObserveNonNumeric())
 949                         node-&gt;mergeFlags(NodeMayHaveNonNumericResult);
 950                     if (arithProfile-&gt;didObserveBigInt())
 951                         node-&gt;mergeFlags(NodeMayHaveBigIntResult);
 952                     break;
 953 
 954                 case ValueMul:
 955                 case ArithMul: {
 956                     if (arithProfile-&gt;didObserveInt52Overflow())
</pre>
<hr />
<pre>
 977                         node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline);
 978                     if (arithProfile-&gt;didObserveNonNumeric())
 979                         node-&gt;mergeFlags(NodeMayHaveNonNumericResult);
 980                     if (arithProfile-&gt;didObserveBigInt())
 981                         node-&gt;mergeFlags(NodeMayHaveBigIntResult);
 982                     break;
 983                 }
 984 
 985                 default:
 986                     break;
 987                 }
 988             }
 989         }
 990 
 991         if (m_inlineStackTop-&gt;m_profiledBlock-&gt;likelyToTakeSlowCase(m_currentIndex)) {
 992             switch (node-&gt;op()) {
 993             case UInt32ToNumber:
 994             case ArithAdd:
 995             case ArithSub:
 996             case ValueAdd:
<span class="line-added"> 997             case ValueMod:</span>
 998             case ArithMod: // for ArithMod &quot;MayOverflow&quot; means we tried to divide by zero, or we saw double.
 999                 node-&gt;mergeFlags(NodeMayOverflowInt32InBaseline);
1000                 break;
1001 
1002             default:
1003                 break;
1004             }
1005         }
1006 
1007         return node;
1008     }
1009 
1010     Node* makeDivSafe(Node* node)
1011     {
1012         ASSERT(node-&gt;op() == ArithDiv || node-&gt;op() == ValueDiv);
1013 
1014         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, Overflow))
1015             node-&gt;mergeFlags(NodeMayOverflowInt32InDFG);
1016         if (m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, NegativeZero))
1017             node-&gt;mergeFlags(NodeMayNegZeroInDFG);
</pre>
<hr />
<pre>
1424         } else
1425             addToGraph(SetCallee, callTargetNode);
1426 
1427         // We must set the arguments to the right values
1428         if (!stackEntry-&gt;m_inlineCallFrame)
1429             addToGraph(SetArgumentCountIncludingThis, OpInfo(argumentCountIncludingThis));
1430         int argIndex = 0;
1431         for (; argIndex &lt; argumentCountIncludingThis; ++argIndex) {
1432             Node* value = get(virtualRegisterForArgument(argIndex, registerOffset));
1433             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForArgument(argIndex)), value, NormalSet);
1434         }
1435         Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
1436         for (; argIndex &lt; stackEntry-&gt;m_codeBlock-&gt;numParameters(); ++argIndex)
1437             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForArgument(argIndex)), undefined, NormalSet);
1438 
1439         // We must repeat the work of op_enter here as we will jump right after it.
1440         // We jump right after it and not before it, because of some invariant saying that a CFG root cannot have predecessors in the IR.
1441         for (int i = 0; i &lt; stackEntry-&gt;m_codeBlock-&gt;numVars(); ++i)
1442             setDirect(stackEntry-&gt;remapOperand(virtualRegisterForLocal(i)), undefined, NormalSet);
1443 

1444         unsigned oldIndex = m_currentIndex;
1445         auto oldStackTop = m_inlineStackTop;
<span class="line-added">1446 </span>
<span class="line-added">1447         // First, we emit check-traps operation pointing to bc#0 as exit.</span>
1448         m_inlineStackTop = stackEntry;
<span class="line-added">1449         m_currentIndex = 0;</span>
<span class="line-added">1450         m_exitOK = true;</span>
<span class="line-added">1451         addToGraph(Options::usePollingTraps() ? CheckTraps : InvalidationPoint);</span>
<span class="line-added">1452 </span>
<span class="line-added">1453         // Then, we want to emit the SetLocals with an exit origin that points to the place we are jumping to.</span>
1454         m_currentIndex = opcodeLengths[op_enter];
1455         m_exitOK = true;
1456         processSetLocalQueue();
1457         m_currentIndex = oldIndex;
1458         m_inlineStackTop = oldStackTop;
1459         m_exitOK = false;
1460 
1461         BasicBlock** entryBlockPtr = tryBinarySearch&lt;BasicBlock*, unsigned&gt;(stackEntry-&gt;m_blockLinkingTargets, stackEntry-&gt;m_blockLinkingTargets.size(), opcodeLengths[op_enter], getBytecodeBeginForBlock);
1462         RELEASE_ASSERT(entryBlockPtr);
1463         addJumpTo(*entryBlockPtr);
1464         return true;
1465         // It would be unsound to jump over a non-tail call: the &quot;tail&quot; call is not really a tail call in that case.
1466     } while (stackEntry-&gt;m_inlineCallFrame &amp;&amp; stackEntry-&gt;m_inlineCallFrame-&gt;kind == InlineCallFrame::TailCall &amp;&amp; (stackEntry = stackEntry-&gt;m_caller));
1467 
1468     // The tail call was not recursive
1469     return false;
1470 }
1471 
1472 unsigned ByteCodeParser::inliningCost(CallVariant callee, int argumentCountIncludingThis, InlineCallFrame::Kind kind)
1473 {
</pre>
<hr />
<pre>
1547 
1548     for (InlineStackEntry* entry = m_inlineStackTop; entry; entry = entry-&gt;m_caller) {
1549         ++depth;
1550         if (depth &gt;= Options::maximumInliningDepth()) {
1551             VERBOSE_LOG(&quot;    Failing because depth exceeded.\n&quot;);
1552             return UINT_MAX;
1553         }
1554 
1555         if (entry-&gt;executable() == executable) {
1556             ++recursion;
1557             if (recursion &gt;= Options::maximumInliningRecursion()) {
1558                 VERBOSE_LOG(&quot;    Failing because recursion detected.\n&quot;);
1559                 return UINT_MAX;
1560             }
1561         }
1562     }
1563 
1564     VERBOSE_LOG(&quot;    Inlining should be possible.\n&quot;);
1565 
1566     // It might be possible to inline.
<span class="line-modified">1567     return codeBlock-&gt;bytecodeCost();</span>
1568 }
1569 
1570 template&lt;typename ChecksFunctor&gt;
1571 void ByteCodeParser::inlineCall(Node* callTargetNode, VirtualRegister result, CallVariant callee, int registerOffset, int argumentCountIncludingThis, InlineCallFrame::Kind kind, BasicBlock* continuationBlock, const ChecksFunctor&amp; insertChecks)
1572 {
1573     const Instruction* savedCurrentInstruction = m_currentInstruction;
1574     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1575 


1576     CodeBlock* codeBlock = callee.functionExecutable()-&gt;baselineCodeBlockFor(specializationKind);
1577     insertChecks(codeBlock);
1578 
1579     // FIXME: Don&#39;t flush constants!
1580 
1581     // arityFixupCount and numberOfStackPaddingSlots are different. While arityFixupCount does not consider about stack alignment,
1582     // numberOfStackPaddingSlots consider alignment. Consider the following case,
1583     //
1584     // before: [ ... ][arg0][header]
1585     // after:  [ ... ][ext ][arg1][arg0][header]
1586     //
1587     // In the above case, arityFixupCount is 1. But numberOfStackPaddingSlots is 2 because the stack needs to be aligned.
1588     // We insert extra slots to align stack.
1589     int arityFixupCount = std::max&lt;int&gt;(codeBlock-&gt;numParameters() - argumentCountIncludingThis, 0);
1590     int numberOfStackPaddingSlots = CommonSlowPaths::numberOfStackPaddingSlots(codeBlock, argumentCountIncludingThis);
1591     ASSERT(!(numberOfStackPaddingSlots % stackAlignmentRegisters()));
1592     int registerOffsetAfterFixup = registerOffset - numberOfStackPaddingSlots;
1593 
1594     int inlineCallFrameStart = m_inlineStackTop-&gt;remapOperand(VirtualRegister(registerOffsetAfterFixup)).offset() + CallFrame::headerSizeInRegisters;
1595 
1596     ensureLocals(
1597         VirtualRegister(inlineCallFrameStart).toLocal() + 1 +
1598         CallFrame::headerSizeInRegisters + codeBlock-&gt;numCalleeLocals());
1599 
1600     size_t argumentPositionStart = m_graph.m_argumentPositions.size();
1601 
1602     if (result.isValid())
1603         result = m_inlineStackTop-&gt;remapOperand(result);
1604 
1605     VariableAccessData* calleeVariable = nullptr;
1606     if (callee.isClosureCall()) {
1607         Node* calleeSet = set(
1608             VirtualRegister(registerOffsetAfterFixup + CallFrameSlot::callee), callTargetNode, ImmediateNakedSet);
1609 
1610         calleeVariable = calleeSet-&gt;variableAccessData();
1611         calleeVariable-&gt;mergeShouldNeverUnbox(true);
1612     }
1613 
<span class="line-added">1614     InlineStackEntry* callerStackTop = m_inlineStackTop;</span>
<span class="line-added">1615     InlineStackEntry inlineStackEntry(this, codeBlock, codeBlock, callee.function(), result,</span>
<span class="line-added">1616         (VirtualRegister)inlineCallFrameStart, argumentCountIncludingThis, kind, continuationBlock);</span>
<span class="line-added">1617 </span>
<span class="line-added">1618     // This is where the actual inlining really happens.</span>
<span class="line-added">1619     unsigned oldIndex = m_currentIndex;</span>
<span class="line-added">1620     m_currentIndex = 0;</span>
<span class="line-added">1621 </span>
<span class="line-added">1622     switch (kind) {</span>
<span class="line-added">1623     case InlineCallFrame::GetterCall:</span>
<span class="line-added">1624     case InlineCallFrame::SetterCall: {</span>
<span class="line-added">1625         // When inlining getter and setter calls, we setup a stack frame which does not appear in the bytecode.</span>
<span class="line-added">1626         // Because Inlining can switch on executable, we could have a graph like this.</span>
<span class="line-added">1627         //</span>
<span class="line-added">1628         // BB#0</span>
<span class="line-added">1629         //     ...</span>
<span class="line-added">1630         //     30: GetSetter</span>
<span class="line-added">1631         //     31: MovHint(loc10)</span>
<span class="line-added">1632         //     32: SetLocal(loc10)</span>
<span class="line-added">1633         //     33: MovHint(loc9)</span>
<span class="line-added">1634         //     34: SetLocal(loc9)</span>
<span class="line-added">1635         //     ...</span>
<span class="line-added">1636         //     37: GetExecutable(@30)</span>
<span class="line-added">1637         //     ...</span>
<span class="line-added">1638         //     41: Switch(@37)</span>
<span class="line-added">1639         //</span>
<span class="line-added">1640         // BB#2</span>
<span class="line-added">1641         //     42: GetLocal(loc12, bc#7 of caller)</span>
<span class="line-added">1642         //     ...</span>
<span class="line-added">1643         //     --&gt; callee: loc9 and loc10 are arguments of callee.</span>
<span class="line-added">1644         //       ...</span>
<span class="line-added">1645         //       &lt;HERE, exit to callee, loc9 and loc10 are required in the bytecode&gt;</span>
<span class="line-added">1646         //</span>
<span class="line-added">1647         // When we prune OSR availability at the beginning of BB#2 (bc#7 in the caller), we prune loc9 and loc10&#39;s liveness because the caller does not actually have loc9 and loc10.</span>
<span class="line-added">1648         // However, when we begin executing the callee, we need OSR exit to be aware of where it can recover the arguments to the setter, loc9 and loc10. The MovHints in the inlined</span>
<span class="line-added">1649         // callee make it so that if we exit at &lt;HERE&gt;, we can recover loc9 and loc10.</span>
<span class="line-added">1650         for (int index = 0; index &lt; argumentCountIncludingThis; ++index) {</span>
<span class="line-added">1651             VirtualRegister argumentToGet = callerStackTop-&gt;remapOperand(virtualRegisterForArgument(index, registerOffset));</span>
<span class="line-added">1652             Node* value = getDirect(argumentToGet);</span>
<span class="line-added">1653             addToGraph(MovHint, OpInfo(argumentToGet.offset()), value);</span>
<span class="line-added">1654             m_setLocalQueue.append(DelayedSetLocal { currentCodeOrigin(), argumentToGet, value, ImmediateNakedSet });</span>
<span class="line-added">1655         }</span>
<span class="line-added">1656         break;</span>
<span class="line-added">1657     }</span>
<span class="line-added">1658     default:</span>
<span class="line-added">1659         break;</span>
<span class="line-added">1660     }</span>
<span class="line-added">1661 </span>
1662     if (arityFixupCount) {
1663         // Note: we do arity fixup in two phases:
1664         // 1. We get all the values we need and MovHint them to the expected locals.
<span class="line-modified">1665         // 2. We SetLocal them after that. This way, if we exit, the callee&#39;s</span>
1666         //    frame is already set up. If any SetLocal exits, we have a valid exit state.
1667         //    This is required because if we didn&#39;t do this in two phases, we may exit in
<span class="line-modified">1668         //    the middle of arity fixup from the callee&#39;s CodeOrigin. This is unsound because exited</span>
<span class="line-modified">1669         //    code does not have arity fixup so that remaining necessary fixups are not executed.</span>
<span class="line-modified">1670         //    For example, consider if we need to pad two args:</span>
1671         //    [arg3][arg2][arg1][arg0]
1672         //    [fix ][fix ][arg3][arg2][arg1][arg0]
1673         //    We memcpy starting from arg0 in the direction of arg3. If we were to exit at a type check
<span class="line-modified">1674         //    for arg3&#39;s SetLocal in the callee&#39;s CodeOrigin, we&#39;d exit with a frame like so:</span>
1675         //    [arg3][arg2][arg1][arg2][arg1][arg0]
<span class="line-modified">1676         //    Since we do not perform arity fixup in the callee, this is the frame used by the callee.</span>
<span class="line-modified">1677         //    And the callee would then just end up thinking its argument are:</span>
<span class="line-added">1678         //    [fix ][fix ][arg3][arg2][arg1][arg0]</span>
1679         //    which is incorrect.
1680 
1681         Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
1682         // The stack needs to be aligned due to the JS calling convention. Thus, we have a hole if the count of arguments is not aligned.
1683         // We call this hole &quot;extra slot&quot;. Consider the following case, the number of arguments is 2. If this argument
1684         // count does not fulfill the stack alignment requirement, we already inserted extra slots.
1685         //
1686         // before: [ ... ][ext ][arg1][arg0][header]
1687         //
1688         // In the above case, one extra slot is inserted. If the code&#39;s parameter count is 3, we will fixup arguments.
1689         // At that time, we can simply use this extra slots. So the fixuped stack is the following.
1690         //
1691         // before: [ ... ][ext ][arg1][arg0][header]
1692         // after:  [ ... ][arg2][arg1][arg0][header]
1693         //
1694         // In such cases, we do not need to move frames.
1695         if (registerOffsetAfterFixup != registerOffset) {
1696             for (int index = 0; index &lt; argumentCountIncludingThis; ++index) {
<span class="line-modified">1697                 VirtualRegister argumentToGet = callerStackTop-&gt;remapOperand(virtualRegisterForArgument(index, registerOffset));</span>
<span class="line-modified">1698                 Node* value = getDirect(argumentToGet);</span>
<span class="line-added">1699                 VirtualRegister argumentToSet = m_inlineStackTop-&gt;remapOperand(virtualRegisterForArgument(index));</span>
1700                 addToGraph(MovHint, OpInfo(argumentToSet.offset()), value);
1701                 m_setLocalQueue.append(DelayedSetLocal { currentCodeOrigin(), argumentToSet, value, ImmediateNakedSet });
1702             }
1703         }
1704         for (int index = 0; index &lt; arityFixupCount; ++index) {
<span class="line-modified">1705             VirtualRegister argumentToSet = m_inlineStackTop-&gt;remapOperand(virtualRegisterForArgument(argumentCountIncludingThis + index));</span>
1706             addToGraph(MovHint, OpInfo(argumentToSet.offset()), undefined);
1707             m_setLocalQueue.append(DelayedSetLocal { currentCodeOrigin(), argumentToSet, undefined, ImmediateNakedSet });
1708         }
1709 
1710         // At this point, it&#39;s OK to OSR exit because we finished setting up
<span class="line-modified">1711         // our callee&#39;s frame. We emit an ExitOK below.</span>
1712     }
1713 







1714     // At this point, it&#39;s again OK to OSR exit.
1715     m_exitOK = true;
1716     addToGraph(ExitOK);
1717 
1718     processSetLocalQueue();
1719 
1720     InlineVariableData inlineVariableData;
1721     inlineVariableData.inlineCallFrame = m_inlineStackTop-&gt;m_inlineCallFrame;
1722     inlineVariableData.argumentPositionStart = argumentPositionStart;
1723     inlineVariableData.calleeVariable = 0;
1724 
1725     RELEASE_ASSERT(
1726         m_inlineStackTop-&gt;m_inlineCallFrame-&gt;isClosureCall
1727         == callee.isClosureCall());
1728     if (callee.isClosureCall()) {
1729         RELEASE_ASSERT(calleeVariable);
1730         inlineVariableData.calleeVariable = calleeVariable;
1731     }
1732 
1733     m_graph.m_inlineVariableData.append(inlineVariableData);
</pre>
<hr />
<pre>
1858     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1859     if (inliningCost(callVariant, maxNumArguments, kind) &gt; getInliningBalance(callLinkStatus, specializationKind)) {
1860         VERBOSE_LOG(&quot;Bailing inlining: inlining cost too high.\n&quot;);
1861         return false;
1862     }
1863 
1864     int registerOffset = firstFreeReg + 1;
1865     registerOffset -= maxNumArguments; // includes &quot;this&quot;
1866     registerOffset -= CallFrame::headerSizeInRegisters;
1867     registerOffset = -WTF::roundUpToMultipleOf(stackAlignmentRegisters(), -registerOffset);
1868 
1869     auto insertChecks = [&amp;] (CodeBlock* codeBlock) {
1870         emitFunctionChecks(callVariant, callTargetNode, thisArgument);
1871 
1872         int remappedRegisterOffset =
1873         m_inlineStackTop-&gt;remapOperand(VirtualRegister(registerOffset)).offset();
1874 
1875         ensureLocals(VirtualRegister(remappedRegisterOffset).toLocal());
1876 
1877         int argumentStart = registerOffset + CallFrame::headerSizeInRegisters;
<span class="line-modified">1878         int remappedArgumentStart = m_inlineStackTop-&gt;remapOperand(VirtualRegister(argumentStart)).offset();</span>

1879 
1880         LoadVarargsData* data = m_graph.m_loadVarargsData.add();
1881         data-&gt;start = VirtualRegister(remappedArgumentStart + 1);
1882         data-&gt;count = VirtualRegister(remappedRegisterOffset + CallFrameSlot::argumentCount);
1883         data-&gt;offset = argumentsOffset;
1884         data-&gt;limit = maxNumArguments;
1885         data-&gt;mandatoryMinimum = mandatoryMinimum;
1886 
1887         if (callOp == TailCallForwardVarargs)
1888             addToGraph(ForwardVarargs, OpInfo(data));
1889         else
1890             addToGraph(LoadVarargs, OpInfo(data), get(argumentsArgument));
1891 
1892         // LoadVarargs may OSR exit. Hence, we need to keep alive callTargetNode, thisArgument
1893         // and argumentsArgument for the baseline JIT. However, we only need a Phantom for
1894         // callTargetNode because the other 2 are still in use and alive at this point.
1895         addToGraph(Phantom, callTargetNode);
1896 
1897         // In DFG IR before SSA, we cannot insert control flow between after the
<span class="line-modified">1898         // LoadVarargs and the last SetArgumentDefinitely. This isn&#39;t a problem once we get to DFG</span>
1899         // SSA. Fortunately, we also have other reasons for not inserting control flow
1900         // before SSA.
1901 
1902         VariableAccessData* countVariable = newVariableAccessData(VirtualRegister(remappedRegisterOffset + CallFrameSlot::argumentCount));
1903         // This is pretty lame, but it will force the count to be flushed as an int. This doesn&#39;t
<span class="line-modified">1904         // matter very much, since our use of a SetArgumentDefinitely and Flushes for this local slot is</span>
1905         // mostly just a formality.
1906         countVariable-&gt;predict(SpecInt32Only);
1907         countVariable-&gt;mergeIsProfitableToUnbox(true);
<span class="line-modified">1908         Node* setArgumentCount = addToGraph(SetArgumentDefinitely, OpInfo(countVariable));</span>
1909         m_currentBlock-&gt;variablesAtTail.setOperand(countVariable-&gt;local(), setArgumentCount);
1910 
1911         set(VirtualRegister(argumentStart), get(thisArgument), ImmediateNakedSet);
<span class="line-added">1912         unsigned numSetArguments = 0;</span>
1913         for (unsigned argument = 1; argument &lt; maxNumArguments; ++argument) {
1914             VariableAccessData* variable = newVariableAccessData(VirtualRegister(remappedArgumentStart + argument));
1915             variable-&gt;mergeShouldNeverUnbox(true); // We currently have nowhere to put the type check on the LoadVarargs. LoadVarargs is effectful, so after it finishes, we cannot exit.
1916 
1917             // For a while it had been my intention to do things like this inside the
1918             // prediction injection phase. But in this case it&#39;s really best to do it here,
1919             // because it&#39;s here that we have access to the variable access datas for the
1920             // inlining we&#39;re about to do.
1921             //
1922             // Something else that&#39;s interesting here is that we&#39;d really love to get
1923             // predictions from the arguments loaded at the callsite, rather than the
1924             // arguments received inside the callee. But that probably won&#39;t matter for most
1925             // calls.
1926             if (codeBlock &amp;&amp; argument &lt; static_cast&lt;unsigned&gt;(codeBlock-&gt;numParameters())) {
1927                 ConcurrentJSLocker locker(codeBlock-&gt;m_lock);
1928                 ValueProfile&amp; profile = codeBlock-&gt;valueProfileForArgument(argument);
1929                 variable-&gt;predict(profile.computeUpdatedPrediction(locker));
1930             }
1931 
<span class="line-modified">1932             Node* setArgument = addToGraph(numSetArguments &gt;= mandatoryMinimum ? SetArgumentMaybe : SetArgumentDefinitely, OpInfo(variable));</span>
1933             m_currentBlock-&gt;variablesAtTail.setOperand(variable-&gt;local(), setArgument);
<span class="line-added">1934             ++numSetArguments;</span>
1935         }
1936     };
1937 
1938     // Intrinsics and internal functions can only be inlined if we&#39;re not doing varargs. This is because
1939     // we currently don&#39;t have any way of getting profiling information for arguments to non-JS varargs
1940     // calls. The prediction propagator won&#39;t be of any help because LoadVarargs obscures the data flow,
1941     // and there are no callsite value profiles and native function won&#39;t have callee value profiles for
1942     // those arguments. Even worse, if the intrinsic decides to exit, it won&#39;t really have anywhere to
1943     // exit to: LoadVarargs is effectful and it&#39;s part of the op_call_varargs, so we can&#39;t exit without
1944     // calling LoadVarargs twice.
1945     inlineCall(callTargetNode, result, callVariant, registerOffset, maxNumArguments, kind, nullptr, insertChecks);
1946 
<span class="line-added">1947 </span>
1948     VERBOSE_LOG(&quot;Successful inlining (varargs, monomorphic).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1949     return true;
1950 }
1951 
1952 unsigned ByteCodeParser::getInliningBalance(const CallLinkStatus&amp; callLinkStatus, CodeSpecializationKind specializationKind)
1953 {
<span class="line-modified">1954     unsigned inliningBalance = Options::maximumFunctionForCallInlineCandidateBytecodeCost();</span>
1955     if (specializationKind == CodeForConstruct)
<span class="line-modified">1956         inliningBalance = std::min(inliningBalance, Options::maximumFunctionForConstructInlineCandidateBytecoodeCost());</span>
1957     if (callLinkStatus.isClosureCall())
<span class="line-modified">1958         inliningBalance = std::min(inliningBalance, Options::maximumFunctionForClosureCallInlineCandidateBytecodeCost());</span>
1959     return inliningBalance;
1960 }
1961 
1962 ByteCodeParser::CallOptimizationResult ByteCodeParser::handleInlining(
1963     Node* callTargetNode, VirtualRegister result, const CallLinkStatus&amp; callLinkStatus,
1964     int registerOffset, VirtualRegister thisArgument,
1965     int argumentCountIncludingThis,
1966     unsigned nextOffset, NodeType callOp, InlineCallFrame::Kind kind, SpeculatedType prediction)
1967 {
1968     VERBOSE_LOG(&quot;Handling inlining...\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
1969 
1970     CodeSpecializationKind specializationKind = InlineCallFrame::specializationKindFor(kind);
1971     unsigned inliningBalance = getInliningBalance(callLinkStatus, specializationKind);
1972 
1973     // First check if we can avoid creating control flow. Our inliner does some CFG
1974     // simplification on the fly and this helps reduce compile times, but we can only leverage
1975     // this in cases where we don&#39;t need control flow diamonds to check the callee.
1976     if (!callLinkStatus.couldTakeSlowPath() &amp;&amp; callLinkStatus.size() == 1) {
1977         return handleCallVariant(
1978             callTargetNode, result, callLinkStatus[0], registerOffset, thisArgument,
</pre>
<hr />
<pre>
2093         VERBOSE_LOG(&quot;Finished optimizing &quot;, callLinkStatus[i], &quot; at &quot;, currentCodeOrigin(), &quot;.\n&quot;);
2094     }
2095 
2096     // Slow path block
2097     m_currentBlock = allocateUntargetableBlock();
2098     m_currentIndex = oldOffset;
2099     m_exitOK = true;
2100     data.fallThrough = BranchTarget(m_currentBlock);
2101     prepareToParseBlock();
2102     Node* myCallTargetNode = getDirect(calleeReg);
2103     if (couldTakeSlowPath) {
2104         addCall(
2105             result, callOp, nullptr, myCallTargetNode, argumentCountIncludingThis,
2106             registerOffset, prediction);
2107         VERBOSE_LOG(&quot;We added a call in the slow path\n&quot;);
2108     } else {
2109         addToGraph(CheckBadCell);
2110         addToGraph(Phantom, myCallTargetNode);
2111         emitArgumentPhantoms(registerOffset, argumentCountIncludingThis);
2112 
<span class="line-modified">2113         if (result.isValid())</span>
<span class="line-added">2114             set(result, addToGraph(BottomValue));</span>
2115         VERBOSE_LOG(&quot;couldTakeSlowPath was false\n&quot;);
2116     }
2117 
2118     m_currentIndex = nextOffset;
2119     m_exitOK = true; // Origin changed, so it&#39;s fine to exit again.
2120     processSetLocalQueue();
2121 
2122     if (Node* terminal = m_currentBlock-&gt;terminal())
2123         ASSERT_UNUSED(terminal, terminal-&gt;op() == TailCall || terminal-&gt;op() == TailCallVarargs || terminal-&gt;op() == TailCallForwardVarargs);
2124     else {
2125         addJumpTo(continuationBlock);
2126     }
2127 
2128     prepareToParseBlock();
2129 
2130     m_currentIndex = oldOffset;
2131     m_currentBlock = continuationBlock;
2132     m_exitOK = true;
2133 
2134     VERBOSE_LOG(&quot;Done inlining (hard).\nStack: &quot;, currentCodeOrigin(), &quot;\n&quot;);
</pre>
<hr />
<pre>
2333                 return false;
2334 
2335             ArrayMode arrayMode = getArrayMode(Array::Read);
2336             if (!arrayMode.isJSArray())
2337                 return false;
2338 
2339             if (!arrayMode.isJSArrayWithOriginalStructure())
2340                 return false;
2341 
2342             switch (arrayMode.type()) {
2343             case Array::Double:
2344             case Array::Int32:
2345             case Array::Contiguous: {
2346                 JSGlobalObject* globalObject = m_graph.globalObjectFor(currentNodeOrigin().semantic);
2347 
2348                 Structure* arrayPrototypeStructure = globalObject-&gt;arrayPrototype()-&gt;structure(*m_vm);
2349                 Structure* objectPrototypeStructure = globalObject-&gt;objectPrototype()-&gt;structure(*m_vm);
2350 
2351                 // FIXME: We could easily relax the Array/Object.prototype transition as long as we OSR exitted if we saw a hole.
2352                 // https://bugs.webkit.org/show_bug.cgi?id=173171
<span class="line-modified">2353                 if (globalObject-&gt;arraySpeciesWatchpointSet().state() == IsWatched</span>
2354                     &amp;&amp; globalObject-&gt;havingABadTimeWatchpoint()-&gt;isStillValid()
2355                     &amp;&amp; arrayPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2356                     &amp;&amp; objectPrototypeStructure-&gt;transitionWatchpointSetIsStillValid()
2357                     &amp;&amp; globalObject-&gt;arrayPrototypeChainIsSane()) {
2358 
<span class="line-modified">2359                     m_graph.watchpoints().addLazily(globalObject-&gt;arraySpeciesWatchpointSet());</span>
2360                     m_graph.watchpoints().addLazily(globalObject-&gt;havingABadTimeWatchpoint());
2361                     m_graph.registerAndWatchStructureTransition(arrayPrototypeStructure);
2362                     m_graph.registerAndWatchStructureTransition(objectPrototypeStructure);
2363 
2364                     insertChecks();
2365 
2366                     Node* array = get(virtualRegisterForArgument(0, registerOffset));
2367                     // We do a few things here to prove that we aren&#39;t skipping doing side-effects in an observable way:
2368                     // 1. We ensure that the &quot;constructor&quot; property hasn&#39;t been changed (because the observable
2369                     // effects of slice require that we perform a Get(array, &quot;constructor&quot;) and we can skip
2370                     // that if we&#39;re an original array structure. (We can relax this in the future by using
2371                     // TryGetById and CheckCell).
2372                     //
2373                     // 2. We check that the array we&#39;re calling slice on has the same global object as the lexical
2374                     // global object that this code is running in. This requirement is necessary because we setup the
2375                     // watchpoints above on the lexical global object. This means that code that calls slice on
2376                     // arrays produced by other global objects won&#39;t get this optimization. We could relax this
2377                     // requirement in the future by checking that the watchpoint hasn&#39;t fired at runtime in the code
2378                     // we generate instead of registering it as a watchpoint that would invalidate the compilation.
2379                     //
</pre>
<hr />
<pre>
3226                 byteSize = 4;
3227                 op = DataViewGetFloat;
3228                 break;
3229             case DataViewGetFloat64:
3230                 byteSize = 8;
3231                 op = DataViewGetFloat;
3232                 break;
3233             default:
3234                 RELEASE_ASSERT_NOT_REACHED();
3235             }
3236 
3237             TriState isLittleEndian = MixedTriState;
3238             Node* littleEndianChild = nullptr;
3239             if (byteSize &gt; 1) {
3240                 if (argumentCountIncludingThis &lt; 3)
3241                     isLittleEndian = FalseTriState;
3242                 else {
3243                     littleEndianChild = get(virtualRegisterForArgument(2, registerOffset));
3244                     if (littleEndianChild-&gt;hasConstant()) {
3245                         JSValue constant = littleEndianChild-&gt;constant()-&gt;value();
<span class="line-modified">3246                         if (constant) {</span>
<span class="line-modified">3247                             isLittleEndian = constant.pureToBoolean();</span>
<span class="line-modified">3248                             if (isLittleEndian != MixedTriState)</span>
<span class="line-added">3249                                 littleEndianChild = nullptr;</span>
<span class="line-added">3250                         }</span>
3251                     } else
3252                         isLittleEndian = MixedTriState;
3253                 }
3254             }
3255 
3256             DataViewData data { };
3257             data.isLittleEndian = isLittleEndian;
3258             data.isSigned = isSigned;
3259             data.byteSize = byteSize;
3260 
3261             setResult(
3262                 addToGraph(op, OpInfo(data.asQuadWord), OpInfo(prediction), get(virtualRegisterForArgument(0, registerOffset)), get(virtualRegisterForArgument(1, registerOffset)), littleEndianChild));
3263             return true;
3264         }
3265 
3266         case DataViewSetInt8:
3267         case DataViewSetUint8:
3268         case DataViewSetInt16:
3269         case DataViewSetUint16:
3270         case DataViewSetInt32:
</pre>
<hr />
<pre>
3311                 isFloatingPoint = true;
3312                 byteSize = 4;
3313                 break;
3314             case DataViewSetFloat64:
3315                 isFloatingPoint = true;
3316                 byteSize = 8;
3317                 break;
3318             default:
3319                 RELEASE_ASSERT_NOT_REACHED();
3320             }
3321 
3322             TriState isLittleEndian = MixedTriState;
3323             Node* littleEndianChild = nullptr;
3324             if (byteSize &gt; 1) {
3325                 if (argumentCountIncludingThis &lt; 4)
3326                     isLittleEndian = FalseTriState;
3327                 else {
3328                     littleEndianChild = get(virtualRegisterForArgument(3, registerOffset));
3329                     if (littleEndianChild-&gt;hasConstant()) {
3330                         JSValue constant = littleEndianChild-&gt;constant()-&gt;value();
<span class="line-modified">3331                         if (constant) {</span>
<span class="line-modified">3332                             isLittleEndian = constant.pureToBoolean();</span>
<span class="line-modified">3333                             if (isLittleEndian != MixedTriState)</span>
<span class="line-added">3334                                 littleEndianChild = nullptr;</span>
<span class="line-added">3335                         }</span>
3336                     } else
3337                         isLittleEndian = MixedTriState;
3338                 }
3339             }
3340 
3341             DataViewData data { };
3342             data.isLittleEndian = isLittleEndian;
3343             data.isSigned = isSigned;
3344             data.byteSize = byteSize;
3345             data.isFloatingPoint = isFloatingPoint;
3346 
3347             addVarArgChild(get(virtualRegisterForArgument(0, registerOffset)));
3348             addVarArgChild(get(virtualRegisterForArgument(1, registerOffset)));
3349             addVarArgChild(get(virtualRegisterForArgument(2, registerOffset)));
3350             addVarArgChild(littleEndianChild);
3351 
3352             addToGraph(Node::VarArg, DataViewSet, OpInfo(data.asQuadWord), OpInfo());
3353             setResult(addToGraph(JSConstant, OpInfo(m_constantUndefined)));
3354             return true;
3355         }
</pre>
<hr />
<pre>
3495         Array::Type arrayType = toArrayType(type);
3496         size_t logSize = logElementSize(type);
3497 
3498         variant.structureSet().forEach([&amp;] (Structure* structure) {
3499             TypedArrayType curType = structure-&gt;classInfo()-&gt;typedArrayStorageType;
3500             ASSERT(logSize == logElementSize(curType));
3501             arrayType = refineTypedArrayType(arrayType, curType);
3502             ASSERT(arrayType != Array::Generic);
3503         });
3504 
3505         Node* lengthNode = addToGraph(GetArrayLength, OpInfo(ArrayMode(arrayType, Array::Read).asWord()), thisNode);
3506 
3507         if (!logSize) {
3508             set(result, lengthNode);
3509             return true;
3510         }
3511 
3512         // We can use a BitLShift here because typed arrays will never have a byteLength
3513         // that overflows int32.
3514         Node* shiftNode = jsConstant(jsNumber(logSize));
<span class="line-modified">3515         set(result, addToGraph(ArithBitLShift, lengthNode, shiftNode));</span>
3516 
3517         return true;
3518     }
3519 
3520     case TypedArrayLengthIntrinsic: {
3521         insertChecks();
3522 
3523         TypedArrayType type = (*variant.structureSet().begin())-&gt;classInfo()-&gt;typedArrayStorageType;
3524         Array::Type arrayType = toArrayType(type);
3525 
3526         variant.structureSet().forEach([&amp;] (Structure* structure) {
3527             TypedArrayType curType = structure-&gt;classInfo()-&gt;typedArrayStorageType;
3528             arrayType = refineTypedArrayType(arrayType, curType);
3529             ASSERT(arrayType != Array::Generic);
3530         });
3531 
3532         set(result, addToGraph(GetArrayLength, OpInfo(ArrayMode(arrayType, Array::Read).asWord()), thisNode));
3533 
3534         return true;
3535 
</pre>
<hr />
<pre>
4406     int registerOffset = virtualRegisterForLocal(
4407         m_inlineStackTop-&gt;m_profiledBlock-&gt;numCalleeLocals() - 1).offset();
4408     registerOffset -= numberOfParameters;
4409     registerOffset -= CallFrame::headerSizeInRegisters;
4410 
4411     // Get the alignment right.
4412     registerOffset = -WTF::roundUpToMultipleOf(
4413         stackAlignmentRegisters(),
4414         -registerOffset);
4415 
4416     ensureLocals(
4417         m_inlineStackTop-&gt;remapOperand(
4418             VirtualRegister(registerOffset)).toLocal());
4419 
4420     // Issue SetLocals. This has two effects:
4421     // 1) That&#39;s how handleCall() sees the arguments.
4422     // 2) If we inline then this ensures that the arguments are flushed so that if you use
4423     //    the dreaded arguments object on the getter, the right things happen. Well, sort of -
4424     //    since we only really care about &#39;this&#39; in this case. But we&#39;re not going to take that
4425     //    shortcut.
<span class="line-modified">4426     set(virtualRegisterForArgument(0, registerOffset), base, ImmediateNakedSet);</span>

4427 
4428     // We&#39;ve set some locals, but they are not user-visible. It&#39;s still OK to exit from here.
4429     m_exitOK = true;
4430     addToGraph(ExitOK);
4431 
4432     handleCall(
4433         destination, Call, InlineCallFrame::GetterCall, instructionSize,
4434         getter, numberOfParameters - 1, registerOffset, *variant.callLinkStatus(), prediction);
4435 }
4436 
4437 void ByteCodeParser::emitPutById(
4438     Node* base, unsigned identifierNumber, Node* value, const PutByIdStatus&amp; putByIdStatus, bool isDirect)
4439 {
4440     if (isDirect)
4441         addToGraph(PutByIdDirect, OpInfo(identifierNumber), base, value);
4442     else
4443         addToGraph(putByIdStatus.makesCalls() ? PutByIdFlush : PutById, OpInfo(identifierNumber), base, value);
4444 }
4445 
4446 void ByteCodeParser::handlePutById(
</pre>
<hr />
<pre>
4589         unsigned numberOfParameters = 0;
4590         numberOfParameters++; // The &#39;this&#39; argument.
4591         numberOfParameters++; // The new value.
4592         numberOfParameters++; // True return PC.
4593 
4594         // Start with a register offset that corresponds to the last in-use register.
4595         int registerOffset = virtualRegisterForLocal(
4596             m_inlineStackTop-&gt;m_profiledBlock-&gt;numCalleeLocals() - 1).offset();
4597         registerOffset -= numberOfParameters;
4598         registerOffset -= CallFrame::headerSizeInRegisters;
4599 
4600         // Get the alignment right.
4601         registerOffset = -WTF::roundUpToMultipleOf(
4602             stackAlignmentRegisters(),
4603             -registerOffset);
4604 
4605         ensureLocals(
4606             m_inlineStackTop-&gt;remapOperand(
4607                 VirtualRegister(registerOffset)).toLocal());
4608 
<span class="line-modified">4609         set(virtualRegisterForArgument(0, registerOffset), base, ImmediateNakedSet);</span>
<span class="line-modified">4610         set(virtualRegisterForArgument(1, registerOffset), value, ImmediateNakedSet);</span>

4611 
4612         // We&#39;ve set some locals, but they are not user-visible. It&#39;s still OK to exit from here.
4613         m_exitOK = true;
4614         addToGraph(ExitOK);
4615 
4616         handleCall(
4617             VirtualRegister(), Call, InlineCallFrame::SetterCall,
4618             instructionSize, setter, numberOfParameters - 1, registerOffset,
4619             *variant.callLinkStatus(), SpecOther);
4620         return;
4621     }
4622 
4623     default: {
4624         emitPutById(base, identifierNumber, value, putByIdStatus, isDirect);
4625         return;
4626     } }
4627 }
4628 
4629 void ByteCodeParser::prepareToParseBlock()
4630 {
</pre>
<hr />
<pre>
4704             default: break; \
4705             } \
4706         } \
4707         LAST_OPCODE_LINKED(name); \
4708     } while (false)
4709 
4710 void ByteCodeParser::parseBlock(unsigned limit)
4711 {
4712     auto&amp; instructions = m_inlineStackTop-&gt;m_codeBlock-&gt;instructions();
4713     unsigned blockBegin = m_currentIndex;
4714 
4715     // If we are the first basic block, introduce markers for arguments. This allows
4716     // us to track if a use of an argument may use the actual argument passed, as
4717     // opposed to using a value we set explicitly.
4718     if (m_currentBlock == m_graph.block(0) &amp;&amp; !inlineCallFrame()) {
4719         auto addResult = m_graph.m_rootToArguments.add(m_currentBlock, ArgumentsVector());
4720         RELEASE_ASSERT(addResult.isNewEntry);
4721         ArgumentsVector&amp; entrypointArguments = addResult.iterator-&gt;value;
4722         entrypointArguments.resize(m_numArguments);
4723 
<span class="line-modified">4724         // We will emit SetArgumentDefinitely nodes. They don&#39;t exit, but we&#39;re at the top of an op_enter so</span>
4725         // exitOK = true.
4726         m_exitOK = true;
4727         for (unsigned argument = 0; argument &lt; m_numArguments; ++argument) {
4728             VariableAccessData* variable = newVariableAccessData(
4729                 virtualRegisterForArgument(argument));
4730             variable-&gt;mergeStructureCheckHoistingFailed(
4731                 m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache));
4732             variable-&gt;mergeCheckArrayHoistingFailed(
4733                 m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIndexingType));
4734 
<span class="line-modified">4735             Node* setArgument = addToGraph(SetArgumentDefinitely, OpInfo(variable));</span>
4736             entrypointArguments[argument] = setArgument;
4737             m_currentBlock-&gt;variablesAtTail.setArgumentFirstTime(argument, setArgument);
4738         }
4739     }
4740 
4741     CodeBlock* codeBlock = m_inlineStackTop-&gt;m_codeBlock;
4742 
4743     auto jumpTarget = [&amp;](int target) {
4744         if (target)
4745             return target;
4746         return codeBlock-&gt;outOfLineJumpOffset(m_currentInstruction);
4747     };
4748 
4749     while (true) {
4750         // We&#39;re staring at a new bytecode instruction. So we once again have a place that we can exit
4751         // to.
4752         m_exitOK = true;
4753 
4754         processSetLocalQueue();
4755 
</pre>
<hr />
<pre>
4768             return;
4769         }
4770 
4771         // Switch on the current bytecode opcode.
4772         const Instruction* currentInstruction = instructions.at(m_currentIndex).ptr();
4773         m_currentInstruction = currentInstruction; // Some methods want to use this, and we&#39;d rather not thread it through calls.
4774         OpcodeID opcodeID = currentInstruction-&gt;opcodeID();
4775 
4776         VERBOSE_LOG(&quot;    parsing &quot;, currentCodeOrigin(), &quot;: &quot;, opcodeID, &quot;\n&quot;);
4777 
4778         if (UNLIKELY(m_graph.compilation())) {
4779             addToGraph(CountExecution, OpInfo(m_graph.compilation()-&gt;executionCounterFor(
4780                 Profiler::OriginStack(*m_vm-&gt;m_perBytecodeProfiler, m_codeBlock, currentCodeOrigin()))));
4781         }
4782 
4783         switch (opcodeID) {
4784 
4785         // === Function entry opcodes ===
4786 
4787         case op_enter: {
<span class="line-added">4788             addToGraph(Options::usePollingTraps() ? CheckTraps : InvalidationPoint);</span>
4789             Node* undefined = addToGraph(JSConstant, OpInfo(m_constantUndefined));
4790             // Initialize all locals to undefined.
4791             for (int i = 0; i &lt; m_inlineStackTop-&gt;m_codeBlock-&gt;numVars(); ++i)
4792                 set(virtualRegisterForLocal(i), undefined, ImmediateNakedSet);

4793             NEXT_OPCODE(op_enter);
4794         }
4795 
4796         case op_to_this: {
4797             Node* op1 = getThis();
4798             auto&amp; metadata = currentInstruction-&gt;as&lt;OpToThis&gt;().metadata(codeBlock);
<span class="line-modified">4799             StructureID cachedStructureID = metadata.m_cachedStructureID;</span>
<span class="line-added">4800             Structure* cachedStructure = nullptr;</span>
<span class="line-added">4801             if (cachedStructureID)</span>
<span class="line-added">4802                 cachedStructure = m_vm-&gt;heap.structureIDTable().get(cachedStructureID);</span>
4803             if (metadata.m_toThisStatus != ToThisOK
4804                 || !cachedStructure
4805                 || cachedStructure-&gt;classInfo()-&gt;methodTable.toThis != JSObject::info()-&gt;methodTable.toThis
4806                 || m_inlineStackTop-&gt;m_profiledBlock-&gt;couldTakeSlowCase(m_currentIndex)
4807                 || m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCache)
4808                 || (op1-&gt;op() == GetLocal &amp;&amp; op1-&gt;variableAccessData()-&gt;structureCheckHoistingFailed())) {
4809                 setThis(addToGraph(ToThis, OpInfo(), OpInfo(getPrediction()), op1));
4810             } else {
4811                 addToGraph(
4812                     CheckStructure,
4813                     OpInfo(m_graph.addStructureSet(cachedStructure)),
4814                     op1);
4815             }
4816             NEXT_OPCODE(op_to_this);
4817         }
4818 
4819         case op_create_this: {
4820             auto bytecode = currentInstruction-&gt;as&lt;OpCreateThis&gt;();
4821             Node* callee = get(VirtualRegister(bytecode.m_callee));
4822 
</pre>
<hr />
<pre>
4869                     addToGraph(CreateThis, OpInfo(bytecode.m_inlineCapacity), callee));
4870             }
4871             NEXT_OPCODE(op_create_this);
4872         }
4873 
4874         case op_new_object: {
4875             auto bytecode = currentInstruction-&gt;as&lt;OpNewObject&gt;();
4876             set(bytecode.m_dst,
4877                 addToGraph(NewObject,
4878                     OpInfo(m_graph.registerStructure(bytecode.metadata(codeBlock).m_objectAllocationProfile.structure()))));
4879             NEXT_OPCODE(op_new_object);
4880         }
4881 
4882         case op_new_array: {
4883             auto bytecode = currentInstruction-&gt;as&lt;OpNewArray&gt;();
4884             int startOperand = bytecode.m_argv.offset();
4885             int numOperands = bytecode.m_argc;
4886             ArrayAllocationProfile&amp; profile = bytecode.metadata(codeBlock).m_arrayAllocationProfile;
4887             for (int operandIdx = startOperand; operandIdx &gt; startOperand - numOperands; --operandIdx)
4888                 addVarArgChild(get(VirtualRegister(operandIdx)));
<span class="line-modified">4889             unsigned vectorLengthHint = std::max&lt;unsigned&gt;(profile.vectorLengthHintConcurrently(), numOperands);</span>
<span class="line-modified">4890             set(bytecode.m_dst, addToGraph(Node::VarArg, NewArray, OpInfo(profile.selectIndexingTypeConcurrently()), OpInfo(vectorLengthHint)));</span>
4891             NEXT_OPCODE(op_new_array);
4892         }
4893 
4894         case op_new_array_with_spread: {
4895             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSpread&gt;();
4896             int startOperand = bytecode.m_argv.offset();
4897             int numOperands = bytecode.m_argc;
4898             const BitVector&amp; bitVector = m_inlineStackTop-&gt;m_profiledBlock-&gt;unlinkedCodeBlock()-&gt;bitVector(bytecode.m_bitVector);
4899             for (int operandIdx = startOperand; operandIdx &gt; startOperand - numOperands; --operandIdx)
4900                 addVarArgChild(get(VirtualRegister(operandIdx)));
4901 
4902             BitVector* copy = m_graph.m_bitVectors.add(bitVector);
4903             ASSERT(*copy == bitVector);
4904 
4905             set(bytecode.m_dst,
4906                 addToGraph(Node::VarArg, NewArrayWithSpread, OpInfo(copy)));
4907             NEXT_OPCODE(op_new_array_with_spread);
4908         }
4909 
4910         case op_spread: {
4911             auto bytecode = currentInstruction-&gt;as&lt;OpSpread&gt;();
4912             set(bytecode.m_dst,
4913                 addToGraph(Spread, get(bytecode.m_argument)));
4914             NEXT_OPCODE(op_spread);
4915         }
4916 
4917         case op_new_array_with_size: {
4918             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayWithSize&gt;();
4919             ArrayAllocationProfile&amp; profile = bytecode.metadata(codeBlock).m_arrayAllocationProfile;
<span class="line-modified">4920             set(bytecode.m_dst, addToGraph(NewArrayWithSize, OpInfo(profile.selectIndexingTypeConcurrently()), get(bytecode.m_length)));</span>
4921             NEXT_OPCODE(op_new_array_with_size);
4922         }
4923 
4924         case op_new_array_buffer: {
4925             auto bytecode = currentInstruction-&gt;as&lt;OpNewArrayBuffer&gt;();
4926             // Unfortunately, we can&#39;t allocate a new JSImmutableButterfly if the profile tells us new information because we
4927             // cannot allocate from compilation threads.
4928             WTF::loadLoadFence();
4929             FrozenValue* frozen = get(VirtualRegister(bytecode.m_immutableButterfly))-&gt;constant();
4930             WTF::loadLoadFence();
4931             JSImmutableButterfly* immutableButterfly = frozen-&gt;cast&lt;JSImmutableButterfly*&gt;();
4932             NewArrayBufferData data { };
4933             data.indexingMode = immutableButterfly-&gt;indexingMode();
4934             data.vectorLengthHint = immutableButterfly-&gt;toButterfly()-&gt;vectorLength();
4935 
4936             set(VirtualRegister(bytecode.m_dst), addToGraph(NewArrayBuffer, OpInfo(frozen), OpInfo(data.asQuadWord)));
4937             NEXT_OPCODE(op_new_array_buffer);
4938         }
4939 
4940         case op_new_regexp: {
</pre>
<hr />
<pre>
4960                 length = jsConstant(restLength);
4961             } else
4962                 length = addToGraph(GetRestLength, OpInfo(bytecode.m_numParametersToSkip));
4963             set(bytecode.m_dst, length);
4964             NEXT_OPCODE(op_get_rest_length);
4965         }
4966 
4967         case op_create_rest: {
4968             auto bytecode = currentInstruction-&gt;as&lt;OpCreateRest&gt;();
4969             noticeArgumentsUse();
4970             Node* arrayLength = get(bytecode.m_arraySize);
4971             set(bytecode.m_dst,
4972                 addToGraph(CreateRest, OpInfo(bytecode.m_numParametersToSkip), arrayLength));
4973             NEXT_OPCODE(op_create_rest);
4974         }
4975 
4976         // === Bitwise operations ===
4977 
4978         case op_bitnot: {
4979             auto bytecode = currentInstruction-&gt;as&lt;OpBitnot&gt;();
<span class="line-added">4980             SpeculatedType prediction = getPrediction();</span>
4981             Node* op1 = get(bytecode.m_operand);
<span class="line-modified">4982             if (op1-&gt;hasNumberOrAnyIntResult())</span>
<span class="line-added">4983                 set(bytecode.m_dst, addToGraph(ArithBitNot, op1));</span>
<span class="line-added">4984             else</span>
<span class="line-added">4985                 set(bytecode.m_dst, addToGraph(ValueBitNot, OpInfo(), OpInfo(prediction), op1));</span>
4986             NEXT_OPCODE(op_bitnot);
4987         }
4988 
4989         case op_bitand: {
4990             auto bytecode = currentInstruction-&gt;as&lt;OpBitand&gt;();
4991             SpeculatedType prediction = getPrediction();
4992             Node* op1 = get(bytecode.m_lhs);
4993             Node* op2 = get(bytecode.m_rhs);
4994             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
4995                 set(bytecode.m_dst, addToGraph(ArithBitAnd, op1, op2));
4996             else
4997                 set(bytecode.m_dst, addToGraph(ValueBitAnd, OpInfo(), OpInfo(prediction), op1, op2));
4998             NEXT_OPCODE(op_bitand);
4999         }
5000 
5001         case op_bitor: {
5002             auto bytecode = currentInstruction-&gt;as&lt;OpBitor&gt;();
5003             SpeculatedType prediction = getPrediction();
5004             Node* op1 = get(bytecode.m_lhs);
5005             Node* op2 = get(bytecode.m_rhs);
</pre>
<hr />
<pre>
5017             Node* op2 = get(bytecode.m_rhs);
5018             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())
5019                 set(bytecode.m_dst, addToGraph(ArithBitXor, op1, op2));
5020             else
5021                 set(bytecode.m_dst, addToGraph(ValueBitXor, OpInfo(), OpInfo(prediction), op1, op2));
5022             NEXT_OPCODE(op_bitxor);
5023         }
5024 
5025         case op_rshift: {
5026             auto bytecode = currentInstruction-&gt;as&lt;OpRshift&gt;();
5027             Node* op1 = get(bytecode.m_lhs);
5028             Node* op2 = get(bytecode.m_rhs);
5029             set(bytecode.m_dst, addToGraph(BitRShift, op1, op2));
5030             NEXT_OPCODE(op_rshift);
5031         }
5032 
5033         case op_lshift: {
5034             auto bytecode = currentInstruction-&gt;as&lt;OpLshift&gt;();
5035             Node* op1 = get(bytecode.m_lhs);
5036             Node* op2 = get(bytecode.m_rhs);
<span class="line-modified">5037             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())</span>
<span class="line-added">5038                 set(bytecode.m_dst, addToGraph(ArithBitLShift, op1, op2));</span>
<span class="line-added">5039             else {</span>
<span class="line-added">5040                 SpeculatedType prediction = getPredictionWithoutOSRExit();</span>
<span class="line-added">5041                 set(bytecode.m_dst, addToGraph(ValueBitLShift, OpInfo(), OpInfo(prediction), op1, op2));</span>
<span class="line-added">5042             }</span>
5043             NEXT_OPCODE(op_lshift);
5044         }
5045 
5046         case op_urshift: {
5047             auto bytecode = currentInstruction-&gt;as&lt;OpUrshift&gt;();
5048             Node* op1 = get(bytecode.m_lhs);
5049             Node* op2 = get(bytecode.m_rhs);
5050             set(bytecode.m_dst, addToGraph(BitURShift, op1, op2));
5051             NEXT_OPCODE(op_urshift);
5052         }
5053 
5054         case op_unsigned: {
5055             auto bytecode = currentInstruction-&gt;as&lt;OpUnsigned&gt;();
5056             set(bytecode.m_dst, makeSafe(addToGraph(UInt32ToNumber, get(bytecode.m_operand))));
5057             NEXT_OPCODE(op_unsigned);
5058         }
5059 
5060         // === Increment/Decrement opcodes ===
5061 
5062         case op_inc: {
</pre>
<hr />
<pre>
5106                 set(bytecode.m_dst, makeSafe(addToGraph(ValueNegate, op1)));
5107             NEXT_OPCODE(op_negate);
5108         }
5109 
5110         case op_mul: {
5111             // Multiply requires that the inputs are not truncated, unfortunately.
5112             auto bytecode = currentInstruction-&gt;as&lt;OpMul&gt;();
5113             Node* op1 = get(bytecode.m_lhs);
5114             Node* op2 = get(bytecode.m_rhs);
5115             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5116                 set(bytecode.m_dst, makeSafe(addToGraph(ArithMul, op1, op2)));
5117             else
5118                 set(bytecode.m_dst, makeSafe(addToGraph(ValueMul, op1, op2)));
5119             NEXT_OPCODE(op_mul);
5120         }
5121 
5122         case op_mod: {
5123             auto bytecode = currentInstruction-&gt;as&lt;OpMod&gt;();
5124             Node* op1 = get(bytecode.m_lhs);
5125             Node* op2 = get(bytecode.m_rhs);
<span class="line-modified">5126             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())</span>
<span class="line-added">5127                 set(bytecode.m_dst, makeSafe(addToGraph(ArithMod, op1, op2)));</span>
<span class="line-added">5128             else</span>
<span class="line-added">5129                 set(bytecode.m_dst, makeSafe(addToGraph(ValueMod, op1, op2)));</span>
5130             NEXT_OPCODE(op_mod);
5131         }
5132 
5133         case op_pow: {


5134             auto bytecode = currentInstruction-&gt;as&lt;OpPow&gt;();
5135             Node* op1 = get(bytecode.m_lhs);
5136             Node* op2 = get(bytecode.m_rhs);
<span class="line-modified">5137             if (op1-&gt;hasNumberOrAnyIntResult() &amp;&amp; op2-&gt;hasNumberOrAnyIntResult())</span>
<span class="line-added">5138                 set(bytecode.m_dst, addToGraph(ArithPow, op1, op2));</span>
<span class="line-added">5139             else</span>
<span class="line-added">5140                 set(bytecode.m_dst, addToGraph(ValuePow, op1, op2));</span>
5141             NEXT_OPCODE(op_pow);
5142         }
5143 
5144         case op_div: {
5145             auto bytecode = currentInstruction-&gt;as&lt;OpDiv&gt;();
5146             Node* op1 = get(bytecode.m_lhs);
5147             Node* op2 = get(bytecode.m_rhs);
5148             if (op1-&gt;hasNumberResult() &amp;&amp; op2-&gt;hasNumberResult())
5149                 set(bytecode.m_dst, makeDivSafe(addToGraph(ArithDiv, op1, op2)));
5150             else
5151                 set(bytecode.m_dst, makeDivSafe(addToGraph(ValueDiv, op1, op2)));
5152             NEXT_OPCODE(op_div);
5153         }
5154 
5155         // === Misc operations ===
5156 
5157         case op_debug: {
5158             // This is a nop in the DFG/FTL because when we set a breakpoint in the debugger,
5159             // we will jettison all optimized CodeBlocks that contains the breakpoint.
5160             addToGraph(Check); // We add a nop here so that basic block linking doesn&#39;t break.
</pre>
<hr />
<pre>
5447             Node* op2 = get(bytecode.m_rhs);
5448             Node* invertedResult;
5449             invertedResult = addToGraph(CompareStrictEq, op1, op2);
5450             set(bytecode.m_dst, addToGraph(LogicalNot, invertedResult));
5451             NEXT_OPCODE(op_nstricteq);
5452         }
5453 
5454         // === Property access operations ===
5455 
5456         case op_get_by_val: {
5457             auto bytecode = currentInstruction-&gt;as&lt;OpGetByVal&gt;();
5458             SpeculatedType prediction = getPredictionWithoutOSRExit();
5459 
5460             Node* base = get(bytecode.m_base);
5461             Node* property = get(bytecode.m_property);
5462             bool compiledAsGetById = false;
5463             GetByIdStatus getByIdStatus;
5464             unsigned identifierNumber = 0;
5465             {
5466                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
<span class="line-modified">5467                 ByValInfo* byValInfo = m_inlineStackTop-&gt;m_baselineMap.get(CodeOrigin(currentCodeOrigin().bytecodeIndex())).byValInfo;</span>
5468                 // FIXME: When the bytecode is not compiled in the baseline JIT, byValInfo becomes null.
5469                 // At that time, there is no information.
5470                 if (byValInfo
5471                     &amp;&amp; byValInfo-&gt;stubInfo
5472                     &amp;&amp; !byValInfo-&gt;tookSlowPath
5473                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIdent)
5474                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType)
5475                     &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
5476                     compiledAsGetById = true;
5477                     identifierNumber = m_graph.identifiers().ensure(byValInfo-&gt;cachedId.impl());
5478                     UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
5479 
5480                     if (Symbol* symbol = byValInfo-&gt;cachedSymbol.get()) {
5481                         FrozenValue* frozen = m_graph.freezeStrong(symbol);
5482                         addToGraph(CheckCell, OpInfo(frozen), property);
5483                     } else {
5484                         ASSERT(!uid-&gt;isSymbol());
5485                         addToGraph(CheckStringIdent, OpInfo(uid), property);
5486                     }
5487 
</pre>
<hr />
<pre>
5721         case op_jeq_null: {
5722             auto bytecode = currentInstruction-&gt;as&lt;OpJeqNull&gt;();
5723             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5724             Node* value = get(bytecode.m_value);
5725             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
5726             Node* condition = addToGraph(CompareEq, value, nullConstant);
5727             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5728             LAST_OPCODE(op_jeq_null);
5729         }
5730 
5731         case op_jneq_null: {
5732             auto bytecode = currentInstruction-&gt;as&lt;OpJneqNull&gt;();
5733             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5734             Node* value = get(bytecode.m_value);
5735             Node* nullConstant = addToGraph(JSConstant, OpInfo(m_constantNull));
5736             Node* condition = addToGraph(CompareEq, value, nullConstant);
5737             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);
5738             LAST_OPCODE(op_jneq_null);
5739         }
5740 
<span class="line-added">5741         case op_jundefined_or_null: {</span>
<span class="line-added">5742             auto bytecode = currentInstruction-&gt;as&lt;OpJundefinedOrNull&gt;();</span>
<span class="line-added">5743             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);</span>
<span class="line-added">5744             Node* value = get(bytecode.m_value);</span>
<span class="line-added">5745             Node* condition = addToGraph(IsUndefinedOrNull, value);</span>
<span class="line-added">5746             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);</span>
<span class="line-added">5747             LAST_OPCODE(op_jundefined_or_null);</span>
<span class="line-added">5748         }</span>
<span class="line-added">5749 </span>
<span class="line-added">5750         case op_jnundefined_or_null: {</span>
<span class="line-added">5751             auto bytecode = currentInstruction-&gt;as&lt;OpJnundefinedOrNull&gt;();</span>
<span class="line-added">5752             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);</span>
<span class="line-added">5753             Node* value = get(bytecode.m_value);</span>
<span class="line-added">5754             Node* condition = addToGraph(IsUndefinedOrNull, value);</span>
<span class="line-added">5755             addToGraph(Branch, OpInfo(branchData(m_currentIndex + currentInstruction-&gt;size(), m_currentIndex + relativeOffset)), condition);</span>
<span class="line-added">5756             LAST_OPCODE(op_jnundefined_or_null);</span>
<span class="line-added">5757         }</span>
<span class="line-added">5758 </span>
5759         case op_jless: {
5760             auto bytecode = currentInstruction-&gt;as&lt;OpJless&gt;();
5761             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5762             Node* op1 = get(bytecode.m_lhs);
5763             Node* op2 = get(bytecode.m_rhs);
5764             Node* condition = addToGraph(CompareLess, op1, op2);
5765             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5766             LAST_OPCODE(op_jless);
5767         }
5768 
5769         case op_jlesseq: {
5770             auto bytecode = currentInstruction-&gt;as&lt;OpJlesseq&gt;();
5771             unsigned relativeOffset = jumpTarget(bytecode.m_targetLabel);
5772             Node* op1 = get(bytecode.m_lhs);
5773             Node* op2 = get(bytecode.m_rhs);
5774             Node* condition = addToGraph(CompareLessEq, op1, op2);
5775             addToGraph(Branch, OpInfo(branchData(m_currentIndex + relativeOffset, m_currentIndex + currentInstruction-&gt;size())), condition);
5776             LAST_OPCODE(op_jlesseq);
5777         }
5778 
</pre>
<hr />
<pre>
6022 
6023             ValueProfileAndOperandBuffer* buffer = bytecode.metadata(codeBlock).m_buffer;
6024 
6025             if (!buffer) {
6026                 NEXT_OPCODE(op_catch); // This catch has yet to execute. Note: this load can be racy with the main thread.
6027             }
6028 
6029             // We&#39;re now committed to compiling this as an entrypoint.
6030             m_currentBlock-&gt;isCatchEntrypoint = true;
6031             m_graph.m_roots.append(m_currentBlock);
6032 
6033             Vector&lt;SpeculatedType&gt; argumentPredictions(m_numArguments);
6034             Vector&lt;SpeculatedType&gt; localPredictions;
6035             HashSet&lt;unsigned, WTF::IntHash&lt;unsigned&gt;, WTF::UnsignedWithZeroKeyHashTraits&lt;unsigned&gt;&gt; seenArguments;
6036 
6037             {
6038                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
6039 
6040                 buffer-&gt;forEach([&amp;] (ValueProfileAndOperand&amp; profile) {
6041                     VirtualRegister operand(profile.m_operand);
<span class="line-modified">6042                     SpeculatedType prediction = profile.computeUpdatedPrediction(locker);</span>
6043                     if (operand.isLocal())
6044                         localPredictions.append(prediction);
6045                     else {
6046                         RELEASE_ASSERT(operand.isArgument());
6047                         RELEASE_ASSERT(static_cast&lt;uint32_t&gt;(operand.toArgument()) &lt; argumentPredictions.size());
6048                         if (validationEnabled())
6049                             seenArguments.add(operand.toArgument());
6050                         argumentPredictions[operand.toArgument()] = prediction;
6051                     }
6052                 });
6053 
6054                 if (validationEnabled()) {
6055                     for (unsigned argument = 0; argument &lt; m_numArguments; ++argument)
6056                         RELEASE_ASSERT(seenArguments.contains(argument));
6057                 }
6058             }
6059 
6060             Vector&lt;std::pair&lt;VirtualRegister, Node*&gt;&gt; localsToSet;
6061             localsToSet.reserveInitialCapacity(buffer-&gt;m_size); // Note: This will reserve more than the number of locals we see below because the buffer includes arguments.
6062 
</pre>
<hr />
<pre>
6071                     return;
6072                 ASSERT(operand.isLocal());
6073                 Node* value = addToGraph(ExtractCatchLocal, OpInfo(numberOfLocals), OpInfo(localPredictions[numberOfLocals]));
6074                 ++numberOfLocals;
6075                 addToGraph(MovHint, OpInfo(profile.m_operand), value);
6076                 localsToSet.uncheckedAppend(std::make_pair(operand, value));
6077             });
6078             if (numberOfLocals)
6079                 addToGraph(ClearCatchLocals);
6080 
6081             if (!m_graph.m_maxLocalsForCatchOSREntry)
6082                 m_graph.m_maxLocalsForCatchOSREntry = 0;
6083             m_graph.m_maxLocalsForCatchOSREntry = std::max(numberOfLocals, *m_graph.m_maxLocalsForCatchOSREntry);
6084 
6085             // We could not exit before this point in the program because we would not know how to do value
6086             // recovery for live locals. The above IR sets up the necessary state so we can recover values
6087             // during OSR exit.
6088             //
6089             // The nodes that follow here all exit to the following bytecode instruction, not
6090             // the op_catch. Exiting to op_catch is reserved for when an exception is thrown.
<span class="line-modified">6091             // The SetArgumentDefinitely nodes that follow below may exit because we may hoist type checks</span>
6092             // to them. The SetLocal nodes that follow below may exit because we may choose
6093             // a flush format that speculates on the type of the local.
6094             m_exitOK = true;
6095             addToGraph(ExitOK);
6096 
6097             {
6098                 auto addResult = m_graph.m_rootToArguments.add(m_currentBlock, ArgumentsVector());
6099                 RELEASE_ASSERT(addResult.isNewEntry);
6100                 ArgumentsVector&amp; entrypointArguments = addResult.iterator-&gt;value;
6101                 entrypointArguments.resize(m_numArguments);
6102 
6103                 unsigned exitBytecodeIndex = m_currentIndex + currentInstruction-&gt;size();
6104 
6105                 for (unsigned argument = 0; argument &lt; argumentPredictions.size(); ++argument) {
6106                     VariableAccessData* variable = newVariableAccessData(virtualRegisterForArgument(argument));
6107                     variable-&gt;predict(argumentPredictions[argument]);
6108 
6109                     variable-&gt;mergeStructureCheckHoistingFailed(
6110                         m_inlineStackTop-&gt;m_exitProfile.hasExitSite(exitBytecodeIndex, BadCache));
6111                     variable-&gt;mergeCheckArrayHoistingFailed(
6112                         m_inlineStackTop-&gt;m_exitProfile.hasExitSite(exitBytecodeIndex, BadIndexingType));
6113 
<span class="line-modified">6114                     Node* setArgument = addToGraph(SetArgumentDefinitely, OpInfo(variable));</span>
<span class="line-modified">6115                     setArgument-&gt;origin.forExit = CodeOrigin(exitBytecodeIndex, setArgument-&gt;origin.forExit.inlineCallFrame());</span>
6116                     m_currentBlock-&gt;variablesAtTail.setArgumentFirstTime(argument, setArgument);
6117                     entrypointArguments[argument] = setArgument;
6118                 }
6119             }
6120 
6121             for (const std::pair&lt;VirtualRegister, Node*&gt;&amp; pair : localsToSet) {
6122                 DelayedSetLocal delayed { currentCodeOrigin(), pair.first, pair.second, ImmediateNakedSet };
6123                 m_setLocalQueue.append(delayed);
6124             }
6125 
6126             NEXT_OPCODE(op_catch);
6127         }
6128 
6129         case op_call:
6130             handleCall&lt;OpCall&gt;(currentInstruction, Call, CallMode::Regular);
6131             ASSERT_WITH_MESSAGE(m_currentInstruction == currentInstruction, &quot;handleCall, which may have inlined the callee, trashed m_currentInstruction&quot;);
6132             NEXT_OPCODE(op_call);
6133 
6134         case op_tail_call: {
6135             flushForReturn();
</pre>
<hr />
<pre>
6283                 RELEASE_ASSERT(constantScope == JSScope::constantScopeForCodeBlock(resolveType, m_inlineStackTop-&gt;m_codeBlock));
6284                 set(bytecode.m_dst, weakJSConstant(constantScope));
6285                 addToGraph(Phantom, get(bytecode.m_scope));
6286                 break;
6287             }
6288             case ModuleVar: {
6289                 // Since the value of the &quot;scope&quot; virtual register is not used in LLInt / baseline op_resolve_scope with ModuleVar,
6290                 // we need not to keep it alive by the Phantom node.
6291                 // Module environment is already strongly referenced by the CodeBlock.
6292                 set(bytecode.m_dst, weakJSConstant(lexicalEnvironment));
6293                 break;
6294             }
6295             case LocalClosureVar:
6296             case ClosureVar:
6297             case ClosureVarWithVarInjectionChecks: {
6298                 Node* localBase = get(bytecode.m_scope);
6299                 addToGraph(Phantom, localBase); // OSR exit cannot handle resolve_scope on a DCE&#39;d scope.
6300 
6301                 // We have various forms of constant folding here. This is necessary to avoid
6302                 // spurious recompiles in dead-but-foldable code.
<span class="line-added">6303 </span>
6304                 if (symbolTable) {
<span class="line-modified">6305                     if (JSScope* scope = symbolTable-&gt;singleton().inferredValue()) {</span>
<span class="line-modified">6306                         m_graph.watchpoints().addLazily(symbolTable);</span>
<span class="line-modified">6307                         set(bytecode.m_dst, weakJSConstant(scope));</span>

6308                         break;
6309                     }
6310                 }
6311                 if (JSScope* scope = localBase-&gt;dynamicCastConstant&lt;JSScope*&gt;(*m_vm)) {
6312                     for (unsigned n = depth; n--;)
6313                         scope = scope-&gt;next();
6314                     set(bytecode.m_dst, weakJSConstant(scope));
6315                     break;
6316                 }
6317                 for (unsigned n = depth; n--;)
6318                     localBase = addToGraph(SkipScope, localBase);
6319                 set(bytecode.m_dst, localBase);
6320                 break;
6321             }
6322             case UnresolvedProperty:
6323             case UnresolvedPropertyWithVarInjectionChecks: {
6324                 addToGraph(Phantom, get(bytecode.m_scope));
6325                 addToGraph(ForceOSRExit);
6326                 set(bytecode.m_dst, addToGraph(JSConstant, OpInfo(m_constantNull)));
6327                 break;
6328             }
6329             case Dynamic:
6330                 RELEASE_ASSERT_NOT_REACHED();
6331                 break;
6332             }
6333             NEXT_OPCODE(op_resolve_scope);
6334         }
6335         case op_resolve_scope_for_hoisting_func_decl_in_eval: {
6336             auto bytecode = currentInstruction-&gt;as&lt;OpResolveScopeForHoistingFuncDeclInEval&gt;();
6337             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_property];

6338             set(bytecode.m_dst, addToGraph(ResolveScopeForHoistingFuncDeclInEval, OpInfo(identifierNumber), get(bytecode.m_scope)));
6339 
6340             NEXT_OPCODE(op_resolve_scope_for_hoisting_func_decl_in_eval);
6341         }
6342 
6343         case op_get_from_scope: {
6344             auto bytecode = currentInstruction-&gt;as&lt;OpGetFromScope&gt;();
6345             auto&amp; metadata = bytecode.metadata(codeBlock);
6346             unsigned identifierNumber = m_inlineStackTop-&gt;m_identifierRemap[bytecode.m_var];
6347             UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
6348 
6349             ResolveType resolveType;
6350             GetPutInfo getPutInfo(0);
6351             Structure* structure = 0;
6352             WatchpointSet* watchpoints = 0;
6353             uintptr_t operand;
6354             {
6355                 ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
6356                 getPutInfo = metadata.m_getPutInfo;
6357                 resolveType = getPutInfo.resolveType();
</pre>
<hr />
<pre>
6629             case UnresolvedPropertyWithVarInjectionChecks:
6630                 RELEASE_ASSERT_NOT_REACHED();
6631                 break;
6632             }
6633             NEXT_OPCODE(op_put_to_scope);
6634         }
6635 
6636         case op_loop_hint: {
6637             // Baseline-&gt;DFG OSR jumps between loop hints. The DFG assumes that Baseline-&gt;DFG
6638             // OSR can only happen at basic block boundaries. Assert that these two statements
6639             // are compatible.
6640             RELEASE_ASSERT(m_currentIndex == blockBegin);
6641 
6642             // We never do OSR into an inlined code block. That could not happen, since OSR
6643             // looks up the code block that is the replacement for the baseline JIT code
6644             // block. Hence, machine code block = true code block = not inline code block.
6645             if (!m_inlineStackTop-&gt;m_caller)
6646                 m_currentBlock-&gt;isOSRTarget = true;
6647 
6648             addToGraph(LoopHint);




6649             addToGraph(Options::usePollingTraps() ? CheckTraps : InvalidationPoint);
<span class="line-modified">6650             NEXT_OPCODE(op_loop_hint);</span>
6651         }
6652 
6653         case op_nop: {
6654             addToGraph(Check); // We add a nop here so that basic block linking doesn&#39;t break.
6655             NEXT_OPCODE(op_nop);
6656         }
6657 
6658         case op_super_sampler_begin: {
6659             addToGraph(SuperSamplerBegin);
6660             NEXT_OPCODE(op_super_sampler_begin);
6661         }
6662 
6663         case op_super_sampler_end: {
6664             addToGraph(SuperSamplerEnd);
6665             NEXT_OPCODE(op_super_sampler_end);
6666         }
6667 
6668         case op_create_lexical_environment: {
6669             auto bytecode = currentInstruction-&gt;as&lt;OpCreateLexicalEnvironment&gt;();
6670             ASSERT(bytecode.m_symbolTable.isConstant() &amp;&amp; bytecode.m_initialValue.isConstant());
</pre>
<hr />
<pre>
7101         m_inlineCallFrame-&gt;argumentCountIncludingThis = argumentCountIncludingThis;
7102         if (callee) {
7103             m_inlineCallFrame-&gt;calleeRecovery = ValueRecovery::constant(callee);
7104             m_inlineCallFrame-&gt;isClosureCall = false;
7105         } else
7106             m_inlineCallFrame-&gt;isClosureCall = true;
7107         m_inlineCallFrame-&gt;directCaller = byteCodeParser-&gt;currentCodeOrigin();
7108         m_inlineCallFrame-&gt;argumentsWithFixup.resizeToFit(argumentCountIncludingThisWithFixup); // Set the number of arguments including this, but don&#39;t configure the value recoveries, yet.
7109         m_inlineCallFrame-&gt;kind = kind;
7110 
7111         m_identifierRemap.resize(codeBlock-&gt;numberOfIdentifiers());
7112         m_switchRemap.resize(codeBlock-&gt;numberOfSwitchJumpTables());
7113 
7114         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfIdentifiers(); ++i) {
7115             UniquedStringImpl* rep = codeBlock-&gt;identifier(i).impl();
7116             unsigned index = byteCodeParser-&gt;m_graph.identifiers().ensure(rep);
7117             m_identifierRemap[i] = index;
7118         }
7119         for (unsigned i = 0; i &lt; codeBlock-&gt;numberOfSwitchJumpTables(); ++i) {
7120             m_switchRemap[i] = byteCodeParser-&gt;m_codeBlock-&gt;numberOfSwitchJumpTables();
<span class="line-modified">7121             byteCodeParser-&gt;m_codeBlock-&gt;addSwitchJumpTableFromProfiledCodeBlock(codeBlock-&gt;switchJumpTable(i));</span>
7122         }
7123     } else {
7124         // Machine code block case.
7125         ASSERT(codeBlock == byteCodeParser-&gt;m_codeBlock);
7126         ASSERT(!callee);
7127         ASSERT(!returnValueVR.isValid());
7128         ASSERT(!inlineCallFrameStart.isValid());
7129 
7130         m_inlineCallFrame = 0;
7131 
7132         m_identifierRemap.resize(codeBlock-&gt;numberOfIdentifiers());
7133         m_switchRemap.resize(codeBlock-&gt;numberOfSwitchJumpTables());
7134         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfIdentifiers(); ++i)
7135             m_identifierRemap[i] = i;
7136         for (size_t i = 0; i &lt; codeBlock-&gt;numberOfSwitchJumpTables(); ++i)
7137             m_switchRemap[i] = i;
7138     }
7139 
7140     m_argumentPositions.resize(argumentCountIncludingThisWithFixup);
7141     for (int i = 0; i &lt; argumentCountIncludingThisWithFixup; ++i) {
</pre>
<hr />
<pre>
7152 {
7153     m_byteCodeParser-&gt;m_inlineStackTop = m_caller;
7154     RELEASE_ASSERT(m_byteCodeParser-&gt;m_icContextStack.last() == &amp;m_optimizedContext);
7155     m_byteCodeParser-&gt;m_icContextStack.removeLast();
7156 }
7157 
7158 void ByteCodeParser::parseCodeBlock()
7159 {
7160     clearCaches();
7161 
7162     CodeBlock* codeBlock = m_inlineStackTop-&gt;m_codeBlock;
7163 
7164     if (UNLIKELY(m_graph.compilation())) {
7165         m_graph.compilation()-&gt;addProfiledBytecodes(
7166             *m_vm-&gt;m_perBytecodeProfiler, m_inlineStackTop-&gt;m_profiledBlock);
7167     }
7168 
7169     if (UNLIKELY(Options::dumpSourceAtDFGTime())) {
7170         Vector&lt;DeferredSourceDump&gt;&amp; deferredSourceDump = m_graph.m_plan.callback()-&gt;ensureDeferredSourceDump();
7171         if (inlineCallFrame()) {
<span class="line-modified">7172             DeferredSourceDump dump(codeBlock-&gt;baselineVersion(), m_codeBlock, JITType::DFGJIT, inlineCallFrame()-&gt;directCaller.bytecodeIndex());</span>
7173             deferredSourceDump.append(dump);
7174         } else
7175             deferredSourceDump.append(DeferredSourceDump(codeBlock-&gt;baselineVersion()));
7176     }
7177 
7178     if (Options::dumpBytecodeAtDFGTime()) {
7179         dataLog(&quot;Parsing &quot;, *codeBlock);
7180         if (inlineCallFrame()) {
7181             dataLog(
<span class="line-modified">7182                 &quot; for inlining at &quot;, CodeBlockWithJITType(m_codeBlock, JITType::DFGJIT),</span>
7183                 &quot; &quot;, inlineCallFrame()-&gt;directCaller);
7184         }
7185         dataLog(
7186             &quot;, isStrictMode = &quot;, codeBlock-&gt;ownerExecutable()-&gt;isStrictMode(), &quot;\n&quot;);
7187         codeBlock-&gt;baselineVersion()-&gt;dumpBytecode();
7188     }
7189 
7190     Vector&lt;InstructionStream::Offset, 32&gt; jumpTargets;
7191     computePreciseJumpTargets(codeBlock, jumpTargets);
7192     if (Options::dumpBytecodeAtDFGTime()) {
7193         dataLog(&quot;Jump targets: &quot;);
7194         CommaPrinter comma;
7195         for (unsigned i = 0; i &lt; jumpTargets.size(); ++i)
7196             dataLog(comma, jumpTargets[i]);
7197         dataLog(&quot;\n&quot;);
7198     }
7199 
7200     for (unsigned jumpTargetIndex = 0; jumpTargetIndex &lt;= jumpTargets.size(); ++jumpTargetIndex) {
7201         // The maximum bytecode offset to go into the current basicblock is either the next jump target, or the end of the instructions.
7202         unsigned limit = jumpTargetIndex &lt; jumpTargets.size() ? jumpTargets[jumpTargetIndex] : codeBlock-&gt;instructions().size();
</pre>
<hr />
<pre>
7240 
7241     // Should have reached the end of the instructions.
7242     ASSERT(m_currentIndex == codeBlock-&gt;instructions().size());
7243 
7244     VERBOSE_LOG(&quot;Done parsing &quot;, *codeBlock, &quot; (fell off end)\n&quot;);
7245 }
7246 
7247 template &lt;typename Bytecode&gt;
7248 void ByteCodeParser::handlePutByVal(Bytecode bytecode, unsigned instructionSize)
7249 {
7250     Node* base = get(bytecode.m_base);
7251     Node* property = get(bytecode.m_property);
7252     Node* value = get(bytecode.m_value);
7253     bool isDirect = Bytecode::opcodeID == op_put_by_val_direct;
7254     bool compiledAsPutById = false;
7255     {
7256         unsigned identifierNumber = std::numeric_limits&lt;unsigned&gt;::max();
7257         PutByIdStatus putByIdStatus;
7258         {
7259             ConcurrentJSLocker locker(m_inlineStackTop-&gt;m_profiledBlock-&gt;m_lock);
<span class="line-modified">7260             ByValInfo* byValInfo = m_inlineStackTop-&gt;m_baselineMap.get(CodeOrigin(currentCodeOrigin().bytecodeIndex())).byValInfo;</span>
7261             // FIXME: When the bytecode is not compiled in the baseline JIT, byValInfo becomes null.
7262             // At that time, there is no information.
7263             if (byValInfo
7264                 &amp;&amp; byValInfo-&gt;stubInfo
7265                 &amp;&amp; !byValInfo-&gt;tookSlowPath
7266                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadIdent)
7267                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadType)
7268                 &amp;&amp; !m_inlineStackTop-&gt;m_exitProfile.hasExitSite(m_currentIndex, BadCell)) {
7269                 compiledAsPutById = true;
7270                 identifierNumber = m_graph.identifiers().ensure(byValInfo-&gt;cachedId.impl());
7271                 UniquedStringImpl* uid = m_graph.identifiers()[identifierNumber];
7272 
7273                 if (Symbol* symbol = byValInfo-&gt;cachedSymbol.get()) {
7274                     FrozenValue* frozen = m_graph.freezeStrong(symbol);
7275                     addToGraph(CheckCell, OpInfo(frozen), property);
7276                 } else {
7277                     ASSERT(!uid-&gt;isSymbol());
7278                     addToGraph(CheckStringIdent, OpInfo(uid), property);
7279                 }
7280 
</pre>
<hr />
<pre>
7394             } while (changed);
7395         }
7396 
7397         InsertionSet insertionSet(m_graph);
7398         Operands&lt;VariableAccessData*&gt; mapping(OperandsLike, m_graph.block(0)-&gt;variablesAtHead);
7399 
7400         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
7401             if (blocksToIgnore.contains(block))
7402                 continue;
7403 
7404             mapping.fill(nullptr);
7405             if (validationEnabled()) {
7406                 // Verify that it&#39;s correct to fill mapping with nullptr.
7407                 for (unsigned i = 0; i &lt; block-&gt;variablesAtHead.size(); ++i) {
7408                     Node* node = block-&gt;variablesAtHead.at(i);
7409                     RELEASE_ASSERT(!node);
7410                 }
7411             }
7412 
7413             for (unsigned nodeIndex = 0; nodeIndex &lt; block-&gt;size(); ++nodeIndex) {
<span class="line-modified">7414                 {</span>
<span class="line-added">7415                     Node* node = block-&gt;at(nodeIndex);</span>
<span class="line-added">7416 </span>
<span class="line-added">7417                     if (node-&gt;hasVariableAccessData(m_graph))</span>
<span class="line-added">7418                         mapping.operand(node-&gt;local()) = node-&gt;variableAccessData();</span>
7419 
<span class="line-modified">7420                     if (node-&gt;op() != ForceOSRExit)</span>
<span class="line-modified">7421                         continue;</span>
<span class="line-added">7422                 }</span>
<span class="line-added">7423 </span>
<span class="line-added">7424                 NodeOrigin origin = block-&gt;at(nodeIndex)-&gt;origin;</span>
<span class="line-added">7425                 RELEASE_ASSERT(origin.exitOK);</span>
7426 
<span class="line-modified">7427                 ++nodeIndex;</span>

7428 
<span class="line-added">7429                 {</span>
7430                     if (validationEnabled()) {
7431                         // This verifies that we don&#39;t need to change any of the successors&#39;s predecessor
7432                         // list after planting the Unreachable below. At this point in the bytecode
7433                         // parser, we haven&#39;t linked up the predecessor lists yet.
7434                         for (BasicBlock* successor : block-&gt;successors())
7435                             RELEASE_ASSERT(successor-&gt;predecessors.isEmpty());
7436                     }
7437 




7438                     auto insertLivenessPreservingOp = [&amp;] (InlineCallFrame* inlineCallFrame, NodeType op, VirtualRegister operand) {
7439                         VariableAccessData* variable = mapping.operand(operand);
7440                         if (!variable) {
7441                             variable = newVariableAccessData(operand);
7442                             mapping.operand(operand) = variable;
7443                         }
7444 
7445                         VirtualRegister argument = operand - (inlineCallFrame ? inlineCallFrame-&gt;stackOffset : 0);
7446                         if (argument.isArgument() &amp;&amp; !argument.isHeader()) {
7447                             const Vector&lt;ArgumentPosition*&gt;&amp; arguments = m_inlineCallFrameToArgumentPositions.get(inlineCallFrame);
7448                             arguments[argument.toArgument()]-&gt;addVariable(variable);
<span class="line-modified">7449                         }</span>
<span class="line-added">7450                         insertionSet.insertNode(nodeIndex, SpecNone, op, origin, OpInfo(variable));</span>
7451                     };
7452                     auto addFlushDirect = [&amp;] (InlineCallFrame* inlineCallFrame, VirtualRegister operand) {
7453                         insertLivenessPreservingOp(inlineCallFrame, Flush, operand);
7454                     };
7455                     auto addPhantomLocalDirect = [&amp;] (InlineCallFrame* inlineCallFrame, VirtualRegister operand) {
7456                         insertLivenessPreservingOp(inlineCallFrame, PhantomLocal, operand);
7457                     };
<span class="line-modified">7458                     flushForTerminalImpl(origin.semantic, addFlushDirect, addPhantomLocalDirect);</span>
<span class="line-added">7459                 }</span>
7460 
<span class="line-modified">7461                 while (true) {</span>
<span class="line-modified">7462                     RELEASE_ASSERT(nodeIndex &lt; block-&gt;size());</span>
<span class="line-modified">7463 </span>
<span class="line-added">7464                     Node* node = block-&gt;at(nodeIndex);</span>
<span class="line-added">7465 </span>
<span class="line-added">7466                     node-&gt;origin = origin;</span>
<span class="line-added">7467                     m_graph.doToChildren(node, [&amp;] (Edge edge) {</span>
<span class="line-added">7468                         // We only need to keep data flow edges to nodes defined prior to the ForceOSRExit. The reason</span>
<span class="line-added">7469                         // for this is we rely on backwards propagation being able to see the &quot;full&quot; bytecode. To model</span>
<span class="line-added">7470                         // this, we preserve uses of a node in a generic way so that backwards propagation can reason</span>
<span class="line-added">7471                         // about them. Therefore, we can&#39;t remove uses of a node which is defined before the ForceOSRExit</span>
<span class="line-added">7472                         // even when we&#39;re at a point in the program after the ForceOSRExit, because that would break backwards</span>
<span class="line-added">7473                         // propagation&#39;s analysis over the uses of a node. However, we don&#39;t need this same preservation for</span>
<span class="line-added">7474                         // nodes defined after ForceOSRExit, as we&#39;ve already exitted before those defs.</span>
<span class="line-added">7475                         if (edge-&gt;hasResult())</span>
<span class="line-added">7476                             insertionSet.insertNode(nodeIndex, SpecNone, Phantom, origin, Edge(edge.node(), UntypedUse));</span>
<span class="line-added">7477                     });</span>
<span class="line-added">7478 </span>
<span class="line-added">7479                     bool isTerminal = node-&gt;isTerminal();</span>
<span class="line-added">7480 </span>
<span class="line-added">7481                     node-&gt;removeWithoutChecks();</span>
<span class="line-added">7482 </span>
<span class="line-added">7483                     if (isTerminal) {</span>
<span class="line-added">7484                         insertionSet.insertNode(nodeIndex, SpecNone, Unreachable, origin);</span>
<span class="line-added">7485                         break;</span>
<span class="line-added">7486                     }</span>
<span class="line-added">7487 </span>
<span class="line-added">7488                     ++nodeIndex;</span>
7489                 }
<span class="line-added">7490 </span>
<span class="line-added">7491                 insertionSet.execute(block);</span>
<span class="line-added">7492 </span>
<span class="line-added">7493                 auto nodeAndIndex = block-&gt;findTerminal();</span>
<span class="line-added">7494                 RELEASE_ASSERT(nodeAndIndex.node-&gt;op() == Unreachable);</span>
<span class="line-added">7495                 block-&gt;resize(nodeAndIndex.index + 1);</span>
<span class="line-added">7496                 break;</span>
7497             }
7498         }
7499     } else if (validationEnabled()) {
7500         // Ensure our bookkeeping for ForceOSRExit nodes is working.
7501         for (BasicBlock* block : m_graph.blocksInNaturalOrder()) {
7502             for (Node* node : *block)
7503                 RELEASE_ASSERT(node-&gt;op() != ForceOSRExit);
7504         }
7505     }
7506 
7507     m_graph.determineReachability();
7508     m_graph.killUnreachableBlocks();
7509 
7510     for (BlockIndex blockIndex = m_graph.numBlocks(); blockIndex--;) {
7511         BasicBlock* block = m_graph.block(blockIndex);
7512         if (!block)
7513             continue;
7514         ASSERT(block-&gt;variablesAtHead.numberOfLocals() == m_graph.block(0)-&gt;variablesAtHead.numberOfLocals());
7515         ASSERT(block-&gt;variablesAtHead.numberOfArguments() == m_graph.block(0)-&gt;variablesAtHead.numberOfArguments());
7516         ASSERT(block-&gt;variablesAtTail.numberOfLocals() == m_graph.block(0)-&gt;variablesAtHead.numberOfLocals());
</pre>
</td>
</tr>
</table>
<center><a href="DFGBasicBlock.h.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="DFGCFAPhase.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>