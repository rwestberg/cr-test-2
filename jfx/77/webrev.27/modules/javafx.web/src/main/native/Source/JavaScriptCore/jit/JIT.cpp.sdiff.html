<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="IntrinsicEmitter.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="JIT.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/JavaScriptCore/jit/JIT.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (C) 2008-2018 Apple Inc. All rights reserved.</span>
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
</pre>
<hr />
<pre>
  57 #include &lt;wtf/SimpleStats.h&gt;
  58 
  59 namespace JSC {
  60 namespace JITInternal {
  61 static constexpr const bool verbose = false;
  62 }
  63 
  64 Seconds totalBaselineCompileTime;
  65 Seconds totalDFGCompileTime;
  66 Seconds totalFTLCompileTime;
  67 Seconds totalFTLDFGCompileTime;
  68 Seconds totalFTLB3CompileTime;
  69 
  70 void ctiPatchCallByReturnAddress(ReturnAddressPtr returnAddress, FunctionPtr&lt;CFunctionPtrTag&gt; newCalleeFunction)
  71 {
  72     MacroAssembler::repatchCall(
  73         CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)),
  74         newCalleeFunction.retagged&lt;OperationPtrTag&gt;());
  75 }
  76 
<span class="line-modified">  77 JIT::JIT(VM* vm, CodeBlock* codeBlock, unsigned loopOSREntryBytecodeOffset)</span>
<span class="line-modified">  78     : JSInterfaceJIT(vm, codeBlock)</span>
<span class="line-modified">  79     , m_interpreter(vm-&gt;interpreter)</span>
  80     , m_labels(codeBlock ? codeBlock-&gt;instructions().size() : 0)
  81     , m_bytecodeOffset(std::numeric_limits&lt;unsigned&gt;::max())
<span class="line-modified">  82     , m_pcToCodeOriginMapBuilder(*vm)</span>
  83     , m_canBeOptimized(false)
  84     , m_shouldEmitProfiling(false)
  85     , m_shouldUseIndexMasking(Options::enableSpectreMitigations())
  86     , m_loopOSREntryBytecodeOffset(loopOSREntryBytecodeOffset)
  87 {
  88 }
  89 
  90 JIT::~JIT()
  91 {
  92 }
  93 
<span class="line-removed">  94 #if ENABLE(DFG_JIT)</span>
<span class="line-removed">  95 void JIT::emitEnterOptimizationCheck()</span>
<span class="line-removed">  96 {</span>
<span class="line-removed">  97     if (!canBeOptimized())</span>
<span class="line-removed">  98         return;</span>
<span class="line-removed">  99 </span>
<span class="line-removed"> 100     JumpList skipOptimize;</span>
<span class="line-removed"> 101 </span>
<span class="line-removed"> 102     skipOptimize.append(branchAdd32(Signed, TrustedImm32(Options::executionCounterIncrementForEntry()), AbsoluteAddress(m_codeBlock-&gt;addressOfJITExecuteCounter())));</span>
<span class="line-removed"> 103     ASSERT(!m_bytecodeOffset);</span>
<span class="line-removed"> 104 </span>
<span class="line-removed"> 105     copyCalleeSavesFromFrameOrRegisterToEntryFrameCalleeSavesBuffer(vm()-&gt;topEntryFrame);</span>
<span class="line-removed"> 106 </span>
<span class="line-removed"> 107     callOperation(operationOptimize, m_bytecodeOffset);</span>
<span class="line-removed"> 108     skipOptimize.append(branchTestPtr(Zero, returnValueGPR));</span>
<span class="line-removed"> 109     jump(returnValueGPR, GPRInfo::callFrameRegister);</span>
<span class="line-removed"> 110     skipOptimize.link(this);</span>
<span class="line-removed"> 111 }</span>
<span class="line-removed"> 112 #endif</span>
<span class="line-removed"> 113 </span>
 114 void JIT::emitNotifyWrite(WatchpointSet* set)
 115 {
 116     if (!set || set-&gt;state() == IsInvalidated) {
 117         addSlowCase(Jump());
 118         return;
 119     }
 120 
 121     addSlowCase(branch8(NotEqual, AbsoluteAddress(set-&gt;addressOfState()), TrustedImm32(IsInvalidated)));
 122 }
 123 
 124 void JIT::emitNotifyWrite(GPRReg pointerToSet)
 125 {
 126     addSlowCase(branch8(NotEqual, Address(pointerToSet, WatchpointSet::offsetOfState()), TrustedImm32(IsInvalidated)));
 127 }
 128 
 129 void JIT::assertStackPointerOffset()
 130 {
 131     if (ASSERT_DISABLED)
 132         return;
 133 
</pre>
<hr />
<pre>
 174 {
 175     linkAllSlowCases(iter);
 176 
 177     JITSlowPathCall slowPathCall(this, currentInstruction, stub);
 178     slowPathCall.call();
 179 }
 180 
 181 void JIT::privateCompileMainPass()
 182 {
 183     if (JITInternal::verbose)
 184         dataLog(&quot;Compiling &quot;, *m_codeBlock, &quot;\n&quot;);
 185 
 186     jitAssertTagsInPlace();
 187     jitAssertArgumentCountSane();
 188 
 189     auto&amp; instructions = m_codeBlock-&gt;instructions();
 190     unsigned instructionCount = m_codeBlock-&gt;instructions().size();
 191 
 192     m_callLinkInfoIndex = 0;
 193 
<span class="line-modified"> 194     VM&amp; vm = *m_codeBlock-&gt;vm();</span>
 195     unsigned startBytecodeOffset = 0;
 196     if (m_loopOSREntryBytecodeOffset &amp;&amp; (m_codeBlock-&gt;inherits&lt;ProgramCodeBlock&gt;(vm) || m_codeBlock-&gt;inherits&lt;ModuleProgramCodeBlock&gt;(vm))) {
 197         // We can only do this optimization because we execute ProgramCodeBlock&#39;s exactly once.
 198         // This optimization would be invalid otherwise. When the LLInt determines it wants to
 199         // do OSR entry into the baseline JIT in a loop, it will pass in the bytecode offset it
 200         // was executing at when it kicked off our compilation. We only need to compile code for
 201         // anything reachable from that bytecode offset.
 202 
 203         // We only bother building the bytecode graph if it could save time and executable
 204         // memory. We pick an arbitrary offset where we deem this is profitable.
 205         if (m_loopOSREntryBytecodeOffset &gt;= 200) {
 206             // As a simplification, we don&#39;t find all bytecode ranges that are unreachable.
 207             // Instead, we just find the minimum bytecode offset that is reachable, and
 208             // compile code from that bytecode offset onwards.
 209 
 210             BytecodeGraph graph(m_codeBlock, m_codeBlock-&gt;instructions());
 211             BytecodeBasicBlock* block = graph.findBasicBlockForBytecodeOffset(m_loopOSREntryBytecodeOffset);
 212             RELEASE_ASSERT(block);
 213 
 214             GraphNodeWorklist&lt;BytecodeBasicBlock*&gt; worklist;
</pre>
<hr />
<pre>
 347         DEFINE_OP(op_try_get_by_id)
 348         DEFINE_OP(op_in_by_id)
 349         DEFINE_OP(op_get_by_id)
 350         DEFINE_OP(op_get_by_id_with_this)
 351         DEFINE_OP(op_get_by_id_direct)
 352         DEFINE_OP(op_get_by_val)
 353         DEFINE_OP(op_overrides_has_instance)
 354         DEFINE_OP(op_instanceof)
 355         DEFINE_OP(op_instanceof_custom)
 356         DEFINE_OP(op_is_empty)
 357         DEFINE_OP(op_is_undefined)
 358         DEFINE_OP(op_is_undefined_or_null)
 359         DEFINE_OP(op_is_boolean)
 360         DEFINE_OP(op_is_number)
 361         DEFINE_OP(op_is_object)
 362         DEFINE_OP(op_is_cell_with_type)
 363         DEFINE_OP(op_jeq_null)
 364         DEFINE_OP(op_jfalse)
 365         DEFINE_OP(op_jmp)
 366         DEFINE_OP(op_jneq_null)


 367         DEFINE_OP(op_jneq_ptr)
 368         DEFINE_OP(op_jless)
 369         DEFINE_OP(op_jlesseq)
 370         DEFINE_OP(op_jgreater)
 371         DEFINE_OP(op_jgreatereq)
 372         DEFINE_OP(op_jnless)
 373         DEFINE_OP(op_jnlesseq)
 374         DEFINE_OP(op_jngreater)
 375         DEFINE_OP(op_jngreatereq)
 376         DEFINE_OP(op_jeq)
 377         DEFINE_OP(op_jneq)
 378         DEFINE_OP(op_jstricteq)
 379         DEFINE_OP(op_jnstricteq)
 380         DEFINE_OP(op_jbelow)
 381         DEFINE_OP(op_jbeloweq)
 382         DEFINE_OP(op_jtrue)
 383         DEFINE_OP(op_loop_hint)
<span class="line-removed"> 384         DEFINE_OP(op_check_traps)</span>
 385         DEFINE_OP(op_nop)
 386         DEFINE_OP(op_super_sampler_begin)
 387         DEFINE_OP(op_super_sampler_end)
 388         DEFINE_OP(op_lshift)
 389         DEFINE_OP(op_mod)
 390         DEFINE_OP(op_mov)
 391         DEFINE_OP(op_mul)
 392         DEFINE_OP(op_negate)
 393         DEFINE_OP(op_neq)
 394         DEFINE_OP(op_neq_null)
 395         DEFINE_OP(op_new_array)
 396         DEFINE_OP(op_new_array_with_size)
 397         DEFINE_OP(op_new_func)
 398         DEFINE_OP(op_new_func_exp)
 399         DEFINE_OP(op_new_generator_func)
 400         DEFINE_OP(op_new_generator_func_exp)
 401         DEFINE_OP(op_new_async_func)
 402         DEFINE_OP(op_new_async_func_exp)
 403         DEFINE_OP(op_new_async_generator_func)
 404         DEFINE_OP(op_new_async_generator_func_exp)
</pre>
<hr />
<pre>
 529         DEFINE_SLOWCASE_OP(op_in_by_id)
 530         DEFINE_SLOWCASE_OP(op_get_by_id)
 531         DEFINE_SLOWCASE_OP(op_get_by_id_with_this)
 532         DEFINE_SLOWCASE_OP(op_get_by_id_direct)
 533         DEFINE_SLOWCASE_OP(op_get_by_val)
 534         DEFINE_SLOWCASE_OP(op_instanceof)
 535         DEFINE_SLOWCASE_OP(op_instanceof_custom)
 536         DEFINE_SLOWCASE_OP(op_jless)
 537         DEFINE_SLOWCASE_OP(op_jlesseq)
 538         DEFINE_SLOWCASE_OP(op_jgreater)
 539         DEFINE_SLOWCASE_OP(op_jgreatereq)
 540         DEFINE_SLOWCASE_OP(op_jnless)
 541         DEFINE_SLOWCASE_OP(op_jnlesseq)
 542         DEFINE_SLOWCASE_OP(op_jngreater)
 543         DEFINE_SLOWCASE_OP(op_jngreatereq)
 544         DEFINE_SLOWCASE_OP(op_jeq)
 545         DEFINE_SLOWCASE_OP(op_jneq)
 546         DEFINE_SLOWCASE_OP(op_jstricteq)
 547         DEFINE_SLOWCASE_OP(op_jnstricteq)
 548         DEFINE_SLOWCASE_OP(op_loop_hint)
<span class="line-modified"> 549         DEFINE_SLOWCASE_OP(op_check_traps)</span>
 550         DEFINE_SLOWCASE_OP(op_mod)
 551         DEFINE_SLOWCASE_OP(op_mul)
 552         DEFINE_SLOWCASE_OP(op_negate)
 553         DEFINE_SLOWCASE_OP(op_neq)
 554         DEFINE_SLOWCASE_OP(op_new_object)
 555         DEFINE_SLOWCASE_OP(op_put_by_id)
 556         case op_put_by_val_direct:
 557         DEFINE_SLOWCASE_OP(op_put_by_val)
 558         DEFINE_SLOWCASE_OP(op_sub)
 559         DEFINE_SLOWCASE_OP(op_has_indexed_property)
 560         DEFINE_SLOWCASE_OP(op_get_from_scope)
 561         DEFINE_SLOWCASE_OP(op_put_to_scope)
 562 
 563         DEFINE_SLOWCASE_SLOW_OP(unsigned)
 564         DEFINE_SLOWCASE_SLOW_OP(inc)
 565         DEFINE_SLOWCASE_SLOW_OP(dec)
 566         DEFINE_SLOWCASE_SLOW_OP(bitnot)
 567         DEFINE_SLOWCASE_SLOW_OP(bitand)
 568         DEFINE_SLOWCASE_SLOW_OP(bitor)
 569         DEFINE_SLOWCASE_SLOW_OP(bitxor)
</pre>
<hr />
<pre>
 635         break;
 636     default:
 637         RELEASE_ASSERT_NOT_REACHED();
 638         break;
 639     }
 640 
 641     switch (m_codeBlock-&gt;codeType()) {
 642     case GlobalCode:
 643     case ModuleCode:
 644     case EvalCode:
 645         m_codeBlock-&gt;m_shouldAlwaysBeInlined = false;
 646         break;
 647     case FunctionCode:
 648         // We could have already set it to false because we detected an uninlineable call.
 649         // Don&#39;t override that observation.
 650         m_codeBlock-&gt;m_shouldAlwaysBeInlined &amp;= canInline(level) &amp;&amp; DFG::mightInlineFunction(m_codeBlock);
 651         break;
 652     }
 653 
 654     if (UNLIKELY(Options::dumpDisassembly() || (m_vm-&gt;m_perBytecodeProfiler &amp;&amp; Options::disassembleBaselineForProfiler())))
<span class="line-modified"> 655         m_disassembler = std::make_unique&lt;JITDisassembler&gt;(m_codeBlock);</span>
 656     if (UNLIKELY(m_vm-&gt;m_perBytecodeProfiler)) {
 657         m_compilation = adoptRef(
 658             new Profiler::Compilation(
 659                 m_vm-&gt;m_perBytecodeProfiler-&gt;ensureBytecodesFor(m_codeBlock),
 660                 Profiler::Baseline));
 661         m_compilation-&gt;addProfiledBytecodes(*m_vm-&gt;m_perBytecodeProfiler, m_codeBlock);
 662     }
 663 
 664     m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(0, nullptr));
 665 
 666     Label entryLabel(this);
 667     if (m_disassembler)
 668         m_disassembler-&gt;setStartOfCode(entryLabel);
 669 
 670     // Just add a little bit of randomness to the codegen
 671     if (random() &amp; 1)
 672         nop();
 673 
 674     emitFunctionPrologue();
 675     emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
</pre>
<hr />
<pre>
 882     for (unsigned bytecodeOffset = 0; bytecodeOffset &lt; m_labels.size(); ++bytecodeOffset) {
 883         if (m_labels[bytecodeOffset].isSet())
 884             jitCodeMap.append(bytecodeOffset, patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_labels[bytecodeOffset]));
 885     }
 886     jitCodeMap.finish();
 887     m_codeBlock-&gt;setJITCodeMap(WTFMove(jitCodeMap));
 888 
 889     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; withArityCheck = patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_arityCheck);
 890 
 891     if (Options::dumpDisassembly()) {
 892         m_disassembler-&gt;dump(patchBuffer);
 893         patchBuffer.didAlreadyDisassemble();
 894     }
 895     if (UNLIKELY(m_compilation)) {
 896         if (Options::disassembleBaselineForProfiler())
 897             m_disassembler-&gt;reportToProfiler(m_compilation.get(), patchBuffer);
 898         m_vm-&gt;m_perBytecodeProfiler-&gt;addCompilation(m_codeBlock, *m_compilation);
 899     }
 900 
 901     if (m_pcToCodeOriginMapBuilder.didBuildMapping())
<span class="line-modified"> 902         m_codeBlock-&gt;setPCToCodeOriginMap(std::make_unique&lt;PCToCodeOriginMap&gt;(WTFMove(m_pcToCodeOriginMapBuilder), patchBuffer));</span>
 903 
 904     CodeRef&lt;JSEntryPtrTag&gt; result = FINALIZE_CODE(
 905         patchBuffer, JSEntryPtrTag,
<span class="line-modified"> 906         &quot;Baseline JIT code for %s&quot;, toCString(CodeBlockWithJITType(m_codeBlock, JITCode::BaselineJIT)).data());</span>
 907 
 908     m_vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;add(
 909         static_cast&lt;double&gt;(result.size()) /
<span class="line-modified"> 910         static_cast&lt;double&gt;(m_codeBlock-&gt;instructionCount()));</span>
 911 
 912     m_codeBlock-&gt;shrinkToFit(CodeBlock::LateShrink);
 913     m_codeBlock-&gt;setJITCode(
<span class="line-modified"> 914         adoptRef(*new DirectJITCode(result, withArityCheck, JITCode::BaselineJIT)));</span>
 915 
 916     if (JITInternal::verbose)
 917         dataLogF(&quot;JIT generated code for %p at [%p, %p).\n&quot;, m_codeBlock, result.executableMemory()-&gt;start().untaggedPtr(), result.executableMemory()-&gt;end().untaggedPtr());
 918 
 919     return CompilationSuccessful;
 920 }
 921 
 922 CompilationResult JIT::privateCompile(JITCompilationEffort effort)
 923 {
 924     doMainThreadPreparationBeforeCompile();
 925     compileWithoutLinking(effort);
 926     return link();
 927 }
 928 
 929 void JIT::privateCompileExceptionHandlers()
 930 {
 931     if (!m_exceptionChecksWithCallFrameRollback.empty()) {
 932         m_exceptionChecksWithCallFrameRollback.link(this);
 933 
<span class="line-modified"> 934         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm()-&gt;topEntryFrame);</span>
 935 
 936         // lookupExceptionHandlerFromCallerFrame is passed two arguments, the VM and the exec (the CallFrame*).
 937 
<span class="line-modified"> 938         move(TrustedImmPtr(vm()), GPRInfo::argumentGPR0);</span>
 939         move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
 940 
 941 #if CPU(X86)
 942         // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!
 943         poke(GPRInfo::argumentGPR0);
 944         poke(GPRInfo::argumentGPR1, 1);
 945 #endif
 946         m_calls.append(CallRecord(call(OperationPtrTag), std::numeric_limits&lt;unsigned&gt;::max(), FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandlerFromCallerFrame)));
<span class="line-modified"> 947         jumpToExceptionHandler(*vm());</span>
 948     }
 949 
 950     if (!m_exceptionChecks.empty() || m_byValCompilationInfo.size()) {
 951         m_exceptionHandler = label();
 952         m_exceptionChecks.link(this);
 953 
<span class="line-modified"> 954         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm()-&gt;topEntryFrame);</span>
 955 
 956         // lookupExceptionHandler is passed two arguments, the VM and the exec (the CallFrame*).
<span class="line-modified"> 957         move(TrustedImmPtr(vm()), GPRInfo::argumentGPR0);</span>
 958         move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
 959 
 960 #if CPU(X86)
 961         // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!
 962         poke(GPRInfo::argumentGPR0);
 963         poke(GPRInfo::argumentGPR1, 1);
 964 #endif
 965         m_calls.append(CallRecord(call(OperationPtrTag), std::numeric_limits&lt;unsigned&gt;::max(), FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandler)));
<span class="line-modified"> 966         jumpToExceptionHandler(*vm());</span>
 967     }
 968 }
 969 
 970 void JIT::doMainThreadPreparationBeforeCompile()
 971 {
 972     // This ensures that we have the most up to date type information when performing typecheck optimizations for op_profile_type.
 973     if (m_vm-&gt;typeProfiler())
 974         m_vm-&gt;typeProfilerLog()-&gt;processLogEntries(*m_vm, &quot;Preparing for JIT compilation.&quot;_s);
 975 }
 976 
 977 unsigned JIT::frameRegisterCountFor(CodeBlock* codeBlock)
 978 {
 979     ASSERT(static_cast&lt;unsigned&gt;(codeBlock-&gt;numCalleeLocals()) == WTF::roundUpToMultipleOf(stackAlignmentRegisters(), static_cast&lt;unsigned&gt;(codeBlock-&gt;numCalleeLocals())));
 980 
 981     return roundLocalRegisterCountForFramePointerOffset(codeBlock-&gt;numCalleeLocals() + maxFrameExtentForSlowPathCallInRegisters);
 982 }
 983 
 984 int JIT::stackPointerOffsetFor(CodeBlock* codeBlock)
 985 {
 986     return virtualRegisterForLocal(frameRegisterCountFor(codeBlock) - 1).offset();
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.</span>
   3  *
   4  * Redistribution and use in source and binary forms, with or without
   5  * modification, are permitted provided that the following conditions
   6  * are met:
   7  * 1. Redistributions of source code must retain the above copyright
   8  *    notice, this list of conditions and the following disclaimer.
   9  * 2. Redistributions in binary form must reproduce the above copyright
  10  *    notice, this list of conditions and the following disclaimer in the
  11  *    documentation and/or other materials provided with the distribution.
  12  *
  13  * THIS SOFTWARE IS PROVIDED BY APPLE INC. ``AS IS&#39;&#39; AND ANY
  14  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  15  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  16  * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL APPLE INC. OR
  17  * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
  18  * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
  19  * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
  20  * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
  21  * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  22  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
</pre>
<hr />
<pre>
  57 #include &lt;wtf/SimpleStats.h&gt;
  58 
  59 namespace JSC {
  60 namespace JITInternal {
  61 static constexpr const bool verbose = false;
  62 }
  63 
  64 Seconds totalBaselineCompileTime;
  65 Seconds totalDFGCompileTime;
  66 Seconds totalFTLCompileTime;
  67 Seconds totalFTLDFGCompileTime;
  68 Seconds totalFTLB3CompileTime;
  69 
  70 void ctiPatchCallByReturnAddress(ReturnAddressPtr returnAddress, FunctionPtr&lt;CFunctionPtrTag&gt; newCalleeFunction)
  71 {
  72     MacroAssembler::repatchCall(
  73         CodeLocationCall&lt;NoPtrTag&gt;(MacroAssemblerCodePtr&lt;NoPtrTag&gt;(returnAddress)),
  74         newCalleeFunction.retagged&lt;OperationPtrTag&gt;());
  75 }
  76 
<span class="line-modified">  77 JIT::JIT(VM&amp; vm, CodeBlock* codeBlock, unsigned loopOSREntryBytecodeOffset)</span>
<span class="line-modified">  78     : JSInterfaceJIT(&amp;vm, codeBlock)</span>
<span class="line-modified">  79     , m_interpreter(vm.interpreter)</span>
  80     , m_labels(codeBlock ? codeBlock-&gt;instructions().size() : 0)
  81     , m_bytecodeOffset(std::numeric_limits&lt;unsigned&gt;::max())
<span class="line-modified">  82     , m_pcToCodeOriginMapBuilder(vm)</span>
  83     , m_canBeOptimized(false)
  84     , m_shouldEmitProfiling(false)
  85     , m_shouldUseIndexMasking(Options::enableSpectreMitigations())
  86     , m_loopOSREntryBytecodeOffset(loopOSREntryBytecodeOffset)
  87 {
  88 }
  89 
  90 JIT::~JIT()
  91 {
  92 }
  93 




















  94 void JIT::emitNotifyWrite(WatchpointSet* set)
  95 {
  96     if (!set || set-&gt;state() == IsInvalidated) {
  97         addSlowCase(Jump());
  98         return;
  99     }
 100 
 101     addSlowCase(branch8(NotEqual, AbsoluteAddress(set-&gt;addressOfState()), TrustedImm32(IsInvalidated)));
 102 }
 103 
 104 void JIT::emitNotifyWrite(GPRReg pointerToSet)
 105 {
 106     addSlowCase(branch8(NotEqual, Address(pointerToSet, WatchpointSet::offsetOfState()), TrustedImm32(IsInvalidated)));
 107 }
 108 
 109 void JIT::assertStackPointerOffset()
 110 {
 111     if (ASSERT_DISABLED)
 112         return;
 113 
</pre>
<hr />
<pre>
 154 {
 155     linkAllSlowCases(iter);
 156 
 157     JITSlowPathCall slowPathCall(this, currentInstruction, stub);
 158     slowPathCall.call();
 159 }
 160 
 161 void JIT::privateCompileMainPass()
 162 {
 163     if (JITInternal::verbose)
 164         dataLog(&quot;Compiling &quot;, *m_codeBlock, &quot;\n&quot;);
 165 
 166     jitAssertTagsInPlace();
 167     jitAssertArgumentCountSane();
 168 
 169     auto&amp; instructions = m_codeBlock-&gt;instructions();
 170     unsigned instructionCount = m_codeBlock-&gt;instructions().size();
 171 
 172     m_callLinkInfoIndex = 0;
 173 
<span class="line-modified"> 174     VM&amp; vm = m_codeBlock-&gt;vm();</span>
 175     unsigned startBytecodeOffset = 0;
 176     if (m_loopOSREntryBytecodeOffset &amp;&amp; (m_codeBlock-&gt;inherits&lt;ProgramCodeBlock&gt;(vm) || m_codeBlock-&gt;inherits&lt;ModuleProgramCodeBlock&gt;(vm))) {
 177         // We can only do this optimization because we execute ProgramCodeBlock&#39;s exactly once.
 178         // This optimization would be invalid otherwise. When the LLInt determines it wants to
 179         // do OSR entry into the baseline JIT in a loop, it will pass in the bytecode offset it
 180         // was executing at when it kicked off our compilation. We only need to compile code for
 181         // anything reachable from that bytecode offset.
 182 
 183         // We only bother building the bytecode graph if it could save time and executable
 184         // memory. We pick an arbitrary offset where we deem this is profitable.
 185         if (m_loopOSREntryBytecodeOffset &gt;= 200) {
 186             // As a simplification, we don&#39;t find all bytecode ranges that are unreachable.
 187             // Instead, we just find the minimum bytecode offset that is reachable, and
 188             // compile code from that bytecode offset onwards.
 189 
 190             BytecodeGraph graph(m_codeBlock, m_codeBlock-&gt;instructions());
 191             BytecodeBasicBlock* block = graph.findBasicBlockForBytecodeOffset(m_loopOSREntryBytecodeOffset);
 192             RELEASE_ASSERT(block);
 193 
 194             GraphNodeWorklist&lt;BytecodeBasicBlock*&gt; worklist;
</pre>
<hr />
<pre>
 327         DEFINE_OP(op_try_get_by_id)
 328         DEFINE_OP(op_in_by_id)
 329         DEFINE_OP(op_get_by_id)
 330         DEFINE_OP(op_get_by_id_with_this)
 331         DEFINE_OP(op_get_by_id_direct)
 332         DEFINE_OP(op_get_by_val)
 333         DEFINE_OP(op_overrides_has_instance)
 334         DEFINE_OP(op_instanceof)
 335         DEFINE_OP(op_instanceof_custom)
 336         DEFINE_OP(op_is_empty)
 337         DEFINE_OP(op_is_undefined)
 338         DEFINE_OP(op_is_undefined_or_null)
 339         DEFINE_OP(op_is_boolean)
 340         DEFINE_OP(op_is_number)
 341         DEFINE_OP(op_is_object)
 342         DEFINE_OP(op_is_cell_with_type)
 343         DEFINE_OP(op_jeq_null)
 344         DEFINE_OP(op_jfalse)
 345         DEFINE_OP(op_jmp)
 346         DEFINE_OP(op_jneq_null)
<span class="line-added"> 347         DEFINE_OP(op_jundefined_or_null)</span>
<span class="line-added"> 348         DEFINE_OP(op_jnundefined_or_null)</span>
 349         DEFINE_OP(op_jneq_ptr)
 350         DEFINE_OP(op_jless)
 351         DEFINE_OP(op_jlesseq)
 352         DEFINE_OP(op_jgreater)
 353         DEFINE_OP(op_jgreatereq)
 354         DEFINE_OP(op_jnless)
 355         DEFINE_OP(op_jnlesseq)
 356         DEFINE_OP(op_jngreater)
 357         DEFINE_OP(op_jngreatereq)
 358         DEFINE_OP(op_jeq)
 359         DEFINE_OP(op_jneq)
 360         DEFINE_OP(op_jstricteq)
 361         DEFINE_OP(op_jnstricteq)
 362         DEFINE_OP(op_jbelow)
 363         DEFINE_OP(op_jbeloweq)
 364         DEFINE_OP(op_jtrue)
 365         DEFINE_OP(op_loop_hint)

 366         DEFINE_OP(op_nop)
 367         DEFINE_OP(op_super_sampler_begin)
 368         DEFINE_OP(op_super_sampler_end)
 369         DEFINE_OP(op_lshift)
 370         DEFINE_OP(op_mod)
 371         DEFINE_OP(op_mov)
 372         DEFINE_OP(op_mul)
 373         DEFINE_OP(op_negate)
 374         DEFINE_OP(op_neq)
 375         DEFINE_OP(op_neq_null)
 376         DEFINE_OP(op_new_array)
 377         DEFINE_OP(op_new_array_with_size)
 378         DEFINE_OP(op_new_func)
 379         DEFINE_OP(op_new_func_exp)
 380         DEFINE_OP(op_new_generator_func)
 381         DEFINE_OP(op_new_generator_func_exp)
 382         DEFINE_OP(op_new_async_func)
 383         DEFINE_OP(op_new_async_func_exp)
 384         DEFINE_OP(op_new_async_generator_func)
 385         DEFINE_OP(op_new_async_generator_func_exp)
</pre>
<hr />
<pre>
 510         DEFINE_SLOWCASE_OP(op_in_by_id)
 511         DEFINE_SLOWCASE_OP(op_get_by_id)
 512         DEFINE_SLOWCASE_OP(op_get_by_id_with_this)
 513         DEFINE_SLOWCASE_OP(op_get_by_id_direct)
 514         DEFINE_SLOWCASE_OP(op_get_by_val)
 515         DEFINE_SLOWCASE_OP(op_instanceof)
 516         DEFINE_SLOWCASE_OP(op_instanceof_custom)
 517         DEFINE_SLOWCASE_OP(op_jless)
 518         DEFINE_SLOWCASE_OP(op_jlesseq)
 519         DEFINE_SLOWCASE_OP(op_jgreater)
 520         DEFINE_SLOWCASE_OP(op_jgreatereq)
 521         DEFINE_SLOWCASE_OP(op_jnless)
 522         DEFINE_SLOWCASE_OP(op_jnlesseq)
 523         DEFINE_SLOWCASE_OP(op_jngreater)
 524         DEFINE_SLOWCASE_OP(op_jngreatereq)
 525         DEFINE_SLOWCASE_OP(op_jeq)
 526         DEFINE_SLOWCASE_OP(op_jneq)
 527         DEFINE_SLOWCASE_OP(op_jstricteq)
 528         DEFINE_SLOWCASE_OP(op_jnstricteq)
 529         DEFINE_SLOWCASE_OP(op_loop_hint)
<span class="line-modified"> 530         DEFINE_SLOWCASE_OP(op_enter)</span>
 531         DEFINE_SLOWCASE_OP(op_mod)
 532         DEFINE_SLOWCASE_OP(op_mul)
 533         DEFINE_SLOWCASE_OP(op_negate)
 534         DEFINE_SLOWCASE_OP(op_neq)
 535         DEFINE_SLOWCASE_OP(op_new_object)
 536         DEFINE_SLOWCASE_OP(op_put_by_id)
 537         case op_put_by_val_direct:
 538         DEFINE_SLOWCASE_OP(op_put_by_val)
 539         DEFINE_SLOWCASE_OP(op_sub)
 540         DEFINE_SLOWCASE_OP(op_has_indexed_property)
 541         DEFINE_SLOWCASE_OP(op_get_from_scope)
 542         DEFINE_SLOWCASE_OP(op_put_to_scope)
 543 
 544         DEFINE_SLOWCASE_SLOW_OP(unsigned)
 545         DEFINE_SLOWCASE_SLOW_OP(inc)
 546         DEFINE_SLOWCASE_SLOW_OP(dec)
 547         DEFINE_SLOWCASE_SLOW_OP(bitnot)
 548         DEFINE_SLOWCASE_SLOW_OP(bitand)
 549         DEFINE_SLOWCASE_SLOW_OP(bitor)
 550         DEFINE_SLOWCASE_SLOW_OP(bitxor)
</pre>
<hr />
<pre>
 616         break;
 617     default:
 618         RELEASE_ASSERT_NOT_REACHED();
 619         break;
 620     }
 621 
 622     switch (m_codeBlock-&gt;codeType()) {
 623     case GlobalCode:
 624     case ModuleCode:
 625     case EvalCode:
 626         m_codeBlock-&gt;m_shouldAlwaysBeInlined = false;
 627         break;
 628     case FunctionCode:
 629         // We could have already set it to false because we detected an uninlineable call.
 630         // Don&#39;t override that observation.
 631         m_codeBlock-&gt;m_shouldAlwaysBeInlined &amp;= canInline(level) &amp;&amp; DFG::mightInlineFunction(m_codeBlock);
 632         break;
 633     }
 634 
 635     if (UNLIKELY(Options::dumpDisassembly() || (m_vm-&gt;m_perBytecodeProfiler &amp;&amp; Options::disassembleBaselineForProfiler())))
<span class="line-modified"> 636         m_disassembler = makeUnique&lt;JITDisassembler&gt;(m_codeBlock);</span>
 637     if (UNLIKELY(m_vm-&gt;m_perBytecodeProfiler)) {
 638         m_compilation = adoptRef(
 639             new Profiler::Compilation(
 640                 m_vm-&gt;m_perBytecodeProfiler-&gt;ensureBytecodesFor(m_codeBlock),
 641                 Profiler::Baseline));
 642         m_compilation-&gt;addProfiledBytecodes(*m_vm-&gt;m_perBytecodeProfiler, m_codeBlock);
 643     }
 644 
 645     m_pcToCodeOriginMapBuilder.appendItem(label(), CodeOrigin(0, nullptr));
 646 
 647     Label entryLabel(this);
 648     if (m_disassembler)
 649         m_disassembler-&gt;setStartOfCode(entryLabel);
 650 
 651     // Just add a little bit of randomness to the codegen
 652     if (random() &amp; 1)
 653         nop();
 654 
 655     emitFunctionPrologue();
 656     emitPutToCallFrameHeader(m_codeBlock, CallFrameSlot::codeBlock);
</pre>
<hr />
<pre>
 863     for (unsigned bytecodeOffset = 0; bytecodeOffset &lt; m_labels.size(); ++bytecodeOffset) {
 864         if (m_labels[bytecodeOffset].isSet())
 865             jitCodeMap.append(bytecodeOffset, patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_labels[bytecodeOffset]));
 866     }
 867     jitCodeMap.finish();
 868     m_codeBlock-&gt;setJITCodeMap(WTFMove(jitCodeMap));
 869 
 870     MacroAssemblerCodePtr&lt;JSEntryPtrTag&gt; withArityCheck = patchBuffer.locationOf&lt;JSEntryPtrTag&gt;(m_arityCheck);
 871 
 872     if (Options::dumpDisassembly()) {
 873         m_disassembler-&gt;dump(patchBuffer);
 874         patchBuffer.didAlreadyDisassemble();
 875     }
 876     if (UNLIKELY(m_compilation)) {
 877         if (Options::disassembleBaselineForProfiler())
 878             m_disassembler-&gt;reportToProfiler(m_compilation.get(), patchBuffer);
 879         m_vm-&gt;m_perBytecodeProfiler-&gt;addCompilation(m_codeBlock, *m_compilation);
 880     }
 881 
 882     if (m_pcToCodeOriginMapBuilder.didBuildMapping())
<span class="line-modified"> 883         m_codeBlock-&gt;setPCToCodeOriginMap(makeUnique&lt;PCToCodeOriginMap&gt;(WTFMove(m_pcToCodeOriginMapBuilder), patchBuffer));</span>
 884 
 885     CodeRef&lt;JSEntryPtrTag&gt; result = FINALIZE_CODE(
 886         patchBuffer, JSEntryPtrTag,
<span class="line-modified"> 887         &quot;Baseline JIT code for %s&quot;, toCString(CodeBlockWithJITType(m_codeBlock, JITType::BaselineJIT)).data());</span>
 888 
 889     m_vm-&gt;machineCodeBytesPerBytecodeWordForBaselineJIT-&gt;add(
 890         static_cast&lt;double&gt;(result.size()) /
<span class="line-modified"> 891         static_cast&lt;double&gt;(m_codeBlock-&gt;instructionsSize()));</span>
 892 
 893     m_codeBlock-&gt;shrinkToFit(CodeBlock::LateShrink);
 894     m_codeBlock-&gt;setJITCode(
<span class="line-modified"> 895         adoptRef(*new DirectJITCode(result, withArityCheck, JITType::BaselineJIT)));</span>
 896 
 897     if (JITInternal::verbose)
 898         dataLogF(&quot;JIT generated code for %p at [%p, %p).\n&quot;, m_codeBlock, result.executableMemory()-&gt;start().untaggedPtr(), result.executableMemory()-&gt;end().untaggedPtr());
 899 
 900     return CompilationSuccessful;
 901 }
 902 
 903 CompilationResult JIT::privateCompile(JITCompilationEffort effort)
 904 {
 905     doMainThreadPreparationBeforeCompile();
 906     compileWithoutLinking(effort);
 907     return link();
 908 }
 909 
 910 void JIT::privateCompileExceptionHandlers()
 911 {
 912     if (!m_exceptionChecksWithCallFrameRollback.empty()) {
 913         m_exceptionChecksWithCallFrameRollback.link(this);
 914 
<span class="line-modified"> 915         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
 916 
 917         // lookupExceptionHandlerFromCallerFrame is passed two arguments, the VM and the exec (the CallFrame*).
 918 
<span class="line-modified"> 919         move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);</span>
 920         move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
 921 
 922 #if CPU(X86)
 923         // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!
 924         poke(GPRInfo::argumentGPR0);
 925         poke(GPRInfo::argumentGPR1, 1);
 926 #endif
 927         m_calls.append(CallRecord(call(OperationPtrTag), std::numeric_limits&lt;unsigned&gt;::max(), FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandlerFromCallerFrame)));
<span class="line-modified"> 928         jumpToExceptionHandler(vm());</span>
 929     }
 930 
 931     if (!m_exceptionChecks.empty() || m_byValCompilationInfo.size()) {
 932         m_exceptionHandler = label();
 933         m_exceptionChecks.link(this);
 934 
<span class="line-modified"> 935         copyCalleeSavesToEntryFrameCalleeSavesBuffer(vm().topEntryFrame);</span>
 936 
 937         // lookupExceptionHandler is passed two arguments, the VM and the exec (the CallFrame*).
<span class="line-modified"> 938         move(TrustedImmPtr(&amp;vm()), GPRInfo::argumentGPR0);</span>
 939         move(GPRInfo::callFrameRegister, GPRInfo::argumentGPR1);
 940 
 941 #if CPU(X86)
 942         // FIXME: should use the call abstraction, but this is currently in the SpeculativeJIT layer!
 943         poke(GPRInfo::argumentGPR0);
 944         poke(GPRInfo::argumentGPR1, 1);
 945 #endif
 946         m_calls.append(CallRecord(call(OperationPtrTag), std::numeric_limits&lt;unsigned&gt;::max(), FunctionPtr&lt;OperationPtrTag&gt;(lookupExceptionHandler)));
<span class="line-modified"> 947         jumpToExceptionHandler(vm());</span>
 948     }
 949 }
 950 
 951 void JIT::doMainThreadPreparationBeforeCompile()
 952 {
 953     // This ensures that we have the most up to date type information when performing typecheck optimizations for op_profile_type.
 954     if (m_vm-&gt;typeProfiler())
 955         m_vm-&gt;typeProfilerLog()-&gt;processLogEntries(*m_vm, &quot;Preparing for JIT compilation.&quot;_s);
 956 }
 957 
 958 unsigned JIT::frameRegisterCountFor(CodeBlock* codeBlock)
 959 {
 960     ASSERT(static_cast&lt;unsigned&gt;(codeBlock-&gt;numCalleeLocals()) == WTF::roundUpToMultipleOf(stackAlignmentRegisters(), static_cast&lt;unsigned&gt;(codeBlock-&gt;numCalleeLocals())));
 961 
 962     return roundLocalRegisterCountForFramePointerOffset(codeBlock-&gt;numCalleeLocals() + maxFrameExtentForSlowPathCallInRegisters);
 963 }
 964 
 965 int JIT::stackPointerOffsetFor(CodeBlock* codeBlock)
 966 {
 967     return virtualRegisterForLocal(frameRegisterCountFor(codeBlock) - 1).offset();
</pre>
</td>
</tr>
</table>
<center><a href="IntrinsicEmitter.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="JIT.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>