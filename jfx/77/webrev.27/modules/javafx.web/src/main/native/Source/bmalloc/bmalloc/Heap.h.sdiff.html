<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/Heap.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
<body>
<center><a href="Heap.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="IsoAllocator.h.sdiff.html" target="_top">next &gt;</a></center>    <h2>modules/javafx.web/src/main/native/Source/bmalloc/bmalloc/Heap.h</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 59 public:
 60     Heap(HeapKind, std::lock_guard&lt;Mutex&gt;&amp;);
 61 
 62     static Mutex&amp; mutex() { return PerProcess&lt;PerHeapKind&lt;Heap&gt;&gt;::mutex(); }
 63 
 64     HeapKind kind() const { return m_kind; }
 65 
 66     void allocateSmallBumpRanges(std::unique_lock&lt;Mutex&gt;&amp;, size_t sizeClass,
 67         BumpAllocator&amp;, BumpRangeCache&amp;, LineCache&amp;);
 68     void derefSmallLine(std::unique_lock&lt;Mutex&gt;&amp;, Object, LineCache&amp;);
 69     void deallocateLineCache(std::unique_lock&lt;Mutex&gt;&amp;, LineCache&amp;);
 70 
 71     void* allocateLarge(std::unique_lock&lt;Mutex&gt;&amp;, size_t alignment, size_t);
 72     void* tryAllocateLarge(std::unique_lock&lt;Mutex&gt;&amp;, size_t alignment, size_t);
 73     void deallocateLarge(std::unique_lock&lt;Mutex&gt;&amp;, void*);
 74 
 75     bool isLarge(std::unique_lock&lt;Mutex&gt;&amp;, void*);
 76     size_t largeSize(std::unique_lock&lt;Mutex&gt;&amp;, void*);
 77     void shrinkLarge(std::unique_lock&lt;Mutex&gt;&amp;, const Range&amp;, size_t);
 78 
<span class="line-modified"> 79     void scavenge(std::lock_guard&lt;Mutex&gt;&amp;, BulkDecommit&amp;);</span>
 80     void scavenge(std::lock_guard&lt;Mutex&gt;&amp;, BulkDecommit&amp;, size_t&amp; freed, size_t goal);
<span class="line-removed"> 81     void scavengeToHighWatermark(std::lock_guard&lt;Mutex&gt;&amp;, BulkDecommit&amp;);</span>
 82 
 83     size_t freeableMemory(std::lock_guard&lt;Mutex&gt;&amp;);
 84     size_t footprint();
 85 
 86     void externalDecommit(void* ptr, size_t);
 87     void externalDecommit(std::unique_lock&lt;Mutex&gt;&amp;, void* ptr, size_t);
 88     void externalCommit(void* ptr, size_t);
 89     void externalCommit(std::unique_lock&lt;Mutex&gt;&amp;, void* ptr, size_t);
 90 
 91     void markAllLargeAsEligibile(std::lock_guard&lt;Mutex&gt;&amp;);
 92 
 93 private:
 94     void decommitLargeRange(std::lock_guard&lt;Mutex&gt;&amp;, LargeRange&amp;, BulkDecommit&amp;);
 95 
 96     struct LargeObjectHash {
 97         static unsigned hash(void* key)
 98         {
 99             return static_cast&lt;unsigned&gt;(
100                 reinterpret_cast&lt;uintptr_t&gt;(key) / smallMax);
101         }
</pre>
<hr />
<pre>
112 
113     void allocateSmallBumpRangesByMetadata(std::unique_lock&lt;Mutex&gt;&amp;,
114         size_t sizeClass, BumpAllocator&amp;, BumpRangeCache&amp;, LineCache&amp;);
115     void allocateSmallBumpRangesByObject(std::unique_lock&lt;Mutex&gt;&amp;,
116         size_t sizeClass, BumpAllocator&amp;, BumpRangeCache&amp;, LineCache&amp;);
117 
118     SmallPage* allocateSmallPage(std::unique_lock&lt;Mutex&gt;&amp;, size_t sizeClass, LineCache&amp;);
119     void deallocateSmallLine(std::unique_lock&lt;Mutex&gt;&amp;, Object, LineCache&amp;);
120 
121     void allocateSmallChunk(std::unique_lock&lt;Mutex&gt;&amp;, size_t pageClass);
122     void deallocateSmallChunk(Chunk*, size_t pageClass);
123 
124     void mergeLarge(BeginTag*&amp;, EndTag*&amp;, Range&amp;);
125     void mergeLargeLeft(EndTag*&amp;, BeginTag*&amp;, Range&amp;, bool&amp; inVMHeap);
126     void mergeLargeRight(EndTag*&amp;, BeginTag*&amp;, Range&amp;, bool&amp; inVMHeap);
127 
128     LargeRange splitAndAllocate(std::unique_lock&lt;Mutex&gt;&amp;, LargeRange&amp;, size_t alignment, size_t);
129 
130     HeapKind m_kind;
131 



132     size_t m_vmPageSizePhysical;
133     Vector&lt;LineMetadata&gt; m_smallLineMetadata;
134     std::array&lt;size_t, sizeClassCount&gt; m_pageClasses;
135 
136     LineCache m_lineCache;
137     std::array&lt;List&lt;Chunk&gt;, pageClassCount&gt; m_freePages;
138     std::array&lt;List&lt;Chunk&gt;, pageClassCount&gt; m_chunkCache;
139 
140     Map&lt;void*, size_t, LargeObjectHash&gt; m_largeAllocated;
141     LargeMap m_largeFree;
142 
143     Map&lt;Chunk*, ObjectType, ChunkHash&gt; m_objectTypes;
144 
145     Scavenger* m_scavenger { nullptr };
146 
147     size_t m_footprint { 0 };
148     size_t m_freeableMemory { 0 };
149 
<span class="line-removed">150     bool m_hasPendingDecommits { false };</span>
<span class="line-removed">151     std::condition_variable_any m_condition;</span>
<span class="line-removed">152 </span>
153 #if ENABLE_PHYSICAL_PAGE_MAP
154     PhysicalPageMap m_physicalPageMap;
155 #endif
<span class="line-removed">156 </span>
<span class="line-removed">157     void* m_highWatermark { nullptr };</span>
158 };
159 
160 inline void Heap::allocateSmallBumpRanges(
161     std::unique_lock&lt;Mutex&gt;&amp; lock, size_t sizeClass,
162     BumpAllocator&amp; allocator, BumpRangeCache&amp; rangeCache,
163     LineCache&amp; lineCache)
164 {
165     if (sizeClass &lt; bmalloc::sizeClass(smallLineSize))
166         return allocateSmallBumpRangesByMetadata(lock, sizeClass, allocator, rangeCache, lineCache);
167     return allocateSmallBumpRangesByObject(lock, sizeClass, allocator, rangeCache, lineCache);
168 }
169 
170 inline void Heap::derefSmallLine(std::unique_lock&lt;Mutex&gt;&amp; lock, Object object, LineCache&amp; lineCache)
171 {
172     if (!object.line()-&gt;deref(lock))
173         return;
174     deallocateSmallLine(lock, object, lineCache);
175 }
176 
177 } // namespace bmalloc
</pre>
</td>
<td>
<hr />
<pre>
 59 public:
 60     Heap(HeapKind, std::lock_guard&lt;Mutex&gt;&amp;);
 61 
 62     static Mutex&amp; mutex() { return PerProcess&lt;PerHeapKind&lt;Heap&gt;&gt;::mutex(); }
 63 
 64     HeapKind kind() const { return m_kind; }
 65 
 66     void allocateSmallBumpRanges(std::unique_lock&lt;Mutex&gt;&amp;, size_t sizeClass,
 67         BumpAllocator&amp;, BumpRangeCache&amp;, LineCache&amp;);
 68     void derefSmallLine(std::unique_lock&lt;Mutex&gt;&amp;, Object, LineCache&amp;);
 69     void deallocateLineCache(std::unique_lock&lt;Mutex&gt;&amp;, LineCache&amp;);
 70 
 71     void* allocateLarge(std::unique_lock&lt;Mutex&gt;&amp;, size_t alignment, size_t);
 72     void* tryAllocateLarge(std::unique_lock&lt;Mutex&gt;&amp;, size_t alignment, size_t);
 73     void deallocateLarge(std::unique_lock&lt;Mutex&gt;&amp;, void*);
 74 
 75     bool isLarge(std::unique_lock&lt;Mutex&gt;&amp;, void*);
 76     size_t largeSize(std::unique_lock&lt;Mutex&gt;&amp;, void*);
 77     void shrinkLarge(std::unique_lock&lt;Mutex&gt;&amp;, const Range&amp;, size_t);
 78 
<span class="line-modified"> 79     void scavenge(std::lock_guard&lt;Mutex&gt;&amp;, BulkDecommit&amp;, size_t&amp; deferredDecommits);</span>
 80     void scavenge(std::lock_guard&lt;Mutex&gt;&amp;, BulkDecommit&amp;, size_t&amp; freed, size_t goal);

 81 
 82     size_t freeableMemory(std::lock_guard&lt;Mutex&gt;&amp;);
 83     size_t footprint();
 84 
 85     void externalDecommit(void* ptr, size_t);
 86     void externalDecommit(std::unique_lock&lt;Mutex&gt;&amp;, void* ptr, size_t);
 87     void externalCommit(void* ptr, size_t);
 88     void externalCommit(std::unique_lock&lt;Mutex&gt;&amp;, void* ptr, size_t);
 89 
 90     void markAllLargeAsEligibile(std::lock_guard&lt;Mutex&gt;&amp;);
 91 
 92 private:
 93     void decommitLargeRange(std::lock_guard&lt;Mutex&gt;&amp;, LargeRange&amp;, BulkDecommit&amp;);
 94 
 95     struct LargeObjectHash {
 96         static unsigned hash(void* key)
 97         {
 98             return static_cast&lt;unsigned&gt;(
 99                 reinterpret_cast&lt;uintptr_t&gt;(key) / smallMax);
100         }
</pre>
<hr />
<pre>
111 
112     void allocateSmallBumpRangesByMetadata(std::unique_lock&lt;Mutex&gt;&amp;,
113         size_t sizeClass, BumpAllocator&amp;, BumpRangeCache&amp;, LineCache&amp;);
114     void allocateSmallBumpRangesByObject(std::unique_lock&lt;Mutex&gt;&amp;,
115         size_t sizeClass, BumpAllocator&amp;, BumpRangeCache&amp;, LineCache&amp;);
116 
117     SmallPage* allocateSmallPage(std::unique_lock&lt;Mutex&gt;&amp;, size_t sizeClass, LineCache&amp;);
118     void deallocateSmallLine(std::unique_lock&lt;Mutex&gt;&amp;, Object, LineCache&amp;);
119 
120     void allocateSmallChunk(std::unique_lock&lt;Mutex&gt;&amp;, size_t pageClass);
121     void deallocateSmallChunk(Chunk*, size_t pageClass);
122 
123     void mergeLarge(BeginTag*&amp;, EndTag*&amp;, Range&amp;);
124     void mergeLargeLeft(EndTag*&amp;, BeginTag*&amp;, Range&amp;, bool&amp; inVMHeap);
125     void mergeLargeRight(EndTag*&amp;, BeginTag*&amp;, Range&amp;, bool&amp; inVMHeap);
126 
127     LargeRange splitAndAllocate(std::unique_lock&lt;Mutex&gt;&amp;, LargeRange&amp;, size_t alignment, size_t);
128 
129     HeapKind m_kind;
130 
<span class="line-added">131     bool m_hasPendingDecommits { false };</span>
<span class="line-added">132     std::condition_variable_any m_condition;</span>
<span class="line-added">133 </span>
134     size_t m_vmPageSizePhysical;
135     Vector&lt;LineMetadata&gt; m_smallLineMetadata;
136     std::array&lt;size_t, sizeClassCount&gt; m_pageClasses;
137 
138     LineCache m_lineCache;
139     std::array&lt;List&lt;Chunk&gt;, pageClassCount&gt; m_freePages;
140     std::array&lt;List&lt;Chunk&gt;, pageClassCount&gt; m_chunkCache;
141 
142     Map&lt;void*, size_t, LargeObjectHash&gt; m_largeAllocated;
143     LargeMap m_largeFree;
144 
145     Map&lt;Chunk*, ObjectType, ChunkHash&gt; m_objectTypes;
146 
147     Scavenger* m_scavenger { nullptr };
148 
149     size_t m_footprint { 0 };
150     size_t m_freeableMemory { 0 };
151 



152 #if ENABLE_PHYSICAL_PAGE_MAP
153     PhysicalPageMap m_physicalPageMap;
154 #endif


155 };
156 
157 inline void Heap::allocateSmallBumpRanges(
158     std::unique_lock&lt;Mutex&gt;&amp; lock, size_t sizeClass,
159     BumpAllocator&amp; allocator, BumpRangeCache&amp; rangeCache,
160     LineCache&amp; lineCache)
161 {
162     if (sizeClass &lt; bmalloc::sizeClass(smallLineSize))
163         return allocateSmallBumpRangesByMetadata(lock, sizeClass, allocator, rangeCache, lineCache);
164     return allocateSmallBumpRangesByObject(lock, sizeClass, allocator, rangeCache, lineCache);
165 }
166 
167 inline void Heap::derefSmallLine(std::unique_lock&lt;Mutex&gt;&amp; lock, Object object, LineCache&amp; lineCache)
168 {
169     if (!object.line()-&gt;deref(lock))
170         return;
171     deallocateSmallLine(lock, object, lineCache);
172 }
173 
174 } // namespace bmalloc
</pre>
</td>
</tr>
</table>
<center><a href="Heap.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../index.html" target="_top">index</a> <a href="IsoAllocator.h.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>