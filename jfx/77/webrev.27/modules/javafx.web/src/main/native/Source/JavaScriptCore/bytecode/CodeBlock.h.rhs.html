<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.web/src/main/native/Source/JavaScriptCore/bytecode/CodeBlock.h</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (C) 2008-2019 Apple Inc. All rights reserved.
   3  * Copyright (C) 2008 Cameron Zwarich &lt;cwzwarich@uwaterloo.ca&gt;
   4  *
   5  * Redistribution and use in source and binary forms, with or without
   6  * modification, are permitted provided that the following conditions
   7  * are met:
   8  *
   9  * 1.  Redistributions of source code must retain the above copyright
  10  *     notice, this list of conditions and the following disclaimer.
  11  * 2.  Redistributions in binary form must reproduce the above copyright
  12  *     notice, this list of conditions and the following disclaimer in the
  13  *     documentation and/or other materials provided with the distribution.
  14  * 3.  Neither the name of Apple Inc. (&quot;Apple&quot;) nor the names of
  15  *     its contributors may be used to endorse or promote products derived
  16  *     from this software without specific prior written permission.
  17  *
  18  * THIS SOFTWARE IS PROVIDED BY APPLE AND ITS CONTRIBUTORS &quot;AS IS&quot; AND ANY
  19  * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
  20  * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  21  * DISCLAIMED. IN NO EVENT SHALL APPLE OR ITS CONTRIBUTORS BE LIABLE FOR ANY
  22  * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
  23  * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
  24  * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
  25  * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
  26  * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
  27  * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  28  */
  29 
  30 #pragma once
  31 
  32 #include &quot;ArrayProfile.h&quot;
  33 #include &quot;ByValInfo.h&quot;
  34 #include &quot;BytecodeConventions.h&quot;
  35 #include &quot;CallLinkInfo.h&quot;
  36 #include &quot;CodeBlockHash.h&quot;
  37 #include &quot;CodeOrigin.h&quot;
  38 #include &quot;CodeType.h&quot;
  39 #include &quot;CompilationResult.h&quot;
  40 #include &quot;ConcurrentJSLock.h&quot;
  41 #include &quot;DFGCommon.h&quot;
  42 #include &quot;DirectEvalCodeCache.h&quot;
  43 #include &quot;EvalExecutable.h&quot;
  44 #include &quot;ExecutionCounter.h&quot;
  45 #include &quot;ExpressionRangeInfo.h&quot;
  46 #include &quot;FunctionExecutable.h&quot;
  47 #include &quot;HandlerInfo.h&quot;
  48 #include &quot;ICStatusMap.h&quot;
  49 #include &quot;Instruction.h&quot;
  50 #include &quot;InstructionStream.h&quot;
  51 #include &quot;JITCode.h&quot;
  52 #include &quot;JITCodeMap.h&quot;
  53 #include &quot;JITMathICForwards.h&quot;
  54 #include &quot;JSCast.h&quot;
  55 #include &quot;JSGlobalObject.h&quot;
  56 #include &quot;JumpTable.h&quot;
  57 #include &quot;LLIntCallLinkInfo.h&quot;
  58 #include &quot;LazyOperandValueProfile.h&quot;
  59 #include &quot;MetadataTable.h&quot;
  60 #include &quot;ModuleProgramExecutable.h&quot;
  61 #include &quot;ObjectAllocationProfile.h&quot;
  62 #include &quot;Options.h&quot;
  63 #include &quot;Printer.h&quot;
  64 #include &quot;ProfilerJettisonReason.h&quot;
  65 #include &quot;ProgramExecutable.h&quot;
  66 #include &quot;PutPropertySlot.h&quot;
  67 #include &quot;ValueProfile.h&quot;
  68 #include &quot;VirtualRegister.h&quot;
  69 #include &quot;Watchpoint.h&quot;
  70 #include &lt;wtf/Bag.h&gt;
  71 #include &lt;wtf/FastMalloc.h&gt;
  72 #include &lt;wtf/RefCountedArray.h&gt;
  73 #include &lt;wtf/RefPtr.h&gt;
  74 #include &lt;wtf/SegmentedVector.h&gt;
  75 #include &lt;wtf/Vector.h&gt;
  76 #include &lt;wtf/text/WTFString.h&gt;
  77 
  78 namespace JSC {
  79 
  80 #if ENABLE(DFG_JIT)
  81 namespace DFG {
  82 struct OSRExitState;
  83 } // namespace DFG
  84 #endif
  85 
  86 class BytecodeLivenessAnalysis;
  87 class CodeBlockSet;
  88 class ExecutableToCodeBlockEdge;
  89 class JSModuleEnvironment;
  90 class LLIntOffsetsExtractor;
  91 class LLIntPrototypeLoadAdaptiveStructureWatchpoint;
  92 class MetadataTable;
  93 class PCToCodeOriginMap;
  94 class RegisterAtOffsetList;
  95 class StructureStubInfo;
  96 
  97 enum class AccessType : int8_t;
  98 
  99 struct ArithProfile;
 100 struct OpCatch;
 101 
 102 enum ReoptimizationMode { DontCountReoptimization, CountReoptimization };
 103 
 104 class CodeBlock : public JSCell {
 105     typedef JSCell Base;
 106     friend class BytecodeLivenessAnalysis;
 107     friend class JIT;
 108     friend class LLIntOffsetsExtractor;
 109 
 110 public:
 111 
 112     enum CopyParsedBlockTag { CopyParsedBlock };
 113 
 114     static const unsigned StructureFlags = Base::StructureFlags | StructureIsImmortal;
 115     static const bool needsDestruction = true;
 116 
 117     template&lt;typename, SubspaceAccess&gt;
<a name="1" id="anc1"></a><span class="line-modified"> 118     static IsoSubspace* subspaceFor(VM&amp;) { return nullptr; }</span>
 119 
 120     DECLARE_INFO;
 121 
 122 protected:
<a name="2" id="anc2"></a><span class="line-modified"> 123     CodeBlock(VM&amp;, Structure*, CopyParsedBlockTag, CodeBlock&amp; other);</span>
<span class="line-modified"> 124     CodeBlock(VM&amp;, Structure*, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock*, JSScope*);</span>
 125 
 126     void finishCreation(VM&amp;, CopyParsedBlockTag, CodeBlock&amp; other);
 127     bool finishCreation(VM&amp;, ScriptExecutable* ownerExecutable, UnlinkedCodeBlock*, JSScope*);
 128 
 129     void finishCreationCommon(VM&amp;);
 130 
 131     WriteBarrier&lt;JSGlobalObject&gt; m_globalObject;
 132 
 133 public:
 134     JS_EXPORT_PRIVATE ~CodeBlock();
 135 
 136     UnlinkedCodeBlock* unlinkedCodeBlock() const { return m_unlinkedCode.get(); }
 137 
 138     CString inferredName() const;
 139     CodeBlockHash hash() const;
 140     bool hasHash() const;
 141     bool isSafeToComputeHash() const;
 142     CString hashAsStringIfPossible() const;
 143     CString sourceCodeForTools() const; // Not quite the actual source we parsed; this will do things like prefix the source for a function with a reified signature.
 144     CString sourceCodeOnOneLine() const; // As sourceCodeForTools(), but replaces all whitespace runs with a single space.
<a name="3" id="anc3"></a><span class="line-modified"> 145     void dumpAssumingJITType(PrintStream&amp;, JITType) const;</span>
 146     JS_EXPORT_PRIVATE void dump(PrintStream&amp;) const;
 147 
<a name="4" id="anc4"></a><span class="line-added"> 148     MetadataTable* metadataTable() const { return m_metadata.get(); }</span>
<span class="line-added"> 149 </span>
 150     int numParameters() const { return m_numParameters; }
 151     void setNumParameters(int newValue);
 152 
 153     int numberOfArgumentsToSkip() const { return m_numberOfArgumentsToSkip; }
 154 
 155     int numCalleeLocals() const { return m_numCalleeLocals; }
 156 
 157     int numVars() const { return m_numVars; }
 158 
 159     int* addressOfNumParameters() { return &amp;m_numParameters; }
 160     static ptrdiff_t offsetOfNumParameters() { return OBJECT_OFFSETOF(CodeBlock, m_numParameters); }
 161 
 162     CodeBlock* alternative() const { return static_cast&lt;CodeBlock*&gt;(m_alternative.get()); }
 163     void setAlternative(VM&amp;, CodeBlock*);
 164 
 165     template &lt;typename Functor&gt; void forEachRelatedCodeBlock(Functor&amp;&amp; functor)
 166     {
 167         Functor f(std::forward&lt;Functor&gt;(functor));
 168         Vector&lt;CodeBlock*, 4&gt; codeBlocks;
 169         codeBlocks.append(this);
 170 
 171         while (!codeBlocks.isEmpty()) {
 172             CodeBlock* currentCodeBlock = codeBlocks.takeLast();
 173             f(currentCodeBlock);
 174 
 175             if (CodeBlock* alternative = currentCodeBlock-&gt;alternative())
 176                 codeBlocks.append(alternative);
 177             if (CodeBlock* osrEntryBlock = currentCodeBlock-&gt;specialOSREntryBlockOrNull())
 178                 codeBlocks.append(osrEntryBlock);
 179         }
 180     }
 181 
 182     CodeSpecializationKind specializationKind() const
 183     {
 184         return specializationFromIsConstruct(isConstructor());
 185     }
 186 
 187     CodeBlock* alternativeForJettison();
 188     JS_EXPORT_PRIVATE CodeBlock* baselineAlternative();
 189 
 190     // FIXME: Get rid of this.
 191     // https://bugs.webkit.org/show_bug.cgi?id=123677
 192     CodeBlock* baselineVersion();
 193 
 194     static size_t estimatedSize(JSCell*, VM&amp;);
 195     static void visitChildren(JSCell*, SlotVisitor&amp;);
 196     static void destroy(JSCell*);
 197     void visitChildren(SlotVisitor&amp;);
 198     void finalizeUnconditionally(VM&amp;);
 199 
 200     void notifyLexicalBindingUpdate();
 201 
 202     void dumpSource();
 203     void dumpSource(PrintStream&amp;);
 204 
 205     void dumpBytecode();
 206     void dumpBytecode(PrintStream&amp;);
 207     void dumpBytecode(PrintStream&amp; out, const InstructionStream::Ref&amp; it, const ICStatusMap&amp; = ICStatusMap());
 208     void dumpBytecode(PrintStream&amp; out, unsigned bytecodeOffset, const ICStatusMap&amp; = ICStatusMap());
 209 
 210     void dumpExceptionHandlers(PrintStream&amp;);
 211     void printStructures(PrintStream&amp;, const Instruction*);
 212     void printStructure(PrintStream&amp;, const char* name, const Instruction*, int operand);
 213 
 214     void dumpMathICStats();
 215 
 216     bool isStrictMode() const { return m_unlinkedCode-&gt;isStrictMode(); }
 217     bool isConstructor() const { return m_unlinkedCode-&gt;isConstructor(); }
 218     ECMAMode ecmaMode() const { return isStrictMode() ? StrictMode : NotStrictMode; }
 219     CodeType codeType() const { return m_unlinkedCode-&gt;codeType(); }
 220 
 221     JSParserScriptMode scriptMode() const { return m_unlinkedCode-&gt;scriptMode(); }
 222 
 223     bool hasInstalledVMTrapBreakpoints() const;
 224     bool installVMTrapBreakpoints();
 225 
 226     inline bool isKnownNotImmediate(int index)
 227     {
 228         if (index == thisRegister().offset() &amp;&amp; !isStrictMode())
 229             return true;
 230 
 231         if (isConstantRegisterIndex(index))
 232             return getConstant(index).isCell();
 233 
 234         return false;
 235     }
 236 
 237     ALWAYS_INLINE bool isTemporaryRegisterIndex(int index)
 238     {
 239         return index &gt;= m_numVars;
 240     }
 241 
 242     HandlerInfo* handlerForBytecodeOffset(unsigned bytecodeOffset, RequiredHandler = RequiredHandler::AnyHandler);
 243     HandlerInfo* handlerForIndex(unsigned, RequiredHandler = RequiredHandler::AnyHandler);
 244     void removeExceptionHandlerForCallSite(DisposableCallSiteIndex);
 245     unsigned lineNumberForBytecodeOffset(unsigned bytecodeOffset);
 246     unsigned columnNumberForBytecodeOffset(unsigned bytecodeOffset);
 247     void expressionRangeForBytecodeOffset(unsigned bytecodeOffset, int&amp; divot,
 248         int&amp; startOffset, int&amp; endOffset, unsigned&amp; line, unsigned&amp; column) const;
 249 
 250     Optional&lt;unsigned&gt; bytecodeOffsetFromCallSiteIndex(CallSiteIndex);
 251 
 252     void getICStatusMap(const ConcurrentJSLocker&amp;, ICStatusMap&amp; result);
 253     void getICStatusMap(ICStatusMap&amp; result);
 254 
 255 #if ENABLE(JIT)
 256     struct JITData {
 257         WTF_MAKE_STRUCT_FAST_ALLOCATED;
 258 
 259         Bag&lt;StructureStubInfo&gt; m_stubInfos;
 260         Bag&lt;JITAddIC&gt; m_addICs;
 261         Bag&lt;JITMulIC&gt; m_mulICs;
 262         Bag&lt;JITNegIC&gt; m_negICs;
 263         Bag&lt;JITSubIC&gt; m_subICs;
 264         Bag&lt;ByValInfo&gt; m_byValInfos;
 265         Bag&lt;CallLinkInfo&gt; m_callLinkInfos;
<a name="5" id="anc5"></a><span class="line-modified"> 266         SentinelLinkedList&lt;CallLinkInfo, PackedRawSentinelNode&lt;CallLinkInfo&gt;&gt; m_incomingCalls;</span>
<span class="line-modified"> 267         SentinelLinkedList&lt;PolymorphicCallNode, PackedRawSentinelNode&lt;PolymorphicCallNode&gt;&gt; m_incomingPolymorphicCalls;</span>
 268         SegmentedVector&lt;RareCaseProfile, 8&gt; m_rareCaseProfiles;
 269         std::unique_ptr&lt;PCToCodeOriginMap&gt; m_pcToCodeOriginMap;
 270         std::unique_ptr&lt;RegisterAtOffsetList&gt; m_calleeSaveRegisters;
 271         JITCodeMap m_jitCodeMap;
 272     };
 273 
 274     JITData&amp; ensureJITData(const ConcurrentJSLocker&amp; locker)
 275     {
 276         if (LIKELY(m_jitData))
 277             return *m_jitData;
 278         return ensureJITDataSlow(locker);
 279     }
 280     JITData&amp; ensureJITDataSlow(const ConcurrentJSLocker&amp;);
 281 
<a name="6" id="anc6"></a><span class="line-modified"> 282     JITAddIC* addJITAddIC(ArithProfile*);</span>
<span class="line-modified"> 283     JITMulIC* addJITMulIC(ArithProfile*);</span>
<span class="line-modified"> 284     JITNegIC* addJITNegIC(ArithProfile*);</span>
<span class="line-modified"> 285     JITSubIC* addJITSubIC(ArithProfile*);</span>
 286 
 287     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITAddGenerator&gt;::value&gt;::type&gt;
<a name="7" id="anc7"></a><span class="line-modified"> 288     JITAddIC* addMathIC(ArithProfile* profile) { return addJITAddIC(profile); }</span>
 289 
 290     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITMulGenerator&gt;::value&gt;::type&gt;
<a name="8" id="anc8"></a><span class="line-modified"> 291     JITMulIC* addMathIC(ArithProfile* profile) { return addJITMulIC(profile); }</span>
 292 
 293     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITNegGenerator&gt;::value&gt;::type&gt;
<a name="9" id="anc9"></a><span class="line-modified"> 294     JITNegIC* addMathIC(ArithProfile* profile) { return addJITNegIC(profile); }</span>
 295 
 296     template &lt;typename Generator, typename = typename std::enable_if&lt;std::is_same&lt;Generator, JITSubGenerator&gt;::value&gt;::type&gt;
<a name="10" id="anc10"></a><span class="line-modified"> 297     JITSubIC* addMathIC(ArithProfile* profile) { return addJITSubIC(profile); }</span>
 298 
 299     StructureStubInfo* addStubInfo(AccessType);
 300 
 301     // O(n) operation. Use getStubInfoMap() unless you really only intend to get one
 302     // stub info.
 303     StructureStubInfo* findStubInfo(CodeOrigin);
 304 
 305     ByValInfo* addByValInfo();
 306 
 307     CallLinkInfo* addCallLinkInfo();
 308 
 309     // This is a slow function call used primarily for compiling OSR exits in the case
 310     // that there had been inlining. Chances are if you want to use this, you&#39;re really
 311     // looking for a CallLinkInfoMap to amortize the cost of calling this.
 312     CallLinkInfo* getCallLinkInfoForBytecodeIndex(unsigned bytecodeIndex);
 313 
 314     void setJITCodeMap(JITCodeMap&amp;&amp; jitCodeMap)
 315     {
 316         ConcurrentJSLocker locker(m_lock);
 317         ensureJITData(locker).m_jitCodeMap = WTFMove(jitCodeMap);
 318     }
 319     const JITCodeMap&amp; jitCodeMap()
 320     {
 321         ConcurrentJSLocker locker(m_lock);
 322         return ensureJITData(locker).m_jitCodeMap;
 323     }
 324 
 325     void setPCToCodeOriginMap(std::unique_ptr&lt;PCToCodeOriginMap&gt;&amp;&amp;);
 326     Optional&lt;CodeOrigin&gt; findPC(void* pc);
 327 
 328     void setCalleeSaveRegisters(RegisterSet);
 329     void setCalleeSaveRegisters(std::unique_ptr&lt;RegisterAtOffsetList&gt;);
 330 
 331     RareCaseProfile* addRareCaseProfile(int bytecodeOffset);
 332     RareCaseProfile* rareCaseProfileForBytecodeOffset(const ConcurrentJSLocker&amp;, int bytecodeOffset);
 333     unsigned rareCaseProfileCountForBytecodeOffset(const ConcurrentJSLocker&amp;, int bytecodeOffset);
 334 
 335     bool likelyToTakeSlowCase(int bytecodeOffset)
 336     {
 337         if (!hasBaselineJITProfiling())
 338             return false;
 339         ConcurrentJSLocker locker(m_lock);
 340         unsigned value = rareCaseProfileCountForBytecodeOffset(locker, bytecodeOffset);
 341         return value &gt;= Options::likelyToTakeSlowCaseMinimumCount();
 342     }
 343 
 344     bool couldTakeSlowCase(int bytecodeOffset)
 345     {
 346         if (!hasBaselineJITProfiling())
 347             return false;
 348         ConcurrentJSLocker locker(m_lock);
 349         unsigned value = rareCaseProfileCountForBytecodeOffset(locker, bytecodeOffset);
 350         return value &gt;= Options::couldTakeSlowCaseMinimumCount();
 351     }
 352 
 353     // We call this when we want to reattempt compiling something with the baseline JIT. Ideally
 354     // the baseline JIT would not add data to CodeBlock, but instead it would put its data into
 355     // a newly created JITCode, which could be thrown away if we bail on JIT compilation. Then we
 356     // would be able to get rid of this silly function.
 357     // FIXME: https://bugs.webkit.org/show_bug.cgi?id=159061
 358     void resetJITData();
 359 #endif // ENABLE(JIT)
 360 
 361     void unlinkIncomingCalls();
 362 
 363 #if ENABLE(JIT)
 364     void linkIncomingCall(ExecState* callerFrame, CallLinkInfo*);
 365     void linkIncomingPolymorphicCall(ExecState* callerFrame, PolymorphicCallNode*);
 366 #endif // ENABLE(JIT)
 367 
 368     void linkIncomingCall(ExecState* callerFrame, LLIntCallLinkInfo*);
 369 
 370     const Instruction* outOfLineJumpTarget(const Instruction* pc);
 371     int outOfLineJumpOffset(const Instruction* pc);
 372     int outOfLineJumpOffset(const InstructionStream::Ref&amp; instruction)
 373     {
 374         return outOfLineJumpOffset(instruction.ptr());
 375     }
 376 
 377     inline unsigned bytecodeOffset(const Instruction* returnAddress)
 378     {
 379         const auto* instructionsBegin = instructions().at(0).ptr();
 380         const auto* instructionsEnd = reinterpret_cast&lt;const Instruction*&gt;(reinterpret_cast&lt;uintptr_t&gt;(instructionsBegin) + instructions().size());
 381         RELEASE_ASSERT(returnAddress &gt;= instructionsBegin &amp;&amp; returnAddress &lt; instructionsEnd);
 382         return returnAddress - instructionsBegin;
 383     }
 384 
 385     const InstructionStream&amp; instructions() const { return m_unlinkedCode-&gt;instructions(); }
 386 
 387     size_t predictedMachineCodeSize();
 388 
<a name="11" id="anc11"></a><span class="line-modified"> 389     unsigned instructionsSize() const { return instructions().size(); }</span>
<span class="line-added"> 390     unsigned bytecodeCost() const { return m_bytecodeCost; }</span>
 391 
 392     // Exactly equivalent to codeBlock-&gt;ownerExecutable()-&gt;newReplacementCodeBlockFor(codeBlock-&gt;specializationKind())
 393     CodeBlock* newReplacement();
 394 
 395     void setJITCode(Ref&lt;JITCode&gt;&amp;&amp; code)
 396     {
 397         ASSERT(heap()-&gt;isDeferred());
<a name="12" id="anc12"></a><span class="line-modified"> 398         if (!code-&gt;isShared())</span>
<span class="line-added"> 399             heap()-&gt;reportExtraMemoryAllocated(code-&gt;size());</span>
<span class="line-added"> 400 </span>
 401         ConcurrentJSLocker locker(m_lock);
 402         WTF::storeStoreFence(); // This is probably not needed because the lock will also do something similar, but it&#39;s good to be paranoid.
 403         m_jitCode = WTFMove(code);
 404     }
<a name="13" id="anc13"></a><span class="line-added"> 405 </span>
 406     RefPtr&lt;JITCode&gt; jitCode() { return m_jitCode; }
 407     static ptrdiff_t jitCodeOffset() { return OBJECT_OFFSETOF(CodeBlock, m_jitCode); }
<a name="14" id="anc14"></a><span class="line-modified"> 408     JITType jitType() const</span>
 409     {
 410         JITCode* jitCode = m_jitCode.get();
 411         WTF::loadLoadFence();
<a name="15" id="anc15"></a><span class="line-modified"> 412         JITType result = JITCode::jitTypeFor(jitCode);</span>
 413         WTF::loadLoadFence(); // This probably isn&#39;t needed. Oh well, paranoia is good.
 414         return result;
 415     }
 416 
 417     bool hasBaselineJITProfiling() const
 418     {
<a name="16" id="anc16"></a><span class="line-modified"> 419         return jitType() == JITType::BaselineJIT;</span>
 420     }
 421 
 422 #if ENABLE(JIT)
 423     CodeBlock* replacement();
 424 
 425     DFG::CapabilityLevel computeCapabilityLevel();
 426     DFG::CapabilityLevel capabilityLevel();
 427     DFG::CapabilityLevel capabilityLevelState() { return static_cast&lt;DFG::CapabilityLevel&gt;(m_capabilityLevelState); }
 428 
<a name="17" id="anc17"></a><span class="line-modified"> 429     bool hasOptimizedReplacement(JITType typeToReplace);</span>
 430     bool hasOptimizedReplacement(); // the typeToReplace is my JITType
 431 #endif
 432 
 433     void jettison(Profiler::JettisonReason, ReoptimizationMode = DontCountReoptimization, const FireDetail* = nullptr);
 434 
 435     ScriptExecutable* ownerExecutable() const { return m_ownerExecutable.get(); }
 436 
 437     ExecutableToCodeBlockEdge* ownerEdge() const { return m_ownerEdge.get(); }
 438 
<a name="18" id="anc18"></a><span class="line-modified"> 439     VM&amp; vm() const { return *m_vm; }</span>
 440 
 441     VirtualRegister thisRegister() const { return m_unlinkedCode-&gt;thisRegister(); }
 442 
 443     bool usesEval() const { return m_unlinkedCode-&gt;usesEval(); }
 444 
 445     void setScopeRegister(VirtualRegister scopeRegister)
 446     {
 447         ASSERT(scopeRegister.isLocal() || !scopeRegister.isValid());
 448         m_scopeRegister = scopeRegister;
 449     }
 450 
 451     VirtualRegister scopeRegister() const
 452     {
 453         return m_scopeRegister;
 454     }
 455 
 456     PutPropertySlot::Context putByIdContext() const
 457     {
 458         if (codeType() == EvalCode)
 459             return PutPropertySlot::PutByIdEval;
 460         return PutPropertySlot::PutById;
 461     }
 462 
 463     const SourceCode&amp; source() const { return m_ownerExecutable-&gt;source(); }
 464     unsigned sourceOffset() const { return m_ownerExecutable-&gt;source().startOffset(); }
 465     unsigned firstLineColumnOffset() const { return m_ownerExecutable-&gt;startColumn(); }
 466 
 467     size_t numberOfJumpTargets() const { return m_unlinkedCode-&gt;numberOfJumpTargets(); }
 468     unsigned jumpTarget(int index) const { return m_unlinkedCode-&gt;jumpTarget(index); }
 469 
 470     String nameForRegister(VirtualRegister);
 471 
 472     unsigned numberOfArgumentValueProfiles()
 473     {
 474         ASSERT(m_numParameters &gt;= 0);
<a name="19" id="anc19"></a><span class="line-modified"> 475         ASSERT(m_argumentValueProfiles.size() == static_cast&lt;unsigned&gt;(m_numParameters) || !vm().canUseJIT());</span>
 476         return m_argumentValueProfiles.size();
 477     }
 478 
 479     ValueProfile&amp; valueProfileForArgument(unsigned argumentIndex)
 480     {
<a name="20" id="anc20"></a><span class="line-modified"> 481         ASSERT(vm().canUseJIT()); // This is only called from the various JIT compilers or places that first check numberOfArgumentValueProfiles before calling this.</span>
 482         ValueProfile&amp; result = m_argumentValueProfiles[argumentIndex];
<a name="21" id="anc21"></a>
 483         return result;
 484     }
 485 
 486     ValueProfile&amp; valueProfileForBytecodeOffset(int bytecodeOffset);
 487     SpeculatedType valueProfilePredictionForBytecodeOffset(const ConcurrentJSLocker&amp;, int bytecodeOffset);
 488 
 489     template&lt;typename Functor&gt; void forEachValueProfile(const Functor&amp;);
 490     template&lt;typename Functor&gt; void forEachArrayProfile(const Functor&amp;);
 491     template&lt;typename Functor&gt; void forEachArrayAllocationProfile(const Functor&amp;);
 492     template&lt;typename Functor&gt; void forEachObjectAllocationProfile(const Functor&amp;);
 493     template&lt;typename Functor&gt; void forEachLLIntCallLinkInfo(const Functor&amp;);
 494 
 495     ArithProfile* arithProfileForBytecodeOffset(InstructionStream::Offset bytecodeOffset);
 496     ArithProfile* arithProfileForPC(const Instruction*);
 497 
 498     bool couldTakeSpecialFastCase(InstructionStream::Offset bytecodeOffset);
 499 
 500     ArrayProfile* getArrayProfile(const ConcurrentJSLocker&amp;, unsigned bytecodeOffset);
 501     ArrayProfile* getArrayProfile(unsigned bytecodeOffset);
 502 
 503     // Exception handling support
 504 
 505     size_t numberOfExceptionHandlers() const { return m_rareData ? m_rareData-&gt;m_exceptionHandlers.size() : 0; }
 506     HandlerInfo&amp; exceptionHandler(int index) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_exceptionHandlers[index]; }
 507 
 508     bool hasExpressionInfo() { return m_unlinkedCode-&gt;hasExpressionInfo(); }
 509 
 510 #if ENABLE(DFG_JIT)
 511     Vector&lt;CodeOrigin, 0, UnsafeVectorOverflow&gt;&amp; codeOrigins();
 512 
 513     // Having code origins implies that there has been some inlining.
 514     bool hasCodeOrigins()
 515     {
 516         return JITCode::isOptimizingJIT(jitType());
 517     }
 518 
 519     bool canGetCodeOrigin(CallSiteIndex index)
 520     {
 521         if (!hasCodeOrigins())
 522             return false;
 523         return index.bits() &lt; codeOrigins().size();
 524     }
 525 
 526     CodeOrigin codeOrigin(CallSiteIndex index)
 527     {
 528         return codeOrigins()[index.bits()];
 529     }
 530 
 531     CompressedLazyOperandValueProfileHolder&amp; lazyOperandValueProfiles(const ConcurrentJSLocker&amp;)
 532     {
 533         return m_lazyOperandValueProfiles;
 534     }
 535 #endif // ENABLE(DFG_JIT)
 536 
 537     // Constant Pool
 538 #if ENABLE(DFG_JIT)
 539     size_t numberOfIdentifiers() const { return m_unlinkedCode-&gt;numberOfIdentifiers() + numberOfDFGIdentifiers(); }
 540     size_t numberOfDFGIdentifiers() const;
 541     const Identifier&amp; identifier(int index) const;
 542 #else
 543     size_t numberOfIdentifiers() const { return m_unlinkedCode-&gt;numberOfIdentifiers(); }
 544     const Identifier&amp; identifier(int index) const { return m_unlinkedCode-&gt;identifier(index); }
 545 #endif
 546 
 547     Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants() { return m_constantRegisters; }
 548     Vector&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation() { return m_constantsSourceCodeRepresentation; }
 549     unsigned addConstant(JSValue v)
 550     {
 551         unsigned result = m_constantRegisters.size();
 552         m_constantRegisters.append(WriteBarrier&lt;Unknown&gt;());
 553         m_constantRegisters.last().set(*m_vm, this, v);
 554         m_constantsSourceCodeRepresentation.append(SourceCodeRepresentation::Other);
 555         return result;
 556     }
 557 
 558     unsigned addConstantLazily()
 559     {
 560         unsigned result = m_constantRegisters.size();
 561         m_constantRegisters.append(WriteBarrier&lt;Unknown&gt;());
 562         m_constantsSourceCodeRepresentation.append(SourceCodeRepresentation::Other);
 563         return result;
 564     }
 565 
 566     const Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constantRegisters() { return m_constantRegisters; }
 567     WriteBarrier&lt;Unknown&gt;&amp; constantRegister(int index) { return m_constantRegisters[index - FirstConstantRegisterIndex]; }
 568     static ALWAYS_INLINE bool isConstantRegisterIndex(int index) { return index &gt;= FirstConstantRegisterIndex; }
 569     ALWAYS_INLINE JSValue getConstant(int index) const { return m_constantRegisters[index - FirstConstantRegisterIndex].get(); }
 570     ALWAYS_INLINE SourceCodeRepresentation constantSourceCodeRepresentation(int index) const { return m_constantsSourceCodeRepresentation[index - FirstConstantRegisterIndex]; }
 571 
 572     FunctionExecutable* functionDecl(int index) { return m_functionDecls[index].get(); }
 573     int numberOfFunctionDecls() { return m_functionDecls.size(); }
 574     FunctionExecutable* functionExpr(int index) { return m_functionExprs[index].get(); }
 575 
 576     const BitVector&amp; bitVector(size_t i) { return m_unlinkedCode-&gt;bitVector(i); }
 577 
 578     Heap* heap() const { return &amp;m_vm-&gt;heap; }
 579     JSGlobalObject* globalObject() { return m_globalObject.get(); }
 580 
 581     JSGlobalObject* globalObjectFor(CodeOrigin);
 582 
 583     BytecodeLivenessAnalysis&amp; livenessAnalysis()
 584     {
 585         return m_unlinkedCode-&gt;livenessAnalysis(this);
 586     }
 587 
 588     void validate();
 589 
 590     // Jump Tables
 591 
 592     size_t numberOfSwitchJumpTables() const { return m_rareData ? m_rareData-&gt;m_switchJumpTables.size() : 0; }
 593     SimpleJumpTable&amp; addSwitchJumpTable() { createRareDataIfNecessary(); m_rareData-&gt;m_switchJumpTables.append(SimpleJumpTable()); return m_rareData-&gt;m_switchJumpTables.last(); }
 594     SimpleJumpTable&amp; switchJumpTable(int tableIndex) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_switchJumpTables[tableIndex]; }
 595     void clearSwitchJumpTables()
 596     {
 597         if (!m_rareData)
 598             return;
 599         m_rareData-&gt;m_switchJumpTables.clear();
 600     }
<a name="22" id="anc22"></a><span class="line-added"> 601 #if ENABLE(DFG_JIT)</span>
<span class="line-added"> 602     void addSwitchJumpTableFromProfiledCodeBlock(SimpleJumpTable&amp; profiled)</span>
<span class="line-added"> 603     {</span>
<span class="line-added"> 604         createRareDataIfNecessary();</span>
<span class="line-added"> 605         m_rareData-&gt;m_switchJumpTables.append(profiled.cloneNonJITPart());</span>
<span class="line-added"> 606     }</span>
<span class="line-added"> 607 #endif</span>
 608 
 609     size_t numberOfStringSwitchJumpTables() const { return m_rareData ? m_rareData-&gt;m_stringSwitchJumpTables.size() : 0; }
 610     StringJumpTable&amp; addStringSwitchJumpTable() { createRareDataIfNecessary(); m_rareData-&gt;m_stringSwitchJumpTables.append(StringJumpTable()); return m_rareData-&gt;m_stringSwitchJumpTables.last(); }
 611     StringJumpTable&amp; stringSwitchJumpTable(int tableIndex) { RELEASE_ASSERT(m_rareData); return m_rareData-&gt;m_stringSwitchJumpTables[tableIndex]; }
 612 
 613     DirectEvalCodeCache&amp; directEvalCodeCache() { createRareDataIfNecessary(); return m_rareData-&gt;m_directEvalCodeCache; }
 614 
 615     enum ShrinkMode {
 616         // Shrink prior to generating machine code that may point directly into vectors.
 617         EarlyShrink,
 618 
 619         // Shrink after generating machine code, and after possibly creating new vectors
 620         // and appending to others. At this time it is not safe to shrink certain vectors
 621         // because we would have generated machine code that references them directly.
 622         LateShrink
 623     };
 624     void shrinkToFit(ShrinkMode);
 625 
 626     // Functions for controlling when JITting kicks in, in a mixed mode
 627     // execution world.
 628 
 629     bool checkIfJITThresholdReached()
 630     {
 631         return m_llintExecuteCounter.checkIfThresholdCrossedAndSet(this);
 632     }
 633 
 634     void dontJITAnytimeSoon()
 635     {
 636         m_llintExecuteCounter.deferIndefinitely();
 637     }
 638 
 639     int32_t thresholdForJIT(int32_t threshold);
 640     void jitAfterWarmUp();
 641     void jitSoon();
 642 
 643     const BaselineExecutionCounter&amp; llintExecuteCounter() const
 644     {
 645         return m_llintExecuteCounter;
 646     }
 647 
<a name="23" id="anc23"></a><span class="line-modified"> 648     typedef HashMap&lt;std::tuple&lt;StructureID, unsigned&gt;, Vector&lt;LLIntPrototypeLoadAdaptiveStructureWatchpoint&gt;&gt; StructureWatchpointMap;</span>
 649     StructureWatchpointMap&amp; llintGetByIdWatchpointMap() { return m_llintGetByIdWatchpointMap; }
 650 
 651     // Functions for controlling when tiered compilation kicks in. This
 652     // controls both when the optimizing compiler is invoked and when OSR
 653     // entry happens. Two triggers exist: the loop trigger and the return
 654     // trigger. In either case, when an addition to m_jitExecuteCounter
 655     // causes it to become non-negative, the optimizing compiler is
 656     // invoked. This includes a fast check to see if this CodeBlock has
 657     // already been optimized (i.e. replacement() returns a CodeBlock
 658     // that was optimized with a higher tier JIT than this one). In the
 659     // case of the loop trigger, if the optimized compilation succeeds
 660     // (or has already succeeded in the past) then OSR is attempted to
 661     // redirect program flow into the optimized code.
 662 
 663     // These functions are called from within the optimization triggers,
 664     // and are used as a single point at which we define the heuristics
 665     // for how much warm-up is mandated before the next optimization
 666     // trigger files. All CodeBlocks start out with optimizeAfterWarmUp(),
 667     // as this is called from the CodeBlock constructor.
 668 
 669     // When we observe a lot of speculation failures, we trigger a
 670     // reoptimization. But each time, we increase the optimization trigger
 671     // to avoid thrashing.
 672     JS_EXPORT_PRIVATE unsigned reoptimizationRetryCounter() const;
 673     void countReoptimization();
 674 
 675 #if !ENABLE(C_LOOP)
 676     const RegisterAtOffsetList* calleeSaveRegisters() const;
 677 
 678     static unsigned numberOfLLIntBaselineCalleeSaveRegisters() { return RegisterSet::llintBaselineCalleeSaveRegisters().numberOfSetRegisters(); }
 679     static size_t llintBaselineCalleeSaveSpaceAsVirtualRegisters();
 680     size_t calleeSaveSpaceAsVirtualRegisters();
 681 #else
 682     static unsigned numberOfLLIntBaselineCalleeSaveRegisters() { return 0; }
 683     static size_t llintBaselineCalleeSaveSpaceAsVirtualRegisters() { return 1; };
 684     size_t calleeSaveSpaceAsVirtualRegisters() { return 0; }
 685 #endif
 686 
 687 #if ENABLE(JIT)
 688     unsigned numberOfDFGCompiles();
 689 
 690     int32_t codeTypeThresholdMultiplier() const;
 691 
 692     int32_t adjustedCounterValue(int32_t desiredThreshold);
 693 
 694     int32_t* addressOfJITExecuteCounter()
 695     {
 696         return &amp;m_jitExecuteCounter.m_counter;
 697     }
 698 
 699     static ptrdiff_t offsetOfJITExecuteCounter() { return OBJECT_OFFSETOF(CodeBlock, m_jitExecuteCounter) + OBJECT_OFFSETOF(BaselineExecutionCounter, m_counter); }
 700     static ptrdiff_t offsetOfJITExecutionActiveThreshold() { return OBJECT_OFFSETOF(CodeBlock, m_jitExecuteCounter) + OBJECT_OFFSETOF(BaselineExecutionCounter, m_activeThreshold); }
 701     static ptrdiff_t offsetOfJITExecutionTotalCount() { return OBJECT_OFFSETOF(CodeBlock, m_jitExecuteCounter) + OBJECT_OFFSETOF(BaselineExecutionCounter, m_totalCount); }
 702 
 703     const BaselineExecutionCounter&amp; jitExecuteCounter() const { return m_jitExecuteCounter; }
 704 
 705     unsigned optimizationDelayCounter() const { return m_optimizationDelayCounter; }
 706 
 707     // Check if the optimization threshold has been reached, and if not,
 708     // adjust the heuristics accordingly. Returns true if the threshold has
 709     // been reached.
 710     bool checkIfOptimizationThresholdReached();
 711 
 712     // Call this to force the next optimization trigger to fire. This is
 713     // rarely wise, since optimization triggers are typically more
 714     // expensive than executing baseline code.
 715     void optimizeNextInvocation();
 716 
 717     // Call this to prevent optimization from happening again. Note that
 718     // optimization will still happen after roughly 2^29 invocations,
 719     // so this is really meant to delay that as much as possible. This
 720     // is called if optimization failed, and we expect it to fail in
 721     // the future as well.
 722     void dontOptimizeAnytimeSoon();
 723 
 724     // Call this to reinitialize the counter to its starting state,
 725     // forcing a warm-up to happen before the next optimization trigger
 726     // fires. This is called in the CodeBlock constructor. It also
 727     // makes sense to call this if an OSR exit occurred. Note that
 728     // OSR exit code is code generated, so the value of the execute
 729     // counter that this corresponds to is also available directly.
 730     void optimizeAfterWarmUp();
 731 
 732     // Call this to force an optimization trigger to fire only after
 733     // a lot of warm-up.
 734     void optimizeAfterLongWarmUp();
 735 
 736     // Call this to cause an optimization trigger to fire soon, but
 737     // not necessarily the next one. This makes sense if optimization
 738     // succeeds. Successful optimization means that all calls are
 739     // relinked to the optimized code, so this only affects call
 740     // frames that are still executing this CodeBlock. The value here
 741     // is tuned to strike a balance between the cost of OSR entry
 742     // (which is too high to warrant making every loop back edge to
 743     // trigger OSR immediately) and the cost of executing baseline
 744     // code (which is high enough that we don&#39;t necessarily want to
 745     // have a full warm-up). The intuition for calling this instead of
 746     // optimizeNextInvocation() is for the case of recursive functions
 747     // with loops. Consider that there may be N call frames of some
 748     // recursive function, for a reasonably large value of N. The top
 749     // one triggers optimization, and then returns, and then all of
 750     // the others return. We don&#39;t want optimization to be triggered on
 751     // each return, as that would be superfluous. It only makes sense
 752     // to trigger optimization if one of those functions becomes hot
 753     // in the baseline code.
 754     void optimizeSoon();
 755 
 756     void forceOptimizationSlowPathConcurrently();
 757 
 758     void setOptimizationThresholdBasedOnCompilationResult(CompilationResult);
 759 
 760     uint32_t osrExitCounter() const { return m_osrExitCounter; }
 761 
 762     void countOSRExit() { m_osrExitCounter++; }
 763 
 764     enum class OptimizeAction { None, ReoptimizeNow };
 765 #if ENABLE(DFG_JIT)
 766     OptimizeAction updateOSRExitCounterAndCheckIfNeedToReoptimize(DFG::OSRExitState&amp;);
 767 #endif
 768 
 769     static ptrdiff_t offsetOfOSRExitCounter() { return OBJECT_OFFSETOF(CodeBlock, m_osrExitCounter); }
 770 
 771     uint32_t adjustedExitCountThreshold(uint32_t desiredThreshold);
 772     uint32_t exitCountThresholdForReoptimization();
 773     uint32_t exitCountThresholdForReoptimizationFromLoop();
 774     bool shouldReoptimizeNow();
 775     bool shouldReoptimizeFromLoopNow();
 776 
 777 #else // No JIT
 778     void optimizeAfterWarmUp() { }
 779     unsigned numberOfDFGCompiles() { return 0; }
 780 #endif
 781 
 782     bool shouldOptimizeNow();
 783     void updateAllValueProfilePredictions();
 784     void updateAllArrayPredictions();
 785     void updateAllPredictions();
 786 
 787     unsigned frameRegisterCount();
 788     int stackPointerOffset();
 789 
 790     bool hasOpDebugForLineAndColumn(unsigned line, unsigned column);
 791 
 792     bool hasDebuggerRequests() const { return m_debuggerRequests; }
 793     void* debuggerRequestsAddress() { return &amp;m_debuggerRequests; }
 794 
 795     void addBreakpoint(unsigned numBreakpoints);
 796     void removeBreakpoint(unsigned numBreakpoints)
 797     {
 798         ASSERT(m_numBreakpoints &gt;= numBreakpoints);
 799         m_numBreakpoints -= numBreakpoints;
 800     }
 801 
 802     enum SteppingMode {
 803         SteppingModeDisabled,
 804         SteppingModeEnabled
 805     };
 806     void setSteppingMode(SteppingMode);
 807 
 808     void clearDebuggerRequests()
 809     {
 810         m_steppingMode = SteppingModeDisabled;
 811         m_numBreakpoints = 0;
 812     }
 813 
 814     bool wasCompiledWithDebuggingOpcodes() const { return m_unlinkedCode-&gt;wasCompiledWithDebuggingOpcodes(); }
 815 
 816     // This is intentionally public; it&#39;s the responsibility of anyone doing any
 817     // of the following to hold the lock:
 818     //
 819     // - Modifying any inline cache in this code block.
 820     //
 821     // - Quering any inline cache in this code block, from a thread other than
 822     //   the main thread.
 823     //
 824     // Additionally, it&#39;s only legal to modify the inline cache on the main
 825     // thread. This means that the main thread can query the inline cache without
 826     // locking. This is crucial since executing the inline cache is effectively
 827     // &quot;querying&quot; it.
 828     //
 829     // Another exception to the rules is that the GC can do whatever it wants
 830     // without holding any locks, because the GC is guaranteed to wait until any
 831     // concurrent compilation threads finish what they&#39;re doing.
 832     mutable ConcurrentJSLock m_lock;
 833 
 834     bool m_shouldAlwaysBeInlined; // Not a bitfield because the JIT wants to store to it.
 835 
 836 #if ENABLE(JIT)
 837     unsigned m_capabilityLevelState : 2; // DFG::CapabilityLevel
 838 #endif
 839 
 840     bool m_allTransitionsHaveBeenMarked : 1; // Initialized and used on every GC.
 841 
 842     bool m_didFailJITCompilation : 1;
 843     bool m_didFailFTLCompilation : 1;
 844     bool m_hasBeenCompiledWithFTL : 1;
 845 
 846     // Internal methods for use by validation code. It would be private if it wasn&#39;t
 847     // for the fact that we use it from anonymous namespaces.
 848     void beginValidationDidFail();
 849     NO_RETURN_DUE_TO_CRASH void endValidationDidFail();
 850 
 851     struct RareData {
 852         WTF_MAKE_FAST_ALLOCATED;
 853     public:
 854         Vector&lt;HandlerInfo&gt; m_exceptionHandlers;
 855 
 856         // Jump Tables
 857         Vector&lt;SimpleJumpTable&gt; m_switchJumpTables;
 858         Vector&lt;StringJumpTable&gt; m_stringSwitchJumpTables;
 859 
 860         Vector&lt;std::unique_ptr&lt;ValueProfileAndOperandBuffer&gt;&gt; m_catchProfiles;
 861 
 862         DirectEvalCodeCache m_directEvalCodeCache;
 863     };
 864 
 865     void clearExceptionHandlers()
 866     {
 867         if (m_rareData)
 868             m_rareData-&gt;m_exceptionHandlers.clear();
 869     }
 870 
 871     void appendExceptionHandler(const HandlerInfo&amp; handler)
 872     {
 873         createRareDataIfNecessary(); // We may be handling the exception of an inlined call frame.
 874         m_rareData-&gt;m_exceptionHandlers.append(handler);
 875     }
 876 
 877     DisposableCallSiteIndex newExceptionHandlingCallSiteIndex(CallSiteIndex originalCallSite);
 878 
 879     void ensureCatchLivenessIsComputedForBytecodeOffset(InstructionStream::Offset bytecodeOffset);
 880 
 881     bool hasTailCalls() const { return m_unlinkedCode-&gt;hasTailCalls(); }
 882 
 883     template&lt;typename Metadata&gt;
 884     Metadata&amp; metadata(OpcodeID opcodeID, unsigned metadataID)
 885     {
 886         ASSERT(m_metadata);
 887         return bitwise_cast&lt;Metadata*&gt;(m_metadata-&gt;get(opcodeID))[metadataID];
 888     }
 889 
 890     size_t metadataSizeInBytes()
 891     {
 892         return m_unlinkedCode-&gt;metadataSizeInBytes();
 893     }
 894 
 895 protected:
 896     void finalizeLLIntInlineCaches();
 897 #if ENABLE(JIT)
 898     void finalizeBaselineJITInlineCaches();
 899 #endif
 900 #if ENABLE(DFG_JIT)
 901     void tallyFrequentExitSites();
 902 #else
 903     void tallyFrequentExitSites() { }
 904 #endif
 905 
 906 private:
 907     friend class CodeBlockSet;
 908     friend class ExecutableToCodeBlockEdge;
 909 
 910     BytecodeLivenessAnalysis&amp; livenessAnalysisSlow();
 911 
 912     CodeBlock* specialOSREntryBlockOrNull();
 913 
 914     void noticeIncomingCall(ExecState* callerFrame);
 915 
 916     double optimizationThresholdScalingFactor();
 917 
<a name="24" id="anc24"></a><span class="line-modified"> 918     void updateAllValueProfilePredictionsAndCountLiveness(unsigned&amp; numberOfLiveNonArgumentValueProfiles, unsigned&amp; numberOfSamplesInProfiles);</span>
 919 
 920     void setConstantIdentifierSetRegisters(VM&amp;, const Vector&lt;ConstantIdentifierSetEntry&gt;&amp; constants);
 921 
<a name="25" id="anc25"></a><span class="line-modified"> 922     void setConstantRegisters(const Vector&lt;WriteBarrier&lt;Unknown&gt;&gt;&amp; constants, const Vector&lt;SourceCodeRepresentation&gt;&amp; constantsSourceCodeRepresentation, ScriptExecutable* topLevelExecutable);</span>
 923 
 924     void replaceConstant(int index, JSValue value)
 925     {
 926         ASSERT(isConstantRegisterIndex(index) &amp;&amp; static_cast&lt;size_t&gt;(index - FirstConstantRegisterIndex) &lt; m_constantRegisters.size());
 927         m_constantRegisters[index - FirstConstantRegisterIndex].set(*m_vm, this, value);
 928     }
 929 
 930     bool shouldVisitStrongly(const ConcurrentJSLocker&amp;);
<a name="26" id="anc26"></a><span class="line-modified"> 931     bool shouldJettisonDueToWeakReference(VM&amp;);</span>
 932     bool shouldJettisonDueToOldAge(const ConcurrentJSLocker&amp;);
 933 
 934     void propagateTransitions(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 935     void determineLiveness(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 936 
 937     void stronglyVisitStrongReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 938     void stronglyVisitWeakReferences(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 939     void visitOSRExitTargets(const ConcurrentJSLocker&amp;, SlotVisitor&amp;);
 940 
 941     unsigned numberOfNonArgumentValueProfiles() { return m_numberOfNonArgumentValueProfiles; }
 942     unsigned totalNumberOfValueProfiles() { return numberOfArgumentValueProfiles() + numberOfNonArgumentValueProfiles(); }
 943     ValueProfile* tryGetValueProfileForBytecodeOffset(int bytecodeOffset);
 944 
 945     Seconds timeSinceCreation()
 946     {
 947         return MonotonicTime::now() - m_creationTime;
 948     }
 949 
 950     void createRareDataIfNecessary()
 951     {
 952         if (!m_rareData) {
<a name="27" id="anc27"></a><span class="line-modified"> 953             auto rareData = makeUnique&lt;RareData&gt;();</span>
 954             WTF::storeStoreFence(); // m_catchProfiles can be touched from compiler threads.
 955             m_rareData = WTFMove(rareData);
 956         }
 957     }
 958 
 959     void insertBasicBlockBoundariesForControlFlowProfiler();
 960     void ensureCatchLivenessIsComputedForBytecodeOffsetSlow(const OpCatch&amp;, InstructionStream::Offset);
 961 
 962     int m_numCalleeLocals;
 963     int m_numVars;
 964     int m_numParameters;
 965     int m_numberOfArgumentsToSkip { 0 };
 966     unsigned m_numberOfNonArgumentValueProfiles { 0 };
 967     union {
 968         unsigned m_debuggerRequests;
 969         struct {
 970             unsigned m_hasDebuggerStatement : 1;
 971             unsigned m_steppingMode : 1;
 972             unsigned m_numBreakpoints : 30;
 973         };
 974     };
<a name="28" id="anc28"></a><span class="line-modified"> 975     unsigned m_bytecodeCost { 0 };</span>
 976     VirtualRegister m_scopeRegister;
 977     mutable CodeBlockHash m_hash;
 978 
 979     WriteBarrier&lt;UnlinkedCodeBlock&gt; m_unlinkedCode;
 980     WriteBarrier&lt;ScriptExecutable&gt; m_ownerExecutable;
 981     WriteBarrier&lt;ExecutableToCodeBlockEdge&gt; m_ownerEdge;
<a name="29" id="anc29"></a><span class="line-added"> 982     // m_vm must be a pointer (instead of a reference) because the JSCLLIntOffsetsExtractor</span>
<span class="line-added"> 983     // cannot handle it being a reference.</span>
 984     VM* m_vm;
 985 
 986     const void* m_instructionsRawPointer { nullptr };
<a name="30" id="anc30"></a><span class="line-modified"> 987     SentinelLinkedList&lt;LLIntCallLinkInfo, PackedRawSentinelNode&lt;LLIntCallLinkInfo&gt;&gt; m_incomingLLIntCalls;</span>
 988     StructureWatchpointMap m_llintGetByIdWatchpointMap;
 989     RefPtr&lt;JITCode&gt; m_jitCode;
 990 #if ENABLE(JIT)
 991     std::unique_ptr&lt;JITData&gt; m_jitData;
 992 #endif
 993 #if ENABLE(DFG_JIT)
 994     // This is relevant to non-DFG code blocks that serve as the profiled code block
 995     // for DFG code blocks.
 996     CompressedLazyOperandValueProfileHolder m_lazyOperandValueProfiles;
 997 #endif
 998     RefCountedArray&lt;ValueProfile&gt; m_argumentValueProfiles;
 999 
1000     // Constant Pool
1001     COMPILE_ASSERT(sizeof(Register) == sizeof(WriteBarrier&lt;Unknown&gt;), Register_must_be_same_size_as_WriteBarrier_Unknown);
1002     // TODO: This could just be a pointer to m_unlinkedCodeBlock&#39;s data, but the DFG mutates
1003     // it, so we&#39;re stuck with it for now.
1004     Vector&lt;WriteBarrier&lt;Unknown&gt;&gt; m_constantRegisters;
1005     Vector&lt;SourceCodeRepresentation&gt; m_constantsSourceCodeRepresentation;
1006     RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt; m_functionDecls;
1007     RefCountedArray&lt;WriteBarrier&lt;FunctionExecutable&gt;&gt; m_functionExprs;
1008 
1009     WriteBarrier&lt;CodeBlock&gt; m_alternative;
1010 
1011     BaselineExecutionCounter m_llintExecuteCounter;
1012 
1013     BaselineExecutionCounter m_jitExecuteCounter;
1014     uint32_t m_osrExitCounter;
1015 
1016     uint16_t m_optimizationDelayCounter;
1017     uint16_t m_reoptimizationRetryCounter;
1018 
1019     RefPtr&lt;MetadataTable&gt; m_metadata;
1020 
1021     MonotonicTime m_creationTime;
<a name="31" id="anc31"></a><span class="line-added">1022     double m_previousCounter { 0 };</span>
1023 
1024     std::unique_ptr&lt;RareData&gt; m_rareData;
1025 };
1026 
1027 inline Register&amp; ExecState::r(int index)
1028 {
1029     CodeBlock* codeBlock = this-&gt;codeBlock();
1030     if (codeBlock-&gt;isConstantRegisterIndex(index))
1031         return *reinterpret_cast&lt;Register*&gt;(&amp;codeBlock-&gt;constantRegister(index));
1032     return this[index];
1033 }
1034 
1035 inline Register&amp; ExecState::r(VirtualRegister reg)
1036 {
1037     return r(reg.offset());
1038 }
1039 
1040 inline Register&amp; ExecState::uncheckedR(int index)
1041 {
1042     RELEASE_ASSERT(index &lt; FirstConstantRegisterIndex);
1043     return this[index];
1044 }
1045 
1046 inline Register&amp; ExecState::uncheckedR(VirtualRegister reg)
1047 {
1048     return uncheckedR(reg.offset());
1049 }
1050 
1051 template &lt;typename ExecutableType&gt;
<a name="32" id="anc32"></a><span class="line-modified">1052 Exception* ScriptExecutable::prepareForExecution(VM&amp; vm, JSFunction* function, JSScope* scope, CodeSpecializationKind kind, CodeBlock*&amp; resultCodeBlock)</span>
1053 {
1054     if (hasJITCodeFor(kind)) {
1055         if (std::is_same&lt;ExecutableType, EvalExecutable&gt;::value)
1056             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;EvalExecutable*&gt;(this)-&gt;codeBlock());
1057         else if (std::is_same&lt;ExecutableType, ProgramExecutable&gt;::value)
1058             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;ProgramExecutable*&gt;(this)-&gt;codeBlock());
1059         else if (std::is_same&lt;ExecutableType, ModuleProgramExecutable&gt;::value)
1060             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;ModuleProgramExecutable*&gt;(this)-&gt;codeBlock());
1061         else if (std::is_same&lt;ExecutableType, FunctionExecutable&gt;::value)
1062             resultCodeBlock = jsCast&lt;CodeBlock*&gt;(jsCast&lt;FunctionExecutable*&gt;(this)-&gt;codeBlockFor(kind));
1063         else
1064             RELEASE_ASSERT_NOT_REACHED();
1065         return nullptr;
1066     }
1067     return prepareForExecutionImpl(vm, function, scope, kind, resultCodeBlock);
1068 }
1069 
1070 #define CODEBLOCK_LOG_EVENT(codeBlock, summary, details) \
<a name="33" id="anc33"></a><span class="line-modified">1071     do { \</span>
<span class="line-added">1072         if (codeBlock) \</span>
<span class="line-added">1073             (codeBlock-&gt;vm().logEvent(codeBlock, summary, [&amp;] () { return toCString details; })); \</span>
<span class="line-added">1074     } while (0)</span>
1075 
1076 
1077 void setPrinter(Printer::PrintRecord&amp;, CodeBlock*);
1078 
1079 } // namespace JSC
1080 
1081 namespace WTF {
1082 
1083 JS_EXPORT_PRIVATE void printInternal(PrintStream&amp;, JSC::CodeBlock*);
1084 
1085 } // namespace WTF
<a name="34" id="anc34"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="34" type="hidden" />
</body>
</html>