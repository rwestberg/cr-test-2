diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/B3LowerToAir.cpp b/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/B3LowerToAir.cpp
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/B3LowerToAir.cpp
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/b3/B3LowerToAir.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (C) 2015-2017 Apple Inc. All rights reserved.
+ * Copyright (C) 2015-2019 Apple Inc. All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions
  * are met:
  * 1. Redistributions of source code must retain the above copyright
@@ -41,10 +41,11 @@
 #include "B3BlockWorklist.h"
 #include "B3CCallValue.h"
 #include "B3CheckSpecial.h"
 #include "B3Commutativity.h"
 #include "B3Dominators.h"
+#include "B3ExtractValue.h"
 #include "B3FenceValue.h"
 #include "B3MemoryValueInlines.h"
 #include "B3PatchpointSpecial.h"
 #include "B3PatchpointValue.h"
 #include "B3PhaseScope.h"
@@ -113,27 +114,54 @@
     {
         using namespace Air;
         for (B3::BasicBlock* block : m_procedure)
             m_blockToBlock[block] = m_code.addBlock(block->frequency());
 
+        auto ensureTupleTmps = [&] (Value* tupleValue, auto& hashTable) {
+            hashTable.ensure(tupleValue, [&] {
+                const auto tuple = m_procedure.tupleForType(tupleValue->type());
+                Vector<Tmp> tmps(tuple.size());
+
+                for (unsigned i = 0; i < tuple.size(); ++i)
+                    tmps[i] = tmpForType(tuple[i]);
+                return tmps;
+            });
+        };
+
         for (Value* value : m_procedure.values()) {
             switch (value->opcode()) {
             case Phi: {
+                if (value->type().isTuple()) {
+                    ensureTupleTmps(value, m_tuplePhiToTmps);
+                    ensureTupleTmps(value, m_tupleValueToTmps);
+                    break;
+                }
+
                 m_phiToTmp[value] = m_code.newTmp(value->resultBank());
                 if (B3LowerToAirInternal::verbose)
                     dataLog("Phi tmp for ", *value, ": ", m_phiToTmp[value], "\n");
                 break;
             }
+            case Get:
+            case Patchpoint: {
+                if (value->type().isTuple())
+                    ensureTupleTmps(value, m_tupleValueToTmps);
+                break;
+            }
             default:
                 break;
             }
         }
 
         for (B3::StackSlot* stack : m_procedure.stackSlots())
             m_stackToStack.add(stack, m_code.addStackSlot(stack));
-        for (Variable* variable : m_procedure.variables())
-            m_variableToTmp.add(variable, m_code.newTmp(variable->bank()));
+        for (Variable* variable : m_procedure.variables()) {
+            auto addResult = m_variableToTmps.add(variable, Vector<Tmp, 1>(m_procedure.returnCount(variable->type())));
+            ASSERT(addResult.isNewEntry);
+            for (unsigned i = 0; i < m_procedure.returnCount(variable->type()); ++i)
+                addResult.iterator->value[i] = tmpForType(variable->type().isNumeric() ? variable->type() : m_procedure.extractFromTuple(variable->type(), i));
+        }
 
         // Figure out which blocks are not rare.
         m_fastWorklist.push(m_procedure[0]);
         while (B3::BasicBlock* block = m_fastWorklist.pop()) {
             for (B3::FrequentedBlock& successor : block->successors()) {
@@ -395,10 +423,33 @@
     ArgPromise tmpPromise(Value* value)
     {
         return ArgPromise::tmp(value);
     }
 
+    Tmp tmpForType(Type type)
+    {
+        return m_code.newTmp(bankForType(type));
+    }
+
+    const Vector<Tmp>& tmpsForTuple(Value* tupleValue)
+    {
+        ASSERT(tupleValue->type().isTuple());
+
+        switch (tupleValue->opcode()) {
+        case Phi:
+        case Patchpoint: {
+            return m_tupleValueToTmps.find(tupleValue)->value;
+        }
+        case Get:
+        case Set:
+            return m_variableToTmps.find(tupleValue->as<VariableValue>()->variable())->value;
+        default:
+            break;
+        }
+        RELEASE_ASSERT_NOT_REACHED();
+    }
+
     bool canBeInternal(Value* value)
     {
         // If one of the internal things has already been computed, then we don't want to cause
         // it to be recomputed again.
         if (m_valueToTmp[value])
@@ -655,16 +706,31 @@
         if (Arg result = imm(value))
             return result;
         return tmp(value);
     }
 
+    template<typename Functor>
+    void forEachImmOrTmp(Value* value, const Functor& func)
+    {
+        ASSERT(value->type() != Void);
+        if (!value->type().isTuple()) {
+            func(immOrTmp(value), value->type(), 0);
+            return;
+        }
+
+        const Vector<Type>& tuple = m_procedure.tupleForType(value->type());
+        const auto& tmps = tmpsForTuple(value);
+        for (unsigned i = 0; i < tuple.size(); ++i)
+            func(tmps[i], tuple[i], i);
+    }
+
     // By convention, we use Oops to mean "I don't know".
     Air::Opcode tryOpcodeForType(
         Air::Opcode opcode32, Air::Opcode opcode64, Air::Opcode opcodeDouble, Air::Opcode opcodeFloat, Type type)
     {
         Air::Opcode opcode;
-        switch (type) {
+        switch (type.kind()) {
         case Int32:
             opcode = opcode32;
             break;
         case Int64:
             opcode = opcode64;
@@ -1108,31 +1174,32 @@
     }
 
     Air::Opcode moveForType(Type type)
     {
         using namespace Air;
-        switch (type) {
+        switch (type.kind()) {
         case Int32:
             return Move32;
         case Int64:
             RELEASE_ASSERT(is64Bit());
             return Move;
         case Float:
             return MoveFloat;
         case Double:
             return MoveDouble;
         case Void:
+        case Tuple:
             break;
         }
         RELEASE_ASSERT_NOT_REACHED();
         return Air::Oops;
     }
 
     Air::Opcode relaxedMoveForType(Type type)
     {
         using namespace Air;
-        switch (type) {
+        switch (type.kind()) {
         case Int32:
         case Int64:
             // For Int32, we could return Move or Move32. It's a trade-off.
             //
             // Move32: Using Move32 guarantees that we use the narrower move, but in cases where the
@@ -1153,10 +1220,11 @@
             // should use MoveFloat when we know that the temporaries involved are 32-bit.
             return MoveFloat;
         case Double:
             return MoveDouble;
         case Void:
+        case Tuple:
             break;
         }
         RELEASE_ASSERT_NOT_REACHED();
         return Air::Oops;
     }
@@ -1171,11 +1239,11 @@
 
     template<typename... Arguments>
     void print(Value* origin, Arguments&&... arguments)
     {
         auto printList = Printer::makePrintRecordList(arguments...);
-        auto printSpecial = static_cast<Air::PrintSpecial*>(m_code.addSpecial(std::make_unique<Air::PrintSpecial>(printList)));
+        auto printSpecial = static_cast<Air::PrintSpecial*>(m_code.addSpecial(makeUnique<Air::PrintSpecial>(printList)));
         Inst inst(Air::Patch, origin, Arg::special(printSpecial));
         Printer::appendAirArgs(inst, std::forward<Arguments>(arguments)...);
         append(WTFMove(inst));
     }
 #endif // ENABLE(MASM_PROBE)
@@ -1238,11 +1306,11 @@
     template<typename T, typename... Arguments>
     T* ensureSpecial(T*& field, Arguments&&... arguments)
     {
         if (!field) {
             field = static_cast<T*>(
-                m_code.addSpecial(std::make_unique<T>(std::forward<Arguments>(arguments)...)));
+                m_code.addSpecial(makeUnique<T>(std::forward<Arguments>(arguments)...)));
         }
         return field;
     }
 
     template<typename... Arguments>
@@ -1268,14 +1336,18 @@
                 else if (value.value()->hasInt64())
                     arg = Arg::bigImm(value.value()->asInt64());
                 else if (value.value()->hasDouble() && canBeInternal(value.value())) {
                     commitInternal(value.value());
                     arg = Arg::bigImm(bitwise_cast<int64_t>(value.value()->asDouble()));
+                } else if (value.value()->hasFloat() && canBeInternal(value.value())) {
+                    commitInternal(value.value());
+                    arg = Arg::bigImm(static_cast<uint64_t>(bitwise_cast<uint32_t>(value.value()->asFloat())));
                 } else
                     arg = tmp(value.value());
                 break;
             case ValueRep::SomeRegister:
+            case ValueRep::SomeLateRegister:
                 arg = tmp(value.value());
                 break;
             case ValueRep::SomeRegisterWithClobber: {
                 Tmp dstTmp = m_code.newTmp(value.value()->resultBank());
                 append(relaxedMoveForType(value.value()->type()), immOrTmp(value.value()), dstTmp);
@@ -1458,11 +1530,11 @@
             Arg relCond = Arg::relCond(relationalCondition).inverted(inverted);
             Arg doubleCond = Arg::doubleCond(doubleCondition).inverted(inverted);
             Value* left = value->child(0);
             Value* right = value->child(1);
 
-            if (isInt(value->child(0)->type())) {
+            if (value->child(0)->type().isInt()) {
                 Arg rightImm = imm(right);
 
                 auto tryCompare = [&] (
                     Width width, ArgPromise&& left, ArgPromise&& right) -> Inst {
                     if (Inst result = compare(width, relCond, left, right))
@@ -2122,11 +2194,11 @@
     void appendX86Div(B3::Opcode op)
     {
         using namespace Air;
         Air::Opcode convertToDoubleWord;
         Air::Opcode div;
-        switch (m_value->type()) {
+        switch (m_value->type().kind()) {
         case Int32:
             convertToDoubleWord = X86ConvertToDoubleWord32;
             div = X86Div32;
             break;
         case Int64:
@@ -2446,11 +2518,11 @@
             Air::Kind kind = moveForType(memory->type());
             if (memory->hasFence()) {
                 if (isX86())
                     kind.effects = true;
                 else {
-                    switch (memory->type()) {
+                    switch (memory->type().kind()) {
                     case Int32:
                         kind = LoadAcq32;
                         break;
                     case Int64:
                         kind = LoadAcq64;
@@ -2599,35 +2671,56 @@
             appendUnOp<Neg32, Neg64, NegateDouble, NegateFloat>(m_value->child(0));
             return;
         }
 
         case Mul: {
+            if (m_value->type() == Int64
+                && isValidForm(MultiplySignExtend32, Arg::Tmp, Arg::Tmp, Arg::Tmp)
+                && m_value->child(0)->opcode() == SExt32
+                && !m_locked.contains(m_value->child(0))) {
+                Value* opLeft = m_value->child(0);
+                Value* left = opLeft->child(0);
+                Value* opRight = m_value->child(1);
+                Value* right = nullptr;
+
+                if (opRight->opcode() == SExt32 && !m_locked.contains(opRight->child(0))) {
+                    right = opRight->child(0);
+                } else if (m_value->child(1)->isRepresentableAs<int32_t>() && !m_locked.contains(m_value->child(1))) {
+                    // We just use the 64-bit const int as a 32 bit const int directly
+                    right = opRight;
+                }
+
+                if (right) {
+                    append(MultiplySignExtend32, tmp(left), tmp(right), tmp(m_value));
+                    return;
+                }
+            }
             appendBinOp<Mul32, Mul64, MulDouble, MulFloat, Commutative>(
                 m_value->child(0), m_value->child(1));
             return;
         }
 
         case Div: {
             if (m_value->isChill())
                 RELEASE_ASSERT(isARM64());
-            if (isInt(m_value->type()) && isX86()) {
+            if (m_value->type().isInt() && isX86()) {
                 appendX86Div(Div);
                 return;
             }
-            ASSERT(!isX86() || isFloat(m_value->type()));
+            ASSERT(!isX86() || m_value->type().isFloat());
 
             appendBinOp<Div32, Div64, DivDouble, DivFloat>(m_value->child(0), m_value->child(1));
             return;
         }
 
         case UDiv: {
-            if (isInt(m_value->type()) && isX86()) {
+            if (m_value->type().isInt() && isX86()) {
                 appendX86UDiv(UDiv);
                 return;
             }
 
-            ASSERT(!isX86() && !isFloat(m_value->type()));
+            ASSERT(!isX86() && !m_value->type().isFloat());
 
             appendBinOp<UDiv32, UDiv64, Air::Oops, Air::Oops>(m_value->child(0), m_value->child(1));
             return;
 
         }
@@ -2986,11 +3079,11 @@
             return;
         }
 
         case Select: {
             MoveConditionallyConfig config;
-            if (isInt(m_value->type())) {
+            if (m_value->type().isInt()) {
                 config.moveConditionally32 = MoveConditionally32;
                 config.moveConditionally64 = MoveConditionally64;
                 config.moveConditionallyTest32 = MoveConditionallyTest32;
                 config.moveConditionallyTest64 = MoveConditionallyTest64;
                 config.moveConditionallyDouble = MoveConditionallyDouble;
@@ -3051,43 +3144,49 @@
             ensureSpecial(m_patchpointSpecial);
 
             Inst inst(Patch, patchpointValue, Arg::special(m_patchpointSpecial));
 
             Vector<Inst> after;
-            if (patchpointValue->type() != Void) {
-                switch (patchpointValue->resultConstraint.kind()) {
+            auto generateResultOperand = [&] (Type type, ValueRep rep, Tmp tmp) {
+                switch (rep.kind()) {
                 case ValueRep::WarmAny:
                 case ValueRep::ColdAny:
                 case ValueRep::LateColdAny:
                 case ValueRep::SomeRegister:
                 case ValueRep::SomeEarlyRegister:
-                    inst.args.append(tmp(patchpointValue));
-                    break;
+                case ValueRep::SomeLateRegister:
+                    inst.args.append(tmp);
+                    return;
                 case ValueRep::Register: {
-                    Tmp reg = Tmp(patchpointValue->resultConstraint.reg());
+                    Tmp reg = Tmp(rep.reg());
                     inst.args.append(reg);
-                    after.append(Inst(
-                        relaxedMoveForType(patchpointValue->type()), m_value, reg, tmp(patchpointValue)));
-                    break;
+                    after.append(Inst(relaxedMoveForType(type), m_value, reg, tmp));
+                    return;
                 }
                 case ValueRep::StackArgument: {
-                    Arg arg = Arg::callArg(patchpointValue->resultConstraint.offsetFromSP());
+                    Arg arg = Arg::callArg(rep.offsetFromSP());
                     inst.args.append(arg);
-                    after.append(Inst(
-                        moveForType(patchpointValue->type()), m_value, arg, tmp(patchpointValue)));
-                    break;
+                    after.append(Inst(moveForType(type), m_value, arg, tmp));
+                    return;
                 }
                 default:
                     RELEASE_ASSERT_NOT_REACHED();
-                    break;
+                    return;
                 }
+            };
+
+            if (patchpointValue->type() != Void) {
+                forEachImmOrTmp(patchpointValue, [&] (Arg arg, Type type, unsigned index) {
+                    generateResultOperand(type, patchpointValue->resultConstraints[index], arg.tmp());
+                });
             }
 
             fillStackmap(inst, patchpointValue, 0);
-
-            if (patchpointValue->resultConstraint.isReg())
-                patchpointValue->lateClobbered().clear(patchpointValue->resultConstraint.reg());
+            for (auto& constraint : patchpointValue->resultConstraints) {
+                if (constraint.isReg())
+                    patchpointValue->lateClobbered().clear(constraint.reg());
+            }
 
             for (unsigned i = patchpointValue->numGPScratchRegisters; i--;)
                 inst.args.append(m_code.newTmp(GP));
             for (unsigned i = patchpointValue->numFPScratchRegisters; i--;)
                 inst.args.append(m_code.newTmp(FP));
@@ -3095,10 +3194,19 @@
             m_insts.last().append(WTFMove(inst));
             m_insts.last().appendVector(after);
             return;
         }
 
+        case Extract: {
+            Value* tupleValue = m_value->child(0);
+            unsigned index = m_value->as<ExtractValue>()->index();
+
+            const auto& tmps = tmpsForTuple(tupleValue);
+            append(relaxedMoveForType(m_value->type()), tmps[index], tmp(m_value));
+            return;
+        }
+
         case CheckAdd:
         case CheckSub:
         case CheckMul: {
             CheckValue* checkValue = m_value->as<CheckValue>();
 
@@ -3267,45 +3375,148 @@
             return;
         }
 
         case Upsilon: {
             Value* value = m_value->child(0);
-            append(
-                relaxedMoveForType(value->type()), immOrTmp(value),
-                m_phiToTmp[m_value->as<UpsilonValue>()->phi()]);
+            Value* phi = m_value->as<UpsilonValue>()->phi();
+            if (value->type().isNumeric()) {
+                append(relaxedMoveForType(value->type()), immOrTmp(value), m_phiToTmp[phi]);
+                return;
+            }
+
+            const Vector<Type>& tuple = m_procedure.tupleForType(value->type());
+            const auto& valueTmps = tmpsForTuple(value);
+            const auto& phiTmps = m_tuplePhiToTmps.find(phi)->value;
+            ASSERT(valueTmps.size() == phiTmps.size());
+            for (unsigned i = 0; i < valueTmps.size(); ++i)
+                append(relaxedMoveForType(tuple[i]), valueTmps[i], phiTmps[i]);
             return;
         }
 
         case Phi: {
             // Snapshot the value of the Phi. It may change under us because you could do:
             // a = Phi()
             // Upsilon(@x, ^a)
             // @a => this should get the value of the Phi before the Upsilon, i.e. not @x.
 
-            append(relaxedMoveForType(m_value->type()), m_phiToTmp[m_value], tmp(m_value));
+            if (m_value->type().isNumeric()) {
+                append(relaxedMoveForType(m_value->type()), m_phiToTmp[m_value], tmp(m_value));
+                return;
+            }
+
+            const Vector<Type>& tuple = m_procedure.tupleForType(m_value->type());
+            const auto& valueTmps = tmpsForTuple(m_value);
+            const auto& phiTmps = m_tuplePhiToTmps.find(m_value)->value;
+            ASSERT(valueTmps.size() == phiTmps.size());
+            for (unsigned i = 0; i < valueTmps.size(); ++i)
+                append(relaxedMoveForType(tuple[i]), phiTmps[i], valueTmps[i]);
             return;
         }
 
         case Set: {
             Value* value = m_value->child(0);
-            append(
-                relaxedMoveForType(value->type()), immOrTmp(value),
-                m_variableToTmp.get(m_value->as<VariableValue>()->variable()));
+            const Vector<Tmp>& variableTmps = m_variableToTmps.get(m_value->as<VariableValue>()->variable());
+            forEachImmOrTmp(value, [&] (Arg immOrTmp, Type type, unsigned index) {
+                append(relaxedMoveForType(type), immOrTmp, variableTmps[index]);
+            });
             return;
         }
 
         case Get: {
-            append(
-                relaxedMoveForType(m_value->type()),
-                m_variableToTmp.get(m_value->as<VariableValue>()->variable()), tmp(m_value));
+            // Snapshot the value of the Get. It may change under us because you could do:
+            // a = Get(var)
+            // Set(@x, var)
+            // @a => this should get the value of the Get before the Set, i.e. not @x.
+
+            const Vector<Tmp>& variableTmps = m_variableToTmps.get(m_value->as<VariableValue>()->variable());
+            forEachImmOrTmp(m_value, [&] (Arg tmp, Type type, unsigned index) {
+                append(relaxedMoveForType(type), variableTmps[index], tmp.tmp());
+            });
             return;
         }
 
         case Branch: {
             if (canBeInternal(m_value->child(0))) {
                 Value* branchChild = m_value->child(0);
+
                 switch (branchChild->opcode()) {
+                case BitAnd: {
+                    Value* andValue = branchChild->child(0);
+                    Value* andMask = branchChild->child(1);
+                    Air::Opcode opcode = opcodeForType(BranchTestBit32, BranchTestBit64, andValue->type());
+
+                    Value* testValue = nullptr;
+                    Value* bitOffset = nullptr;
+                    Value* internalNode = nullptr;
+                    Value* negationNode = nullptr;
+                    bool inverted = false;
+
+                    // if (~(val >> x)&1)
+                    if (andMask->isInt(1)
+                        && andValue->opcode() == BitXor && (andValue->child(1)->isInt32(-1) || andValue->child(1)->isInt64(-1l))
+                        && (andValue->child(0)->opcode() == SShr || andValue->child(0)->opcode() == ZShr)) {
+
+                        negationNode = andValue;
+                        testValue = andValue->child(0)->child(0);
+                        bitOffset = andValue->child(0)->child(1);
+                        internalNode = andValue->child(0);
+                        inverted = !inverted;
+                    }
+
+                    // Turn if ((val >> x)&1) -> Bt val x
+                    if (andMask->isInt(1) && (andValue->opcode() == SShr || andValue->opcode() == ZShr)) {
+                        testValue = andValue->child(0);
+                        bitOffset = andValue->child(1);
+                        internalNode = andValue;
+                    }
+
+                    // Turn if (val & (1<<x)) -> Bt val x
+                    if ((andMask->opcode() == Shl) && andMask->child(0)->isInt(1)) {
+                        testValue = andValue;
+                        bitOffset = andMask->child(1);
+                        internalNode = andMask;
+                    }
+
+                    // if (~val & (1<<x)) or if ((~val >> x)&1)
+                    if (!negationNode && testValue && testValue->opcode() == BitXor && (testValue->child(1)->isInt32(-1) || testValue->child(1)->isInt64(-1l))) {
+                        negationNode = testValue;
+                        testValue = testValue->child(0);
+                        inverted = !inverted;
+                    }
+
+                    if (testValue && bitOffset) {
+                        for (auto& basePromise : Vector<ArgPromise>::from(loadPromise(testValue), tmpPromise(testValue))) {
+                            bool hasLoad = basePromise.kind() != Arg::Tmp;
+                            bool canMakeInternal = (hasLoad ? canBeInternal(testValue) : !m_locked.contains(testValue))
+                                && (!negationNode || canBeInternal(negationNode))
+                                && (!internalNode || canBeInternal(internalNode));
+
+                            if (basePromise && canMakeInternal) {
+                                if (bitOffset->hasInt() && isValidForm(opcode, Arg::ResCond, basePromise.kind(), Arg::Imm)) {
+                                    commitInternal(branchChild);
+                                    commitInternal(internalNode);
+                                    if (hasLoad)
+                                        commitInternal(testValue);
+                                    commitInternal(negationNode);
+                                    append(basePromise.inst(opcode, m_value, Arg::resCond(MacroAssembler::NonZero).inverted(inverted), basePromise.consume(*this), Arg::imm(bitOffset->asInt())));
+                                    return;
+                                }
+
+                                if (!m_locked.contains(bitOffset) && isValidForm(opcode, Arg::ResCond, basePromise.kind(), Arg::Tmp)) {
+                                    commitInternal(branchChild);
+                                    commitInternal(internalNode);
+                                    if (hasLoad)
+                                        commitInternal(testValue);
+                                    commitInternal(negationNode);
+                                    append(basePromise.inst(opcode, m_value, Arg::resCond(MacroAssembler::NonZero).inverted(inverted), basePromise.consume(*this), tmp(bitOffset)));
+                                    return;
+                                }
+                            }
+                        }
+                    }
+                    break;
+                }
                 case AtomicWeakCAS:
                     commitInternal(branchChild);
                     appendCAS(branchChild, false);
                     return;
 
@@ -3362,12 +3573,13 @@
                 return;
             }
             Value* value = m_value->child(0);
             Tmp returnValueGPR = Tmp(GPRInfo::returnValueGPR);
             Tmp returnValueFPR = Tmp(FPRInfo::returnValueFPR);
-            switch (value->type()) {
+            switch (value->type().kind()) {
             case Void:
+            case Tuple:
                 // It's impossible for a void value to be used as a child. We use RetVoid
                 // for void returns.
                 RELEASE_ASSERT_NOT_REACHED();
                 break;
             case Int32:
@@ -3483,13 +3695,15 @@
     }
 
     IndexSet<Value*> m_locked; // These are values that will have no Tmp in Air.
     IndexMap<Value*, Tmp> m_valueToTmp; // These are values that must have a Tmp in Air. We say that a Value* with a non-null Tmp is "pinned".
     IndexMap<Value*, Tmp> m_phiToTmp; // Each Phi gets its own Tmp.
+    HashMap<Value*, Vector<Tmp>> m_tupleValueToTmps; // This is the same as m_valueToTmp for Values that are Tuples.
+    HashMap<Value*, Vector<Tmp>> m_tuplePhiToTmps; // This is the same as m_phiToTmp for Phis that are Tuples.
     IndexMap<B3::BasicBlock*, Air::BasicBlock*> m_blockToBlock;
     HashMap<B3::StackSlot*, Air::StackSlot*> m_stackToStack;
-    HashMap<Variable*, Tmp> m_variableToTmp;
+    HashMap<Variable*, Vector<Tmp>> m_variableToTmps;
 
     UseCounts m_useCounts;
     PhiChildren m_phiChildren;
     BlockWorklist m_fastWorklist;
     Dominators& m_dominators;
