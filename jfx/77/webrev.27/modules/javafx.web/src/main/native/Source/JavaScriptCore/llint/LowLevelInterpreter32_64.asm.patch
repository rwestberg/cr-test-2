diff a/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter32_64.asm b/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter32_64.asm
--- a/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter32_64.asm
+++ b/modules/javafx.web/src/main/native/Source/JavaScriptCore/llint/LowLevelInterpreter32_64.asm
@@ -27,29 +27,43 @@
     loadb [PC], t0
     leap _g_opcodeMap, t1
     jmp [t1, t0, 4], BytecodePtrTag
 end
 
-macro nextInstructionWide()
+macro nextInstructionWide16()
+    loadh 1[PC], t0
+    leap _g_opcodeMapWide16, t1
+    jmp [t1, t0, 4], BytecodePtrTag
+end
+
+macro nextInstructionWide32()
     loadi 1[PC], t0
-    leap _g_opcodeMapWide, t1
+    leap _g_opcodeMapWide32, t1
     jmp [t1, t0, 4], BytecodePtrTag
 end
 
 macro getuOperandNarrow(opcodeStruct, fieldName, dst)
     loadb constexpr %opcodeStruct%_%fieldName%_index[PC], dst
 end
 
 macro getOperandNarrow(opcodeStruct, fieldName, dst)
-    loadbsp constexpr %opcodeStruct%_%fieldName%_index[PC], dst
+    loadbsi constexpr %opcodeStruct%_%fieldName%_index[PC], dst
 end
 
-macro getuOperandWide(opcodeStruct, fieldName, dst)
+macro getuOperandWide16(opcodeStruct, fieldName, dst)
+    loadh constexpr %opcodeStruct%_%fieldName%_index * 2 + 1[PC], dst
+end
+
+macro getOperandWide16(opcodeStruct, fieldName, dst)
+    loadhsi constexpr %opcodeStruct%_%fieldName%_index * 2 + 1[PC], dst
+end
+
+macro getuOperandWide32(opcodeStruct, fieldName, dst)
     loadi constexpr %opcodeStruct%_%fieldName%_index * 4 + 1[PC], dst
 end
 
-macro getOperandWide(opcodeStruct, fieldName, dst)
+macro getOperandWide32(opcodeStruct, fieldName, dst)
     loadis constexpr %opcodeStruct%_%fieldName%_index * 4 + 1[PC], dst
 end
 
 macro makeReturn(get, dispatch, fn)
     fn(macro(tag, payload)
@@ -94,19 +108,19 @@
         subp 8, sp
         push a1
         push a0
         call function
         addp 16, sp
-    elsif C_LOOP
+    elsif C_LOOP or C_LOOP_WIN
         cloopCallSlowPath function, a0, a1
     else
         error
     end
 end
 
 macro cCall2Void(function)
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         cloopCallSlowPathVoid function, a0, a1
     else
         cCall2(function)
     end
 end
@@ -119,11 +133,11 @@
         push a2
         push a1
         push a0
         call function
         addp 16, sp
-    elsif C_LOOP
+    elsif C_LOOP or C_LOOP_WIN
         error
     else
         error
     end
 end
@@ -188,11 +202,11 @@
     bpa t3, sp, .throwStackOverflow
 
     # Ensure that we have enough additional stack capacity for the incoming args,
     # and the frame for the JS code we're executing. We need to do this check
     # before we start copying the args from the protoCallFrame below.
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         bpaeq t3, VM::m_cloopStackLimit[vm], .stackHeightOK
         move entry, t4
         move vm, t5
         cloopCallSlowPath _llint_stack_check_at_vm_entry, vm, t3
         bpeq t0, 0, .stackCheckFailed
@@ -306,11 +320,11 @@
 end
 
 macro makeJavaScriptCall(entry, temp, unused)
     addp CallerFrameAndPCSize, sp
     checkStackPointerAlignment(temp, 0xbad0dc02)
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         cloopCallJSFunction entry
     else
         call entry
     end
     checkStackPointerAlignment(temp, 0xbad0dc03)
@@ -318,11 +332,11 @@
 end
 
 macro makeHostFunctionCall(entry, temp1, temp2)
     move entry, temp1
     storep cfr, [sp]
-    if C_LOOP
+    if C_LOOP or C_LOOP_WIN
         move sp, a0
         storep lr, PtrSize[sp]
         cloopCallNative temp1
     elsif X86 or X86_WIN
         # Put callee frame pointer on stack as arg0, also put it in ecx for "fastcall" targets
@@ -445,11 +459,11 @@
 end
 
 # Index, tag, and payload must be different registers. Index is not
 # changed.
 macro loadConstantOrVariable(size, index, tag, payload)
-    size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide, macro (FirstConstantRegisterIndex)
+    size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide16, FirstConstantRegisterIndexWide32, macro (FirstConstantRegisterIndex)
         bigteq index, FirstConstantRegisterIndex, .constant
         loadi TagOffset[cfr, index, 8], tag
         loadi PayloadOffset[cfr, index, 8], payload
         jmp .done
     .constant:
@@ -461,11 +475,11 @@
     .done:
     end)
 end
 
 macro loadConstantOrVariableTag(size, index, tag)
-    size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide, macro (FirstConstantRegisterIndex)
+    size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide16, FirstConstantRegisterIndexWide32, macro (FirstConstantRegisterIndex)
         bigteq index, FirstConstantRegisterIndex, .constant
         loadi TagOffset[cfr, index, 8], tag
         jmp .done
     .constant:
         loadp CodeBlock[cfr], tag
@@ -476,11 +490,11 @@
     end)
 end
 
 # Index and payload may be the same register. Index may be clobbered.
 macro loadConstantOrVariable2Reg(size, index, tag, payload)
-    size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide, macro (FirstConstantRegisterIndex)
+    size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide16, FirstConstantRegisterIndexWide32, macro (FirstConstantRegisterIndex)
         bigteq index, FirstConstantRegisterIndex, .constant
         loadi TagOffset[cfr, index, 8], tag
         loadi PayloadOffset[cfr, index, 8], payload
         jmp .done
     .constant:
@@ -494,11 +508,11 @@
     .done:
     end)
 end
 
 macro loadConstantOrVariablePayloadTagCustom(size, index, tagCheck, payload)
-    size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide, macro (FirstConstantRegisterIndex)
+    size(FirstConstantRegisterIndexNarrow, FirstConstantRegisterIndexWide16, FirstConstantRegisterIndexWide32, macro (FirstConstantRegisterIndex)
         bigteq index, FirstConstantRegisterIndex, .constant
         tagCheck(TagOffset[cfr, index, 8])
         loadi PayloadOffset[cfr, index, 8], payload
         jmp .done
     .constant:
@@ -530,25 +544,30 @@
         index,
         macro (actualTag) end,
         payload)
 end
 
-macro writeBarrierOnOperand(size, get, cellFieldName)
-    get(cellFieldName, t1)
-    loadConstantOrVariablePayload(size, t1, CellTag, t2, .writeBarrierDone)
+macro writeBarrierOnCellWithReload(cell, reloadAfterSlowPath)
     skipIfIsRememberedOrInEden(
-        t2, 
+        cell,
         macro()
             push cfr, PC
             # We make two extra slots because cCall2 will poke.
             subp 8, sp
-            move t2, a1 # t2 can be a0 on x86
+            move cell, a1 # cell can be a0
             move cfr, a0
             cCall2Void(_llint_write_barrier_slow)
             addp 8, sp
             pop PC, cfr
+            reloadAfterSlowPath()
         end)
+end
+
+macro writeBarrierOnOperand(size, get, cellFieldName)
+    get(cellFieldName, t1)
+    loadConstantOrVariablePayload(size, t1, CellTag, t2, .writeBarrierDone)
+    writeBarrierOnCellWithReload(t2, macro() end)
 .writeBarrierDone:
 end
 
 macro writeBarrierOnOperands(size, get, cellFieldName, valueFieldName)
     get(valueFieldName, t1)
@@ -564,22 +583,11 @@
     loadConstantOrVariableTag(size, t1, t0)
     bineq t0, CellTag, .writeBarrierDone
 
     loadMacro(t3)
 
-    skipIfIsRememberedOrInEden(
-        t3,
-        macro()
-            push cfr, PC
-            # We make two extra slots because cCall2 will poke.
-            subp 8, sp
-            move cfr, a0
-            move t3, a1
-            cCall2Void(_llint_write_barrier_slow)
-            addp 8, sp
-            pop PC, cfr
-        end)
+    writeBarrierOnCellWithReload(t3, macro() end)
 .writeBarrierDone:
 end
 
 macro writeBarrierOnGlobalObject(size, get, valueFieldName)
     writeBarrierOnGlobal(size, get, valueFieldName,
@@ -691,28 +699,35 @@
 # Instruction implementations
 
 _llint_op_enter:
     traceExecution()
     checkStackPointerAlignment(t2, 0xdead00e1)
-    loadp CodeBlock[cfr], t2                // t2<CodeBlock> = cfr.CodeBlock
-    loadi CodeBlock::m_numVars[t2], t2      // t2<size_t> = t2<CodeBlock>.m_numVars
+    loadp CodeBlock[cfr], t1                // t1<CodeBlock> = cfr.CodeBlock
+    loadi CodeBlock::m_numVars[t1], t2      // t2<size_t> = t1<CodeBlock>.m_numVars
     subi CalleeSaveSpaceAsVirtualRegisters, t2
     move cfr, t3
     subp CalleeSaveSpaceAsVirtualRegisters * SlotSize, t3
     btiz t2, .opEnterDone
     move UndefinedTag, t0
-    move 0, t1
     negi t2
 .opEnterLoop:
     storei t0, TagOffset[t3, t2, 8]
-    storei t1, PayloadOffset[t3, t2, 8]
+    storei 0, PayloadOffset[t3, t2, 8]
     addi 1, t2
     btinz t2, .opEnterLoop
 .opEnterDone:
-    callSlowPath(_slow_path_enter)
+    writeBarrierOnCellWithReload(t1, macro ()
+        loadp CodeBlock[cfr], t1 # Reload CodeBlock
+    end)
+    # Checking traps.
+    loadp CodeBlock::m_vm[t1], t1
+    btpnz VM::m_traps + VMTraps::m_needTrapHandling[t1], .handleTraps
+.afterHandlingTraps:
     dispatchOp(narrow, op_enter)
-
+.handleTraps:
+    callTrapHandler(_llint_throw_from_slow_path_trampoline)
+    jmp .afterHandlingTraps
 
 llintOpWithProfile(op_get_argument, OpGetArgument, macro (size, get, dispatch, return)
     get(m_index, t2)
     loadi PayloadOffset + ArgumentCount[cfr], t0
     bilteq t0, t2, .opGetArgumentOutOfBounds
@@ -743,11 +758,11 @@
     get(m_srcDst, t0)
     bineq TagOffset[cfr, t0, 8], CellTag, .opToThisSlow
     loadi PayloadOffset[cfr, t0, 8], t0
     bbneq JSCell::m_type[t0], FinalObjectType, .opToThisSlow
     metadata(t2, t3)
-    loadp OpToThis::Metadata::m_cachedStructure[t2], t2
+    loadi OpToThis::Metadata::m_cachedStructureID[t2], t2
     bineq JSCell::m_structureID[t0], t2, .opToThisSlow
     dispatch()
 
 .opToThisSlow:
     callSlowPath(_slow_path_to_this)
@@ -859,12 +874,11 @@
     macro (value) xori 1, value end)
 
 
 llintOpWithReturn(op_is_undefined_or_null, OpIsUndefinedOrNull, macro (size, get, dispatch, return)
     get(m_operand, t0)
-    assertNotConstant(size, t0)
-    loadi TagOffset[cfr, t0, 8], t1
+    loadConstantOrVariableTag(size, t0, t1)
     ori 1, t1
     cieq t1, NullTag, t1
     return(BooleanTag, t1)
 end)
 
@@ -1154,11 +1168,11 @@
 macro bitOpProfiled(opcodeName, opcodeStruct, operation)
     commonBitOp(llintOpWithProfile, opcodeName, opcodeStruct, operation)
 end
 
 
-bitOp(lshift, OpLshift,
+bitOpProfiled(lshift, OpLshift,
     macro (left, right) lshifti left, right end)
 
 
 bitOp(rshift, OpRshift,
     macro (left, right) rshifti left, right end)
@@ -1342,11 +1356,11 @@
 end)
 
 
 llintOpWithMetadata(op_get_by_id, OpGetById, macro (size, get, dispatch, metadata, return)
     metadata(t5, t0)
-    loadb OpGetById::Metadata::m_mode[t5], t1
+    loadb OpGetById::Metadata::m_modeMetadata.mode[t5], t1
     get(m_base, t0)
 
 .opGetByIdProtoLoad:
     bbneq t1, constexpr GetByIdMode::ProtoLoad, .opGetByIdArrayLength
     loadi OpGetById::Metadata::m_modeMetadata.protoLoadMode.structureID[t5], t1
@@ -1660,10 +1674,28 @@
         loadp CodeBlock::m_globalObject[t0], t0
         bpneq Structure::m_globalObject[structure], t0, target
     end,
     macro (value, target) bineq value, NullTag, target end)
 
+macro undefinedOrNullJumpOp(opcodeName, opcodeStruct, fn)
+    llintOpWithJump(op_%opcodeName%, opcodeStruct, macro (size, get, jump, dispatch)
+        get(m_value, t1)
+        loadConstantOrVariableTag(size, t1, t0)
+        ori 1, t0
+        fn(t0, .target)
+        dispatch()
+
+    .target:
+        jump(m_targetLabel)
+    end)
+end
+
+undefinedOrNullJumpOp(jundefined_or_null, OpJundefinedOrNull,
+    macro (value, target) bieq value, NullTag, target end)
+
+undefinedOrNullJumpOp(jnundefined_or_null, OpJnundefinedOrNull,
+    macro (value, target) bineq value, NullTag, target end)
 
 llintOpWithMetadata(op_jneq_ptr, OpJneqPtr, macro (size, get, dispatch, metadata, return)
     get(m_value, t0)
     getu(size, OpJneqPtr, m_specialPointer, t1)
     loadp CodeBlock[cfr], t2
@@ -1786,19 +1818,19 @@
     muli sizeof SimpleJumpTable, t3
     loadp CodeBlock::RareData::m_switchJumpTables + VectorBufferOffset[t2], t2
     addp t3, t2
     bineq t1, CellTag, .opSwitchCharFallThrough
     bbneq JSCell::m_type[t0], StringType, .opSwitchCharFallThrough
-    loadp JSString::m_fiber[t0], t0
-    btpnz t0, isRopeInPointer, .opSwitchOnRope
-    bineq StringImpl::m_length[t0], 1, .opSwitchCharFallThrough
-    loadp StringImpl::m_data8[t0], t1
-    btinz StringImpl::m_hashAndFlags[t0], HashFlags8BitBuffer, .opSwitchChar8Bit
-    loadh [t1], t0
+    loadp JSString::m_fiber[t0], t1
+    btpnz t1, isRopeInPointer, .opSwitchOnRope
+    bineq StringImpl::m_length[t1], 1, .opSwitchCharFallThrough
+    loadp StringImpl::m_data8[t1], t0
+    btinz StringImpl::m_hashAndFlags[t1], HashFlags8BitBuffer, .opSwitchChar8Bit
+    loadh [t0], t0
     jmp .opSwitchCharReady
 .opSwitchChar8Bit:
-    loadb [t1], t0
+    loadb [t0], t0
 .opSwitchCharReady:
     subi SimpleJumpTable::min[t2], t0
     biaeq t0, SimpleJumpTable::branchOffsets + VectorSizeOffset[t2], .opSwitchCharFallThrough
     loadp SimpleJumpTable::branchOffsets + VectorBufferOffset[t2], t2
     loadi [t2, t0, 4], t1
@@ -1807,10 +1839,13 @@
 
 .opSwitchCharFallThrough:
     jump(m_defaultOffset)
 
 .opSwitchOnRope:
+    bineq JSRopeString::m_compactFibers + JSRopeString::CompactFibers::m_length[t0], 1, .opSwitchCharFallThrough
+
+.opSwitchOnRopeChar:
     callSlowPath(_llint_slow_path_switch_char)
     nextInstruction()
 end)
 
 
@@ -1818,11 +1853,11 @@
     getu(m_argv, t3)
     negi t3
     bineq ThisArgumentOffset + TagOffset[cfr, t3, 8], CellTag, .done
     loadi ThisArgumentOffset + PayloadOffset[cfr, t3, 8], t0
     loadi JSCell::m_structureID[t0], t0
-    storei t0, %opcodeStruct%::Metadata::m_arrayProfile.m_lastSeenStructureID[t5]
+    storei t0, %opcodeStruct%::Metadata::m_callLinkInfo.m_arrayProfile.m_lastSeenStructureID[t5]
 .done:
 end
 
 macro commonCallOp(opcodeName, slowPath, opcodeStruct, prepareCall, prologue)
     llintOpWithMetadata(opcodeName, opcodeStruct, macro (size, get, dispatch, metadata, return)
@@ -1831,11 +1866,11 @@
         prologue(macro (fieldName, dst)
             getu(size, opcodeStruct, fieldName, dst)
         end, metadata)
 
         get(m_callee, t0)
-        loadp %opcodeStruct%::Metadata::m_callLinkInfo.callee[t5], t2
+        loadp %opcodeStruct%::Metadata::m_callLinkInfo.m_calleeOrLastSeenCalleeWithLinkBit[t5], t2
         loadConstantOrVariablePayload(size, t0, CellTag, t3, .opCallSlow)
         bineq t3, t2, .opCallSlow
         getu(size, opcodeStruct, m_argv, t3)
         lshifti 3, t3
         negi t3
@@ -1844,12 +1879,12 @@
         getu(size, opcodeStruct, m_argc, t2)
         storei PC, ArgumentCount + TagOffset[cfr]
         storei t2, ArgumentCount + PayloadOffset[t3]
         storei CellTag, Callee + TagOffset[t3]
         move t3, sp
-        prepareCall(%opcodeStruct%::Metadata::m_callLinkInfo.machineCodeTarget[t5], t2, t3, t4, JSEntryPtrTag)
-        callTargetFunction(size, opcodeStruct, dispatch, %opcodeStruct%::Metadata::m_callLinkInfo.machineCodeTarget[t5], JSEntryPtrTag)
+        prepareCall(%opcodeStruct%::Metadata::m_callLinkInfo.m_machineCodeTarget[t5], t2, t3, t4, JSEntryPtrTag)
+        callTargetFunction(size, opcodeStruct, dispatch, %opcodeStruct%::Metadata::m_callLinkInfo.m_machineCodeTarget[t5], JSEntryPtrTag)
 
     .opCallSlow:
         slowPathForCall(size, opcodeStruct, dispatch, slowPath, prepareCall)
     end)
 end
@@ -1977,11 +2012,11 @@
         call executableOffsetToFunction[t1]
         loadp Callee + PayloadOffset[cfr], t3
         andp MarkedBlockMask, t3
         loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
         addp 8, sp
-    elsif ARMv7 or C_LOOP or MIPS
+    elsif ARMv7 or C_LOOP or C_LOOP_WIN or MIPS
         if MIPS
         # calling convention says to save stack space for 4 first registers in
         # all cases. To match our 16-byte alignment, that means we need to
         # take 24 bytes
             subp 24, sp
@@ -1994,11 +2029,11 @@
         storep cfr, VM::topCallFrame[t1]
         move cfr, a0
         loadi Callee + PayloadOffset[cfr], t1
         loadp JSFunction::m_executable[t1], t1
         checkStackPointerAlignment(t3, 0xdead0001)
-        if C_LOOP
+        if C_LOOP or C_LOOP_WIN
             cloopCallNative executableOffsetToFunction[t1]
         else
             call executableOffsetToFunction[t1]
         end
         loadp Callee + PayloadOffset[cfr], t3
@@ -2044,20 +2079,20 @@
         call offsetOfFunction[t1]
         loadp Callee + PayloadOffset[cfr], t3
         andp MarkedBlockMask, t3
         loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t3], t3
         addp 8, sp
-    elsif ARMv7 or C_LOOP or MIPS
+    elsif ARMv7 or C_LOOP or C_LOOP_WIN or MIPS
         subp 8, sp # align stack pointer
         # t1 already contains the Callee.
         andp MarkedBlockMask, t1
         loadp MarkedBlockFooterOffset + MarkedBlock::Footer::m_vm[t1], t1
         storep cfr, VM::topCallFrame[t1]
         move cfr, a0
         loadi Callee + PayloadOffset[cfr], t1
         checkStackPointerAlignment(t3, 0xdead0001)
-        if C_LOOP
+        if C_LOOP or C_LOOP_WIN
             cloopCallNative offsetOfFunction[t1]
         else
             call offsetOfFunction[t1]
         end
         loadp Callee + PayloadOffset[cfr], t3
