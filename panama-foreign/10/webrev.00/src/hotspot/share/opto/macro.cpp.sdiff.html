<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/macro.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="escape.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macro.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/macro.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  62 // Returns the number of replacements made.
  63 //
  64 int PhaseMacroExpand::replace_input(Node *use, Node *oldref, Node *newref) {
  65   int nreplacements = 0;
  66   uint req = use-&gt;req();
  67   for (uint j = 0; j &lt; use-&gt;len(); j++) {
  68     Node *uin = use-&gt;in(j);
  69     if (uin == oldref) {
  70       if (j &lt; req)
  71         use-&gt;set_req(j, newref);
  72       else
  73         use-&gt;set_prec(j, newref);
  74       nreplacements++;
  75     } else if (j &gt;= req &amp;&amp; uin == NULL) {
  76       break;
  77     }
  78   }
  79   return nreplacements;
  80 }
  81 












  82 void PhaseMacroExpand::copy_call_debug_info(CallNode *oldcall, CallNode * newcall) {
  83   // Copy debug information and adjust JVMState information
  84   uint old_dbg_start = oldcall-&gt;tf()-&gt;domain()-&gt;cnt();
  85   uint new_dbg_start = newcall-&gt;tf()-&gt;domain()-&gt;cnt();
  86   int jvms_adj  = new_dbg_start - old_dbg_start;
  87   assert (new_dbg_start == newcall-&gt;req(), &quot;argument count mismatch&quot;);
  88 
  89   // SafePointScalarObject node could be referenced several times in debug info.
  90   // Use Dict to record cloned nodes.
  91   Dict* sosn_map = new Dict(cmpkey,hashkey);
  92   for (uint i = old_dbg_start; i &lt; oldcall-&gt;req(); i++) {
  93     Node* old_in = oldcall-&gt;in(i);
  94     // Clone old SafePointScalarObjectNodes, adjusting their field contents.
  95     if (old_in != NULL &amp;&amp; old_in-&gt;is_SafePointScalarObject()) {
  96       SafePointScalarObjectNode* old_sosn = old_in-&gt;as_SafePointScalarObject();
  97       uint old_unique = C-&gt;unique();
  98       Node* new_in = old_sosn-&gt;clone(sosn_map);
  99       if (old_unique != C-&gt;unique()) { // New node?
 100         new_in-&gt;set_req(0, C-&gt;root()); // reset control edge
 101         new_in = transform_later(new_in); // Register new node.
</pre>
<hr />
<pre>
1260 // Allocations bigger than this always go the slow route.
1261 // This value must be small enough that allocation attempts that need to
1262 // trigger exceptions go the slow route.  Also, it must be small enough so
1263 // that heap_top + size_in_bytes does not wrap around the 4Gig limit.
1264 //=============================================================================j//
1265 // %%% Here is an old comment from parseHelper.cpp; is it outdated?
1266 // The allocator will coalesce int-&gt;oop copies away.  See comment in
1267 // coalesce.cpp about how this works.  It depends critically on the exact
1268 // code shape produced here, so if you are changing this code shape
1269 // make sure the GC info for the heap-top is correct in and around the
1270 // slow-path call.
1271 //
1272 
1273 void PhaseMacroExpand::expand_allocate_common(
1274             AllocateNode* alloc, // allocation node to be expanded
1275             Node* length,  // array length for an array allocation
1276             const TypeFunc* slow_call_type, // Type of slow call
1277             address slow_call_address  // Address of slow call
1278     )
1279 {
<span class="line-removed">1280 </span>
1281   Node* ctrl = alloc-&gt;in(TypeFunc::Control);
1282   Node* mem  = alloc-&gt;in(TypeFunc::Memory);
1283   Node* i_o  = alloc-&gt;in(TypeFunc::I_O);
1284   Node* size_in_bytes     = alloc-&gt;in(AllocateNode::AllocSize);
1285   Node* klass_node        = alloc-&gt;in(AllocateNode::KlassNode);
1286   Node* initial_slow_test = alloc-&gt;in(AllocateNode::InitialTest);
<span class="line-removed">1287 </span>
1288   assert(ctrl != NULL, &quot;must have control&quot;);

1289   // We need a Region and corresponding Phi&#39;s to merge the slow-path and fast-path results.
1290   // they will not be used if &quot;always_slow&quot; is set
1291   enum { slow_result_path = 1, fast_result_path = 2 };
1292   Node *result_region = NULL;
1293   Node *result_phi_rawmem = NULL;
1294   Node *result_phi_rawoop = NULL;
1295   Node *result_phi_i_o = NULL;
1296 
1297   // The initial slow comparison is a size check, the comparison
1298   // we want to do is a BoolTest::gt
<span class="line-modified">1299   bool always_slow = false;</span>
1300   int tv = _igvn.find_int_con(initial_slow_test, -1);
1301   if (tv &gt;= 0) {
<span class="line-modified">1302     always_slow = (tv == 1);</span>




1303     initial_slow_test = NULL;
1304   } else {
1305     initial_slow_test = BoolNode::make_predicate(initial_slow_test, &amp;_igvn);
1306   }
1307 
1308   if (C-&gt;env()-&gt;dtrace_alloc_probes() ||
1309       (!UseTLAB &amp;&amp; !Universe::heap()-&gt;supports_inline_contig_alloc())) {
1310     // Force slow-path allocation
<span class="line-modified">1311     always_slow = true;</span>
1312     initial_slow_test = NULL;
1313   }
1314 
















1315 
1316   enum { too_big_or_final_path = 1, need_gc_path = 2 };
1317   Node *slow_region = NULL;
1318   Node *toobig_false = ctrl;
1319 
<span class="line-removed">1320   assert (initial_slow_test == NULL || !always_slow, &quot;arguments must be consistent&quot;);</span>
1321   // generate the initial test if necessary
1322   if (initial_slow_test != NULL ) {

1323     slow_region = new RegionNode(3);
1324 
1325     // Now make the initial failure test.  Usually a too-big test but
1326     // might be a TRUE for finalizers or a fancy class check for
1327     // newInstance0.
1328     IfNode *toobig_iff = new IfNode(ctrl, initial_slow_test, PROB_MIN, COUNT_UNKNOWN);
1329     transform_later(toobig_iff);
1330     // Plug the failing-too-big test into the slow-path region
1331     Node *toobig_true = new IfTrueNode( toobig_iff );
1332     transform_later(toobig_true);
1333     slow_region    -&gt;init_req( too_big_or_final_path, toobig_true );
1334     toobig_false = new IfFalseNode( toobig_iff );
1335     transform_later(toobig_false);
<span class="line-modified">1336   } else {         // No initial test, just fall into next case</span>


1337     toobig_false = ctrl;
1338     debug_only(slow_region = NodeSentinel);
1339   }
1340 










1341   Node *slow_mem = mem;  // save the current memory state for slow path
1342   // generate the fast allocation code unless we know that the initial test will always go slow
<span class="line-modified">1343   if (!always_slow) {</span>
1344     // Fast path modifies only raw memory.
1345     if (mem-&gt;is_MergeMem()) {
1346       mem = mem-&gt;as_MergeMem()-&gt;memory_at(Compile::AliasIdxRaw);
1347     }
1348 
1349     // allocate the Region and Phi nodes for the result
1350     result_region = new RegionNode(3);
1351     result_phi_rawmem = new PhiNode(result_region, Type::MEMORY, TypeRawPtr::BOTTOM);
<span class="line-removed">1352     result_phi_rawoop = new PhiNode(result_region, TypeRawPtr::BOTTOM);</span>
1353     result_phi_i_o    = new PhiNode(result_region, Type::ABIO); // I/O is used for Prefetch
1354 
1355     // Grab regular I/O before optional prefetch may change it.
1356     // Slow-path does no I/O so just set it to the original I/O.
1357     result_phi_i_o-&gt;init_req(slow_result_path, i_o);
1358 
<span class="line-removed">1359     Node* needgc_ctrl = NULL;</span>
1360     // Name successful fast-path variables
1361     Node* fast_oop_ctrl;
1362     Node* fast_oop_rawmem;



1363 
<span class="line-modified">1364     intx prefetch_lines = length != NULL ? AllocatePrefetchLines : AllocateInstancePrefetchLines;</span>
<span class="line-modified">1365 </span>
<span class="line-modified">1366     BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();</span>
<span class="line-modified">1367     Node* fast_oop = bs-&gt;obj_allocate(this, ctrl, mem, toobig_false, size_in_bytes, i_o, needgc_ctrl,</span>
<span class="line-modified">1368                                       fast_oop_ctrl, fast_oop_rawmem,</span>
<span class="line-modified">1369                                       prefetch_lines);</span>
<span class="line-modified">1370 </span>
<span class="line-modified">1371     if (initial_slow_test) {</span>
<span class="line-modified">1372       slow_region-&gt;init_req(need_gc_path, needgc_ctrl);</span>
<span class="line-modified">1373       // This completes all paths into the slow merge point</span>
<span class="line-removed">1374       transform_later(slow_region);</span>
<span class="line-removed">1375     } else {                      // No initial slow path needed!</span>
<span class="line-removed">1376       // Just fall from the need-GC path straight into the VM call.</span>
<span class="line-removed">1377       slow_region = needgc_ctrl;</span>
<span class="line-removed">1378     }</span>
<span class="line-removed">1379 </span>
<span class="line-removed">1380     InitializeNode* init = alloc-&gt;initialization();</span>
<span class="line-removed">1381     fast_oop_rawmem = initialize_object(alloc,</span>
<span class="line-removed">1382                                         fast_oop_ctrl, fast_oop_rawmem, fast_oop,</span>
<span class="line-removed">1383                                         klass_node, length, size_in_bytes);</span>
<span class="line-removed">1384 </span>
<span class="line-removed">1385     // If initialization is performed by an array copy, any required</span>
<span class="line-removed">1386     // MemBarStoreStore was already added. If the object does not</span>
<span class="line-removed">1387     // escape no need for a MemBarStoreStore. If the object does not</span>
<span class="line-removed">1388     // escape in its initializer and memory barrier (MemBarStoreStore or</span>
<span class="line-removed">1389     // stronger) is already added at exit of initializer, also no need</span>
<span class="line-removed">1390     // for a MemBarStoreStore. Otherwise we need a MemBarStoreStore</span>
<span class="line-removed">1391     // so that stores that initialize this object can&#39;t be reordered</span>
<span class="line-removed">1392     // with a subsequent store that makes this object accessible by</span>
<span class="line-removed">1393     // other threads.</span>
<span class="line-removed">1394     // Other threads include java threads and JVM internal threads</span>
<span class="line-removed">1395     // (for example concurrent GC threads). Current concurrent GC</span>
<span class="line-removed">1396     // implementation: G1 will not scan newly created object,</span>
<span class="line-removed">1397     // so it&#39;s safe to skip storestore barrier when allocation does</span>
<span class="line-removed">1398     // not escape.</span>
<span class="line-removed">1399     if (!alloc-&gt;does_not_escape_thread() &amp;&amp;</span>
<span class="line-removed">1400         !alloc-&gt;is_allocation_MemBar_redundant() &amp;&amp;</span>
<span class="line-removed">1401         (init == NULL || !init-&gt;is_complete_with_arraycopy())) {</span>
<span class="line-removed">1402       if (init == NULL || init-&gt;req() &lt; InitializeNode::RawStores) {</span>
<span class="line-removed">1403         // No InitializeNode or no stores captured by zeroing</span>
<span class="line-removed">1404         // elimination. Simply add the MemBarStoreStore after object</span>
<span class="line-removed">1405         // initialization.</span>
<span class="line-removed">1406         MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);</span>
<span class="line-removed">1407         transform_later(mb);</span>
<span class="line-removed">1408 </span>
<span class="line-removed">1409         mb-&gt;init_req(TypeFunc::Memory, fast_oop_rawmem);</span>
<span class="line-removed">1410         mb-&gt;init_req(TypeFunc::Control, fast_oop_ctrl);</span>
<span class="line-removed">1411         fast_oop_ctrl = new ProjNode(mb,TypeFunc::Control);</span>
<span class="line-removed">1412         transform_later(fast_oop_ctrl);</span>
<span class="line-removed">1413         fast_oop_rawmem = new ProjNode(mb,TypeFunc::Memory);</span>
<span class="line-removed">1414         transform_later(fast_oop_rawmem);</span>
1415       } else {
<span class="line-modified">1416         // Add the MemBarStoreStore after the InitializeNode so that</span>
<span class="line-modified">1417         // all stores performing the initialization that were moved</span>
<span class="line-modified">1418         // before the InitializeNode happen before the storestore</span>
<span class="line-removed">1419         // barrier.</span>
<span class="line-removed">1420 </span>
<span class="line-removed">1421         Node* init_ctrl = init-&gt;proj_out_or_null(TypeFunc::Control);</span>
<span class="line-removed">1422         Node* init_mem = init-&gt;proj_out_or_null(TypeFunc::Memory);</span>
<span class="line-removed">1423 </span>
<span class="line-removed">1424         MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);</span>
<span class="line-removed">1425         transform_later(mb);</span>
<span class="line-removed">1426 </span>
<span class="line-removed">1427         Node* ctrl = new ProjNode(init,TypeFunc::Control);</span>
<span class="line-removed">1428         transform_later(ctrl);</span>
<span class="line-removed">1429         Node* mem = new ProjNode(init,TypeFunc::Memory);</span>
<span class="line-removed">1430         transform_later(mem);</span>
<span class="line-removed">1431 </span>
<span class="line-removed">1432         // The MemBarStoreStore depends on control and memory coming</span>
<span class="line-removed">1433         // from the InitializeNode</span>
<span class="line-removed">1434         mb-&gt;init_req(TypeFunc::Memory, mem);</span>
<span class="line-removed">1435         mb-&gt;init_req(TypeFunc::Control, ctrl);</span>
<span class="line-removed">1436 </span>
<span class="line-removed">1437         ctrl = new ProjNode(mb,TypeFunc::Control);</span>
<span class="line-removed">1438         transform_later(ctrl);</span>
<span class="line-removed">1439         mem = new ProjNode(mb,TypeFunc::Memory);</span>
<span class="line-removed">1440         transform_later(mem);</span>
<span class="line-removed">1441 </span>
<span class="line-removed">1442         // All nodes that depended on the InitializeNode for control</span>
<span class="line-removed">1443         // and memory must now depend on the MemBarNode that itself</span>
<span class="line-removed">1444         // depends on the InitializeNode</span>
<span class="line-removed">1445         if (init_ctrl != NULL) {</span>
<span class="line-removed">1446           _igvn.replace_node(init_ctrl, ctrl);</span>
<span class="line-removed">1447         }</span>
<span class="line-removed">1448         if (init_mem != NULL) {</span>
<span class="line-removed">1449           _igvn.replace_node(init_mem, mem);</span>
<span class="line-removed">1450         }</span>
1451       }
<span class="line-removed">1452     }</span>
1453 
<span class="line-modified">1454     if (C-&gt;env()-&gt;dtrace_extended_probes()) {</span>
<span class="line-modified">1455       // Slow-path call</span>
<span class="line-modified">1456       int size = TypeFunc::Parms + 2;</span>
<span class="line-modified">1457       CallLeafNode *call = new CallLeafNode(OptoRuntime::dtrace_object_alloc_Type(),</span>
<span class="line-modified">1458                                             CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc_base),</span>
<span class="line-modified">1459                                             &quot;dtrace_object_alloc&quot;,</span>
<span class="line-modified">1460                                             TypeRawPtr::BOTTOM);</span>
<span class="line-modified">1461 </span>
<span class="line-modified">1462       // Get base of thread-local storage area</span>
<span class="line-modified">1463       Node* thread = new ThreadLocalNode();</span>
<span class="line-modified">1464       transform_later(thread);</span>
<span class="line-modified">1465 </span>
<span class="line-modified">1466       call-&gt;init_req(TypeFunc::Parms+0, thread);</span>
<span class="line-removed">1467       call-&gt;init_req(TypeFunc::Parms+1, fast_oop);</span>
<span class="line-removed">1468       call-&gt;init_req(TypeFunc::Control, fast_oop_ctrl);</span>
<span class="line-removed">1469       call-&gt;init_req(TypeFunc::I_O    , top()); // does no i/o</span>
<span class="line-removed">1470       call-&gt;init_req(TypeFunc::Memory , fast_oop_rawmem);</span>
<span class="line-removed">1471       call-&gt;init_req(TypeFunc::ReturnAdr, alloc-&gt;in(TypeFunc::ReturnAdr));</span>
<span class="line-removed">1472       call-&gt;init_req(TypeFunc::FramePtr, alloc-&gt;in(TypeFunc::FramePtr));</span>
<span class="line-removed">1473       transform_later(call);</span>
<span class="line-removed">1474       fast_oop_ctrl = new ProjNode(call,TypeFunc::Control);</span>
<span class="line-removed">1475       transform_later(fast_oop_ctrl);</span>
<span class="line-removed">1476       fast_oop_rawmem = new ProjNode(call,TypeFunc::Memory);</span>
<span class="line-removed">1477       transform_later(fast_oop_rawmem);</span>
1478     }
1479 
1480     // Plug in the successful fast-path into the result merge point
1481     result_region    -&gt;init_req(fast_result_path, fast_oop_ctrl);
<span class="line-removed">1482     result_phi_rawoop-&gt;init_req(fast_result_path, fast_oop);</span>
1483     result_phi_i_o   -&gt;init_req(fast_result_path, i_o);
1484     result_phi_rawmem-&gt;init_req(fast_result_path, fast_oop_rawmem);
1485   } else {
1486     slow_region = ctrl;
1487     result_phi_i_o = i_o; // Rename it to use in the following code.
1488   }
1489 
1490   // Generate slow-path call
1491   CallNode *call = new CallStaticJavaNode(slow_call_type, slow_call_address,
1492                                OptoRuntime::stub_name(slow_call_address),
1493                                alloc-&gt;jvms()-&gt;bci(),
1494                                TypePtr::BOTTOM);
<span class="line-modified">1495   call-&gt;init_req( TypeFunc::Control, slow_region );</span>
<span class="line-modified">1496   call-&gt;init_req( TypeFunc::I_O    , top() )     ;   // does no i/o</span>
<span class="line-modified">1497   call-&gt;init_req( TypeFunc::Memory , slow_mem ); // may gc ptrs</span>
<span class="line-modified">1498   call-&gt;init_req( TypeFunc::ReturnAdr, alloc-&gt;in(TypeFunc::ReturnAdr) );</span>
<span class="line-modified">1499   call-&gt;init_req( TypeFunc::FramePtr, alloc-&gt;in(TypeFunc::FramePtr) );</span>
1500 
1501   call-&gt;init_req(TypeFunc::Parms+0, klass_node);
1502   if (length != NULL) {
1503     call-&gt;init_req(TypeFunc::Parms+1, length);
1504   }
1505 
1506   // Copy debug information and adjust JVMState information, then replace
1507   // allocate node with the call
1508   copy_call_debug_info((CallNode *) alloc,  call);
<span class="line-modified">1509   if (!always_slow) {</span>
1510     call-&gt;set_cnt(PROB_UNLIKELY_MAG(4));  // Same effect as RC_UNCOMMON.
1511   } else {
1512     // Hook i_o projection to avoid its elimination during allocation
1513     // replacement (when only a slow call is generated).
1514     call-&gt;set_req(TypeFunc::I_O, result_phi_i_o);
1515   }
1516   _igvn.replace_node(alloc, call);
1517   transform_later(call);
1518 
1519   // Identify the output projections from the allocate node and
1520   // adjust any references to them.
1521   // The control and io projections look like:
1522   //
1523   //        v---Proj(ctrl) &lt;-----+   v---CatchProj(ctrl)
1524   //  Allocate                   Catch
1525   //        ^---Proj(io) &lt;-------+   ^---CatchProj(io)
1526   //
1527   //  We are interested in the CatchProj nodes.
1528   //
1529   extract_call_projections(call);
1530 
1531   // An allocate node has separate memory projections for the uses on
1532   // the control and i_o paths. Replace the control memory projection with
1533   // result_phi_rawmem (unless we are only generating a slow call when
1534   // both memory projections are combined)
<span class="line-modified">1535   if (!always_slow &amp;&amp; _memproj_fallthrough != NULL) {</span>
<span class="line-modified">1536     for (DUIterator_Fast imax, i = _memproj_fallthrough-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-removed">1537       Node *use = _memproj_fallthrough-&gt;fast_out(i);</span>
<span class="line-removed">1538       _igvn.rehash_node_delayed(use);</span>
<span class="line-removed">1539       imax -= replace_input(use, _memproj_fallthrough, result_phi_rawmem);</span>
<span class="line-removed">1540       // back up iterator</span>
<span class="line-removed">1541       --i;</span>
<span class="line-removed">1542     }</span>
1543   }
1544   // Now change uses of _memproj_catchall to use _memproj_fallthrough and delete
1545   // _memproj_catchall so we end up with a call that has only 1 memory projection.
1546   if (_memproj_catchall != NULL ) {
1547     if (_memproj_fallthrough == NULL) {
1548       _memproj_fallthrough = new ProjNode(call, TypeFunc::Memory);
1549       transform_later(_memproj_fallthrough);
1550     }
<span class="line-modified">1551     for (DUIterator_Fast imax, i = _memproj_catchall-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-removed">1552       Node *use = _memproj_catchall-&gt;fast_out(i);</span>
<span class="line-removed">1553       _igvn.rehash_node_delayed(use);</span>
<span class="line-removed">1554       imax -= replace_input(use, _memproj_catchall, _memproj_fallthrough);</span>
<span class="line-removed">1555       // back up iterator</span>
<span class="line-removed">1556       --i;</span>
<span class="line-removed">1557     }</span>
<span class="line-removed">1558     assert(_memproj_catchall-&gt;outcnt() == 0, &quot;all uses must be deleted&quot;);</span>
1559     _igvn.remove_dead_node(_memproj_catchall);
1560   }
1561 
1562   // An allocate node has separate i_o projections for the uses on the control
1563   // and i_o paths. Always replace the control i_o projection with result i_o
1564   // otherwise incoming i_o become dead when only a slow call is generated
1565   // (it is different from memory projections where both projections are
1566   // combined in such case).
1567   if (_ioproj_fallthrough != NULL) {
<span class="line-modified">1568     for (DUIterator_Fast imax, i = _ioproj_fallthrough-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-removed">1569       Node *use = _ioproj_fallthrough-&gt;fast_out(i);</span>
<span class="line-removed">1570       _igvn.rehash_node_delayed(use);</span>
<span class="line-removed">1571       imax -= replace_input(use, _ioproj_fallthrough, result_phi_i_o);</span>
<span class="line-removed">1572       // back up iterator</span>
<span class="line-removed">1573       --i;</span>
<span class="line-removed">1574     }</span>
1575   }
1576   // Now change uses of _ioproj_catchall to use _ioproj_fallthrough and delete
1577   // _ioproj_catchall so we end up with a call that has only 1 i_o projection.
1578   if (_ioproj_catchall != NULL ) {
1579     if (_ioproj_fallthrough == NULL) {
1580       _ioproj_fallthrough = new ProjNode(call, TypeFunc::I_O);
1581       transform_later(_ioproj_fallthrough);
1582     }
<span class="line-modified">1583     for (DUIterator_Fast imax, i = _ioproj_catchall-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-removed">1584       Node *use = _ioproj_catchall-&gt;fast_out(i);</span>
<span class="line-removed">1585       _igvn.rehash_node_delayed(use);</span>
<span class="line-removed">1586       imax -= replace_input(use, _ioproj_catchall, _ioproj_fallthrough);</span>
<span class="line-removed">1587       // back up iterator</span>
<span class="line-removed">1588       --i;</span>
<span class="line-removed">1589     }</span>
<span class="line-removed">1590     assert(_ioproj_catchall-&gt;outcnt() == 0, &quot;all uses must be deleted&quot;);</span>
1591     _igvn.remove_dead_node(_ioproj_catchall);
1592   }
1593 
1594   // if we generated only a slow call, we are done
<span class="line-modified">1595   if (always_slow) {</span>
1596     // Now we can unhook i_o.
1597     if (result_phi_i_o-&gt;outcnt() &gt; 1) {
1598       call-&gt;set_req(TypeFunc::I_O, top());
1599     } else {
<span class="line-modified">1600       assert(result_phi_i_o-&gt;unique_ctrl_out() == call, &quot;&quot;);</span>
1601       // Case of new array with negative size known during compilation.
1602       // AllocateArrayNode::Ideal() optimization disconnect unreachable
1603       // following code since call to runtime will throw exception.
1604       // As result there will be no users of i_o after the call.
1605       // Leave i_o attached to this call to avoid problems in preceding graph.
1606     }
1607     return;
1608   }
1609 
<span class="line-removed">1610 </span>
1611   if (_fallthroughcatchproj != NULL) {
1612     ctrl = _fallthroughcatchproj-&gt;clone();
1613     transform_later(ctrl);
1614     _igvn.replace_node(_fallthroughcatchproj, result_region);
1615   } else {
1616     ctrl = top();
1617   }
1618   Node *slow_result;
1619   if (_resproj == NULL) {
1620     // no uses of the allocation result
1621     slow_result = top();
1622   } else {
1623     slow_result = _resproj-&gt;clone();
1624     transform_later(slow_result);
1625     _igvn.replace_node(_resproj, result_phi_rawoop);
1626   }
1627 
1628   // Plug slow-path into result merge point
<span class="line-modified">1629   result_region    -&gt;init_req( slow_result_path, ctrl );</span>
<span class="line-removed">1630   result_phi_rawoop-&gt;init_req( slow_result_path, slow_result);</span>
<span class="line-removed">1631   result_phi_rawmem-&gt;init_req( slow_result_path, _memproj_fallthrough );</span>
1632   transform_later(result_region);
<span class="line-modified">1633   transform_later(result_phi_rawoop);</span>




1634   transform_later(result_phi_rawmem);
1635   transform_later(result_phi_i_o);
1636   // This completes all paths into the result merge point
1637 }
1638 































































































































































1639 
1640 // Helper for PhaseMacroExpand::expand_allocate_common.
1641 // Initializes the newly-allocated storage.
1642 Node*
1643 PhaseMacroExpand::initialize_object(AllocateNode* alloc,
1644                                     Node* control, Node* rawmem, Node* object,
1645                                     Node* klass_node, Node* length,
1646                                     Node* size_in_bytes) {
1647   InitializeNode* init = alloc-&gt;initialization();
1648   // Store the klass &amp; mark bits
1649   Node* mark_node = alloc-&gt;make_ideal_mark(&amp;_igvn, object, control, rawmem);
1650   if (!mark_node-&gt;is_Con()) {
1651     transform_later(mark_node);
1652   }
1653   rawmem = make_store(control, rawmem, object, oopDesc::mark_offset_in_bytes(), mark_node, TypeX_X-&gt;basic_type());
1654 
1655   rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);
1656   int header_size = alloc-&gt;minimum_header_size();  // conservatively small
1657 
1658   // Array length
</pre>
</td>
<td>
<hr />
<pre>
  62 // Returns the number of replacements made.
  63 //
  64 int PhaseMacroExpand::replace_input(Node *use, Node *oldref, Node *newref) {
  65   int nreplacements = 0;
  66   uint req = use-&gt;req();
  67   for (uint j = 0; j &lt; use-&gt;len(); j++) {
  68     Node *uin = use-&gt;in(j);
  69     if (uin == oldref) {
  70       if (j &lt; req)
  71         use-&gt;set_req(j, newref);
  72       else
  73         use-&gt;set_prec(j, newref);
  74       nreplacements++;
  75     } else if (j &gt;= req &amp;&amp; uin == NULL) {
  76       break;
  77     }
  78   }
  79   return nreplacements;
  80 }
  81 
<span class="line-added">  82 void PhaseMacroExpand::migrate_outs(Node *old, Node *target) {</span>
<span class="line-added">  83   assert(old != NULL, &quot;sanity&quot;);</span>
<span class="line-added">  84   for (DUIterator_Fast imax, i = old-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-added">  85     Node* use = old-&gt;fast_out(i);</span>
<span class="line-added">  86     _igvn.rehash_node_delayed(use);</span>
<span class="line-added">  87     imax -= replace_input(use, old, target);</span>
<span class="line-added">  88     // back up iterator</span>
<span class="line-added">  89     --i;</span>
<span class="line-added">  90   }</span>
<span class="line-added">  91   assert(old-&gt;outcnt() == 0, &quot;all uses must be deleted&quot;);</span>
<span class="line-added">  92 }</span>
<span class="line-added">  93 </span>
  94 void PhaseMacroExpand::copy_call_debug_info(CallNode *oldcall, CallNode * newcall) {
  95   // Copy debug information and adjust JVMState information
  96   uint old_dbg_start = oldcall-&gt;tf()-&gt;domain()-&gt;cnt();
  97   uint new_dbg_start = newcall-&gt;tf()-&gt;domain()-&gt;cnt();
  98   int jvms_adj  = new_dbg_start - old_dbg_start;
  99   assert (new_dbg_start == newcall-&gt;req(), &quot;argument count mismatch&quot;);
 100 
 101   // SafePointScalarObject node could be referenced several times in debug info.
 102   // Use Dict to record cloned nodes.
 103   Dict* sosn_map = new Dict(cmpkey,hashkey);
 104   for (uint i = old_dbg_start; i &lt; oldcall-&gt;req(); i++) {
 105     Node* old_in = oldcall-&gt;in(i);
 106     // Clone old SafePointScalarObjectNodes, adjusting their field contents.
 107     if (old_in != NULL &amp;&amp; old_in-&gt;is_SafePointScalarObject()) {
 108       SafePointScalarObjectNode* old_sosn = old_in-&gt;as_SafePointScalarObject();
 109       uint old_unique = C-&gt;unique();
 110       Node* new_in = old_sosn-&gt;clone(sosn_map);
 111       if (old_unique != C-&gt;unique()) { // New node?
 112         new_in-&gt;set_req(0, C-&gt;root()); // reset control edge
 113         new_in = transform_later(new_in); // Register new node.
</pre>
<hr />
<pre>
1272 // Allocations bigger than this always go the slow route.
1273 // This value must be small enough that allocation attempts that need to
1274 // trigger exceptions go the slow route.  Also, it must be small enough so
1275 // that heap_top + size_in_bytes does not wrap around the 4Gig limit.
1276 //=============================================================================j//
1277 // %%% Here is an old comment from parseHelper.cpp; is it outdated?
1278 // The allocator will coalesce int-&gt;oop copies away.  See comment in
1279 // coalesce.cpp about how this works.  It depends critically on the exact
1280 // code shape produced here, so if you are changing this code shape
1281 // make sure the GC info for the heap-top is correct in and around the
1282 // slow-path call.
1283 //
1284 
1285 void PhaseMacroExpand::expand_allocate_common(
1286             AllocateNode* alloc, // allocation node to be expanded
1287             Node* length,  // array length for an array allocation
1288             const TypeFunc* slow_call_type, // Type of slow call
1289             address slow_call_address  // Address of slow call
1290     )
1291 {

1292   Node* ctrl = alloc-&gt;in(TypeFunc::Control);
1293   Node* mem  = alloc-&gt;in(TypeFunc::Memory);
1294   Node* i_o  = alloc-&gt;in(TypeFunc::I_O);
1295   Node* size_in_bytes     = alloc-&gt;in(AllocateNode::AllocSize);
1296   Node* klass_node        = alloc-&gt;in(AllocateNode::KlassNode);
1297   Node* initial_slow_test = alloc-&gt;in(AllocateNode::InitialTest);

1298   assert(ctrl != NULL, &quot;must have control&quot;);
<span class="line-added">1299 </span>
1300   // We need a Region and corresponding Phi&#39;s to merge the slow-path and fast-path results.
1301   // they will not be used if &quot;always_slow&quot; is set
1302   enum { slow_result_path = 1, fast_result_path = 2 };
1303   Node *result_region = NULL;
1304   Node *result_phi_rawmem = NULL;
1305   Node *result_phi_rawoop = NULL;
1306   Node *result_phi_i_o = NULL;
1307 
1308   // The initial slow comparison is a size check, the comparison
1309   // we want to do is a BoolTest::gt
<span class="line-modified">1310   bool expand_fast_path = true;</span>
1311   int tv = _igvn.find_int_con(initial_slow_test, -1);
1312   if (tv &gt;= 0) {
<span class="line-modified">1313     // InitialTest has constant result</span>
<span class="line-added">1314     //   0 - can fit in TLAB</span>
<span class="line-added">1315     //   1 - always too big or negative</span>
<span class="line-added">1316     assert(tv &lt;= 1, &quot;0 or 1 if a constant&quot;);</span>
<span class="line-added">1317     expand_fast_path = (tv == 0);</span>
1318     initial_slow_test = NULL;
1319   } else {
1320     initial_slow_test = BoolNode::make_predicate(initial_slow_test, &amp;_igvn);
1321   }
1322 
1323   if (C-&gt;env()-&gt;dtrace_alloc_probes() ||
1324       (!UseTLAB &amp;&amp; !Universe::heap()-&gt;supports_inline_contig_alloc())) {
1325     // Force slow-path allocation
<span class="line-modified">1326     expand_fast_path = false;</span>
1327     initial_slow_test = NULL;
1328   }
1329 
<span class="line-added">1330   bool allocation_has_use = (alloc-&gt;result_cast() != NULL);</span>
<span class="line-added">1331   if (!allocation_has_use) {</span>
<span class="line-added">1332     InitializeNode* init = alloc-&gt;initialization();</span>
<span class="line-added">1333     if (init != NULL) {</span>
<span class="line-added">1334       yank_initalize_node(init);</span>
<span class="line-added">1335       assert(init-&gt;outcnt() == 0, &quot;all uses must be deleted&quot;);</span>
<span class="line-added">1336       _igvn.remove_dead_node(init);</span>
<span class="line-added">1337     }</span>
<span class="line-added">1338     if (expand_fast_path &amp;&amp; (initial_slow_test == NULL)) {</span>
<span class="line-added">1339       // Remove allocation node and return.</span>
<span class="line-added">1340       // Size is a non-negative constant -&gt; no initial check needed -&gt; directly to fast path.</span>
<span class="line-added">1341       // Also, no usages -&gt; empty fast path -&gt; no fall out to slow path -&gt; nothing left.</span>
<span class="line-added">1342       yank_alloc_node(alloc);</span>
<span class="line-added">1343       return;</span>
<span class="line-added">1344     }</span>
<span class="line-added">1345   }</span>
1346 
1347   enum { too_big_or_final_path = 1, need_gc_path = 2 };
1348   Node *slow_region = NULL;
1349   Node *toobig_false = ctrl;
1350 

1351   // generate the initial test if necessary
1352   if (initial_slow_test != NULL ) {
<span class="line-added">1353     assert (expand_fast_path, &quot;Only need test if there is a fast path&quot;);</span>
1354     slow_region = new RegionNode(3);
1355 
1356     // Now make the initial failure test.  Usually a too-big test but
1357     // might be a TRUE for finalizers or a fancy class check for
1358     // newInstance0.
1359     IfNode *toobig_iff = new IfNode(ctrl, initial_slow_test, PROB_MIN, COUNT_UNKNOWN);
1360     transform_later(toobig_iff);
1361     // Plug the failing-too-big test into the slow-path region
1362     Node *toobig_true = new IfTrueNode( toobig_iff );
1363     transform_later(toobig_true);
1364     slow_region    -&gt;init_req( too_big_or_final_path, toobig_true );
1365     toobig_false = new IfFalseNode( toobig_iff );
1366     transform_later(toobig_false);
<span class="line-modified">1367   } else {</span>
<span class="line-added">1368     // No initial test, just fall into next case</span>
<span class="line-added">1369     assert(allocation_has_use || !expand_fast_path, &quot;Should already have been handled&quot;);</span>
1370     toobig_false = ctrl;
1371     debug_only(slow_region = NodeSentinel);
1372   }
1373 
<span class="line-added">1374   // If we are here there are several possibilities</span>
<span class="line-added">1375   // - expand_fast_path is false - then only a slow path is expanded. That&#39;s it.</span>
<span class="line-added">1376   // no_initial_check means a constant allocation.</span>
<span class="line-added">1377   // - If check always evaluates to false -&gt; expand_fast_path is false (see above)</span>
<span class="line-added">1378   // - If check always evaluates to true -&gt; directly into fast path (but may bailout to slowpath)</span>
<span class="line-added">1379   // if !allocation_has_use the fast path is empty</span>
<span class="line-added">1380   // if !allocation_has_use &amp;&amp; no_initial_check</span>
<span class="line-added">1381   // - Then there are no fastpath that can fall out to slowpath -&gt; no allocation code at all.</span>
<span class="line-added">1382   //   removed by yank_alloc_node above.</span>
<span class="line-added">1383 </span>
1384   Node *slow_mem = mem;  // save the current memory state for slow path
1385   // generate the fast allocation code unless we know that the initial test will always go slow
<span class="line-modified">1386   if (expand_fast_path) {</span>
1387     // Fast path modifies only raw memory.
1388     if (mem-&gt;is_MergeMem()) {
1389       mem = mem-&gt;as_MergeMem()-&gt;memory_at(Compile::AliasIdxRaw);
1390     }
1391 
1392     // allocate the Region and Phi nodes for the result
1393     result_region = new RegionNode(3);
1394     result_phi_rawmem = new PhiNode(result_region, Type::MEMORY, TypeRawPtr::BOTTOM);

1395     result_phi_i_o    = new PhiNode(result_region, Type::ABIO); // I/O is used for Prefetch
1396 
1397     // Grab regular I/O before optional prefetch may change it.
1398     // Slow-path does no I/O so just set it to the original I/O.
1399     result_phi_i_o-&gt;init_req(slow_result_path, i_o);
1400 

1401     // Name successful fast-path variables
1402     Node* fast_oop_ctrl;
1403     Node* fast_oop_rawmem;
<span class="line-added">1404     if (allocation_has_use) {</span>
<span class="line-added">1405       Node* needgc_ctrl = NULL;</span>
<span class="line-added">1406       result_phi_rawoop = new PhiNode(result_region, TypeRawPtr::BOTTOM);</span>
1407 
<span class="line-modified">1408       intx prefetch_lines = length != NULL ? AllocatePrefetchLines : AllocateInstancePrefetchLines;</span>
<span class="line-modified">1409       BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();</span>
<span class="line-modified">1410       Node* fast_oop = bs-&gt;obj_allocate(this, ctrl, mem, toobig_false, size_in_bytes, i_o, needgc_ctrl,</span>
<span class="line-modified">1411                                         fast_oop_ctrl, fast_oop_rawmem,</span>
<span class="line-modified">1412                                         prefetch_lines);</span>
<span class="line-modified">1413 </span>
<span class="line-modified">1414       if (initial_slow_test != NULL) {</span>
<span class="line-modified">1415         // This completes all paths into the slow merge point</span>
<span class="line-modified">1416         slow_region-&gt;init_req(need_gc_path, needgc_ctrl);</span>
<span class="line-modified">1417         transform_later(slow_region);</span>









































1418       } else {
<span class="line-modified">1419         // No initial slow path needed!</span>
<span class="line-modified">1420         // Just fall from the need-GC path straight into the VM call.</span>
<span class="line-modified">1421         slow_region = needgc_ctrl;</span>
































1422       }

1423 
<span class="line-modified">1424       InitializeNode* init = alloc-&gt;initialization();</span>
<span class="line-modified">1425       fast_oop_rawmem = initialize_object(alloc,</span>
<span class="line-modified">1426                                           fast_oop_ctrl, fast_oop_rawmem, fast_oop,</span>
<span class="line-modified">1427                                           klass_node, length, size_in_bytes);</span>
<span class="line-modified">1428       expand_initialize_membar(alloc, init, fast_oop_ctrl, fast_oop_rawmem);</span>
<span class="line-modified">1429       expand_dtrace_alloc_probe(alloc, fast_oop, fast_oop_ctrl, fast_oop_rawmem);</span>
<span class="line-modified">1430 </span>
<span class="line-modified">1431       result_phi_rawoop-&gt;init_req(fast_result_path, fast_oop);</span>
<span class="line-modified">1432     } else {</span>
<span class="line-modified">1433       assert (initial_slow_test != NULL, &quot;sanity&quot;);</span>
<span class="line-modified">1434       fast_oop_ctrl   = toobig_false;</span>
<span class="line-modified">1435       fast_oop_rawmem = mem;</span>
<span class="line-modified">1436       transform_later(slow_region);</span>











1437     }
1438 
1439     // Plug in the successful fast-path into the result merge point
1440     result_region    -&gt;init_req(fast_result_path, fast_oop_ctrl);

1441     result_phi_i_o   -&gt;init_req(fast_result_path, i_o);
1442     result_phi_rawmem-&gt;init_req(fast_result_path, fast_oop_rawmem);
1443   } else {
1444     slow_region = ctrl;
1445     result_phi_i_o = i_o; // Rename it to use in the following code.
1446   }
1447 
1448   // Generate slow-path call
1449   CallNode *call = new CallStaticJavaNode(slow_call_type, slow_call_address,
1450                                OptoRuntime::stub_name(slow_call_address),
1451                                alloc-&gt;jvms()-&gt;bci(),
1452                                TypePtr::BOTTOM);
<span class="line-modified">1453   call-&gt;init_req(TypeFunc::Control,   slow_region);</span>
<span class="line-modified">1454   call-&gt;init_req(TypeFunc::I_O,       top());    // does no i/o</span>
<span class="line-modified">1455   call-&gt;init_req(TypeFunc::Memory,    slow_mem); // may gc ptrs</span>
<span class="line-modified">1456   call-&gt;init_req(TypeFunc::ReturnAdr, alloc-&gt;in(TypeFunc::ReturnAdr));</span>
<span class="line-modified">1457   call-&gt;init_req(TypeFunc::FramePtr,  alloc-&gt;in(TypeFunc::FramePtr));</span>
1458 
1459   call-&gt;init_req(TypeFunc::Parms+0, klass_node);
1460   if (length != NULL) {
1461     call-&gt;init_req(TypeFunc::Parms+1, length);
1462   }
1463 
1464   // Copy debug information and adjust JVMState information, then replace
1465   // allocate node with the call
1466   copy_call_debug_info((CallNode *) alloc,  call);
<span class="line-modified">1467   if (expand_fast_path) {</span>
1468     call-&gt;set_cnt(PROB_UNLIKELY_MAG(4));  // Same effect as RC_UNCOMMON.
1469   } else {
1470     // Hook i_o projection to avoid its elimination during allocation
1471     // replacement (when only a slow call is generated).
1472     call-&gt;set_req(TypeFunc::I_O, result_phi_i_o);
1473   }
1474   _igvn.replace_node(alloc, call);
1475   transform_later(call);
1476 
1477   // Identify the output projections from the allocate node and
1478   // adjust any references to them.
1479   // The control and io projections look like:
1480   //
1481   //        v---Proj(ctrl) &lt;-----+   v---CatchProj(ctrl)
1482   //  Allocate                   Catch
1483   //        ^---Proj(io) &lt;-------+   ^---CatchProj(io)
1484   //
1485   //  We are interested in the CatchProj nodes.
1486   //
1487   extract_call_projections(call);
1488 
1489   // An allocate node has separate memory projections for the uses on
1490   // the control and i_o paths. Replace the control memory projection with
1491   // result_phi_rawmem (unless we are only generating a slow call when
1492   // both memory projections are combined)
<span class="line-modified">1493   if (expand_fast_path &amp;&amp; _memproj_fallthrough != NULL) {</span>
<span class="line-modified">1494     migrate_outs(_memproj_fallthrough, result_phi_rawmem);</span>






1495   }
1496   // Now change uses of _memproj_catchall to use _memproj_fallthrough and delete
1497   // _memproj_catchall so we end up with a call that has only 1 memory projection.
1498   if (_memproj_catchall != NULL ) {
1499     if (_memproj_fallthrough == NULL) {
1500       _memproj_fallthrough = new ProjNode(call, TypeFunc::Memory);
1501       transform_later(_memproj_fallthrough);
1502     }
<span class="line-modified">1503     migrate_outs(_memproj_catchall, _memproj_fallthrough);</span>







1504     _igvn.remove_dead_node(_memproj_catchall);
1505   }
1506 
1507   // An allocate node has separate i_o projections for the uses on the control
1508   // and i_o paths. Always replace the control i_o projection with result i_o
1509   // otherwise incoming i_o become dead when only a slow call is generated
1510   // (it is different from memory projections where both projections are
1511   // combined in such case).
1512   if (_ioproj_fallthrough != NULL) {
<span class="line-modified">1513     migrate_outs(_ioproj_fallthrough, result_phi_i_o);</span>






1514   }
1515   // Now change uses of _ioproj_catchall to use _ioproj_fallthrough and delete
1516   // _ioproj_catchall so we end up with a call that has only 1 i_o projection.
1517   if (_ioproj_catchall != NULL ) {
1518     if (_ioproj_fallthrough == NULL) {
1519       _ioproj_fallthrough = new ProjNode(call, TypeFunc::I_O);
1520       transform_later(_ioproj_fallthrough);
1521     }
<span class="line-modified">1522     migrate_outs(_ioproj_catchall, _ioproj_fallthrough);</span>







1523     _igvn.remove_dead_node(_ioproj_catchall);
1524   }
1525 
1526   // if we generated only a slow call, we are done
<span class="line-modified">1527   if (!expand_fast_path) {</span>
1528     // Now we can unhook i_o.
1529     if (result_phi_i_o-&gt;outcnt() &gt; 1) {
1530       call-&gt;set_req(TypeFunc::I_O, top());
1531     } else {
<span class="line-modified">1532       assert(result_phi_i_o-&gt;unique_ctrl_out() == call, &quot;sanity&quot;);</span>
1533       // Case of new array with negative size known during compilation.
1534       // AllocateArrayNode::Ideal() optimization disconnect unreachable
1535       // following code since call to runtime will throw exception.
1536       // As result there will be no users of i_o after the call.
1537       // Leave i_o attached to this call to avoid problems in preceding graph.
1538     }
1539     return;
1540   }
1541 

1542   if (_fallthroughcatchproj != NULL) {
1543     ctrl = _fallthroughcatchproj-&gt;clone();
1544     transform_later(ctrl);
1545     _igvn.replace_node(_fallthroughcatchproj, result_region);
1546   } else {
1547     ctrl = top();
1548   }
1549   Node *slow_result;
1550   if (_resproj == NULL) {
1551     // no uses of the allocation result
1552     slow_result = top();
1553   } else {
1554     slow_result = _resproj-&gt;clone();
1555     transform_later(slow_result);
1556     _igvn.replace_node(_resproj, result_phi_rawoop);
1557   }
1558 
1559   // Plug slow-path into result merge point
<span class="line-modified">1560   result_region-&gt;init_req( slow_result_path, ctrl);</span>


1561   transform_later(result_region);
<span class="line-modified">1562   if (allocation_has_use) {</span>
<span class="line-added">1563     result_phi_rawoop-&gt;init_req(slow_result_path, slow_result);</span>
<span class="line-added">1564     transform_later(result_phi_rawoop);</span>
<span class="line-added">1565   }</span>
<span class="line-added">1566   result_phi_rawmem-&gt;init_req(slow_result_path, _memproj_fallthrough);</span>
1567   transform_later(result_phi_rawmem);
1568   transform_later(result_phi_i_o);
1569   // This completes all paths into the result merge point
1570 }
1571 
<span class="line-added">1572 // Remove alloc node that has no uses.</span>
<span class="line-added">1573 void PhaseMacroExpand::yank_alloc_node(AllocateNode* alloc) {</span>
<span class="line-added">1574   Node* ctrl = alloc-&gt;in(TypeFunc::Control);</span>
<span class="line-added">1575   Node* mem  = alloc-&gt;in(TypeFunc::Memory);</span>
<span class="line-added">1576   Node* i_o  = alloc-&gt;in(TypeFunc::I_O);</span>
<span class="line-added">1577 </span>
<span class="line-added">1578   extract_call_projections(alloc);</span>
<span class="line-added">1579   if (_fallthroughcatchproj != NULL) {</span>
<span class="line-added">1580     migrate_outs(_fallthroughcatchproj, ctrl);</span>
<span class="line-added">1581     _igvn.remove_dead_node(_fallthroughcatchproj);</span>
<span class="line-added">1582   }</span>
<span class="line-added">1583   if (_catchallcatchproj != NULL) {</span>
<span class="line-added">1584     _igvn.rehash_node_delayed(_catchallcatchproj);</span>
<span class="line-added">1585     _catchallcatchproj-&gt;set_req(0, top());</span>
<span class="line-added">1586   }</span>
<span class="line-added">1587   if (_fallthroughproj != NULL) {</span>
<span class="line-added">1588     Node* catchnode = _fallthroughproj-&gt;unique_ctrl_out();</span>
<span class="line-added">1589     _igvn.remove_dead_node(catchnode);</span>
<span class="line-added">1590     _igvn.remove_dead_node(_fallthroughproj);</span>
<span class="line-added">1591   }</span>
<span class="line-added">1592   if (_memproj_fallthrough != NULL) {</span>
<span class="line-added">1593     migrate_outs(_memproj_fallthrough, mem);</span>
<span class="line-added">1594     _igvn.remove_dead_node(_memproj_fallthrough);</span>
<span class="line-added">1595   }</span>
<span class="line-added">1596   if (_ioproj_fallthrough != NULL) {</span>
<span class="line-added">1597     migrate_outs(_ioproj_fallthrough, i_o);</span>
<span class="line-added">1598     _igvn.remove_dead_node(_ioproj_fallthrough);</span>
<span class="line-added">1599   }</span>
<span class="line-added">1600   if (_memproj_catchall != NULL) {</span>
<span class="line-added">1601     _igvn.rehash_node_delayed(_memproj_catchall);</span>
<span class="line-added">1602     _memproj_catchall-&gt;set_req(0, top());</span>
<span class="line-added">1603   }</span>
<span class="line-added">1604   if (_ioproj_catchall != NULL) {</span>
<span class="line-added">1605     _igvn.rehash_node_delayed(_ioproj_catchall);</span>
<span class="line-added">1606     _ioproj_catchall-&gt;set_req(0, top());</span>
<span class="line-added">1607   }</span>
<span class="line-added">1608   _igvn.remove_dead_node(alloc);</span>
<span class="line-added">1609 }</span>
<span class="line-added">1610 </span>
<span class="line-added">1611 void PhaseMacroExpand::expand_initialize_membar(AllocateNode* alloc, InitializeNode* init,</span>
<span class="line-added">1612                                                 Node*&amp; fast_oop_ctrl, Node*&amp; fast_oop_rawmem) {</span>
<span class="line-added">1613   // If initialization is performed by an array copy, any required</span>
<span class="line-added">1614   // MemBarStoreStore was already added. If the object does not</span>
<span class="line-added">1615   // escape no need for a MemBarStoreStore. If the object does not</span>
<span class="line-added">1616   // escape in its initializer and memory barrier (MemBarStoreStore or</span>
<span class="line-added">1617   // stronger) is already added at exit of initializer, also no need</span>
<span class="line-added">1618   // for a MemBarStoreStore. Otherwise we need a MemBarStoreStore</span>
<span class="line-added">1619   // so that stores that initialize this object can&#39;t be reordered</span>
<span class="line-added">1620   // with a subsequent store that makes this object accessible by</span>
<span class="line-added">1621   // other threads.</span>
<span class="line-added">1622   // Other threads include java threads and JVM internal threads</span>
<span class="line-added">1623   // (for example concurrent GC threads). Current concurrent GC</span>
<span class="line-added">1624   // implementation: G1 will not scan newly created object,</span>
<span class="line-added">1625   // so it&#39;s safe to skip storestore barrier when allocation does</span>
<span class="line-added">1626   // not escape.</span>
<span class="line-added">1627   if (!alloc-&gt;does_not_escape_thread() &amp;&amp;</span>
<span class="line-added">1628     !alloc-&gt;is_allocation_MemBar_redundant() &amp;&amp;</span>
<span class="line-added">1629     (init == NULL || !init-&gt;is_complete_with_arraycopy())) {</span>
<span class="line-added">1630     if (init == NULL || init-&gt;req() &lt; InitializeNode::RawStores) {</span>
<span class="line-added">1631       // No InitializeNode or no stores captured by zeroing</span>
<span class="line-added">1632       // elimination. Simply add the MemBarStoreStore after object</span>
<span class="line-added">1633       // initialization.</span>
<span class="line-added">1634       MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);</span>
<span class="line-added">1635       transform_later(mb);</span>
<span class="line-added">1636 </span>
<span class="line-added">1637       mb-&gt;init_req(TypeFunc::Memory, fast_oop_rawmem);</span>
<span class="line-added">1638       mb-&gt;init_req(TypeFunc::Control, fast_oop_ctrl);</span>
<span class="line-added">1639       fast_oop_ctrl = new ProjNode(mb, TypeFunc::Control);</span>
<span class="line-added">1640       transform_later(fast_oop_ctrl);</span>
<span class="line-added">1641       fast_oop_rawmem = new ProjNode(mb, TypeFunc::Memory);</span>
<span class="line-added">1642       transform_later(fast_oop_rawmem);</span>
<span class="line-added">1643     } else {</span>
<span class="line-added">1644       // Add the MemBarStoreStore after the InitializeNode so that</span>
<span class="line-added">1645       // all stores performing the initialization that were moved</span>
<span class="line-added">1646       // before the InitializeNode happen before the storestore</span>
<span class="line-added">1647       // barrier.</span>
<span class="line-added">1648 </span>
<span class="line-added">1649       Node* init_ctrl = init-&gt;proj_out_or_null(TypeFunc::Control);</span>
<span class="line-added">1650       Node* init_mem = init-&gt;proj_out_or_null(TypeFunc::Memory);</span>
<span class="line-added">1651 </span>
<span class="line-added">1652       MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);</span>
<span class="line-added">1653       transform_later(mb);</span>
<span class="line-added">1654 </span>
<span class="line-added">1655       Node* ctrl = new ProjNode(init, TypeFunc::Control);</span>
<span class="line-added">1656       transform_later(ctrl);</span>
<span class="line-added">1657       Node* mem = new ProjNode(init, TypeFunc::Memory);</span>
<span class="line-added">1658       transform_later(mem);</span>
<span class="line-added">1659 </span>
<span class="line-added">1660       // The MemBarStoreStore depends on control and memory coming</span>
<span class="line-added">1661       // from the InitializeNode</span>
<span class="line-added">1662       mb-&gt;init_req(TypeFunc::Memory, mem);</span>
<span class="line-added">1663       mb-&gt;init_req(TypeFunc::Control, ctrl);</span>
<span class="line-added">1664 </span>
<span class="line-added">1665       ctrl = new ProjNode(mb, TypeFunc::Control);</span>
<span class="line-added">1666       transform_later(ctrl);</span>
<span class="line-added">1667       mem = new ProjNode(mb, TypeFunc::Memory);</span>
<span class="line-added">1668       transform_later(mem);</span>
<span class="line-added">1669 </span>
<span class="line-added">1670       // All nodes that depended on the InitializeNode for control</span>
<span class="line-added">1671       // and memory must now depend on the MemBarNode that itself</span>
<span class="line-added">1672       // depends on the InitializeNode</span>
<span class="line-added">1673       if (init_ctrl != NULL) {</span>
<span class="line-added">1674         _igvn.replace_node(init_ctrl, ctrl);</span>
<span class="line-added">1675       }</span>
<span class="line-added">1676       if (init_mem != NULL) {</span>
<span class="line-added">1677         _igvn.replace_node(init_mem, mem);</span>
<span class="line-added">1678       }</span>
<span class="line-added">1679     }</span>
<span class="line-added">1680   }</span>
<span class="line-added">1681 }</span>
<span class="line-added">1682 </span>
<span class="line-added">1683 void PhaseMacroExpand::expand_dtrace_alloc_probe(AllocateNode* alloc, Node* oop,</span>
<span class="line-added">1684                                                 Node*&amp; ctrl, Node*&amp; rawmem) {</span>
<span class="line-added">1685   if (C-&gt;env()-&gt;dtrace_extended_probes()) {</span>
<span class="line-added">1686     // Slow-path call</span>
<span class="line-added">1687     int size = TypeFunc::Parms + 2;</span>
<span class="line-added">1688     CallLeafNode *call = new CallLeafNode(OptoRuntime::dtrace_object_alloc_Type(),</span>
<span class="line-added">1689                                           CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc_base),</span>
<span class="line-added">1690                                           &quot;dtrace_object_alloc&quot;,</span>
<span class="line-added">1691                                           TypeRawPtr::BOTTOM);</span>
<span class="line-added">1692 </span>
<span class="line-added">1693     // Get base of thread-local storage area</span>
<span class="line-added">1694     Node* thread = new ThreadLocalNode();</span>
<span class="line-added">1695     transform_later(thread);</span>
<span class="line-added">1696 </span>
<span class="line-added">1697     call-&gt;init_req(TypeFunc::Parms + 0, thread);</span>
<span class="line-added">1698     call-&gt;init_req(TypeFunc::Parms + 1, oop);</span>
<span class="line-added">1699     call-&gt;init_req(TypeFunc::Control, ctrl);</span>
<span class="line-added">1700     call-&gt;init_req(TypeFunc::I_O    , top()); // does no i/o</span>
<span class="line-added">1701     call-&gt;init_req(TypeFunc::Memory , ctrl);</span>
<span class="line-added">1702     call-&gt;init_req(TypeFunc::ReturnAdr, alloc-&gt;in(TypeFunc::ReturnAdr));</span>
<span class="line-added">1703     call-&gt;init_req(TypeFunc::FramePtr, alloc-&gt;in(TypeFunc::FramePtr));</span>
<span class="line-added">1704     transform_later(call);</span>
<span class="line-added">1705     ctrl = new ProjNode(call, TypeFunc::Control);</span>
<span class="line-added">1706     transform_later(ctrl);</span>
<span class="line-added">1707     rawmem = new ProjNode(call, TypeFunc::Memory);</span>
<span class="line-added">1708     transform_later(rawmem);</span>
<span class="line-added">1709   }</span>
<span class="line-added">1710 }</span>
<span class="line-added">1711 </span>
<span class="line-added">1712 // Remove InitializeNode without use</span>
<span class="line-added">1713 void PhaseMacroExpand::yank_initalize_node(InitializeNode* initnode) {</span>
<span class="line-added">1714   assert(initnode-&gt;proj_out_or_null(TypeFunc::Parms) == NULL, &quot;No uses allowed&quot;);</span>
<span class="line-added">1715 </span>
<span class="line-added">1716   Node* ctrl_out  = initnode-&gt;proj_out_or_null(TypeFunc::Control);</span>
<span class="line-added">1717   Node* mem_out   = initnode-&gt;proj_out_or_null(TypeFunc::Memory);</span>
<span class="line-added">1718 </span>
<span class="line-added">1719   // Move all uses of each to</span>
<span class="line-added">1720   if (ctrl_out != NULL ) {</span>
<span class="line-added">1721     migrate_outs(ctrl_out, initnode-&gt;in(TypeFunc::Control));</span>
<span class="line-added">1722     _igvn.remove_dead_node(ctrl_out);</span>
<span class="line-added">1723   }</span>
<span class="line-added">1724 </span>
<span class="line-added">1725   // Move all uses of each to</span>
<span class="line-added">1726   if (mem_out != NULL ) {</span>
<span class="line-added">1727     migrate_outs(mem_out, initnode-&gt;in(TypeFunc::Memory));</span>
<span class="line-added">1728     _igvn.remove_dead_node(mem_out);</span>
<span class="line-added">1729   }</span>
<span class="line-added">1730 }</span>
1731 
1732 // Helper for PhaseMacroExpand::expand_allocate_common.
1733 // Initializes the newly-allocated storage.
1734 Node*
1735 PhaseMacroExpand::initialize_object(AllocateNode* alloc,
1736                                     Node* control, Node* rawmem, Node* object,
1737                                     Node* klass_node, Node* length,
1738                                     Node* size_in_bytes) {
1739   InitializeNode* init = alloc-&gt;initialization();
1740   // Store the klass &amp; mark bits
1741   Node* mark_node = alloc-&gt;make_ideal_mark(&amp;_igvn, object, control, rawmem);
1742   if (!mark_node-&gt;is_Con()) {
1743     transform_later(mark_node);
1744   }
1745   rawmem = make_store(control, rawmem, object, oopDesc::mark_offset_in_bytes(), mark_node, TypeX_X-&gt;basic_type());
1746 
1747   rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);
1748   int header_size = alloc-&gt;minimum_header_size();  // conservatively small
1749 
1750   // Array length
</pre>
</td>
</tr>
</table>
<center><a href="escape.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macro.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>