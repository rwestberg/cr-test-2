<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/parallel/psParallelCompact.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="psCompactionManager.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="psParallelCompact.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/parallel/psParallelCompact.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2005, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
</pre>
<hr />
<pre>
  36 #include &quot;gc/parallel/psAdaptiveSizePolicy.hpp&quot;
  37 #include &quot;gc/parallel/psCompactionManager.inline.hpp&quot;
  38 #include &quot;gc/parallel/psOldGen.hpp&quot;
  39 #include &quot;gc/parallel/psParallelCompact.inline.hpp&quot;
  40 #include &quot;gc/parallel/psPromotionManager.inline.hpp&quot;
  41 #include &quot;gc/parallel/psRootType.hpp&quot;
  42 #include &quot;gc/parallel/psScavenge.hpp&quot;
  43 #include &quot;gc/parallel/psYoungGen.hpp&quot;
  44 #include &quot;gc/shared/gcCause.hpp&quot;
  45 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
  46 #include &quot;gc/shared/gcId.hpp&quot;
  47 #include &quot;gc/shared/gcLocker.hpp&quot;
  48 #include &quot;gc/shared/gcTimer.hpp&quot;
  49 #include &quot;gc/shared/gcTrace.hpp&quot;
  50 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  51 #include &quot;gc/shared/isGCActiveMark.hpp&quot;
  52 #include &quot;gc/shared/referencePolicy.hpp&quot;
  53 #include &quot;gc/shared/referenceProcessor.hpp&quot;
  54 #include &quot;gc/shared/referenceProcessorPhaseTimes.hpp&quot;
  55 #include &quot;gc/shared/spaceDecorator.inline.hpp&quot;

  56 #include &quot;gc/shared/weakProcessor.hpp&quot;
  57 #include &quot;gc/shared/workerPolicy.hpp&quot;
  58 #include &quot;gc/shared/workgroup.hpp&quot;
  59 #include &quot;logging/log.hpp&quot;
  60 #include &quot;memory/iterator.inline.hpp&quot;
  61 #include &quot;memory/resourceArea.hpp&quot;
  62 #include &quot;memory/universe.hpp&quot;
  63 #include &quot;oops/access.inline.hpp&quot;
  64 #include &quot;oops/instanceClassLoaderKlass.inline.hpp&quot;
  65 #include &quot;oops/instanceKlass.inline.hpp&quot;
  66 #include &quot;oops/instanceMirrorKlass.inline.hpp&quot;
  67 #include &quot;oops/methodData.hpp&quot;
  68 #include &quot;oops/objArrayKlass.inline.hpp&quot;
  69 #include &quot;oops/oop.inline.hpp&quot;
  70 #include &quot;runtime/atomic.hpp&quot;
  71 #include &quot;runtime/handles.inline.hpp&quot;
  72 #include &quot;runtime/safepoint.hpp&quot;
  73 #include &quot;runtime/vmThread.hpp&quot;
  74 #include &quot;services/management.hpp&quot;
  75 #include &quot;services/memTracker.hpp&quot;
</pre>
<hr />
<pre>
1951   if (VerifyObjectStartArray &amp;&amp;
1952       VerifyAfterGC) {
1953     old_gen-&gt;verify_object_start_array();
1954   }
1955 
1956   if (ZapUnusedHeapArea) {
1957     old_gen-&gt;object_space()-&gt;check_mangled_unused_area_complete();
1958   }
1959 
1960   NOT_PRODUCT(ref_processor()-&gt;verify_no_references_recorded());
1961 
1962   collection_exit.update();
1963 
1964   heap-&gt;print_heap_after_gc();
1965   heap-&gt;trace_heap_after_gc(&amp;_gc_tracer);
1966 
1967   log_debug(gc, task, time)(&quot;VM-Thread &quot; JLONG_FORMAT &quot; &quot; JLONG_FORMAT &quot; &quot; JLONG_FORMAT,
1968                          marking_start.ticks(), compaction_start.ticks(),
1969                          collection_exit.ticks());
1970 
<span class="line-removed">1971 #ifdef TRACESPINNING</span>
<span class="line-removed">1972   ParallelTaskTerminator::print_termination_counts();</span>
<span class="line-removed">1973 #endif</span>
<span class="line-removed">1974 </span>
1975   AdaptiveSizePolicyOutput::print(size_policy, heap-&gt;total_collections());
1976 
1977   _gc_timer.register_gc_end();
1978 
1979   _gc_tracer.report_dense_prefix(dense_prefix(old_space_id));
1980   _gc_tracer.report_gc_end(_gc_timer.gc_end(), _gc_timer.time_partitions());
1981 
1982   return true;
1983 }
1984 
1985 bool PSParallelCompact::absorb_live_data_from_eden(PSAdaptiveSizePolicy* size_policy,
1986                                              PSYoungGen* young_gen,
1987                                              PSOldGen* old_gen) {
1988   MutableSpace* const eden_space = young_gen-&gt;eden_space();
1989   assert(!eden_space-&gt;is_empty(), &quot;eden must be non-empty&quot;);
1990   assert(young_gen-&gt;virtual_space()-&gt;alignment() ==
1991          old_gen-&gt;virtual_space()-&gt;alignment(), &quot;alignments do not match&quot;);
1992 
1993   // We also return false when it&#39;s a heterogeneous heap because old generation cannot absorb data from eden
1994   // when it is allocated on different memory (example, nv-dimm) than young.
</pre>
<hr />
<pre>
2132         ClassLoaderDataGraph::always_strong_cld_do(&amp;cld_closure);
2133       }
2134       break;
2135 
2136     case ParallelRootType::code_cache:
2137       // Do not treat nmethods as strong roots for mark/sweep, since we can unload them.
2138       //ScavengableNMethods::scavengable_nmethods_do(CodeBlobToOopClosure(&amp;mark_and_push_closure));
2139       AOTLoader::oops_do(&amp;mark_and_push_closure);
2140       break;
2141 
2142     case ParallelRootType::sentinel:
2143     DEBUG_ONLY(default:) // DEBUG_ONLY hack will create compile error on release builds (-Wswitch) and runtime check on debug builds
2144       fatal(&quot;Bad enumeration value: %u&quot;, root_type);
2145       break;
2146   }
2147 
2148   // Do the real work
2149   cm-&gt;follow_marking_stacks();
2150 }
2151 
<span class="line-modified">2152 static void steal_marking_work(ParallelTaskTerminator&amp; terminator, uint worker_id) {</span>
2153   assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), &quot;called outside gc&quot;);
2154 
2155   ParCompactionManager* cm =
2156     ParCompactionManager::gc_thread_compaction_manager(worker_id);
2157 
2158   oop obj = NULL;
2159   ObjArrayTask task;
2160   do {
2161     while (ParCompactionManager::steal_objarray(worker_id,  task)) {
2162       cm-&gt;follow_array((objArrayOop)task.obj(), task.index());
2163       cm-&gt;follow_marking_stacks();
2164     }
2165     while (ParCompactionManager::steal(worker_id, obj)) {
2166       cm-&gt;follow_contents(obj);
2167       cm-&gt;follow_marking_stacks();
2168     }
2169   } while (!terminator.offer_termination());
2170 }
2171 
2172 class MarkFromRootsTask : public AbstractGangTask {
</pre>
<hr />
<pre>
2180   MarkFromRootsTask(uint active_workers) :
2181       AbstractGangTask(&quot;MarkFromRootsTask&quot;),
2182       _strong_roots_scope(active_workers),
2183       _subtasks(),
2184       _terminator(active_workers, ParCompactionManager::stack_array()),
2185       _active_workers(active_workers) {
2186     _subtasks.set_n_threads(active_workers);
2187     _subtasks.set_n_tasks(ParallelRootType::sentinel);
2188   }
2189 
2190   virtual void work(uint worker_id) {
2191     for (uint task = 0; _subtasks.try_claim_task(task); /*empty*/ ) {
2192       mark_from_roots_work(static_cast&lt;ParallelRootType::Value&gt;(task), worker_id);
2193     }
2194     _subtasks.all_tasks_completed();
2195 
2196     PCAddThreadRootsMarkingTaskClosure closure(worker_id);
2197     Threads::possibly_parallel_threads_do(true /*parallel */, &amp;closure);
2198 
2199     if (_active_workers &gt; 1) {
<span class="line-modified">2200       steal_marking_work(*_terminator.terminator(), worker_id);</span>
2201     }
2202   }
2203 };
2204 
2205 class PCRefProcTask : public AbstractGangTask {
2206   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
2207   ProcessTask&amp; _task;
2208   uint _ergo_workers;
2209   TaskTerminator _terminator;
2210 
2211 public:
2212   PCRefProcTask(ProcessTask&amp; task, uint ergo_workers) :
2213       AbstractGangTask(&quot;PCRefProcTask&quot;),
2214       _task(task),
2215       _ergo_workers(ergo_workers),
2216       _terminator(_ergo_workers, ParCompactionManager::stack_array()) {
2217   }
2218 
2219   virtual void work(uint worker_id) {
2220     ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
2221     assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), &quot;called outside gc&quot;);
2222 
2223     ParCompactionManager* cm =
2224       ParCompactionManager::gc_thread_compaction_manager(worker_id);
2225     PCMarkAndPushClosure mark_and_push_closure(cm);
2226     ParCompactionManager::FollowStackClosure follow_stack_closure(cm);
2227     _task.work(worker_id, *PSParallelCompact::is_alive_closure(),
2228                mark_and_push_closure, follow_stack_closure);
2229 
<span class="line-modified">2230     steal_marking_work(*_terminator.terminator(), worker_id);</span>
2231   }
2232 };
2233 
2234 class RefProcTaskExecutor: public AbstractRefProcTaskExecutor {
2235   void execute(ProcessTask&amp; process_task, uint ergo_workers) {
2236     assert(ParallelScavengeHeap::heap()-&gt;workers().active_workers() == ergo_workers,
2237            &quot;Ergonomically chosen workers (%u) must be equal to active workers (%u)&quot;,
2238            ergo_workers, ParallelScavengeHeap::heap()-&gt;workers().active_workers());
2239 
2240     PCRefProcTask task(process_task, ergo_workers);
2241     ParallelScavengeHeap::heap()-&gt;workers().run_task(&amp;task);
2242   }
2243 };
2244 
2245 void PSParallelCompact::marking_phase(ParCompactionManager* cm,
2246                                       bool maximum_heap_compaction,
2247                                       ParallelOldTracer *gc_tracer) {
2248   // Recursively traverse all live objects and mark them
2249   GCTraceTime(Info, gc, phases) tm(&quot;Marking Phase&quot;, &amp;_gc_timer);
2250 
</pre>
<hr />
<pre>
2569       const rd_t* const end = sd.addr_to_region_ptr(top_aligned_up);
2570 
2571       size_t histo[5] = { 0, 0, 0, 0, 0 };
2572       const size_t histo_len = sizeof(histo) / sizeof(size_t);
2573       const size_t region_cnt = pointer_delta(end, beg, sizeof(rd_t));
2574 
2575       for (const rd_t* cur = beg; cur &lt; end; ++cur) {
2576         ++histo[MIN2(cur-&gt;blocks_filled_count(), histo_len - 1)];
2577       }
2578       out-&gt;print(&quot;Block fill histogram: %u %-4s&quot; SIZE_FORMAT_W(5), id, space_names[id], region_cnt);
2579       for (size_t i = 0; i &lt; histo_len; ++i) {
2580         out-&gt;print(&quot; &quot; SIZE_FORMAT_W(5) &quot; %5.1f%%&quot;,
2581                    histo[i], 100.0 * histo[i] / region_cnt);
2582       }
2583       out-&gt;cr();
2584     }
2585   }
2586 }
2587 #endif // #ifdef ASSERT
2588 
<span class="line-modified">2589 static void compaction_with_stealing_work(ParallelTaskTerminator* terminator, uint worker_id) {</span>
2590   assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), &quot;called outside gc&quot;);
2591 
2592   ParCompactionManager* cm =
2593     ParCompactionManager::gc_thread_compaction_manager(worker_id);
2594 
2595   // Drain the stacks that have been preloaded with regions
2596   // that are ready to fill.
2597 
2598   cm-&gt;drain_region_stacks();
2599 
2600   guarantee(cm-&gt;region_stack()-&gt;is_empty(), &quot;Not empty&quot;);
2601 
2602   size_t region_index = 0;
2603 
2604   while (true) {
2605     if (ParCompactionManager::steal(worker_id, region_index)) {
2606       PSParallelCompact::fill_and_update_region(cm, region_index);
2607       cm-&gt;drain_region_stacks();
2608     } else if (PSParallelCompact::steal_unavailable_region(cm, region_index)) {
2609       // Fill and update an unavailable region with the help of a shadow region
</pre>
<hr />
<pre>
2627 
2628 public:
2629   UpdateDensePrefixAndCompactionTask(TaskQueue&amp; tq, uint active_workers) :
2630       AbstractGangTask(&quot;UpdateDensePrefixAndCompactionTask&quot;),
2631       _tq(tq),
2632       _terminator(active_workers, ParCompactionManager::region_array()),
2633       _active_workers(active_workers) {
2634   }
2635   virtual void work(uint worker_id) {
2636     ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(worker_id);
2637 
2638     for (PSParallelCompact::UpdateDensePrefixTask task; _tq.try_claim(task); /* empty */) {
2639       PSParallelCompact::update_and_deadwood_in_dense_prefix(cm,
2640                                                              task._space_id,
2641                                                              task._region_index_start,
2642                                                              task._region_index_end);
2643     }
2644 
2645     // Once a thread has drained it&#39;s stack, it should try to steal regions from
2646     // other threads.
<span class="line-modified">2647     compaction_with_stealing_work(_terminator.terminator(), worker_id);</span>
2648   }
2649 };
2650 
2651 void PSParallelCompact::compact() {
2652   GCTraceTime(Info, gc, phases) tm(&quot;Compaction Phase&quot;, &amp;_gc_timer);
2653 
2654   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
2655   PSOldGen* old_gen = heap-&gt;old_gen();
2656   old_gen-&gt;start_array()-&gt;reset();
2657   uint active_gc_threads = ParallelScavengeHeap::heap()-&gt;workers().active_workers();
2658 
2659   // for [0..last_space_id)
2660   //     for [0..active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING)
2661   //         push
2662   //     push
2663   //
2664   // max push count is thus: last_space_id * (active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING + 1)
2665   TaskQueue task_queue(last_space_id * (active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING + 1));
2666   initialize_shadow_regions(active_gc_threads);
2667   prepare_region_draining_tasks(active_gc_threads);
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2005, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
</pre>
<hr />
<pre>
  36 #include &quot;gc/parallel/psAdaptiveSizePolicy.hpp&quot;
  37 #include &quot;gc/parallel/psCompactionManager.inline.hpp&quot;
  38 #include &quot;gc/parallel/psOldGen.hpp&quot;
  39 #include &quot;gc/parallel/psParallelCompact.inline.hpp&quot;
  40 #include &quot;gc/parallel/psPromotionManager.inline.hpp&quot;
  41 #include &quot;gc/parallel/psRootType.hpp&quot;
  42 #include &quot;gc/parallel/psScavenge.hpp&quot;
  43 #include &quot;gc/parallel/psYoungGen.hpp&quot;
  44 #include &quot;gc/shared/gcCause.hpp&quot;
  45 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
  46 #include &quot;gc/shared/gcId.hpp&quot;
  47 #include &quot;gc/shared/gcLocker.hpp&quot;
  48 #include &quot;gc/shared/gcTimer.hpp&quot;
  49 #include &quot;gc/shared/gcTrace.hpp&quot;
  50 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  51 #include &quot;gc/shared/isGCActiveMark.hpp&quot;
  52 #include &quot;gc/shared/referencePolicy.hpp&quot;
  53 #include &quot;gc/shared/referenceProcessor.hpp&quot;
  54 #include &quot;gc/shared/referenceProcessorPhaseTimes.hpp&quot;
  55 #include &quot;gc/shared/spaceDecorator.inline.hpp&quot;
<span class="line-added">  56 #include &quot;gc/shared/taskTerminator.hpp&quot;</span>
  57 #include &quot;gc/shared/weakProcessor.hpp&quot;
  58 #include &quot;gc/shared/workerPolicy.hpp&quot;
  59 #include &quot;gc/shared/workgroup.hpp&quot;
  60 #include &quot;logging/log.hpp&quot;
  61 #include &quot;memory/iterator.inline.hpp&quot;
  62 #include &quot;memory/resourceArea.hpp&quot;
  63 #include &quot;memory/universe.hpp&quot;
  64 #include &quot;oops/access.inline.hpp&quot;
  65 #include &quot;oops/instanceClassLoaderKlass.inline.hpp&quot;
  66 #include &quot;oops/instanceKlass.inline.hpp&quot;
  67 #include &quot;oops/instanceMirrorKlass.inline.hpp&quot;
  68 #include &quot;oops/methodData.hpp&quot;
  69 #include &quot;oops/objArrayKlass.inline.hpp&quot;
  70 #include &quot;oops/oop.inline.hpp&quot;
  71 #include &quot;runtime/atomic.hpp&quot;
  72 #include &quot;runtime/handles.inline.hpp&quot;
  73 #include &quot;runtime/safepoint.hpp&quot;
  74 #include &quot;runtime/vmThread.hpp&quot;
  75 #include &quot;services/management.hpp&quot;
  76 #include &quot;services/memTracker.hpp&quot;
</pre>
<hr />
<pre>
1952   if (VerifyObjectStartArray &amp;&amp;
1953       VerifyAfterGC) {
1954     old_gen-&gt;verify_object_start_array();
1955   }
1956 
1957   if (ZapUnusedHeapArea) {
1958     old_gen-&gt;object_space()-&gt;check_mangled_unused_area_complete();
1959   }
1960 
1961   NOT_PRODUCT(ref_processor()-&gt;verify_no_references_recorded());
1962 
1963   collection_exit.update();
1964 
1965   heap-&gt;print_heap_after_gc();
1966   heap-&gt;trace_heap_after_gc(&amp;_gc_tracer);
1967 
1968   log_debug(gc, task, time)(&quot;VM-Thread &quot; JLONG_FORMAT &quot; &quot; JLONG_FORMAT &quot; &quot; JLONG_FORMAT,
1969                          marking_start.ticks(), compaction_start.ticks(),
1970                          collection_exit.ticks());
1971 




1972   AdaptiveSizePolicyOutput::print(size_policy, heap-&gt;total_collections());
1973 
1974   _gc_timer.register_gc_end();
1975 
1976   _gc_tracer.report_dense_prefix(dense_prefix(old_space_id));
1977   _gc_tracer.report_gc_end(_gc_timer.gc_end(), _gc_timer.time_partitions());
1978 
1979   return true;
1980 }
1981 
1982 bool PSParallelCompact::absorb_live_data_from_eden(PSAdaptiveSizePolicy* size_policy,
1983                                              PSYoungGen* young_gen,
1984                                              PSOldGen* old_gen) {
1985   MutableSpace* const eden_space = young_gen-&gt;eden_space();
1986   assert(!eden_space-&gt;is_empty(), &quot;eden must be non-empty&quot;);
1987   assert(young_gen-&gt;virtual_space()-&gt;alignment() ==
1988          old_gen-&gt;virtual_space()-&gt;alignment(), &quot;alignments do not match&quot;);
1989 
1990   // We also return false when it&#39;s a heterogeneous heap because old generation cannot absorb data from eden
1991   // when it is allocated on different memory (example, nv-dimm) than young.
</pre>
<hr />
<pre>
2129         ClassLoaderDataGraph::always_strong_cld_do(&amp;cld_closure);
2130       }
2131       break;
2132 
2133     case ParallelRootType::code_cache:
2134       // Do not treat nmethods as strong roots for mark/sweep, since we can unload them.
2135       //ScavengableNMethods::scavengable_nmethods_do(CodeBlobToOopClosure(&amp;mark_and_push_closure));
2136       AOTLoader::oops_do(&amp;mark_and_push_closure);
2137       break;
2138 
2139     case ParallelRootType::sentinel:
2140     DEBUG_ONLY(default:) // DEBUG_ONLY hack will create compile error on release builds (-Wswitch) and runtime check on debug builds
2141       fatal(&quot;Bad enumeration value: %u&quot;, root_type);
2142       break;
2143   }
2144 
2145   // Do the real work
2146   cm-&gt;follow_marking_stacks();
2147 }
2148 
<span class="line-modified">2149 static void steal_marking_work(TaskTerminator&amp; terminator, uint worker_id) {</span>
2150   assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), &quot;called outside gc&quot;);
2151 
2152   ParCompactionManager* cm =
2153     ParCompactionManager::gc_thread_compaction_manager(worker_id);
2154 
2155   oop obj = NULL;
2156   ObjArrayTask task;
2157   do {
2158     while (ParCompactionManager::steal_objarray(worker_id,  task)) {
2159       cm-&gt;follow_array((objArrayOop)task.obj(), task.index());
2160       cm-&gt;follow_marking_stacks();
2161     }
2162     while (ParCompactionManager::steal(worker_id, obj)) {
2163       cm-&gt;follow_contents(obj);
2164       cm-&gt;follow_marking_stacks();
2165     }
2166   } while (!terminator.offer_termination());
2167 }
2168 
2169 class MarkFromRootsTask : public AbstractGangTask {
</pre>
<hr />
<pre>
2177   MarkFromRootsTask(uint active_workers) :
2178       AbstractGangTask(&quot;MarkFromRootsTask&quot;),
2179       _strong_roots_scope(active_workers),
2180       _subtasks(),
2181       _terminator(active_workers, ParCompactionManager::stack_array()),
2182       _active_workers(active_workers) {
2183     _subtasks.set_n_threads(active_workers);
2184     _subtasks.set_n_tasks(ParallelRootType::sentinel);
2185   }
2186 
2187   virtual void work(uint worker_id) {
2188     for (uint task = 0; _subtasks.try_claim_task(task); /*empty*/ ) {
2189       mark_from_roots_work(static_cast&lt;ParallelRootType::Value&gt;(task), worker_id);
2190     }
2191     _subtasks.all_tasks_completed();
2192 
2193     PCAddThreadRootsMarkingTaskClosure closure(worker_id);
2194     Threads::possibly_parallel_threads_do(true /*parallel */, &amp;closure);
2195 
2196     if (_active_workers &gt; 1) {
<span class="line-modified">2197       steal_marking_work(_terminator, worker_id);</span>
2198     }
2199   }
2200 };
2201 
2202 class PCRefProcTask : public AbstractGangTask {
2203   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
2204   ProcessTask&amp; _task;
2205   uint _ergo_workers;
2206   TaskTerminator _terminator;
2207 
2208 public:
2209   PCRefProcTask(ProcessTask&amp; task, uint ergo_workers) :
2210       AbstractGangTask(&quot;PCRefProcTask&quot;),
2211       _task(task),
2212       _ergo_workers(ergo_workers),
2213       _terminator(_ergo_workers, ParCompactionManager::stack_array()) {
2214   }
2215 
2216   virtual void work(uint worker_id) {
2217     ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
2218     assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), &quot;called outside gc&quot;);
2219 
2220     ParCompactionManager* cm =
2221       ParCompactionManager::gc_thread_compaction_manager(worker_id);
2222     PCMarkAndPushClosure mark_and_push_closure(cm);
2223     ParCompactionManager::FollowStackClosure follow_stack_closure(cm);
2224     _task.work(worker_id, *PSParallelCompact::is_alive_closure(),
2225                mark_and_push_closure, follow_stack_closure);
2226 
<span class="line-modified">2227     steal_marking_work(_terminator, worker_id);</span>
2228   }
2229 };
2230 
2231 class RefProcTaskExecutor: public AbstractRefProcTaskExecutor {
2232   void execute(ProcessTask&amp; process_task, uint ergo_workers) {
2233     assert(ParallelScavengeHeap::heap()-&gt;workers().active_workers() == ergo_workers,
2234            &quot;Ergonomically chosen workers (%u) must be equal to active workers (%u)&quot;,
2235            ergo_workers, ParallelScavengeHeap::heap()-&gt;workers().active_workers());
2236 
2237     PCRefProcTask task(process_task, ergo_workers);
2238     ParallelScavengeHeap::heap()-&gt;workers().run_task(&amp;task);
2239   }
2240 };
2241 
2242 void PSParallelCompact::marking_phase(ParCompactionManager* cm,
2243                                       bool maximum_heap_compaction,
2244                                       ParallelOldTracer *gc_tracer) {
2245   // Recursively traverse all live objects and mark them
2246   GCTraceTime(Info, gc, phases) tm(&quot;Marking Phase&quot;, &amp;_gc_timer);
2247 
</pre>
<hr />
<pre>
2566       const rd_t* const end = sd.addr_to_region_ptr(top_aligned_up);
2567 
2568       size_t histo[5] = { 0, 0, 0, 0, 0 };
2569       const size_t histo_len = sizeof(histo) / sizeof(size_t);
2570       const size_t region_cnt = pointer_delta(end, beg, sizeof(rd_t));
2571 
2572       for (const rd_t* cur = beg; cur &lt; end; ++cur) {
2573         ++histo[MIN2(cur-&gt;blocks_filled_count(), histo_len - 1)];
2574       }
2575       out-&gt;print(&quot;Block fill histogram: %u %-4s&quot; SIZE_FORMAT_W(5), id, space_names[id], region_cnt);
2576       for (size_t i = 0; i &lt; histo_len; ++i) {
2577         out-&gt;print(&quot; &quot; SIZE_FORMAT_W(5) &quot; %5.1f%%&quot;,
2578                    histo[i], 100.0 * histo[i] / region_cnt);
2579       }
2580       out-&gt;cr();
2581     }
2582   }
2583 }
2584 #endif // #ifdef ASSERT
2585 
<span class="line-modified">2586 static void compaction_with_stealing_work(TaskTerminator* terminator, uint worker_id) {</span>
2587   assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), &quot;called outside gc&quot;);
2588 
2589   ParCompactionManager* cm =
2590     ParCompactionManager::gc_thread_compaction_manager(worker_id);
2591 
2592   // Drain the stacks that have been preloaded with regions
2593   // that are ready to fill.
2594 
2595   cm-&gt;drain_region_stacks();
2596 
2597   guarantee(cm-&gt;region_stack()-&gt;is_empty(), &quot;Not empty&quot;);
2598 
2599   size_t region_index = 0;
2600 
2601   while (true) {
2602     if (ParCompactionManager::steal(worker_id, region_index)) {
2603       PSParallelCompact::fill_and_update_region(cm, region_index);
2604       cm-&gt;drain_region_stacks();
2605     } else if (PSParallelCompact::steal_unavailable_region(cm, region_index)) {
2606       // Fill and update an unavailable region with the help of a shadow region
</pre>
<hr />
<pre>
2624 
2625 public:
2626   UpdateDensePrefixAndCompactionTask(TaskQueue&amp; tq, uint active_workers) :
2627       AbstractGangTask(&quot;UpdateDensePrefixAndCompactionTask&quot;),
2628       _tq(tq),
2629       _terminator(active_workers, ParCompactionManager::region_array()),
2630       _active_workers(active_workers) {
2631   }
2632   virtual void work(uint worker_id) {
2633     ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(worker_id);
2634 
2635     for (PSParallelCompact::UpdateDensePrefixTask task; _tq.try_claim(task); /* empty */) {
2636       PSParallelCompact::update_and_deadwood_in_dense_prefix(cm,
2637                                                              task._space_id,
2638                                                              task._region_index_start,
2639                                                              task._region_index_end);
2640     }
2641 
2642     // Once a thread has drained it&#39;s stack, it should try to steal regions from
2643     // other threads.
<span class="line-modified">2644     compaction_with_stealing_work(&amp;_terminator, worker_id);</span>
2645   }
2646 };
2647 
2648 void PSParallelCompact::compact() {
2649   GCTraceTime(Info, gc, phases) tm(&quot;Compaction Phase&quot;, &amp;_gc_timer);
2650 
2651   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
2652   PSOldGen* old_gen = heap-&gt;old_gen();
2653   old_gen-&gt;start_array()-&gt;reset();
2654   uint active_gc_threads = ParallelScavengeHeap::heap()-&gt;workers().active_workers();
2655 
2656   // for [0..last_space_id)
2657   //     for [0..active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING)
2658   //         push
2659   //     push
2660   //
2661   // max push count is thus: last_space_id * (active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING + 1)
2662   TaskQueue task_queue(last_space_id * (active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING + 1));
2663   initialize_shadow_regions(active_gc_threads);
2664   prepare_region_draining_tasks(active_gc_threads);
</pre>
</td>
</tr>
</table>
<center><a href="psCompactionManager.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="psParallelCompact.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>