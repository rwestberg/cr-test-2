<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1CollectedHeap.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1BarrierSet.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1CollectedHeap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  62 #include &quot;gc/g1/g1Trace.hpp&quot;
  63 #include &quot;gc/g1/g1YCTypes.hpp&quot;
  64 #include &quot;gc/g1/g1YoungRemSetSamplingThread.hpp&quot;
  65 #include &quot;gc/g1/g1VMOperations.hpp&quot;
  66 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  67 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
  68 #include &quot;gc/g1/heapRegionSet.inline.hpp&quot;
  69 #include &quot;gc/shared/gcBehaviours.hpp&quot;
  70 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
  71 #include &quot;gc/shared/gcId.hpp&quot;
  72 #include &quot;gc/shared/gcLocker.hpp&quot;
  73 #include &quot;gc/shared/gcTimer.hpp&quot;
  74 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  75 #include &quot;gc/shared/generationSpec.hpp&quot;
  76 #include &quot;gc/shared/isGCActiveMark.hpp&quot;
  77 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;
  78 #include &quot;gc/shared/oopStorageParState.hpp&quot;
  79 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
  80 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
  81 #include &quot;gc/shared/referenceProcessor.inline.hpp&quot;

  82 #include &quot;gc/shared/taskqueue.inline.hpp&quot;
  83 #include &quot;gc/shared/weakProcessor.inline.hpp&quot;
  84 #include &quot;gc/shared/workerPolicy.hpp&quot;
  85 #include &quot;logging/log.hpp&quot;
  86 #include &quot;memory/allocation.hpp&quot;
  87 #include &quot;memory/iterator.hpp&quot;
  88 #include &quot;memory/resourceArea.hpp&quot;
  89 #include &quot;memory/universe.hpp&quot;
  90 #include &quot;oops/access.inline.hpp&quot;
  91 #include &quot;oops/compressedOops.inline.hpp&quot;
  92 #include &quot;oops/oop.inline.hpp&quot;
  93 #include &quot;runtime/atomic.hpp&quot;
  94 #include &quot;runtime/flags/flagSetting.hpp&quot;
  95 #include &quot;runtime/handles.inline.hpp&quot;
  96 #include &quot;runtime/init.hpp&quot;
  97 #include &quot;runtime/orderAccess.hpp&quot;
  98 #include &quot;runtime/threadSMR.hpp&quot;
  99 #include &quot;runtime/vmThread.hpp&quot;
 100 #include &quot;utilities/align.hpp&quot;
 101 #include &quot;utilities/bitMap.inline.hpp&quot;
</pre>
<hr />
<pre>
1114   // marking is no longer active. Therefore we need not
1115   // re-enable reference discovery for the CM ref processor.
1116   // That will be done at the start of the next marking cycle.
1117   // We also know that the STW processor should no longer
1118   // discover any new references.
1119   assert(!_ref_processor_stw-&gt;discovery_enabled(), &quot;Postcondition&quot;);
1120   assert(!_ref_processor_cm-&gt;discovery_enabled(), &quot;Postcondition&quot;);
1121   _ref_processor_stw-&gt;verify_no_references_recorded();
1122   _ref_processor_cm-&gt;verify_no_references_recorded();
1123 }
1124 
1125 void G1CollectedHeap::print_heap_after_full_collection(G1HeapTransition* heap_transition) {
1126   // Post collection logging.
1127   // We should do this after we potentially resize the heap so
1128   // that all the COMMIT / UNCOMMIT events are generated before
1129   // the compaction events.
1130   print_hrm_post_compaction();
1131   heap_transition-&gt;print();
1132   print_heap_after_gc();
1133   print_heap_regions();
<span class="line-removed">1134 #ifdef TRACESPINNING</span>
<span class="line-removed">1135   ParallelTaskTerminator::print_termination_counts();</span>
<span class="line-removed">1136 #endif</span>
1137 }
1138 
1139 bool G1CollectedHeap::do_full_collection(bool explicit_gc,
1140                                          bool clear_all_soft_refs) {
1141   assert_at_safepoint_on_vm_thread();
1142 
1143   if (GCLocker::check_active_before_gc()) {
1144     // Full GC was not completed.
1145     return false;
1146   }
1147 
1148   const bool do_clear_all_soft_refs = clear_all_soft_refs ||
1149       soft_ref_policy()-&gt;should_clear_all_soft_refs();
1150 
1151   G1FullCollector collector(this, explicit_gc, do_clear_all_soft_refs);
1152   GCTraceTime(Info, gc) tm(&quot;Pause Full&quot;, NULL, gc_cause(), true);
1153 
1154   collector.prepare_collection();
1155   collector.collect();
1156   collector.complete_collection();
</pre>
<hr />
<pre>
2761 
2762 void G1CollectedHeap::do_concurrent_mark() {
2763   MutexLocker x(CGC_lock, Mutex::_no_safepoint_check_flag);
2764   if (!_cm_thread-&gt;in_progress()) {
2765     _cm_thread-&gt;set_started();
2766     CGC_lock-&gt;notify();
2767   }
2768 }
2769 
2770 size_t G1CollectedHeap::pending_card_num() {
2771   struct CountCardsClosure : public ThreadClosure {
2772     size_t _cards;
2773     CountCardsClosure() : _cards(0) {}
2774     virtual void do_thread(Thread* t) {
2775       _cards += G1ThreadLocalData::dirty_card_queue(t).size();
2776     }
2777   } count_from_threads;
2778   Threads::threads_do(&amp;count_from_threads);
2779 
2780   G1DirtyCardQueueSet&amp; dcqs = G1BarrierSet::dirty_card_queue_set();
<span class="line-removed">2781   dcqs.verify_num_cards();</span>
<span class="line-removed">2782 </span>
2783   return dcqs.num_cards() + count_from_threads._cards;
2784 }
2785 
2786 bool G1CollectedHeap::is_potential_eager_reclaim_candidate(HeapRegion* r) const {
2787   // We don&#39;t nominate objects with many remembered set entries, on
2788   // the assumption that such objects are likely still live.
2789   HeapRegionRemSet* rem_set = r-&gt;rem_set();
2790 
2791   return G1EagerReclaimHumongousObjectsWithStaleRefs ?
2792          rem_set-&gt;occupancy_less_or_equal_than(G1RSetSparseRegionEntries) :
2793          G1EagerReclaimHumongousObjects &amp;&amp; rem_set-&gt;is_empty();
2794 }
2795 
2796 #ifndef PRODUCT
2797 void G1CollectedHeap::verify_region_attr_remset_update() {
2798   class VerifyRegionAttrRemSet : public HeapRegionClosure {
2799   public:
2800     virtual bool do_heap_region(HeapRegion* r) {
2801       G1CollectedHeap* g1h = G1CollectedHeap::heap();
2802       bool const needs_remset_update = g1h-&gt;region_attr(r-&gt;bottom()).needs_remset_update();
</pre>
<hr />
<pre>
3122           // appropriate initialization is done on the CM object.
3123           concurrent_mark()-&gt;post_initial_mark();
3124           // Note that we don&#39;t actually trigger the CM thread at
3125           // this point. We do that later when we&#39;re sure that
3126           // the current thread has completed its logging output.
3127         }
3128 
3129         allocate_dummy_regions();
3130 
3131         _allocator-&gt;init_mutator_alloc_regions();
3132 
3133         expand_heap_after_young_collection();
3134 
3135         double sample_end_time_sec = os::elapsedTime();
3136         double pause_time_ms = (sample_end_time_sec - sample_start_time_sec) * MILLIUNITS;
3137         policy()-&gt;record_collection_pause_end(pause_time_ms);
3138       }
3139 
3140       verify_after_young_collection(verify_type);
3141 
<span class="line-removed">3142 #ifdef TRACESPINNING</span>
<span class="line-removed">3143       ParallelTaskTerminator::print_termination_counts();</span>
<span class="line-removed">3144 #endif</span>
<span class="line-removed">3145 </span>
3146       gc_epilogue(false);
3147     }
3148 
3149     // Print the remainder of the GC log output.
3150     if (evacuation_failed()) {
3151       log_info(gc)(&quot;To-space exhausted&quot;);
3152     }
3153 
3154     policy()-&gt;print_phases();
3155     heap_transition.print();
3156 
3157     _hrm-&gt;verify_optional();
3158     _verifier-&gt;verify_region_sets_optional();
3159 
3160     TASKQUEUE_STATS_ONLY(print_taskqueue_stats());
3161     TASKQUEUE_STATS_ONLY(reset_taskqueue_stats());
3162 
3163     print_heap_after_gc();
3164     print_heap_regions();
3165     trace_heap_after_gc(_gc_tracer_stw);
</pre>
<hr />
<pre>
3459     _g1h(g1h),
3460     _pss(per_thread_states),
3461     _queues(task_queues),
3462     _workers(workers)
3463   {
3464     g1h-&gt;ref_processor_stw()-&gt;set_active_mt_degree(workers-&gt;active_workers());
3465   }
3466 
3467   // Executes the given task using concurrent marking worker threads.
3468   virtual void execute(ProcessTask&amp; task, uint ergo_workers);
3469 };
3470 
3471 // Gang task for possibly parallel reference processing
3472 
3473 class G1STWRefProcTaskProxy: public AbstractGangTask {
3474   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
3475   ProcessTask&amp;     _proc_task;
3476   G1CollectedHeap* _g1h;
3477   G1ParScanThreadStateSet* _pss;
3478   RefToScanQueueSet* _task_queues;
<span class="line-modified">3479   ParallelTaskTerminator* _terminator;</span>
3480 
3481 public:
3482   G1STWRefProcTaskProxy(ProcessTask&amp; proc_task,
3483                         G1CollectedHeap* g1h,
3484                         G1ParScanThreadStateSet* per_thread_states,
3485                         RefToScanQueueSet *task_queues,
<span class="line-modified">3486                         ParallelTaskTerminator* terminator) :</span>
3487     AbstractGangTask(&quot;Process reference objects in parallel&quot;),
3488     _proc_task(proc_task),
3489     _g1h(g1h),
3490     _pss(per_thread_states),
3491     _task_queues(task_queues),
3492     _terminator(terminator)
3493   {}
3494 
3495   virtual void work(uint worker_id) {
3496     // The reference processing task executed by a single worker.
3497     ResourceMark rm;
3498     HandleMark   hm;
3499 
3500     G1STWIsAliveClosure is_alive(_g1h);
3501 
3502     G1ParScanThreadState* pss = _pss-&gt;state_for_worker(worker_id);
3503     pss-&gt;set_ref_discoverer(NULL);
3504 
3505     // Keep alive closure.
3506     G1CopyingKeepAliveClosure keep_alive(_g1h, pss);
</pre>
<hr />
<pre>
3511     // Call the reference processing task&#39;s work routine.
3512     _proc_task.work(worker_id, is_alive, keep_alive, drain_queue);
3513 
3514     // Note we cannot assert that the refs array is empty here as not all
3515     // of the processing tasks (specifically phase2 - pp2_work) execute
3516     // the complete_gc closure (which ordinarily would drain the queue) so
3517     // the queue may not be empty.
3518   }
3519 };
3520 
3521 // Driver routine for parallel reference processing.
3522 // Creates an instance of the ref processing gang
3523 // task and has the worker threads execute it.
3524 void G1STWRefProcTaskExecutor::execute(ProcessTask&amp; proc_task, uint ergo_workers) {
3525   assert(_workers != NULL, &quot;Need parallel worker threads.&quot;);
3526 
3527   assert(_workers-&gt;active_workers() &gt;= ergo_workers,
3528          &quot;Ergonomically chosen workers (%u) should be less than or equal to active workers (%u)&quot;,
3529          ergo_workers, _workers-&gt;active_workers());
3530   TaskTerminator terminator(ergo_workers, _queues);
<span class="line-modified">3531   G1STWRefProcTaskProxy proc_task_proxy(proc_task, _g1h, _pss, _queues, terminator.terminator());</span>
3532 
3533   _workers-&gt;run_task(&amp;proc_task_proxy, ergo_workers);
3534 }
3535 
3536 // End of weak reference support closures
3537 
3538 void G1CollectedHeap::process_discovered_references(G1ParScanThreadStateSet* per_thread_states) {
3539   double ref_proc_start = os::elapsedTime();
3540 
3541   ReferenceProcessor* rp = _ref_processor_stw;
3542   assert(rp-&gt;discovery_enabled(), &quot;should have been enabled&quot;);
3543 
3544   // Closure to test whether a referent is alive.
3545   G1STWIsAliveClosure is_alive(this);
3546 
3547   // Even when parallel reference processing is enabled, the processing
3548   // of JNI refs is serial and performed serially by the current thread
3549   // rather than by a worker. The following PSS will be used for processing
3550   // JNI refs.
3551 
</pre>
<hr />
<pre>
3807 
3808   // Should G1EvacuationFailureALot be in effect for this GC?
3809   NOT_PRODUCT(set_evacuation_failure_alot_for_current_gc();)
3810 }
3811 
3812 class G1EvacuateRegionsBaseTask : public AbstractGangTask {
3813 protected:
3814   G1CollectedHeap* _g1h;
3815   G1ParScanThreadStateSet* _per_thread_states;
3816   RefToScanQueueSet* _task_queues;
3817   TaskTerminator _terminator;
3818   uint _num_workers;
3819 
3820   void evacuate_live_objects(G1ParScanThreadState* pss,
3821                              uint worker_id,
3822                              G1GCPhaseTimes::GCParPhases objcopy_phase,
3823                              G1GCPhaseTimes::GCParPhases termination_phase) {
3824     G1GCPhaseTimes* p = _g1h-&gt;phase_times();
3825 
3826     Ticks start = Ticks::now();
<span class="line-modified">3827     G1ParEvacuateFollowersClosure cl(_g1h, pss, _task_queues, _terminator.terminator(), objcopy_phase);</span>
3828     cl.do_void();
3829 
3830     assert(pss-&gt;queue_is_empty(), &quot;should be empty&quot;);
3831 
3832     Tickspan evac_time = (Ticks::now() - start);
3833     p-&gt;record_or_add_time_secs(objcopy_phase, worker_id, evac_time.seconds() - cl.term_time());
3834 
3835     if (termination_phase == G1GCPhaseTimes::Termination) {
3836       p-&gt;record_time_secs(termination_phase, worker_id, cl.term_time());
3837       p-&gt;record_thread_work_item(termination_phase, worker_id, cl.term_attempts());
3838     } else {
3839       p-&gt;record_or_add_time_secs(termination_phase, worker_id, cl.term_time());
3840       p-&gt;record_or_add_thread_work_item(termination_phase, worker_id, cl.term_attempts());
3841     }
3842     assert(pss-&gt;trim_ticks().seconds() == 0.0, &quot;Unexpected partial trimming during evacuation&quot;);
3843   }
3844 
3845   virtual void start_work(uint worker_id) { }
3846 
3847   virtual void end_work(uint worker_id) { }
</pre>
</td>
<td>
<hr />
<pre>
  62 #include &quot;gc/g1/g1Trace.hpp&quot;
  63 #include &quot;gc/g1/g1YCTypes.hpp&quot;
  64 #include &quot;gc/g1/g1YoungRemSetSamplingThread.hpp&quot;
  65 #include &quot;gc/g1/g1VMOperations.hpp&quot;
  66 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  67 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
  68 #include &quot;gc/g1/heapRegionSet.inline.hpp&quot;
  69 #include &quot;gc/shared/gcBehaviours.hpp&quot;
  70 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
  71 #include &quot;gc/shared/gcId.hpp&quot;
  72 #include &quot;gc/shared/gcLocker.hpp&quot;
  73 #include &quot;gc/shared/gcTimer.hpp&quot;
  74 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  75 #include &quot;gc/shared/generationSpec.hpp&quot;
  76 #include &quot;gc/shared/isGCActiveMark.hpp&quot;
  77 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;
  78 #include &quot;gc/shared/oopStorageParState.hpp&quot;
  79 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
  80 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
  81 #include &quot;gc/shared/referenceProcessor.inline.hpp&quot;
<span class="line-added">  82 #include &quot;gc/shared/taskTerminator.hpp&quot;</span>
  83 #include &quot;gc/shared/taskqueue.inline.hpp&quot;
  84 #include &quot;gc/shared/weakProcessor.inline.hpp&quot;
  85 #include &quot;gc/shared/workerPolicy.hpp&quot;
  86 #include &quot;logging/log.hpp&quot;
  87 #include &quot;memory/allocation.hpp&quot;
  88 #include &quot;memory/iterator.hpp&quot;
  89 #include &quot;memory/resourceArea.hpp&quot;
  90 #include &quot;memory/universe.hpp&quot;
  91 #include &quot;oops/access.inline.hpp&quot;
  92 #include &quot;oops/compressedOops.inline.hpp&quot;
  93 #include &quot;oops/oop.inline.hpp&quot;
  94 #include &quot;runtime/atomic.hpp&quot;
  95 #include &quot;runtime/flags/flagSetting.hpp&quot;
  96 #include &quot;runtime/handles.inline.hpp&quot;
  97 #include &quot;runtime/init.hpp&quot;
  98 #include &quot;runtime/orderAccess.hpp&quot;
  99 #include &quot;runtime/threadSMR.hpp&quot;
 100 #include &quot;runtime/vmThread.hpp&quot;
 101 #include &quot;utilities/align.hpp&quot;
 102 #include &quot;utilities/bitMap.inline.hpp&quot;
</pre>
<hr />
<pre>
1115   // marking is no longer active. Therefore we need not
1116   // re-enable reference discovery for the CM ref processor.
1117   // That will be done at the start of the next marking cycle.
1118   // We also know that the STW processor should no longer
1119   // discover any new references.
1120   assert(!_ref_processor_stw-&gt;discovery_enabled(), &quot;Postcondition&quot;);
1121   assert(!_ref_processor_cm-&gt;discovery_enabled(), &quot;Postcondition&quot;);
1122   _ref_processor_stw-&gt;verify_no_references_recorded();
1123   _ref_processor_cm-&gt;verify_no_references_recorded();
1124 }
1125 
1126 void G1CollectedHeap::print_heap_after_full_collection(G1HeapTransition* heap_transition) {
1127   // Post collection logging.
1128   // We should do this after we potentially resize the heap so
1129   // that all the COMMIT / UNCOMMIT events are generated before
1130   // the compaction events.
1131   print_hrm_post_compaction();
1132   heap_transition-&gt;print();
1133   print_heap_after_gc();
1134   print_heap_regions();



1135 }
1136 
1137 bool G1CollectedHeap::do_full_collection(bool explicit_gc,
1138                                          bool clear_all_soft_refs) {
1139   assert_at_safepoint_on_vm_thread();
1140 
1141   if (GCLocker::check_active_before_gc()) {
1142     // Full GC was not completed.
1143     return false;
1144   }
1145 
1146   const bool do_clear_all_soft_refs = clear_all_soft_refs ||
1147       soft_ref_policy()-&gt;should_clear_all_soft_refs();
1148 
1149   G1FullCollector collector(this, explicit_gc, do_clear_all_soft_refs);
1150   GCTraceTime(Info, gc) tm(&quot;Pause Full&quot;, NULL, gc_cause(), true);
1151 
1152   collector.prepare_collection();
1153   collector.collect();
1154   collector.complete_collection();
</pre>
<hr />
<pre>
2759 
2760 void G1CollectedHeap::do_concurrent_mark() {
2761   MutexLocker x(CGC_lock, Mutex::_no_safepoint_check_flag);
2762   if (!_cm_thread-&gt;in_progress()) {
2763     _cm_thread-&gt;set_started();
2764     CGC_lock-&gt;notify();
2765   }
2766 }
2767 
2768 size_t G1CollectedHeap::pending_card_num() {
2769   struct CountCardsClosure : public ThreadClosure {
2770     size_t _cards;
2771     CountCardsClosure() : _cards(0) {}
2772     virtual void do_thread(Thread* t) {
2773       _cards += G1ThreadLocalData::dirty_card_queue(t).size();
2774     }
2775   } count_from_threads;
2776   Threads::threads_do(&amp;count_from_threads);
2777 
2778   G1DirtyCardQueueSet&amp; dcqs = G1BarrierSet::dirty_card_queue_set();


2779   return dcqs.num_cards() + count_from_threads._cards;
2780 }
2781 
2782 bool G1CollectedHeap::is_potential_eager_reclaim_candidate(HeapRegion* r) const {
2783   // We don&#39;t nominate objects with many remembered set entries, on
2784   // the assumption that such objects are likely still live.
2785   HeapRegionRemSet* rem_set = r-&gt;rem_set();
2786 
2787   return G1EagerReclaimHumongousObjectsWithStaleRefs ?
2788          rem_set-&gt;occupancy_less_or_equal_than(G1RSetSparseRegionEntries) :
2789          G1EagerReclaimHumongousObjects &amp;&amp; rem_set-&gt;is_empty();
2790 }
2791 
2792 #ifndef PRODUCT
2793 void G1CollectedHeap::verify_region_attr_remset_update() {
2794   class VerifyRegionAttrRemSet : public HeapRegionClosure {
2795   public:
2796     virtual bool do_heap_region(HeapRegion* r) {
2797       G1CollectedHeap* g1h = G1CollectedHeap::heap();
2798       bool const needs_remset_update = g1h-&gt;region_attr(r-&gt;bottom()).needs_remset_update();
</pre>
<hr />
<pre>
3118           // appropriate initialization is done on the CM object.
3119           concurrent_mark()-&gt;post_initial_mark();
3120           // Note that we don&#39;t actually trigger the CM thread at
3121           // this point. We do that later when we&#39;re sure that
3122           // the current thread has completed its logging output.
3123         }
3124 
3125         allocate_dummy_regions();
3126 
3127         _allocator-&gt;init_mutator_alloc_regions();
3128 
3129         expand_heap_after_young_collection();
3130 
3131         double sample_end_time_sec = os::elapsedTime();
3132         double pause_time_ms = (sample_end_time_sec - sample_start_time_sec) * MILLIUNITS;
3133         policy()-&gt;record_collection_pause_end(pause_time_ms);
3134       }
3135 
3136       verify_after_young_collection(verify_type);
3137 




3138       gc_epilogue(false);
3139     }
3140 
3141     // Print the remainder of the GC log output.
3142     if (evacuation_failed()) {
3143       log_info(gc)(&quot;To-space exhausted&quot;);
3144     }
3145 
3146     policy()-&gt;print_phases();
3147     heap_transition.print();
3148 
3149     _hrm-&gt;verify_optional();
3150     _verifier-&gt;verify_region_sets_optional();
3151 
3152     TASKQUEUE_STATS_ONLY(print_taskqueue_stats());
3153     TASKQUEUE_STATS_ONLY(reset_taskqueue_stats());
3154 
3155     print_heap_after_gc();
3156     print_heap_regions();
3157     trace_heap_after_gc(_gc_tracer_stw);
</pre>
<hr />
<pre>
3451     _g1h(g1h),
3452     _pss(per_thread_states),
3453     _queues(task_queues),
3454     _workers(workers)
3455   {
3456     g1h-&gt;ref_processor_stw()-&gt;set_active_mt_degree(workers-&gt;active_workers());
3457   }
3458 
3459   // Executes the given task using concurrent marking worker threads.
3460   virtual void execute(ProcessTask&amp; task, uint ergo_workers);
3461 };
3462 
3463 // Gang task for possibly parallel reference processing
3464 
3465 class G1STWRefProcTaskProxy: public AbstractGangTask {
3466   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
3467   ProcessTask&amp;     _proc_task;
3468   G1CollectedHeap* _g1h;
3469   G1ParScanThreadStateSet* _pss;
3470   RefToScanQueueSet* _task_queues;
<span class="line-modified">3471   TaskTerminator* _terminator;</span>
3472 
3473 public:
3474   G1STWRefProcTaskProxy(ProcessTask&amp; proc_task,
3475                         G1CollectedHeap* g1h,
3476                         G1ParScanThreadStateSet* per_thread_states,
3477                         RefToScanQueueSet *task_queues,
<span class="line-modified">3478                         TaskTerminator* terminator) :</span>
3479     AbstractGangTask(&quot;Process reference objects in parallel&quot;),
3480     _proc_task(proc_task),
3481     _g1h(g1h),
3482     _pss(per_thread_states),
3483     _task_queues(task_queues),
3484     _terminator(terminator)
3485   {}
3486 
3487   virtual void work(uint worker_id) {
3488     // The reference processing task executed by a single worker.
3489     ResourceMark rm;
3490     HandleMark   hm;
3491 
3492     G1STWIsAliveClosure is_alive(_g1h);
3493 
3494     G1ParScanThreadState* pss = _pss-&gt;state_for_worker(worker_id);
3495     pss-&gt;set_ref_discoverer(NULL);
3496 
3497     // Keep alive closure.
3498     G1CopyingKeepAliveClosure keep_alive(_g1h, pss);
</pre>
<hr />
<pre>
3503     // Call the reference processing task&#39;s work routine.
3504     _proc_task.work(worker_id, is_alive, keep_alive, drain_queue);
3505 
3506     // Note we cannot assert that the refs array is empty here as not all
3507     // of the processing tasks (specifically phase2 - pp2_work) execute
3508     // the complete_gc closure (which ordinarily would drain the queue) so
3509     // the queue may not be empty.
3510   }
3511 };
3512 
3513 // Driver routine for parallel reference processing.
3514 // Creates an instance of the ref processing gang
3515 // task and has the worker threads execute it.
3516 void G1STWRefProcTaskExecutor::execute(ProcessTask&amp; proc_task, uint ergo_workers) {
3517   assert(_workers != NULL, &quot;Need parallel worker threads.&quot;);
3518 
3519   assert(_workers-&gt;active_workers() &gt;= ergo_workers,
3520          &quot;Ergonomically chosen workers (%u) should be less than or equal to active workers (%u)&quot;,
3521          ergo_workers, _workers-&gt;active_workers());
3522   TaskTerminator terminator(ergo_workers, _queues);
<span class="line-modified">3523   G1STWRefProcTaskProxy proc_task_proxy(proc_task, _g1h, _pss, _queues, &amp;terminator);</span>
3524 
3525   _workers-&gt;run_task(&amp;proc_task_proxy, ergo_workers);
3526 }
3527 
3528 // End of weak reference support closures
3529 
3530 void G1CollectedHeap::process_discovered_references(G1ParScanThreadStateSet* per_thread_states) {
3531   double ref_proc_start = os::elapsedTime();
3532 
3533   ReferenceProcessor* rp = _ref_processor_stw;
3534   assert(rp-&gt;discovery_enabled(), &quot;should have been enabled&quot;);
3535 
3536   // Closure to test whether a referent is alive.
3537   G1STWIsAliveClosure is_alive(this);
3538 
3539   // Even when parallel reference processing is enabled, the processing
3540   // of JNI refs is serial and performed serially by the current thread
3541   // rather than by a worker. The following PSS will be used for processing
3542   // JNI refs.
3543 
</pre>
<hr />
<pre>
3799 
3800   // Should G1EvacuationFailureALot be in effect for this GC?
3801   NOT_PRODUCT(set_evacuation_failure_alot_for_current_gc();)
3802 }
3803 
3804 class G1EvacuateRegionsBaseTask : public AbstractGangTask {
3805 protected:
3806   G1CollectedHeap* _g1h;
3807   G1ParScanThreadStateSet* _per_thread_states;
3808   RefToScanQueueSet* _task_queues;
3809   TaskTerminator _terminator;
3810   uint _num_workers;
3811 
3812   void evacuate_live_objects(G1ParScanThreadState* pss,
3813                              uint worker_id,
3814                              G1GCPhaseTimes::GCParPhases objcopy_phase,
3815                              G1GCPhaseTimes::GCParPhases termination_phase) {
3816     G1GCPhaseTimes* p = _g1h-&gt;phase_times();
3817 
3818     Ticks start = Ticks::now();
<span class="line-modified">3819     G1ParEvacuateFollowersClosure cl(_g1h, pss, _task_queues, &amp;_terminator, objcopy_phase);</span>
3820     cl.do_void();
3821 
3822     assert(pss-&gt;queue_is_empty(), &quot;should be empty&quot;);
3823 
3824     Tickspan evac_time = (Ticks::now() - start);
3825     p-&gt;record_or_add_time_secs(objcopy_phase, worker_id, evac_time.seconds() - cl.term_time());
3826 
3827     if (termination_phase == G1GCPhaseTimes::Termination) {
3828       p-&gt;record_time_secs(termination_phase, worker_id, cl.term_time());
3829       p-&gt;record_thread_work_item(termination_phase, worker_id, cl.term_attempts());
3830     } else {
3831       p-&gt;record_or_add_time_secs(termination_phase, worker_id, cl.term_time());
3832       p-&gt;record_or_add_thread_work_item(termination_phase, worker_id, cl.term_attempts());
3833     }
3834     assert(pss-&gt;trim_ticks().seconds() == 0.0, &quot;Unexpected partial trimming during evacuation&quot;);
3835   }
3836 
3837   virtual void start_work(uint worker_id) { }
3838 
3839   virtual void end_work(uint worker_id) { }
</pre>
</td>
</tr>
</table>
<center><a href="g1BarrierSet.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>