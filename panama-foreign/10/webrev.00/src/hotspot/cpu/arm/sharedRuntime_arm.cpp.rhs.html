<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/arm/sharedRuntime_arm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
<a name="1" id="anc1"></a><span class="line-modified">   2  * Copyright (c) 2008, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/assembler.hpp&quot;
  27 #include &quot;assembler_arm.inline.hpp&quot;
  28 #include &quot;code/debugInfoRec.hpp&quot;
  29 #include &quot;code/icBuffer.hpp&quot;
  30 #include &quot;code/vtableStubs.hpp&quot;
  31 #include &quot;interpreter/interpreter.hpp&quot;
  32 #include &quot;logging/log.hpp&quot;
  33 #include &quot;memory/resourceArea.hpp&quot;
  34 #include &quot;oops/compiledICHolder.hpp&quot;
  35 #include &quot;oops/klass.inline.hpp&quot;
  36 #include &quot;runtime/sharedRuntime.hpp&quot;
<a name="2" id="anc2"></a><span class="line-added">  37 #include &quot;runtime/safepointMechanism.hpp&quot;</span>
  38 #include &quot;runtime/vframeArray.hpp&quot;
  39 #include &quot;utilities/align.hpp&quot;
  40 #include &quot;vmreg_arm.inline.hpp&quot;
  41 #ifdef COMPILER1
  42 #include &quot;c1/c1_Runtime1.hpp&quot;
  43 #endif
  44 #ifdef COMPILER2
  45 #include &quot;opto/runtime.hpp&quot;
  46 #endif
  47 
  48 #define __ masm-&gt;
  49 
  50 class RegisterSaver {
  51 public:
  52 
  53   // Special registers:
  54   //              32-bit ARM     64-bit ARM
  55   //  Rthread:       R10            R28
  56   //  LR:            R14            R30
  57 
  58   // Rthread is callee saved in the C ABI and never changed by compiled code:
  59   // no need to save it.
  60 
  61   // 2 slots for LR: the one at LR_offset and an other one at R14/R30_offset.
  62   // The one at LR_offset is a return address that is needed by stack walking.
  63   // A c2 method uses LR as a standard register so it may be live when we
  64   // branch to the runtime. The slot at R14/R30_offset is for the value of LR
  65   // in case it&#39;s live in the method we are coming from.
  66 
  67 
  68   enum RegisterLayout {
  69     fpu_save_size = FloatRegisterImpl::number_of_registers,
  70 #ifndef __SOFTFP__
  71     D0_offset = 0,
  72 #endif
  73     R0_offset = fpu_save_size,
  74     R1_offset,
  75     R2_offset,
  76     R3_offset,
  77     R4_offset,
  78     R5_offset,
  79     R6_offset,
  80 #if (FP_REG_NUM != 7)
  81     // if not saved as FP
  82     R7_offset,
  83 #endif
  84     R8_offset,
  85     R9_offset,
  86 #if (FP_REG_NUM != 11)
  87     // if not saved as FP
  88     R11_offset,
  89 #endif
  90     R12_offset,
  91     R14_offset,
  92     FP_offset,
  93     LR_offset,
  94     reg_save_size,
  95 
  96     Rmethod_offset = R9_offset,
  97     Rtemp_offset = R12_offset,
  98   };
  99 
 100   // all regs but Rthread (R10), FP (R7 or R11), SP and PC
 101   // (altFP_7_11 is the one amoung R7 and R11 which is not FP)
 102 #define SAVED_BASE_REGS (RegisterSet(R0, R6) | RegisterSet(R8, R9) | RegisterSet(R12) | R14 | altFP_7_11)
 103 
 104 
 105   //  When LR may be live in the nmethod from which we are comming
 106   //  then lr_saved is true, the return address is saved before the
 107   //  call to save_live_register by the caller and LR contains the
 108   //  live value.
 109 
 110   static OopMap* save_live_registers(MacroAssembler* masm,
 111                                      int* total_frame_words,
 112                                      bool lr_saved = false);
 113   static void restore_live_registers(MacroAssembler* masm, bool restore_lr = true);
 114 
 115 };
 116 
 117 
 118 
 119 
 120 OopMap* RegisterSaver::save_live_registers(MacroAssembler* masm,
 121                                            int* total_frame_words,
 122                                            bool lr_saved) {
 123   *total_frame_words = reg_save_size;
 124 
 125   OopMapSet *oop_maps = new OopMapSet();
 126   OopMap* map = new OopMap(VMRegImpl::slots_per_word * (*total_frame_words), 0);
 127 
 128   if (lr_saved) {
 129     __ push(RegisterSet(FP));
 130   } else {
 131     __ push(RegisterSet(FP) | RegisterSet(LR));
 132   }
 133   __ push(SAVED_BASE_REGS);
 134   if (HaveVFP) {
 135     if (VM_Version::has_vfp3_32()) {
 136       __ fstmdbd(SP, FloatRegisterSet(D16, 16), writeback);
 137     } else {
 138       if (FloatRegisterImpl::number_of_registers &gt; 32) {
 139         assert(FloatRegisterImpl::number_of_registers == 64, &quot;nb fp registers should be 64&quot;);
 140         __ sub(SP, SP, 32 * wordSize);
 141       }
 142     }
 143     __ fstmdbd(SP, FloatRegisterSet(D0, 16), writeback);
 144   } else {
 145     __ sub(SP, SP, fpu_save_size * wordSize);
 146   }
 147 
 148   int i;
 149   int j=0;
 150   for (i = R0_offset; i &lt;= R9_offset; i++) {
 151     if (j == FP_REG_NUM) {
 152       // skip the FP register, managed below.
 153       j++;
 154     }
 155     map-&gt;set_callee_saved(VMRegImpl::stack2reg(i), as_Register(j)-&gt;as_VMReg());
 156     j++;
 157   }
 158   assert(j == R10-&gt;encoding(), &quot;must be&quot;);
 159 #if (FP_REG_NUM != 11)
 160   // add R11, if not managed as FP
 161   map-&gt;set_callee_saved(VMRegImpl::stack2reg(R11_offset), R11-&gt;as_VMReg());
 162 #endif
 163   map-&gt;set_callee_saved(VMRegImpl::stack2reg(R12_offset), R12-&gt;as_VMReg());
 164   map-&gt;set_callee_saved(VMRegImpl::stack2reg(R14_offset), R14-&gt;as_VMReg());
 165   if (HaveVFP) {
 166     for (i = 0; i &lt; (VM_Version::has_vfp3_32() ? 64 : 32); i+=2) {
 167       map-&gt;set_callee_saved(VMRegImpl::stack2reg(i), as_FloatRegister(i)-&gt;as_VMReg());
 168       map-&gt;set_callee_saved(VMRegImpl::stack2reg(i + 1), as_FloatRegister(i)-&gt;as_VMReg()-&gt;next());
 169     }
 170   }
 171 
 172   return map;
 173 }
 174 
 175 void RegisterSaver::restore_live_registers(MacroAssembler* masm, bool restore_lr) {
 176   if (HaveVFP) {
 177     __ fldmiad(SP, FloatRegisterSet(D0, 16), writeback);
 178     if (VM_Version::has_vfp3_32()) {
 179       __ fldmiad(SP, FloatRegisterSet(D16, 16), writeback);
 180     } else {
 181       if (FloatRegisterImpl::number_of_registers &gt; 32) {
 182         assert(FloatRegisterImpl::number_of_registers == 64, &quot;nb fp registers should be 64&quot;);
 183         __ add(SP, SP, 32 * wordSize);
 184       }
 185     }
 186   } else {
 187     __ add(SP, SP, fpu_save_size * wordSize);
 188   }
 189   __ pop(SAVED_BASE_REGS);
 190   if (restore_lr) {
 191     __ pop(RegisterSet(FP) | RegisterSet(LR));
 192   } else {
 193     __ pop(RegisterSet(FP));
 194   }
 195 }
 196 
 197 
 198 static void push_result_registers(MacroAssembler* masm, BasicType ret_type) {
 199 #ifdef __ABI_HARD__
 200   if (ret_type == T_DOUBLE || ret_type == T_FLOAT) {
 201     __ sub(SP, SP, 8);
 202     __ fstd(D0, Address(SP));
 203     return;
 204   }
 205 #endif // __ABI_HARD__
 206   __ raw_push(R0, R1);
 207 }
 208 
 209 static void pop_result_registers(MacroAssembler* masm, BasicType ret_type) {
 210 #ifdef __ABI_HARD__
 211   if (ret_type == T_DOUBLE || ret_type == T_FLOAT) {
 212     __ fldd(D0, Address(SP));
 213     __ add(SP, SP, 8);
 214     return;
 215   }
 216 #endif // __ABI_HARD__
 217   __ raw_pop(R0, R1);
 218 }
 219 
 220 static void push_param_registers(MacroAssembler* masm, int fp_regs_in_arguments) {
 221   // R1-R3 arguments need to be saved, but we push 4 registers for 8-byte alignment
 222   __ push(RegisterSet(R0, R3));
 223 
 224 #ifdef __ABI_HARD__
 225   // preserve arguments
 226   // Likely not needed as the locking code won&#39;t probably modify volatile FP registers,
 227   // but there is no way to guarantee that
 228   if (fp_regs_in_arguments) {
 229     // convert fp_regs_in_arguments to a number of double registers
 230     int double_regs_num = (fp_regs_in_arguments + 1) &gt;&gt; 1;
 231     __ fstmdbd(SP, FloatRegisterSet(D0, double_regs_num), writeback);
 232   }
 233 #endif // __ ABI_HARD__
 234 }
 235 
 236 static void pop_param_registers(MacroAssembler* masm, int fp_regs_in_arguments) {
 237 #ifdef __ABI_HARD__
 238   if (fp_regs_in_arguments) {
 239     int double_regs_num = (fp_regs_in_arguments + 1) &gt;&gt; 1;
 240     __ fldmiad(SP, FloatRegisterSet(D0, double_regs_num), writeback);
 241   }
 242 #endif // __ABI_HARD__
 243 
 244   __ pop(RegisterSet(R0, R3));
 245 }
 246 
 247 
 248 
 249 // Is vector&#39;s size (in bytes) bigger than a size saved by default?
 250 // All vector registers are saved by default on ARM.
 251 bool SharedRuntime::is_wide_vector(int size) {
 252   return false;
 253 }
 254 
 255 size_t SharedRuntime::trampoline_size() {
 256   return 16;
 257 }
 258 
 259 void SharedRuntime::generate_trampoline(MacroAssembler *masm, address destination) {
 260   InlinedAddress dest(destination);
 261   __ indirect_jump(dest, Rtemp);
 262   __ bind_literal(dest);
 263 }
 264 
 265 int SharedRuntime::c_calling_convention(const BasicType *sig_bt,
 266                                         VMRegPair *regs,
 267                                         VMRegPair *regs2,
 268                                         int total_args_passed) {
 269   assert(regs2 == NULL, &quot;not needed on arm&quot;);
 270 
 271   int slot = 0;
 272   int ireg = 0;
 273 #ifdef __ABI_HARD__
 274   int fp_slot = 0;
 275   int single_fpr_slot = 0;
 276 #endif // __ABI_HARD__
 277   for (int i = 0; i &lt; total_args_passed; i++) {
 278     switch (sig_bt[i]) {
 279     case T_SHORT:
 280     case T_CHAR:
 281     case T_BYTE:
 282     case T_BOOLEAN:
 283     case T_INT:
 284     case T_ARRAY:
 285     case T_OBJECT:
 286     case T_ADDRESS:
 287     case T_METADATA:
 288 #ifndef __ABI_HARD__
 289     case T_FLOAT:
 290 #endif // !__ABI_HARD__
 291       if (ireg &lt; 4) {
 292         Register r = as_Register(ireg);
 293         regs[i].set1(r-&gt;as_VMReg());
 294         ireg++;
 295       } else {
 296         regs[i].set1(VMRegImpl::stack2reg(slot));
 297         slot++;
 298       }
 299       break;
 300     case T_LONG:
 301 #ifndef __ABI_HARD__
 302     case T_DOUBLE:
 303 #endif // !__ABI_HARD__
 304       assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i+1] == T_VOID, &quot;missing Half&quot; );
 305       if (ireg &lt;= 2) {
 306 #if (ALIGN_WIDE_ARGUMENTS == 1)
 307         if(ireg &amp; 1) ireg++;  // Aligned location required
 308 #endif
 309         Register r1 = as_Register(ireg);
 310         Register r2 = as_Register(ireg + 1);
 311         regs[i].set_pair(r2-&gt;as_VMReg(), r1-&gt;as_VMReg());
 312         ireg += 2;
 313 #if (ALIGN_WIDE_ARGUMENTS == 0)
 314       } else if (ireg == 3) {
 315         // uses R3 + one stack slot
 316         Register r = as_Register(ireg);
 317         regs[i].set_pair(VMRegImpl::stack2reg(slot), r-&gt;as_VMReg());
 318         ireg += 1;
 319         slot += 1;
 320 #endif
 321       } else {
 322         if (slot &amp; 1) slot++; // Aligned location required
 323         regs[i].set_pair(VMRegImpl::stack2reg(slot+1), VMRegImpl::stack2reg(slot));
 324         slot += 2;
 325         ireg = 4;
 326       }
 327       break;
 328     case T_VOID:
 329       regs[i].set_bad();
 330       break;
 331 #ifdef __ABI_HARD__
 332     case T_FLOAT:
 333       if ((fp_slot &lt; 16)||(single_fpr_slot &amp; 1)) {
 334         if ((single_fpr_slot &amp; 1) == 0) {
 335           single_fpr_slot = fp_slot;
 336           fp_slot += 2;
 337         }
 338         FloatRegister r = as_FloatRegister(single_fpr_slot);
 339         single_fpr_slot++;
 340         regs[i].set1(r-&gt;as_VMReg());
 341       } else {
 342         regs[i].set1(VMRegImpl::stack2reg(slot));
 343         slot++;
 344       }
 345       break;
 346     case T_DOUBLE:
 347       assert(ALIGN_WIDE_ARGUMENTS == 1, &quot;ABI_HARD not supported with unaligned wide arguments&quot;);
 348       if (fp_slot &lt;= 14) {
 349         FloatRegister r1 = as_FloatRegister(fp_slot);
 350         FloatRegister r2 = as_FloatRegister(fp_slot+1);
 351         regs[i].set_pair(r2-&gt;as_VMReg(), r1-&gt;as_VMReg());
 352         fp_slot += 2;
 353       } else {
 354         if(slot &amp; 1) slot++;
 355         regs[i].set_pair(VMRegImpl::stack2reg(slot+1), VMRegImpl::stack2reg(slot));
 356         slot += 2;
 357         single_fpr_slot = 16;
 358       }
 359       break;
 360 #endif // __ABI_HARD__
 361     default:
 362       ShouldNotReachHere();
 363     }
 364   }
 365   return slot;
 366 }
 367 
 368 int SharedRuntime::java_calling_convention(const BasicType *sig_bt,
 369                                            VMRegPair *regs,
 370                                            int total_args_passed,
 371                                            int is_outgoing) {
 372 #ifdef __SOFTFP__
 373   // soft float is the same as the C calling convention.
 374   return c_calling_convention(sig_bt, regs, NULL, total_args_passed);
 375 #endif // __SOFTFP__
 376   (void) is_outgoing;
 377   int slot = 0;
 378   int ireg = 0;
 379   int freg = 0;
 380   int single_fpr = 0;
 381 
 382   for (int i = 0; i &lt; total_args_passed; i++) {
 383     switch (sig_bt[i]) {
 384     case T_SHORT:
 385     case T_CHAR:
 386     case T_BYTE:
 387     case T_BOOLEAN:
 388     case T_INT:
 389     case T_ARRAY:
 390     case T_OBJECT:
 391     case T_ADDRESS:
 392       if (ireg &lt; 4) {
 393         Register r = as_Register(ireg++);
 394         regs[i].set1(r-&gt;as_VMReg());
 395       } else {
 396         regs[i].set1(VMRegImpl::stack2reg(slot++));
 397       }
 398       break;
 399     case T_FLOAT:
 400       // C2 utilizes S14/S15 for mem-mem moves
 401       if ((freg &lt; 16 COMPILER2_PRESENT(-2)) || (single_fpr &amp; 1)) {
 402         if ((single_fpr &amp; 1) == 0) {
 403           single_fpr = freg;
 404           freg += 2;
 405         }
 406         FloatRegister r = as_FloatRegister(single_fpr++);
 407         regs[i].set1(r-&gt;as_VMReg());
 408       } else {
 409         regs[i].set1(VMRegImpl::stack2reg(slot++));
 410       }
 411       break;
 412     case T_DOUBLE:
 413       // C2 utilizes S14/S15 for mem-mem moves
 414       if (freg &lt;= 14 COMPILER2_PRESENT(-2)) {
 415         FloatRegister r1 = as_FloatRegister(freg);
 416         FloatRegister r2 = as_FloatRegister(freg + 1);
 417         regs[i].set_pair(r2-&gt;as_VMReg(), r1-&gt;as_VMReg());
 418         freg += 2;
 419       } else {
 420         // Keep internally the aligned calling convention,
 421         // ignoring ALIGN_WIDE_ARGUMENTS
 422         if (slot &amp; 1) slot++;
 423         regs[i].set_pair(VMRegImpl::stack2reg(slot + 1), VMRegImpl::stack2reg(slot));
 424         slot += 2;
 425         single_fpr = 16;
 426       }
 427       break;
 428     case T_LONG:
 429       // Keep internally the aligned calling convention,
 430       // ignoring ALIGN_WIDE_ARGUMENTS
 431       if (ireg &lt;= 2) {
 432         if (ireg &amp; 1) ireg++;
 433         Register r1 = as_Register(ireg);
 434         Register r2 = as_Register(ireg + 1);
 435         regs[i].set_pair(r2-&gt;as_VMReg(), r1-&gt;as_VMReg());
 436         ireg += 2;
 437       } else {
 438         if (slot &amp; 1) slot++;
 439         regs[i].set_pair(VMRegImpl::stack2reg(slot + 1), VMRegImpl::stack2reg(slot));
 440         slot += 2;
 441         ireg = 4;
 442       }
 443       break;
 444     case T_VOID:
 445       regs[i].set_bad();
 446       break;
 447     default:
 448       ShouldNotReachHere();
 449     }
 450   }
 451 
 452   if (slot &amp; 1) slot++;
 453   return slot;
 454 }
 455 
 456 static void patch_callers_callsite(MacroAssembler *masm) {
 457   Label skip;
 458 
 459   __ ldr(Rtemp, Address(Rmethod, Method::code_offset()));
 460   __ cbz(Rtemp, skip);
 461 
 462   // Pushing an even number of registers for stack alignment.
 463   // Selecting R9, which had to be saved anyway for some platforms.
 464   __ push(RegisterSet(R0, R3) | R9 | LR);
 465 
 466   __ mov(R0, Rmethod);
 467   __ mov(R1, LR);
 468   __ call(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite));
 469 
 470   __ pop(RegisterSet(R0, R3) | R9 | LR);
 471 
 472   __ bind(skip);
 473 }
 474 
 475 void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,
 476                                     int total_args_passed, int comp_args_on_stack,
 477                                     const BasicType *sig_bt, const VMRegPair *regs) {
 478   // TODO: ARM - May be can use ldm to load arguments
 479   const Register tmp = Rtemp; // avoid erasing R5_mh
 480 
 481   // Next assert may not be needed but safer. Extra analysis required
 482   // if this there is not enough free registers and we need to use R5 here.
 483   assert_different_registers(tmp, R5_mh);
 484 
 485   // 6243940 We might end up in handle_wrong_method if
 486   // the callee is deoptimized as we race thru here. If that
 487   // happens we don&#39;t want to take a safepoint because the
 488   // caller frame will look interpreted and arguments are now
 489   // &quot;compiled&quot; so it is much better to make this transition
 490   // invisible to the stack walking code. Unfortunately if
 491   // we try and find the callee by normal means a safepoint
 492   // is possible. So we stash the desired callee in the thread
 493   // and the vm will find there should this case occur.
 494   Address callee_target_addr(Rthread, JavaThread::callee_target_offset());
 495   __ str(Rmethod, callee_target_addr);
 496 
 497 
 498   assert_different_registers(tmp, R0, R1, R2, R3, Rsender_sp, Rmethod);
 499 
 500   const Register initial_sp = Rmethod; // temporarily scratched
 501 
 502   // Old code was modifying R4 but this looks unsafe (particularly with JSR292)
 503   assert_different_registers(tmp, R0, R1, R2, R3, Rsender_sp, initial_sp);
 504 
 505   __ mov(initial_sp, SP);
 506 
 507   if (comp_args_on_stack) {
 508     __ sub_slow(SP, SP, comp_args_on_stack * VMRegImpl::stack_slot_size);
 509   }
 510   __ bic(SP, SP, StackAlignmentInBytes - 1);
 511 
 512   for (int i = 0; i &lt; total_args_passed; i++) {
 513     if (sig_bt[i] == T_VOID) {
 514       assert(i &gt; 0 &amp;&amp; (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), &quot;missing half&quot;);
 515       continue;
 516     }
 517     assert(!regs[i].second()-&gt;is_valid() || regs[i].first()-&gt;next() == regs[i].second(), &quot;must be ordered&quot;);
 518     int arg_offset = Interpreter::expr_offset_in_bytes(total_args_passed - 1 - i);
 519 
 520     VMReg r_1 = regs[i].first();
 521     VMReg r_2 = regs[i].second();
 522     if (r_1-&gt;is_stack()) {
 523       int stack_offset = r_1-&gt;reg2stack() * VMRegImpl::stack_slot_size;
 524       if (!r_2-&gt;is_valid()) {
 525         __ ldr(tmp, Address(initial_sp, arg_offset));
 526         __ str(tmp, Address(SP, stack_offset));
 527       } else {
 528         __ ldr(tmp, Address(initial_sp, arg_offset - Interpreter::stackElementSize));
 529         __ str(tmp, Address(SP, stack_offset));
 530         __ ldr(tmp, Address(initial_sp, arg_offset));
 531         __ str(tmp, Address(SP, stack_offset + wordSize));
 532       }
 533     } else if (r_1-&gt;is_Register()) {
 534       if (!r_2-&gt;is_valid()) {
 535         __ ldr(r_1-&gt;as_Register(), Address(initial_sp, arg_offset));
 536       } else {
 537         __ ldr(r_1-&gt;as_Register(), Address(initial_sp, arg_offset - Interpreter::stackElementSize));
 538         __ ldr(r_2-&gt;as_Register(), Address(initial_sp, arg_offset));
 539       }
 540     } else if (r_1-&gt;is_FloatRegister()) {
 541 #ifdef __SOFTFP__
 542       ShouldNotReachHere();
 543 #endif // __SOFTFP__
 544       if (!r_2-&gt;is_valid()) {
 545         __ flds(r_1-&gt;as_FloatRegister(), Address(initial_sp, arg_offset));
 546       } else {
 547         __ fldd(r_1-&gt;as_FloatRegister(), Address(initial_sp, arg_offset - Interpreter::stackElementSize));
 548       }
 549     } else {
 550       assert(!r_1-&gt;is_valid() &amp;&amp; !r_2-&gt;is_valid(), &quot;must be&quot;);
 551     }
 552   }
 553 
 554   // restore Rmethod (scratched for initial_sp)
 555   __ ldr(Rmethod, callee_target_addr);
 556   __ ldr(PC, Address(Rmethod, Method::from_compiled_offset()));
 557 
 558 }
 559 
 560 static void gen_c2i_adapter(MacroAssembler *masm,
 561                             int total_args_passed,  int comp_args_on_stack,
 562                             const BasicType *sig_bt, const VMRegPair *regs,
 563                             Label&amp; skip_fixup) {
 564   // TODO: ARM - May be can use stm to deoptimize arguments
 565   const Register tmp = Rtemp;
 566 
 567   patch_callers_callsite(masm);
 568   __ bind(skip_fixup);
 569 
 570   __ mov(Rsender_sp, SP); // not yet saved
 571 
 572 
 573   int extraspace = total_args_passed * Interpreter::stackElementSize;
 574   if (extraspace) {
 575     __ sub_slow(SP, SP, extraspace);
 576   }
 577 
 578   for (int i = 0; i &lt; total_args_passed; i++) {
 579     if (sig_bt[i] == T_VOID) {
 580       assert(i &gt; 0 &amp;&amp; (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), &quot;missing half&quot;);
 581       continue;
 582     }
 583     int stack_offset = (total_args_passed - 1 - i) * Interpreter::stackElementSize;
 584 
 585     VMReg r_1 = regs[i].first();
 586     VMReg r_2 = regs[i].second();
 587     if (r_1-&gt;is_stack()) {
 588       int arg_offset = r_1-&gt;reg2stack() * VMRegImpl::stack_slot_size + extraspace;
 589       if (!r_2-&gt;is_valid()) {
 590         __ ldr(tmp, Address(SP, arg_offset));
 591         __ str(tmp, Address(SP, stack_offset));
 592       } else {
 593         __ ldr(tmp, Address(SP, arg_offset));
 594         __ str(tmp, Address(SP, stack_offset - Interpreter::stackElementSize));
 595         __ ldr(tmp, Address(SP, arg_offset + wordSize));
 596         __ str(tmp, Address(SP, stack_offset));
 597       }
 598     } else if (r_1-&gt;is_Register()) {
 599       if (!r_2-&gt;is_valid()) {
 600         __ str(r_1-&gt;as_Register(), Address(SP, stack_offset));
 601       } else {
 602         __ str(r_1-&gt;as_Register(), Address(SP, stack_offset - Interpreter::stackElementSize));
 603         __ str(r_2-&gt;as_Register(), Address(SP, stack_offset));
 604       }
 605     } else if (r_1-&gt;is_FloatRegister()) {
 606 #ifdef __SOFTFP__
 607       ShouldNotReachHere();
 608 #endif // __SOFTFP__
 609       if (!r_2-&gt;is_valid()) {
 610         __ fsts(r_1-&gt;as_FloatRegister(), Address(SP, stack_offset));
 611       } else {
 612         __ fstd(r_1-&gt;as_FloatRegister(), Address(SP, stack_offset - Interpreter::stackElementSize));
 613       }
 614     } else {
 615       assert(!r_1-&gt;is_valid() &amp;&amp; !r_2-&gt;is_valid(), &quot;must be&quot;);
 616     }
 617   }
 618 
 619   __ ldr(PC, Address(Rmethod, Method::interpreter_entry_offset()));
 620 
 621 }
 622 
 623 AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,
 624                                                             int total_args_passed,
 625                                                             int comp_args_on_stack,
 626                                                             const BasicType *sig_bt,
 627                                                             const VMRegPair *regs,
 628                                                             AdapterFingerPrint* fingerprint) {
 629   address i2c_entry = __ pc();
 630   gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);
 631 
 632   address c2i_unverified_entry = __ pc();
 633   Label skip_fixup;
 634   const Register receiver       = R0;
 635   const Register holder_klass   = Rtemp; // XXX should be OK for C2 but not 100% sure
 636   const Register receiver_klass = R4;
 637 
 638   __ load_klass(receiver_klass, receiver);
 639   __ ldr(holder_klass, Address(Ricklass, CompiledICHolder::holder_klass_offset()));
 640   __ ldr(Rmethod, Address(Ricklass, CompiledICHolder::holder_metadata_offset()));
 641   __ cmp(receiver_klass, holder_klass);
 642 
 643   __ ldr(Rtemp, Address(Rmethod, Method::code_offset()), eq);
 644   __ cmp(Rtemp, 0, eq);
 645   __ b(skip_fixup, eq);
 646   __ jump(SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type, noreg, ne);
 647 
 648   address c2i_entry = __ pc();
 649   gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);
 650 
 651   __ flush();
 652   return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry);
 653 }
 654 
 655 
 656 static int reg2offset_in(VMReg r) {
 657   // Account for saved FP and LR
 658   return r-&gt;reg2stack() * VMRegImpl::stack_slot_size + 2*wordSize;
 659 }
 660 
 661 static int reg2offset_out(VMReg r) {
 662   return (r-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;
 663 }
 664 
 665 
 666 static void verify_oop_args(MacroAssembler* masm,
 667                             const methodHandle&amp; method,
 668                             const BasicType* sig_bt,
 669                             const VMRegPair* regs) {
 670   Register temp_reg = Rmethod;  // not part of any compiled calling seq
 671   if (VerifyOops) {
 672     for (int i = 0; i &lt; method-&gt;size_of_parameters(); i++) {
 673       if (sig_bt[i] == T_OBJECT || sig_bt[i] == T_ARRAY) {
 674         VMReg r = regs[i].first();
 675         assert(r-&gt;is_valid(), &quot;bad oop arg&quot;);
 676         if (r-&gt;is_stack()) {
 677           __ ldr(temp_reg, Address(SP, r-&gt;reg2stack() * VMRegImpl::stack_slot_size));
 678           __ verify_oop(temp_reg);
 679         } else {
 680           __ verify_oop(r-&gt;as_Register());
 681         }
 682       }
 683     }
 684   }
 685 }
 686 
 687 static void gen_special_dispatch(MacroAssembler* masm,
 688                                  const methodHandle&amp; method,
 689                                  const BasicType* sig_bt,
 690                                  const VMRegPair* regs) {
 691   verify_oop_args(masm, method, sig_bt, regs);
 692   vmIntrinsics::ID iid = method-&gt;intrinsic_id();
 693 
 694   // Now write the args into the outgoing interpreter space
 695   bool     has_receiver   = false;
 696   Register receiver_reg   = noreg;
 697   int      member_arg_pos = -1;
 698   Register member_reg     = noreg;
 699   int      ref_kind       = MethodHandles::signature_polymorphic_intrinsic_ref_kind(iid);
 700   if (ref_kind != 0) {
 701     member_arg_pos = method-&gt;size_of_parameters() - 1;  // trailing MemberName argument
 702     member_reg = Rmethod;  // known to be free at this point
 703     has_receiver = MethodHandles::ref_kind_has_receiver(ref_kind);
 704   } else if (iid == vmIntrinsics::_invokeBasic) {
 705     has_receiver = true;
 706   } else {
 707     fatal(&quot;unexpected intrinsic id %d&quot;, iid);
 708   }
 709 
 710   if (member_reg != noreg) {
 711     // Load the member_arg into register, if necessary.
 712     SharedRuntime::check_member_name_argument_is_last_argument(method, sig_bt, regs);
 713     VMReg r = regs[member_arg_pos].first();
 714     if (r-&gt;is_stack()) {
 715       __ ldr(member_reg, Address(SP, r-&gt;reg2stack() * VMRegImpl::stack_slot_size));
 716     } else {
 717       // no data motion is needed
 718       member_reg = r-&gt;as_Register();
 719     }
 720   }
 721 
 722   if (has_receiver) {
 723     // Make sure the receiver is loaded into a register.
 724     assert(method-&gt;size_of_parameters() &gt; 0, &quot;oob&quot;);
 725     assert(sig_bt[0] == T_OBJECT, &quot;receiver argument must be an object&quot;);
 726     VMReg r = regs[0].first();
 727     assert(r-&gt;is_valid(), &quot;bad receiver arg&quot;);
 728     if (r-&gt;is_stack()) {
 729       // Porting note:  This assumes that compiled calling conventions always
 730       // pass the receiver oop in a register.  If this is not true on some
 731       // platform, pick a temp and load the receiver from stack.
 732       assert(false, &quot;receiver always in a register&quot;);
 733       receiver_reg = j_rarg0;  // known to be free at this point
 734       __ ldr(receiver_reg, Address(SP, r-&gt;reg2stack() * VMRegImpl::stack_slot_size));
 735     } else {
 736       // no data motion is needed
 737       receiver_reg = r-&gt;as_Register();
 738     }
 739   }
 740 
 741   // Figure out which address we are really jumping to:
 742   MethodHandles::generate_method_handle_dispatch(masm, iid,
 743                                                  receiver_reg, member_reg, /*for_compiler_entry:*/ true);
 744 }
 745 
 746 // ---------------------------------------------------------------------------
 747 // Generate a native wrapper for a given method.  The method takes arguments
 748 // in the Java compiled code convention, marshals them to the native
 749 // convention (handlizes oops, etc), transitions to native, makes the call,
 750 // returns to java state (possibly blocking), unhandlizes any result and
 751 // returns.
 752 nmethod* SharedRuntime::generate_native_wrapper(MacroAssembler* masm,
 753                                                 const methodHandle&amp; method,
 754                                                 int compile_id,
 755                                                 BasicType* in_sig_bt,
 756                                                 VMRegPair* in_regs,
 757                                                 BasicType ret_type,
 758                                                 address critical_entry) {
 759   if (method-&gt;is_method_handle_intrinsic()) {
 760     vmIntrinsics::ID iid = method-&gt;intrinsic_id();
 761     intptr_t start = (intptr_t)__ pc();
 762     int vep_offset = ((intptr_t)__ pc()) - start;
 763     gen_special_dispatch(masm,
 764                          method,
 765                          in_sig_bt,
 766                          in_regs);
 767     int frame_complete = ((intptr_t)__ pc()) - start;  // not complete, period
 768     __ flush();
 769     int stack_slots = SharedRuntime::out_preserve_stack_slots();  // no out slots at all, actually
 770     return nmethod::new_native_nmethod(method,
 771                                        compile_id,
 772                                        masm-&gt;code(),
 773                                        vep_offset,
 774                                        frame_complete,
 775                                        stack_slots / VMRegImpl::slots_per_word,
 776                                        in_ByteSize(-1),
 777                                        in_ByteSize(-1),
 778                                        (OopMapSet*)NULL);
 779   }
 780   // Arguments for JNI method include JNIEnv and Class if static
 781 
 782   // Usage of Rtemp should be OK since scratched by native call
 783 
 784   bool is_static = method-&gt;is_static();
 785 
 786   const int total_in_args = method-&gt;size_of_parameters();
 787   int total_c_args = total_in_args + 1;
 788   if (is_static) {
 789     total_c_args++;
 790   }
 791 
 792   BasicType* out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_c_args);
 793   VMRegPair* out_regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_c_args);
 794 
 795   int argc = 0;
 796   out_sig_bt[argc++] = T_ADDRESS;
 797   if (is_static) {
 798     out_sig_bt[argc++] = T_OBJECT;
 799   }
 800 
 801   int i;
 802   for (i = 0; i &lt; total_in_args; i++) {
 803     out_sig_bt[argc++] = in_sig_bt[i];
 804   }
 805 
 806   int out_arg_slots = c_calling_convention(out_sig_bt, out_regs, NULL, total_c_args);
 807   int stack_slots = SharedRuntime::out_preserve_stack_slots() + out_arg_slots;
 808   // Since object arguments need to be wrapped, we must preserve space
 809   // for those object arguments which come in registers (GPR_PARAMS maximum)
 810   // plus one more slot for Klass handle (for static methods)
 811   int oop_handle_offset = stack_slots;
 812   stack_slots += (GPR_PARAMS + 1) * VMRegImpl::slots_per_word;
 813 
 814   // Plus a lock if needed
 815   int lock_slot_offset = 0;
 816   if (method-&gt;is_synchronized()) {
 817     lock_slot_offset = stack_slots;
 818     assert(sizeof(BasicLock) == wordSize, &quot;adjust this code&quot;);
 819     stack_slots += VMRegImpl::slots_per_word;
 820   }
 821 
 822   // Space to save return address and FP
 823   stack_slots += 2 * VMRegImpl::slots_per_word;
 824 
 825   // Calculate the final stack size taking account of alignment
 826   stack_slots = align_up(stack_slots, StackAlignmentInBytes / VMRegImpl::stack_slot_size);
 827   int stack_size = stack_slots * VMRegImpl::stack_slot_size;
 828   int lock_slot_fp_offset = stack_size - 2 * wordSize -
 829     lock_slot_offset * VMRegImpl::stack_slot_size;
 830 
 831   // Unverified entry point
 832   address start = __ pc();
 833 
 834   // Inline cache check, same as in C1_MacroAssembler::inline_cache_check()
 835   const Register receiver = R0; // see receiverOpr()
 836   __ load_klass(Rtemp, receiver);
 837   __ cmp(Rtemp, Ricklass);
 838   Label verified;
 839 
 840   __ b(verified, eq); // jump over alignment no-ops too
 841   __ jump(SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type, Rtemp);
 842   __ align(CodeEntryAlignment);
 843 
 844   // Verified entry point
 845   __ bind(verified);
 846   int vep_offset = __ pc() - start;
 847 
 848 
 849   if ((InlineObjectHash &amp;&amp; method-&gt;intrinsic_id() == vmIntrinsics::_hashCode) || (method-&gt;intrinsic_id() == vmIntrinsics::_identityHashCode)) {
 850     // Object.hashCode, System.identityHashCode can pull the hashCode from the header word
 851     // instead of doing a full VM transition once it&#39;s been computed.
 852     Label slow_case;
 853     const Register obj_reg = R0;
 854 
 855     // Unlike for Object.hashCode, System.identityHashCode is static method and
 856     // gets object as argument instead of the receiver.
 857     if (method-&gt;intrinsic_id() == vmIntrinsics::_identityHashCode) {
 858       assert(method-&gt;is_static(), &quot;method should be static&quot;);
 859       // return 0 for null reference input, return val = R0 = obj_reg = 0
 860       __ cmp(obj_reg, 0);
 861       __ bx(LR, eq);
 862     }
 863 
 864     __ ldr(Rtemp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
 865 
 866     assert(markWord::unlocked_value == 1, &quot;adjust this code&quot;);
 867     __ tbz(Rtemp, exact_log2(markWord::unlocked_value), slow_case);
 868 
 869     if (UseBiasedLocking) {
 870       assert(is_power_of_2(markWord::biased_lock_bit_in_place), &quot;adjust this code&quot;);
 871       __ tbnz(Rtemp, exact_log2(markWord::biased_lock_bit_in_place), slow_case);
 872     }
 873 
 874     __ bics(Rtemp, Rtemp, ~markWord::hash_mask_in_place);
 875     __ mov(R0, AsmOperand(Rtemp, lsr, markWord::hash_shift), ne);
 876     __ bx(LR, ne);
 877 
 878     __ bind(slow_case);
 879   }
 880 
 881   // Bang stack pages
 882   __ arm_stack_overflow_check(stack_size, Rtemp);
 883 
 884   // Setup frame linkage
 885   __ raw_push(FP, LR);
 886   __ mov(FP, SP);
 887   __ sub_slow(SP, SP, stack_size - 2*wordSize);
 888 
 889   int frame_complete = __ pc() - start;
 890 
 891   OopMapSet* oop_maps = new OopMapSet();
 892   OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
 893   const int extra_args = is_static ? 2 : 1;
 894   int receiver_offset = -1;
 895   int fp_regs_in_arguments = 0;
 896 
 897   for (i = total_in_args; --i &gt;= 0; ) {
 898     switch (in_sig_bt[i]) {
 899     case T_ARRAY:
 900     case T_OBJECT: {
 901       VMReg src = in_regs[i].first();
 902       VMReg dst = out_regs[i + extra_args].first();
 903       if (src-&gt;is_stack()) {
 904         assert(dst-&gt;is_stack(), &quot;must be&quot;);
 905         assert(i != 0, &quot;Incoming receiver is always in a register&quot;);
 906         __ ldr(Rtemp, Address(FP, reg2offset_in(src)));
 907         __ cmp(Rtemp, 0);
 908         __ add(Rtemp, FP, reg2offset_in(src), ne);
 909         __ str(Rtemp, Address(SP, reg2offset_out(dst)));
 910         int offset_in_older_frame = src-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
 911         map-&gt;set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));
 912       } else {
 913         int offset = oop_handle_offset * VMRegImpl::stack_slot_size;
 914         __ str(src-&gt;as_Register(), Address(SP, offset));
 915         map-&gt;set_oop(VMRegImpl::stack2reg(oop_handle_offset));
 916         if ((i == 0) &amp;&amp; (!is_static)) {
 917           receiver_offset = offset;
 918         }
 919         oop_handle_offset += VMRegImpl::slots_per_word;
 920 
 921         if (dst-&gt;is_stack()) {
 922           __ movs(Rtemp, src-&gt;as_Register());
 923           __ add(Rtemp, SP, offset, ne);
 924           __ str(Rtemp, Address(SP, reg2offset_out(dst)));
 925         } else {
 926           __ movs(dst-&gt;as_Register(), src-&gt;as_Register());
 927           __ add(dst-&gt;as_Register(), SP, offset, ne);
 928         }
 929       }
 930     }
 931 
 932     case T_VOID:
 933       break;
 934 
 935 
 936 #ifdef __SOFTFP__
 937     case T_DOUBLE:
 938 #endif
 939     case T_LONG: {
 940       VMReg src_1 = in_regs[i].first();
 941       VMReg src_2 = in_regs[i].second();
 942       VMReg dst_1 = out_regs[i + extra_args].first();
 943       VMReg dst_2 = out_regs[i + extra_args].second();
 944 #if (ALIGN_WIDE_ARGUMENTS == 0)
 945       // C convention can mix a register and a stack slot for a
 946       // 64-bits native argument.
 947 
 948       // Note: following code should work independently of whether
 949       // the Java calling convention follows C convention or whether
 950       // it aligns 64-bit values.
 951       if (dst_2-&gt;is_Register()) {
 952         if (src_1-&gt;as_Register() != dst_1-&gt;as_Register()) {
 953           assert(src_1-&gt;as_Register() != dst_2-&gt;as_Register() &amp;&amp;
 954                  src_2-&gt;as_Register() != dst_2-&gt;as_Register(), &quot;must be&quot;);
 955           __ mov(dst_2-&gt;as_Register(), src_2-&gt;as_Register());
 956           __ mov(dst_1-&gt;as_Register(), src_1-&gt;as_Register());
 957         } else {
 958           assert(src_2-&gt;as_Register() == dst_2-&gt;as_Register(), &quot;must be&quot;);
 959         }
 960       } else if (src_2-&gt;is_Register()) {
 961         if (dst_1-&gt;is_Register()) {
 962           // dst mixes a register and a stack slot
 963           assert(dst_2-&gt;is_stack() &amp;&amp; src_1-&gt;is_Register() &amp;&amp; src_2-&gt;is_Register(), &quot;must be&quot;);
 964           assert(src_1-&gt;as_Register() != dst_1-&gt;as_Register(), &quot;must be&quot;);
 965           __ str(src_2-&gt;as_Register(), Address(SP, reg2offset_out(dst_2)));
 966           __ mov(dst_1-&gt;as_Register(), src_1-&gt;as_Register());
 967         } else {
 968           // registers to stack slots
 969           assert(dst_2-&gt;is_stack() &amp;&amp; src_1-&gt;is_Register() &amp;&amp; src_2-&gt;is_Register(), &quot;must be&quot;);
 970           __ str(src_1-&gt;as_Register(), Address(SP, reg2offset_out(dst_1)));
 971           __ str(src_2-&gt;as_Register(), Address(SP, reg2offset_out(dst_2)));
 972         }
 973       } else if (src_1-&gt;is_Register()) {
 974         if (dst_1-&gt;is_Register()) {
 975           // src and dst must be R3 + stack slot
 976           assert(dst_1-&gt;as_Register() == src_1-&gt;as_Register(), &quot;must be&quot;);
 977           __ ldr(Rtemp,    Address(FP, reg2offset_in(src_2)));
 978           __ str(Rtemp,    Address(SP, reg2offset_out(dst_2)));
 979         } else {
 980           // &lt;R3,stack&gt; -&gt; &lt;stack,stack&gt;
 981           assert(dst_2-&gt;is_stack() &amp;&amp; src_2-&gt;is_stack(), &quot;must be&quot;);
 982           __ ldr(LR, Address(FP, reg2offset_in(src_2)));
 983           __ str(src_1-&gt;as_Register(), Address(SP, reg2offset_out(dst_1)));
 984           __ str(LR, Address(SP, reg2offset_out(dst_2)));
 985         }
 986       } else {
 987         assert(src_2-&gt;is_stack() &amp;&amp; dst_1-&gt;is_stack() &amp;&amp; dst_2-&gt;is_stack(), &quot;must be&quot;);
 988         __ ldr(Rtemp, Address(FP, reg2offset_in(src_1)));
 989         __ ldr(LR,    Address(FP, reg2offset_in(src_2)));
 990         __ str(Rtemp, Address(SP, reg2offset_out(dst_1)));
 991         __ str(LR,    Address(SP, reg2offset_out(dst_2)));
 992       }
 993 #else // ALIGN_WIDE_ARGUMENTS
 994       if (src_1-&gt;is_stack()) {
 995         assert(src_2-&gt;is_stack() &amp;&amp; dst_1-&gt;is_stack() &amp;&amp; dst_2-&gt;is_stack(), &quot;must be&quot;);
 996         __ ldr(Rtemp, Address(FP, reg2offset_in(src_1)));
 997         __ ldr(LR,    Address(FP, reg2offset_in(src_2)));
 998         __ str(Rtemp, Address(SP, reg2offset_out(dst_1)));
 999         __ str(LR,    Address(SP, reg2offset_out(dst_2)));
1000       } else if (dst_1-&gt;is_stack()) {
1001         assert(dst_2-&gt;is_stack() &amp;&amp; src_1-&gt;is_Register() &amp;&amp; src_2-&gt;is_Register(), &quot;must be&quot;);
1002         __ str(src_1-&gt;as_Register(), Address(SP, reg2offset_out(dst_1)));
1003         __ str(src_2-&gt;as_Register(), Address(SP, reg2offset_out(dst_2)));
1004       } else if (src_1-&gt;as_Register() == dst_1-&gt;as_Register()) {
1005         assert(src_2-&gt;as_Register() == dst_2-&gt;as_Register(), &quot;must be&quot;);
1006       } else {
1007         assert(src_1-&gt;as_Register() != dst_2-&gt;as_Register() &amp;&amp;
1008                src_2-&gt;as_Register() != dst_2-&gt;as_Register(), &quot;must be&quot;);
1009         __ mov(dst_2-&gt;as_Register(), src_2-&gt;as_Register());
1010         __ mov(dst_1-&gt;as_Register(), src_1-&gt;as_Register());
1011       }
1012 #endif // ALIGN_WIDE_ARGUMENTS
1013       break;
1014     }
1015 
1016 #if (!defined __SOFTFP__ &amp;&amp; !defined __ABI_HARD__)
1017     case T_FLOAT: {
1018       VMReg src = in_regs[i].first();
1019       VMReg dst = out_regs[i + extra_args].first();
1020       if (src-&gt;is_stack()) {
1021         assert(dst-&gt;is_stack(), &quot;must be&quot;);
1022         __ ldr(Rtemp, Address(FP, reg2offset_in(src)));
1023         __ str(Rtemp, Address(SP, reg2offset_out(dst)));
1024       } else if (dst-&gt;is_stack()) {
1025         __ fsts(src-&gt;as_FloatRegister(), Address(SP, reg2offset_out(dst)));
1026       } else {
1027         assert(src-&gt;is_FloatRegister() &amp;&amp; dst-&gt;is_Register(), &quot;must be&quot;);
1028         __ fmrs(dst-&gt;as_Register(), src-&gt;as_FloatRegister());
1029       }
1030       break;
1031     }
1032 
1033     case T_DOUBLE: {
1034       VMReg src_1 = in_regs[i].first();
1035       VMReg src_2 = in_regs[i].second();
1036       VMReg dst_1 = out_regs[i + extra_args].first();
1037       VMReg dst_2 = out_regs[i + extra_args].second();
1038       if (src_1-&gt;is_stack()) {
1039         assert(src_2-&gt;is_stack() &amp;&amp; dst_1-&gt;is_stack() &amp;&amp; dst_2-&gt;is_stack(), &quot;must be&quot;);
1040         __ ldr(Rtemp, Address(FP, reg2offset_in(src_1)));
1041         __ ldr(LR,    Address(FP, reg2offset_in(src_2)));
1042         __ str(Rtemp, Address(SP, reg2offset_out(dst_1)));
1043         __ str(LR,    Address(SP, reg2offset_out(dst_2)));
1044       } else if (dst_1-&gt;is_stack()) {
1045         assert(dst_2-&gt;is_stack() &amp;&amp; src_1-&gt;is_FloatRegister(), &quot;must be&quot;);
1046         __ fstd(src_1-&gt;as_FloatRegister(), Address(SP, reg2offset_out(dst_1)));
1047 #if (ALIGN_WIDE_ARGUMENTS == 0)
1048       } else if (dst_2-&gt;is_stack()) {
1049         assert(! src_2-&gt;is_stack(), &quot;must be&quot;); // assuming internal java convention is aligned
1050         // double register must go into R3 + one stack slot
1051         __ fmrrd(dst_1-&gt;as_Register(), Rtemp, src_1-&gt;as_FloatRegister());
1052         __ str(Rtemp, Address(SP, reg2offset_out(dst_2)));
1053 #endif
1054       } else {
1055         assert(src_1-&gt;is_FloatRegister() &amp;&amp; dst_1-&gt;is_Register() &amp;&amp; dst_2-&gt;is_Register(), &quot;must be&quot;);
1056         __ fmrrd(dst_1-&gt;as_Register(), dst_2-&gt;as_Register(), src_1-&gt;as_FloatRegister());
1057       }
1058       break;
1059     }
1060 #endif // __SOFTFP__
1061 
1062 #ifdef __ABI_HARD__
1063     case T_FLOAT: {
1064       VMReg src = in_regs[i].first();
1065       VMReg dst = out_regs[i + extra_args].first();
1066       if (src-&gt;is_stack()) {
1067         if (dst-&gt;is_stack()) {
1068           __ ldr(Rtemp, Address(FP, reg2offset_in(src)));
1069           __ str(Rtemp, Address(SP, reg2offset_out(dst)));
1070         } else {
1071           // C2 Java calling convention does not populate S14 and S15, therefore
1072           // those need to be loaded from stack here
1073           __ flds(dst-&gt;as_FloatRegister(), Address(FP, reg2offset_in(src)));
1074           fp_regs_in_arguments++;
1075         }
1076       } else {
1077         assert(src-&gt;is_FloatRegister(), &quot;must be&quot;);
1078         fp_regs_in_arguments++;
1079       }
1080       break;
1081     }
1082     case T_DOUBLE: {
1083       VMReg src_1 = in_regs[i].first();
1084       VMReg src_2 = in_regs[i].second();
1085       VMReg dst_1 = out_regs[i + extra_args].first();
1086       VMReg dst_2 = out_regs[i + extra_args].second();
1087       if (src_1-&gt;is_stack()) {
1088         if (dst_1-&gt;is_stack()) {
1089           assert(dst_2-&gt;is_stack(), &quot;must be&quot;);
1090           __ ldr(Rtemp, Address(FP, reg2offset_in(src_1)));
1091           __ ldr(LR,    Address(FP, reg2offset_in(src_2)));
1092           __ str(Rtemp, Address(SP, reg2offset_out(dst_1)));
1093           __ str(LR,    Address(SP, reg2offset_out(dst_2)));
1094         } else {
1095           // C2 Java calling convention does not populate S14 and S15, therefore
1096           // those need to be loaded from stack here
1097           __ fldd(dst_1-&gt;as_FloatRegister(), Address(FP, reg2offset_in(src_1)));
1098           fp_regs_in_arguments += 2;
1099         }
1100       } else {
1101         assert(src_1-&gt;is_FloatRegister() &amp;&amp; src_2-&gt;is_FloatRegister(), &quot;must be&quot;);
1102         fp_regs_in_arguments += 2;
1103       }
1104       break;
1105     }
1106 #endif // __ABI_HARD__
1107 
1108     default: {
1109       assert(in_sig_bt[i] != T_ADDRESS, &quot;found T_ADDRESS in java args&quot;);
1110       VMReg src = in_regs[i].first();
1111       VMReg dst = out_regs[i + extra_args].first();
1112       if (src-&gt;is_stack()) {
1113         assert(dst-&gt;is_stack(), &quot;must be&quot;);
1114         __ ldr(Rtemp, Address(FP, reg2offset_in(src)));
1115         __ str(Rtemp, Address(SP, reg2offset_out(dst)));
1116       } else if (dst-&gt;is_stack()) {
1117         __ str(src-&gt;as_Register(), Address(SP, reg2offset_out(dst)));
1118       } else {
1119         assert(src-&gt;is_Register() &amp;&amp; dst-&gt;is_Register(), &quot;must be&quot;);
1120         __ mov(dst-&gt;as_Register(), src-&gt;as_Register());
1121       }
1122     }
1123     }
1124   }
1125 
1126   // Get Klass mirror
1127   int klass_offset = -1;
1128   if (is_static) {
1129     klass_offset = oop_handle_offset * VMRegImpl::stack_slot_size;
1130     __ mov_oop(Rtemp, JNIHandles::make_local(method-&gt;method_holder()-&gt;java_mirror()));
1131     __ add(c_rarg1, SP, klass_offset);
1132     __ str(Rtemp, Address(SP, klass_offset));
1133     map-&gt;set_oop(VMRegImpl::stack2reg(oop_handle_offset));
1134   }
1135 
1136   // the PC offset given to add_gc_map must match the PC saved in set_last_Java_frame
1137   int pc_offset = __ set_last_Java_frame(SP, FP, true, Rtemp);
1138   assert(((__ pc()) - start) == __ offset(), &quot;warning: start differs from code_begin&quot;);
1139   oop_maps-&gt;add_gc_map(pc_offset, map);
1140 
1141   // Order last_Java_pc store with the thread state transition (to _thread_in_native)
1142   __ membar(MacroAssembler::StoreStore, Rtemp);
1143 
1144   // RedefineClasses() tracing support for obsolete method entry
1145   if (log_is_enabled(Trace, redefine, class, obsolete)) {
1146     __ save_caller_save_registers();
1147     __ mov(R0, Rthread);
1148     __ mov_metadata(R1, method());
1149     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::rc_trace_method_entry), R0, R1);
1150     __ restore_caller_save_registers();
1151   }
1152 
1153   const Register sync_handle = R5;
1154   const Register sync_obj    = R6;
1155   const Register disp_hdr    = altFP_7_11;
1156   const Register tmp         = R8;
1157 
1158   Label slow_lock, slow_lock_biased, lock_done, fast_lock;
1159   if (method-&gt;is_synchronized()) {
1160     // The first argument is a handle to sync object (a class or an instance)
1161     __ ldr(sync_obj, Address(R1));
1162     // Remember the handle for the unlocking code
1163     __ mov(sync_handle, R1);
1164 
1165     __ resolve(IS_NOT_NULL, sync_obj);
1166 
1167     if(UseBiasedLocking) {
1168       __ biased_locking_enter(sync_obj, tmp, disp_hdr/*scratched*/, false, Rtemp, lock_done, slow_lock_biased);
1169     }
1170 
1171     const Register mark = tmp;
1172     // On MP platforms the next load could return a &#39;stale&#39; value if the memory location has been modified by another thread.
1173     // That would be acceptable as either CAS or slow case path is taken in that case
1174 
1175     __ ldr(mark, Address(sync_obj, oopDesc::mark_offset_in_bytes()));
1176     __ sub(disp_hdr, FP, lock_slot_fp_offset);
1177     __ tst(mark, markWord::unlocked_value);
1178     __ b(fast_lock, ne);
1179 
1180     // Check for recursive lock
1181     // See comments in InterpreterMacroAssembler::lock_object for
1182     // explanations on the fast recursive locking check.
1183     // Check independently the low bits and the distance to SP
1184     // -1- test low 2 bits
1185     __ movs(Rtemp, AsmOperand(mark, lsl, 30));
1186     // -2- test (hdr - SP) if the low two bits are 0
1187     __ sub(Rtemp, mark, SP, eq);
1188     __ movs(Rtemp, AsmOperand(Rtemp, lsr, exact_log2(os::vm_page_size())), eq);
1189     // If still &#39;eq&#39; then recursive locking OK: set displaced header to 0
1190     __ str(Rtemp, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()), eq);
1191     __ b(lock_done, eq);
1192     __ b(slow_lock);
1193 
1194     __ bind(fast_lock);
1195     __ str(mark, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));
1196 
1197     __ cas_for_lock_acquire(mark, disp_hdr, sync_obj, Rtemp, slow_lock);
1198 
1199     __ bind(lock_done);
1200   }
1201 
1202   // Get JNIEnv*
1203   __ add(c_rarg0, Rthread, in_bytes(JavaThread::jni_environment_offset()));
1204 
1205   // Perform thread state transition
1206   __ mov(Rtemp, _thread_in_native);
1207   __ str(Rtemp, Address(Rthread, JavaThread::thread_state_offset()));
1208 
1209   // Finally, call the native method
1210   __ call(method-&gt;native_function());
1211 
1212   // Set FPSCR/FPCR to a known state
1213   if (AlwaysRestoreFPU) {
1214     __ restore_default_fp_mode();
1215   }
1216 
1217   // Ensure a Boolean result is mapped to 0..1
1218   if (ret_type == T_BOOLEAN) {
1219     __ c2bool(R0);
1220   }
1221 
1222   // Do a safepoint check while thread is in transition state
<a name="3" id="anc3"></a>
1223   Label call_safepoint_runtime, return_to_java;
1224   __ mov(Rtemp, _thread_in_native_trans);
<a name="4" id="anc4"></a>
1225   __ str_32(Rtemp, Address(Rthread, JavaThread::thread_state_offset()));
1226 
1227   // make sure the store is observed before reading the SafepointSynchronize state and further mem refs
1228   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreLoad | MacroAssembler::StoreStore), Rtemp);
1229 
<a name="5" id="anc5"></a><span class="line-modified">1230   __ safepoint_poll(R2, call_safepoint_runtime);</span>
1231   __ ldr_u32(R3, Address(Rthread, JavaThread::suspend_flags_offset()));
<a name="6" id="anc6"></a><span class="line-modified">1232   __ cmp(R3, 0);</span>

1233   __ b(call_safepoint_runtime, ne);
<a name="7" id="anc7"></a><span class="line-added">1234 </span>
1235   __ bind(return_to_java);
1236 
1237   // Perform thread state transition and reguard stack yellow pages if needed
1238   Label reguard, reguard_done;
1239   __ mov(Rtemp, _thread_in_Java);
1240   __ ldr_s32(R2, Address(Rthread, JavaThread::stack_guard_state_offset()));
1241   __ str_32(Rtemp, Address(Rthread, JavaThread::thread_state_offset()));
1242 
1243   __ cmp(R2, JavaThread::stack_guard_yellow_reserved_disabled);
1244   __ b(reguard, eq);
1245   __ bind(reguard_done);
1246 
1247   Label slow_unlock, unlock_done;
1248   if (method-&gt;is_synchronized()) {
1249     __ ldr(sync_obj, Address(sync_handle));
1250 
1251     __ resolve(IS_NOT_NULL, sync_obj);
1252 
1253     if(UseBiasedLocking) {
1254       __ biased_locking_exit(sync_obj, Rtemp, unlock_done);
1255       // disp_hdr may not have been saved on entry with biased locking
1256       __ sub(disp_hdr, FP, lock_slot_fp_offset);
1257     }
1258 
1259     // See C1_MacroAssembler::unlock_object() for more comments
1260     __ ldr(R2, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));
1261     __ cbz(R2, unlock_done);
1262 
1263     __ cas_for_lock_release(disp_hdr, R2, sync_obj, Rtemp, slow_unlock);
1264 
1265     __ bind(unlock_done);
1266   }
1267 
1268   // Set last java frame and handle block to zero
1269   __ ldr(LR, Address(Rthread, JavaThread::active_handles_offset()));
1270   __ reset_last_Java_frame(Rtemp); // sets Rtemp to 0 on 32-bit ARM
1271 
1272   __ str_32(Rtemp, Address(LR, JNIHandleBlock::top_offset_in_bytes()));
1273   if (CheckJNICalls) {
1274     __ str(__ zero_register(Rtemp), Address(Rthread, JavaThread::pending_jni_exception_check_fn_offset()));
1275   }
1276 
1277   // Unbox oop result, e.g. JNIHandles::resolve value in R0.
1278   if (ret_type == T_OBJECT || ret_type == T_ARRAY) {
1279     __ resolve_jobject(R0,      // value
1280                        Rtemp,   // tmp1
1281                        R1_tmp); // tmp2
1282   }
1283 
1284   // Any exception pending?
1285   __ ldr(Rtemp, Address(Rthread, Thread::pending_exception_offset()));
1286   __ mov(SP, FP);
1287 
1288   __ cmp(Rtemp, 0);
1289   // Pop the frame and return if no exception pending
1290   __ pop(RegisterSet(FP) | RegisterSet(PC), eq);
1291   // Pop the frame and forward the exception. Rexception_pc contains return address.
1292   __ ldr(FP, Address(SP, wordSize, post_indexed), ne);
1293   __ ldr(Rexception_pc, Address(SP, wordSize, post_indexed), ne);
1294   __ jump(StubRoutines::forward_exception_entry(), relocInfo::runtime_call_type, Rtemp);
1295 
1296   // Safepoint operation and/or pending suspend request is in progress.
1297   // Save the return values and call the runtime function by hand.
1298   __ bind(call_safepoint_runtime);
1299   push_result_registers(masm, ret_type);
1300   __ mov(R0, Rthread);
1301   __ call(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans));
1302   pop_result_registers(masm, ret_type);
1303   __ b(return_to_java);
1304 
<a name="8" id="anc8"></a>

1305   // Reguard stack pages. Save native results around a call to C runtime.
1306   __ bind(reguard);
1307   push_result_registers(masm, ret_type);
1308   __ call(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));
1309   pop_result_registers(masm, ret_type);
1310   __ b(reguard_done);
1311 
1312   if (method-&gt;is_synchronized()) {
1313     // Locking slow case
1314     if(UseBiasedLocking) {
1315       __ bind(slow_lock_biased);
1316       __ sub(disp_hdr, FP, lock_slot_fp_offset);
1317     }
1318 
1319     __ bind(slow_lock);
1320 
1321     push_param_registers(masm, fp_regs_in_arguments);
1322 
1323     // last_Java_frame is already set, so do call_VM manually; no exception can occur
1324     __ mov(R0, sync_obj);
1325     __ mov(R1, disp_hdr);
1326     __ mov(R2, Rthread);
1327     __ call(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_locking_C));
1328 
1329     pop_param_registers(masm, fp_regs_in_arguments);
1330 
1331     __ b(lock_done);
1332 
1333     // Unlocking slow case
1334     __ bind(slow_unlock);
1335 
1336     push_result_registers(masm, ret_type);
1337 
1338     // Clear pending exception before reentering VM.
1339     // Can store the oop in register since it is a leaf call.
1340     assert_different_registers(Rtmp_save1, sync_obj, disp_hdr);
1341     __ ldr(Rtmp_save1, Address(Rthread, Thread::pending_exception_offset()));
1342     Register zero = __ zero_register(Rtemp);
1343     __ str(zero, Address(Rthread, Thread::pending_exception_offset()));
1344     __ mov(R0, sync_obj);
1345     __ mov(R1, disp_hdr);
1346     __ mov(R2, Rthread);
1347     __ call(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C));
1348     __ str(Rtmp_save1, Address(Rthread, Thread::pending_exception_offset()));
1349 
1350     pop_result_registers(masm, ret_type);
1351 
1352     __ b(unlock_done);
1353   }
1354 
1355   __ flush();
1356   return nmethod::new_native_nmethod(method,
1357                                      compile_id,
1358                                      masm-&gt;code(),
1359                                      vep_offset,
1360                                      frame_complete,
1361                                      stack_slots / VMRegImpl::slots_per_word,
1362                                      in_ByteSize(is_static ? klass_offset : receiver_offset),
1363                                      in_ByteSize(lock_slot_offset * VMRegImpl::stack_slot_size),
1364                                      oop_maps);
1365 }
1366 
1367 // this function returns the adjust size (in number of words) to a c2i adapter
1368 // activation for use during deoptimization
1369 int Deoptimization::last_frame_adjust(int callee_parameters, int callee_locals) {
1370   int extra_locals_size = (callee_locals - callee_parameters) * Interpreter::stackElementWords;
1371   return extra_locals_size;
1372 }
1373 
1374 
1375 uint SharedRuntime::out_preserve_stack_slots() {
1376   return 0;
1377 }
1378 
1379 
1380 //------------------------------generate_deopt_blob----------------------------
1381 void SharedRuntime::generate_deopt_blob() {
1382   ResourceMark rm;
1383   CodeBuffer buffer(&quot;deopt_blob&quot;, 1024, 1024);
1384   int frame_size_in_words;
1385   OopMapSet* oop_maps;
1386   int reexecute_offset;
1387   int exception_in_tls_offset;
1388   int exception_offset;
1389 
1390   MacroAssembler* masm = new MacroAssembler(&amp;buffer);
1391   Label cont;
1392   const Register Rkind   = R9; // caller-saved
1393   const Register Rublock = R6;
1394   const Register Rsender = altFP_7_11;
1395   assert_different_registers(Rkind, Rublock, Rsender, Rexception_obj, Rexception_pc, R0, R1, R2, R3, R8, Rtemp);
1396 
1397   address start = __ pc();
1398 
1399   oop_maps = new OopMapSet();
1400   // LR saved by caller (can be live in c2 method)
1401 
1402   // A deopt is a case where LR may be live in the c2 nmethod. So it&#39;s
1403   // not possible to call the deopt blob from the nmethod and pass the
1404   // address of the deopt handler of the nmethod in LR. What happens
1405   // now is that the caller of the deopt blob pushes the current
1406   // address so the deopt blob doesn&#39;t have to do it. This way LR can
1407   // be preserved, contains the live value from the nmethod and is
1408   // saved at R14/R30_offset here.
1409   OopMap* map = RegisterSaver::save_live_registers(masm, &amp;frame_size_in_words, true);
1410   __ mov(Rkind, Deoptimization::Unpack_deopt);
1411   __ b(cont);
1412 
1413   exception_offset = __ pc() - start;
1414 
1415   // Transfer Rexception_obj &amp; Rexception_pc in TLS and fall thru to the
1416   // exception_in_tls_offset entry point.
1417   __ str(Rexception_obj, Address(Rthread, JavaThread::exception_oop_offset()));
1418   __ str(Rexception_pc, Address(Rthread, JavaThread::exception_pc_offset()));
1419   // Force return value to NULL to avoid confusing the escape analysis
1420   // logic. Everything is dead here anyway.
1421   __ mov(R0, 0);
1422 
1423   exception_in_tls_offset = __ pc() - start;
1424 
1425   // Exception data is in JavaThread structure
1426   // Patch the return address of the current frame
1427   __ ldr(LR, Address(Rthread, JavaThread::exception_pc_offset()));
1428   (void) RegisterSaver::save_live_registers(masm, &amp;frame_size_in_words);
1429   {
1430     const Register Rzero = __ zero_register(Rtemp); // XXX should be OK for C2 but not 100% sure
1431     __ str(Rzero, Address(Rthread, JavaThread::exception_pc_offset()));
1432   }
1433   __ mov(Rkind, Deoptimization::Unpack_exception);
1434   __ b(cont);
1435 
1436   reexecute_offset = __ pc() - start;
1437 
1438   (void) RegisterSaver::save_live_registers(masm, &amp;frame_size_in_words);
1439   __ mov(Rkind, Deoptimization::Unpack_reexecute);
1440 
1441   // Calculate UnrollBlock and save the result in Rublock
1442   __ bind(cont);
1443   __ mov(R0, Rthread);
1444   __ mov(R1, Rkind);
1445 
1446   int pc_offset = __ set_last_Java_frame(SP, FP, false, Rtemp); // note: FP may not need to be saved (not on x86)
1447   assert(((__ pc()) - start) == __ offset(), &quot;warning: start differs from code_begin&quot;);
1448   __ call(CAST_FROM_FN_PTR(address, Deoptimization::fetch_unroll_info));
1449   if (pc_offset == -1) {
1450     pc_offset = __ offset();
1451   }
1452   oop_maps-&gt;add_gc_map(pc_offset, map);
1453   __ reset_last_Java_frame(Rtemp); // Rtemp free since scratched by far call
1454 
1455   __ mov(Rublock, R0);
1456 
1457   // Reload Rkind from the UnrollBlock (might have changed)
1458   __ ldr_s32(Rkind, Address(Rublock, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));
1459   Label noException;
1460   __ cmp_32(Rkind, Deoptimization::Unpack_exception);   // Was exception pending?
1461   __ b(noException, ne);
1462   // handle exception case
1463 #ifdef ASSERT
1464   // assert that exception_pc is zero in tls
1465   { Label L;
1466     __ ldr(Rexception_pc, Address(Rthread, JavaThread::exception_pc_offset()));
1467     __ cbz(Rexception_pc, L);
1468     __ stop(&quot;exception pc should be null&quot;);
1469     __ bind(L);
1470   }
1471 #endif
1472   __ ldr(Rexception_obj, Address(Rthread, JavaThread::exception_oop_offset()));
1473   __ verify_oop(Rexception_obj);
1474   {
1475     const Register Rzero = __ zero_register(Rtemp);
1476     __ str(Rzero, Address(Rthread, JavaThread::exception_oop_offset()));
1477   }
1478 
1479   __ bind(noException);
1480 
1481   // This frame is going away.  Fetch return value, so we can move it to
1482   // a new frame.
1483   __ ldr(R0, Address(SP, RegisterSaver::R0_offset * wordSize));
1484   __ ldr(R1, Address(SP, RegisterSaver::R1_offset * wordSize));
1485 #ifndef __SOFTFP__
1486   __ ldr_double(D0, Address(SP, RegisterSaver::D0_offset * wordSize));
1487 #endif
1488   // pop frame
1489   __ add(SP, SP, RegisterSaver::reg_save_size * wordSize);
1490 
1491   // Set initial stack state before pushing interpreter frames
1492   __ ldr_s32(Rtemp, Address(Rublock, Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset_in_bytes()));
1493   __ ldr(R2, Address(Rublock, Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));
1494   __ ldr(R3, Address(Rublock, Deoptimization::UnrollBlock::frame_sizes_offset_in_bytes()));
1495 
1496   __ add(SP, SP, Rtemp);
1497 
1498 #ifdef ASSERT
1499   // Compilers generate code that bang the stack by as much as the
1500   // interpreter would need. So this stack banging should never
1501   // trigger a fault. Verify that it does not on non product builds.
1502   // See if it is enough stack to push deoptimized frames
1503   if (UseStackBanging) {
1504     // The compiled method that we are deoptimizing was popped from the stack.
1505     // If the stack bang results in a stack overflow, we don&#39;t return to the
1506     // method that is being deoptimized. The stack overflow exception is
1507     // propagated to the caller of the deoptimized method. Need to get the pc
1508     // from the caller in LR and restore FP.
1509     __ ldr(LR, Address(R2, 0));
1510     __ ldr(FP, Address(Rublock, Deoptimization::UnrollBlock::initial_info_offset_in_bytes()));
1511     __ ldr_s32(R8, Address(Rublock, Deoptimization::UnrollBlock::total_frame_sizes_offset_in_bytes()));
1512     __ arm_stack_overflow_check(R8, Rtemp);
1513   }
1514 #endif
1515   __ ldr_s32(R8, Address(Rublock, Deoptimization::UnrollBlock::number_of_frames_offset_in_bytes()));
1516 
1517   // Pick up the initial fp we should save
1518   // XXX Note: was ldr(FP, Address(FP));
1519 
1520   // The compiler no longer uses FP as a frame pointer for the
1521   // compiled code. It can be used by the allocator in C2 or to
1522   // memorize the original SP for JSR292 call sites.
1523 
1524   // Hence, ldr(FP, Address(FP)) is probably not correct. For x86,
1525   // Deoptimization::fetch_unroll_info computes the right FP value and
1526   // stores it in Rublock.initial_info. This has been activated for ARM.
1527   __ ldr(FP, Address(Rublock, Deoptimization::UnrollBlock::initial_info_offset_in_bytes()));
1528 
1529   __ ldr_s32(Rtemp, Address(Rublock, Deoptimization::UnrollBlock::caller_adjustment_offset_in_bytes()));
1530   __ mov(Rsender, SP);
1531   __ sub(SP, SP, Rtemp);
1532 
1533   // Push interpreter frames in a loop
1534   Label loop;
1535   __ bind(loop);
1536   __ ldr(LR, Address(R2, wordSize, post_indexed));         // load frame pc
1537   __ ldr(Rtemp, Address(R3, wordSize, post_indexed));      // load frame size
1538 
1539   __ raw_push(FP, LR);                                     // create new frame
1540   __ mov(FP, SP);
1541   __ sub(Rtemp, Rtemp, 2*wordSize);
1542 
1543   __ sub(SP, SP, Rtemp);
1544 
1545   __ str(Rsender, Address(FP, frame::interpreter_frame_sender_sp_offset * wordSize));
1546   __ mov(LR, 0);
1547   __ str(LR, Address(FP, frame::interpreter_frame_last_sp_offset * wordSize));
1548 
1549   __ subs(R8, R8, 1);                               // decrement counter
1550   __ mov(Rsender, SP);
1551   __ b(loop, ne);
1552 
1553   // Re-push self-frame
1554   __ ldr(LR, Address(R2));
1555   __ raw_push(FP, LR);
1556   __ mov(FP, SP);
1557   __ sub(SP, SP, (frame_size_in_words - 2) * wordSize);
1558 
1559   // Restore frame locals after moving the frame
1560   __ str(R0, Address(SP, RegisterSaver::R0_offset * wordSize));
1561   __ str(R1, Address(SP, RegisterSaver::R1_offset * wordSize));
1562 
1563 #ifndef __SOFTFP__
1564   __ str_double(D0, Address(SP, RegisterSaver::D0_offset * wordSize));
1565 #endif // !__SOFTFP__
1566 
1567 #ifdef ASSERT
1568   // Reload Rkind from the UnrollBlock and check that it was not overwritten (Rkind is not callee-saved)
1569   { Label L;
1570     __ ldr_s32(Rtemp, Address(Rublock, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));
1571     __ cmp_32(Rkind, Rtemp);
1572     __ b(L, eq);
1573     __ stop(&quot;Rkind was overwritten&quot;);
1574     __ bind(L);
1575   }
1576 #endif
1577 
1578   // Call unpack_frames with proper arguments
1579   __ mov(R0, Rthread);
1580   __ mov(R1, Rkind);
1581 
1582   pc_offset = __ set_last_Java_frame(SP, FP, true, Rtemp);
1583   assert(((__ pc()) - start) == __ offset(), &quot;warning: start differs from code_begin&quot;);
1584   __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames));
1585   if (pc_offset == -1) {
1586     pc_offset = __ offset();
1587   }
1588   oop_maps-&gt;add_gc_map(pc_offset, new OopMap(frame_size_in_words * VMRegImpl::slots_per_word, 0));
1589   __ reset_last_Java_frame(Rtemp); // Rtemp free since scratched by far call
1590 
1591   // Collect return values, pop self-frame and jump to interpreter
1592   __ ldr(R0, Address(SP, RegisterSaver::R0_offset * wordSize));
1593   __ ldr(R1, Address(SP, RegisterSaver::R1_offset * wordSize));
1594   // Interpreter floats controlled by __SOFTFP__, but compiler
1595   // float return value registers controlled by __ABI_HARD__
1596   // This matters for vfp-sflt builds.
1597 #ifndef __SOFTFP__
1598   // Interpreter hard float
1599 #ifdef __ABI_HARD__
1600   // Compiler float return value in FP registers
1601   __ ldr_double(D0, Address(SP, RegisterSaver::D0_offset * wordSize));
1602 #else
1603   // Compiler float return value in integer registers,
1604   // copy to D0 for interpreter (S0 &lt;-- R0)
1605   __ fmdrr(D0_tos, R0, R1);
1606 #endif
1607 #endif // !__SOFTFP__
1608   __ mov(SP, FP);
1609 
1610   __ pop(RegisterSet(FP) | RegisterSet(PC));
1611 
1612   __ flush();
1613 
1614   _deopt_blob = DeoptimizationBlob::create(&amp;buffer, oop_maps, 0, exception_offset,
1615                                            reexecute_offset, frame_size_in_words);
1616   _deopt_blob-&gt;set_unpack_with_exception_in_tls_offset(exception_in_tls_offset);
1617 }
1618 
1619 #ifdef COMPILER2
1620 
1621 //------------------------------generate_uncommon_trap_blob--------------------
1622 // Ought to generate an ideal graph &amp; compile, but here&#39;s some SPARC ASM
1623 // instead.
1624 void SharedRuntime::generate_uncommon_trap_blob() {
1625   // allocate space for the code
1626   ResourceMark rm;
1627 
1628   // setup code generation tools
1629   int pad = VerifyThread ? 512 : 0;
1630 #ifdef _LP64
1631   CodeBuffer buffer(&quot;uncommon_trap_blob&quot;, 2700+pad, 512);
1632 #else
1633   // Measured 8/7/03 at 660 in 32bit debug build (no VerifyThread)
1634   // Measured 8/7/03 at 1028 in 32bit debug build (VerifyThread)
1635   CodeBuffer buffer(&quot;uncommon_trap_blob&quot;, 2000+pad, 512);
1636 #endif
1637   // bypassed when code generation useless
1638   MacroAssembler* masm               = new MacroAssembler(&amp;buffer);
1639   const Register Rublock = R6;
1640   const Register Rsender = altFP_7_11;
1641   assert_different_registers(Rublock, Rsender, Rexception_obj, R0, R1, R2, R3, R8, Rtemp);
1642 
1643   //
1644   // This is the entry point for all traps the compiler takes when it thinks
1645   // it cannot handle further execution of compilation code. The frame is
1646   // deoptimized in these cases and converted into interpreter frames for
1647   // execution
1648   // The steps taken by this frame are as follows:
1649   //   - push a fake &quot;unpack_frame&quot;
1650   //   - call the C routine Deoptimization::uncommon_trap (this function
1651   //     packs the current compiled frame into vframe arrays and returns
1652   //     information about the number and size of interpreter frames which
1653   //     are equivalent to the frame which is being deoptimized)
1654   //   - deallocate the &quot;unpack_frame&quot;
1655   //   - deallocate the deoptimization frame
1656   //   - in a loop using the information returned in the previous step
1657   //     push interpreter frames;
1658   //   - create a dummy &quot;unpack_frame&quot;
1659   //   - call the C routine: Deoptimization::unpack_frames (this function
1660   //     lays out values on the interpreter frame which was just created)
1661   //   - deallocate the dummy unpack_frame
1662   //   - return to the interpreter entry point
1663   //
1664   //  Refer to the following methods for more information:
1665   //   - Deoptimization::uncommon_trap
1666   //   - Deoptimization::unpack_frame
1667 
1668   // the unloaded class index is in R0 (first parameter to this blob)
1669 
1670   __ raw_push(FP, LR);
1671   __ set_last_Java_frame(SP, FP, false, Rtemp);
1672   __ mov(R2, Deoptimization::Unpack_uncommon_trap);
1673   __ mov(R1, R0);
1674   __ mov(R0, Rthread);
1675   __ call(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap));
1676   __ mov(Rublock, R0);
1677   __ reset_last_Java_frame(Rtemp);
1678   __ raw_pop(FP, LR);
1679 
1680 #ifdef ASSERT
1681   { Label L;
1682     __ ldr_s32(Rtemp, Address(Rublock, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));
1683     __ cmp_32(Rtemp, Deoptimization::Unpack_uncommon_trap);
1684     __ b(L, eq);
1685     __ stop(&quot;SharedRuntime::generate_uncommon_trap_blob: expected Unpack_uncommon_trap&quot;);
1686     __ bind(L);
1687   }
1688 #endif
1689 
1690 
1691   // Set initial stack state before pushing interpreter frames
1692   __ ldr_s32(Rtemp, Address(Rublock, Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset_in_bytes()));
1693   __ ldr(R2, Address(Rublock, Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));
1694   __ ldr(R3, Address(Rublock, Deoptimization::UnrollBlock::frame_sizes_offset_in_bytes()));
1695 
1696   __ add(SP, SP, Rtemp);
1697 
1698   // See if it is enough stack to push deoptimized frames
1699 #ifdef ASSERT
1700   // Compilers generate code that bang the stack by as much as the
1701   // interpreter would need. So this stack banging should never
1702   // trigger a fault. Verify that it does not on non product builds.
1703   if (UseStackBanging) {
1704     // The compiled method that we are deoptimizing was popped from the stack.
1705     // If the stack bang results in a stack overflow, we don&#39;t return to the
1706     // method that is being deoptimized. The stack overflow exception is
1707     // propagated to the caller of the deoptimized method. Need to get the pc
1708     // from the caller in LR and restore FP.
1709     __ ldr(LR, Address(R2, 0));
1710     __ ldr(FP, Address(Rublock, Deoptimization::UnrollBlock::initial_info_offset_in_bytes()));
1711     __ ldr_s32(R8, Address(Rublock, Deoptimization::UnrollBlock::total_frame_sizes_offset_in_bytes()));
1712     __ arm_stack_overflow_check(R8, Rtemp);
1713   }
1714 #endif
1715   __ ldr_s32(R8, Address(Rublock, Deoptimization::UnrollBlock::number_of_frames_offset_in_bytes()));
1716   __ ldr_s32(Rtemp, Address(Rublock, Deoptimization::UnrollBlock::caller_adjustment_offset_in_bytes()));
1717   __ mov(Rsender, SP);
1718   __ sub(SP, SP, Rtemp);
1719   //  __ ldr(FP, Address(FP));
1720   __ ldr(FP, Address(Rublock, Deoptimization::UnrollBlock::initial_info_offset_in_bytes()));
1721 
1722   // Push interpreter frames in a loop
1723   Label loop;
1724   __ bind(loop);
1725   __ ldr(LR, Address(R2, wordSize, post_indexed));         // load frame pc
1726   __ ldr(Rtemp, Address(R3, wordSize, post_indexed));      // load frame size
1727 
1728   __ raw_push(FP, LR);                                     // create new frame
1729   __ mov(FP, SP);
1730   __ sub(Rtemp, Rtemp, 2*wordSize);
1731 
1732   __ sub(SP, SP, Rtemp);
1733 
1734   __ str(Rsender, Address(FP, frame::interpreter_frame_sender_sp_offset * wordSize));
1735   __ mov(LR, 0);
1736   __ str(LR, Address(FP, frame::interpreter_frame_last_sp_offset * wordSize));
1737   __ subs(R8, R8, 1);                               // decrement counter
1738   __ mov(Rsender, SP);
1739   __ b(loop, ne);
1740 
1741   // Re-push self-frame
1742   __ ldr(LR, Address(R2));
1743   __ raw_push(FP, LR);
1744   __ mov(FP, SP);
1745 
1746   // Call unpack_frames with proper arguments
1747   __ mov(R0, Rthread);
1748   __ mov(R1, Deoptimization::Unpack_uncommon_trap);
1749   __ set_last_Java_frame(SP, FP, true, Rtemp);
1750   __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames));
1751   //  oop_maps-&gt;add_gc_map(__ pc() - start, new OopMap(frame_size_in_words, 0));
1752   __ reset_last_Java_frame(Rtemp);
1753 
1754   __ mov(SP, FP);
1755   __ pop(RegisterSet(FP) | RegisterSet(PC));
1756 
1757   masm-&gt;flush();
1758   _uncommon_trap_blob = UncommonTrapBlob::create(&amp;buffer, NULL, 2 /* LR+FP */);
1759 }
1760 
1761 #endif // COMPILER2
1762 
1763 //------------------------------generate_handler_blob------
1764 //
1765 // Generate a special Compile2Runtime blob that saves all registers,
1766 // setup oopmap, and calls safepoint code to stop the compiled code for
1767 // a safepoint.
1768 //
1769 SafepointBlob* SharedRuntime::generate_handler_blob(address call_ptr, int poll_type) {
1770   assert(StubRoutines::forward_exception_entry() != NULL, &quot;must be generated before&quot;);
1771 
1772   ResourceMark rm;
1773   CodeBuffer buffer(&quot;handler_blob&quot;, 256, 256);
1774   int frame_size_words;
1775   OopMapSet* oop_maps;
1776 
1777   bool cause_return = (poll_type == POLL_AT_RETURN);
1778 
1779   MacroAssembler* masm = new MacroAssembler(&amp;buffer);
1780   address start = __ pc();
1781   oop_maps = new OopMapSet();
1782 
1783   if (!cause_return) {
1784     __ sub(SP, SP, 4); // make room for LR which may still be live
1785                        // here if we are coming from a c2 method
1786   }
1787 
1788   OopMap* map = RegisterSaver::save_live_registers(masm, &amp;frame_size_words, !cause_return);
1789   if (!cause_return) {
1790     // update saved PC with correct value
1791     // need 2 steps because LR can be live in c2 method
1792     __ ldr(LR, Address(Rthread, JavaThread::saved_exception_pc_offset()));
1793     __ str(LR, Address(SP, RegisterSaver::LR_offset * wordSize));
1794   }
1795 
1796   __ mov(R0, Rthread);
1797   int pc_offset = __ set_last_Java_frame(SP, FP, false, Rtemp); // note: FP may not need to be saved (not on x86)
1798   assert(((__ pc()) - start) == __ offset(), &quot;warning: start differs from code_begin&quot;);
1799   __ call(call_ptr);
1800   if (pc_offset == -1) {
1801     pc_offset = __ offset();
1802   }
1803   oop_maps-&gt;add_gc_map(pc_offset, map);
1804   __ reset_last_Java_frame(Rtemp); // Rtemp free since scratched by far call
1805 
<a name="9" id="anc9"></a>



1806   if (!cause_return) {
<a name="10" id="anc10"></a><span class="line-added">1807     if (SafepointMechanism::uses_thread_local_poll()) {</span>
<span class="line-added">1808       // If our stashed return pc was modified by the runtime we avoid touching it</span>
<span class="line-added">1809       __ ldr(R3_tmp, Address(Rthread, JavaThread::saved_exception_pc_offset()));</span>
<span class="line-added">1810       __ ldr(R2_tmp, Address(SP, RegisterSaver::LR_offset * wordSize));</span>
<span class="line-added">1811       __ cmp(R2_tmp, R3_tmp);</span>
<span class="line-added">1812       // Adjust return pc forward to step over the safepoint poll instruction</span>
<span class="line-added">1813       __ add(R2_tmp, R2_tmp, 4, eq);</span>
<span class="line-added">1814       __ str(R2_tmp, Address(SP, RegisterSaver::LR_offset * wordSize), eq);</span>
<span class="line-added">1815     }</span>
<span class="line-added">1816 </span>
<span class="line-added">1817     // Check for pending exception</span>
<span class="line-added">1818     __ ldr(Rtemp, Address(Rthread, Thread::pending_exception_offset()));</span>
<span class="line-added">1819     __ cmp(Rtemp, 0);</span>
<span class="line-added">1820 </span>
1821     RegisterSaver::restore_live_registers(masm, false);
1822     __ pop(PC, eq);
1823     __ pop(Rexception_pc);
1824   } else {
<a name="11" id="anc11"></a><span class="line-added">1825     // Check for pending exception</span>
<span class="line-added">1826     __ ldr(Rtemp, Address(Rthread, Thread::pending_exception_offset()));</span>
<span class="line-added">1827     __ cmp(Rtemp, 0);</span>
<span class="line-added">1828 </span>
1829     RegisterSaver::restore_live_registers(masm);
1830     __ bx(LR, eq);
1831     __ mov(Rexception_pc, LR);
1832   }
1833 
1834   __ jump(StubRoutines::forward_exception_entry(), relocInfo::runtime_call_type, Rtemp);
1835 
1836   __ flush();
1837 
1838   return SafepointBlob::create(&amp;buffer, oop_maps, frame_size_words);
1839 }
1840 
1841 RuntimeStub* SharedRuntime::generate_resolve_blob(address destination, const char* name) {
1842   assert(StubRoutines::forward_exception_entry() != NULL, &quot;must be generated before&quot;);
1843 
1844   ResourceMark rm;
1845   CodeBuffer buffer(name, 1000, 512);
1846   int frame_size_words;
1847   OopMapSet *oop_maps;
1848   int frame_complete;
1849 
1850   MacroAssembler* masm = new MacroAssembler(&amp;buffer);
1851   Label pending_exception;
1852 
1853   int start = __ offset();
1854 
1855   oop_maps = new OopMapSet();
1856   OopMap* map = RegisterSaver::save_live_registers(masm, &amp;frame_size_words);
1857 
1858   frame_complete = __ offset();
1859 
1860   __ mov(R0, Rthread);
1861 
1862   int pc_offset = __ set_last_Java_frame(SP, FP, false, Rtemp);
1863   assert(start == 0, &quot;warning: start differs from code_begin&quot;);
1864   __ call(destination);
1865   if (pc_offset == -1) {
1866     pc_offset = __ offset();
1867   }
1868   oop_maps-&gt;add_gc_map(pc_offset, map);
1869   __ reset_last_Java_frame(Rtemp); // Rtemp free since scratched by far call
1870 
1871   __ ldr(R1, Address(Rthread, Thread::pending_exception_offset()));
1872   __ cbnz(R1, pending_exception);
1873 
1874   // Overwrite saved register values
1875 
1876   // Place metadata result of VM call into Rmethod
1877   __ get_vm_result_2(R1, Rtemp);
1878   __ str(R1, Address(SP, RegisterSaver::Rmethod_offset * wordSize));
1879 
1880   // Place target address (VM call result) into Rtemp
1881   __ str(R0, Address(SP, RegisterSaver::Rtemp_offset * wordSize));
1882 
1883   RegisterSaver::restore_live_registers(masm);
1884   __ jump(Rtemp);
1885 
1886   __ bind(pending_exception);
1887 
1888   RegisterSaver::restore_live_registers(masm);
1889   const Register Rzero = __ zero_register(Rtemp);
1890   __ str(Rzero, Address(Rthread, JavaThread::vm_result_2_offset()));
1891   __ mov(Rexception_pc, LR);
1892   __ jump(StubRoutines::forward_exception_entry(), relocInfo::runtime_call_type, Rtemp);
1893 
1894   __ flush();
1895 
1896   return RuntimeStub::new_runtime_stub(name, &amp;buffer, frame_complete, frame_size_words, oop_maps, true);
1897 }
<a name="12" id="anc12"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="12" type="hidden" />
</body>
</html>