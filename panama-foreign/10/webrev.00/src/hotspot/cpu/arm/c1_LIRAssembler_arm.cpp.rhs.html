<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/arm/c1_LIRAssembler_arm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
<a name="1" id="anc1"></a><span class="line-modified">   2  * Copyright (c) 2008, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.inline.hpp&quot;
  27 #include &quot;c1/c1_Compilation.hpp&quot;
  28 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  29 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  30 #include &quot;c1/c1_Runtime1.hpp&quot;
  31 #include &quot;c1/c1_ValueStack.hpp&quot;
  32 #include &quot;ci/ciArrayKlass.hpp&quot;
  33 #include &quot;ci/ciInstance.hpp&quot;
  34 #include &quot;gc/shared/collectedHeap.hpp&quot;
  35 #include &quot;memory/universe.hpp&quot;
  36 #include &quot;nativeInst_arm.hpp&quot;
  37 #include &quot;oops/objArrayKlass.hpp&quot;
  38 #include &quot;runtime/frame.inline.hpp&quot;
  39 #include &quot;runtime/sharedRuntime.hpp&quot;
  40 #include &quot;vmreg_arm.inline.hpp&quot;
  41 
  42 #define __ _masm-&gt;
  43 
  44 // Note: Rtemp usage is this file should not impact C2 and should be
  45 // correct as long as it is not implicitly used in lower layers (the
  46 // arm [macro]assembler) and used with care in the other C1 specific
  47 // files.
  48 
  49 bool LIR_Assembler::is_small_constant(LIR_Opr opr) {
  50   ShouldNotCallThis(); // Not used on ARM
  51   return false;
  52 }
  53 
  54 
  55 LIR_Opr LIR_Assembler::receiverOpr() {
  56   // The first register in Java calling conventions
  57   return FrameMap::R0_oop_opr;
  58 }
  59 
  60 LIR_Opr LIR_Assembler::osrBufferPointer() {
  61   return FrameMap::as_pointer_opr(R0);
  62 }
  63 
  64 #ifndef PRODUCT
  65 void LIR_Assembler::verify_reserved_argument_area_size(int args_count) {
  66   assert(args_count * wordSize &lt;= frame_map()-&gt;reserved_argument_area_size(), &quot;not enough space for arguments&quot;);
  67 }
  68 #endif // !PRODUCT
  69 
  70 void LIR_Assembler::store_parameter(jint c, int offset_from_sp_in_words) {
  71   assert(offset_from_sp_in_words &gt;= 0, &quot;invalid offset from sp&quot;);
  72   int offset_from_sp_in_bytes = offset_from_sp_in_words * BytesPerWord;
  73   assert(offset_from_sp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;not enough space&quot;);
  74   __ mov_slow(Rtemp, c);
  75   __ str(Rtemp, Address(SP, offset_from_sp_in_bytes));
  76 }
  77 
  78 void LIR_Assembler::store_parameter(Metadata* m, int offset_from_sp_in_words) {
  79   assert(offset_from_sp_in_words &gt;= 0, &quot;invalid offset from sp&quot;);
  80   int offset_from_sp_in_bytes = offset_from_sp_in_words * BytesPerWord;
  81   assert(offset_from_sp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;not enough space&quot;);
  82   __ mov_metadata(Rtemp, m);
  83   __ str(Rtemp, Address(SP, offset_from_sp_in_bytes));
  84 }
  85 
  86 //--------------fpu register translations-----------------------
  87 
  88 
  89 void LIR_Assembler::breakpoint() {
  90   __ breakpoint();
  91 }
  92 
  93 void LIR_Assembler::push(LIR_Opr opr) {
  94   Unimplemented();
  95 }
  96 
  97 void LIR_Assembler::pop(LIR_Opr opr) {
  98   Unimplemented();
  99 }
 100 
 101 //-------------------------------------------
 102 Address LIR_Assembler::as_Address(LIR_Address* addr) {
 103   Register base = addr-&gt;base()-&gt;as_pointer_register();
 104 
 105 
 106   if (addr-&gt;index()-&gt;is_illegal() || addr-&gt;index()-&gt;is_constant()) {
 107     int offset = addr-&gt;disp();
 108     if (addr-&gt;index()-&gt;is_constant()) {
 109       offset += addr-&gt;index()-&gt;as_constant_ptr()-&gt;as_jint() &lt;&lt; addr-&gt;scale();
 110     }
 111 
 112     if ((offset &lt;= -4096) || (offset &gt;= 4096)) {
 113       BAILOUT_(&quot;offset not in range&quot;, Address(base));
 114     }
 115 
 116     return Address(base, offset);
 117 
 118   } else {
 119     assert(addr-&gt;disp() == 0, &quot;can&#39;t have both&quot;);
 120     int scale = addr-&gt;scale();
 121 
 122     assert(addr-&gt;index()-&gt;is_single_cpu(), &quot;should be&quot;);
 123     return scale &gt;= 0 ? Address(base, addr-&gt;index()-&gt;as_register(), lsl, scale) :
 124                         Address(base, addr-&gt;index()-&gt;as_register(), lsr, -scale);
 125   }
 126 }
 127 
 128 Address LIR_Assembler::as_Address_hi(LIR_Address* addr) {
 129   Address base = as_Address(addr);
 130   assert(base.index() == noreg, &quot;must be&quot;);
 131   if (base.disp() + BytesPerWord &gt;= 4096) { BAILOUT_(&quot;offset not in range&quot;, Address(base.base(),0)); }
 132   return Address(base.base(), base.disp() + BytesPerWord);
 133 }
 134 
 135 Address LIR_Assembler::as_Address_lo(LIR_Address* addr) {
 136   return as_Address(addr);
 137 }
 138 
 139 
 140 void LIR_Assembler::osr_entry() {
 141   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, code_offset());
 142   BlockBegin* osr_entry = compilation()-&gt;hir()-&gt;osr_entry();
 143   ValueStack* entry_state = osr_entry-&gt;end()-&gt;state();
 144   int number_of_locks = entry_state-&gt;locks_size();
 145 
 146   __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());
 147   Register OSR_buf = osrBufferPointer()-&gt;as_pointer_register();
 148 
 149   assert(frame::interpreter_frame_monitor_size() == BasicObjectLock::size(), &quot;adjust code below&quot;);
 150   int monitor_offset = (method()-&gt;max_locals() + 2 * (number_of_locks - 1)) * BytesPerWord;
 151   for (int i = 0; i &lt; number_of_locks; i++) {
 152     int slot_offset = monitor_offset - (i * 2 * BytesPerWord);
 153     __ ldr(R1, Address(OSR_buf, slot_offset + 0*BytesPerWord));
 154     __ ldr(R2, Address(OSR_buf, slot_offset + 1*BytesPerWord));
 155     __ str(R1, frame_map()-&gt;address_for_monitor_lock(i));
 156     __ str(R2, frame_map()-&gt;address_for_monitor_object(i));
 157   }
 158 }
 159 
 160 
 161 int LIR_Assembler::check_icache() {
 162   Register receiver = LIR_Assembler::receiverOpr()-&gt;as_register();
 163   int offset = __ offset();
 164   __ inline_cache_check(receiver, Ricklass);
 165   return offset;
 166 }
 167 
 168 void LIR_Assembler::clinit_barrier(ciMethod* method) {
 169   ShouldNotReachHere(); // not implemented
 170 }
 171 
 172 void LIR_Assembler::jobject2reg_with_patching(Register reg, CodeEmitInfo* info) {
 173   jobject o = (jobject)Universe::non_oop_word();
 174   int index = __ oop_recorder()-&gt;allocate_oop_index(o);
 175 
 176   PatchingStub* patch = new PatchingStub(_masm, patching_id(info), index);
 177 
 178   __ patchable_mov_oop(reg, o, index);
 179   patching_epilog(patch, lir_patch_normal, reg, info);
 180 }
 181 
 182 
 183 void LIR_Assembler::klass2reg_with_patching(Register reg, CodeEmitInfo* info) {
 184   Metadata* o = (Metadata*)Universe::non_oop_word();
 185   int index = __ oop_recorder()-&gt;allocate_metadata_index(o);
 186   PatchingStub* patch = new PatchingStub(_masm, PatchingStub::load_klass_id, index);
 187 
 188   __ patchable_mov_metadata(reg, o, index);
 189   patching_epilog(patch, lir_patch_normal, reg, info);
 190 }
 191 
 192 
 193 int LIR_Assembler::initial_frame_size_in_bytes() const {
 194   // Subtracts two words to account for return address and link
 195   return frame_map()-&gt;framesize()*VMRegImpl::stack_slot_size - 2*wordSize;
 196 }
 197 
 198 
 199 int LIR_Assembler::emit_exception_handler() {
 200   // TODO: ARM
 201   __ nop(); // See comments in other ports
 202 
 203   address handler_base = __ start_a_stub(exception_handler_size());
 204   if (handler_base == NULL) {
 205     bailout(&quot;exception handler overflow&quot;);
 206     return -1;
 207   }
 208 
 209   int offset = code_offset();
 210 
 211   // check that there is really an exception
 212   __ verify_not_null_oop(Rexception_obj);
 213 
 214   __ call(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id), relocInfo::runtime_call_type);
 215   __ should_not_reach_here();
 216 
 217   assert(code_offset() - offset &lt;= exception_handler_size(), &quot;overflow&quot;);
 218   __ end_a_stub();
 219 
 220   return offset;
 221 }
 222 
 223 // Emit the code to remove the frame from the stack in the exception
 224 // unwind path.
 225 int LIR_Assembler::emit_unwind_handler() {
 226 #ifndef PRODUCT
 227   if (CommentedAssembly) {
 228     _masm-&gt;block_comment(&quot;Unwind handler&quot;);
 229   }
 230 #endif
 231 
 232   int offset = code_offset();
 233 
 234   // Fetch the exception from TLS and clear out exception related thread state
 235   Register zero = __ zero_register(Rtemp);
 236   __ ldr(Rexception_obj, Address(Rthread, JavaThread::exception_oop_offset()));
 237   __ str(zero, Address(Rthread, JavaThread::exception_oop_offset()));
 238   __ str(zero, Address(Rthread, JavaThread::exception_pc_offset()));
 239 
 240   __ bind(_unwind_handler_entry);
 241   __ verify_not_null_oop(Rexception_obj);
 242 
 243   // Preform needed unlocking
 244   MonitorExitStub* stub = NULL;
 245   if (method()-&gt;is_synchronized()) {
 246     monitor_address(0, FrameMap::R0_opr);
 247     stub = new MonitorExitStub(FrameMap::R0_opr, true, 0);
 248     __ unlock_object(R2, R1, R0, Rtemp, *stub-&gt;entry());
 249     __ bind(*stub-&gt;continuation());
 250   }
 251 
 252   // remove the activation and dispatch to the unwind handler
 253   __ remove_frame(initial_frame_size_in_bytes()); // restores FP and LR
 254   __ jump(Runtime1::entry_for(Runtime1::unwind_exception_id), relocInfo::runtime_call_type, Rtemp);
 255 
 256   // Emit the slow path assembly
 257   if (stub != NULL) {
 258     stub-&gt;emit_code(this);
 259   }
 260 
 261   return offset;
 262 }
 263 
 264 
 265 int LIR_Assembler::emit_deopt_handler() {
 266   address handler_base = __ start_a_stub(deopt_handler_size());
 267   if (handler_base == NULL) {
 268     bailout(&quot;deopt handler overflow&quot;);
 269     return -1;
 270   }
 271 
 272   int offset = code_offset();
 273 
 274   __ mov_relative_address(LR, __ pc());
 275   __ push(LR); // stub expects LR to be saved
 276   __ jump(SharedRuntime::deopt_blob()-&gt;unpack(), relocInfo::runtime_call_type, noreg);
 277 
 278   assert(code_offset() - offset &lt;= deopt_handler_size(), &quot;overflow&quot;);
 279   __ end_a_stub();
 280 
 281   return offset;
 282 }
 283 
 284 
 285 void LIR_Assembler::return_op(LIR_Opr result) {
 286   // Pop the frame before safepoint polling
 287   __ remove_frame(initial_frame_size_in_bytes());
<a name="2" id="anc2"></a><span class="line-modified"> 288   __ read_polling_page(Rtemp, relocInfo::poll_return_type);</span>




 289   __ ret();
 290 }
 291 
<a name="3" id="anc3"></a>
 292 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
<a name="4" id="anc4"></a>
 293   if (info != NULL) {
 294     add_debug_info_for_branch(info);
 295   }
 296   int offset = __ offset();
<a name="5" id="anc5"></a><span class="line-modified"> 297   __ read_polling_page(Rtemp, relocInfo::poll_type);</span>

 298   return offset;
 299 }
 300 
 301 
 302 void LIR_Assembler::move_regs(Register from_reg, Register to_reg) {
 303   if (from_reg != to_reg) {
 304     __ mov(to_reg, from_reg);
 305   }
 306 }
 307 
 308 void LIR_Assembler::const2reg(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
 309   assert(src-&gt;is_constant() &amp;&amp; dest-&gt;is_register(), &quot;must be&quot;);
 310   LIR_Const* c = src-&gt;as_constant_ptr();
 311 
 312   switch (c-&gt;type()) {
 313     case T_ADDRESS:
 314     case T_INT:
 315       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 316       __ mov_slow(dest-&gt;as_register(), c-&gt;as_jint());
 317       break;
 318 
 319     case T_LONG:
 320       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 321       __ mov_slow(dest-&gt;as_register_lo(), c-&gt;as_jint_lo());
 322       __ mov_slow(dest-&gt;as_register_hi(), c-&gt;as_jint_hi());
 323       break;
 324 
 325     case T_OBJECT:
 326       if (patch_code == lir_patch_none) {
 327         __ mov_oop(dest-&gt;as_register(), c-&gt;as_jobject());
 328       } else {
 329         jobject2reg_with_patching(dest-&gt;as_register(), info);
 330       }
 331       break;
 332 
 333     case T_METADATA:
 334       if (patch_code == lir_patch_none) {
 335         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 336       } else {
 337         klass2reg_with_patching(dest-&gt;as_register(), info);
 338       }
 339       break;
 340 
 341     case T_FLOAT:
 342       if (dest-&gt;is_single_fpu()) {
 343         __ mov_float(dest-&gt;as_float_reg(), c-&gt;as_jfloat());
 344       } else {
 345         // Simple getters can return float constant directly into r0
 346         __ mov_slow(dest-&gt;as_register(), c-&gt;as_jint_bits());
 347       }
 348       break;
 349 
 350     case T_DOUBLE:
 351       if (dest-&gt;is_double_fpu()) {
 352         __ mov_double(dest-&gt;as_double_reg(), c-&gt;as_jdouble());
 353       } else {
 354         // Simple getters can return double constant directly into r1r0
 355         __ mov_slow(dest-&gt;as_register_lo(), c-&gt;as_jint_lo_bits());
 356         __ mov_slow(dest-&gt;as_register_hi(), c-&gt;as_jint_hi_bits());
 357       }
 358       break;
 359 
 360     default:
 361       ShouldNotReachHere();
 362   }
 363 }
 364 
 365 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 366   assert(src-&gt;is_constant(), &quot;must be&quot;);
 367   assert(dest-&gt;is_stack(), &quot;must be&quot;);
 368   LIR_Const* c = src-&gt;as_constant_ptr();
 369 
 370   switch (c-&gt;type()) {
 371     case T_INT:  // fall through
 372     case T_FLOAT:
 373       __ mov_slow(Rtemp, c-&gt;as_jint_bits());
 374       __ str_32(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 375       break;
 376 
 377     case T_ADDRESS:
 378       __ mov_slow(Rtemp, c-&gt;as_jint());
 379       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 380       break;
 381 
 382     case T_OBJECT:
 383       __ mov_oop(Rtemp, c-&gt;as_jobject());
 384       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 385       break;
 386 
 387     case T_LONG:  // fall through
 388     case T_DOUBLE:
 389       __ mov_slow(Rtemp, c-&gt;as_jint_lo_bits());
 390       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes));
 391       if (c-&gt;as_jint_hi_bits() != c-&gt;as_jint_lo_bits()) {
 392         __ mov_slow(Rtemp, c-&gt;as_jint_hi_bits());
 393       }
 394       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 395       break;
 396 
 397     default:
 398       ShouldNotReachHere();
 399   }
 400 }
 401 
 402 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type,
 403                               CodeEmitInfo* info, bool wide) {
 404   assert((src-&gt;as_constant_ptr()-&gt;type() == T_OBJECT &amp;&amp; src-&gt;as_constant_ptr()-&gt;as_jobject() == NULL),&quot;cannot handle otherwise&quot;);
 405   __ mov(Rtemp, 0);
 406 
 407   int null_check_offset = code_offset();
 408   __ str(Rtemp, as_Address(dest-&gt;as_address_ptr()));
 409 
 410   if (info != NULL) {
 411     assert(false, &quot;arm32 didn&#39;t support this before, investigate if bug&quot;);
 412     add_debug_info_for_null_check(null_check_offset, info);
 413   }
 414 }
 415 
 416 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 417   assert(src-&gt;is_register() &amp;&amp; dest-&gt;is_register(), &quot;must be&quot;);
 418 
 419   if (src-&gt;is_single_cpu()) {
 420     if (dest-&gt;is_single_cpu()) {
 421       move_regs(src-&gt;as_register(), dest-&gt;as_register());
 422     } else if (dest-&gt;is_single_fpu()) {
 423       __ fmsr(dest-&gt;as_float_reg(), src-&gt;as_register());
 424     } else {
 425       ShouldNotReachHere();
 426     }
 427   } else if (src-&gt;is_double_cpu()) {
 428     if (dest-&gt;is_double_cpu()) {
 429       __ long_move(dest-&gt;as_register_lo(), dest-&gt;as_register_hi(), src-&gt;as_register_lo(), src-&gt;as_register_hi());
 430     } else {
 431       __ fmdrr(dest-&gt;as_double_reg(), src-&gt;as_register_lo(), src-&gt;as_register_hi());
 432     }
 433   } else if (src-&gt;is_single_fpu()) {
 434     if (dest-&gt;is_single_fpu()) {
 435       __ mov_float(dest-&gt;as_float_reg(), src-&gt;as_float_reg());
 436     } else if (dest-&gt;is_single_cpu()) {
 437       __ mov_fpr2gpr_float(dest-&gt;as_register(), src-&gt;as_float_reg());
 438     } else {
 439       ShouldNotReachHere();
 440     }
 441   } else if (src-&gt;is_double_fpu()) {
 442     if (dest-&gt;is_double_fpu()) {
 443       __ mov_double(dest-&gt;as_double_reg(), src-&gt;as_double_reg());
 444     } else if (dest-&gt;is_double_cpu()) {
 445       __ fmrrd(dest-&gt;as_register_lo(), dest-&gt;as_register_hi(), src-&gt;as_double_reg());
 446     } else {
 447       ShouldNotReachHere();
 448     }
 449   } else {
 450     ShouldNotReachHere();
 451   }
 452 }
 453 
 454 void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
 455   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 456   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 457 
 458   Address addr = dest-&gt;is_single_word() ?
 459     frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()) :
 460     frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 461 
 462   assert(lo_word_offset_in_bytes == 0 &amp;&amp; hi_word_offset_in_bytes == 4, &quot;little ending&quot;);
 463   if (src-&gt;is_single_fpu() || src-&gt;is_double_fpu()) {
 464     if (addr.disp() &gt;= 1024) { BAILOUT(&quot;Too exotic case to handle here&quot;); }
 465   }
 466 
 467   if (src-&gt;is_single_cpu()) {
 468     switch (type) {
 469       case T_OBJECT:
 470       case T_ARRAY:    __ verify_oop(src-&gt;as_register());   // fall through
 471       case T_ADDRESS:
 472       case T_METADATA: __ str(src-&gt;as_register(), addr);    break;
 473       case T_FLOAT:    // used in intBitsToFloat intrinsic implementation, fall through
 474       case T_INT:      __ str_32(src-&gt;as_register(), addr); break;
 475       default:
 476         ShouldNotReachHere();
 477     }
 478   } else if (src-&gt;is_double_cpu()) {
 479     __ str(src-&gt;as_register_lo(), addr);
 480     __ str(src-&gt;as_register_hi(), frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 481   } else if (src-&gt;is_single_fpu()) {
 482     __ str_float(src-&gt;as_float_reg(), addr);
 483   } else if (src-&gt;is_double_fpu()) {
 484     __ str_double(src-&gt;as_double_reg(), addr);
 485   } else {
 486     ShouldNotReachHere();
 487   }
 488 }
 489 
 490 
 491 void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type,
 492                             LIR_PatchCode patch_code, CodeEmitInfo* info,
 493                             bool pop_fpu_stack, bool wide,
 494                             bool unaligned) {
 495   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 496   Register base_reg = to_addr-&gt;base()-&gt;as_pointer_register();
 497   const bool needs_patching = (patch_code != lir_patch_none);
 498 
 499   PatchingStub* patch = NULL;
 500   if (needs_patching) {
 501     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 502   }
 503 
 504   int null_check_offset = code_offset();
 505 
 506   switch (type) {
 507     case T_ARRAY:
 508     case T_OBJECT:
 509       if (UseCompressedOops &amp;&amp; !wide) {
 510         ShouldNotReachHere();
 511       } else {
 512         __ str(src-&gt;as_register(), as_Address(to_addr));
 513       }
 514       break;
 515 
 516     case T_ADDRESS:
 517       __ str(src-&gt;as_pointer_register(), as_Address(to_addr));
 518       break;
 519 
 520     case T_BYTE:
 521     case T_BOOLEAN:
 522       __ strb(src-&gt;as_register(), as_Address(to_addr));
 523       break;
 524 
 525     case T_CHAR:
 526     case T_SHORT:
 527       __ strh(src-&gt;as_register(), as_Address(to_addr));
 528       break;
 529 
 530     case T_INT:
 531 #ifdef __SOFTFP__
 532     case T_FLOAT:
 533 #endif // __SOFTFP__
 534       __ str_32(src-&gt;as_register(), as_Address(to_addr));
 535       break;
 536 
 537 
 538 #ifdef __SOFTFP__
 539     case T_DOUBLE:
 540 #endif // __SOFTFP__
 541     case T_LONG: {
 542       Register from_lo = src-&gt;as_register_lo();
 543       Register from_hi = src-&gt;as_register_hi();
 544       if (to_addr-&gt;index()-&gt;is_register()) {
 545         assert(to_addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 546         assert(to_addr-&gt;disp() == 0, &quot;Not yet supporting both&quot;);
 547         __ add(Rtemp, base_reg, to_addr-&gt;index()-&gt;as_register());
 548         base_reg = Rtemp;
 549         __ str(from_lo, Address(Rtemp));
 550         if (patch != NULL) {
 551           __ nop(); // see comment before patching_epilog for 2nd str
 552           patching_epilog(patch, lir_patch_low, base_reg, info);
 553           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 554           patch_code = lir_patch_high;
 555         }
 556         __ str(from_hi, Address(Rtemp, BytesPerWord));
 557       } else if (base_reg == from_lo) {
 558         __ str(from_hi, as_Address_hi(to_addr));
 559         if (patch != NULL) {
 560           __ nop(); // see comment before patching_epilog for 2nd str
 561           patching_epilog(patch, lir_patch_high, base_reg, info);
 562           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 563           patch_code = lir_patch_low;
 564         }
 565         __ str(from_lo, as_Address_lo(to_addr));
 566       } else {
 567         __ str(from_lo, as_Address_lo(to_addr));
 568         if (patch != NULL) {
 569           __ nop(); // see comment before patching_epilog for 2nd str
 570           patching_epilog(patch, lir_patch_low, base_reg, info);
 571           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 572           patch_code = lir_patch_high;
 573         }
 574         __ str(from_hi, as_Address_hi(to_addr));
 575       }
 576       break;
 577     }
 578 
 579 #ifndef __SOFTFP__
 580     case T_FLOAT:
 581       if (to_addr-&gt;index()-&gt;is_register()) {
 582         assert(to_addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 583         __ add(Rtemp, base_reg, to_addr-&gt;index()-&gt;as_register());
 584         if ((to_addr-&gt;disp() &lt;= -4096) || (to_addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 585         __ fsts(src-&gt;as_float_reg(), Address(Rtemp, to_addr-&gt;disp()));
 586       } else {
 587         __ fsts(src-&gt;as_float_reg(), as_Address(to_addr));
 588       }
 589       break;
 590 
 591     case T_DOUBLE:
 592       if (to_addr-&gt;index()-&gt;is_register()) {
 593         assert(to_addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 594         __ add(Rtemp, base_reg, to_addr-&gt;index()-&gt;as_register());
 595         if ((to_addr-&gt;disp() &lt;= -4096) || (to_addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 596         __ fstd(src-&gt;as_double_reg(), Address(Rtemp, to_addr-&gt;disp()));
 597       } else {
 598         __ fstd(src-&gt;as_double_reg(), as_Address(to_addr));
 599       }
 600       break;
 601 #endif // __SOFTFP__
 602 
 603 
 604     default:
 605       ShouldNotReachHere();
 606   }
 607 
 608   if (info != NULL) {
 609     add_debug_info_for_null_check(null_check_offset, info);
 610   }
 611 
 612   if (patch != NULL) {
 613     // Offset embedded into LDR/STR instruction may appear not enough
 614     // to address a field. So, provide a space for one more instruction
 615     // that will deal with larger offsets.
 616     __ nop();
 617     patching_epilog(patch, patch_code, base_reg, info);
 618   }
 619 }
 620 
 621 
 622 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
 623   assert(src-&gt;is_stack(), &quot;should not call otherwise&quot;);
 624   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 625 
 626   Address addr = src-&gt;is_single_word() ?
 627     frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()) :
 628     frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
 629 
 630   assert(lo_word_offset_in_bytes == 0 &amp;&amp; hi_word_offset_in_bytes == 4, &quot;little ending&quot;);
 631   if (dest-&gt;is_single_fpu() || dest-&gt;is_double_fpu()) {
 632     if (addr.disp() &gt;= 1024) { BAILOUT(&quot;Too exotic case to handle here&quot;); }
 633   }
 634 
 635   if (dest-&gt;is_single_cpu()) {
 636     switch (type) {
 637       case T_OBJECT:
 638       case T_ARRAY:
 639       case T_ADDRESS:
 640       case T_METADATA: __ ldr(dest-&gt;as_register(), addr); break;
 641       case T_FLOAT:    // used in floatToRawIntBits intrinsic implemenation
 642       case T_INT:      __ ldr_u32(dest-&gt;as_register(), addr); break;
 643       default:
 644         ShouldNotReachHere();
 645     }
 646     if ((type == T_OBJECT) || (type == T_ARRAY)) {
 647       __ verify_oop(dest-&gt;as_register());
 648     }
 649   } else if (dest-&gt;is_double_cpu()) {
 650     __ ldr(dest-&gt;as_register_lo(), addr);
 651     __ ldr(dest-&gt;as_register_hi(), frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 652   } else if (dest-&gt;is_single_fpu()) {
 653     __ ldr_float(dest-&gt;as_float_reg(), addr);
 654   } else if (dest-&gt;is_double_fpu()) {
 655     __ ldr_double(dest-&gt;as_double_reg(), addr);
 656   } else {
 657     ShouldNotReachHere();
 658   }
 659 }
 660 
 661 
 662 void LIR_Assembler::stack2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {
 663   if (src-&gt;is_single_stack()) {
 664     switch (src-&gt;type()) {
 665       case T_OBJECT:
 666       case T_ARRAY:
 667       case T_ADDRESS:
 668       case T_METADATA:
 669         __ ldr(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 670         __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 671         break;
 672 
 673       case T_INT:
 674       case T_FLOAT:
 675         __ ldr_u32(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 676         __ str_32(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 677         break;
 678 
 679       default:
 680         ShouldNotReachHere();
 681     }
 682   } else {
 683     assert(src-&gt;is_double_stack(), &quot;must be&quot;);
 684     __ ldr(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), lo_word_offset_in_bytes));
 685     __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes));
 686     __ ldr(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 687     __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 688   }
 689 }
 690 
 691 
 692 void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type,
 693                             LIR_PatchCode patch_code, CodeEmitInfo* info,
 694                             bool wide, bool unaligned) {
 695   assert(src-&gt;is_address(), &quot;should not call otherwise&quot;);
 696   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 697   LIR_Address* addr = src-&gt;as_address_ptr();
 698 
 699   Register base_reg = addr-&gt;base()-&gt;as_pointer_register();
 700 
 701   PatchingStub* patch = NULL;
 702   if (patch_code != lir_patch_none) {
 703     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 704   }
 705   if (info != NULL) {
 706     add_debug_info_for_null_check_here(info);
 707   }
 708 
 709   switch (type) {
 710     case T_OBJECT:  // fall through
 711     case T_ARRAY:
 712       if (UseCompressedOops &amp;&amp; !wide) {
 713         __ ldr_u32(dest-&gt;as_register(), as_Address(addr));
 714       } else {
 715         __ ldr(dest-&gt;as_register(), as_Address(addr));
 716       }
 717       break;
 718 
 719     case T_ADDRESS:
 720       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
 721         __ ldr_u32(dest-&gt;as_pointer_register(), as_Address(addr));
 722       } else {
 723         __ ldr(dest-&gt;as_pointer_register(), as_Address(addr));
 724       }
 725       break;
 726 
 727     case T_INT:
 728 #ifdef __SOFTFP__
 729     case T_FLOAT:
 730 #endif // __SOFTFP__
 731       __ ldr(dest-&gt;as_pointer_register(), as_Address(addr));
 732       break;
 733 
 734     case T_BOOLEAN:
 735       __ ldrb(dest-&gt;as_register(), as_Address(addr));
 736       break;
 737 
 738     case T_BYTE:
 739       __ ldrsb(dest-&gt;as_register(), as_Address(addr));
 740       break;
 741 
 742     case T_CHAR:
 743       __ ldrh(dest-&gt;as_register(), as_Address(addr));
 744       break;
 745 
 746     case T_SHORT:
 747       __ ldrsh(dest-&gt;as_register(), as_Address(addr));
 748       break;
 749 
 750 
 751 #ifdef __SOFTFP__
 752     case T_DOUBLE:
 753 #endif // __SOFTFP__
 754     case T_LONG: {
 755       Register to_lo = dest-&gt;as_register_lo();
 756       Register to_hi = dest-&gt;as_register_hi();
 757       if (addr-&gt;index()-&gt;is_register()) {
 758         assert(addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 759         assert(addr-&gt;disp() == 0, &quot;Not yet supporting both&quot;);
 760         __ add(Rtemp, base_reg, addr-&gt;index()-&gt;as_register());
 761         base_reg = Rtemp;
 762         __ ldr(to_lo, Address(Rtemp));
 763         if (patch != NULL) {
 764           __ nop(); // see comment before patching_epilog for 2nd ldr
 765           patching_epilog(patch, lir_patch_low, base_reg, info);
 766           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 767           patch_code = lir_patch_high;
 768         }
 769         __ ldr(to_hi, Address(Rtemp, BytesPerWord));
 770       } else if (base_reg == to_lo) {
 771         __ ldr(to_hi, as_Address_hi(addr));
 772         if (patch != NULL) {
 773           __ nop(); // see comment before patching_epilog for 2nd ldr
 774           patching_epilog(patch, lir_patch_high, base_reg, info);
 775           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 776           patch_code = lir_patch_low;
 777         }
 778         __ ldr(to_lo, as_Address_lo(addr));
 779       } else {
 780         __ ldr(to_lo, as_Address_lo(addr));
 781         if (patch != NULL) {
 782           __ nop(); // see comment before patching_epilog for 2nd ldr
 783           patching_epilog(patch, lir_patch_low, base_reg, info);
 784           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 785           patch_code = lir_patch_high;
 786         }
 787         __ ldr(to_hi, as_Address_hi(addr));
 788       }
 789       break;
 790     }
 791 
 792 #ifndef __SOFTFP__
 793     case T_FLOAT:
 794       if (addr-&gt;index()-&gt;is_register()) {
 795         assert(addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 796         __ add(Rtemp, base_reg, addr-&gt;index()-&gt;as_register());
 797         if ((addr-&gt;disp() &lt;= -4096) || (addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 798         __ flds(dest-&gt;as_float_reg(), Address(Rtemp, addr-&gt;disp()));
 799       } else {
 800         __ flds(dest-&gt;as_float_reg(), as_Address(addr));
 801       }
 802       break;
 803 
 804     case T_DOUBLE:
 805       if (addr-&gt;index()-&gt;is_register()) {
 806         assert(addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 807         __ add(Rtemp, base_reg, addr-&gt;index()-&gt;as_register());
 808         if ((addr-&gt;disp() &lt;= -4096) || (addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 809         __ fldd(dest-&gt;as_double_reg(), Address(Rtemp, addr-&gt;disp()));
 810       } else {
 811         __ fldd(dest-&gt;as_double_reg(), as_Address(addr));
 812       }
 813       break;
 814 #endif // __SOFTFP__
 815 
 816 
 817     default:
 818       ShouldNotReachHere();
 819   }
 820 
 821   if (patch != NULL) {
 822     // Offset embedded into LDR/STR instruction may appear not enough
 823     // to address a field. So, provide a space for one more instruction
 824     // that will deal with larger offsets.
 825     __ nop();
 826     patching_epilog(patch, patch_code, base_reg, info);
 827   }
 828 
 829 }
 830 
 831 
 832 void LIR_Assembler::emit_op3(LIR_Op3* op) {
 833   bool is_32 = op-&gt;result_opr()-&gt;is_single_cpu();
 834 
 835   if (op-&gt;code() == lir_idiv &amp;&amp; op-&gt;in_opr2()-&gt;is_constant() &amp;&amp; is_32) {
 836     int c = op-&gt;in_opr2()-&gt;as_constant_ptr()-&gt;as_jint();
 837     assert(is_power_of_2(c), &quot;non power-of-2 constant should be put in a register&quot;);
 838 
 839     Register left = op-&gt;in_opr1()-&gt;as_register();
 840     Register dest = op-&gt;result_opr()-&gt;as_register();
 841     if (c == 1) {
 842       __ mov(dest, left);
 843     } else if (c == 2) {
 844       __ add_32(dest, left, AsmOperand(left, lsr, 31));
 845       __ asr_32(dest, dest, 1);
 846     } else if (c != (int) 0x80000000) {
 847       int power = log2_intptr(c);
 848       __ asr_32(Rtemp, left, 31);
 849       __ add_32(dest, left, AsmOperand(Rtemp, lsr, 32-power)); // dest = left + (left &lt; 0 ? 2^power - 1 : 0);
 850       __ asr_32(dest, dest, power);                            // dest = dest &gt;&gt;&gt; power;
 851     } else {
 852       // x/0x80000000 is a special case, since dividend is a power of two, but is negative.
 853       // The only possible result values are 0 and 1, with 1 only for dividend == divisor == 0x80000000.
 854       __ cmp_32(left, c);
 855       __ mov(dest, 0, ne);
 856       __ mov(dest, 1, eq);
 857     }
 858   } else {
 859     assert(op-&gt;code() == lir_idiv || op-&gt;code() == lir_irem, &quot;unexpected op3&quot;);
 860     __ call(StubRoutines::Arm::idiv_irem_entry(), relocInfo::runtime_call_type);
 861     add_debug_info_for_div0_here(op-&gt;info());
 862   }
 863 }
 864 
 865 
 866 void LIR_Assembler::emit_opBranch(LIR_OpBranch* op) {
 867 #ifdef ASSERT
 868   assert(op-&gt;block() == NULL || op-&gt;block()-&gt;label() == op-&gt;label(), &quot;wrong label&quot;);
 869   if (op-&gt;block() != NULL)  _branch_target_blocks.append(op-&gt;block());
 870   if (op-&gt;ublock() != NULL) _branch_target_blocks.append(op-&gt;ublock());
 871   assert(op-&gt;info() == NULL, &quot;CodeEmitInfo?&quot;);
 872 #endif // ASSERT
 873 
 874 #ifdef __SOFTFP__
 875   assert (op-&gt;code() != lir_cond_float_branch, &quot;this should be impossible&quot;);
 876 #else
 877   if (op-&gt;code() == lir_cond_float_branch) {
 878     __ fmstat();
 879     __ b(*(op-&gt;ublock()-&gt;label()), vs);
 880   }
 881 #endif // __SOFTFP__
 882 
 883   AsmCondition acond = al;
 884   switch (op-&gt;cond()) {
 885     case lir_cond_equal:        acond = eq; break;
 886     case lir_cond_notEqual:     acond = ne; break;
 887     case lir_cond_less:         acond = lt; break;
 888     case lir_cond_lessEqual:    acond = le; break;
 889     case lir_cond_greaterEqual: acond = ge; break;
 890     case lir_cond_greater:      acond = gt; break;
 891     case lir_cond_aboveEqual:   acond = hs; break;
 892     case lir_cond_belowEqual:   acond = ls; break;
 893     default: assert(op-&gt;cond() == lir_cond_always, &quot;must be&quot;);
 894   }
 895   __ b(*(op-&gt;label()), acond);
 896 }
 897 
 898 
 899 void LIR_Assembler::emit_opConvert(LIR_OpConvert* op) {
 900   LIR_Opr src  = op-&gt;in_opr();
 901   LIR_Opr dest = op-&gt;result_opr();
 902 
 903   switch (op-&gt;bytecode()) {
 904     case Bytecodes::_i2l:
 905       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 906       __ mov(dest-&gt;as_register_hi(), AsmOperand(src-&gt;as_register(), asr, 31));
 907       break;
 908     case Bytecodes::_l2i:
 909       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 910       break;
 911     case Bytecodes::_i2b:
 912       __ sign_extend(dest-&gt;as_register(), src-&gt;as_register(), 8);
 913       break;
 914     case Bytecodes::_i2s:
 915       __ sign_extend(dest-&gt;as_register(), src-&gt;as_register(), 16);
 916       break;
 917     case Bytecodes::_i2c:
 918       __ zero_extend(dest-&gt;as_register(), src-&gt;as_register(), 16);
 919       break;
 920     case Bytecodes::_f2d:
 921       __ convert_f2d(dest-&gt;as_double_reg(), src-&gt;as_float_reg());
 922       break;
 923     case Bytecodes::_d2f:
 924       __ convert_d2f(dest-&gt;as_float_reg(), src-&gt;as_double_reg());
 925       break;
 926     case Bytecodes::_i2f:
 927       __ fmsr(Stemp, src-&gt;as_register());
 928       __ fsitos(dest-&gt;as_float_reg(), Stemp);
 929       break;
 930     case Bytecodes::_i2d:
 931       __ fmsr(Stemp, src-&gt;as_register());
 932       __ fsitod(dest-&gt;as_double_reg(), Stemp);
 933       break;
 934     case Bytecodes::_f2i:
 935       __ ftosizs(Stemp, src-&gt;as_float_reg());
 936       __ fmrs(dest-&gt;as_register(), Stemp);
 937       break;
 938     case Bytecodes::_d2i:
 939       __ ftosizd(Stemp, src-&gt;as_double_reg());
 940       __ fmrs(dest-&gt;as_register(), Stemp);
 941       break;
 942     default:
 943       ShouldNotReachHere();
 944   }
 945 }
 946 
 947 
 948 void LIR_Assembler::emit_alloc_obj(LIR_OpAllocObj* op) {
 949   if (op-&gt;init_check()) {
 950     Register tmp = op-&gt;tmp1()-&gt;as_register();
 951     __ ldrb(tmp, Address(op-&gt;klass()-&gt;as_register(), InstanceKlass::init_state_offset()));
 952     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
 953     __ cmp(tmp, InstanceKlass::fully_initialized);
 954     __ b(*op-&gt;stub()-&gt;entry(), ne);
 955   }
 956   __ allocate_object(op-&gt;obj()-&gt;as_register(),
 957                      op-&gt;tmp1()-&gt;as_register(),
 958                      op-&gt;tmp2()-&gt;as_register(),
 959                      op-&gt;tmp3()-&gt;as_register(),
 960                      op-&gt;header_size(),
 961                      op-&gt;object_size(),
 962                      op-&gt;klass()-&gt;as_register(),
 963                      *op-&gt;stub()-&gt;entry());
 964   __ bind(*op-&gt;stub()-&gt;continuation());
 965 }
 966 
 967 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
 968   if (UseSlowPath ||
 969       (!UseFastNewObjectArray &amp;&amp; (op-&gt;type() == T_OBJECT || op-&gt;type() == T_ARRAY)) ||
 970       (!UseFastNewTypeArray   &amp;&amp; (op-&gt;type() != T_OBJECT &amp;&amp; op-&gt;type() != T_ARRAY))) {
 971     __ b(*op-&gt;stub()-&gt;entry());
 972   } else {
 973     __ allocate_array(op-&gt;obj()-&gt;as_register(),
 974                       op-&gt;len()-&gt;as_register(),
 975                       op-&gt;tmp1()-&gt;as_register(),
 976                       op-&gt;tmp2()-&gt;as_register(),
 977                       op-&gt;tmp3()-&gt;as_register(),
 978                       arrayOopDesc::header_size(op-&gt;type()),
 979                       type2aelembytes(op-&gt;type()),
 980                       op-&gt;klass()-&gt;as_register(),
 981                       *op-&gt;stub()-&gt;entry());
 982   }
 983   __ bind(*op-&gt;stub()-&gt;continuation());
 984 }
 985 
 986 void LIR_Assembler::type_profile_helper(Register mdo, int mdo_offset_bias,
 987                                         ciMethodData *md, ciProfileData *data,
 988                                         Register recv, Register tmp1, Label* update_done) {
 989   assert_different_registers(mdo, recv, tmp1);
 990   uint i;
 991   for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
 992     Label next_test;
 993     // See if the receiver is receiver[n].
 994     Address receiver_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)) -
 995                           mdo_offset_bias);
 996     __ ldr(tmp1, receiver_addr);
 997     __ verify_klass_ptr(tmp1);
 998     __ cmp(recv, tmp1);
 999     __ b(next_test, ne);
1000     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)) -
1001                       mdo_offset_bias);
1002     __ ldr(tmp1, data_addr);
1003     __ add(tmp1, tmp1, DataLayout::counter_increment);
1004     __ str(tmp1, data_addr);
1005     __ b(*update_done);
1006     __ bind(next_test);
1007   }
1008 
1009   // Didn&#39;t find receiver; find next empty slot and fill it in
1010   for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
1011     Label next_test;
1012     Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)) -
1013                       mdo_offset_bias);
1014     __ ldr(tmp1, recv_addr);
1015     __ cbnz(tmp1, next_test);
1016     __ str(recv, recv_addr);
1017     __ mov(tmp1, DataLayout::counter_increment);
1018     __ str(tmp1, Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)) -
1019                          mdo_offset_bias));
1020     __ b(*update_done);
1021     __ bind(next_test);
1022   }
1023 }
1024 
1025 void LIR_Assembler::setup_md_access(ciMethod* method, int bci,
1026                                     ciMethodData*&amp; md, ciProfileData*&amp; data, int&amp; mdo_offset_bias) {
1027   md = method-&gt;method_data_or_null();
1028   assert(md != NULL, &quot;Sanity&quot;);
1029   data = md-&gt;bci_to_data(bci);
1030   assert(data != NULL,       &quot;need data for checkcast&quot;);
1031   assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1032   if (md-&gt;byte_offset_of_slot(data, DataLayout::header_offset()) + data-&gt;size_in_bytes() &gt;= 4096) {
1033     // The offset is large so bias the mdo by the base of the slot so
1034     // that the ldr can use an immediate offset to reference the slots of the data
1035     mdo_offset_bias = md-&gt;byte_offset_of_slot(data, DataLayout::header_offset());
1036   }
1037 }
1038 
1039 // On 32-bit ARM, code before this helper should test obj for null (ZF should be set if obj is null).
1040 void LIR_Assembler::typecheck_profile_helper1(ciMethod* method, int bci,
1041                                               ciMethodData*&amp; md, ciProfileData*&amp; data, int&amp; mdo_offset_bias,
1042                                               Register obj, Register mdo, Register data_val, Label* obj_is_null) {
1043   assert(method != NULL, &quot;Should have method&quot;);
1044   assert_different_registers(obj, mdo, data_val);
1045   setup_md_access(method, bci, md, data, mdo_offset_bias);
1046   Label not_null;
1047   __ b(not_null, ne);
1048   __ mov_metadata(mdo, md-&gt;constant_encoding());
1049   if (mdo_offset_bias &gt; 0) {
1050     __ mov_slow(data_val, mdo_offset_bias);
1051     __ add(mdo, mdo, data_val);
1052   }
1053   Address flags_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()) - mdo_offset_bias);
1054   __ ldrb(data_val, flags_addr);
1055   __ orr(data_val, data_val, (uint)BitData::null_seen_byte_constant());
1056   __ strb(data_val, flags_addr);
1057   __ b(*obj_is_null);
1058   __ bind(not_null);
1059 }
1060 
1061 void LIR_Assembler::typecheck_profile_helper2(ciMethodData* md, ciProfileData* data, int mdo_offset_bias,
1062                                               Register mdo, Register recv, Register value, Register tmp1,
1063                                               Label* profile_cast_success, Label* profile_cast_failure,
1064                                               Label* success, Label* failure) {
1065   assert_different_registers(mdo, value, tmp1);
1066   __ bind(*profile_cast_success);
1067   __ mov_metadata(mdo, md-&gt;constant_encoding());
1068   if (mdo_offset_bias &gt; 0) {
1069     __ mov_slow(tmp1, mdo_offset_bias);
1070     __ add(mdo, mdo, tmp1);
1071   }
1072   __ load_klass(recv, value);
1073   type_profile_helper(mdo, mdo_offset_bias, md, data, recv, tmp1, success);
1074   __ b(*success);
1075   // Cast failure case
1076   __ bind(*profile_cast_failure);
1077   __ mov_metadata(mdo, md-&gt;constant_encoding());
1078   if (mdo_offset_bias &gt; 0) {
1079     __ mov_slow(tmp1, mdo_offset_bias);
1080     __ add(mdo, mdo, tmp1);
1081   }
1082   Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) - mdo_offset_bias);
1083   __ ldr(tmp1, data_addr);
1084   __ sub(tmp1, tmp1, DataLayout::counter_increment);
1085   __ str(tmp1, data_addr);
1086   __ b(*failure);
1087 }
1088 
1089 // Sets `res` to true, if `cond` holds.
1090 static void set_instanceof_result(MacroAssembler* _masm, Register res, AsmCondition cond) {
1091   __ mov(res, 1, cond);
1092 }
1093 
1094 
1095 void LIR_Assembler::emit_opTypeCheck(LIR_OpTypeCheck* op) {
1096   // TODO: ARM - can be more effective with one more register
1097   switch (op-&gt;code()) {
1098     case lir_store_check: {
1099       CodeStub* stub = op-&gt;stub();
1100       Register value = op-&gt;object()-&gt;as_register();
1101       Register array = op-&gt;array()-&gt;as_register();
1102       Register klass_RInfo = op-&gt;tmp1()-&gt;as_register();
1103       Register k_RInfo = op-&gt;tmp2()-&gt;as_register();
1104       assert_different_registers(klass_RInfo, k_RInfo, Rtemp);
1105       if (op-&gt;should_profile()) {
1106         assert_different_registers(value, klass_RInfo, k_RInfo, Rtemp);
1107       }
1108 
1109       // check if it needs to be profiled
1110       ciMethodData* md;
1111       ciProfileData* data;
1112       int mdo_offset_bias = 0;
1113       Label profile_cast_success, profile_cast_failure, done;
1114       Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
1115       Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : stub-&gt;entry();
1116 
1117       if (op-&gt;should_profile()) {
1118         __ cmp(value, 0);
1119         typecheck_profile_helper1(op-&gt;profiled_method(), op-&gt;profiled_bci(), md, data, mdo_offset_bias, value, k_RInfo, Rtemp, &amp;done);
1120       } else {
1121         __ cbz(value, done);
1122       }
1123       assert_different_registers(k_RInfo, value);
1124       add_debug_info_for_null_check_here(op-&gt;info_for_exception());
1125       __ load_klass(k_RInfo, array);
1126       __ load_klass(klass_RInfo, value);
1127       __ ldr(k_RInfo, Address(k_RInfo, ObjArrayKlass::element_klass_offset()));
1128       __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1129       // check for immediate positive hit
1130       __ ldr(Rtemp, Address(klass_RInfo, Rtemp));
1131       __ cmp(klass_RInfo, k_RInfo);
1132       __ cond_cmp(Rtemp, k_RInfo, ne);
1133       __ b(*success_target, eq);
1134       // check for immediate negative hit
1135       __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1136       __ cmp(Rtemp, in_bytes(Klass::secondary_super_cache_offset()));
1137       __ b(*failure_target, ne);
1138       // slow case
1139       assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1140       __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1141       __ cbz(R0, *failure_target);
1142       if (op-&gt;should_profile()) {
1143         Register mdo  = klass_RInfo, recv = k_RInfo, tmp1 = Rtemp;
1144         if (mdo == value) {
1145           mdo = k_RInfo;
1146           recv = klass_RInfo;
1147         }
1148         typecheck_profile_helper2(md, data, mdo_offset_bias, mdo, recv, value, tmp1,
1149                                   &amp;profile_cast_success, &amp;profile_cast_failure,
1150                                   &amp;done, stub-&gt;entry());
1151       }
1152       __ bind(done);
1153       break;
1154     }
1155 
1156     case lir_checkcast: {
1157       CodeStub* stub = op-&gt;stub();
1158       Register obj = op-&gt;object()-&gt;as_register();
1159       Register res = op-&gt;result_opr()-&gt;as_register();
1160       Register klass_RInfo = op-&gt;tmp1()-&gt;as_register();
1161       Register k_RInfo = op-&gt;tmp2()-&gt;as_register();
1162       ciKlass* k = op-&gt;klass();
1163       assert_different_registers(res, k_RInfo, klass_RInfo, Rtemp);
1164 
1165       if (stub-&gt;is_simple_exception_stub()) {
1166       // TODO: ARM - Late binding is used to prevent confusion of register allocator
1167       assert(stub-&gt;is_exception_throw_stub(), &quot;must be&quot;);
1168       ((SimpleExceptionStub*)stub)-&gt;set_obj(op-&gt;result_opr());
1169       }
1170       ciMethodData* md;
1171       ciProfileData* data;
1172       int mdo_offset_bias = 0;
1173 
1174       Label done;
1175 
1176       Label profile_cast_failure, profile_cast_success;
1177       Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : op-&gt;stub()-&gt;entry();
1178       Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
1179 
1180 
1181       __ movs(res, obj);
1182       if (op-&gt;should_profile()) {
1183         typecheck_profile_helper1(op-&gt;profiled_method(), op-&gt;profiled_bci(), md, data, mdo_offset_bias, res, klass_RInfo, Rtemp, &amp;done);
1184       } else {
1185         __ b(done, eq);
1186       }
1187       if (k-&gt;is_loaded()) {
1188         __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1189       } else if (k_RInfo != obj) {
1190         klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1191         __ movs(res, obj);
1192       } else {
1193         // Patching doesn&#39;t update &quot;res&quot; register after GC, so do patching first
1194         klass2reg_with_patching(Rtemp, op-&gt;info_for_patch());
1195         __ movs(res, obj);
1196         __ mov(k_RInfo, Rtemp);
1197       }
1198       __ load_klass(klass_RInfo, res, ne);
1199 
1200       if (op-&gt;fast_check()) {
1201         __ cmp(klass_RInfo, k_RInfo, ne);
1202         __ b(*failure_target, ne);
1203       } else if (k-&gt;is_loaded()) {
1204         __ b(*success_target, eq);
1205         __ ldr(Rtemp, Address(klass_RInfo, k-&gt;super_check_offset()));
1206         if (in_bytes(Klass::secondary_super_cache_offset()) != (int) k-&gt;super_check_offset()) {
1207           __ cmp(Rtemp, k_RInfo);
1208           __ b(*failure_target, ne);
1209         } else {
1210           __ cmp(klass_RInfo, k_RInfo);
1211           __ cmp(Rtemp, k_RInfo, ne);
1212           __ b(*success_target, eq);
1213           assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1214           __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1215           __ cbz(R0, *failure_target);
1216         }
1217       } else {
1218         __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1219         __ b(*success_target, eq);
1220         // check for immediate positive hit
1221         __ ldr(Rtemp, Address(klass_RInfo, Rtemp));
1222         __ cmp(klass_RInfo, k_RInfo);
1223         __ cmp(Rtemp, k_RInfo, ne);
1224         __ b(*success_target, eq);
1225         // check for immediate negative hit
1226         __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1227         __ cmp(Rtemp, in_bytes(Klass::secondary_super_cache_offset()));
1228         __ b(*failure_target, ne);
1229         // slow case
1230         assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1231         __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1232         __ cbz(R0, *failure_target);
1233       }
1234 
1235       if (op-&gt;should_profile()) {
1236         Register mdo  = klass_RInfo, recv = k_RInfo, tmp1 = Rtemp;
1237         typecheck_profile_helper2(md, data, mdo_offset_bias, mdo, recv, res, tmp1,
1238                                   &amp;profile_cast_success, &amp;profile_cast_failure,
1239                                   &amp;done, stub-&gt;entry());
1240       }
1241       __ bind(done);
1242       break;
1243     }
1244 
1245     case lir_instanceof: {
1246       Register obj = op-&gt;object()-&gt;as_register();
1247       Register res = op-&gt;result_opr()-&gt;as_register();
1248       Register klass_RInfo = op-&gt;tmp1()-&gt;as_register();
1249       Register k_RInfo = op-&gt;tmp2()-&gt;as_register();
1250       ciKlass* k = op-&gt;klass();
1251       assert_different_registers(res, klass_RInfo, k_RInfo, Rtemp);
1252 
1253       ciMethodData* md;
1254       ciProfileData* data;
1255       int mdo_offset_bias = 0;
1256 
1257       Label done;
1258 
1259       Label profile_cast_failure, profile_cast_success;
1260       Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : &amp;done;
1261       Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
1262 
1263       __ movs(res, obj);
1264 
1265       if (op-&gt;should_profile()) {
1266         typecheck_profile_helper1(op-&gt;profiled_method(), op-&gt;profiled_bci(), md, data, mdo_offset_bias, res, klass_RInfo, Rtemp, &amp;done);
1267       } else {
1268         __ b(done, eq);
1269       }
1270 
1271       if (k-&gt;is_loaded()) {
1272         __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1273       } else {
1274         op-&gt;info_for_patch()-&gt;add_register_oop(FrameMap::as_oop_opr(res));
1275         klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1276       }
1277       __ load_klass(klass_RInfo, res);
1278 
1279       if (!op-&gt;should_profile()) {
1280         __ mov(res, 0);
1281       }
1282 
1283       if (op-&gt;fast_check()) {
1284         __ cmp(klass_RInfo, k_RInfo);
1285         if (!op-&gt;should_profile()) {
1286           set_instanceof_result(_masm, res, eq);
1287         } else {
1288           __ b(profile_cast_failure, ne);
1289         }
1290       } else if (k-&gt;is_loaded()) {
1291         __ ldr(Rtemp, Address(klass_RInfo, k-&gt;super_check_offset()));
1292         if (in_bytes(Klass::secondary_super_cache_offset()) != (int) k-&gt;super_check_offset()) {
1293           __ cmp(Rtemp, k_RInfo);
1294           if (!op-&gt;should_profile()) {
1295             set_instanceof_result(_masm, res, eq);
1296           } else {
1297             __ b(profile_cast_failure, ne);
1298           }
1299         } else {
1300           __ cmp(klass_RInfo, k_RInfo);
1301           __ cond_cmp(Rtemp, k_RInfo, ne);
1302           if (!op-&gt;should_profile()) {
1303             set_instanceof_result(_masm, res, eq);
1304           }
1305           __ b(*success_target, eq);
1306           assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1307           __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1308           if (!op-&gt;should_profile()) {
1309             move_regs(R0, res);
1310           } else {
1311             __ cbz(R0, *failure_target);
1312           }
1313         }
1314       } else {
1315         __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1316         // check for immediate positive hit
1317         __ cmp(klass_RInfo, k_RInfo);
1318         if (!op-&gt;should_profile()) {
1319           __ ldr(res, Address(klass_RInfo, Rtemp), ne);
1320           __ cond_cmp(res, k_RInfo, ne);
1321           set_instanceof_result(_masm, res, eq);
1322         } else {
1323           __ ldr(Rtemp, Address(klass_RInfo, Rtemp), ne);
1324           __ cond_cmp(Rtemp, k_RInfo, ne);
1325         }
1326         __ b(*success_target, eq);
1327         // check for immediate negative hit
1328         if (op-&gt;should_profile()) {
1329           __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1330         }
1331         __ cmp(Rtemp, in_bytes(Klass::secondary_super_cache_offset()));
1332         if (!op-&gt;should_profile()) {
1333           __ mov(res, 0, ne);
1334         }
1335         __ b(*failure_target, ne);
1336         // slow case
1337         assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1338         __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1339         if (!op-&gt;should_profile()) {
1340           move_regs(R0, res);
1341         }
1342         if (op-&gt;should_profile()) {
1343           __ cbz(R0, *failure_target);
1344         }
1345       }
1346 
1347       if (op-&gt;should_profile()) {
1348         Label done_ok, done_failure;
1349         Register mdo  = klass_RInfo, recv = k_RInfo, tmp1 = Rtemp;
1350         typecheck_profile_helper2(md, data, mdo_offset_bias, mdo, recv, res, tmp1,
1351                                   &amp;profile_cast_success, &amp;profile_cast_failure,
1352                                   &amp;done_ok, &amp;done_failure);
1353         __ bind(done_failure);
1354         __ mov(res, 0);
1355         __ b(done);
1356         __ bind(done_ok);
1357         __ mov(res, 1);
1358       }
1359       __ bind(done);
1360       break;
1361     }
1362     default:
1363       ShouldNotReachHere();
1364   }
1365 }
1366 
1367 
1368 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
1369   //   if (*addr == cmpval) {
1370   //     *addr = newval;
1371   //     dest = 1;
1372   //   } else {
1373   //     dest = 0;
1374   //   }
1375   // FIXME: membar_release
1376   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
1377   Register addr = op-&gt;addr()-&gt;is_register() ?
1378     op-&gt;addr()-&gt;as_pointer_register() :
1379     op-&gt;addr()-&gt;as_address_ptr()-&gt;base()-&gt;as_pointer_register();
1380   assert(op-&gt;addr()-&gt;is_register() || op-&gt;addr()-&gt;as_address_ptr()-&gt;disp() == 0, &quot;unexpected disp&quot;);
1381   assert(op-&gt;addr()-&gt;is_register() || op-&gt;addr()-&gt;as_address_ptr()-&gt;index() == LIR_OprDesc::illegalOpr(), &quot;unexpected index&quot;);
1382   if (op-&gt;code() == lir_cas_int || op-&gt;code() == lir_cas_obj) {
1383     Register cmpval = op-&gt;cmp_value()-&gt;as_register();
1384     Register newval = op-&gt;new_value()-&gt;as_register();
1385     Register dest = op-&gt;result_opr()-&gt;as_register();
1386     assert_different_registers(dest, addr, cmpval, newval, Rtemp);
1387 
1388     __ atomic_cas_bool(cmpval, newval, addr, 0, Rtemp); // Rtemp free by default at C1 LIR layer
1389     __ mov(dest, 1, eq);
1390     __ mov(dest, 0, ne);
1391   } else if (op-&gt;code() == lir_cas_long) {
1392     assert(VM_Version::supports_cx8(), &quot;wrong machine&quot;);
1393     Register cmp_value_lo = op-&gt;cmp_value()-&gt;as_register_lo();
1394     Register cmp_value_hi = op-&gt;cmp_value()-&gt;as_register_hi();
1395     Register new_value_lo = op-&gt;new_value()-&gt;as_register_lo();
1396     Register new_value_hi = op-&gt;new_value()-&gt;as_register_hi();
1397     Register dest = op-&gt;result_opr()-&gt;as_register();
1398     Register tmp_lo = op-&gt;tmp1()-&gt;as_register_lo();
1399     Register tmp_hi = op-&gt;tmp1()-&gt;as_register_hi();
1400 
1401     assert_different_registers(tmp_lo, tmp_hi, cmp_value_lo, cmp_value_hi, dest, new_value_lo, new_value_hi, addr);
1402     assert(tmp_hi-&gt;encoding() == tmp_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
1403     assert(new_value_hi-&gt;encoding() == new_value_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
1404     assert((tmp_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
1405     assert((new_value_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
1406     __ atomic_cas64(tmp_lo, tmp_hi, dest, cmp_value_lo, cmp_value_hi,
1407                     new_value_lo, new_value_hi, addr, 0);
1408   } else {
1409     Unimplemented();
1410   }
1411   // FIXME: is full membar really needed instead of just membar_acquire?
1412   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreLoad | MacroAssembler::StoreStore), Rtemp);
1413 }
1414 
1415 
1416 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
1417   AsmCondition acond = al;
1418   AsmCondition ncond = nv;
1419   if (opr1 != opr2) {
1420     switch (condition) {
1421       case lir_cond_equal:        acond = eq; ncond = ne; break;
1422       case lir_cond_notEqual:     acond = ne; ncond = eq; break;
1423       case lir_cond_less:         acond = lt; ncond = ge; break;
1424       case lir_cond_lessEqual:    acond = le; ncond = gt; break;
1425       case lir_cond_greaterEqual: acond = ge; ncond = lt; break;
1426       case lir_cond_greater:      acond = gt; ncond = le; break;
1427       case lir_cond_aboveEqual:   acond = hs; ncond = lo; break;
1428       case lir_cond_belowEqual:   acond = ls; ncond = hi; break;
1429       default: ShouldNotReachHere();
1430     }
1431   }
1432 
1433   for (;;) {                         // two iterations only
1434     if (opr1 == result) {
1435       // do nothing
1436     } else if (opr1-&gt;is_single_cpu()) {
1437       __ mov(result-&gt;as_register(), opr1-&gt;as_register(), acond);
1438     } else if (opr1-&gt;is_double_cpu()) {
1439       __ long_move(result-&gt;as_register_lo(), result-&gt;as_register_hi(),
1440                    opr1-&gt;as_register_lo(), opr1-&gt;as_register_hi(), acond);
1441     } else if (opr1-&gt;is_single_stack()) {
1442       __ ldr(result-&gt;as_register(), frame_map()-&gt;address_for_slot(opr1-&gt;single_stack_ix()), acond);
1443     } else if (opr1-&gt;is_double_stack()) {
1444       __ ldr(result-&gt;as_register_lo(),
1445              frame_map()-&gt;address_for_slot(opr1-&gt;double_stack_ix(), lo_word_offset_in_bytes), acond);
1446       __ ldr(result-&gt;as_register_hi(),
1447              frame_map()-&gt;address_for_slot(opr1-&gt;double_stack_ix(), hi_word_offset_in_bytes), acond);
1448     } else if (opr1-&gt;is_illegal()) {
1449       // do nothing: this part of the cmove has been optimized away in the peephole optimizer
1450     } else {
1451       assert(opr1-&gt;is_constant(), &quot;must be&quot;);
1452       LIR_Const* c = opr1-&gt;as_constant_ptr();
1453 
1454       switch (c-&gt;type()) {
1455         case T_INT:
1456           __ mov_slow(result-&gt;as_register(), c-&gt;as_jint(), acond);
1457           break;
1458         case T_LONG:
1459           __ mov_slow(result-&gt;as_register_lo(), c-&gt;as_jint_lo(), acond);
1460           __ mov_slow(result-&gt;as_register_hi(), c-&gt;as_jint_hi(), acond);
1461           break;
1462         case T_OBJECT:
1463           __ mov_oop(result-&gt;as_register(), c-&gt;as_jobject(), 0, acond);
1464           break;
1465         case T_FLOAT:
1466 #ifdef __SOFTFP__
1467           // not generated now.
1468           __ mov_slow(result-&gt;as_register(), c-&gt;as_jint(), acond);
1469 #else
1470           __ mov_float(result-&gt;as_float_reg(), c-&gt;as_jfloat(), acond);
1471 #endif // __SOFTFP__
1472           break;
1473         case T_DOUBLE:
1474 #ifdef __SOFTFP__
1475           // not generated now.
1476           __ mov_slow(result-&gt;as_register_lo(), c-&gt;as_jint_lo(), acond);
1477           __ mov_slow(result-&gt;as_register_hi(), c-&gt;as_jint_hi(), acond);
1478 #else
1479           __ mov_double(result-&gt;as_double_reg(), c-&gt;as_jdouble(), acond);
1480 #endif // __SOFTFP__
1481           break;
1482         default:
1483           ShouldNotReachHere();
1484       }
1485     }
1486 
1487     // Negate the condition and repeat the algorithm with the second operand
1488     if (opr1 == opr2) { break; }
1489     opr1 = opr2;
1490     acond = ncond;
1491   }
1492 }
1493 
1494 #ifdef ASSERT
1495 static int reg_size(LIR_Opr op) {
1496   switch (op-&gt;type()) {
1497   case T_FLOAT:
1498   case T_INT:      return BytesPerInt;
1499   case T_LONG:
1500   case T_DOUBLE:   return BytesPerLong;
1501   case T_OBJECT:
1502   case T_ARRAY:
1503   case T_METADATA: return BytesPerWord;
1504   case T_ADDRESS:
1505   case T_ILLEGAL:  // fall through
1506   default: ShouldNotReachHere(); return -1;
1507   }
1508 }
1509 #endif
1510 
1511 void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info, bool pop_fpu_stack) {
1512   assert(info == NULL, &quot;unused on this code path&quot;);
1513   assert(dest-&gt;is_register(), &quot;wrong items state&quot;);
1514 
1515   if (right-&gt;is_address()) {
1516     // special case for adding shifted/extended register
1517     const Register res = dest-&gt;as_pointer_register();
1518     const Register lreg = left-&gt;as_pointer_register();
1519     const LIR_Address* addr = right-&gt;as_address_ptr();
1520 
1521     assert(addr-&gt;base()-&gt;as_pointer_register() == lreg &amp;&amp; addr-&gt;index()-&gt;is_register() &amp;&amp; addr-&gt;disp() == 0, &quot;must be&quot;);
1522 
1523     int scale = addr-&gt;scale();
1524     AsmShift shift = lsl;
1525 
1526 
1527     assert(reg_size(addr-&gt;base()) == reg_size(addr-&gt;index()), &quot;should be&quot;);
1528     assert(reg_size(addr-&gt;base()) == reg_size(dest), &quot;should be&quot;);
1529     assert(reg_size(dest) == wordSize, &quot;should be&quot;);
1530 
1531     AsmOperand operand(addr-&gt;index()-&gt;as_pointer_register(), shift, scale);
1532     switch (code) {
1533       case lir_add: __ add(res, lreg, operand); break;
1534       case lir_sub: __ sub(res, lreg, operand); break;
1535       default: ShouldNotReachHere();
1536     }
1537 
1538   } else if (left-&gt;is_address()) {
1539     assert(code == lir_sub &amp;&amp; right-&gt;is_single_cpu(), &quot;special case used by strength_reduce_multiply()&quot;);
1540     const LIR_Address* addr = left-&gt;as_address_ptr();
1541     const Register res = dest-&gt;as_register();
1542     const Register rreg = right-&gt;as_register();
1543     assert(addr-&gt;base()-&gt;as_register() == rreg &amp;&amp; addr-&gt;index()-&gt;is_register() &amp;&amp; addr-&gt;disp() == 0, &quot;must be&quot;);
1544     __ rsb(res, rreg, AsmOperand(addr-&gt;index()-&gt;as_register(), lsl, addr-&gt;scale()));
1545 
1546   } else if (dest-&gt;is_single_cpu()) {
1547     assert(left-&gt;is_single_cpu(), &quot;unexpected left operand&quot;);
1548 
1549     const Register res = dest-&gt;as_register();
1550     const Register lreg = left-&gt;as_register();
1551 
1552     if (right-&gt;is_single_cpu()) {
1553       const Register rreg = right-&gt;as_register();
1554       switch (code) {
1555         case lir_add: __ add_32(res, lreg, rreg); break;
1556         case lir_sub: __ sub_32(res, lreg, rreg); break;
1557         case lir_mul: __ mul_32(res, lreg, rreg); break;
1558         default: ShouldNotReachHere();
1559       }
1560     } else {
1561       assert(right-&gt;is_constant(), &quot;must be&quot;);
1562       const jint c = right-&gt;as_constant_ptr()-&gt;as_jint();
1563       if (!Assembler::is_arith_imm_in_range(c)) {
1564         BAILOUT(&quot;illegal arithmetic operand&quot;);
1565       }
1566       switch (code) {
1567         case lir_add: __ add_32(res, lreg, c); break;
1568         case lir_sub: __ sub_32(res, lreg, c); break;
1569         default: ShouldNotReachHere();
1570       }
1571     }
1572 
1573   } else if (dest-&gt;is_double_cpu()) {
1574     Register res_lo = dest-&gt;as_register_lo();
1575     Register res_hi = dest-&gt;as_register_hi();
1576     Register lreg_lo = left-&gt;as_register_lo();
1577     Register lreg_hi = left-&gt;as_register_hi();
1578     if (right-&gt;is_double_cpu()) {
1579       Register rreg_lo = right-&gt;as_register_lo();
1580       Register rreg_hi = right-&gt;as_register_hi();
1581       if (res_lo == lreg_hi || res_lo == rreg_hi) {
1582         res_lo = Rtemp;
1583       }
1584       switch (code) {
1585         case lir_add:
1586           __ adds(res_lo, lreg_lo, rreg_lo);
1587           __ adc(res_hi, lreg_hi, rreg_hi);
1588           break;
1589         case lir_sub:
1590           __ subs(res_lo, lreg_lo, rreg_lo);
1591           __ sbc(res_hi, lreg_hi, rreg_hi);
1592           break;
1593         default:
1594           ShouldNotReachHere();
1595       }
1596     } else {
1597       assert(right-&gt;is_constant(), &quot;must be&quot;);
1598       assert((right-&gt;as_constant_ptr()-&gt;as_jlong() &gt;&gt; 32) == 0, &quot;out of range&quot;);
1599       const jint c = (jint) right-&gt;as_constant_ptr()-&gt;as_jlong();
1600       if (res_lo == lreg_hi) {
1601         res_lo = Rtemp;
1602       }
1603       switch (code) {
1604         case lir_add:
1605           __ adds(res_lo, lreg_lo, c);
1606           __ adc(res_hi, lreg_hi, 0);
1607           break;
1608         case lir_sub:
1609           __ subs(res_lo, lreg_lo, c);
1610           __ sbc(res_hi, lreg_hi, 0);
1611           break;
1612         default:
1613           ShouldNotReachHere();
1614       }
1615     }
1616     move_regs(res_lo, dest-&gt;as_register_lo());
1617 
1618   } else if (dest-&gt;is_single_fpu()) {
1619     assert(left-&gt;is_single_fpu(), &quot;must be&quot;);
1620     assert(right-&gt;is_single_fpu(), &quot;must be&quot;);
1621     const FloatRegister res = dest-&gt;as_float_reg();
1622     const FloatRegister lreg = left-&gt;as_float_reg();
1623     const FloatRegister rreg = right-&gt;as_float_reg();
1624     switch (code) {
1625       case lir_add: __ add_float(res, lreg, rreg); break;
1626       case lir_sub: __ sub_float(res, lreg, rreg); break;
1627       case lir_mul_strictfp: // fall through
1628       case lir_mul: __ mul_float(res, lreg, rreg); break;
1629       case lir_div_strictfp: // fall through
1630       case lir_div: __ div_float(res, lreg, rreg); break;
1631       default: ShouldNotReachHere();
1632     }
1633   } else if (dest-&gt;is_double_fpu()) {
1634     assert(left-&gt;is_double_fpu(), &quot;must be&quot;);
1635     assert(right-&gt;is_double_fpu(), &quot;must be&quot;);
1636     const FloatRegister res = dest-&gt;as_double_reg();
1637     const FloatRegister lreg = left-&gt;as_double_reg();
1638     const FloatRegister rreg = right-&gt;as_double_reg();
1639     switch (code) {
1640       case lir_add: __ add_double(res, lreg, rreg); break;
1641       case lir_sub: __ sub_double(res, lreg, rreg); break;
1642       case lir_mul_strictfp: // fall through
1643       case lir_mul: __ mul_double(res, lreg, rreg); break;
1644       case lir_div_strictfp: // fall through
1645       case lir_div: __ div_double(res, lreg, rreg); break;
1646       default: ShouldNotReachHere();
1647     }
1648   } else {
1649     ShouldNotReachHere();
1650   }
1651 }
1652 
1653 
1654 void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr unused, LIR_Opr dest, LIR_Op* op) {
1655   switch (code) {
1656     case lir_abs:
1657       __ abs_double(dest-&gt;as_double_reg(), value-&gt;as_double_reg());
1658       break;
1659     case lir_sqrt:
1660       __ sqrt_double(dest-&gt;as_double_reg(), value-&gt;as_double_reg());
1661       break;
1662     default:
1663       ShouldNotReachHere();
1664   }
1665 }
1666 
1667 
1668 void LIR_Assembler::logic_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest) {
1669   assert(dest-&gt;is_register(), &quot;wrong items state&quot;);
1670   assert(left-&gt;is_register(), &quot;wrong items state&quot;);
1671 
1672   if (dest-&gt;is_single_cpu()) {
1673 
1674     const Register res = dest-&gt;as_register();
1675     const Register lreg = left-&gt;as_register();
1676 
1677     if (right-&gt;is_single_cpu()) {
1678       const Register rreg = right-&gt;as_register();
1679       switch (code) {
1680         case lir_logic_and: __ and_32(res, lreg, rreg); break;
1681         case lir_logic_or:  __ orr_32(res, lreg, rreg); break;
1682         case lir_logic_xor: __ eor_32(res, lreg, rreg); break;
1683         default: ShouldNotReachHere();
1684       }
1685     } else {
1686       assert(right-&gt;is_constant(), &quot;must be&quot;);
1687       const uint c = (uint)right-&gt;as_constant_ptr()-&gt;as_jint();
1688       switch (code) {
1689         case lir_logic_and: __ and_32(res, lreg, c); break;
1690         case lir_logic_or:  __ orr_32(res, lreg, c); break;
1691         case lir_logic_xor: __ eor_32(res, lreg, c); break;
1692         default: ShouldNotReachHere();
1693       }
1694     }
1695   } else {
1696     assert(dest-&gt;is_double_cpu(), &quot;should be&quot;);
1697     Register res_lo = dest-&gt;as_register_lo();
1698 
1699     assert (dest-&gt;type() == T_LONG, &quot;unexpected result type&quot;);
1700     assert (left-&gt;type() == T_LONG, &quot;unexpected left type&quot;);
1701     assert (right-&gt;type() == T_LONG, &quot;unexpected right type&quot;);
1702 
1703     const Register res_hi = dest-&gt;as_register_hi();
1704     const Register lreg_lo = left-&gt;as_register_lo();
1705     const Register lreg_hi = left-&gt;as_register_hi();
1706 
1707     if (right-&gt;is_register()) {
1708       const Register rreg_lo = right-&gt;as_register_lo();
1709       const Register rreg_hi = right-&gt;as_register_hi();
1710       if (res_lo == lreg_hi || res_lo == rreg_hi) {
1711         res_lo = Rtemp; // Temp register helps to avoid overlap between result and input
1712       }
1713       switch (code) {
1714         case lir_logic_and:
1715           __ andr(res_lo, lreg_lo, rreg_lo);
1716           __ andr(res_hi, lreg_hi, rreg_hi);
1717           break;
1718         case lir_logic_or:
1719           __ orr(res_lo, lreg_lo, rreg_lo);
1720           __ orr(res_hi, lreg_hi, rreg_hi);
1721           break;
1722         case lir_logic_xor:
1723           __ eor(res_lo, lreg_lo, rreg_lo);
1724           __ eor(res_hi, lreg_hi, rreg_hi);
1725           break;
1726         default:
1727           ShouldNotReachHere();
1728       }
1729       move_regs(res_lo, dest-&gt;as_register_lo());
1730     } else {
1731       assert(right-&gt;is_constant(), &quot;must be&quot;);
1732       const jint c_lo = (jint) right-&gt;as_constant_ptr()-&gt;as_jlong();
1733       const jint c_hi = (jint) (right-&gt;as_constant_ptr()-&gt;as_jlong() &gt;&gt; 32);
1734       // Case for logic_or from do_ClassIDIntrinsic()
1735       if (c_hi == 0 &amp;&amp; AsmOperand::is_rotated_imm(c_lo)) {
1736         switch (code) {
1737           case lir_logic_and:
1738             __ andr(res_lo, lreg_lo, c_lo);
1739             __ mov(res_hi, 0);
1740             break;
1741           case lir_logic_or:
1742             __ orr(res_lo, lreg_lo, c_lo);
1743             break;
1744           case lir_logic_xor:
1745             __ eor(res_lo, lreg_lo, c_lo);
1746             break;
1747         default:
1748           ShouldNotReachHere();
1749         }
1750       } else if (code == lir_logic_and &amp;&amp;
1751                  c_hi == -1 &amp;&amp;
1752                  (AsmOperand::is_rotated_imm(c_lo) ||
1753                   AsmOperand::is_rotated_imm(~c_lo))) {
1754         // Another case which handles logic_and from do_ClassIDIntrinsic()
1755         if (AsmOperand::is_rotated_imm(c_lo)) {
1756           __ andr(res_lo, lreg_lo, c_lo);
1757         } else {
1758           __ bic(res_lo, lreg_lo, ~c_lo);
1759         }
1760         if (res_hi != lreg_hi) {
1761           __ mov(res_hi, lreg_hi);
1762         }
1763       } else {
1764         BAILOUT(&quot;64 bit constant cannot be inlined&quot;);
1765       }
1766     }
1767   }
1768 }
1769 
1770 
1771 
1772 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
1773   if (opr1-&gt;is_single_cpu()) {
1774     if (opr2-&gt;is_constant()) {
1775       switch (opr2-&gt;as_constant_ptr()-&gt;type()) {
1776         case T_INT: {
1777           const jint c = opr2-&gt;as_constant_ptr()-&gt;as_jint();
1778           if (Assembler::is_arith_imm_in_range(c)) {
1779             __ cmp_32(opr1-&gt;as_register(), c);
1780           } else if (Assembler::is_arith_imm_in_range(-c)) {
1781             __ cmn_32(opr1-&gt;as_register(), -c);
1782           } else {
1783             // This can happen when compiling lookupswitch
1784             __ mov_slow(Rtemp, c);
1785             __ cmp_32(opr1-&gt;as_register(), Rtemp);
1786           }
1787           break;
1788         }
1789         case T_OBJECT:
1790           assert(opr2-&gt;as_constant_ptr()-&gt;as_jobject() == NULL, &quot;cannot handle otherwise&quot;);
1791           __ cmp(opr1-&gt;as_register(), 0);
1792           break;
1793         case T_METADATA:
1794           assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;Only equality tests&quot;);
1795           assert(opr2-&gt;as_constant_ptr()-&gt;as_metadata() == NULL, &quot;cannot handle otherwise&quot;);
1796           __ cmp(opr1-&gt;as_register(), 0);
1797           break;
1798         default:
1799           ShouldNotReachHere();
1800       }
1801     } else if (opr2-&gt;is_single_cpu()) {
1802       if (opr1-&gt;type() == T_OBJECT || opr1-&gt;type() == T_ARRAY) {
1803         assert(opr2-&gt;type() == T_OBJECT || opr2-&gt;type() == T_ARRAY, &quot;incompatibe type&quot;);
1804         __ cmpoop(opr1-&gt;as_register(), opr2-&gt;as_register());
1805       } else if (opr1-&gt;type() == T_METADATA || opr1-&gt;type() == T_ADDRESS) {
1806         assert(opr2-&gt;type() == T_METADATA || opr2-&gt;type() == T_ADDRESS, &quot;incompatibe type&quot;);
1807         __ cmp(opr1-&gt;as_register(), opr2-&gt;as_register());
1808       } else {
1809         assert(opr2-&gt;type() != T_OBJECT &amp;&amp; opr2-&gt;type() != T_ARRAY &amp;&amp; opr2-&gt;type() != T_METADATA &amp;&amp; opr2-&gt;type() != T_ADDRESS, &quot;incompatibe type&quot;);
1810         __ cmp_32(opr1-&gt;as_register(), opr2-&gt;as_register());
1811       }
1812     } else {
1813       ShouldNotReachHere();
1814     }
1815   } else if (opr1-&gt;is_double_cpu()) {
1816     Register xlo = opr1-&gt;as_register_lo();
1817     Register xhi = opr1-&gt;as_register_hi();
1818     if (opr2-&gt;is_constant() &amp;&amp; opr2-&gt;as_jlong() == 0) {
1819       assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;cannot handle otherwise&quot;);
1820       __ orrs(Rtemp, xlo, xhi);
1821     } else if (opr2-&gt;is_register()) {
1822       Register ylo = opr2-&gt;as_register_lo();
1823       Register yhi = opr2-&gt;as_register_hi();
1824       if (condition == lir_cond_equal || condition == lir_cond_notEqual) {
1825         __ teq(xhi, yhi);
1826         __ teq(xlo, ylo, eq);
1827       } else {
1828         __ subs(xlo, xlo, ylo);
1829         __ sbcs(xhi, xhi, yhi);
1830       }
1831     } else {
1832       ShouldNotReachHere();
1833     }
1834   } else if (opr1-&gt;is_single_fpu()) {
1835     if (opr2-&gt;is_constant()) {
1836       assert(opr2-&gt;as_jfloat() == 0.0f, &quot;cannot handle otherwise&quot;);
1837       __ cmp_zero_float(opr1-&gt;as_float_reg());
1838     } else {
1839       __ cmp_float(opr1-&gt;as_float_reg(), opr2-&gt;as_float_reg());
1840     }
1841   } else if (opr1-&gt;is_double_fpu()) {
1842     if (opr2-&gt;is_constant()) {
1843       assert(opr2-&gt;as_jdouble() == 0.0, &quot;cannot handle otherwise&quot;);
1844       __ cmp_zero_double(opr1-&gt;as_double_reg());
1845     } else {
1846       __ cmp_double(opr1-&gt;as_double_reg(), opr2-&gt;as_double_reg());
1847     }
1848   } else {
1849     ShouldNotReachHere();
1850   }
1851 }
1852 
1853 void LIR_Assembler::comp_fl2i(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst, LIR_Op2* op) {
1854   const Register res = dst-&gt;as_register();
1855   if (code == lir_cmp_fd2i || code == lir_ucmp_fd2i) {
1856     comp_op(lir_cond_unknown, left, right, op);
1857     __ fmstat();
1858     if (code == lir_ucmp_fd2i) {  // unordered is less
1859       __ mvn(res, 0, lt);
1860       __ mov(res, 1, ge);
1861     } else {                      // unordered is greater
1862       __ mov(res, 1, cs);
1863       __ mvn(res, 0, cc);
1864     }
1865     __ mov(res, 0, eq);
1866 
1867   } else {
1868     assert(code == lir_cmp_l2i, &quot;must be&quot;);
1869 
1870     Label done;
1871     const Register xlo = left-&gt;as_register_lo();
1872     const Register xhi = left-&gt;as_register_hi();
1873     const Register ylo = right-&gt;as_register_lo();
1874     const Register yhi = right-&gt;as_register_hi();
1875     __ cmp(xhi, yhi);
1876     __ mov(res, 1, gt);
1877     __ mvn(res, 0, lt);
1878     __ b(done, ne);
1879     __ subs(res, xlo, ylo);
1880     __ mov(res, 1, hi);
1881     __ mvn(res, 0, lo);
1882     __ bind(done);
1883   }
1884 }
1885 
1886 
1887 void LIR_Assembler::align_call(LIR_Code code) {
1888   // Not needed
1889 }
1890 
1891 
1892 void LIR_Assembler::call(LIR_OpJavaCall *op, relocInfo::relocType rtype) {
1893   int ret_addr_offset = __ patchable_call(op-&gt;addr(), rtype);
1894   assert(ret_addr_offset == __ offset(), &quot;embedded return address not allowed&quot;);
1895   add_call_info_here(op-&gt;info());
1896 }
1897 
1898 
1899 void LIR_Assembler::ic_call(LIR_OpJavaCall *op) {
1900   bool near_range = __ cache_fully_reachable();
1901   address oop_address = pc();
1902 
1903   bool use_movw = VM_Version::supports_movw();
1904 
1905   // Ricklass may contain something that is not a metadata pointer so
1906   // mov_metadata can&#39;t be used
1907   InlinedAddress value((address)Universe::non_oop_word());
1908   InlinedAddress addr(op-&gt;addr());
1909   if (use_movw) {
1910     __ movw(Ricklass, ((unsigned int)Universe::non_oop_word()) &amp; 0xffff);
1911     __ movt(Ricklass, ((unsigned int)Universe::non_oop_word()) &gt;&gt; 16);
1912   } else {
1913     // No movw/movt, must be load a pc relative value but no
1914     // relocation so no metadata table to load from.
1915     // Use a b instruction rather than a bl, inline constant after the
1916     // branch, use a PC relative ldr to load the constant, arrange for
1917     // the call to return after the constant(s).
1918     __ ldr_literal(Ricklass, value);
1919   }
1920   __ relocate(virtual_call_Relocation::spec(oop_address));
1921   if (near_range &amp;&amp; use_movw) {
1922     __ bl(op-&gt;addr());
1923   } else {
1924     Label call_return;
1925     __ adr(LR, call_return);
1926     if (near_range) {
1927       __ b(op-&gt;addr());
1928     } else {
1929       __ indirect_jump(addr, Rtemp);
1930       __ bind_literal(addr);
1931     }
1932     if (!use_movw) {
1933       __ bind_literal(value);
1934     }
1935     __ bind(call_return);
1936   }
1937   add_call_info(code_offset(), op-&gt;info());
1938 }
1939 
1940 
1941 /* Currently, vtable-dispatch is only enabled for sparc platforms */
1942 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
1943   ShouldNotReachHere();
1944 }
1945 
1946 void LIR_Assembler::emit_static_call_stub() {
1947   address call_pc = __ pc();
1948   address stub = __ start_a_stub(call_stub_size());
1949   if (stub == NULL) {
1950     BAILOUT(&quot;static call stub overflow&quot;);
1951   }
1952 
1953   DEBUG_ONLY(int offset = code_offset();)
1954 
1955   InlinedMetadata metadata_literal(NULL);
1956   __ relocate(static_stub_Relocation::spec(call_pc));
1957   // If not a single instruction, NativeMovConstReg::next_instruction_address()
1958   // must jump over the whole following ldr_literal.
1959   // (See CompiledStaticCall::set_to_interpreted())
1960 #ifdef ASSERT
1961   address ldr_site = __ pc();
1962 #endif
1963   __ ldr_literal(Rmethod, metadata_literal);
1964   assert(nativeMovConstReg_at(ldr_site)-&gt;next_instruction_address() == __ pc(), &quot;Fix ldr_literal or its parsing&quot;);
1965   bool near_range = __ cache_fully_reachable();
1966   InlinedAddress dest((address)-1);
1967   if (near_range) {
1968     address branch_site = __ pc();
1969     __ b(branch_site); // b to self maps to special NativeJump -1 destination
1970   } else {
1971     __ indirect_jump(dest, Rtemp);
1972   }
1973   __ bind_literal(metadata_literal); // includes spec_for_immediate reloc
1974   if (!near_range) {
1975     __ bind_literal(dest); // special NativeJump -1 destination
1976   }
1977 
1978   assert(code_offset() - offset &lt;= call_stub_size(), &quot;overflow&quot;);
1979   __ end_a_stub();
1980 }
1981 
1982 void LIR_Assembler::throw_op(LIR_Opr exceptionPC, LIR_Opr exceptionOop, CodeEmitInfo* info) {
1983   assert(exceptionOop-&gt;as_register() == Rexception_obj, &quot;must match&quot;);
1984   assert(exceptionPC-&gt;as_register()  == Rexception_pc, &quot;must match&quot;);
1985   info-&gt;add_register_oop(exceptionOop);
1986 
1987   Runtime1::StubID handle_id = compilation()-&gt;has_fpu_code() ?
1988                                Runtime1::handle_exception_id :
1989                                Runtime1::handle_exception_nofpu_id;
1990   Label return_address;
1991   __ adr(Rexception_pc, return_address);
1992   __ call(Runtime1::entry_for(handle_id), relocInfo::runtime_call_type);
1993   __ bind(return_address);
1994   add_call_info_here(info);  // for exception handler
1995 }
1996 
1997 void LIR_Assembler::unwind_op(LIR_Opr exceptionOop) {
1998   assert(exceptionOop-&gt;as_register() == Rexception_obj, &quot;must match&quot;);
1999   __ b(_unwind_handler_entry);
2000 }
2001 
2002 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, LIR_Opr count, LIR_Opr dest, LIR_Opr tmp) {
2003   AsmShift shift = lsl;
2004   switch (code) {
2005     case lir_shl:  shift = lsl; break;
2006     case lir_shr:  shift = asr; break;
2007     case lir_ushr: shift = lsr; break;
2008     default: ShouldNotReachHere();
2009   }
2010 
2011   if (dest-&gt;is_single_cpu()) {
2012     __ andr(Rtemp, count-&gt;as_register(), 31);
2013     __ mov(dest-&gt;as_register(), AsmOperand(left-&gt;as_register(), shift, Rtemp));
2014   } else if (dest-&gt;is_double_cpu()) {
2015     Register dest_lo = dest-&gt;as_register_lo();
2016     Register dest_hi = dest-&gt;as_register_hi();
2017     Register src_lo  = left-&gt;as_register_lo();
2018     Register src_hi  = left-&gt;as_register_hi();
2019     Register Rcount  = count-&gt;as_register();
2020     // Resolve possible register conflicts
2021     if (shift == lsl &amp;&amp; dest_hi == src_lo) {
2022       dest_hi = Rtemp;
2023     } else if (shift != lsl &amp;&amp; dest_lo == src_hi) {
2024       dest_lo = Rtemp;
2025     } else if (dest_lo == src_lo &amp;&amp; dest_hi == src_hi) {
2026       dest_lo = Rtemp;
2027     } else if (dest_lo == Rcount || dest_hi == Rcount) {
2028       Rcount = Rtemp;
2029     }
2030     __ andr(Rcount, count-&gt;as_register(), 63);
2031     __ long_shift(dest_lo, dest_hi, src_lo, src_hi, shift, Rcount);
2032     move_regs(dest_lo, dest-&gt;as_register_lo());
2033     move_regs(dest_hi, dest-&gt;as_register_hi());
2034   } else {
2035     ShouldNotReachHere();
2036   }
2037 }
2038 
2039 
2040 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, jint count, LIR_Opr dest) {
2041   AsmShift shift = lsl;
2042   switch (code) {
2043     case lir_shl:  shift = lsl; break;
2044     case lir_shr:  shift = asr; break;
2045     case lir_ushr: shift = lsr; break;
2046     default: ShouldNotReachHere();
2047   }
2048 
2049   if (dest-&gt;is_single_cpu()) {
2050     count &amp;= 31;
2051     if (count != 0) {
2052       __ mov(dest-&gt;as_register(), AsmOperand(left-&gt;as_register(), shift, count));
2053     } else {
2054       move_regs(left-&gt;as_register(), dest-&gt;as_register());
2055     }
2056   } else if (dest-&gt;is_double_cpu()) {
2057     count &amp;= 63;
2058     if (count != 0) {
2059       Register dest_lo = dest-&gt;as_register_lo();
2060       Register dest_hi = dest-&gt;as_register_hi();
2061       Register src_lo  = left-&gt;as_register_lo();
2062       Register src_hi  = left-&gt;as_register_hi();
2063       // Resolve possible register conflicts
2064       if (shift == lsl &amp;&amp; dest_hi == src_lo) {
2065         dest_hi = Rtemp;
2066       } else if (shift != lsl &amp;&amp; dest_lo == src_hi) {
2067         dest_lo = Rtemp;
2068       }
2069       __ long_shift(dest_lo, dest_hi, src_lo, src_hi, shift, count);
2070       move_regs(dest_lo, dest-&gt;as_register_lo());
2071       move_regs(dest_hi, dest-&gt;as_register_hi());
2072     } else {
2073       __ long_move(dest-&gt;as_register_lo(), dest-&gt;as_register_hi(),
2074                    left-&gt;as_register_lo(), left-&gt;as_register_hi());
2075     }
2076   } else {
2077     ShouldNotReachHere();
2078   }
2079 }
2080 
2081 
2082 // Saves 4 given registers in reserved argument area.
2083 void LIR_Assembler::save_in_reserved_area(Register r1, Register r2, Register r3, Register r4) {
2084   verify_reserved_argument_area_size(4);
2085   __ stmia(SP, RegisterSet(r1) | RegisterSet(r2) | RegisterSet(r3) | RegisterSet(r4));
2086 }
2087 
2088 // Restores 4 given registers from reserved argument area.
2089 void LIR_Assembler::restore_from_reserved_area(Register r1, Register r2, Register r3, Register r4) {
2090   __ ldmia(SP, RegisterSet(r1) | RegisterSet(r2) | RegisterSet(r3) | RegisterSet(r4), no_writeback);
2091 }
2092 
2093 
2094 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
2095   ciArrayKlass* default_type = op-&gt;expected_type();
2096   Register src = op-&gt;src()-&gt;as_register();
2097   Register src_pos = op-&gt;src_pos()-&gt;as_register();
2098   Register dst = op-&gt;dst()-&gt;as_register();
2099   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
2100   Register length  = op-&gt;length()-&gt;as_register();
2101   Register tmp = op-&gt;tmp()-&gt;as_register();
2102   Register tmp2 = Rtemp;
2103 
2104   assert(src == R0 &amp;&amp; src_pos == R1 &amp;&amp; dst == R2 &amp;&amp; dst_pos == R3, &quot;code assumption&quot;);
2105 
2106   __ resolve(ACCESS_READ, src);
2107   __ resolve(ACCESS_WRITE, dst);
2108 
2109   CodeStub* stub = op-&gt;stub();
2110 
2111   int flags = op-&gt;flags();
2112   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
2113   if (basic_type == T_ARRAY) basic_type = T_OBJECT;
2114 
2115   // If we don&#39;t know anything or it&#39;s an object array, just go through the generic arraycopy
2116   if (default_type == NULL) {
2117 
2118     // save arguments, because they will be killed by a runtime call
2119     save_in_reserved_area(R0, R1, R2, R3);
2120 
2121     // pass length argument on SP[0]
2122     __ str(length, Address(SP, -2*wordSize, pre_indexed));  // 2 words for a proper stack alignment
2123 
2124     address copyfunc_addr = StubRoutines::generic_arraycopy();
2125     assert(copyfunc_addr != NULL, &quot;generic arraycopy stub required&quot;);
2126 #ifndef PRODUCT
2127     if (PrintC1Statistics) {
2128       __ inc_counter((address)&amp;Runtime1::_generic_arraycopystub_cnt, tmp, tmp2);
2129     }
2130 #endif // !PRODUCT
2131     // the stub is in the code cache so close enough
2132     __ call(copyfunc_addr, relocInfo::runtime_call_type);
2133 
2134     __ add(SP, SP, 2*wordSize);
2135 
2136     __ cbz_32(R0, *stub-&gt;continuation());
2137 
2138     __ mvn_32(tmp, R0);
2139     restore_from_reserved_area(R0, R1, R2, R3);  // load saved arguments in slow case only
2140     __ sub_32(length, length, tmp);
2141     __ add_32(src_pos, src_pos, tmp);
2142     __ add_32(dst_pos, dst_pos, tmp);
2143 
2144     __ b(*stub-&gt;entry());
2145 
2146     __ bind(*stub-&gt;continuation());
2147     return;
2148   }
2149 
2150   assert(default_type != NULL &amp;&amp; default_type-&gt;is_array_klass() &amp;&amp; default_type-&gt;is_loaded(),
2151          &quot;must be true at this point&quot;);
2152   int elem_size = type2aelembytes(basic_type);
2153   int shift = exact_log2(elem_size);
2154 
2155   // Check for NULL
2156   if (flags &amp; LIR_OpArrayCopy::src_null_check) {
2157     if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
2158       __ cmp(src, 0);
2159       __ cond_cmp(dst, 0, ne);  // make one instruction shorter if both checks are needed
2160       __ b(*stub-&gt;entry(), eq);
2161     } else {
2162       __ cbz(src, *stub-&gt;entry());
2163     }
2164   } else if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
2165     __ cbz(dst, *stub-&gt;entry());
2166   }
2167 
2168   // If the compiler was not able to prove that exact type of the source or the destination
2169   // of the arraycopy is an array type, check at runtime if the source or the destination is
2170   // an instance type.
2171   if (flags &amp; LIR_OpArrayCopy::type_check) {
2172     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::dst_objarray)) {
2173       __ load_klass(tmp, dst);
2174       __ ldr_u32(tmp2, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2175       __ mov_slow(tmp, Klass::_lh_neutral_value);
2176       __ cmp_32(tmp2, tmp);
2177       __ b(*stub-&gt;entry(), ge);
2178     }
2179 
2180     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::src_objarray)) {
2181       __ load_klass(tmp, src);
2182       __ ldr_u32(tmp2, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2183       __ mov_slow(tmp, Klass::_lh_neutral_value);
2184       __ cmp_32(tmp2, tmp);
2185       __ b(*stub-&gt;entry(), ge);
2186     }
2187   }
2188 
2189   // Check if negative
2190   const int all_positive_checks = LIR_OpArrayCopy::src_pos_positive_check |
2191                                   LIR_OpArrayCopy::dst_pos_positive_check |
2192                                   LIR_OpArrayCopy::length_positive_check;
2193   switch (flags &amp; all_positive_checks) {
2194     case LIR_OpArrayCopy::src_pos_positive_check:
2195       __ branch_if_negative_32(src_pos, *stub-&gt;entry());
2196       break;
2197     case LIR_OpArrayCopy::dst_pos_positive_check:
2198       __ branch_if_negative_32(dst_pos, *stub-&gt;entry());
2199       break;
2200     case LIR_OpArrayCopy::length_positive_check:
2201       __ branch_if_negative_32(length, *stub-&gt;entry());
2202       break;
2203     case LIR_OpArrayCopy::src_pos_positive_check | LIR_OpArrayCopy::dst_pos_positive_check:
2204       __ branch_if_any_negative_32(src_pos, dst_pos, tmp, *stub-&gt;entry());
2205       break;
2206     case LIR_OpArrayCopy::src_pos_positive_check | LIR_OpArrayCopy::length_positive_check:
2207       __ branch_if_any_negative_32(src_pos, length, tmp, *stub-&gt;entry());
2208       break;
2209     case LIR_OpArrayCopy::dst_pos_positive_check | LIR_OpArrayCopy::length_positive_check:
2210       __ branch_if_any_negative_32(dst_pos, length, tmp, *stub-&gt;entry());
2211       break;
2212     case all_positive_checks:
2213       __ branch_if_any_negative_32(src_pos, dst_pos, length, tmp, *stub-&gt;entry());
2214       break;
2215     default:
2216       assert((flags &amp; all_positive_checks) == 0, &quot;the last option&quot;);
2217   }
2218 
2219   // Range checks
2220   if (flags &amp; LIR_OpArrayCopy::src_range_check) {
2221     __ ldr_s32(tmp2, Address(src, arrayOopDesc::length_offset_in_bytes()));
2222     __ add_32(tmp, src_pos, length);
2223     __ cmp_32(tmp, tmp2);
2224     __ b(*stub-&gt;entry(), hi);
2225   }
2226   if (flags &amp; LIR_OpArrayCopy::dst_range_check) {
2227     __ ldr_s32(tmp2, Address(dst, arrayOopDesc::length_offset_in_bytes()));
2228     __ add_32(tmp, dst_pos, length);
2229     __ cmp_32(tmp, tmp2);
2230     __ b(*stub-&gt;entry(), hi);
2231   }
2232 
2233   // Check if src and dst are of the same type
2234   if (flags &amp; LIR_OpArrayCopy::type_check) {
2235     // We don&#39;t know the array types are compatible
2236     if (basic_type != T_OBJECT) {
2237       // Simple test for basic type arrays
2238       if (UseCompressedClassPointers) {
2239         // We don&#39;t need decode because we just need to compare
2240         __ ldr_u32(tmp, Address(src, oopDesc::klass_offset_in_bytes()));
2241         __ ldr_u32(tmp2, Address(dst, oopDesc::klass_offset_in_bytes()));
2242         __ cmp_32(tmp, tmp2);
2243       } else {
2244         __ load_klass(tmp, src);
2245         __ load_klass(tmp2, dst);
2246         __ cmp(tmp, tmp2);
2247       }
2248       __ b(*stub-&gt;entry(), ne);
2249     } else {
2250       // For object arrays, if src is a sub class of dst then we can
2251       // safely do the copy.
2252       Label cont, slow;
2253 
2254       address copyfunc_addr = StubRoutines::checkcast_arraycopy();
2255 
2256       __ load_klass(tmp, src);
2257       __ load_klass(tmp2, dst);
2258 
2259       // We are at a call so all live registers are saved before we
2260       // get here
2261       assert_different_registers(tmp, tmp2, R6, altFP_7_11);
2262 
2263       __ check_klass_subtype_fast_path(tmp, tmp2, R6, altFP_7_11, &amp;cont, copyfunc_addr == NULL ? stub-&gt;entry() : &amp;slow, NULL);
2264 
2265       __ mov(R6, R0);
2266       __ mov(altFP_7_11, R1);
2267       __ mov(R0, tmp);
2268       __ mov(R1, tmp2);
2269       __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type); // does not blow any registers except R0, LR and Rtemp
2270       __ cmp_32(R0, 0);
2271       __ mov(R0, R6);
2272       __ mov(R1, altFP_7_11);
2273 
2274       if (copyfunc_addr != NULL) { // use stub if available
2275         // src is not a sub class of dst so we have to do a
2276         // per-element check.
2277 
2278         __ b(cont, ne);
2279 
2280         __ bind(slow);
2281 
2282         int mask = LIR_OpArrayCopy::src_objarray|LIR_OpArrayCopy::dst_objarray;
2283         if ((flags &amp; mask) != mask) {
2284           // Check that at least both of them object arrays.
2285           assert(flags &amp; mask, &quot;one of the two should be known to be an object array&quot;);
2286 
2287           if (!(flags &amp; LIR_OpArrayCopy::src_objarray)) {
2288             __ load_klass(tmp, src);
2289           } else if (!(flags &amp; LIR_OpArrayCopy::dst_objarray)) {
2290             __ load_klass(tmp, dst);
2291           }
2292           int lh_offset = in_bytes(Klass::layout_helper_offset());
2293 
2294           __ ldr_u32(tmp2, Address(tmp, lh_offset));
2295 
2296           jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2297           __ mov_slow(tmp, objArray_lh);
2298           __ cmp_32(tmp, tmp2);
2299           __ b(*stub-&gt;entry(), ne);
2300         }
2301 
2302         save_in_reserved_area(R0, R1, R2, R3);
2303 
2304         Register src_ptr = R0;
2305         Register dst_ptr = R1;
2306         Register len     = R2;
2307         Register chk_off = R3;
2308         Register super_k = tmp;
2309 
2310         __ add(src_ptr, src, arrayOopDesc::base_offset_in_bytes(basic_type));
2311         __ add_ptr_scaled_int32(src_ptr, src_ptr, src_pos, shift);
2312 
2313         __ add(dst_ptr, dst, arrayOopDesc::base_offset_in_bytes(basic_type));
2314         __ add_ptr_scaled_int32(dst_ptr, dst_ptr, dst_pos, shift);
2315         __ load_klass(tmp, dst);
2316 
2317         int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
2318         int sco_offset = in_bytes(Klass::super_check_offset_offset());
2319 
2320         __ ldr(super_k, Address(tmp, ek_offset));
2321 
2322         __ mov(len, length);
2323         __ ldr_u32(chk_off, Address(super_k, sco_offset));
2324         __ push(super_k);
2325 
2326         __ call(copyfunc_addr, relocInfo::runtime_call_type);
2327 
2328 #ifndef PRODUCT
2329         if (PrintC1Statistics) {
2330           Label failed;
2331           __ cbnz_32(R0, failed);
2332           __ inc_counter((address)&amp;Runtime1::_arraycopy_checkcast_cnt, tmp, tmp2);
2333           __ bind(failed);
2334         }
2335 #endif // PRODUCT
2336 
2337         __ add(SP, SP, wordSize);  // Drop super_k argument
2338 
2339         __ cbz_32(R0, *stub-&gt;continuation());
2340         __ mvn_32(tmp, R0);
2341 
2342         // load saved arguments in slow case only
2343         restore_from_reserved_area(R0, R1, R2, R3);
2344 
2345         __ sub_32(length, length, tmp);
2346         __ add_32(src_pos, src_pos, tmp);
2347         __ add_32(dst_pos, dst_pos, tmp);
2348 
2349 #ifndef PRODUCT
2350         if (PrintC1Statistics) {
2351           __ inc_counter((address)&amp;Runtime1::_arraycopy_checkcast_attempt_cnt, tmp, tmp2);
2352         }
2353 #endif
2354 
2355         __ b(*stub-&gt;entry());
2356 
2357         __ bind(cont);
2358       } else {
2359         __ b(*stub-&gt;entry(), eq);
2360         __ bind(cont);
2361       }
2362     }
2363   }
2364 
2365 #ifndef PRODUCT
2366   if (PrintC1Statistics) {
2367     address counter = Runtime1::arraycopy_count_address(basic_type);
2368     __ inc_counter(counter, tmp, tmp2);
2369   }
2370 #endif // !PRODUCT
2371 
2372   bool disjoint = (flags &amp; LIR_OpArrayCopy::overlapping) == 0;
2373   bool aligned = (flags &amp; LIR_OpArrayCopy::unaligned) == 0;
2374   const char *name;
2375   address entry = StubRoutines::select_arraycopy_function(basic_type, aligned, disjoint, name, false);
2376 
2377   Register src_ptr = R0;
2378   Register dst_ptr = R1;
2379   Register len     = R2;
2380 
2381   __ add(src_ptr, src, arrayOopDesc::base_offset_in_bytes(basic_type));
2382   __ add_ptr_scaled_int32(src_ptr, src_ptr, src_pos, shift);
2383 
2384   __ add(dst_ptr, dst, arrayOopDesc::base_offset_in_bytes(basic_type));
2385   __ add_ptr_scaled_int32(dst_ptr, dst_ptr, dst_pos, shift);
2386 
2387   __ mov(len, length);
2388 
2389   __ call(entry, relocInfo::runtime_call_type);
2390 
2391   __ bind(*stub-&gt;continuation());
2392 }
2393 
2394 #ifdef ASSERT
2395  // emit run-time assertion
2396 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
2397   assert(op-&gt;code() == lir_assert, &quot;must be&quot;);
2398 
2399   if (op-&gt;in_opr1()-&gt;is_valid()) {
2400     assert(op-&gt;in_opr2()-&gt;is_valid(), &quot;both operands must be valid&quot;);
2401     comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
2402   } else {
2403     assert(op-&gt;in_opr2()-&gt;is_illegal(), &quot;both operands must be illegal&quot;);
2404     assert(op-&gt;condition() == lir_cond_always, &quot;no other conditions allowed&quot;);
2405   }
2406 
2407   Label ok;
2408   if (op-&gt;condition() != lir_cond_always) {
2409     AsmCondition acond = al;
2410     switch (op-&gt;condition()) {
2411       case lir_cond_equal:        acond = eq; break;
2412       case lir_cond_notEqual:     acond = ne; break;
2413       case lir_cond_less:         acond = lt; break;
2414       case lir_cond_lessEqual:    acond = le; break;
2415       case lir_cond_greaterEqual: acond = ge; break;
2416       case lir_cond_greater:      acond = gt; break;
2417       case lir_cond_aboveEqual:   acond = hs; break;
2418       case lir_cond_belowEqual:   acond = ls; break;
2419       default:                    ShouldNotReachHere();
2420     }
2421     __ b(ok, acond);
2422   }
2423   if (op-&gt;halt()) {
2424     const char* str = __ code_string(op-&gt;msg());
2425     __ stop(str);
2426   } else {
2427     breakpoint();
2428   }
2429   __ bind(ok);
2430 }
2431 #endif // ASSERT
2432 
2433 void LIR_Assembler::emit_updatecrc32(LIR_OpUpdateCRC32* op) {
2434   fatal(&quot;CRC32 intrinsic is not implemented on this platform&quot;);
2435 }
2436 
2437 void LIR_Assembler::emit_lock(LIR_OpLock* op) {
2438   Register obj = op-&gt;obj_opr()-&gt;as_pointer_register();
2439   Register hdr = op-&gt;hdr_opr()-&gt;as_pointer_register();
2440   Register lock = op-&gt;lock_opr()-&gt;as_pointer_register();
2441   Register tmp = op-&gt;scratch_opr()-&gt;is_illegal() ? noreg :
2442                  op-&gt;scratch_opr()-&gt;as_pointer_register();
2443 
2444   if (!UseFastLocking) {
2445     __ b(*op-&gt;stub()-&gt;entry());
2446   } else if (op-&gt;code() == lir_lock) {
2447     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
2448     __ resolve(ACCESS_READ | ACCESS_WRITE, obj);
2449     int null_check_offset = __ lock_object(hdr, obj, lock, tmp, *op-&gt;stub()-&gt;entry());
2450     if (op-&gt;info() != NULL) {
2451       add_debug_info_for_null_check(null_check_offset, op-&gt;info());
2452     }
2453   } else if (op-&gt;code() == lir_unlock) {
2454     __ unlock_object(hdr, obj, lock, tmp, *op-&gt;stub()-&gt;entry());
2455   } else {
2456     ShouldNotReachHere();
2457   }
2458   __ bind(*op-&gt;stub()-&gt;continuation());
2459 }
2460 
2461 
2462 void LIR_Assembler::emit_profile_call(LIR_OpProfileCall* op) {
2463   ciMethod* method = op-&gt;profiled_method();
2464   int bci          = op-&gt;profiled_bci();
2465   ciMethod* callee = op-&gt;profiled_callee();
2466 
2467   // Update counter for all call types
2468   ciMethodData* md = method-&gt;method_data_or_null();
2469   assert(md != NULL, &quot;Sanity&quot;);
2470   ciProfileData* data = md-&gt;bci_to_data(bci);
2471   assert(data != NULL &amp;&amp; data-&gt;is_CounterData(), &quot;need CounterData for calls&quot;);
2472   assert(op-&gt;mdo()-&gt;is_single_cpu(),  &quot;mdo must be allocated&quot;);
2473   Register mdo  = op-&gt;mdo()-&gt;as_register();
2474   assert(op-&gt;tmp1()-&gt;is_register(), &quot;tmp1 must be allocated&quot;);
2475   Register tmp1 = op-&gt;tmp1()-&gt;as_pointer_register();
2476   assert_different_registers(mdo, tmp1);
2477   __ mov_metadata(mdo, md-&gt;constant_encoding());
2478   int mdo_offset_bias = 0;
2479   int max_offset = 4096;
2480   if (md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) + data-&gt;size_in_bytes() &gt;= max_offset) {
2481     // The offset is large so bias the mdo by the base of the slot so
2482     // that the ldr can use an immediate offset to reference the slots of the data
2483     mdo_offset_bias = md-&gt;byte_offset_of_slot(data, CounterData::count_offset());
2484     __ mov_slow(tmp1, mdo_offset_bias);
2485     __ add(mdo, mdo, tmp1);
2486   }
2487 
2488   Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) - mdo_offset_bias);
2489   // Perform additional virtual call profiling for invokevirtual and
2490   // invokeinterface bytecodes
2491   if (op-&gt;should_profile_receiver_type()) {
2492     assert(op-&gt;recv()-&gt;is_single_cpu(), &quot;recv must be allocated&quot;);
2493     Register recv = op-&gt;recv()-&gt;as_register();
2494     assert_different_registers(mdo, tmp1, recv);
2495     assert(data-&gt;is_VirtualCallData(), &quot;need VirtualCallData for virtual calls&quot;);
2496     ciKlass* known_klass = op-&gt;known_holder();
2497     if (C1OptimizeVirtualCallProfiling &amp;&amp; known_klass != NULL) {
2498       // We know the type that will be seen at this call site; we can
2499       // statically update the MethodData* rather than needing to do
2500       // dynamic tests on the receiver type
2501 
2502       // NOTE: we should probably put a lock around this search to
2503       // avoid collisions by concurrent compilations
2504       ciVirtualCallData* vc_data = (ciVirtualCallData*) data;
2505       uint i;
2506       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2507         ciKlass* receiver = vc_data-&gt;receiver(i);
2508         if (known_klass-&gt;equals(receiver)) {
2509           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data,
2510                                                          VirtualCallData::receiver_count_offset(i)) -
2511                             mdo_offset_bias);
2512           __ ldr(tmp1, data_addr);
2513           __ add(tmp1, tmp1, DataLayout::counter_increment);
2514           __ str(tmp1, data_addr);
2515           return;
2516         }
2517       }
2518 
2519       // Receiver type not found in profile data; select an empty slot
2520 
2521       // Note that this is less efficient than it should be because it
2522       // always does a write to the receiver part of the
2523       // VirtualCallData rather than just the first time
2524       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2525         ciKlass* receiver = vc_data-&gt;receiver(i);
2526         if (receiver == NULL) {
2527           Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_offset(i)) -
2528                             mdo_offset_bias);
2529           __ mov_metadata(tmp1, known_klass-&gt;constant_encoding());
2530           __ str(tmp1, recv_addr);
2531           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)) -
2532                             mdo_offset_bias);
2533           __ ldr(tmp1, data_addr);
2534           __ add(tmp1, tmp1, DataLayout::counter_increment);
2535           __ str(tmp1, data_addr);
2536           return;
2537         }
2538       }
2539     } else {
2540       __ load_klass(recv, recv);
2541       Label update_done;
2542       type_profile_helper(mdo, mdo_offset_bias, md, data, recv, tmp1, &amp;update_done);
2543       // Receiver did not match any saved receiver and there is no empty row for it.
2544       // Increment total counter to indicate polymorphic case.
2545       __ ldr(tmp1, counter_addr);
2546       __ add(tmp1, tmp1, DataLayout::counter_increment);
2547       __ str(tmp1, counter_addr);
2548 
2549       __ bind(update_done);
2550     }
2551   } else {
2552     // Static call
2553     __ ldr(tmp1, counter_addr);
2554     __ add(tmp1, tmp1, DataLayout::counter_increment);
2555     __ str(tmp1, counter_addr);
2556   }
2557 }
2558 
2559 void LIR_Assembler::emit_profile_type(LIR_OpProfileType* op) {
2560   fatal(&quot;Type profiling not implemented on this platform&quot;);
2561 }
2562 
2563 void LIR_Assembler::emit_delay(LIR_OpDelay*) {
2564   Unimplemented();
2565 }
2566 
2567 
2568 void LIR_Assembler::monitor_address(int monitor_no, LIR_Opr dst) {
2569   Address mon_addr = frame_map()-&gt;address_for_monitor_lock(monitor_no);
2570   __ add_slow(dst-&gt;as_pointer_register(), mon_addr.base(), mon_addr.disp());
2571 }
2572 
2573 
2574 void LIR_Assembler::align_backward_branch_target() {
2575   // Some ARM processors do better with 8-byte branch target alignment
2576   __ align(8);
2577 }
2578 
2579 
2580 void LIR_Assembler::negate(LIR_Opr left, LIR_Opr dest, LIR_Opr tmp) {
2581   // tmp must be unused
2582   assert(tmp-&gt;is_illegal(), &quot;wasting a register if tmp is allocated&quot;);
2583 
2584   if (left-&gt;is_single_cpu()) {
2585     assert (dest-&gt;type() == T_INT, &quot;unexpected result type&quot;);
2586     assert (left-&gt;type() == T_INT, &quot;unexpected left type&quot;);
2587     __ neg_32(dest-&gt;as_register(), left-&gt;as_register());
2588   } else if (left-&gt;is_double_cpu()) {
2589     Register dest_lo = dest-&gt;as_register_lo();
2590     Register dest_hi = dest-&gt;as_register_hi();
2591     Register src_lo = left-&gt;as_register_lo();
2592     Register src_hi = left-&gt;as_register_hi();
2593     if (dest_lo == src_hi) {
2594       dest_lo = Rtemp;
2595     }
2596     __ rsbs(dest_lo, src_lo, 0);
2597     __ rsc(dest_hi, src_hi, 0);
2598     move_regs(dest_lo, dest-&gt;as_register_lo());
2599   } else if (left-&gt;is_single_fpu()) {
2600     __ neg_float(dest-&gt;as_float_reg(), left-&gt;as_float_reg());
2601   } else if (left-&gt;is_double_fpu()) {
2602     __ neg_double(dest-&gt;as_double_reg(), left-&gt;as_double_reg());
2603   } else {
2604     ShouldNotReachHere();
2605   }
2606 }
2607 
2608 
2609 void LIR_Assembler::leal(LIR_Opr addr_opr, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
2610   assert(patch_code == lir_patch_none, &quot;Patch code not supported&quot;);
2611   LIR_Address* addr = addr_opr-&gt;as_address_ptr();
2612   if (addr-&gt;index()-&gt;is_illegal()) {
2613     jint c = addr-&gt;disp();
2614     if (!Assembler::is_arith_imm_in_range(c)) {
2615       BAILOUT(&quot;illegal arithmetic operand&quot;);
2616     }
2617     __ add(dest-&gt;as_pointer_register(), addr-&gt;base()-&gt;as_pointer_register(), c);
2618   } else {
2619     assert(addr-&gt;disp() == 0, &quot;cannot handle otherwise&quot;);
2620     __ add(dest-&gt;as_pointer_register(), addr-&gt;base()-&gt;as_pointer_register(),
2621            AsmOperand(addr-&gt;index()-&gt;as_pointer_register(), lsl, addr-&gt;scale()));
2622   }
2623 }
2624 
2625 
2626 void LIR_Assembler::rt_call(LIR_Opr result, address dest, const LIR_OprList* args, LIR_Opr tmp, CodeEmitInfo* info) {
2627   assert(!tmp-&gt;is_valid(), &quot;don&#39;t need temporary&quot;);
2628   __ call(dest);
2629   if (info != NULL) {
2630     add_call_info_here(info);
2631   }
2632 }
2633 
2634 
2635 void LIR_Assembler::volatile_move_op(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info) {
2636   assert(src-&gt;is_double_cpu() &amp;&amp; dest-&gt;is_address() ||
2637          src-&gt;is_address() &amp;&amp; dest-&gt;is_double_cpu(),
2638          &quot;Simple move_op is called for all other cases&quot;);
2639 
2640   int null_check_offset;
2641   if (dest-&gt;is_address()) {
2642     // Store
2643     const LIR_Address* addr = dest-&gt;as_address_ptr();
2644     const Register src_lo = src-&gt;as_register_lo();
2645     const Register src_hi = src-&gt;as_register_hi();
2646     assert(addr-&gt;index()-&gt;is_illegal() &amp;&amp; addr-&gt;disp() == 0, &quot;The address is simple already&quot;);
2647 
2648     if (src_lo &lt; src_hi) {
2649       null_check_offset = __ offset();
2650       __ stmia(addr-&gt;base()-&gt;as_register(), RegisterSet(src_lo) | RegisterSet(src_hi));
2651     } else {
2652       assert(src_lo &lt; Rtemp, &quot;Rtemp is higher than any allocatable register&quot;);
2653       __ mov(Rtemp, src_hi);
2654       null_check_offset = __ offset();
2655       __ stmia(addr-&gt;base()-&gt;as_register(), RegisterSet(src_lo) | RegisterSet(Rtemp));
2656     }
2657   } else {
2658     // Load
2659     const LIR_Address* addr = src-&gt;as_address_ptr();
2660     const Register dest_lo = dest-&gt;as_register_lo();
2661     const Register dest_hi = dest-&gt;as_register_hi();
2662     assert(addr-&gt;index()-&gt;is_illegal() &amp;&amp; addr-&gt;disp() == 0, &quot;The address is simple already&quot;);
2663 
2664     null_check_offset = __ offset();
2665     if (dest_lo &lt; dest_hi) {
2666       __ ldmia(addr-&gt;base()-&gt;as_register(), RegisterSet(dest_lo) | RegisterSet(dest_hi));
2667     } else {
2668       assert(dest_lo &lt; Rtemp, &quot;Rtemp is higher than any allocatable register&quot;);
2669       __ ldmia(addr-&gt;base()-&gt;as_register(), RegisterSet(dest_lo) | RegisterSet(Rtemp));
2670       __ mov(dest_hi, Rtemp);
2671     }
2672   }
2673 
2674   if (info != NULL) {
2675     add_debug_info_for_null_check(null_check_offset, info);
2676   }
2677 }
2678 
2679 
2680 void LIR_Assembler::membar() {
2681   __ membar(MacroAssembler::StoreLoad, Rtemp);
2682 }
2683 
2684 void LIR_Assembler::membar_acquire() {
2685   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);
2686 }
2687 
2688 void LIR_Assembler::membar_release() {
2689   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
2690 }
2691 
2692 void LIR_Assembler::membar_loadload() {
2693   __ membar(MacroAssembler::LoadLoad, Rtemp);
2694 }
2695 
2696 void LIR_Assembler::membar_storestore() {
2697   __ membar(MacroAssembler::StoreStore, Rtemp);
2698 }
2699 
2700 void LIR_Assembler::membar_loadstore() {
2701   __ membar(MacroAssembler::LoadStore, Rtemp);
2702 }
2703 
2704 void LIR_Assembler::membar_storeload() {
2705   __ membar(MacroAssembler::StoreLoad, Rtemp);
2706 }
2707 
2708 void LIR_Assembler::on_spin_wait() {
2709   Unimplemented();
2710 }
2711 
2712 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
2713   // Not used on ARM
2714   Unimplemented();
2715 }
2716 
2717 void LIR_Assembler::peephole(LIR_List* lir) {
2718   LIR_OpList* inst = lir-&gt;instructions_list();
2719   const int inst_length = inst-&gt;length();
2720   for (int i = 0; i &lt; inst_length; i++) {
2721     LIR_Op* op = inst-&gt;at(i);
2722     switch (op-&gt;code()) {
2723       case lir_cmp: {
2724         // Replace:
2725         //   cmp rX, y
2726         //   cmove [EQ] y, z, rX
2727         // with
2728         //   cmp rX, y
2729         //   cmove [EQ] illegalOpr, z, rX
2730         //
2731         // or
2732         //   cmp rX, y
2733         //   cmove [NE] z, y, rX
2734         // with
2735         //   cmp rX, y
2736         //   cmove [NE] z, illegalOpr, rX
2737         //
2738         // moves from illegalOpr should be removed when converting LIR to native assembly
2739 
2740         LIR_Op2* cmp = op-&gt;as_Op2();
2741         assert(cmp != NULL, &quot;cmp LIR instruction is not an op2&quot;);
2742 
2743         if (i + 1 &lt; inst_length) {
2744           LIR_Op2* cmove = inst-&gt;at(i + 1)-&gt;as_Op2();
2745           if (cmove != NULL &amp;&amp; cmove-&gt;code() == lir_cmove) {
2746             LIR_Opr cmove_res = cmove-&gt;result_opr();
2747             bool res_is_op1 = cmove_res == cmp-&gt;in_opr1();
2748             bool res_is_op2 = cmove_res == cmp-&gt;in_opr2();
2749             LIR_Opr cmp_res, cmp_arg;
2750             if (res_is_op1) {
2751               cmp_res = cmp-&gt;in_opr1();
2752               cmp_arg = cmp-&gt;in_opr2();
2753             } else if (res_is_op2) {
2754               cmp_res = cmp-&gt;in_opr2();
2755               cmp_arg = cmp-&gt;in_opr1();
2756             } else {
2757               cmp_res = LIR_OprFact::illegalOpr;
2758               cmp_arg = LIR_OprFact::illegalOpr;
2759             }
2760 
2761             if (cmp_res != LIR_OprFact::illegalOpr) {
2762               LIR_Condition cond = cmove-&gt;condition();
2763               if (cond == lir_cond_equal &amp;&amp; cmove-&gt;in_opr1() == cmp_arg) {
2764                 cmove-&gt;set_in_opr1(LIR_OprFact::illegalOpr);
2765               } else if (cond == lir_cond_notEqual &amp;&amp; cmove-&gt;in_opr2() == cmp_arg) {
2766                 cmove-&gt;set_in_opr2(LIR_OprFact::illegalOpr);
2767               }
2768             }
2769           }
2770         }
2771         break;
2772       }
2773 
2774       default:
2775         break;
2776     }
2777   }
2778 }
2779 
2780 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp) {
2781   assert(src-&gt;is_address(), &quot;sanity&quot;);
2782   Address addr = as_Address(src-&gt;as_address_ptr());
2783 
2784   if (code == lir_xchg) {
2785   } else {
2786     assert (!data-&gt;is_oop(), &quot;xadd for oops&quot;);
2787   }
2788 
2789   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
2790 
2791   Label retry;
2792   __ bind(retry);
2793 
2794   if (data-&gt;type() == T_INT || data-&gt;is_oop()) {
2795     Register dst = dest-&gt;as_register();
2796     Register new_val = noreg;
2797     __ ldrex(dst, addr);
2798     if (code == lir_xadd) {
2799       Register tmp_reg = tmp-&gt;as_register();
2800       if (data-&gt;is_constant()) {
2801         assert_different_registers(dst, tmp_reg);
2802         __ add_32(tmp_reg, dst, data-&gt;as_constant_ptr()-&gt;as_jint());
2803       } else {
2804         assert_different_registers(dst, tmp_reg, data-&gt;as_register());
2805         __ add_32(tmp_reg, dst, data-&gt;as_register());
2806       }
2807       new_val = tmp_reg;
2808     } else {
2809       if (UseCompressedOops &amp;&amp; data-&gt;is_oop()) {
2810         new_val = tmp-&gt;as_pointer_register();
2811       } else {
2812         new_val = data-&gt;as_register();
2813       }
2814       assert_different_registers(dst, new_val);
2815     }
2816     __ strex(Rtemp, new_val, addr);
2817 
2818   } else if (data-&gt;type() == T_LONG) {
2819     Register dst_lo = dest-&gt;as_register_lo();
2820     Register new_val_lo = noreg;
2821     Register dst_hi = dest-&gt;as_register_hi();
2822 
2823     assert(dst_hi-&gt;encoding() == dst_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
2824     assert((dst_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
2825 
2826     __ bind(retry);
2827     __ ldrexd(dst_lo, addr);
2828     if (code == lir_xadd) {
2829       Register tmp_lo = tmp-&gt;as_register_lo();
2830       Register tmp_hi = tmp-&gt;as_register_hi();
2831 
2832       assert(tmp_hi-&gt;encoding() == tmp_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
2833       assert((tmp_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
2834 
2835       if (data-&gt;is_constant()) {
2836         jlong c = data-&gt;as_constant_ptr()-&gt;as_jlong();
2837         assert((jlong)((jint)c) == c, &quot;overflow&quot;);
2838         assert_different_registers(dst_lo, dst_hi, tmp_lo, tmp_hi);
2839         __ adds(tmp_lo, dst_lo, (jint)c);
2840         __ adc(tmp_hi, dst_hi, 0);
2841       } else {
2842         Register new_val_lo = data-&gt;as_register_lo();
2843         Register new_val_hi = data-&gt;as_register_hi();
2844         __ adds(tmp_lo, dst_lo, new_val_lo);
2845         __ adc(tmp_hi, dst_hi, new_val_hi);
2846         assert_different_registers(dst_lo, dst_hi, tmp_lo, tmp_hi, new_val_lo, new_val_hi);
2847       }
2848       new_val_lo = tmp_lo;
2849     } else {
2850       new_val_lo = data-&gt;as_register_lo();
2851       Register new_val_hi = data-&gt;as_register_hi();
2852 
2853       assert_different_registers(dst_lo, dst_hi, new_val_lo, new_val_hi);
2854       assert(new_val_hi-&gt;encoding() == new_val_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
2855       assert((new_val_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
2856     }
2857     __ strexd(Rtemp, new_val_lo, addr);
2858   } else {
2859     ShouldNotReachHere();
2860   }
2861 
2862   __ cbnz_32(Rtemp, retry);
2863   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreLoad | MacroAssembler::StoreStore), Rtemp);
2864 
2865 }
2866 
2867 #undef __
<a name="6" id="anc6"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="6" type="hidden" />
</body>
</html>