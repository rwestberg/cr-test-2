<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/cpu/arm/c1_LIRAssembler_arm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2008, 2019, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.inline.hpp&quot;
  27 #include &quot;c1/c1_Compilation.hpp&quot;
  28 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  29 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  30 #include &quot;c1/c1_Runtime1.hpp&quot;
  31 #include &quot;c1/c1_ValueStack.hpp&quot;
  32 #include &quot;ci/ciArrayKlass.hpp&quot;
  33 #include &quot;ci/ciInstance.hpp&quot;
  34 #include &quot;gc/shared/collectedHeap.hpp&quot;
  35 #include &quot;memory/universe.hpp&quot;
  36 #include &quot;nativeInst_arm.hpp&quot;
  37 #include &quot;oops/objArrayKlass.hpp&quot;
  38 #include &quot;runtime/frame.inline.hpp&quot;
  39 #include &quot;runtime/sharedRuntime.hpp&quot;
  40 #include &quot;vmreg_arm.inline.hpp&quot;
  41 
  42 #define __ _masm-&gt;
  43 
  44 // Note: Rtemp usage is this file should not impact C2 and should be
  45 // correct as long as it is not implicitly used in lower layers (the
  46 // arm [macro]assembler) and used with care in the other C1 specific
  47 // files.
  48 
  49 bool LIR_Assembler::is_small_constant(LIR_Opr opr) {
  50   ShouldNotCallThis(); // Not used on ARM
  51   return false;
  52 }
  53 
  54 
  55 LIR_Opr LIR_Assembler::receiverOpr() {
  56   // The first register in Java calling conventions
  57   return FrameMap::R0_oop_opr;
  58 }
  59 
  60 LIR_Opr LIR_Assembler::osrBufferPointer() {
  61   return FrameMap::as_pointer_opr(R0);
  62 }
  63 
  64 #ifndef PRODUCT
  65 void LIR_Assembler::verify_reserved_argument_area_size(int args_count) {
  66   assert(args_count * wordSize &lt;= frame_map()-&gt;reserved_argument_area_size(), &quot;not enough space for arguments&quot;);
  67 }
  68 #endif // !PRODUCT
  69 
  70 void LIR_Assembler::store_parameter(jint c, int offset_from_sp_in_words) {
  71   assert(offset_from_sp_in_words &gt;= 0, &quot;invalid offset from sp&quot;);
  72   int offset_from_sp_in_bytes = offset_from_sp_in_words * BytesPerWord;
  73   assert(offset_from_sp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;not enough space&quot;);
  74   __ mov_slow(Rtemp, c);
  75   __ str(Rtemp, Address(SP, offset_from_sp_in_bytes));
  76 }
  77 
  78 void LIR_Assembler::store_parameter(Metadata* m, int offset_from_sp_in_words) {
  79   assert(offset_from_sp_in_words &gt;= 0, &quot;invalid offset from sp&quot;);
  80   int offset_from_sp_in_bytes = offset_from_sp_in_words * BytesPerWord;
  81   assert(offset_from_sp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;not enough space&quot;);
  82   __ mov_metadata(Rtemp, m);
  83   __ str(Rtemp, Address(SP, offset_from_sp_in_bytes));
  84 }
  85 
  86 //--------------fpu register translations-----------------------
  87 
  88 
  89 void LIR_Assembler::breakpoint() {
  90   __ breakpoint();
  91 }
  92 
  93 void LIR_Assembler::push(LIR_Opr opr) {
  94   Unimplemented();
  95 }
  96 
  97 void LIR_Assembler::pop(LIR_Opr opr) {
  98   Unimplemented();
  99 }
 100 
 101 //-------------------------------------------
 102 Address LIR_Assembler::as_Address(LIR_Address* addr) {
 103   Register base = addr-&gt;base()-&gt;as_pointer_register();
 104 
 105 
 106   if (addr-&gt;index()-&gt;is_illegal() || addr-&gt;index()-&gt;is_constant()) {
 107     int offset = addr-&gt;disp();
 108     if (addr-&gt;index()-&gt;is_constant()) {
 109       offset += addr-&gt;index()-&gt;as_constant_ptr()-&gt;as_jint() &lt;&lt; addr-&gt;scale();
 110     }
 111 
 112     if ((offset &lt;= -4096) || (offset &gt;= 4096)) {
 113       BAILOUT_(&quot;offset not in range&quot;, Address(base));
 114     }
 115 
 116     return Address(base, offset);
 117 
 118   } else {
 119     assert(addr-&gt;disp() == 0, &quot;can&#39;t have both&quot;);
 120     int scale = addr-&gt;scale();
 121 
 122     assert(addr-&gt;index()-&gt;is_single_cpu(), &quot;should be&quot;);
 123     return scale &gt;= 0 ? Address(base, addr-&gt;index()-&gt;as_register(), lsl, scale) :
 124                         Address(base, addr-&gt;index()-&gt;as_register(), lsr, -scale);
 125   }
 126 }
 127 
 128 Address LIR_Assembler::as_Address_hi(LIR_Address* addr) {
 129   Address base = as_Address(addr);
 130   assert(base.index() == noreg, &quot;must be&quot;);
 131   if (base.disp() + BytesPerWord &gt;= 4096) { BAILOUT_(&quot;offset not in range&quot;, Address(base.base(),0)); }
 132   return Address(base.base(), base.disp() + BytesPerWord);
 133 }
 134 
 135 Address LIR_Assembler::as_Address_lo(LIR_Address* addr) {
 136   return as_Address(addr);
 137 }
 138 
 139 
 140 void LIR_Assembler::osr_entry() {
 141   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, code_offset());
 142   BlockBegin* osr_entry = compilation()-&gt;hir()-&gt;osr_entry();
 143   ValueStack* entry_state = osr_entry-&gt;end()-&gt;state();
 144   int number_of_locks = entry_state-&gt;locks_size();
 145 
 146   __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());
 147   Register OSR_buf = osrBufferPointer()-&gt;as_pointer_register();
 148 
 149   assert(frame::interpreter_frame_monitor_size() == BasicObjectLock::size(), &quot;adjust code below&quot;);
 150   int monitor_offset = (method()-&gt;max_locals() + 2 * (number_of_locks - 1)) * BytesPerWord;
 151   for (int i = 0; i &lt; number_of_locks; i++) {
 152     int slot_offset = monitor_offset - (i * 2 * BytesPerWord);
 153     __ ldr(R1, Address(OSR_buf, slot_offset + 0*BytesPerWord));
 154     __ ldr(R2, Address(OSR_buf, slot_offset + 1*BytesPerWord));
 155     __ str(R1, frame_map()-&gt;address_for_monitor_lock(i));
 156     __ str(R2, frame_map()-&gt;address_for_monitor_object(i));
 157   }
 158 }
 159 
 160 
 161 int LIR_Assembler::check_icache() {
 162   Register receiver = LIR_Assembler::receiverOpr()-&gt;as_register();
 163   int offset = __ offset();
 164   __ inline_cache_check(receiver, Ricklass);
 165   return offset;
 166 }
 167 
 168 void LIR_Assembler::clinit_barrier(ciMethod* method) {
 169   ShouldNotReachHere(); // not implemented
 170 }
 171 
 172 void LIR_Assembler::jobject2reg_with_patching(Register reg, CodeEmitInfo* info) {
 173   jobject o = (jobject)Universe::non_oop_word();
 174   int index = __ oop_recorder()-&gt;allocate_oop_index(o);
 175 
 176   PatchingStub* patch = new PatchingStub(_masm, patching_id(info), index);
 177 
 178   __ patchable_mov_oop(reg, o, index);
 179   patching_epilog(patch, lir_patch_normal, reg, info);
 180 }
 181 
 182 
 183 void LIR_Assembler::klass2reg_with_patching(Register reg, CodeEmitInfo* info) {
 184   Metadata* o = (Metadata*)Universe::non_oop_word();
 185   int index = __ oop_recorder()-&gt;allocate_metadata_index(o);
 186   PatchingStub* patch = new PatchingStub(_masm, PatchingStub::load_klass_id, index);
 187 
 188   __ patchable_mov_metadata(reg, o, index);
 189   patching_epilog(patch, lir_patch_normal, reg, info);
 190 }
 191 
 192 
 193 int LIR_Assembler::initial_frame_size_in_bytes() const {
 194   // Subtracts two words to account for return address and link
 195   return frame_map()-&gt;framesize()*VMRegImpl::stack_slot_size - 2*wordSize;
 196 }
 197 
 198 
 199 int LIR_Assembler::emit_exception_handler() {
 200   // TODO: ARM
 201   __ nop(); // See comments in other ports
 202 
 203   address handler_base = __ start_a_stub(exception_handler_size());
 204   if (handler_base == NULL) {
 205     bailout(&quot;exception handler overflow&quot;);
 206     return -1;
 207   }
 208 
 209   int offset = code_offset();
 210 
 211   // check that there is really an exception
 212   __ verify_not_null_oop(Rexception_obj);
 213 
 214   __ call(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id), relocInfo::runtime_call_type);
 215   __ should_not_reach_here();
 216 
 217   assert(code_offset() - offset &lt;= exception_handler_size(), &quot;overflow&quot;);
 218   __ end_a_stub();
 219 
 220   return offset;
 221 }
 222 
 223 // Emit the code to remove the frame from the stack in the exception
 224 // unwind path.
 225 int LIR_Assembler::emit_unwind_handler() {
 226 #ifndef PRODUCT
 227   if (CommentedAssembly) {
 228     _masm-&gt;block_comment(&quot;Unwind handler&quot;);
 229   }
 230 #endif
 231 
 232   int offset = code_offset();
 233 
 234   // Fetch the exception from TLS and clear out exception related thread state
 235   Register zero = __ zero_register(Rtemp);
 236   __ ldr(Rexception_obj, Address(Rthread, JavaThread::exception_oop_offset()));
 237   __ str(zero, Address(Rthread, JavaThread::exception_oop_offset()));
 238   __ str(zero, Address(Rthread, JavaThread::exception_pc_offset()));
 239 
 240   __ bind(_unwind_handler_entry);
 241   __ verify_not_null_oop(Rexception_obj);
 242 
 243   // Preform needed unlocking
 244   MonitorExitStub* stub = NULL;
 245   if (method()-&gt;is_synchronized()) {
 246     monitor_address(0, FrameMap::R0_opr);
 247     stub = new MonitorExitStub(FrameMap::R0_opr, true, 0);
 248     __ unlock_object(R2, R1, R0, Rtemp, *stub-&gt;entry());
 249     __ bind(*stub-&gt;continuation());
 250   }
 251 
 252   // remove the activation and dispatch to the unwind handler
 253   __ remove_frame(initial_frame_size_in_bytes()); // restores FP and LR
 254   __ jump(Runtime1::entry_for(Runtime1::unwind_exception_id), relocInfo::runtime_call_type, Rtemp);
 255 
 256   // Emit the slow path assembly
 257   if (stub != NULL) {
 258     stub-&gt;emit_code(this);
 259   }
 260 
 261   return offset;
 262 }
 263 
 264 
 265 int LIR_Assembler::emit_deopt_handler() {
 266   address handler_base = __ start_a_stub(deopt_handler_size());
 267   if (handler_base == NULL) {
 268     bailout(&quot;deopt handler overflow&quot;);
 269     return -1;
 270   }
 271 
 272   int offset = code_offset();
 273 
 274   __ mov_relative_address(LR, __ pc());
 275   __ push(LR); // stub expects LR to be saved
 276   __ jump(SharedRuntime::deopt_blob()-&gt;unpack(), relocInfo::runtime_call_type, noreg);
 277 
 278   assert(code_offset() - offset &lt;= deopt_handler_size(), &quot;overflow&quot;);
 279   __ end_a_stub();
 280 
 281   return offset;
 282 }
 283 
 284 
 285 void LIR_Assembler::return_op(LIR_Opr result) {
 286   // Pop the frame before safepoint polling
 287   __ remove_frame(initial_frame_size_in_bytes());
 288 
 289   // mov_slow here is usually one or two instruction
 290   __ mov_address(Rtemp, os::get_polling_page());
 291   __ relocate(relocInfo::poll_return_type);
 292   __ ldr(Rtemp, Address(Rtemp));
 293   __ ret();
 294 }
 295 
 296 
 297 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
 298   __ mov_address(Rtemp, os::get_polling_page());
 299   if (info != NULL) {
 300     add_debug_info_for_branch(info);
 301   }
 302   int offset = __ offset();
 303   __ relocate(relocInfo::poll_type);
 304   __ ldr(Rtemp, Address(Rtemp));
 305   return offset;
 306 }
 307 
 308 
 309 void LIR_Assembler::move_regs(Register from_reg, Register to_reg) {
 310   if (from_reg != to_reg) {
 311     __ mov(to_reg, from_reg);
 312   }
 313 }
 314 
 315 void LIR_Assembler::const2reg(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
 316   assert(src-&gt;is_constant() &amp;&amp; dest-&gt;is_register(), &quot;must be&quot;);
 317   LIR_Const* c = src-&gt;as_constant_ptr();
 318 
 319   switch (c-&gt;type()) {
 320     case T_ADDRESS:
 321     case T_INT:
 322       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 323       __ mov_slow(dest-&gt;as_register(), c-&gt;as_jint());
 324       break;
 325 
 326     case T_LONG:
 327       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 328       __ mov_slow(dest-&gt;as_register_lo(), c-&gt;as_jint_lo());
 329       __ mov_slow(dest-&gt;as_register_hi(), c-&gt;as_jint_hi());
 330       break;
 331 
 332     case T_OBJECT:
 333       if (patch_code == lir_patch_none) {
 334         __ mov_oop(dest-&gt;as_register(), c-&gt;as_jobject());
 335       } else {
 336         jobject2reg_with_patching(dest-&gt;as_register(), info);
 337       }
 338       break;
 339 
 340     case T_METADATA:
 341       if (patch_code == lir_patch_none) {
 342         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 343       } else {
 344         klass2reg_with_patching(dest-&gt;as_register(), info);
 345       }
 346       break;
 347 
 348     case T_FLOAT:
 349       if (dest-&gt;is_single_fpu()) {
 350         __ mov_float(dest-&gt;as_float_reg(), c-&gt;as_jfloat());
 351       } else {
 352         // Simple getters can return float constant directly into r0
 353         __ mov_slow(dest-&gt;as_register(), c-&gt;as_jint_bits());
 354       }
 355       break;
 356 
 357     case T_DOUBLE:
 358       if (dest-&gt;is_double_fpu()) {
 359         __ mov_double(dest-&gt;as_double_reg(), c-&gt;as_jdouble());
 360       } else {
 361         // Simple getters can return double constant directly into r1r0
 362         __ mov_slow(dest-&gt;as_register_lo(), c-&gt;as_jint_lo_bits());
 363         __ mov_slow(dest-&gt;as_register_hi(), c-&gt;as_jint_hi_bits());
 364       }
 365       break;
 366 
 367     default:
 368       ShouldNotReachHere();
 369   }
 370 }
 371 
 372 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 373   assert(src-&gt;is_constant(), &quot;must be&quot;);
 374   assert(dest-&gt;is_stack(), &quot;must be&quot;);
 375   LIR_Const* c = src-&gt;as_constant_ptr();
 376 
 377   switch (c-&gt;type()) {
 378     case T_INT:  // fall through
 379     case T_FLOAT:
 380       __ mov_slow(Rtemp, c-&gt;as_jint_bits());
 381       __ str_32(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 382       break;
 383 
 384     case T_ADDRESS:
 385       __ mov_slow(Rtemp, c-&gt;as_jint());
 386       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 387       break;
 388 
 389     case T_OBJECT:
 390       __ mov_oop(Rtemp, c-&gt;as_jobject());
 391       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 392       break;
 393 
 394     case T_LONG:  // fall through
 395     case T_DOUBLE:
 396       __ mov_slow(Rtemp, c-&gt;as_jint_lo_bits());
 397       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes));
 398       if (c-&gt;as_jint_hi_bits() != c-&gt;as_jint_lo_bits()) {
 399         __ mov_slow(Rtemp, c-&gt;as_jint_hi_bits());
 400       }
 401       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 402       break;
 403 
 404     default:
 405       ShouldNotReachHere();
 406   }
 407 }
 408 
 409 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type,
 410                               CodeEmitInfo* info, bool wide) {
 411   assert((src-&gt;as_constant_ptr()-&gt;type() == T_OBJECT &amp;&amp; src-&gt;as_constant_ptr()-&gt;as_jobject() == NULL),&quot;cannot handle otherwise&quot;);
 412   __ mov(Rtemp, 0);
 413 
 414   int null_check_offset = code_offset();
 415   __ str(Rtemp, as_Address(dest-&gt;as_address_ptr()));
 416 
 417   if (info != NULL) {
 418     assert(false, &quot;arm32 didn&#39;t support this before, investigate if bug&quot;);
 419     add_debug_info_for_null_check(null_check_offset, info);
 420   }
 421 }
 422 
 423 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 424   assert(src-&gt;is_register() &amp;&amp; dest-&gt;is_register(), &quot;must be&quot;);
 425 
 426   if (src-&gt;is_single_cpu()) {
 427     if (dest-&gt;is_single_cpu()) {
 428       move_regs(src-&gt;as_register(), dest-&gt;as_register());
 429     } else if (dest-&gt;is_single_fpu()) {
 430       __ fmsr(dest-&gt;as_float_reg(), src-&gt;as_register());
 431     } else {
 432       ShouldNotReachHere();
 433     }
 434   } else if (src-&gt;is_double_cpu()) {
 435     if (dest-&gt;is_double_cpu()) {
 436       __ long_move(dest-&gt;as_register_lo(), dest-&gt;as_register_hi(), src-&gt;as_register_lo(), src-&gt;as_register_hi());
 437     } else {
 438       __ fmdrr(dest-&gt;as_double_reg(), src-&gt;as_register_lo(), src-&gt;as_register_hi());
 439     }
 440   } else if (src-&gt;is_single_fpu()) {
 441     if (dest-&gt;is_single_fpu()) {
 442       __ mov_float(dest-&gt;as_float_reg(), src-&gt;as_float_reg());
 443     } else if (dest-&gt;is_single_cpu()) {
 444       __ mov_fpr2gpr_float(dest-&gt;as_register(), src-&gt;as_float_reg());
 445     } else {
 446       ShouldNotReachHere();
 447     }
 448   } else if (src-&gt;is_double_fpu()) {
 449     if (dest-&gt;is_double_fpu()) {
 450       __ mov_double(dest-&gt;as_double_reg(), src-&gt;as_double_reg());
 451     } else if (dest-&gt;is_double_cpu()) {
 452       __ fmrrd(dest-&gt;as_register_lo(), dest-&gt;as_register_hi(), src-&gt;as_double_reg());
 453     } else {
 454       ShouldNotReachHere();
 455     }
 456   } else {
 457     ShouldNotReachHere();
 458   }
 459 }
 460 
 461 void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
 462   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 463   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 464 
 465   Address addr = dest-&gt;is_single_word() ?
 466     frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()) :
 467     frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 468 
 469   assert(lo_word_offset_in_bytes == 0 &amp;&amp; hi_word_offset_in_bytes == 4, &quot;little ending&quot;);
 470   if (src-&gt;is_single_fpu() || src-&gt;is_double_fpu()) {
 471     if (addr.disp() &gt;= 1024) { BAILOUT(&quot;Too exotic case to handle here&quot;); }
 472   }
 473 
 474   if (src-&gt;is_single_cpu()) {
 475     switch (type) {
 476       case T_OBJECT:
 477       case T_ARRAY:    __ verify_oop(src-&gt;as_register());   // fall through
 478       case T_ADDRESS:
 479       case T_METADATA: __ str(src-&gt;as_register(), addr);    break;
 480       case T_FLOAT:    // used in intBitsToFloat intrinsic implementation, fall through
 481       case T_INT:      __ str_32(src-&gt;as_register(), addr); break;
 482       default:
 483         ShouldNotReachHere();
 484     }
 485   } else if (src-&gt;is_double_cpu()) {
 486     __ str(src-&gt;as_register_lo(), addr);
 487     __ str(src-&gt;as_register_hi(), frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 488   } else if (src-&gt;is_single_fpu()) {
 489     __ str_float(src-&gt;as_float_reg(), addr);
 490   } else if (src-&gt;is_double_fpu()) {
 491     __ str_double(src-&gt;as_double_reg(), addr);
 492   } else {
 493     ShouldNotReachHere();
 494   }
 495 }
 496 
 497 
 498 void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type,
 499                             LIR_PatchCode patch_code, CodeEmitInfo* info,
 500                             bool pop_fpu_stack, bool wide,
 501                             bool unaligned) {
 502   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 503   Register base_reg = to_addr-&gt;base()-&gt;as_pointer_register();
 504   const bool needs_patching = (patch_code != lir_patch_none);
 505 
 506   PatchingStub* patch = NULL;
 507   if (needs_patching) {
 508     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 509   }
 510 
 511   int null_check_offset = code_offset();
 512 
 513   switch (type) {
 514     case T_ARRAY:
 515     case T_OBJECT:
 516       if (UseCompressedOops &amp;&amp; !wide) {
 517         ShouldNotReachHere();
 518       } else {
 519         __ str(src-&gt;as_register(), as_Address(to_addr));
 520       }
 521       break;
 522 
 523     case T_ADDRESS:
 524       __ str(src-&gt;as_pointer_register(), as_Address(to_addr));
 525       break;
 526 
 527     case T_BYTE:
 528     case T_BOOLEAN:
 529       __ strb(src-&gt;as_register(), as_Address(to_addr));
 530       break;
 531 
 532     case T_CHAR:
 533     case T_SHORT:
 534       __ strh(src-&gt;as_register(), as_Address(to_addr));
 535       break;
 536 
 537     case T_INT:
 538 #ifdef __SOFTFP__
 539     case T_FLOAT:
 540 #endif // __SOFTFP__
 541       __ str_32(src-&gt;as_register(), as_Address(to_addr));
 542       break;
 543 
 544 
 545 #ifdef __SOFTFP__
 546     case T_DOUBLE:
 547 #endif // __SOFTFP__
 548     case T_LONG: {
 549       Register from_lo = src-&gt;as_register_lo();
 550       Register from_hi = src-&gt;as_register_hi();
 551       if (to_addr-&gt;index()-&gt;is_register()) {
 552         assert(to_addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 553         assert(to_addr-&gt;disp() == 0, &quot;Not yet supporting both&quot;);
 554         __ add(Rtemp, base_reg, to_addr-&gt;index()-&gt;as_register());
 555         base_reg = Rtemp;
 556         __ str(from_lo, Address(Rtemp));
 557         if (patch != NULL) {
 558           __ nop(); // see comment before patching_epilog for 2nd str
 559           patching_epilog(patch, lir_patch_low, base_reg, info);
 560           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 561           patch_code = lir_patch_high;
 562         }
 563         __ str(from_hi, Address(Rtemp, BytesPerWord));
 564       } else if (base_reg == from_lo) {
 565         __ str(from_hi, as_Address_hi(to_addr));
 566         if (patch != NULL) {
 567           __ nop(); // see comment before patching_epilog for 2nd str
 568           patching_epilog(patch, lir_patch_high, base_reg, info);
 569           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 570           patch_code = lir_patch_low;
 571         }
 572         __ str(from_lo, as_Address_lo(to_addr));
 573       } else {
 574         __ str(from_lo, as_Address_lo(to_addr));
 575         if (patch != NULL) {
 576           __ nop(); // see comment before patching_epilog for 2nd str
 577           patching_epilog(patch, lir_patch_low, base_reg, info);
 578           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 579           patch_code = lir_patch_high;
 580         }
 581         __ str(from_hi, as_Address_hi(to_addr));
 582       }
 583       break;
 584     }
 585 
 586 #ifndef __SOFTFP__
 587     case T_FLOAT:
 588       if (to_addr-&gt;index()-&gt;is_register()) {
 589         assert(to_addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 590         __ add(Rtemp, base_reg, to_addr-&gt;index()-&gt;as_register());
 591         if ((to_addr-&gt;disp() &lt;= -4096) || (to_addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 592         __ fsts(src-&gt;as_float_reg(), Address(Rtemp, to_addr-&gt;disp()));
 593       } else {
 594         __ fsts(src-&gt;as_float_reg(), as_Address(to_addr));
 595       }
 596       break;
 597 
 598     case T_DOUBLE:
 599       if (to_addr-&gt;index()-&gt;is_register()) {
 600         assert(to_addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 601         __ add(Rtemp, base_reg, to_addr-&gt;index()-&gt;as_register());
 602         if ((to_addr-&gt;disp() &lt;= -4096) || (to_addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 603         __ fstd(src-&gt;as_double_reg(), Address(Rtemp, to_addr-&gt;disp()));
 604       } else {
 605         __ fstd(src-&gt;as_double_reg(), as_Address(to_addr));
 606       }
 607       break;
 608 #endif // __SOFTFP__
 609 
 610 
 611     default:
 612       ShouldNotReachHere();
 613   }
 614 
 615   if (info != NULL) {
 616     add_debug_info_for_null_check(null_check_offset, info);
 617   }
 618 
 619   if (patch != NULL) {
 620     // Offset embedded into LDR/STR instruction may appear not enough
 621     // to address a field. So, provide a space for one more instruction
 622     // that will deal with larger offsets.
 623     __ nop();
 624     patching_epilog(patch, patch_code, base_reg, info);
 625   }
 626 }
 627 
 628 
 629 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
 630   assert(src-&gt;is_stack(), &quot;should not call otherwise&quot;);
 631   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 632 
 633   Address addr = src-&gt;is_single_word() ?
 634     frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()) :
 635     frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
 636 
 637   assert(lo_word_offset_in_bytes == 0 &amp;&amp; hi_word_offset_in_bytes == 4, &quot;little ending&quot;);
 638   if (dest-&gt;is_single_fpu() || dest-&gt;is_double_fpu()) {
 639     if (addr.disp() &gt;= 1024) { BAILOUT(&quot;Too exotic case to handle here&quot;); }
 640   }
 641 
 642   if (dest-&gt;is_single_cpu()) {
 643     switch (type) {
 644       case T_OBJECT:
 645       case T_ARRAY:
 646       case T_ADDRESS:
 647       case T_METADATA: __ ldr(dest-&gt;as_register(), addr); break;
 648       case T_FLOAT:    // used in floatToRawIntBits intrinsic implemenation
 649       case T_INT:      __ ldr_u32(dest-&gt;as_register(), addr); break;
 650       default:
 651         ShouldNotReachHere();
 652     }
 653     if ((type == T_OBJECT) || (type == T_ARRAY)) {
 654       __ verify_oop(dest-&gt;as_register());
 655     }
 656   } else if (dest-&gt;is_double_cpu()) {
 657     __ ldr(dest-&gt;as_register_lo(), addr);
 658     __ ldr(dest-&gt;as_register_hi(), frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 659   } else if (dest-&gt;is_single_fpu()) {
 660     __ ldr_float(dest-&gt;as_float_reg(), addr);
 661   } else if (dest-&gt;is_double_fpu()) {
 662     __ ldr_double(dest-&gt;as_double_reg(), addr);
 663   } else {
 664     ShouldNotReachHere();
 665   }
 666 }
 667 
 668 
 669 void LIR_Assembler::stack2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {
 670   if (src-&gt;is_single_stack()) {
 671     switch (src-&gt;type()) {
 672       case T_OBJECT:
 673       case T_ARRAY:
 674       case T_ADDRESS:
 675       case T_METADATA:
 676         __ ldr(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 677         __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 678         break;
 679 
 680       case T_INT:
 681       case T_FLOAT:
 682         __ ldr_u32(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 683         __ str_32(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 684         break;
 685 
 686       default:
 687         ShouldNotReachHere();
 688     }
 689   } else {
 690     assert(src-&gt;is_double_stack(), &quot;must be&quot;);
 691     __ ldr(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), lo_word_offset_in_bytes));
 692     __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes));
 693     __ ldr(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 694     __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 695   }
 696 }
 697 
 698 
 699 void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type,
 700                             LIR_PatchCode patch_code, CodeEmitInfo* info,
 701                             bool wide, bool unaligned) {
 702   assert(src-&gt;is_address(), &quot;should not call otherwise&quot;);
 703   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 704   LIR_Address* addr = src-&gt;as_address_ptr();
 705 
 706   Register base_reg = addr-&gt;base()-&gt;as_pointer_register();
 707 
 708   PatchingStub* patch = NULL;
 709   if (patch_code != lir_patch_none) {
 710     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 711   }
 712   if (info != NULL) {
 713     add_debug_info_for_null_check_here(info);
 714   }
 715 
 716   switch (type) {
 717     case T_OBJECT:  // fall through
 718     case T_ARRAY:
 719       if (UseCompressedOops &amp;&amp; !wide) {
 720         __ ldr_u32(dest-&gt;as_register(), as_Address(addr));
 721       } else {
 722         __ ldr(dest-&gt;as_register(), as_Address(addr));
 723       }
 724       break;
 725 
 726     case T_ADDRESS:
 727       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
 728         __ ldr_u32(dest-&gt;as_pointer_register(), as_Address(addr));
 729       } else {
 730         __ ldr(dest-&gt;as_pointer_register(), as_Address(addr));
 731       }
 732       break;
 733 
 734     case T_INT:
 735 #ifdef __SOFTFP__
 736     case T_FLOAT:
 737 #endif // __SOFTFP__
 738       __ ldr(dest-&gt;as_pointer_register(), as_Address(addr));
 739       break;
 740 
 741     case T_BOOLEAN:
 742       __ ldrb(dest-&gt;as_register(), as_Address(addr));
 743       break;
 744 
 745     case T_BYTE:
 746       __ ldrsb(dest-&gt;as_register(), as_Address(addr));
 747       break;
 748 
 749     case T_CHAR:
 750       __ ldrh(dest-&gt;as_register(), as_Address(addr));
 751       break;
 752 
 753     case T_SHORT:
 754       __ ldrsh(dest-&gt;as_register(), as_Address(addr));
 755       break;
 756 
 757 
 758 #ifdef __SOFTFP__
 759     case T_DOUBLE:
 760 #endif // __SOFTFP__
 761     case T_LONG: {
 762       Register to_lo = dest-&gt;as_register_lo();
 763       Register to_hi = dest-&gt;as_register_hi();
 764       if (addr-&gt;index()-&gt;is_register()) {
 765         assert(addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 766         assert(addr-&gt;disp() == 0, &quot;Not yet supporting both&quot;);
 767         __ add(Rtemp, base_reg, addr-&gt;index()-&gt;as_register());
 768         base_reg = Rtemp;
 769         __ ldr(to_lo, Address(Rtemp));
 770         if (patch != NULL) {
 771           __ nop(); // see comment before patching_epilog for 2nd ldr
 772           patching_epilog(patch, lir_patch_low, base_reg, info);
 773           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 774           patch_code = lir_patch_high;
 775         }
 776         __ ldr(to_hi, Address(Rtemp, BytesPerWord));
 777       } else if (base_reg == to_lo) {
 778         __ ldr(to_hi, as_Address_hi(addr));
 779         if (patch != NULL) {
 780           __ nop(); // see comment before patching_epilog for 2nd ldr
 781           patching_epilog(patch, lir_patch_high, base_reg, info);
 782           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 783           patch_code = lir_patch_low;
 784         }
 785         __ ldr(to_lo, as_Address_lo(addr));
 786       } else {
 787         __ ldr(to_lo, as_Address_lo(addr));
 788         if (patch != NULL) {
 789           __ nop(); // see comment before patching_epilog for 2nd ldr
 790           patching_epilog(patch, lir_patch_low, base_reg, info);
 791           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 792           patch_code = lir_patch_high;
 793         }
 794         __ ldr(to_hi, as_Address_hi(addr));
 795       }
 796       break;
 797     }
 798 
 799 #ifndef __SOFTFP__
 800     case T_FLOAT:
 801       if (addr-&gt;index()-&gt;is_register()) {
 802         assert(addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 803         __ add(Rtemp, base_reg, addr-&gt;index()-&gt;as_register());
 804         if ((addr-&gt;disp() &lt;= -4096) || (addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 805         __ flds(dest-&gt;as_float_reg(), Address(Rtemp, addr-&gt;disp()));
 806       } else {
 807         __ flds(dest-&gt;as_float_reg(), as_Address(addr));
 808       }
 809       break;
 810 
 811     case T_DOUBLE:
 812       if (addr-&gt;index()-&gt;is_register()) {
 813         assert(addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 814         __ add(Rtemp, base_reg, addr-&gt;index()-&gt;as_register());
 815         if ((addr-&gt;disp() &lt;= -4096) || (addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 816         __ fldd(dest-&gt;as_double_reg(), Address(Rtemp, addr-&gt;disp()));
 817       } else {
 818         __ fldd(dest-&gt;as_double_reg(), as_Address(addr));
 819       }
 820       break;
 821 #endif // __SOFTFP__
 822 
 823 
 824     default:
 825       ShouldNotReachHere();
 826   }
 827 
 828   if (patch != NULL) {
 829     // Offset embedded into LDR/STR instruction may appear not enough
 830     // to address a field. So, provide a space for one more instruction
 831     // that will deal with larger offsets.
 832     __ nop();
 833     patching_epilog(patch, patch_code, base_reg, info);
 834   }
 835 
 836 }
 837 
 838 
 839 void LIR_Assembler::emit_op3(LIR_Op3* op) {
 840   bool is_32 = op-&gt;result_opr()-&gt;is_single_cpu();
 841 
 842   if (op-&gt;code() == lir_idiv &amp;&amp; op-&gt;in_opr2()-&gt;is_constant() &amp;&amp; is_32) {
 843     int c = op-&gt;in_opr2()-&gt;as_constant_ptr()-&gt;as_jint();
 844     assert(is_power_of_2(c), &quot;non power-of-2 constant should be put in a register&quot;);
 845 
 846     Register left = op-&gt;in_opr1()-&gt;as_register();
 847     Register dest = op-&gt;result_opr()-&gt;as_register();
 848     if (c == 1) {
 849       __ mov(dest, left);
 850     } else if (c == 2) {
 851       __ add_32(dest, left, AsmOperand(left, lsr, 31));
 852       __ asr_32(dest, dest, 1);
 853     } else if (c != (int) 0x80000000) {
 854       int power = log2_intptr(c);
 855       __ asr_32(Rtemp, left, 31);
 856       __ add_32(dest, left, AsmOperand(Rtemp, lsr, 32-power)); // dest = left + (left &lt; 0 ? 2^power - 1 : 0);
 857       __ asr_32(dest, dest, power);                            // dest = dest &gt;&gt;&gt; power;
 858     } else {
 859       // x/0x80000000 is a special case, since dividend is a power of two, but is negative.
 860       // The only possible result values are 0 and 1, with 1 only for dividend == divisor == 0x80000000.
 861       __ cmp_32(left, c);
 862       __ mov(dest, 0, ne);
 863       __ mov(dest, 1, eq);
 864     }
 865   } else {
 866     assert(op-&gt;code() == lir_idiv || op-&gt;code() == lir_irem, &quot;unexpected op3&quot;);
 867     __ call(StubRoutines::Arm::idiv_irem_entry(), relocInfo::runtime_call_type);
 868     add_debug_info_for_div0_here(op-&gt;info());
 869   }
 870 }
 871 
 872 
 873 void LIR_Assembler::emit_opBranch(LIR_OpBranch* op) {
 874 #ifdef ASSERT
 875   assert(op-&gt;block() == NULL || op-&gt;block()-&gt;label() == op-&gt;label(), &quot;wrong label&quot;);
 876   if (op-&gt;block() != NULL)  _branch_target_blocks.append(op-&gt;block());
 877   if (op-&gt;ublock() != NULL) _branch_target_blocks.append(op-&gt;ublock());
 878   assert(op-&gt;info() == NULL, &quot;CodeEmitInfo?&quot;);
 879 #endif // ASSERT
 880 
 881 #ifdef __SOFTFP__
 882   assert (op-&gt;code() != lir_cond_float_branch, &quot;this should be impossible&quot;);
 883 #else
 884   if (op-&gt;code() == lir_cond_float_branch) {
 885     __ fmstat();
 886     __ b(*(op-&gt;ublock()-&gt;label()), vs);
 887   }
 888 #endif // __SOFTFP__
 889 
 890   AsmCondition acond = al;
 891   switch (op-&gt;cond()) {
 892     case lir_cond_equal:        acond = eq; break;
 893     case lir_cond_notEqual:     acond = ne; break;
 894     case lir_cond_less:         acond = lt; break;
 895     case lir_cond_lessEqual:    acond = le; break;
 896     case lir_cond_greaterEqual: acond = ge; break;
 897     case lir_cond_greater:      acond = gt; break;
 898     case lir_cond_aboveEqual:   acond = hs; break;
 899     case lir_cond_belowEqual:   acond = ls; break;
 900     default: assert(op-&gt;cond() == lir_cond_always, &quot;must be&quot;);
 901   }
 902   __ b(*(op-&gt;label()), acond);
 903 }
 904 
 905 
 906 void LIR_Assembler::emit_opConvert(LIR_OpConvert* op) {
 907   LIR_Opr src  = op-&gt;in_opr();
 908   LIR_Opr dest = op-&gt;result_opr();
 909 
 910   switch (op-&gt;bytecode()) {
 911     case Bytecodes::_i2l:
 912       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 913       __ mov(dest-&gt;as_register_hi(), AsmOperand(src-&gt;as_register(), asr, 31));
 914       break;
 915     case Bytecodes::_l2i:
 916       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 917       break;
 918     case Bytecodes::_i2b:
 919       __ sign_extend(dest-&gt;as_register(), src-&gt;as_register(), 8);
 920       break;
 921     case Bytecodes::_i2s:
 922       __ sign_extend(dest-&gt;as_register(), src-&gt;as_register(), 16);
 923       break;
 924     case Bytecodes::_i2c:
 925       __ zero_extend(dest-&gt;as_register(), src-&gt;as_register(), 16);
 926       break;
 927     case Bytecodes::_f2d:
 928       __ convert_f2d(dest-&gt;as_double_reg(), src-&gt;as_float_reg());
 929       break;
 930     case Bytecodes::_d2f:
 931       __ convert_d2f(dest-&gt;as_float_reg(), src-&gt;as_double_reg());
 932       break;
 933     case Bytecodes::_i2f:
 934       __ fmsr(Stemp, src-&gt;as_register());
 935       __ fsitos(dest-&gt;as_float_reg(), Stemp);
 936       break;
 937     case Bytecodes::_i2d:
 938       __ fmsr(Stemp, src-&gt;as_register());
 939       __ fsitod(dest-&gt;as_double_reg(), Stemp);
 940       break;
 941     case Bytecodes::_f2i:
 942       __ ftosizs(Stemp, src-&gt;as_float_reg());
 943       __ fmrs(dest-&gt;as_register(), Stemp);
 944       break;
 945     case Bytecodes::_d2i:
 946       __ ftosizd(Stemp, src-&gt;as_double_reg());
 947       __ fmrs(dest-&gt;as_register(), Stemp);
 948       break;
 949     default:
 950       ShouldNotReachHere();
 951   }
 952 }
 953 
 954 
 955 void LIR_Assembler::emit_alloc_obj(LIR_OpAllocObj* op) {
 956   if (op-&gt;init_check()) {
 957     Register tmp = op-&gt;tmp1()-&gt;as_register();
 958     __ ldrb(tmp, Address(op-&gt;klass()-&gt;as_register(), InstanceKlass::init_state_offset()));
 959     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
 960     __ cmp(tmp, InstanceKlass::fully_initialized);
 961     __ b(*op-&gt;stub()-&gt;entry(), ne);
 962   }
 963   __ allocate_object(op-&gt;obj()-&gt;as_register(),
 964                      op-&gt;tmp1()-&gt;as_register(),
 965                      op-&gt;tmp2()-&gt;as_register(),
 966                      op-&gt;tmp3()-&gt;as_register(),
 967                      op-&gt;header_size(),
 968                      op-&gt;object_size(),
 969                      op-&gt;klass()-&gt;as_register(),
 970                      *op-&gt;stub()-&gt;entry());
 971   __ bind(*op-&gt;stub()-&gt;continuation());
 972 }
 973 
 974 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
 975   if (UseSlowPath ||
 976       (!UseFastNewObjectArray &amp;&amp; (op-&gt;type() == T_OBJECT || op-&gt;type() == T_ARRAY)) ||
 977       (!UseFastNewTypeArray   &amp;&amp; (op-&gt;type() != T_OBJECT &amp;&amp; op-&gt;type() != T_ARRAY))) {
 978     __ b(*op-&gt;stub()-&gt;entry());
 979   } else {
 980     __ allocate_array(op-&gt;obj()-&gt;as_register(),
 981                       op-&gt;len()-&gt;as_register(),
 982                       op-&gt;tmp1()-&gt;as_register(),
 983                       op-&gt;tmp2()-&gt;as_register(),
 984                       op-&gt;tmp3()-&gt;as_register(),
 985                       arrayOopDesc::header_size(op-&gt;type()),
 986                       type2aelembytes(op-&gt;type()),
 987                       op-&gt;klass()-&gt;as_register(),
 988                       *op-&gt;stub()-&gt;entry());
 989   }
 990   __ bind(*op-&gt;stub()-&gt;continuation());
 991 }
 992 
 993 void LIR_Assembler::type_profile_helper(Register mdo, int mdo_offset_bias,
 994                                         ciMethodData *md, ciProfileData *data,
 995                                         Register recv, Register tmp1, Label* update_done) {
 996   assert_different_registers(mdo, recv, tmp1);
 997   uint i;
 998   for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
 999     Label next_test;
1000     // See if the receiver is receiver[n].
1001     Address receiver_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)) -
1002                           mdo_offset_bias);
1003     __ ldr(tmp1, receiver_addr);
1004     __ verify_klass_ptr(tmp1);
1005     __ cmp(recv, tmp1);
1006     __ b(next_test, ne);
1007     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)) -
1008                       mdo_offset_bias);
1009     __ ldr(tmp1, data_addr);
1010     __ add(tmp1, tmp1, DataLayout::counter_increment);
1011     __ str(tmp1, data_addr);
1012     __ b(*update_done);
1013     __ bind(next_test);
1014   }
1015 
1016   // Didn&#39;t find receiver; find next empty slot and fill it in
1017   for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
1018     Label next_test;
1019     Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)) -
1020                       mdo_offset_bias);
1021     __ ldr(tmp1, recv_addr);
1022     __ cbnz(tmp1, next_test);
1023     __ str(recv, recv_addr);
1024     __ mov(tmp1, DataLayout::counter_increment);
1025     __ str(tmp1, Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)) -
1026                          mdo_offset_bias));
1027     __ b(*update_done);
1028     __ bind(next_test);
1029   }
1030 }
1031 
1032 void LIR_Assembler::setup_md_access(ciMethod* method, int bci,
1033                                     ciMethodData*&amp; md, ciProfileData*&amp; data, int&amp; mdo_offset_bias) {
1034   md = method-&gt;method_data_or_null();
1035   assert(md != NULL, &quot;Sanity&quot;);
1036   data = md-&gt;bci_to_data(bci);
1037   assert(data != NULL,       &quot;need data for checkcast&quot;);
1038   assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1039   if (md-&gt;byte_offset_of_slot(data, DataLayout::header_offset()) + data-&gt;size_in_bytes() &gt;= 4096) {
1040     // The offset is large so bias the mdo by the base of the slot so
1041     // that the ldr can use an immediate offset to reference the slots of the data
1042     mdo_offset_bias = md-&gt;byte_offset_of_slot(data, DataLayout::header_offset());
1043   }
1044 }
1045 
1046 // On 32-bit ARM, code before this helper should test obj for null (ZF should be set if obj is null).
1047 void LIR_Assembler::typecheck_profile_helper1(ciMethod* method, int bci,
1048                                               ciMethodData*&amp; md, ciProfileData*&amp; data, int&amp; mdo_offset_bias,
1049                                               Register obj, Register mdo, Register data_val, Label* obj_is_null) {
1050   assert(method != NULL, &quot;Should have method&quot;);
1051   assert_different_registers(obj, mdo, data_val);
1052   setup_md_access(method, bci, md, data, mdo_offset_bias);
1053   Label not_null;
1054   __ b(not_null, ne);
1055   __ mov_metadata(mdo, md-&gt;constant_encoding());
1056   if (mdo_offset_bias &gt; 0) {
1057     __ mov_slow(data_val, mdo_offset_bias);
1058     __ add(mdo, mdo, data_val);
1059   }
1060   Address flags_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()) - mdo_offset_bias);
1061   __ ldrb(data_val, flags_addr);
1062   __ orr(data_val, data_val, (uint)BitData::null_seen_byte_constant());
1063   __ strb(data_val, flags_addr);
1064   __ b(*obj_is_null);
1065   __ bind(not_null);
1066 }
1067 
1068 void LIR_Assembler::typecheck_profile_helper2(ciMethodData* md, ciProfileData* data, int mdo_offset_bias,
1069                                               Register mdo, Register recv, Register value, Register tmp1,
1070                                               Label* profile_cast_success, Label* profile_cast_failure,
1071                                               Label* success, Label* failure) {
1072   assert_different_registers(mdo, value, tmp1);
1073   __ bind(*profile_cast_success);
1074   __ mov_metadata(mdo, md-&gt;constant_encoding());
1075   if (mdo_offset_bias &gt; 0) {
1076     __ mov_slow(tmp1, mdo_offset_bias);
1077     __ add(mdo, mdo, tmp1);
1078   }
1079   __ load_klass(recv, value);
1080   type_profile_helper(mdo, mdo_offset_bias, md, data, recv, tmp1, success);
1081   __ b(*success);
1082   // Cast failure case
1083   __ bind(*profile_cast_failure);
1084   __ mov_metadata(mdo, md-&gt;constant_encoding());
1085   if (mdo_offset_bias &gt; 0) {
1086     __ mov_slow(tmp1, mdo_offset_bias);
1087     __ add(mdo, mdo, tmp1);
1088   }
1089   Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) - mdo_offset_bias);
1090   __ ldr(tmp1, data_addr);
1091   __ sub(tmp1, tmp1, DataLayout::counter_increment);
1092   __ str(tmp1, data_addr);
1093   __ b(*failure);
1094 }
1095 
1096 // Sets `res` to true, if `cond` holds.
1097 static void set_instanceof_result(MacroAssembler* _masm, Register res, AsmCondition cond) {
1098   __ mov(res, 1, cond);
1099 }
1100 
1101 
1102 void LIR_Assembler::emit_opTypeCheck(LIR_OpTypeCheck* op) {
1103   // TODO: ARM - can be more effective with one more register
1104   switch (op-&gt;code()) {
1105     case lir_store_check: {
1106       CodeStub* stub = op-&gt;stub();
1107       Register value = op-&gt;object()-&gt;as_register();
1108       Register array = op-&gt;array()-&gt;as_register();
1109       Register klass_RInfo = op-&gt;tmp1()-&gt;as_register();
1110       Register k_RInfo = op-&gt;tmp2()-&gt;as_register();
1111       assert_different_registers(klass_RInfo, k_RInfo, Rtemp);
1112       if (op-&gt;should_profile()) {
1113         assert_different_registers(value, klass_RInfo, k_RInfo, Rtemp);
1114       }
1115 
1116       // check if it needs to be profiled
1117       ciMethodData* md;
1118       ciProfileData* data;
1119       int mdo_offset_bias = 0;
1120       Label profile_cast_success, profile_cast_failure, done;
1121       Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
1122       Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : stub-&gt;entry();
1123 
1124       if (op-&gt;should_profile()) {
1125         __ cmp(value, 0);
1126         typecheck_profile_helper1(op-&gt;profiled_method(), op-&gt;profiled_bci(), md, data, mdo_offset_bias, value, k_RInfo, Rtemp, &amp;done);
1127       } else {
1128         __ cbz(value, done);
1129       }
1130       assert_different_registers(k_RInfo, value);
1131       add_debug_info_for_null_check_here(op-&gt;info_for_exception());
1132       __ load_klass(k_RInfo, array);
1133       __ load_klass(klass_RInfo, value);
1134       __ ldr(k_RInfo, Address(k_RInfo, ObjArrayKlass::element_klass_offset()));
1135       __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1136       // check for immediate positive hit
1137       __ ldr(Rtemp, Address(klass_RInfo, Rtemp));
1138       __ cmp(klass_RInfo, k_RInfo);
1139       __ cond_cmp(Rtemp, k_RInfo, ne);
1140       __ b(*success_target, eq);
1141       // check for immediate negative hit
1142       __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1143       __ cmp(Rtemp, in_bytes(Klass::secondary_super_cache_offset()));
1144       __ b(*failure_target, ne);
1145       // slow case
1146       assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1147       __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1148       __ cbz(R0, *failure_target);
1149       if (op-&gt;should_profile()) {
1150         Register mdo  = klass_RInfo, recv = k_RInfo, tmp1 = Rtemp;
1151         if (mdo == value) {
1152           mdo = k_RInfo;
1153           recv = klass_RInfo;
1154         }
1155         typecheck_profile_helper2(md, data, mdo_offset_bias, mdo, recv, value, tmp1,
1156                                   &amp;profile_cast_success, &amp;profile_cast_failure,
1157                                   &amp;done, stub-&gt;entry());
1158       }
1159       __ bind(done);
1160       break;
1161     }
1162 
1163     case lir_checkcast: {
1164       CodeStub* stub = op-&gt;stub();
1165       Register obj = op-&gt;object()-&gt;as_register();
1166       Register res = op-&gt;result_opr()-&gt;as_register();
1167       Register klass_RInfo = op-&gt;tmp1()-&gt;as_register();
1168       Register k_RInfo = op-&gt;tmp2()-&gt;as_register();
1169       ciKlass* k = op-&gt;klass();
1170       assert_different_registers(res, k_RInfo, klass_RInfo, Rtemp);
1171 
1172       if (stub-&gt;is_simple_exception_stub()) {
1173       // TODO: ARM - Late binding is used to prevent confusion of register allocator
1174       assert(stub-&gt;is_exception_throw_stub(), &quot;must be&quot;);
1175       ((SimpleExceptionStub*)stub)-&gt;set_obj(op-&gt;result_opr());
1176       }
1177       ciMethodData* md;
1178       ciProfileData* data;
1179       int mdo_offset_bias = 0;
1180 
1181       Label done;
1182 
1183       Label profile_cast_failure, profile_cast_success;
1184       Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : op-&gt;stub()-&gt;entry();
1185       Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
1186 
1187 
1188       __ movs(res, obj);
1189       if (op-&gt;should_profile()) {
1190         typecheck_profile_helper1(op-&gt;profiled_method(), op-&gt;profiled_bci(), md, data, mdo_offset_bias, res, klass_RInfo, Rtemp, &amp;done);
1191       } else {
1192         __ b(done, eq);
1193       }
1194       if (k-&gt;is_loaded()) {
1195         __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1196       } else if (k_RInfo != obj) {
1197         klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1198         __ movs(res, obj);
1199       } else {
1200         // Patching doesn&#39;t update &quot;res&quot; register after GC, so do patching first
1201         klass2reg_with_patching(Rtemp, op-&gt;info_for_patch());
1202         __ movs(res, obj);
1203         __ mov(k_RInfo, Rtemp);
1204       }
1205       __ load_klass(klass_RInfo, res, ne);
1206 
1207       if (op-&gt;fast_check()) {
1208         __ cmp(klass_RInfo, k_RInfo, ne);
1209         __ b(*failure_target, ne);
1210       } else if (k-&gt;is_loaded()) {
1211         __ b(*success_target, eq);
1212         __ ldr(Rtemp, Address(klass_RInfo, k-&gt;super_check_offset()));
1213         if (in_bytes(Klass::secondary_super_cache_offset()) != (int) k-&gt;super_check_offset()) {
1214           __ cmp(Rtemp, k_RInfo);
1215           __ b(*failure_target, ne);
1216         } else {
1217           __ cmp(klass_RInfo, k_RInfo);
1218           __ cmp(Rtemp, k_RInfo, ne);
1219           __ b(*success_target, eq);
1220           assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1221           __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1222           __ cbz(R0, *failure_target);
1223         }
1224       } else {
1225         __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1226         __ b(*success_target, eq);
1227         // check for immediate positive hit
1228         __ ldr(Rtemp, Address(klass_RInfo, Rtemp));
1229         __ cmp(klass_RInfo, k_RInfo);
1230         __ cmp(Rtemp, k_RInfo, ne);
1231         __ b(*success_target, eq);
1232         // check for immediate negative hit
1233         __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1234         __ cmp(Rtemp, in_bytes(Klass::secondary_super_cache_offset()));
1235         __ b(*failure_target, ne);
1236         // slow case
1237         assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1238         __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1239         __ cbz(R0, *failure_target);
1240       }
1241 
1242       if (op-&gt;should_profile()) {
1243         Register mdo  = klass_RInfo, recv = k_RInfo, tmp1 = Rtemp;
1244         typecheck_profile_helper2(md, data, mdo_offset_bias, mdo, recv, res, tmp1,
1245                                   &amp;profile_cast_success, &amp;profile_cast_failure,
1246                                   &amp;done, stub-&gt;entry());
1247       }
1248       __ bind(done);
1249       break;
1250     }
1251 
1252     case lir_instanceof: {
1253       Register obj = op-&gt;object()-&gt;as_register();
1254       Register res = op-&gt;result_opr()-&gt;as_register();
1255       Register klass_RInfo = op-&gt;tmp1()-&gt;as_register();
1256       Register k_RInfo = op-&gt;tmp2()-&gt;as_register();
1257       ciKlass* k = op-&gt;klass();
1258       assert_different_registers(res, klass_RInfo, k_RInfo, Rtemp);
1259 
1260       ciMethodData* md;
1261       ciProfileData* data;
1262       int mdo_offset_bias = 0;
1263 
1264       Label done;
1265 
1266       Label profile_cast_failure, profile_cast_success;
1267       Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : &amp;done;
1268       Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
1269 
1270       __ movs(res, obj);
1271 
1272       if (op-&gt;should_profile()) {
1273         typecheck_profile_helper1(op-&gt;profiled_method(), op-&gt;profiled_bci(), md, data, mdo_offset_bias, res, klass_RInfo, Rtemp, &amp;done);
1274       } else {
1275         __ b(done, eq);
1276       }
1277 
1278       if (k-&gt;is_loaded()) {
1279         __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1280       } else {
1281         op-&gt;info_for_patch()-&gt;add_register_oop(FrameMap::as_oop_opr(res));
1282         klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1283       }
1284       __ load_klass(klass_RInfo, res);
1285 
1286       if (!op-&gt;should_profile()) {
1287         __ mov(res, 0);
1288       }
1289 
1290       if (op-&gt;fast_check()) {
1291         __ cmp(klass_RInfo, k_RInfo);
1292         if (!op-&gt;should_profile()) {
1293           set_instanceof_result(_masm, res, eq);
1294         } else {
1295           __ b(profile_cast_failure, ne);
1296         }
1297       } else if (k-&gt;is_loaded()) {
1298         __ ldr(Rtemp, Address(klass_RInfo, k-&gt;super_check_offset()));
1299         if (in_bytes(Klass::secondary_super_cache_offset()) != (int) k-&gt;super_check_offset()) {
1300           __ cmp(Rtemp, k_RInfo);
1301           if (!op-&gt;should_profile()) {
1302             set_instanceof_result(_masm, res, eq);
1303           } else {
1304             __ b(profile_cast_failure, ne);
1305           }
1306         } else {
1307           __ cmp(klass_RInfo, k_RInfo);
1308           __ cond_cmp(Rtemp, k_RInfo, ne);
1309           if (!op-&gt;should_profile()) {
1310             set_instanceof_result(_masm, res, eq);
1311           }
1312           __ b(*success_target, eq);
1313           assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1314           __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1315           if (!op-&gt;should_profile()) {
1316             move_regs(R0, res);
1317           } else {
1318             __ cbz(R0, *failure_target);
1319           }
1320         }
1321       } else {
1322         __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1323         // check for immediate positive hit
1324         __ cmp(klass_RInfo, k_RInfo);
1325         if (!op-&gt;should_profile()) {
1326           __ ldr(res, Address(klass_RInfo, Rtemp), ne);
1327           __ cond_cmp(res, k_RInfo, ne);
1328           set_instanceof_result(_masm, res, eq);
1329         } else {
1330           __ ldr(Rtemp, Address(klass_RInfo, Rtemp), ne);
1331           __ cond_cmp(Rtemp, k_RInfo, ne);
1332         }
1333         __ b(*success_target, eq);
1334         // check for immediate negative hit
1335         if (op-&gt;should_profile()) {
1336           __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1337         }
1338         __ cmp(Rtemp, in_bytes(Klass::secondary_super_cache_offset()));
1339         if (!op-&gt;should_profile()) {
1340           __ mov(res, 0, ne);
1341         }
1342         __ b(*failure_target, ne);
1343         // slow case
1344         assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1345         __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1346         if (!op-&gt;should_profile()) {
1347           move_regs(R0, res);
1348         }
1349         if (op-&gt;should_profile()) {
1350           __ cbz(R0, *failure_target);
1351         }
1352       }
1353 
1354       if (op-&gt;should_profile()) {
1355         Label done_ok, done_failure;
1356         Register mdo  = klass_RInfo, recv = k_RInfo, tmp1 = Rtemp;
1357         typecheck_profile_helper2(md, data, mdo_offset_bias, mdo, recv, res, tmp1,
1358                                   &amp;profile_cast_success, &amp;profile_cast_failure,
1359                                   &amp;done_ok, &amp;done_failure);
1360         __ bind(done_failure);
1361         __ mov(res, 0);
1362         __ b(done);
1363         __ bind(done_ok);
1364         __ mov(res, 1);
1365       }
1366       __ bind(done);
1367       break;
1368     }
1369     default:
1370       ShouldNotReachHere();
1371   }
1372 }
1373 
1374 
1375 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
1376   //   if (*addr == cmpval) {
1377   //     *addr = newval;
1378   //     dest = 1;
1379   //   } else {
1380   //     dest = 0;
1381   //   }
1382   // FIXME: membar_release
1383   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
1384   Register addr = op-&gt;addr()-&gt;is_register() ?
1385     op-&gt;addr()-&gt;as_pointer_register() :
1386     op-&gt;addr()-&gt;as_address_ptr()-&gt;base()-&gt;as_pointer_register();
1387   assert(op-&gt;addr()-&gt;is_register() || op-&gt;addr()-&gt;as_address_ptr()-&gt;disp() == 0, &quot;unexpected disp&quot;);
1388   assert(op-&gt;addr()-&gt;is_register() || op-&gt;addr()-&gt;as_address_ptr()-&gt;index() == LIR_OprDesc::illegalOpr(), &quot;unexpected index&quot;);
1389   if (op-&gt;code() == lir_cas_int || op-&gt;code() == lir_cas_obj) {
1390     Register cmpval = op-&gt;cmp_value()-&gt;as_register();
1391     Register newval = op-&gt;new_value()-&gt;as_register();
1392     Register dest = op-&gt;result_opr()-&gt;as_register();
1393     assert_different_registers(dest, addr, cmpval, newval, Rtemp);
1394 
1395     __ atomic_cas_bool(cmpval, newval, addr, 0, Rtemp); // Rtemp free by default at C1 LIR layer
1396     __ mov(dest, 1, eq);
1397     __ mov(dest, 0, ne);
1398   } else if (op-&gt;code() == lir_cas_long) {
1399     assert(VM_Version::supports_cx8(), &quot;wrong machine&quot;);
1400     Register cmp_value_lo = op-&gt;cmp_value()-&gt;as_register_lo();
1401     Register cmp_value_hi = op-&gt;cmp_value()-&gt;as_register_hi();
1402     Register new_value_lo = op-&gt;new_value()-&gt;as_register_lo();
1403     Register new_value_hi = op-&gt;new_value()-&gt;as_register_hi();
1404     Register dest = op-&gt;result_opr()-&gt;as_register();
1405     Register tmp_lo = op-&gt;tmp1()-&gt;as_register_lo();
1406     Register tmp_hi = op-&gt;tmp1()-&gt;as_register_hi();
1407 
1408     assert_different_registers(tmp_lo, tmp_hi, cmp_value_lo, cmp_value_hi, dest, new_value_lo, new_value_hi, addr);
1409     assert(tmp_hi-&gt;encoding() == tmp_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
1410     assert(new_value_hi-&gt;encoding() == new_value_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
1411     assert((tmp_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
1412     assert((new_value_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
1413     __ atomic_cas64(tmp_lo, tmp_hi, dest, cmp_value_lo, cmp_value_hi,
1414                     new_value_lo, new_value_hi, addr, 0);
1415   } else {
1416     Unimplemented();
1417   }
1418   // FIXME: is full membar really needed instead of just membar_acquire?
1419   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreLoad | MacroAssembler::StoreStore), Rtemp);
1420 }
1421 
1422 
1423 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
1424   AsmCondition acond = al;
1425   AsmCondition ncond = nv;
1426   if (opr1 != opr2) {
1427     switch (condition) {
1428       case lir_cond_equal:        acond = eq; ncond = ne; break;
1429       case lir_cond_notEqual:     acond = ne; ncond = eq; break;
1430       case lir_cond_less:         acond = lt; ncond = ge; break;
1431       case lir_cond_lessEqual:    acond = le; ncond = gt; break;
1432       case lir_cond_greaterEqual: acond = ge; ncond = lt; break;
1433       case lir_cond_greater:      acond = gt; ncond = le; break;
1434       case lir_cond_aboveEqual:   acond = hs; ncond = lo; break;
1435       case lir_cond_belowEqual:   acond = ls; ncond = hi; break;
1436       default: ShouldNotReachHere();
1437     }
1438   }
1439 
1440   for (;;) {                         // two iterations only
1441     if (opr1 == result) {
1442       // do nothing
1443     } else if (opr1-&gt;is_single_cpu()) {
1444       __ mov(result-&gt;as_register(), opr1-&gt;as_register(), acond);
1445     } else if (opr1-&gt;is_double_cpu()) {
1446       __ long_move(result-&gt;as_register_lo(), result-&gt;as_register_hi(),
1447                    opr1-&gt;as_register_lo(), opr1-&gt;as_register_hi(), acond);
1448     } else if (opr1-&gt;is_single_stack()) {
1449       __ ldr(result-&gt;as_register(), frame_map()-&gt;address_for_slot(opr1-&gt;single_stack_ix()), acond);
1450     } else if (opr1-&gt;is_double_stack()) {
1451       __ ldr(result-&gt;as_register_lo(),
1452              frame_map()-&gt;address_for_slot(opr1-&gt;double_stack_ix(), lo_word_offset_in_bytes), acond);
1453       __ ldr(result-&gt;as_register_hi(),
1454              frame_map()-&gt;address_for_slot(opr1-&gt;double_stack_ix(), hi_word_offset_in_bytes), acond);
1455     } else if (opr1-&gt;is_illegal()) {
1456       // do nothing: this part of the cmove has been optimized away in the peephole optimizer
1457     } else {
1458       assert(opr1-&gt;is_constant(), &quot;must be&quot;);
1459       LIR_Const* c = opr1-&gt;as_constant_ptr();
1460 
1461       switch (c-&gt;type()) {
1462         case T_INT:
1463           __ mov_slow(result-&gt;as_register(), c-&gt;as_jint(), acond);
1464           break;
1465         case T_LONG:
1466           __ mov_slow(result-&gt;as_register_lo(), c-&gt;as_jint_lo(), acond);
1467           __ mov_slow(result-&gt;as_register_hi(), c-&gt;as_jint_hi(), acond);
1468           break;
1469         case T_OBJECT:
1470           __ mov_oop(result-&gt;as_register(), c-&gt;as_jobject(), 0, acond);
1471           break;
1472         case T_FLOAT:
1473 #ifdef __SOFTFP__
1474           // not generated now.
1475           __ mov_slow(result-&gt;as_register(), c-&gt;as_jint(), acond);
1476 #else
1477           __ mov_float(result-&gt;as_float_reg(), c-&gt;as_jfloat(), acond);
1478 #endif // __SOFTFP__
1479           break;
1480         case T_DOUBLE:
1481 #ifdef __SOFTFP__
1482           // not generated now.
1483           __ mov_slow(result-&gt;as_register_lo(), c-&gt;as_jint_lo(), acond);
1484           __ mov_slow(result-&gt;as_register_hi(), c-&gt;as_jint_hi(), acond);
1485 #else
1486           __ mov_double(result-&gt;as_double_reg(), c-&gt;as_jdouble(), acond);
1487 #endif // __SOFTFP__
1488           break;
1489         default:
1490           ShouldNotReachHere();
1491       }
1492     }
1493 
1494     // Negate the condition and repeat the algorithm with the second operand
1495     if (opr1 == opr2) { break; }
1496     opr1 = opr2;
1497     acond = ncond;
1498   }
1499 }
1500 
1501 #ifdef ASSERT
1502 static int reg_size(LIR_Opr op) {
1503   switch (op-&gt;type()) {
1504   case T_FLOAT:
1505   case T_INT:      return BytesPerInt;
1506   case T_LONG:
1507   case T_DOUBLE:   return BytesPerLong;
1508   case T_OBJECT:
1509   case T_ARRAY:
1510   case T_METADATA: return BytesPerWord;
1511   case T_ADDRESS:
1512   case T_ILLEGAL:  // fall through
1513   default: ShouldNotReachHere(); return -1;
1514   }
1515 }
1516 #endif
1517 
1518 void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info, bool pop_fpu_stack) {
1519   assert(info == NULL, &quot;unused on this code path&quot;);
1520   assert(dest-&gt;is_register(), &quot;wrong items state&quot;);
1521 
1522   if (right-&gt;is_address()) {
1523     // special case for adding shifted/extended register
1524     const Register res = dest-&gt;as_pointer_register();
1525     const Register lreg = left-&gt;as_pointer_register();
1526     const LIR_Address* addr = right-&gt;as_address_ptr();
1527 
1528     assert(addr-&gt;base()-&gt;as_pointer_register() == lreg &amp;&amp; addr-&gt;index()-&gt;is_register() &amp;&amp; addr-&gt;disp() == 0, &quot;must be&quot;);
1529 
1530     int scale = addr-&gt;scale();
1531     AsmShift shift = lsl;
1532 
1533 
1534     assert(reg_size(addr-&gt;base()) == reg_size(addr-&gt;index()), &quot;should be&quot;);
1535     assert(reg_size(addr-&gt;base()) == reg_size(dest), &quot;should be&quot;);
1536     assert(reg_size(dest) == wordSize, &quot;should be&quot;);
1537 
1538     AsmOperand operand(addr-&gt;index()-&gt;as_pointer_register(), shift, scale);
1539     switch (code) {
1540       case lir_add: __ add(res, lreg, operand); break;
1541       case lir_sub: __ sub(res, lreg, operand); break;
1542       default: ShouldNotReachHere();
1543     }
1544 
1545   } else if (left-&gt;is_address()) {
1546     assert(code == lir_sub &amp;&amp; right-&gt;is_single_cpu(), &quot;special case used by strength_reduce_multiply()&quot;);
1547     const LIR_Address* addr = left-&gt;as_address_ptr();
1548     const Register res = dest-&gt;as_register();
1549     const Register rreg = right-&gt;as_register();
1550     assert(addr-&gt;base()-&gt;as_register() == rreg &amp;&amp; addr-&gt;index()-&gt;is_register() &amp;&amp; addr-&gt;disp() == 0, &quot;must be&quot;);
1551     __ rsb(res, rreg, AsmOperand(addr-&gt;index()-&gt;as_register(), lsl, addr-&gt;scale()));
1552 
1553   } else if (dest-&gt;is_single_cpu()) {
1554     assert(left-&gt;is_single_cpu(), &quot;unexpected left operand&quot;);
1555 
1556     const Register res = dest-&gt;as_register();
1557     const Register lreg = left-&gt;as_register();
1558 
1559     if (right-&gt;is_single_cpu()) {
1560       const Register rreg = right-&gt;as_register();
1561       switch (code) {
1562         case lir_add: __ add_32(res, lreg, rreg); break;
1563         case lir_sub: __ sub_32(res, lreg, rreg); break;
1564         case lir_mul: __ mul_32(res, lreg, rreg); break;
1565         default: ShouldNotReachHere();
1566       }
1567     } else {
1568       assert(right-&gt;is_constant(), &quot;must be&quot;);
1569       const jint c = right-&gt;as_constant_ptr()-&gt;as_jint();
1570       if (!Assembler::is_arith_imm_in_range(c)) {
1571         BAILOUT(&quot;illegal arithmetic operand&quot;);
1572       }
1573       switch (code) {
1574         case lir_add: __ add_32(res, lreg, c); break;
1575         case lir_sub: __ sub_32(res, lreg, c); break;
1576         default: ShouldNotReachHere();
1577       }
1578     }
1579 
1580   } else if (dest-&gt;is_double_cpu()) {
1581     Register res_lo = dest-&gt;as_register_lo();
1582     Register res_hi = dest-&gt;as_register_hi();
1583     Register lreg_lo = left-&gt;as_register_lo();
1584     Register lreg_hi = left-&gt;as_register_hi();
1585     if (right-&gt;is_double_cpu()) {
1586       Register rreg_lo = right-&gt;as_register_lo();
1587       Register rreg_hi = right-&gt;as_register_hi();
1588       if (res_lo == lreg_hi || res_lo == rreg_hi) {
1589         res_lo = Rtemp;
1590       }
1591       switch (code) {
1592         case lir_add:
1593           __ adds(res_lo, lreg_lo, rreg_lo);
1594           __ adc(res_hi, lreg_hi, rreg_hi);
1595           break;
1596         case lir_sub:
1597           __ subs(res_lo, lreg_lo, rreg_lo);
1598           __ sbc(res_hi, lreg_hi, rreg_hi);
1599           break;
1600         default:
1601           ShouldNotReachHere();
1602       }
1603     } else {
1604       assert(right-&gt;is_constant(), &quot;must be&quot;);
1605       assert((right-&gt;as_constant_ptr()-&gt;as_jlong() &gt;&gt; 32) == 0, &quot;out of range&quot;);
1606       const jint c = (jint) right-&gt;as_constant_ptr()-&gt;as_jlong();
1607       if (res_lo == lreg_hi) {
1608         res_lo = Rtemp;
1609       }
1610       switch (code) {
1611         case lir_add:
1612           __ adds(res_lo, lreg_lo, c);
1613           __ adc(res_hi, lreg_hi, 0);
1614           break;
1615         case lir_sub:
1616           __ subs(res_lo, lreg_lo, c);
1617           __ sbc(res_hi, lreg_hi, 0);
1618           break;
1619         default:
1620           ShouldNotReachHere();
1621       }
1622     }
1623     move_regs(res_lo, dest-&gt;as_register_lo());
1624 
1625   } else if (dest-&gt;is_single_fpu()) {
1626     assert(left-&gt;is_single_fpu(), &quot;must be&quot;);
1627     assert(right-&gt;is_single_fpu(), &quot;must be&quot;);
1628     const FloatRegister res = dest-&gt;as_float_reg();
1629     const FloatRegister lreg = left-&gt;as_float_reg();
1630     const FloatRegister rreg = right-&gt;as_float_reg();
1631     switch (code) {
1632       case lir_add: __ add_float(res, lreg, rreg); break;
1633       case lir_sub: __ sub_float(res, lreg, rreg); break;
1634       case lir_mul_strictfp: // fall through
1635       case lir_mul: __ mul_float(res, lreg, rreg); break;
1636       case lir_div_strictfp: // fall through
1637       case lir_div: __ div_float(res, lreg, rreg); break;
1638       default: ShouldNotReachHere();
1639     }
1640   } else if (dest-&gt;is_double_fpu()) {
1641     assert(left-&gt;is_double_fpu(), &quot;must be&quot;);
1642     assert(right-&gt;is_double_fpu(), &quot;must be&quot;);
1643     const FloatRegister res = dest-&gt;as_double_reg();
1644     const FloatRegister lreg = left-&gt;as_double_reg();
1645     const FloatRegister rreg = right-&gt;as_double_reg();
1646     switch (code) {
1647       case lir_add: __ add_double(res, lreg, rreg); break;
1648       case lir_sub: __ sub_double(res, lreg, rreg); break;
1649       case lir_mul_strictfp: // fall through
1650       case lir_mul: __ mul_double(res, lreg, rreg); break;
1651       case lir_div_strictfp: // fall through
1652       case lir_div: __ div_double(res, lreg, rreg); break;
1653       default: ShouldNotReachHere();
1654     }
1655   } else {
1656     ShouldNotReachHere();
1657   }
1658 }
1659 
1660 
1661 void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr unused, LIR_Opr dest, LIR_Op* op) {
1662   switch (code) {
1663     case lir_abs:
1664       __ abs_double(dest-&gt;as_double_reg(), value-&gt;as_double_reg());
1665       break;
1666     case lir_sqrt:
1667       __ sqrt_double(dest-&gt;as_double_reg(), value-&gt;as_double_reg());
1668       break;
1669     default:
1670       ShouldNotReachHere();
1671   }
1672 }
1673 
1674 
1675 void LIR_Assembler::logic_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest) {
1676   assert(dest-&gt;is_register(), &quot;wrong items state&quot;);
1677   assert(left-&gt;is_register(), &quot;wrong items state&quot;);
1678 
1679   if (dest-&gt;is_single_cpu()) {
1680 
1681     const Register res = dest-&gt;as_register();
1682     const Register lreg = left-&gt;as_register();
1683 
1684     if (right-&gt;is_single_cpu()) {
1685       const Register rreg = right-&gt;as_register();
1686       switch (code) {
1687         case lir_logic_and: __ and_32(res, lreg, rreg); break;
1688         case lir_logic_or:  __ orr_32(res, lreg, rreg); break;
1689         case lir_logic_xor: __ eor_32(res, lreg, rreg); break;
1690         default: ShouldNotReachHere();
1691       }
1692     } else {
1693       assert(right-&gt;is_constant(), &quot;must be&quot;);
1694       const uint c = (uint)right-&gt;as_constant_ptr()-&gt;as_jint();
1695       switch (code) {
1696         case lir_logic_and: __ and_32(res, lreg, c); break;
1697         case lir_logic_or:  __ orr_32(res, lreg, c); break;
1698         case lir_logic_xor: __ eor_32(res, lreg, c); break;
1699         default: ShouldNotReachHere();
1700       }
1701     }
1702   } else {
1703     assert(dest-&gt;is_double_cpu(), &quot;should be&quot;);
1704     Register res_lo = dest-&gt;as_register_lo();
1705 
1706     assert (dest-&gt;type() == T_LONG, &quot;unexpected result type&quot;);
1707     assert (left-&gt;type() == T_LONG, &quot;unexpected left type&quot;);
1708     assert (right-&gt;type() == T_LONG, &quot;unexpected right type&quot;);
1709 
1710     const Register res_hi = dest-&gt;as_register_hi();
1711     const Register lreg_lo = left-&gt;as_register_lo();
1712     const Register lreg_hi = left-&gt;as_register_hi();
1713 
1714     if (right-&gt;is_register()) {
1715       const Register rreg_lo = right-&gt;as_register_lo();
1716       const Register rreg_hi = right-&gt;as_register_hi();
1717       if (res_lo == lreg_hi || res_lo == rreg_hi) {
1718         res_lo = Rtemp; // Temp register helps to avoid overlap between result and input
1719       }
1720       switch (code) {
1721         case lir_logic_and:
1722           __ andr(res_lo, lreg_lo, rreg_lo);
1723           __ andr(res_hi, lreg_hi, rreg_hi);
1724           break;
1725         case lir_logic_or:
1726           __ orr(res_lo, lreg_lo, rreg_lo);
1727           __ orr(res_hi, lreg_hi, rreg_hi);
1728           break;
1729         case lir_logic_xor:
1730           __ eor(res_lo, lreg_lo, rreg_lo);
1731           __ eor(res_hi, lreg_hi, rreg_hi);
1732           break;
1733         default:
1734           ShouldNotReachHere();
1735       }
1736       move_regs(res_lo, dest-&gt;as_register_lo());
1737     } else {
1738       assert(right-&gt;is_constant(), &quot;must be&quot;);
1739       const jint c_lo = (jint) right-&gt;as_constant_ptr()-&gt;as_jlong();
1740       const jint c_hi = (jint) (right-&gt;as_constant_ptr()-&gt;as_jlong() &gt;&gt; 32);
1741       // Case for logic_or from do_ClassIDIntrinsic()
1742       if (c_hi == 0 &amp;&amp; AsmOperand::is_rotated_imm(c_lo)) {
1743         switch (code) {
1744           case lir_logic_and:
1745             __ andr(res_lo, lreg_lo, c_lo);
1746             __ mov(res_hi, 0);
1747             break;
1748           case lir_logic_or:
1749             __ orr(res_lo, lreg_lo, c_lo);
1750             break;
1751           case lir_logic_xor:
1752             __ eor(res_lo, lreg_lo, c_lo);
1753             break;
1754         default:
1755           ShouldNotReachHere();
1756         }
1757       } else if (code == lir_logic_and &amp;&amp;
1758                  c_hi == -1 &amp;&amp;
1759                  (AsmOperand::is_rotated_imm(c_lo) ||
1760                   AsmOperand::is_rotated_imm(~c_lo))) {
1761         // Another case which handles logic_and from do_ClassIDIntrinsic()
1762         if (AsmOperand::is_rotated_imm(c_lo)) {
1763           __ andr(res_lo, lreg_lo, c_lo);
1764         } else {
1765           __ bic(res_lo, lreg_lo, ~c_lo);
1766         }
1767         if (res_hi != lreg_hi) {
1768           __ mov(res_hi, lreg_hi);
1769         }
1770       } else {
1771         BAILOUT(&quot;64 bit constant cannot be inlined&quot;);
1772       }
1773     }
1774   }
1775 }
1776 
1777 
1778 
1779 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
1780   if (opr1-&gt;is_single_cpu()) {
1781     if (opr2-&gt;is_constant()) {
1782       switch (opr2-&gt;as_constant_ptr()-&gt;type()) {
1783         case T_INT: {
1784           const jint c = opr2-&gt;as_constant_ptr()-&gt;as_jint();
1785           if (Assembler::is_arith_imm_in_range(c)) {
1786             __ cmp_32(opr1-&gt;as_register(), c);
1787           } else if (Assembler::is_arith_imm_in_range(-c)) {
1788             __ cmn_32(opr1-&gt;as_register(), -c);
1789           } else {
1790             // This can happen when compiling lookupswitch
1791             __ mov_slow(Rtemp, c);
1792             __ cmp_32(opr1-&gt;as_register(), Rtemp);
1793           }
1794           break;
1795         }
1796         case T_OBJECT:
1797           assert(opr2-&gt;as_constant_ptr()-&gt;as_jobject() == NULL, &quot;cannot handle otherwise&quot;);
1798           __ cmp(opr1-&gt;as_register(), 0);
1799           break;
1800         case T_METADATA:
1801           assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;Only equality tests&quot;);
1802           assert(opr2-&gt;as_constant_ptr()-&gt;as_metadata() == NULL, &quot;cannot handle otherwise&quot;);
1803           __ cmp(opr1-&gt;as_register(), 0);
1804           break;
1805         default:
1806           ShouldNotReachHere();
1807       }
1808     } else if (opr2-&gt;is_single_cpu()) {
1809       if (opr1-&gt;type() == T_OBJECT || opr1-&gt;type() == T_ARRAY) {
1810         assert(opr2-&gt;type() == T_OBJECT || opr2-&gt;type() == T_ARRAY, &quot;incompatibe type&quot;);
1811         __ cmpoop(opr1-&gt;as_register(), opr2-&gt;as_register());
1812       } else if (opr1-&gt;type() == T_METADATA || opr1-&gt;type() == T_ADDRESS) {
1813         assert(opr2-&gt;type() == T_METADATA || opr2-&gt;type() == T_ADDRESS, &quot;incompatibe type&quot;);
1814         __ cmp(opr1-&gt;as_register(), opr2-&gt;as_register());
1815       } else {
1816         assert(opr2-&gt;type() != T_OBJECT &amp;&amp; opr2-&gt;type() != T_ARRAY &amp;&amp; opr2-&gt;type() != T_METADATA &amp;&amp; opr2-&gt;type() != T_ADDRESS, &quot;incompatibe type&quot;);
1817         __ cmp_32(opr1-&gt;as_register(), opr2-&gt;as_register());
1818       }
1819     } else {
1820       ShouldNotReachHere();
1821     }
1822   } else if (opr1-&gt;is_double_cpu()) {
1823     Register xlo = opr1-&gt;as_register_lo();
1824     Register xhi = opr1-&gt;as_register_hi();
1825     if (opr2-&gt;is_constant() &amp;&amp; opr2-&gt;as_jlong() == 0) {
1826       assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;cannot handle otherwise&quot;);
1827       __ orrs(Rtemp, xlo, xhi);
1828     } else if (opr2-&gt;is_register()) {
1829       Register ylo = opr2-&gt;as_register_lo();
1830       Register yhi = opr2-&gt;as_register_hi();
1831       if (condition == lir_cond_equal || condition == lir_cond_notEqual) {
1832         __ teq(xhi, yhi);
1833         __ teq(xlo, ylo, eq);
1834       } else {
1835         __ subs(xlo, xlo, ylo);
1836         __ sbcs(xhi, xhi, yhi);
1837       }
1838     } else {
1839       ShouldNotReachHere();
1840     }
1841   } else if (opr1-&gt;is_single_fpu()) {
1842     if (opr2-&gt;is_constant()) {
1843       assert(opr2-&gt;as_jfloat() == 0.0f, &quot;cannot handle otherwise&quot;);
1844       __ cmp_zero_float(opr1-&gt;as_float_reg());
1845     } else {
1846       __ cmp_float(opr1-&gt;as_float_reg(), opr2-&gt;as_float_reg());
1847     }
1848   } else if (opr1-&gt;is_double_fpu()) {
1849     if (opr2-&gt;is_constant()) {
1850       assert(opr2-&gt;as_jdouble() == 0.0, &quot;cannot handle otherwise&quot;);
1851       __ cmp_zero_double(opr1-&gt;as_double_reg());
1852     } else {
1853       __ cmp_double(opr1-&gt;as_double_reg(), opr2-&gt;as_double_reg());
1854     }
1855   } else {
1856     ShouldNotReachHere();
1857   }
1858 }
1859 
1860 void LIR_Assembler::comp_fl2i(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst, LIR_Op2* op) {
1861   const Register res = dst-&gt;as_register();
1862   if (code == lir_cmp_fd2i || code == lir_ucmp_fd2i) {
1863     comp_op(lir_cond_unknown, left, right, op);
1864     __ fmstat();
1865     if (code == lir_ucmp_fd2i) {  // unordered is less
1866       __ mvn(res, 0, lt);
1867       __ mov(res, 1, ge);
1868     } else {                      // unordered is greater
1869       __ mov(res, 1, cs);
1870       __ mvn(res, 0, cc);
1871     }
1872     __ mov(res, 0, eq);
1873 
1874   } else {
1875     assert(code == lir_cmp_l2i, &quot;must be&quot;);
1876 
1877     Label done;
1878     const Register xlo = left-&gt;as_register_lo();
1879     const Register xhi = left-&gt;as_register_hi();
1880     const Register ylo = right-&gt;as_register_lo();
1881     const Register yhi = right-&gt;as_register_hi();
1882     __ cmp(xhi, yhi);
1883     __ mov(res, 1, gt);
1884     __ mvn(res, 0, lt);
1885     __ b(done, ne);
1886     __ subs(res, xlo, ylo);
1887     __ mov(res, 1, hi);
1888     __ mvn(res, 0, lo);
1889     __ bind(done);
1890   }
1891 }
1892 
1893 
1894 void LIR_Assembler::align_call(LIR_Code code) {
1895   // Not needed
1896 }
1897 
1898 
1899 void LIR_Assembler::call(LIR_OpJavaCall *op, relocInfo::relocType rtype) {
1900   int ret_addr_offset = __ patchable_call(op-&gt;addr(), rtype);
1901   assert(ret_addr_offset == __ offset(), &quot;embedded return address not allowed&quot;);
1902   add_call_info_here(op-&gt;info());
1903 }
1904 
1905 
1906 void LIR_Assembler::ic_call(LIR_OpJavaCall *op) {
1907   bool near_range = __ cache_fully_reachable();
1908   address oop_address = pc();
1909 
1910   bool use_movw = VM_Version::supports_movw();
1911 
1912   // Ricklass may contain something that is not a metadata pointer so
1913   // mov_metadata can&#39;t be used
1914   InlinedAddress value((address)Universe::non_oop_word());
1915   InlinedAddress addr(op-&gt;addr());
1916   if (use_movw) {
1917     __ movw(Ricklass, ((unsigned int)Universe::non_oop_word()) &amp; 0xffff);
1918     __ movt(Ricklass, ((unsigned int)Universe::non_oop_word()) &gt;&gt; 16);
1919   } else {
1920     // No movw/movt, must be load a pc relative value but no
1921     // relocation so no metadata table to load from.
1922     // Use a b instruction rather than a bl, inline constant after the
1923     // branch, use a PC relative ldr to load the constant, arrange for
1924     // the call to return after the constant(s).
1925     __ ldr_literal(Ricklass, value);
1926   }
1927   __ relocate(virtual_call_Relocation::spec(oop_address));
1928   if (near_range &amp;&amp; use_movw) {
1929     __ bl(op-&gt;addr());
1930   } else {
1931     Label call_return;
1932     __ adr(LR, call_return);
1933     if (near_range) {
1934       __ b(op-&gt;addr());
1935     } else {
1936       __ indirect_jump(addr, Rtemp);
1937       __ bind_literal(addr);
1938     }
1939     if (!use_movw) {
1940       __ bind_literal(value);
1941     }
1942     __ bind(call_return);
1943   }
1944   add_call_info(code_offset(), op-&gt;info());
1945 }
1946 
1947 
1948 /* Currently, vtable-dispatch is only enabled for sparc platforms */
1949 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
1950   ShouldNotReachHere();
1951 }
1952 
1953 void LIR_Assembler::emit_static_call_stub() {
1954   address call_pc = __ pc();
1955   address stub = __ start_a_stub(call_stub_size());
1956   if (stub == NULL) {
1957     BAILOUT(&quot;static call stub overflow&quot;);
1958   }
1959 
1960   DEBUG_ONLY(int offset = code_offset();)
1961 
1962   InlinedMetadata metadata_literal(NULL);
1963   __ relocate(static_stub_Relocation::spec(call_pc));
1964   // If not a single instruction, NativeMovConstReg::next_instruction_address()
1965   // must jump over the whole following ldr_literal.
1966   // (See CompiledStaticCall::set_to_interpreted())
1967 #ifdef ASSERT
1968   address ldr_site = __ pc();
1969 #endif
1970   __ ldr_literal(Rmethod, metadata_literal);
1971   assert(nativeMovConstReg_at(ldr_site)-&gt;next_instruction_address() == __ pc(), &quot;Fix ldr_literal or its parsing&quot;);
1972   bool near_range = __ cache_fully_reachable();
1973   InlinedAddress dest((address)-1);
1974   if (near_range) {
1975     address branch_site = __ pc();
1976     __ b(branch_site); // b to self maps to special NativeJump -1 destination
1977   } else {
1978     __ indirect_jump(dest, Rtemp);
1979   }
1980   __ bind_literal(metadata_literal); // includes spec_for_immediate reloc
1981   if (!near_range) {
1982     __ bind_literal(dest); // special NativeJump -1 destination
1983   }
1984 
1985   assert(code_offset() - offset &lt;= call_stub_size(), &quot;overflow&quot;);
1986   __ end_a_stub();
1987 }
1988 
1989 void LIR_Assembler::throw_op(LIR_Opr exceptionPC, LIR_Opr exceptionOop, CodeEmitInfo* info) {
1990   assert(exceptionOop-&gt;as_register() == Rexception_obj, &quot;must match&quot;);
1991   assert(exceptionPC-&gt;as_register()  == Rexception_pc, &quot;must match&quot;);
1992   info-&gt;add_register_oop(exceptionOop);
1993 
1994   Runtime1::StubID handle_id = compilation()-&gt;has_fpu_code() ?
1995                                Runtime1::handle_exception_id :
1996                                Runtime1::handle_exception_nofpu_id;
1997   Label return_address;
1998   __ adr(Rexception_pc, return_address);
1999   __ call(Runtime1::entry_for(handle_id), relocInfo::runtime_call_type);
2000   __ bind(return_address);
2001   add_call_info_here(info);  // for exception handler
2002 }
2003 
2004 void LIR_Assembler::unwind_op(LIR_Opr exceptionOop) {
2005   assert(exceptionOop-&gt;as_register() == Rexception_obj, &quot;must match&quot;);
2006   __ b(_unwind_handler_entry);
2007 }
2008 
2009 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, LIR_Opr count, LIR_Opr dest, LIR_Opr tmp) {
2010   AsmShift shift = lsl;
2011   switch (code) {
2012     case lir_shl:  shift = lsl; break;
2013     case lir_shr:  shift = asr; break;
2014     case lir_ushr: shift = lsr; break;
2015     default: ShouldNotReachHere();
2016   }
2017 
2018   if (dest-&gt;is_single_cpu()) {
2019     __ andr(Rtemp, count-&gt;as_register(), 31);
2020     __ mov(dest-&gt;as_register(), AsmOperand(left-&gt;as_register(), shift, Rtemp));
2021   } else if (dest-&gt;is_double_cpu()) {
2022     Register dest_lo = dest-&gt;as_register_lo();
2023     Register dest_hi = dest-&gt;as_register_hi();
2024     Register src_lo  = left-&gt;as_register_lo();
2025     Register src_hi  = left-&gt;as_register_hi();
2026     Register Rcount  = count-&gt;as_register();
2027     // Resolve possible register conflicts
2028     if (shift == lsl &amp;&amp; dest_hi == src_lo) {
2029       dest_hi = Rtemp;
2030     } else if (shift != lsl &amp;&amp; dest_lo == src_hi) {
2031       dest_lo = Rtemp;
2032     } else if (dest_lo == src_lo &amp;&amp; dest_hi == src_hi) {
2033       dest_lo = Rtemp;
2034     } else if (dest_lo == Rcount || dest_hi == Rcount) {
2035       Rcount = Rtemp;
2036     }
2037     __ andr(Rcount, count-&gt;as_register(), 63);
2038     __ long_shift(dest_lo, dest_hi, src_lo, src_hi, shift, Rcount);
2039     move_regs(dest_lo, dest-&gt;as_register_lo());
2040     move_regs(dest_hi, dest-&gt;as_register_hi());
2041   } else {
2042     ShouldNotReachHere();
2043   }
2044 }
2045 
2046 
2047 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, jint count, LIR_Opr dest) {
2048   AsmShift shift = lsl;
2049   switch (code) {
2050     case lir_shl:  shift = lsl; break;
2051     case lir_shr:  shift = asr; break;
2052     case lir_ushr: shift = lsr; break;
2053     default: ShouldNotReachHere();
2054   }
2055 
2056   if (dest-&gt;is_single_cpu()) {
2057     count &amp;= 31;
2058     if (count != 0) {
2059       __ mov(dest-&gt;as_register(), AsmOperand(left-&gt;as_register(), shift, count));
2060     } else {
2061       move_regs(left-&gt;as_register(), dest-&gt;as_register());
2062     }
2063   } else if (dest-&gt;is_double_cpu()) {
2064     count &amp;= 63;
2065     if (count != 0) {
2066       Register dest_lo = dest-&gt;as_register_lo();
2067       Register dest_hi = dest-&gt;as_register_hi();
2068       Register src_lo  = left-&gt;as_register_lo();
2069       Register src_hi  = left-&gt;as_register_hi();
2070       // Resolve possible register conflicts
2071       if (shift == lsl &amp;&amp; dest_hi == src_lo) {
2072         dest_hi = Rtemp;
2073       } else if (shift != lsl &amp;&amp; dest_lo == src_hi) {
2074         dest_lo = Rtemp;
2075       }
2076       __ long_shift(dest_lo, dest_hi, src_lo, src_hi, shift, count);
2077       move_regs(dest_lo, dest-&gt;as_register_lo());
2078       move_regs(dest_hi, dest-&gt;as_register_hi());
2079     } else {
2080       __ long_move(dest-&gt;as_register_lo(), dest-&gt;as_register_hi(),
2081                    left-&gt;as_register_lo(), left-&gt;as_register_hi());
2082     }
2083   } else {
2084     ShouldNotReachHere();
2085   }
2086 }
2087 
2088 
2089 // Saves 4 given registers in reserved argument area.
2090 void LIR_Assembler::save_in_reserved_area(Register r1, Register r2, Register r3, Register r4) {
2091   verify_reserved_argument_area_size(4);
2092   __ stmia(SP, RegisterSet(r1) | RegisterSet(r2) | RegisterSet(r3) | RegisterSet(r4));
2093 }
2094 
2095 // Restores 4 given registers from reserved argument area.
2096 void LIR_Assembler::restore_from_reserved_area(Register r1, Register r2, Register r3, Register r4) {
2097   __ ldmia(SP, RegisterSet(r1) | RegisterSet(r2) | RegisterSet(r3) | RegisterSet(r4), no_writeback);
2098 }
2099 
2100 
2101 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
2102   ciArrayKlass* default_type = op-&gt;expected_type();
2103   Register src = op-&gt;src()-&gt;as_register();
2104   Register src_pos = op-&gt;src_pos()-&gt;as_register();
2105   Register dst = op-&gt;dst()-&gt;as_register();
2106   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
2107   Register length  = op-&gt;length()-&gt;as_register();
2108   Register tmp = op-&gt;tmp()-&gt;as_register();
2109   Register tmp2 = Rtemp;
2110 
2111   assert(src == R0 &amp;&amp; src_pos == R1 &amp;&amp; dst == R2 &amp;&amp; dst_pos == R3, &quot;code assumption&quot;);
2112 
2113   __ resolve(ACCESS_READ, src);
2114   __ resolve(ACCESS_WRITE, dst);
2115 
2116   CodeStub* stub = op-&gt;stub();
2117 
2118   int flags = op-&gt;flags();
2119   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
2120   if (basic_type == T_ARRAY) basic_type = T_OBJECT;
2121 
2122   // If we don&#39;t know anything or it&#39;s an object array, just go through the generic arraycopy
2123   if (default_type == NULL) {
2124 
2125     // save arguments, because they will be killed by a runtime call
2126     save_in_reserved_area(R0, R1, R2, R3);
2127 
2128     // pass length argument on SP[0]
2129     __ str(length, Address(SP, -2*wordSize, pre_indexed));  // 2 words for a proper stack alignment
2130 
2131     address copyfunc_addr = StubRoutines::generic_arraycopy();
2132     assert(copyfunc_addr != NULL, &quot;generic arraycopy stub required&quot;);
2133 #ifndef PRODUCT
2134     if (PrintC1Statistics) {
2135       __ inc_counter((address)&amp;Runtime1::_generic_arraycopystub_cnt, tmp, tmp2);
2136     }
2137 #endif // !PRODUCT
2138     // the stub is in the code cache so close enough
2139     __ call(copyfunc_addr, relocInfo::runtime_call_type);
2140 
2141     __ add(SP, SP, 2*wordSize);
2142 
2143     __ cbz_32(R0, *stub-&gt;continuation());
2144 
2145     __ mvn_32(tmp, R0);
2146     restore_from_reserved_area(R0, R1, R2, R3);  // load saved arguments in slow case only
2147     __ sub_32(length, length, tmp);
2148     __ add_32(src_pos, src_pos, tmp);
2149     __ add_32(dst_pos, dst_pos, tmp);
2150 
2151     __ b(*stub-&gt;entry());
2152 
2153     __ bind(*stub-&gt;continuation());
2154     return;
2155   }
2156 
2157   assert(default_type != NULL &amp;&amp; default_type-&gt;is_array_klass() &amp;&amp; default_type-&gt;is_loaded(),
2158          &quot;must be true at this point&quot;);
2159   int elem_size = type2aelembytes(basic_type);
2160   int shift = exact_log2(elem_size);
2161 
2162   // Check for NULL
2163   if (flags &amp; LIR_OpArrayCopy::src_null_check) {
2164     if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
2165       __ cmp(src, 0);
2166       __ cond_cmp(dst, 0, ne);  // make one instruction shorter if both checks are needed
2167       __ b(*stub-&gt;entry(), eq);
2168     } else {
2169       __ cbz(src, *stub-&gt;entry());
2170     }
2171   } else if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
2172     __ cbz(dst, *stub-&gt;entry());
2173   }
2174 
2175   // If the compiler was not able to prove that exact type of the source or the destination
2176   // of the arraycopy is an array type, check at runtime if the source or the destination is
2177   // an instance type.
2178   if (flags &amp; LIR_OpArrayCopy::type_check) {
2179     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::dst_objarray)) {
2180       __ load_klass(tmp, dst);
2181       __ ldr_u32(tmp2, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2182       __ mov_slow(tmp, Klass::_lh_neutral_value);
2183       __ cmp_32(tmp2, tmp);
2184       __ b(*stub-&gt;entry(), ge);
2185     }
2186 
2187     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::src_objarray)) {
2188       __ load_klass(tmp, src);
2189       __ ldr_u32(tmp2, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2190       __ mov_slow(tmp, Klass::_lh_neutral_value);
2191       __ cmp_32(tmp2, tmp);
2192       __ b(*stub-&gt;entry(), ge);
2193     }
2194   }
2195 
2196   // Check if negative
2197   const int all_positive_checks = LIR_OpArrayCopy::src_pos_positive_check |
2198                                   LIR_OpArrayCopy::dst_pos_positive_check |
2199                                   LIR_OpArrayCopy::length_positive_check;
2200   switch (flags &amp; all_positive_checks) {
2201     case LIR_OpArrayCopy::src_pos_positive_check:
2202       __ branch_if_negative_32(src_pos, *stub-&gt;entry());
2203       break;
2204     case LIR_OpArrayCopy::dst_pos_positive_check:
2205       __ branch_if_negative_32(dst_pos, *stub-&gt;entry());
2206       break;
2207     case LIR_OpArrayCopy::length_positive_check:
2208       __ branch_if_negative_32(length, *stub-&gt;entry());
2209       break;
2210     case LIR_OpArrayCopy::src_pos_positive_check | LIR_OpArrayCopy::dst_pos_positive_check:
2211       __ branch_if_any_negative_32(src_pos, dst_pos, tmp, *stub-&gt;entry());
2212       break;
2213     case LIR_OpArrayCopy::src_pos_positive_check | LIR_OpArrayCopy::length_positive_check:
2214       __ branch_if_any_negative_32(src_pos, length, tmp, *stub-&gt;entry());
2215       break;
2216     case LIR_OpArrayCopy::dst_pos_positive_check | LIR_OpArrayCopy::length_positive_check:
2217       __ branch_if_any_negative_32(dst_pos, length, tmp, *stub-&gt;entry());
2218       break;
2219     case all_positive_checks:
2220       __ branch_if_any_negative_32(src_pos, dst_pos, length, tmp, *stub-&gt;entry());
2221       break;
2222     default:
2223       assert((flags &amp; all_positive_checks) == 0, &quot;the last option&quot;);
2224   }
2225 
2226   // Range checks
2227   if (flags &amp; LIR_OpArrayCopy::src_range_check) {
2228     __ ldr_s32(tmp2, Address(src, arrayOopDesc::length_offset_in_bytes()));
2229     __ add_32(tmp, src_pos, length);
2230     __ cmp_32(tmp, tmp2);
2231     __ b(*stub-&gt;entry(), hi);
2232   }
2233   if (flags &amp; LIR_OpArrayCopy::dst_range_check) {
2234     __ ldr_s32(tmp2, Address(dst, arrayOopDesc::length_offset_in_bytes()));
2235     __ add_32(tmp, dst_pos, length);
2236     __ cmp_32(tmp, tmp2);
2237     __ b(*stub-&gt;entry(), hi);
2238   }
2239 
2240   // Check if src and dst are of the same type
2241   if (flags &amp; LIR_OpArrayCopy::type_check) {
2242     // We don&#39;t know the array types are compatible
2243     if (basic_type != T_OBJECT) {
2244       // Simple test for basic type arrays
2245       if (UseCompressedClassPointers) {
2246         // We don&#39;t need decode because we just need to compare
2247         __ ldr_u32(tmp, Address(src, oopDesc::klass_offset_in_bytes()));
2248         __ ldr_u32(tmp2, Address(dst, oopDesc::klass_offset_in_bytes()));
2249         __ cmp_32(tmp, tmp2);
2250       } else {
2251         __ load_klass(tmp, src);
2252         __ load_klass(tmp2, dst);
2253         __ cmp(tmp, tmp2);
2254       }
2255       __ b(*stub-&gt;entry(), ne);
2256     } else {
2257       // For object arrays, if src is a sub class of dst then we can
2258       // safely do the copy.
2259       Label cont, slow;
2260 
2261       address copyfunc_addr = StubRoutines::checkcast_arraycopy();
2262 
2263       __ load_klass(tmp, src);
2264       __ load_klass(tmp2, dst);
2265 
2266       // We are at a call so all live registers are saved before we
2267       // get here
2268       assert_different_registers(tmp, tmp2, R6, altFP_7_11);
2269 
2270       __ check_klass_subtype_fast_path(tmp, tmp2, R6, altFP_7_11, &amp;cont, copyfunc_addr == NULL ? stub-&gt;entry() : &amp;slow, NULL);
2271 
2272       __ mov(R6, R0);
2273       __ mov(altFP_7_11, R1);
2274       __ mov(R0, tmp);
2275       __ mov(R1, tmp2);
2276       __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type); // does not blow any registers except R0, LR and Rtemp
2277       __ cmp_32(R0, 0);
2278       __ mov(R0, R6);
2279       __ mov(R1, altFP_7_11);
2280 
2281       if (copyfunc_addr != NULL) { // use stub if available
2282         // src is not a sub class of dst so we have to do a
2283         // per-element check.
2284 
2285         __ b(cont, ne);
2286 
2287         __ bind(slow);
2288 
2289         int mask = LIR_OpArrayCopy::src_objarray|LIR_OpArrayCopy::dst_objarray;
2290         if ((flags &amp; mask) != mask) {
2291           // Check that at least both of them object arrays.
2292           assert(flags &amp; mask, &quot;one of the two should be known to be an object array&quot;);
2293 
2294           if (!(flags &amp; LIR_OpArrayCopy::src_objarray)) {
2295             __ load_klass(tmp, src);
2296           } else if (!(flags &amp; LIR_OpArrayCopy::dst_objarray)) {
2297             __ load_klass(tmp, dst);
2298           }
2299           int lh_offset = in_bytes(Klass::layout_helper_offset());
2300 
2301           __ ldr_u32(tmp2, Address(tmp, lh_offset));
2302 
2303           jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2304           __ mov_slow(tmp, objArray_lh);
2305           __ cmp_32(tmp, tmp2);
2306           __ b(*stub-&gt;entry(), ne);
2307         }
2308 
2309         save_in_reserved_area(R0, R1, R2, R3);
2310 
2311         Register src_ptr = R0;
2312         Register dst_ptr = R1;
2313         Register len     = R2;
2314         Register chk_off = R3;
2315         Register super_k = tmp;
2316 
2317         __ add(src_ptr, src, arrayOopDesc::base_offset_in_bytes(basic_type));
2318         __ add_ptr_scaled_int32(src_ptr, src_ptr, src_pos, shift);
2319 
2320         __ add(dst_ptr, dst, arrayOopDesc::base_offset_in_bytes(basic_type));
2321         __ add_ptr_scaled_int32(dst_ptr, dst_ptr, dst_pos, shift);
2322         __ load_klass(tmp, dst);
2323 
2324         int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
2325         int sco_offset = in_bytes(Klass::super_check_offset_offset());
2326 
2327         __ ldr(super_k, Address(tmp, ek_offset));
2328 
2329         __ mov(len, length);
2330         __ ldr_u32(chk_off, Address(super_k, sco_offset));
2331         __ push(super_k);
2332 
2333         __ call(copyfunc_addr, relocInfo::runtime_call_type);
2334 
2335 #ifndef PRODUCT
2336         if (PrintC1Statistics) {
2337           Label failed;
2338           __ cbnz_32(R0, failed);
2339           __ inc_counter((address)&amp;Runtime1::_arraycopy_checkcast_cnt, tmp, tmp2);
2340           __ bind(failed);
2341         }
2342 #endif // PRODUCT
2343 
2344         __ add(SP, SP, wordSize);  // Drop super_k argument
2345 
2346         __ cbz_32(R0, *stub-&gt;continuation());
2347         __ mvn_32(tmp, R0);
2348 
2349         // load saved arguments in slow case only
2350         restore_from_reserved_area(R0, R1, R2, R3);
2351 
2352         __ sub_32(length, length, tmp);
2353         __ add_32(src_pos, src_pos, tmp);
2354         __ add_32(dst_pos, dst_pos, tmp);
2355 
2356 #ifndef PRODUCT
2357         if (PrintC1Statistics) {
2358           __ inc_counter((address)&amp;Runtime1::_arraycopy_checkcast_attempt_cnt, tmp, tmp2);
2359         }
2360 #endif
2361 
2362         __ b(*stub-&gt;entry());
2363 
2364         __ bind(cont);
2365       } else {
2366         __ b(*stub-&gt;entry(), eq);
2367         __ bind(cont);
2368       }
2369     }
2370   }
2371 
2372 #ifndef PRODUCT
2373   if (PrintC1Statistics) {
2374     address counter = Runtime1::arraycopy_count_address(basic_type);
2375     __ inc_counter(counter, tmp, tmp2);
2376   }
2377 #endif // !PRODUCT
2378 
2379   bool disjoint = (flags &amp; LIR_OpArrayCopy::overlapping) == 0;
2380   bool aligned = (flags &amp; LIR_OpArrayCopy::unaligned) == 0;
2381   const char *name;
2382   address entry = StubRoutines::select_arraycopy_function(basic_type, aligned, disjoint, name, false);
2383 
2384   Register src_ptr = R0;
2385   Register dst_ptr = R1;
2386   Register len     = R2;
2387 
2388   __ add(src_ptr, src, arrayOopDesc::base_offset_in_bytes(basic_type));
2389   __ add_ptr_scaled_int32(src_ptr, src_ptr, src_pos, shift);
2390 
2391   __ add(dst_ptr, dst, arrayOopDesc::base_offset_in_bytes(basic_type));
2392   __ add_ptr_scaled_int32(dst_ptr, dst_ptr, dst_pos, shift);
2393 
2394   __ mov(len, length);
2395 
2396   __ call(entry, relocInfo::runtime_call_type);
2397 
2398   __ bind(*stub-&gt;continuation());
2399 }
2400 
2401 #ifdef ASSERT
2402  // emit run-time assertion
2403 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
2404   assert(op-&gt;code() == lir_assert, &quot;must be&quot;);
2405 
2406   if (op-&gt;in_opr1()-&gt;is_valid()) {
2407     assert(op-&gt;in_opr2()-&gt;is_valid(), &quot;both operands must be valid&quot;);
2408     comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
2409   } else {
2410     assert(op-&gt;in_opr2()-&gt;is_illegal(), &quot;both operands must be illegal&quot;);
2411     assert(op-&gt;condition() == lir_cond_always, &quot;no other conditions allowed&quot;);
2412   }
2413 
2414   Label ok;
2415   if (op-&gt;condition() != lir_cond_always) {
2416     AsmCondition acond = al;
2417     switch (op-&gt;condition()) {
2418       case lir_cond_equal:        acond = eq; break;
2419       case lir_cond_notEqual:     acond = ne; break;
2420       case lir_cond_less:         acond = lt; break;
2421       case lir_cond_lessEqual:    acond = le; break;
2422       case lir_cond_greaterEqual: acond = ge; break;
2423       case lir_cond_greater:      acond = gt; break;
2424       case lir_cond_aboveEqual:   acond = hs; break;
2425       case lir_cond_belowEqual:   acond = ls; break;
2426       default:                    ShouldNotReachHere();
2427     }
2428     __ b(ok, acond);
2429   }
2430   if (op-&gt;halt()) {
2431     const char* str = __ code_string(op-&gt;msg());
2432     __ stop(str);
2433   } else {
2434     breakpoint();
2435   }
2436   __ bind(ok);
2437 }
2438 #endif // ASSERT
2439 
2440 void LIR_Assembler::emit_updatecrc32(LIR_OpUpdateCRC32* op) {
2441   fatal(&quot;CRC32 intrinsic is not implemented on this platform&quot;);
2442 }
2443 
2444 void LIR_Assembler::emit_lock(LIR_OpLock* op) {
2445   Register obj = op-&gt;obj_opr()-&gt;as_pointer_register();
2446   Register hdr = op-&gt;hdr_opr()-&gt;as_pointer_register();
2447   Register lock = op-&gt;lock_opr()-&gt;as_pointer_register();
2448   Register tmp = op-&gt;scratch_opr()-&gt;is_illegal() ? noreg :
2449                  op-&gt;scratch_opr()-&gt;as_pointer_register();
2450 
2451   if (!UseFastLocking) {
2452     __ b(*op-&gt;stub()-&gt;entry());
2453   } else if (op-&gt;code() == lir_lock) {
2454     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
2455     __ resolve(ACCESS_READ | ACCESS_WRITE, obj);
2456     int null_check_offset = __ lock_object(hdr, obj, lock, tmp, *op-&gt;stub()-&gt;entry());
2457     if (op-&gt;info() != NULL) {
2458       add_debug_info_for_null_check(null_check_offset, op-&gt;info());
2459     }
2460   } else if (op-&gt;code() == lir_unlock) {
2461     __ unlock_object(hdr, obj, lock, tmp, *op-&gt;stub()-&gt;entry());
2462   } else {
2463     ShouldNotReachHere();
2464   }
2465   __ bind(*op-&gt;stub()-&gt;continuation());
2466 }
2467 
2468 
2469 void LIR_Assembler::emit_profile_call(LIR_OpProfileCall* op) {
2470   ciMethod* method = op-&gt;profiled_method();
2471   int bci          = op-&gt;profiled_bci();
2472   ciMethod* callee = op-&gt;profiled_callee();
2473 
2474   // Update counter for all call types
2475   ciMethodData* md = method-&gt;method_data_or_null();
2476   assert(md != NULL, &quot;Sanity&quot;);
2477   ciProfileData* data = md-&gt;bci_to_data(bci);
2478   assert(data != NULL &amp;&amp; data-&gt;is_CounterData(), &quot;need CounterData for calls&quot;);
2479   assert(op-&gt;mdo()-&gt;is_single_cpu(),  &quot;mdo must be allocated&quot;);
2480   Register mdo  = op-&gt;mdo()-&gt;as_register();
2481   assert(op-&gt;tmp1()-&gt;is_register(), &quot;tmp1 must be allocated&quot;);
2482   Register tmp1 = op-&gt;tmp1()-&gt;as_pointer_register();
2483   assert_different_registers(mdo, tmp1);
2484   __ mov_metadata(mdo, md-&gt;constant_encoding());
2485   int mdo_offset_bias = 0;
2486   int max_offset = 4096;
2487   if (md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) + data-&gt;size_in_bytes() &gt;= max_offset) {
2488     // The offset is large so bias the mdo by the base of the slot so
2489     // that the ldr can use an immediate offset to reference the slots of the data
2490     mdo_offset_bias = md-&gt;byte_offset_of_slot(data, CounterData::count_offset());
2491     __ mov_slow(tmp1, mdo_offset_bias);
2492     __ add(mdo, mdo, tmp1);
2493   }
2494 
2495   Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) - mdo_offset_bias);
2496   // Perform additional virtual call profiling for invokevirtual and
2497   // invokeinterface bytecodes
2498   if (op-&gt;should_profile_receiver_type()) {
2499     assert(op-&gt;recv()-&gt;is_single_cpu(), &quot;recv must be allocated&quot;);
2500     Register recv = op-&gt;recv()-&gt;as_register();
2501     assert_different_registers(mdo, tmp1, recv);
2502     assert(data-&gt;is_VirtualCallData(), &quot;need VirtualCallData for virtual calls&quot;);
2503     ciKlass* known_klass = op-&gt;known_holder();
2504     if (C1OptimizeVirtualCallProfiling &amp;&amp; known_klass != NULL) {
2505       // We know the type that will be seen at this call site; we can
2506       // statically update the MethodData* rather than needing to do
2507       // dynamic tests on the receiver type
2508 
2509       // NOTE: we should probably put a lock around this search to
2510       // avoid collisions by concurrent compilations
2511       ciVirtualCallData* vc_data = (ciVirtualCallData*) data;
2512       uint i;
2513       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2514         ciKlass* receiver = vc_data-&gt;receiver(i);
2515         if (known_klass-&gt;equals(receiver)) {
2516           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data,
2517                                                          VirtualCallData::receiver_count_offset(i)) -
2518                             mdo_offset_bias);
2519           __ ldr(tmp1, data_addr);
2520           __ add(tmp1, tmp1, DataLayout::counter_increment);
2521           __ str(tmp1, data_addr);
2522           return;
2523         }
2524       }
2525 
2526       // Receiver type not found in profile data; select an empty slot
2527 
2528       // Note that this is less efficient than it should be because it
2529       // always does a write to the receiver part of the
2530       // VirtualCallData rather than just the first time
2531       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2532         ciKlass* receiver = vc_data-&gt;receiver(i);
2533         if (receiver == NULL) {
2534           Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_offset(i)) -
2535                             mdo_offset_bias);
2536           __ mov_metadata(tmp1, known_klass-&gt;constant_encoding());
2537           __ str(tmp1, recv_addr);
2538           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)) -
2539                             mdo_offset_bias);
2540           __ ldr(tmp1, data_addr);
2541           __ add(tmp1, tmp1, DataLayout::counter_increment);
2542           __ str(tmp1, data_addr);
2543           return;
2544         }
2545       }
2546     } else {
2547       __ load_klass(recv, recv);
2548       Label update_done;
2549       type_profile_helper(mdo, mdo_offset_bias, md, data, recv, tmp1, &amp;update_done);
2550       // Receiver did not match any saved receiver and there is no empty row for it.
2551       // Increment total counter to indicate polymorphic case.
2552       __ ldr(tmp1, counter_addr);
2553       __ add(tmp1, tmp1, DataLayout::counter_increment);
2554       __ str(tmp1, counter_addr);
2555 
2556       __ bind(update_done);
2557     }
2558   } else {
2559     // Static call
2560     __ ldr(tmp1, counter_addr);
2561     __ add(tmp1, tmp1, DataLayout::counter_increment);
2562     __ str(tmp1, counter_addr);
2563   }
2564 }
2565 
2566 void LIR_Assembler::emit_profile_type(LIR_OpProfileType* op) {
2567   fatal(&quot;Type profiling not implemented on this platform&quot;);
2568 }
2569 
2570 void LIR_Assembler::emit_delay(LIR_OpDelay*) {
2571   Unimplemented();
2572 }
2573 
2574 
2575 void LIR_Assembler::monitor_address(int monitor_no, LIR_Opr dst) {
2576   Address mon_addr = frame_map()-&gt;address_for_monitor_lock(monitor_no);
2577   __ add_slow(dst-&gt;as_pointer_register(), mon_addr.base(), mon_addr.disp());
2578 }
2579 
2580 
2581 void LIR_Assembler::align_backward_branch_target() {
2582   // Some ARM processors do better with 8-byte branch target alignment
2583   __ align(8);
2584 }
2585 
2586 
2587 void LIR_Assembler::negate(LIR_Opr left, LIR_Opr dest, LIR_Opr tmp) {
2588   // tmp must be unused
2589   assert(tmp-&gt;is_illegal(), &quot;wasting a register if tmp is allocated&quot;);
2590 
2591   if (left-&gt;is_single_cpu()) {
2592     assert (dest-&gt;type() == T_INT, &quot;unexpected result type&quot;);
2593     assert (left-&gt;type() == T_INT, &quot;unexpected left type&quot;);
2594     __ neg_32(dest-&gt;as_register(), left-&gt;as_register());
2595   } else if (left-&gt;is_double_cpu()) {
2596     Register dest_lo = dest-&gt;as_register_lo();
2597     Register dest_hi = dest-&gt;as_register_hi();
2598     Register src_lo = left-&gt;as_register_lo();
2599     Register src_hi = left-&gt;as_register_hi();
2600     if (dest_lo == src_hi) {
2601       dest_lo = Rtemp;
2602     }
2603     __ rsbs(dest_lo, src_lo, 0);
2604     __ rsc(dest_hi, src_hi, 0);
2605     move_regs(dest_lo, dest-&gt;as_register_lo());
2606   } else if (left-&gt;is_single_fpu()) {
2607     __ neg_float(dest-&gt;as_float_reg(), left-&gt;as_float_reg());
2608   } else if (left-&gt;is_double_fpu()) {
2609     __ neg_double(dest-&gt;as_double_reg(), left-&gt;as_double_reg());
2610   } else {
2611     ShouldNotReachHere();
2612   }
2613 }
2614 
2615 
2616 void LIR_Assembler::leal(LIR_Opr addr_opr, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
2617   assert(patch_code == lir_patch_none, &quot;Patch code not supported&quot;);
2618   LIR_Address* addr = addr_opr-&gt;as_address_ptr();
2619   if (addr-&gt;index()-&gt;is_illegal()) {
2620     jint c = addr-&gt;disp();
2621     if (!Assembler::is_arith_imm_in_range(c)) {
2622       BAILOUT(&quot;illegal arithmetic operand&quot;);
2623     }
2624     __ add(dest-&gt;as_pointer_register(), addr-&gt;base()-&gt;as_pointer_register(), c);
2625   } else {
2626     assert(addr-&gt;disp() == 0, &quot;cannot handle otherwise&quot;);
2627     __ add(dest-&gt;as_pointer_register(), addr-&gt;base()-&gt;as_pointer_register(),
2628            AsmOperand(addr-&gt;index()-&gt;as_pointer_register(), lsl, addr-&gt;scale()));
2629   }
2630 }
2631 
2632 
2633 void LIR_Assembler::rt_call(LIR_Opr result, address dest, const LIR_OprList* args, LIR_Opr tmp, CodeEmitInfo* info) {
2634   assert(!tmp-&gt;is_valid(), &quot;don&#39;t need temporary&quot;);
2635   __ call(dest);
2636   if (info != NULL) {
2637     add_call_info_here(info);
2638   }
2639 }
2640 
2641 
2642 void LIR_Assembler::volatile_move_op(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info) {
2643   assert(src-&gt;is_double_cpu() &amp;&amp; dest-&gt;is_address() ||
2644          src-&gt;is_address() &amp;&amp; dest-&gt;is_double_cpu(),
2645          &quot;Simple move_op is called for all other cases&quot;);
2646 
2647   int null_check_offset;
2648   if (dest-&gt;is_address()) {
2649     // Store
2650     const LIR_Address* addr = dest-&gt;as_address_ptr();
2651     const Register src_lo = src-&gt;as_register_lo();
2652     const Register src_hi = src-&gt;as_register_hi();
2653     assert(addr-&gt;index()-&gt;is_illegal() &amp;&amp; addr-&gt;disp() == 0, &quot;The address is simple already&quot;);
2654 
2655     if (src_lo &lt; src_hi) {
2656       null_check_offset = __ offset();
2657       __ stmia(addr-&gt;base()-&gt;as_register(), RegisterSet(src_lo) | RegisterSet(src_hi));
2658     } else {
2659       assert(src_lo &lt; Rtemp, &quot;Rtemp is higher than any allocatable register&quot;);
2660       __ mov(Rtemp, src_hi);
2661       null_check_offset = __ offset();
2662       __ stmia(addr-&gt;base()-&gt;as_register(), RegisterSet(src_lo) | RegisterSet(Rtemp));
2663     }
2664   } else {
2665     // Load
2666     const LIR_Address* addr = src-&gt;as_address_ptr();
2667     const Register dest_lo = dest-&gt;as_register_lo();
2668     const Register dest_hi = dest-&gt;as_register_hi();
2669     assert(addr-&gt;index()-&gt;is_illegal() &amp;&amp; addr-&gt;disp() == 0, &quot;The address is simple already&quot;);
2670 
2671     null_check_offset = __ offset();
2672     if (dest_lo &lt; dest_hi) {
2673       __ ldmia(addr-&gt;base()-&gt;as_register(), RegisterSet(dest_lo) | RegisterSet(dest_hi));
2674     } else {
2675       assert(dest_lo &lt; Rtemp, &quot;Rtemp is higher than any allocatable register&quot;);
2676       __ ldmia(addr-&gt;base()-&gt;as_register(), RegisterSet(dest_lo) | RegisterSet(Rtemp));
2677       __ mov(dest_hi, Rtemp);
2678     }
2679   }
2680 
2681   if (info != NULL) {
2682     add_debug_info_for_null_check(null_check_offset, info);
2683   }
2684 }
2685 
2686 
2687 void LIR_Assembler::membar() {
2688   __ membar(MacroAssembler::StoreLoad, Rtemp);
2689 }
2690 
2691 void LIR_Assembler::membar_acquire() {
2692   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);
2693 }
2694 
2695 void LIR_Assembler::membar_release() {
2696   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
2697 }
2698 
2699 void LIR_Assembler::membar_loadload() {
2700   __ membar(MacroAssembler::LoadLoad, Rtemp);
2701 }
2702 
2703 void LIR_Assembler::membar_storestore() {
2704   __ membar(MacroAssembler::StoreStore, Rtemp);
2705 }
2706 
2707 void LIR_Assembler::membar_loadstore() {
2708   __ membar(MacroAssembler::LoadStore, Rtemp);
2709 }
2710 
2711 void LIR_Assembler::membar_storeload() {
2712   __ membar(MacroAssembler::StoreLoad, Rtemp);
2713 }
2714 
2715 void LIR_Assembler::on_spin_wait() {
2716   Unimplemented();
2717 }
2718 
2719 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
2720   // Not used on ARM
2721   Unimplemented();
2722 }
2723 
2724 void LIR_Assembler::peephole(LIR_List* lir) {
2725   LIR_OpList* inst = lir-&gt;instructions_list();
2726   const int inst_length = inst-&gt;length();
2727   for (int i = 0; i &lt; inst_length; i++) {
2728     LIR_Op* op = inst-&gt;at(i);
2729     switch (op-&gt;code()) {
2730       case lir_cmp: {
2731         // Replace:
2732         //   cmp rX, y
2733         //   cmove [EQ] y, z, rX
2734         // with
2735         //   cmp rX, y
2736         //   cmove [EQ] illegalOpr, z, rX
2737         //
2738         // or
2739         //   cmp rX, y
2740         //   cmove [NE] z, y, rX
2741         // with
2742         //   cmp rX, y
2743         //   cmove [NE] z, illegalOpr, rX
2744         //
2745         // moves from illegalOpr should be removed when converting LIR to native assembly
2746 
2747         LIR_Op2* cmp = op-&gt;as_Op2();
2748         assert(cmp != NULL, &quot;cmp LIR instruction is not an op2&quot;);
2749 
2750         if (i + 1 &lt; inst_length) {
2751           LIR_Op2* cmove = inst-&gt;at(i + 1)-&gt;as_Op2();
2752           if (cmove != NULL &amp;&amp; cmove-&gt;code() == lir_cmove) {
2753             LIR_Opr cmove_res = cmove-&gt;result_opr();
2754             bool res_is_op1 = cmove_res == cmp-&gt;in_opr1();
2755             bool res_is_op2 = cmove_res == cmp-&gt;in_opr2();
2756             LIR_Opr cmp_res, cmp_arg;
2757             if (res_is_op1) {
2758               cmp_res = cmp-&gt;in_opr1();
2759               cmp_arg = cmp-&gt;in_opr2();
2760             } else if (res_is_op2) {
2761               cmp_res = cmp-&gt;in_opr2();
2762               cmp_arg = cmp-&gt;in_opr1();
2763             } else {
2764               cmp_res = LIR_OprFact::illegalOpr;
2765               cmp_arg = LIR_OprFact::illegalOpr;
2766             }
2767 
2768             if (cmp_res != LIR_OprFact::illegalOpr) {
2769               LIR_Condition cond = cmove-&gt;condition();
2770               if (cond == lir_cond_equal &amp;&amp; cmove-&gt;in_opr1() == cmp_arg) {
2771                 cmove-&gt;set_in_opr1(LIR_OprFact::illegalOpr);
2772               } else if (cond == lir_cond_notEqual &amp;&amp; cmove-&gt;in_opr2() == cmp_arg) {
2773                 cmove-&gt;set_in_opr2(LIR_OprFact::illegalOpr);
2774               }
2775             }
2776           }
2777         }
2778         break;
2779       }
2780 
2781       default:
2782         break;
2783     }
2784   }
2785 }
2786 
2787 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp) {
2788   assert(src-&gt;is_address(), &quot;sanity&quot;);
2789   Address addr = as_Address(src-&gt;as_address_ptr());
2790 
2791   if (code == lir_xchg) {
2792   } else {
2793     assert (!data-&gt;is_oop(), &quot;xadd for oops&quot;);
2794   }
2795 
2796   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
2797 
2798   Label retry;
2799   __ bind(retry);
2800 
2801   if (data-&gt;type() == T_INT || data-&gt;is_oop()) {
2802     Register dst = dest-&gt;as_register();
2803     Register new_val = noreg;
2804     __ ldrex(dst, addr);
2805     if (code == lir_xadd) {
2806       Register tmp_reg = tmp-&gt;as_register();
2807       if (data-&gt;is_constant()) {
2808         assert_different_registers(dst, tmp_reg);
2809         __ add_32(tmp_reg, dst, data-&gt;as_constant_ptr()-&gt;as_jint());
2810       } else {
2811         assert_different_registers(dst, tmp_reg, data-&gt;as_register());
2812         __ add_32(tmp_reg, dst, data-&gt;as_register());
2813       }
2814       new_val = tmp_reg;
2815     } else {
2816       if (UseCompressedOops &amp;&amp; data-&gt;is_oop()) {
2817         new_val = tmp-&gt;as_pointer_register();
2818       } else {
2819         new_val = data-&gt;as_register();
2820       }
2821       assert_different_registers(dst, new_val);
2822     }
2823     __ strex(Rtemp, new_val, addr);
2824 
2825   } else if (data-&gt;type() == T_LONG) {
2826     Register dst_lo = dest-&gt;as_register_lo();
2827     Register new_val_lo = noreg;
2828     Register dst_hi = dest-&gt;as_register_hi();
2829 
2830     assert(dst_hi-&gt;encoding() == dst_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
2831     assert((dst_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
2832 
2833     __ bind(retry);
2834     __ ldrexd(dst_lo, addr);
2835     if (code == lir_xadd) {
2836       Register tmp_lo = tmp-&gt;as_register_lo();
2837       Register tmp_hi = tmp-&gt;as_register_hi();
2838 
2839       assert(tmp_hi-&gt;encoding() == tmp_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
2840       assert((tmp_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
2841 
2842       if (data-&gt;is_constant()) {
2843         jlong c = data-&gt;as_constant_ptr()-&gt;as_jlong();
2844         assert((jlong)((jint)c) == c, &quot;overflow&quot;);
2845         assert_different_registers(dst_lo, dst_hi, tmp_lo, tmp_hi);
2846         __ adds(tmp_lo, dst_lo, (jint)c);
2847         __ adc(tmp_hi, dst_hi, 0);
2848       } else {
2849         Register new_val_lo = data-&gt;as_register_lo();
2850         Register new_val_hi = data-&gt;as_register_hi();
2851         __ adds(tmp_lo, dst_lo, new_val_lo);
2852         __ adc(tmp_hi, dst_hi, new_val_hi);
2853         assert_different_registers(dst_lo, dst_hi, tmp_lo, tmp_hi, new_val_lo, new_val_hi);
2854       }
2855       new_val_lo = tmp_lo;
2856     } else {
2857       new_val_lo = data-&gt;as_register_lo();
2858       Register new_val_hi = data-&gt;as_register_hi();
2859 
2860       assert_different_registers(dst_lo, dst_hi, new_val_lo, new_val_hi);
2861       assert(new_val_hi-&gt;encoding() == new_val_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
2862       assert((new_val_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
2863     }
2864     __ strexd(Rtemp, new_val_lo, addr);
2865   } else {
2866     ShouldNotReachHere();
2867   }
2868 
2869   __ cbnz_32(Rtemp, retry);
2870   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreLoad | MacroAssembler::StoreStore), Rtemp);
2871 
2872 }
2873 
2874 #undef __
    </pre>
  </body>
</html>