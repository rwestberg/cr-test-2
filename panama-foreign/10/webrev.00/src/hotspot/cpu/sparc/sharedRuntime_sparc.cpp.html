<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/sparc/sharedRuntime_sparc.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.inline.hpp&quot;
  27 #include &quot;code/debugInfoRec.hpp&quot;
  28 #include &quot;code/icBuffer.hpp&quot;
  29 #include &quot;code/vtableStubs.hpp&quot;
  30 #include &quot;gc/shared/gcLocker.hpp&quot;
  31 #include &quot;interpreter/interpreter.hpp&quot;
  32 #include &quot;logging/log.hpp&quot;
  33 #include &quot;memory/resourceArea.hpp&quot;
  34 #include &quot;oops/compiledICHolder.hpp&quot;
  35 #include &quot;oops/klass.inline.hpp&quot;
  36 #include &quot;runtime/safepointMechanism.hpp&quot;
  37 #include &quot;runtime/sharedRuntime.hpp&quot;
  38 #include &quot;runtime/vframeArray.hpp&quot;
  39 #include &quot;utilities/align.hpp&quot;
  40 #include &quot;vmreg_sparc.inline.hpp&quot;
  41 #ifdef COMPILER1
  42 #include &quot;c1/c1_Runtime1.hpp&quot;
  43 #endif
  44 #ifdef COMPILER2
  45 #include &quot;opto/runtime.hpp&quot;
  46 #endif
  47 #if INCLUDE_JVMCI
  48 #include &quot;jvmci/jvmciJavaClasses.hpp&quot;
  49 #endif
  50 
  51 #define __ masm-&gt;
  52 
  53 
  54 class RegisterSaver {
  55 
  56   // Used for saving volatile registers. This is Gregs, Fregs, I/L/O.
  57   // The Oregs are problematic. In the 32bit build the compiler can
  58   // have O registers live with 64 bit quantities. A window save will
  59   // cut the heads off of the registers. We have to do a very extensive
  60   // stack dance to save and restore these properly.
  61 
  62   // Note that the Oregs problem only exists if we block at either a polling
  63   // page exception a compiled code safepoint that was not originally a call
  64   // or deoptimize following one of these kinds of safepoints.
  65 
  66   // Lots of registers to save.  For all builds, a window save will preserve
  67   // the %i and %l registers.  For the 32-bit longs-in-two entries and 64-bit
  68   // builds a window-save will preserve the %o registers.  In the LION build
  69   // we need to save the 64-bit %o registers which requires we save them
  70   // before the window-save (as then they become %i registers and get their
  71   // heads chopped off on interrupt).  We have to save some %g registers here
  72   // as well.
  73   enum {
  74     // This frame&#39;s save area.  Includes extra space for the native call:
  75     // vararg&#39;s layout space and the like.  Briefly holds the caller&#39;s
  76     // register save area.
  77     call_args_area = frame::register_save_words_sp_offset +
  78                      frame::memory_parameter_word_sp_offset*wordSize,
  79     // Make sure save locations are always 8 byte aligned.
  80     // can&#39;t use align_up because it doesn&#39;t produce compile time constant
  81     start_of_extra_save_area = ((call_args_area + 7) &amp; ~7),
  82     g1_offset = start_of_extra_save_area, // g-regs needing saving
  83     g3_offset = g1_offset+8,
  84     g4_offset = g3_offset+8,
  85     g5_offset = g4_offset+8,
  86     o0_offset = g5_offset+8,
  87     o1_offset = o0_offset+8,
  88     o2_offset = o1_offset+8,
  89     o3_offset = o2_offset+8,
  90     o4_offset = o3_offset+8,
  91     o5_offset = o4_offset+8,
  92     start_of_flags_save_area = o5_offset+8,
  93     ccr_offset = start_of_flags_save_area,
  94     fsr_offset = ccr_offset + 8,
  95     d00_offset = fsr_offset+8,  // Start of float save area
  96     register_save_size = d00_offset+8*32
  97   };
  98 
  99 
 100   public:
 101 
 102   static int Oexception_offset() { return o0_offset; };
 103   static int G3_offset() { return g3_offset; };
 104   static int G5_offset() { return g5_offset; };
 105   static OopMap* save_live_registers(MacroAssembler* masm, int additional_frame_words, int* total_frame_words);
 106   static void restore_live_registers(MacroAssembler* masm);
 107 
 108   // During deoptimization only the result register need to be restored
 109   // all the other values have already been extracted.
 110 
 111   static void restore_result_registers(MacroAssembler* masm);
 112 };
 113 
 114 OopMap* RegisterSaver::save_live_registers(MacroAssembler* masm, int additional_frame_words, int* total_frame_words) {
 115   // Record volatile registers as callee-save values in an OopMap so their save locations will be
 116   // propagated to the caller frame&#39;s RegisterMap during StackFrameStream construction (needed for
 117   // deoptimization; see compiledVFrame::create_stack_value).  The caller&#39;s I, L and O registers
 118   // are saved in register windows - I&#39;s and L&#39;s in the caller&#39;s frame and O&#39;s in the stub frame
 119   // (as the stub&#39;s I&#39;s) when the runtime routine called by the stub creates its frame.
 120   int i;
 121   // Always make the frame size 16 byte aligned.
 122   int frame_size = align_up(additional_frame_words + register_save_size, 16);
 123   // OopMap frame size is in c2 stack slots (sizeof(jint)) not bytes or words
 124   int frame_size_in_slots = frame_size / sizeof(jint);
 125   // CodeBlob frame size is in words.
 126   *total_frame_words = frame_size / wordSize;
 127   // OopMap* map = new OopMap(*total_frame_words, 0);
 128   OopMap* map = new OopMap(frame_size_in_slots, 0);
 129 
 130   __ save(SP, -frame_size, SP);
 131 
 132 
 133   int debug_offset = 0;
 134   // Save the G&#39;s
 135   __ stx(G1, SP, g1_offset+STACK_BIAS);
 136   map-&gt;set_callee_saved(VMRegImpl::stack2reg((g1_offset + debug_offset)&gt;&gt;2), G1-&gt;as_VMReg());
 137 
 138   __ stx(G3, SP, g3_offset+STACK_BIAS);
 139   map-&gt;set_callee_saved(VMRegImpl::stack2reg((g3_offset + debug_offset)&gt;&gt;2), G3-&gt;as_VMReg());
 140 
 141   __ stx(G4, SP, g4_offset+STACK_BIAS);
 142   map-&gt;set_callee_saved(VMRegImpl::stack2reg((g4_offset + debug_offset)&gt;&gt;2), G4-&gt;as_VMReg());
 143 
 144   __ stx(G5, SP, g5_offset+STACK_BIAS);
 145   map-&gt;set_callee_saved(VMRegImpl::stack2reg((g5_offset + debug_offset)&gt;&gt;2), G5-&gt;as_VMReg());
 146 
 147   // This is really a waste but we&#39;ll keep things as they were for now
 148   if (true) {
 149   }
 150 
 151 
 152   // Save the flags
 153   __ rdccr( G5 );
 154   __ stx(G5, SP, ccr_offset+STACK_BIAS);
 155   __ stxfsr(SP, fsr_offset+STACK_BIAS);
 156 
 157   // Save all the FP registers: 32 doubles (32 floats correspond to the 2 halves of the first 16 doubles)
 158   int offset = d00_offset;
 159   for( int i=0; i&lt;FloatRegisterImpl::number_of_registers; i+=2 ) {
 160     FloatRegister f = as_FloatRegister(i);
 161     __ stf(FloatRegisterImpl::D,  f, SP, offset+STACK_BIAS);
 162     // Record as callee saved both halves of double registers (2 float registers).
 163     map-&gt;set_callee_saved(VMRegImpl::stack2reg(offset&gt;&gt;2), f-&gt;as_VMReg());
 164     map-&gt;set_callee_saved(VMRegImpl::stack2reg((offset + sizeof(float))&gt;&gt;2), f-&gt;as_VMReg()-&gt;next());
 165     offset += sizeof(double);
 166   }
 167 
 168   // And we&#39;re done.
 169 
 170   return map;
 171 }
 172 
 173 
 174 // Pop the current frame and restore all the registers that we
 175 // saved.
 176 void RegisterSaver::restore_live_registers(MacroAssembler* masm) {
 177 
 178   // Restore all the FP registers
 179   for( int i=0; i&lt;FloatRegisterImpl::number_of_registers; i+=2 ) {
 180     __ ldf(FloatRegisterImpl::D, SP, d00_offset+i*sizeof(float)+STACK_BIAS, as_FloatRegister(i));
 181   }
 182 
 183   __ ldx(SP, ccr_offset+STACK_BIAS, G1);
 184   __ wrccr (G1) ;
 185 
 186   // Restore the G&#39;s
 187   // Note that G2 (AKA GThread) must be saved and restored separately.
 188   // TODO-FIXME: save and restore some of the other ASRs, viz., %asi and %gsr.
 189 
 190   __ ldx(SP, g1_offset+STACK_BIAS, G1);
 191   __ ldx(SP, g3_offset+STACK_BIAS, G3);
 192   __ ldx(SP, g4_offset+STACK_BIAS, G4);
 193   __ ldx(SP, g5_offset+STACK_BIAS, G5);
 194 
 195   // Restore flags
 196 
 197   __ ldxfsr(SP, fsr_offset+STACK_BIAS);
 198 
 199   __ restore();
 200 
 201 }
 202 
 203 // Pop the current frame and restore the registers that might be holding
 204 // a result.
 205 void RegisterSaver::restore_result_registers(MacroAssembler* masm) {
 206 
 207   __ ldf(FloatRegisterImpl::D, SP, d00_offset+STACK_BIAS, as_FloatRegister(0));
 208 
 209   __ restore();
 210 
 211 }
 212 
 213 // Is vector&#39;s size (in bytes) bigger than a size saved by default?
 214 // 8 bytes FP registers are saved by default on SPARC.
 215 bool SharedRuntime::is_wide_vector(int size) {
 216   // Note, MaxVectorSize == 8 on SPARC.
 217   assert(size &lt;= 8, &quot;%d bytes vectors are not supported&quot;, size);
 218   return size &gt; 8;
 219 }
 220 
 221 size_t SharedRuntime::trampoline_size() {
 222   return 40;
 223 }
 224 
 225 void SharedRuntime::generate_trampoline(MacroAssembler *masm, address destination) {
 226   __ set((intptr_t)destination, G3_scratch);
 227   __ JMP(G3_scratch, 0);
 228   __ delayed()-&gt;nop();
 229 }
 230 
 231 // The java_calling_convention describes stack locations as ideal slots on
 232 // a frame with no abi restrictions. Since we must observe abi restrictions
 233 // (like the placement of the register window) the slots must be biased by
 234 // the following value.
 235 static int reg2offset(VMReg r) {
 236   return (r-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;
 237 }
 238 
 239 static VMRegPair reg64_to_VMRegPair(Register r) {
 240   VMRegPair ret;
 241   if (wordSize == 8) {
 242     ret.set2(r-&gt;as_VMReg());
 243   } else {
 244     ret.set_pair(r-&gt;successor()-&gt;as_VMReg(), r-&gt;as_VMReg());
 245   }
 246   return ret;
 247 }
 248 
 249 // ---------------------------------------------------------------------------
 250 // Read the array of BasicTypes from a signature, and compute where the
 251 // arguments should go.  Values in the VMRegPair regs array refer to 4-byte (VMRegImpl::stack_slot_size)
 252 // quantities.  Values less than VMRegImpl::stack0 are registers, those above
 253 // refer to 4-byte stack slots.  All stack slots are based off of the window
 254 // top.  VMRegImpl::stack0 refers to the first slot past the 16-word window,
 255 // and VMRegImpl::stack0+1 refers to the memory word 4-byes higher.  Register
 256 // values 0-63 (up to RegisterImpl::number_of_registers) are the 64-bit
 257 // integer registers.  Values 64-95 are the (32-bit only) float registers.
 258 // Each 32-bit quantity is given its own number, so the integer registers
 259 // (in either 32- or 64-bit builds) use 2 numbers.  For example, there is
 260 // an O0-low and an O0-high.  Essentially, all int register numbers are doubled.
 261 
 262 // Register results are passed in O0-O5, for outgoing call arguments.  To
 263 // convert to incoming arguments, convert all O&#39;s to I&#39;s.  The regs array
 264 // refer to the low and hi 32-bit words of 64-bit registers or stack slots.
 265 // If the regs[].second() field is set to VMRegImpl::Bad(), it means it&#39;s unused (a
 266 // 32-bit value was passed).  If both are VMRegImpl::Bad(), it means no value was
 267 // passed (used as a placeholder for the other half of longs and doubles in
 268 // the 64-bit build).  regs[].second() is either VMRegImpl::Bad() or regs[].second() is
 269 // regs[].first()+1 (regs[].first() may be misaligned in the C calling convention).
 270 // Sparc never passes a value in regs[].second() but not regs[].first() (regs[].first()
 271 // == VMRegImpl::Bad() &amp;&amp; regs[].second() != VMRegImpl::Bad()) nor unrelated values in the
 272 // same VMRegPair.
 273 
 274 // Note: the INPUTS in sig_bt are in units of Java argument words, which are
 275 // either 32-bit or 64-bit depending on the build.  The OUTPUTS are in 32-bit
 276 // units regardless of build.
 277 
 278 
 279 // ---------------------------------------------------------------------------
 280 // The compiled Java calling convention.  The Java convention always passes
 281 // 64-bit values in adjacent aligned locations (either registers or stack),
 282 // floats in float registers and doubles in aligned float pairs.  There is
 283 // no backing varargs store for values in registers.
 284 // In the 32-bit build, longs are passed on the stack (cannot be
 285 // passed in I&#39;s, because longs in I&#39;s get their heads chopped off at
 286 // interrupt).
 287 int SharedRuntime::java_calling_convention(const BasicType *sig_bt,
 288                                            VMRegPair *regs,
 289                                            int total_args_passed,
 290                                            int is_outgoing) {
 291   assert(F31-&gt;as_VMReg()-&gt;is_reg(), &quot;overlapping stack/register numbers&quot;);
 292 
 293   const int int_reg_max = SPARC_ARGS_IN_REGS_NUM;
 294   const int flt_reg_max = 8;
 295 
 296   int int_reg = 0;
 297   int flt_reg = 0;
 298   int slot = 0;
 299 
 300   for (int i = 0; i &lt; total_args_passed; i++) {
 301     switch (sig_bt[i]) {
 302     case T_INT:
 303     case T_SHORT:
 304     case T_CHAR:
 305     case T_BYTE:
 306     case T_BOOLEAN:
 307       if (int_reg &lt; int_reg_max) {
 308         Register r = is_outgoing ? as_oRegister(int_reg++) : as_iRegister(int_reg++);
 309         regs[i].set1(r-&gt;as_VMReg());
 310       } else {
 311         regs[i].set1(VMRegImpl::stack2reg(slot++));
 312       }
 313       break;
 314 
 315     case T_LONG:
 316       assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i+1] == T_VOID, &quot;expecting VOID in other half&quot;);
 317       // fall-through
 318     case T_OBJECT:
 319     case T_ARRAY:
 320     case T_ADDRESS: // Used, e.g., in slow-path locking for the lock&#39;s stack address
 321       if (int_reg &lt; int_reg_max) {
 322         Register r = is_outgoing ? as_oRegister(int_reg++) : as_iRegister(int_reg++);
 323         regs[i].set2(r-&gt;as_VMReg());
 324       } else {
 325         slot = align_up(slot, 2);  // align
 326         regs[i].set2(VMRegImpl::stack2reg(slot));
 327         slot += 2;
 328       }
 329       break;
 330 
 331     case T_FLOAT:
 332       if (flt_reg &lt; flt_reg_max) {
 333         FloatRegister r = as_FloatRegister(flt_reg++);
 334         regs[i].set1(r-&gt;as_VMReg());
 335       } else {
 336         regs[i].set1(VMRegImpl::stack2reg(slot++));
 337       }
 338       break;
 339 
 340     case T_DOUBLE:
 341       assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i+1] == T_VOID, &quot;expecting half&quot;);
 342       if (align_up(flt_reg, 2) + 1 &lt; flt_reg_max) {
 343         flt_reg = align_up(flt_reg, 2);  // align
 344         FloatRegister r = as_FloatRegister(flt_reg);
 345         regs[i].set2(r-&gt;as_VMReg());
 346         flt_reg += 2;
 347       } else {
 348         slot = align_up(slot, 2);  // align
 349         regs[i].set2(VMRegImpl::stack2reg(slot));
 350         slot += 2;
 351       }
 352       break;
 353 
 354     case T_VOID:
 355       regs[i].set_bad();   // Halves of longs &amp; doubles
 356       break;
 357 
 358     default:
 359       fatal(&quot;unknown basic type %d&quot;, sig_bt[i]);
 360       break;
 361     }
 362   }
 363 
 364   // retun the amount of stack space these arguments will need.
 365   return slot;
 366 }
 367 
 368 // Helper class mostly to avoid passing masm everywhere, and handle
 369 // store displacement overflow logic.
 370 class AdapterGenerator {
 371   MacroAssembler *masm;
 372   Register Rdisp;
 373   void set_Rdisp(Register r)  { Rdisp = r; }
 374 
 375   void patch_callers_callsite();
 376 
 377   // base+st_off points to top of argument
 378   int arg_offset(const int st_off) { return st_off; }
 379   int next_arg_offset(const int st_off) {
 380     return st_off - Interpreter::stackElementSize;
 381   }
 382 
 383   // Argument slot values may be loaded first into a register because
 384   // they might not fit into displacement.
 385   RegisterOrConstant arg_slot(const int st_off);
 386   RegisterOrConstant next_arg_slot(const int st_off);
 387 
 388   // Stores long into offset pointed to by base
 389   void store_c2i_long(Register r, Register base,
 390                       const int st_off, bool is_stack);
 391   void store_c2i_object(Register r, Register base,
 392                         const int st_off);
 393   void store_c2i_int(Register r, Register base,
 394                      const int st_off);
 395   void store_c2i_double(VMReg r_2,
 396                         VMReg r_1, Register base, const int st_off);
 397   void store_c2i_float(FloatRegister f, Register base,
 398                        const int st_off);
 399 
 400  public:
 401   void gen_c2i_adapter(int total_args_passed,
 402                               // VMReg max_arg,
 403                               int comp_args_on_stack, // VMRegStackSlots
 404                               const BasicType *sig_bt,
 405                               const VMRegPair *regs,
 406                               Label&amp; skip_fixup);
 407   void gen_i2c_adapter(int total_args_passed,
 408                        // VMReg max_arg,
 409                        int comp_args_on_stack, // VMRegStackSlots
 410                        const BasicType *sig_bt,
 411                        const VMRegPair *regs);
 412 
 413   AdapterGenerator(MacroAssembler *_masm) : masm(_masm) {}
 414 };
 415 
 416 
 417 // Patch the callers callsite with entry to compiled code if it exists.
 418 void AdapterGenerator::patch_callers_callsite() {
 419   Label L;
 420   __ ld_ptr(G5_method, in_bytes(Method::code_offset()), G3_scratch);
 421   __ br_null(G3_scratch, false, Assembler::pt, L);
 422   __ delayed()-&gt;nop();
 423   // Call into the VM to patch the caller, then jump to compiled callee
 424   __ save_frame(4);     // Args in compiled layout; do not blow them
 425 
 426   // Must save all the live Gregs the list is:
 427   // G1: 1st Long arg (32bit build)
 428   // G2: global allocated to TLS
 429   // G3: used in inline cache check (scratch)
 430   // G4: 2nd Long arg (32bit build);
 431   // G5: used in inline cache check (Method*)
 432 
 433   // The longs must go to the stack by hand since in the 32 bit build they can be trashed by window ops.
 434 
 435   // mov(s,d)
 436   __ mov(G1, L1);
 437   __ mov(G4, L4);
 438   __ mov(G5_method, L5);
 439   __ mov(G5_method, O0);         // VM needs target method
 440   __ mov(I7, O1);                // VM needs caller&#39;s callsite
 441   // Must be a leaf call...
 442   // can be very far once the blob has been relocated
 443   AddressLiteral dest(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite));
 444   __ relocate(relocInfo::runtime_call_type);
 445   __ jumpl_to(dest, O7, O7);
 446   __ delayed()-&gt;mov(G2_thread, L7_thread_cache);
 447   __ mov(L7_thread_cache, G2_thread);
 448   __ mov(L1, G1);
 449   __ mov(L4, G4);
 450   __ mov(L5, G5_method);
 451 
 452   __ restore();      // Restore args
 453   __ bind(L);
 454 }
 455 
 456 
 457 RegisterOrConstant AdapterGenerator::arg_slot(const int st_off) {
 458   RegisterOrConstant roc(arg_offset(st_off));
 459   return __ ensure_simm13_or_reg(roc, Rdisp);
 460 }
 461 
 462 RegisterOrConstant AdapterGenerator::next_arg_slot(const int st_off) {
 463   RegisterOrConstant roc(next_arg_offset(st_off));
 464   return __ ensure_simm13_or_reg(roc, Rdisp);
 465 }
 466 
 467 
 468 // Stores long into offset pointed to by base
 469 void AdapterGenerator::store_c2i_long(Register r, Register base,
 470                                       const int st_off, bool is_stack) {
 471   // In V9, longs are given 2 64-bit slots in the interpreter, but the
 472   // data is passed in only 1 slot.
 473   __ stx(r, base, next_arg_slot(st_off));
 474 }
 475 
 476 void AdapterGenerator::store_c2i_object(Register r, Register base,
 477                       const int st_off) {
 478   __ st_ptr (r, base, arg_slot(st_off));
 479 }
 480 
 481 void AdapterGenerator::store_c2i_int(Register r, Register base,
 482                    const int st_off) {
 483   __ st (r, base, arg_slot(st_off));
 484 }
 485 
 486 // Stores into offset pointed to by base
 487 void AdapterGenerator::store_c2i_double(VMReg r_2,
 488                       VMReg r_1, Register base, const int st_off) {
 489   // In V9, doubles are given 2 64-bit slots in the interpreter, but the
 490   // data is passed in only 1 slot.
 491   __ stf(FloatRegisterImpl::D, r_1-&gt;as_FloatRegister(), base, next_arg_slot(st_off));
 492 }
 493 
 494 void AdapterGenerator::store_c2i_float(FloatRegister f, Register base,
 495                                        const int st_off) {
 496   __ stf(FloatRegisterImpl::S, f, base, arg_slot(st_off));
 497 }
 498 
 499 void AdapterGenerator::gen_c2i_adapter(
 500                             int total_args_passed,
 501                             // VMReg max_arg,
 502                             int comp_args_on_stack, // VMRegStackSlots
 503                             const BasicType *sig_bt,
 504                             const VMRegPair *regs,
 505                             Label&amp; L_skip_fixup) {
 506 
 507   // Before we get into the guts of the C2I adapter, see if we should be here
 508   // at all.  We&#39;ve come from compiled code and are attempting to jump to the
 509   // interpreter, which means the caller made a static call to get here
 510   // (vcalls always get a compiled target if there is one).  Check for a
 511   // compiled target.  If there is one, we need to patch the caller&#39;s call.
 512   // However we will run interpreted if we come thru here. The next pass
 513   // thru the call site will run compiled. If we ran compiled here then
 514   // we can (theorectically) do endless i2c-&gt;c2i-&gt;i2c transitions during
 515   // deopt/uncommon trap cycles. If we always go interpreted here then
 516   // we can have at most one and don&#39;t need to play any tricks to keep
 517   // from endlessly growing the stack.
 518   //
 519   // Actually if we detected that we had an i2c-&gt;c2i transition here we
 520   // ought to be able to reset the world back to the state of the interpreted
 521   // call and not bother building another interpreter arg area. We don&#39;t
 522   // do that at this point.
 523 
 524   patch_callers_callsite();
 525 
 526   __ bind(L_skip_fixup);
 527 
 528   // Since all args are passed on the stack, total_args_passed*wordSize is the
 529   // space we need.  Add in varargs area needed by the interpreter. Round up
 530   // to stack alignment.
 531   const int arg_size = total_args_passed * Interpreter::stackElementSize;
 532   const int varargs_area =
 533                  (frame::varargs_offset - frame::register_save_words)*wordSize;
 534   const int extraspace = align_up(arg_size + varargs_area, 2*wordSize);
 535 
 536   const int bias = STACK_BIAS;
 537   const int interp_arg_offset = frame::varargs_offset*wordSize +
 538                         (total_args_passed-1)*Interpreter::stackElementSize;
 539 
 540   const Register base = SP;
 541 
 542   // Make some extra space on the stack.
 543   __ sub(SP, __ ensure_simm13_or_reg(extraspace, G3_scratch), SP);
 544   set_Rdisp(G3_scratch);
 545 
 546   // Write the args into the outgoing interpreter space.
 547   for (int i = 0; i &lt; total_args_passed; i++) {
 548     const int st_off = interp_arg_offset - (i*Interpreter::stackElementSize) + bias;
 549     VMReg r_1 = regs[i].first();
 550     VMReg r_2 = regs[i].second();
 551     if (!r_1-&gt;is_valid()) {
 552       assert(!r_2-&gt;is_valid(), &quot;&quot;);
 553       continue;
 554     }
 555     if (r_1-&gt;is_stack()) {        // Pretend stack targets are loaded into G1
 556       RegisterOrConstant ld_off = reg2offset(r_1) + extraspace + bias;
 557       ld_off = __ ensure_simm13_or_reg(ld_off, Rdisp);
 558       r_1 = G1_scratch-&gt;as_VMReg();// as part of the load/store shuffle
 559       if (!r_2-&gt;is_valid()) __ ld (base, ld_off, G1_scratch);
 560       else                  __ ldx(base, ld_off, G1_scratch);
 561     }
 562 
 563     if (r_1-&gt;is_Register()) {
 564       Register r = r_1-&gt;as_Register()-&gt;after_restore();
 565       if (is_reference_type(sig_bt[i])) {
 566         store_c2i_object(r, base, st_off);
 567       } else if (sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {
 568         store_c2i_long(r, base, st_off, r_2-&gt;is_stack());
 569       } else {
 570         store_c2i_int(r, base, st_off);
 571       }
 572     } else {
 573       assert(r_1-&gt;is_FloatRegister(), &quot;&quot;);
 574       if (sig_bt[i] == T_FLOAT) {
 575         store_c2i_float(r_1-&gt;as_FloatRegister(), base, st_off);
 576       } else {
 577         assert(sig_bt[i] == T_DOUBLE, &quot;wrong type&quot;);
 578         store_c2i_double(r_2, r_1, base, st_off);
 579       }
 580     }
 581   }
 582 
 583   // Load the interpreter entry point.
 584   __ ld_ptr(G5_method, in_bytes(Method::interpreter_entry_offset()), G3_scratch);
 585 
 586   // Pass O5_savedSP as an argument to the interpreter.
 587   // The interpreter will restore SP to this value before returning.
 588   __ add(SP, __ ensure_simm13_or_reg(extraspace, G1), O5_savedSP);
 589 
 590   __ mov((frame::varargs_offset)*wordSize -
 591          1*Interpreter::stackElementSize+bias+BytesPerWord, G1);
 592   // Jump to the interpreter just as if interpreter was doing it.
 593   __ jmpl(G3_scratch, 0, G0);
 594   // Setup Lesp for the call.  Cannot actually set Lesp as the current Lesp
 595   // (really L0) is in use by the compiled frame as a generic temp.  However,
 596   // the interpreter does not know where its args are without some kind of
 597   // arg pointer being passed in.  Pass it in Gargs.
 598   __ delayed()-&gt;add(SP, G1, Gargs);
 599 }
 600 
 601 static void range_check(MacroAssembler* masm, Register pc_reg, Register temp_reg, Register temp2_reg,
 602                         address code_start, address code_end,
 603                         Label&amp; L_ok) {
 604   Label L_fail;
 605   __ set(ExternalAddress(code_start), temp_reg);
 606   __ set(pointer_delta(code_end, code_start, 1), temp2_reg);
 607   __ cmp(pc_reg, temp_reg);
 608   __ brx(Assembler::lessEqualUnsigned, false, Assembler::pn, L_fail);
 609   __ delayed()-&gt;add(temp_reg, temp2_reg, temp_reg);
 610   __ cmp(pc_reg, temp_reg);
 611   __ cmp_and_brx_short(pc_reg, temp_reg, Assembler::lessUnsigned, Assembler::pt, L_ok);
 612   __ bind(L_fail);
 613 }
 614 
 615 void AdapterGenerator::gen_i2c_adapter(int total_args_passed,
 616                                        // VMReg max_arg,
 617                                        int comp_args_on_stack, // VMRegStackSlots
 618                                        const BasicType *sig_bt,
 619                                        const VMRegPair *regs) {
 620   // Generate an I2C adapter: adjust the I-frame to make space for the C-frame
 621   // layout.  Lesp was saved by the calling I-frame and will be restored on
 622   // return.  Meanwhile, outgoing arg space is all owned by the callee
 623   // C-frame, so we can mangle it at will.  After adjusting the frame size,
 624   // hoist register arguments and repack other args according to the compiled
 625   // code convention.  Finally, end in a jump to the compiled code.  The entry
 626   // point address is the start of the buffer.
 627 
 628   // We will only enter here from an interpreted frame and never from after
 629   // passing thru a c2i. Azul allowed this but we do not. If we lose the
 630   // race and use a c2i we will remain interpreted for the race loser(s).
 631   // This removes all sorts of headaches on the x86 side and also eliminates
 632   // the possibility of having c2i -&gt; i2c -&gt; c2i -&gt; ... endless transitions.
 633 
 634   // More detail:
 635   // Adapters can be frameless because they do not require the caller
 636   // to perform additional cleanup work, such as correcting the stack pointer.
 637   // An i2c adapter is frameless because the *caller* frame, which is interpreted,
 638   // routinely repairs its own stack pointer (from interpreter_frame_last_sp),
 639   // even if a callee has modified the stack pointer.
 640   // A c2i adapter is frameless because the *callee* frame, which is interpreted,
 641   // routinely repairs its caller&#39;s stack pointer (from sender_sp, which is set
 642   // up via the senderSP register).
 643   // In other words, if *either* the caller or callee is interpreted, we can
 644   // get the stack pointer repaired after a call.
 645   // This is why c2i and i2c adapters cannot be indefinitely composed.
 646   // In particular, if a c2i adapter were to somehow call an i2c adapter,
 647   // both caller and callee would be compiled methods, and neither would
 648   // clean up the stack pointer changes performed by the two adapters.
 649   // If this happens, control eventually transfers back to the compiled
 650   // caller, but with an uncorrected stack, causing delayed havoc.
 651 
 652   if (VerifyAdapterCalls &amp;&amp;
 653       (Interpreter::code() != NULL || StubRoutines::code1() != NULL)) {
 654     // So, let&#39;s test for cascading c2i/i2c adapters right now.
 655     //  assert(Interpreter::contains($return_addr) ||
 656     //         StubRoutines::contains($return_addr),
 657     //         &quot;i2c adapter must return to an interpreter frame&quot;);
 658     __ block_comment(&quot;verify_i2c { &quot;);
 659     Label L_ok;
 660     if (Interpreter::code() != NULL)
 661       range_check(masm, O7, O0, O1,
 662                   Interpreter::code()-&gt;code_start(), Interpreter::code()-&gt;code_end(),
 663                   L_ok);
 664     if (StubRoutines::code1() != NULL)
 665       range_check(masm, O7, O0, O1,
 666                   StubRoutines::code1()-&gt;code_begin(), StubRoutines::code1()-&gt;code_end(),
 667                   L_ok);
 668     if (StubRoutines::code2() != NULL)
 669       range_check(masm, O7, O0, O1,
 670                   StubRoutines::code2()-&gt;code_begin(), StubRoutines::code2()-&gt;code_end(),
 671                   L_ok);
 672     const char* msg = &quot;i2c adapter must return to an interpreter frame&quot;;
 673     __ block_comment(msg);
 674     __ stop(msg);
 675     __ bind(L_ok);
 676     __ block_comment(&quot;} verify_i2ce &quot;);
 677   }
 678 
 679   // As you can see from the list of inputs &amp; outputs there are not a lot
 680   // of temp registers to work with: mostly G1, G3 &amp; G4.
 681 
 682   // Inputs:
 683   // G2_thread      - TLS
 684   // G5_method      - Method oop
 685   // G4 (Gargs)     - Pointer to interpreter&#39;s args
 686   // O0..O4         - free for scratch
 687   // O5_savedSP     - Caller&#39;s saved SP, to be restored if needed
 688   // O6             - Current SP!
 689   // O7             - Valid return address
 690   // L0-L7, I0-I7   - Caller&#39;s temps (no frame pushed yet)
 691 
 692   // Outputs:
 693   // G2_thread      - TLS
 694   // O0-O5          - Outgoing args in compiled layout
 695   // O6             - Adjusted or restored SP
 696   // O7             - Valid return address
 697   // L0-L7, I0-I7   - Caller&#39;s temps (no frame pushed yet)
 698   // F0-F7          - more outgoing args
 699 
 700 
 701   // Gargs is the incoming argument base, and also an outgoing argument.
 702   __ sub(Gargs, BytesPerWord, Gargs);
 703 
 704   // ON ENTRY TO THE CODE WE ARE MAKING, WE HAVE AN INTERPRETED FRAME
 705   // WITH O7 HOLDING A VALID RETURN PC
 706   //
 707   // |              |
 708   // :  java stack  :
 709   // |              |
 710   // +--------------+ &lt;--- start of outgoing args
 711   // |   receiver   |   |
 712   // : rest of args :   |---size is java-arg-words
 713   // |              |   |
 714   // +--------------+ &lt;--- O4_args (misaligned) and Lesp if prior is not C2I
 715   // |              |   |
 716   // :    unused    :   |---Space for max Java stack, plus stack alignment
 717   // |              |   |
 718   // +--------------+ &lt;--- SP + 16*wordsize
 719   // |              |
 720   // :    window    :
 721   // |              |
 722   // +--------------+ &lt;--- SP
 723 
 724   // WE REPACK THE STACK.  We use the common calling convention layout as
 725   // discovered by calling SharedRuntime::calling_convention.  We assume it
 726   // causes an arbitrary shuffle of memory, which may require some register
 727   // temps to do the shuffle.  We hope for (and optimize for) the case where
 728   // temps are not needed.  We may have to resize the stack slightly, in case
 729   // we need alignment padding (32-bit interpreter can pass longs &amp; doubles
 730   // misaligned, but the compilers expect them aligned).
 731   //
 732   // |              |
 733   // :  java stack  :
 734   // |              |
 735   // +--------------+ &lt;--- start of outgoing args
 736   // |  pad, align  |   |
 737   // +--------------+   |
 738   // | ints, longs, |   |
 739   // |    floats,   |   |---Outgoing stack args.
 740   // :    doubles   :   |   First few args in registers.
 741   // |              |   |
 742   // +--------------+ &lt;--- SP&#39; + 16*wordsize
 743   // |              |
 744   // :    window    :
 745   // |              |
 746   // +--------------+ &lt;--- SP&#39;
 747 
 748   // ON EXIT FROM THE CODE WE ARE MAKING, WE STILL HAVE AN INTERPRETED FRAME
 749   // WITH O7 HOLDING A VALID RETURN PC - ITS JUST THAT THE ARGS ARE NOW SETUP
 750   // FOR COMPILED CODE AND THE FRAME SLIGHTLY GROWN.
 751 
 752   // Cut-out for having no stack args.  Since up to 6 args are passed
 753   // in registers, we will commonly have no stack args.
 754   if (comp_args_on_stack &gt; 0) {
 755     // Convert VMReg stack slots to words.
 756     int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)&gt;&gt;LogBytesPerWord;
 757     // Round up to miminum stack alignment, in wordSize
 758     comp_words_on_stack = align_up(comp_words_on_stack, 2);
 759     // Now compute the distance from Lesp to SP.  This calculation does not
 760     // include the space for total_args_passed because Lesp has not yet popped
 761     // the arguments.
 762     __ sub(SP, (comp_words_on_stack)*wordSize, SP);
 763   }
 764 
 765   // Now generate the shuffle code.  Pick up all register args and move the
 766   // rest through G1_scratch.
 767   for (int i = 0; i &lt; total_args_passed; i++) {
 768     if (sig_bt[i] == T_VOID) {
 769       // Longs and doubles are passed in native word order, but misaligned
 770       // in the 32-bit build.
 771       assert(i &gt; 0 &amp;&amp; (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), &quot;missing half&quot;);
 772       continue;
 773     }
 774 
 775     // Pick up 0, 1 or 2 words from Lesp+offset.  Assume mis-aligned in the
 776     // 32-bit build and aligned in the 64-bit build.  Look for the obvious
 777     // ldx/lddf optimizations.
 778 
 779     // Load in argument order going down.
 780     const int ld_off = (total_args_passed-i)*Interpreter::stackElementSize;
 781     set_Rdisp(G1_scratch);
 782 
 783     VMReg r_1 = regs[i].first();
 784     VMReg r_2 = regs[i].second();
 785     if (!r_1-&gt;is_valid()) {
 786       assert(!r_2-&gt;is_valid(), &quot;&quot;);
 787       continue;
 788     }
 789     if (r_1-&gt;is_stack()) {        // Pretend stack targets are loaded into F8/F9
 790       r_1 = F8-&gt;as_VMReg();        // as part of the load/store shuffle
 791       if (r_2-&gt;is_valid()) r_2 = r_1-&gt;next();
 792     }
 793     if (r_1-&gt;is_Register()) {  // Register argument
 794       Register r = r_1-&gt;as_Register()-&gt;after_restore();
 795       if (!r_2-&gt;is_valid()) {
 796         __ ld(Gargs, arg_slot(ld_off), r);
 797       } else {
 798         // In V9, longs are given 2 64-bit slots in the interpreter, but the
 799         // data is passed in only 1 slot.
 800         RegisterOrConstant slot = (sig_bt[i] == T_LONG) ?
 801               next_arg_slot(ld_off) : arg_slot(ld_off);
 802         __ ldx(Gargs, slot, r);
 803       }
 804     } else {
 805       assert(r_1-&gt;is_FloatRegister(), &quot;&quot;);
 806       if (!r_2-&gt;is_valid()) {
 807         __ ldf(FloatRegisterImpl::S, Gargs,      arg_slot(ld_off), r_1-&gt;as_FloatRegister());
 808       } else {
 809         // In V9, doubles are given 2 64-bit slots in the interpreter, but the
 810         // data is passed in only 1 slot.  This code also handles longs that
 811         // are passed on the stack, but need a stack-to-stack move through a
 812         // spare float register.
 813         RegisterOrConstant slot = (sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) ?
 814               next_arg_slot(ld_off) : arg_slot(ld_off);
 815         __ ldf(FloatRegisterImpl::D, Gargs,                  slot, r_1-&gt;as_FloatRegister());
 816       }
 817     }
 818     // Was the argument really intended to be on the stack, but was loaded
 819     // into F8/F9?
 820     if (regs[i].first()-&gt;is_stack()) {
 821       assert(r_1-&gt;as_FloatRegister() == F8, &quot;fix this code&quot;);
 822       // Convert stack slot to an SP offset
 823       int st_off = reg2offset(regs[i].first()) + STACK_BIAS;
 824       // Store down the shuffled stack word.  Target address _is_ aligned.
 825       RegisterOrConstant slot = __ ensure_simm13_or_reg(st_off, Rdisp);
 826       if (!r_2-&gt;is_valid()) __ stf(FloatRegisterImpl::S, r_1-&gt;as_FloatRegister(), SP, slot);
 827       else                  __ stf(FloatRegisterImpl::D, r_1-&gt;as_FloatRegister(), SP, slot);
 828     }
 829   }
 830 
 831   // Jump to the compiled code just as if compiled code was doing it.
 832   __ ld_ptr(G5_method, in_bytes(Method::from_compiled_offset()), G3);
 833 #if INCLUDE_JVMCI
 834   if (EnableJVMCI) {
 835     // check if this call should be routed towards a specific entry point
 836     __ ld(Address(G2_thread, in_bytes(JavaThread::jvmci_alternate_call_target_offset())), G1);
 837     __ cmp(G0, G1);
 838     Label no_alternative_target;
 839     __ br(Assembler::equal, false, Assembler::pn, no_alternative_target);
 840     __ delayed()-&gt;nop();
 841 
 842     __ ld_ptr(G2_thread, in_bytes(JavaThread::jvmci_alternate_call_target_offset()), G3);
 843     __ st_ptr(G0, Address(G2_thread, in_bytes(JavaThread::jvmci_alternate_call_target_offset())));
 844 
 845     __ bind(no_alternative_target);
 846   }
 847 #endif // INCLUDE_JVMCI
 848 
 849   // 6243940 We might end up in handle_wrong_method if
 850   // the callee is deoptimized as we race thru here. If that
 851   // happens we don&#39;t want to take a safepoint because the
 852   // caller frame will look interpreted and arguments are now
 853   // &quot;compiled&quot; so it is much better to make this transition
 854   // invisible to the stack walking code. Unfortunately if
 855   // we try and find the callee by normal means a safepoint
 856   // is possible. So we stash the desired callee in the thread
 857   // and the vm will find there should this case occur.
 858   Address callee_target_addr(G2_thread, JavaThread::callee_target_offset());
 859   __ st_ptr(G5_method, callee_target_addr);
 860   __ jmpl(G3, 0, G0);
 861   __ delayed()-&gt;nop();
 862 }
 863 
 864 void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,
 865                                     int total_args_passed,
 866                                     int comp_args_on_stack,
 867                                     const BasicType *sig_bt,
 868                                     const VMRegPair *regs) {
 869   AdapterGenerator agen(masm);
 870   agen.gen_i2c_adapter(total_args_passed, comp_args_on_stack, sig_bt, regs);
 871 }
 872 
 873 // ---------------------------------------------------------------
 874 AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,
 875                                                             int total_args_passed,
 876                                                             // VMReg max_arg,
 877                                                             int comp_args_on_stack, // VMRegStackSlots
 878                                                             const BasicType *sig_bt,
 879                                                             const VMRegPair *regs,
 880                                                             AdapterFingerPrint* fingerprint) {
 881   address i2c_entry = __ pc();
 882 
 883   gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);
 884 
 885 
 886   // -------------------------------------------------------------------------
 887   // Generate a C2I adapter.  On entry we know G5 holds the Method*.  The
 888   // args start out packed in the compiled layout.  They need to be unpacked
 889   // into the interpreter layout.  This will almost always require some stack
 890   // space.  We grow the current (compiled) stack, then repack the args.  We
 891   // finally end in a jump to the generic interpreter entry point.  On exit
 892   // from the interpreter, the interpreter will restore our SP (lest the
 893   // compiled code, which relys solely on SP and not FP, get sick).
 894 
 895   address c2i_unverified_entry = __ pc();
 896   Label L_skip_fixup;
 897   {
 898     Register R_temp = G1;  // another scratch register
 899 
 900     AddressLiteral ic_miss(SharedRuntime::get_ic_miss_stub());
 901 
 902     __ verify_oop(O0);
 903     __ load_klass(O0, G3_scratch);
 904 
 905     __ ld_ptr(G5_method, CompiledICHolder::holder_klass_offset(), R_temp);
 906     __ cmp(G3_scratch, R_temp);
 907 
 908     Label ok, ok2;
 909     __ brx(Assembler::equal, false, Assembler::pt, ok);
 910     __ delayed()-&gt;ld_ptr(G5_method, CompiledICHolder::holder_metadata_offset(), G5_method);
 911     __ jump_to(ic_miss, G3_scratch);
 912     __ delayed()-&gt;nop();
 913 
 914     __ bind(ok);
 915     // Method might have been compiled since the call site was patched to
 916     // interpreted if that is the case treat it as a miss so we can get
 917     // the call site corrected.
 918     __ ld_ptr(G5_method, in_bytes(Method::code_offset()), G3_scratch);
 919     __ bind(ok2);
 920     __ br_null(G3_scratch, false, Assembler::pt, L_skip_fixup);
 921     __ delayed()-&gt;nop();
 922     __ jump_to(ic_miss, G3_scratch);
 923     __ delayed()-&gt;nop();
 924 
 925   }
 926 
 927   address c2i_entry = __ pc();
 928   AdapterGenerator agen(masm);
 929   agen.gen_c2i_adapter(total_args_passed, comp_args_on_stack, sig_bt, regs, L_skip_fixup);
 930 
 931   __ flush();
 932   return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry);
 933 
 934 }
 935 
 936 // Helper function for native calling conventions
 937 static VMReg int_stk_helper( int i ) {
 938   // Bias any stack based VMReg we get by ignoring the window area
 939   // but not the register parameter save area.
 940   //
 941   // This is strange for the following reasons. We&#39;d normally expect
 942   // the calling convention to return an VMReg for a stack slot
 943   // completely ignoring any abi reserved area. C2 thinks of that
 944   // abi area as only out_preserve_stack_slots. This does not include
 945   // the area allocated by the C abi to store down integer arguments
 946   // because the java calling convention does not use it. So
 947   // since c2 assumes that there are only out_preserve_stack_slots
 948   // to bias the optoregs (which impacts VMRegs) when actually referencing any actual stack
 949   // location the c calling convention must add in this bias amount
 950   // to make up for the fact that the out_preserve_stack_slots is
 951   // insufficient for C calls. What a mess. I sure hope those 6
 952   // stack words were worth it on every java call!
 953 
 954   // Another way of cleaning this up would be for out_preserve_stack_slots
 955   // to take a parameter to say whether it was C or java calling conventions.
 956   // Then things might look a little better (but not much).
 957 
 958   int mem_parm_offset = i - SPARC_ARGS_IN_REGS_NUM;
 959   if( mem_parm_offset &lt; 0 ) {
 960     return as_oRegister(i)-&gt;as_VMReg();
 961   } else {
 962     int actual_offset = (mem_parm_offset + frame::memory_parameter_word_sp_offset) * VMRegImpl::slots_per_word;
 963     // Now return a biased offset that will be correct when out_preserve_slots is added back in
 964     return VMRegImpl::stack2reg(actual_offset - SharedRuntime::out_preserve_stack_slots());
 965   }
 966 }
 967 
 968 
 969 int SharedRuntime::c_calling_convention(const BasicType *sig_bt,
 970                                          VMRegPair *regs,
 971                                          VMRegPair *regs2,
 972                                          int total_args_passed) {
 973     assert(regs2 == NULL, &quot;not needed on sparc&quot;);
 974 
 975     // Return the number of VMReg stack_slots needed for the args.
 976     // This value does not include an abi space (like register window
 977     // save area).
 978 
 979     // The native convention is V8 if !LP64
 980     // The LP64 convention is the V9 convention which is slightly more sane.
 981 
 982     // We return the amount of VMReg stack slots we need to reserve for all
 983     // the arguments NOT counting out_preserve_stack_slots. Since we always
 984     // have space for storing at least 6 registers to memory we start with that.
 985     // See int_stk_helper for a further discussion.
 986     int max_stack_slots = (frame::varargs_offset * VMRegImpl::slots_per_word) - SharedRuntime::out_preserve_stack_slots();
 987 
 988     // V9 convention: All things &quot;as-if&quot; on double-wide stack slots.
 989     // Hoist any int/ptr/long&#39;s in the first 6 to int regs.
 990     // Hoist any flt/dbl&#39;s in the first 16 dbl regs.
 991     int j = 0;                  // Count of actual args, not HALVES
 992     VMRegPair param_array_reg;  // location of the argument in the parameter array
 993     for (int i = 0; i &lt; total_args_passed; i++, j++) {
 994       param_array_reg.set_bad();
 995       switch (sig_bt[i]) {
 996       case T_BOOLEAN:
 997       case T_BYTE:
 998       case T_CHAR:
 999       case T_INT:
1000       case T_SHORT:
1001         regs[i].set1(int_stk_helper(j));
1002         break;
1003       case T_LONG:
1004         assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i+1] == T_VOID, &quot;expecting half&quot;);
1005       case T_ADDRESS: // raw pointers, like current thread, for VM calls
1006       case T_ARRAY:
1007       case T_OBJECT:
1008       case T_METADATA:
1009         regs[i].set2(int_stk_helper(j));
1010         break;
1011       case T_FLOAT:
1012         // Per SPARC Compliance Definition 2.4.1, page 3P-12 available here
1013         // http://www.sparc.org/wp-content/uploads/2014/01/SCD.2.4.1.pdf.gz
1014         //
1015         // &quot;When a callee prototype exists, and does not indicate variable arguments,
1016         // floating-point values assigned to locations %sp+BIAS+128 through %sp+BIAS+248
1017         // will be promoted to floating-point registers&quot;
1018         //
1019         // By &quot;promoted&quot; it means that the argument is located in two places, an unused
1020         // spill slot in the &quot;parameter array&quot; (starts at %sp+BIAS+128), and a live
1021         // float register.  In most cases, there are 6 or fewer arguments of any type,
1022         // and the standard parameter array slots (%sp+BIAS+128 to %sp+BIAS+176 exclusive)
1023         // serve as shadow slots.  Per the spec floating point registers %d6 to %d16
1024         // require slots beyond that (up to %sp+BIAS+248).
1025         //
1026         {
1027           // V9ism: floats go in ODD registers and stack slots
1028           int float_index = 1 + (j &lt;&lt; 1);
1029           param_array_reg.set1(VMRegImpl::stack2reg(float_index));
1030           if (j &lt; 16) {
1031             regs[i].set1(as_FloatRegister(float_index)-&gt;as_VMReg());
1032           } else {
1033             regs[i] = param_array_reg;
1034           }
1035         }
1036         break;
1037       case T_DOUBLE:
1038         {
1039           assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);
1040           // V9ism: doubles go in EVEN/ODD regs and stack slots
1041           int double_index = (j &lt;&lt; 1);
1042           param_array_reg.set2(VMRegImpl::stack2reg(double_index));
1043           if (j &lt; 16) {
1044             regs[i].set2(as_FloatRegister(double_index)-&gt;as_VMReg());
1045           } else {
1046             // V9ism: doubles go in EVEN/ODD stack slots
1047             regs[i] = param_array_reg;
1048           }
1049         }
1050         break;
1051       case T_VOID:
1052         regs[i].set_bad();
1053         j--;
1054         break; // Do not count HALVES
1055       default:
1056         ShouldNotReachHere();
1057       }
1058       // Keep track of the deepest parameter array slot.
1059       if (!param_array_reg.first()-&gt;is_valid()) {
1060         param_array_reg = regs[i];
1061       }
1062       if (param_array_reg.first()-&gt;is_stack()) {
1063         int off = param_array_reg.first()-&gt;reg2stack();
1064         if (off &gt; max_stack_slots) max_stack_slots = off;
1065       }
1066       if (param_array_reg.second()-&gt;is_stack()) {
1067         int off = param_array_reg.second()-&gt;reg2stack();
1068         if (off &gt; max_stack_slots) max_stack_slots = off;
1069       }
1070     }
1071   return align_up(max_stack_slots + 1, 2);
1072 
1073 }
1074 
1075 
1076 // ---------------------------------------------------------------------------
1077 void SharedRuntime::save_native_result(MacroAssembler *masm, BasicType ret_type, int frame_slots) {
1078   switch (ret_type) {
1079   case T_FLOAT:
1080     __ stf(FloatRegisterImpl::S, F0, SP, frame_slots*VMRegImpl::stack_slot_size - 4+STACK_BIAS);
1081     break;
1082   case T_DOUBLE:
1083     __ stf(FloatRegisterImpl::D, F0, SP, frame_slots*VMRegImpl::stack_slot_size - 8+STACK_BIAS);
1084     break;
1085   }
1086 }
1087 
1088 void SharedRuntime::restore_native_result(MacroAssembler *masm, BasicType ret_type, int frame_slots) {
1089   switch (ret_type) {
1090   case T_FLOAT:
1091     __ ldf(FloatRegisterImpl::S, SP, frame_slots*VMRegImpl::stack_slot_size - 4+STACK_BIAS, F0);
1092     break;
1093   case T_DOUBLE:
1094     __ ldf(FloatRegisterImpl::D, SP, frame_slots*VMRegImpl::stack_slot_size - 8+STACK_BIAS, F0);
1095     break;
1096   }
1097 }
1098 
1099 // Check and forward and pending exception.  Thread is stored in
1100 // L7_thread_cache and possibly NOT in G2_thread.  Since this is a native call, there
1101 // is no exception handler.  We merely pop this frame off and throw the
1102 // exception in the caller&#39;s frame.
1103 static void check_forward_pending_exception(MacroAssembler *masm, Register Rex_oop) {
1104   Label L;
1105   __ br_null(Rex_oop, false, Assembler::pt, L);
1106   __ delayed()-&gt;mov(L7_thread_cache, G2_thread); // restore in case we have exception
1107   // Since this is a native call, we *know* the proper exception handler
1108   // without calling into the VM: it&#39;s the empty function.  Just pop this
1109   // frame and then jump to forward_exception_entry; O7 will contain the
1110   // native caller&#39;s return PC.
1111  AddressLiteral exception_entry(StubRoutines::forward_exception_entry());
1112   __ jump_to(exception_entry, G3_scratch);
1113   __ delayed()-&gt;restore();      // Pop this frame off.
1114   __ bind(L);
1115 }
1116 
1117 // A simple move of integer like type
1118 static void simple_move32(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {
1119   if (src.first()-&gt;is_stack()) {
1120     if (dst.first()-&gt;is_stack()) {
1121       // stack to stack
1122       __ ld(FP, reg2offset(src.first()) + STACK_BIAS, L5);
1123       __ st(L5, SP, reg2offset(dst.first()) + STACK_BIAS);
1124     } else {
1125       // stack to reg
1126       __ ld(FP, reg2offset(src.first()) + STACK_BIAS, dst.first()-&gt;as_Register());
1127     }
1128   } else if (dst.first()-&gt;is_stack()) {
1129     // reg to stack
1130     __ st(src.first()-&gt;as_Register(), SP, reg2offset(dst.first()) + STACK_BIAS);
1131   } else {
1132     __ mov(src.first()-&gt;as_Register(), dst.first()-&gt;as_Register());
1133   }
1134 }
1135 
1136 // On 64 bit we will store integer like items to the stack as
1137 // 64 bits items (sparc abi) even though java would only store
1138 // 32bits for a parameter. On 32bit it will simply be 32 bits
1139 // So this routine will do 32-&gt;32 on 32bit and 32-&gt;64 on 64bit
1140 static void move32_64(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {
1141   if (src.first()-&gt;is_stack()) {
1142     if (dst.first()-&gt;is_stack()) {
1143       // stack to stack
1144       __ ld(FP, reg2offset(src.first()) + STACK_BIAS, L5);
1145       __ st_ptr(L5, SP, reg2offset(dst.first()) + STACK_BIAS);
1146     } else {
1147       // stack to reg
1148       __ ld(FP, reg2offset(src.first()) + STACK_BIAS, dst.first()-&gt;as_Register());
1149     }
1150   } else if (dst.first()-&gt;is_stack()) {
1151     // reg to stack
1152     // Some compilers (gcc) expect a clean 32 bit value on function entry
1153     __ signx(src.first()-&gt;as_Register(), L5);
1154     __ st_ptr(L5, SP, reg2offset(dst.first()) + STACK_BIAS);
1155   } else {
1156     // Some compilers (gcc) expect a clean 32 bit value on function entry
1157     __ signx(src.first()-&gt;as_Register(), dst.first()-&gt;as_Register());
1158   }
1159 }
1160 
1161 
1162 static void move_ptr(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {
1163   if (src.first()-&gt;is_stack()) {
1164     if (dst.first()-&gt;is_stack()) {
1165       // stack to stack
1166       __ ld_ptr(FP, reg2offset(src.first()) + STACK_BIAS, L5);
1167       __ st_ptr(L5, SP, reg2offset(dst.first()) + STACK_BIAS);
1168     } else {
1169       // stack to reg
1170       __ ld_ptr(FP, reg2offset(src.first()) + STACK_BIAS, dst.first()-&gt;as_Register());
1171     }
1172   } else if (dst.first()-&gt;is_stack()) {
1173     // reg to stack
1174     __ st_ptr(src.first()-&gt;as_Register(), SP, reg2offset(dst.first()) + STACK_BIAS);
1175   } else {
1176     __ mov(src.first()-&gt;as_Register(), dst.first()-&gt;as_Register());
1177   }
1178 }
1179 
1180 
1181 // An oop arg. Must pass a handle not the oop itself
1182 static void object_move(MacroAssembler* masm,
1183                         OopMap* map,
1184                         int oop_handle_offset,
1185                         int framesize_in_slots,
1186                         VMRegPair src,
1187                         VMRegPair dst,
1188                         bool is_receiver,
1189                         int* receiver_offset) {
1190 
1191   // must pass a handle. First figure out the location we use as a handle
1192 
1193   if (src.first()-&gt;is_stack()) {
1194     // Oop is already on the stack
1195     Register rHandle = dst.first()-&gt;is_stack() ? L5 : dst.first()-&gt;as_Register();
1196     __ add(FP, reg2offset(src.first()) + STACK_BIAS, rHandle);
1197     __ ld_ptr(rHandle, 0, L4);
1198     __ movr( Assembler::rc_z, L4, G0, rHandle );
1199     if (dst.first()-&gt;is_stack()) {
1200       __ st_ptr(rHandle, SP, reg2offset(dst.first()) + STACK_BIAS);
1201     }
1202     int offset_in_older_frame = src.first()-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
1203     if (is_receiver) {
1204       *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;
1205     }
1206     map-&gt;set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));
1207   } else {
1208     // Oop is in an input register pass we must flush it to the stack
1209     const Register rOop = src.first()-&gt;as_Register();
1210     const Register rHandle = L5;
1211     int oop_slot = rOop-&gt;input_number() * VMRegImpl::slots_per_word + oop_handle_offset;
1212     int offset = oop_slot * VMRegImpl::stack_slot_size;
1213     __ st_ptr(rOop, SP, offset + STACK_BIAS);
1214     if (is_receiver) {
1215        *receiver_offset = offset;
1216     }
1217     map-&gt;set_oop(VMRegImpl::stack2reg(oop_slot));
1218     __ add(SP, offset + STACK_BIAS, rHandle);
1219     __ movr( Assembler::rc_z, rOop, G0, rHandle );
1220 
1221     if (dst.first()-&gt;is_stack()) {
1222       __ st_ptr(rHandle, SP, reg2offset(dst.first()) + STACK_BIAS);
1223     } else {
1224       __ mov(rHandle, dst.first()-&gt;as_Register());
1225     }
1226   }
1227 }
1228 
1229 // A float arg may have to do float reg int reg conversion
1230 static void float_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {
1231   assert(!src.second()-&gt;is_valid() &amp;&amp; !dst.second()-&gt;is_valid(), &quot;bad float_move&quot;);
1232 
1233   if (src.first()-&gt;is_stack()) {
1234     if (dst.first()-&gt;is_stack()) {
1235       // stack to stack the easiest of the bunch
1236       __ ld(FP, reg2offset(src.first()) + STACK_BIAS, L5);
1237       __ st(L5, SP, reg2offset(dst.first()) + STACK_BIAS);
1238     } else {
1239       // stack to reg
1240       if (dst.first()-&gt;is_Register()) {
1241         __ ld(FP, reg2offset(src.first()) + STACK_BIAS, dst.first()-&gt;as_Register());
1242       } else {
1243         __ ldf(FloatRegisterImpl::S, FP, reg2offset(src.first()) + STACK_BIAS, dst.first()-&gt;as_FloatRegister());
1244       }
1245     }
1246   } else if (dst.first()-&gt;is_stack()) {
1247     // reg to stack
1248     if (src.first()-&gt;is_Register()) {
1249       __ st(src.first()-&gt;as_Register(), SP, reg2offset(dst.first()) + STACK_BIAS);
1250     } else {
1251       __ stf(FloatRegisterImpl::S, src.first()-&gt;as_FloatRegister(), SP, reg2offset(dst.first()) + STACK_BIAS);
1252     }
1253   } else {
1254     // reg to reg
1255     if (src.first()-&gt;is_Register()) {
1256       if (dst.first()-&gt;is_Register()) {
1257         // gpr -&gt; gpr
1258         __ mov(src.first()-&gt;as_Register(), dst.first()-&gt;as_Register());
1259       } else {
1260         // gpr -&gt; fpr
1261         __ st(src.first()-&gt;as_Register(), FP, -4 + STACK_BIAS);
1262         __ ldf(FloatRegisterImpl::S, FP, -4 + STACK_BIAS, dst.first()-&gt;as_FloatRegister());
1263       }
1264     } else if (dst.first()-&gt;is_Register()) {
1265       // fpr -&gt; gpr
1266       __ stf(FloatRegisterImpl::S, src.first()-&gt;as_FloatRegister(), FP, -4 + STACK_BIAS);
1267       __ ld(FP, -4 + STACK_BIAS, dst.first()-&gt;as_Register());
1268     } else {
1269       // fpr -&gt; fpr
1270       // In theory these overlap but the ordering is such that this is likely a nop
1271       if ( src.first() != dst.first()) {
1272         __ fmov(FloatRegisterImpl::S, src.first()-&gt;as_FloatRegister(), dst.first()-&gt;as_FloatRegister());
1273       }
1274     }
1275   }
1276 }
1277 
1278 static void split_long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {
1279   VMRegPair src_lo(src.first());
1280   VMRegPair src_hi(src.second());
1281   VMRegPair dst_lo(dst.first());
1282   VMRegPair dst_hi(dst.second());
1283   simple_move32(masm, src_lo, dst_lo);
1284   simple_move32(masm, src_hi, dst_hi);
1285 }
1286 
1287 // A long move
1288 static void long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {
1289 
1290   // Do the simple ones here else do two int moves
1291   if (src.is_single_phys_reg() ) {
1292     if (dst.is_single_phys_reg()) {
1293       __ mov(src.first()-&gt;as_Register(), dst.first()-&gt;as_Register());
1294     } else {
1295       // split src into two separate registers
1296       // Remember hi means hi address or lsw on sparc
1297       // Move msw to lsw
1298       if (dst.second()-&gt;is_reg()) {
1299         // MSW -&gt; MSW
1300         __ srax(src.first()-&gt;as_Register(), 32, dst.first()-&gt;as_Register());
1301         // Now LSW -&gt; LSW
1302         // this will only move lo -&gt; lo and ignore hi
1303         VMRegPair split(dst.second());
1304         simple_move32(masm, src, split);
1305       } else {
1306         VMRegPair split(src.first(), L4-&gt;as_VMReg());
1307         // MSW -&gt; MSW (lo ie. first word)
1308         __ srax(src.first()-&gt;as_Register(), 32, L4);
1309         split_long_move(masm, split, dst);
1310       }
1311     }
1312   } else if (dst.is_single_phys_reg()) {
1313     if (src.is_adjacent_aligned_on_stack(2)) {
1314       __ ldx(FP, reg2offset(src.first()) + STACK_BIAS, dst.first()-&gt;as_Register());
1315     } else {
1316       // dst is a single reg.
1317       // Remember lo is low address not msb for stack slots
1318       // and lo is the &quot;real&quot; register for registers
1319       // src is
1320 
1321       VMRegPair split;
1322 
1323       if (src.first()-&gt;is_reg()) {
1324         // src.lo (msw) is a reg, src.hi is stk/reg
1325         // we will move: src.hi (LSW) -&gt; dst.lo, src.lo (MSW) -&gt; src.lo [the MSW is in the LSW of the reg]
1326         split.set_pair(dst.first(), src.first());
1327       } else {
1328         // msw is stack move to L5
1329         // lsw is stack move to dst.lo (real reg)
1330         // we will move: src.hi (LSW) -&gt; dst.lo, src.lo (MSW) -&gt; L5
1331         split.set_pair(dst.first(), L5-&gt;as_VMReg());
1332       }
1333 
1334       // src.lo -&gt; src.lo/L5, src.hi -&gt; dst.lo (the real reg)
1335       // msw   -&gt; src.lo/L5,  lsw -&gt; dst.lo
1336       split_long_move(masm, src, split);
1337 
1338       // So dst now has the low order correct position the
1339       // msw half
1340       __ sllx(split.first()-&gt;as_Register(), 32, L5);
1341 
1342       const Register d = dst.first()-&gt;as_Register();
1343       __ or3(L5, d, d);
1344     }
1345   } else {
1346     // For LP64 we can probably do better.
1347     split_long_move(masm, src, dst);
1348   }
1349 }
1350 
1351 // A double move
1352 static void double_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {
1353 
1354   // The painful thing here is that like long_move a VMRegPair might be
1355   // 1: a single physical register
1356   // 2: two physical registers (v8)
1357   // 3: a physical reg [lo] and a stack slot [hi] (v8)
1358   // 4: two stack slots
1359 
1360   // Since src is always a java calling convention we know that the src pair
1361   // is always either all registers or all stack (and aligned?)
1362 
1363   // in a register [lo] and a stack slot [hi]
1364   if (src.first()-&gt;is_stack()) {
1365     if (dst.first()-&gt;is_stack()) {
1366       // stack to stack the easiest of the bunch
1367       // ought to be a way to do this where if alignment is ok we use ldd/std when possible
1368       __ ld(FP, reg2offset(src.first()) + STACK_BIAS, L5);
1369       __ ld(FP, reg2offset(src.second()) + STACK_BIAS, L4);
1370       __ st(L5, SP, reg2offset(dst.first()) + STACK_BIAS);
1371       __ st(L4, SP, reg2offset(dst.second()) + STACK_BIAS);
1372     } else {
1373       // stack to reg
1374       if (dst.second()-&gt;is_stack()) {
1375         // stack -&gt; reg, stack -&gt; stack
1376         __ ld(FP, reg2offset(src.second()) + STACK_BIAS, L4);
1377         if (dst.first()-&gt;is_Register()) {
1378           __ ld(FP, reg2offset(src.first()) + STACK_BIAS, dst.first()-&gt;as_Register());
1379         } else {
1380           __ ldf(FloatRegisterImpl::S, FP, reg2offset(src.first()) + STACK_BIAS, dst.first()-&gt;as_FloatRegister());
1381         }
1382         // This was missing. (very rare case)
1383         __ st(L4, SP, reg2offset(dst.second()) + STACK_BIAS);
1384       } else {
1385         // stack -&gt; reg
1386         // Eventually optimize for alignment QQQ
1387         if (dst.first()-&gt;is_Register()) {
1388           __ ld(FP, reg2offset(src.first()) + STACK_BIAS, dst.first()-&gt;as_Register());
1389           __ ld(FP, reg2offset(src.second()) + STACK_BIAS, dst.second()-&gt;as_Register());
1390         } else {
1391           __ ldf(FloatRegisterImpl::S, FP, reg2offset(src.first()) + STACK_BIAS, dst.first()-&gt;as_FloatRegister());
1392           __ ldf(FloatRegisterImpl::S, FP, reg2offset(src.second()) + STACK_BIAS, dst.second()-&gt;as_FloatRegister());
1393         }
1394       }
1395     }
1396   } else if (dst.first()-&gt;is_stack()) {
1397     // reg to stack
1398     if (src.first()-&gt;is_Register()) {
1399       // Eventually optimize for alignment QQQ
1400       __ st(src.first()-&gt;as_Register(), SP, reg2offset(dst.first()) + STACK_BIAS);
1401       if (src.second()-&gt;is_stack()) {
1402         __ ld(FP, reg2offset(src.second()) + STACK_BIAS, L4);
1403         __ st(L4, SP, reg2offset(dst.second()) + STACK_BIAS);
1404       } else {
1405         __ st(src.second()-&gt;as_Register(), SP, reg2offset(dst.second()) + STACK_BIAS);
1406       }
1407     } else {
1408       // fpr to stack
1409       if (src.second()-&gt;is_stack()) {
1410         ShouldNotReachHere();
1411       } else {
1412         // Is the stack aligned?
1413         if (reg2offset(dst.first()) &amp; 0x7) {
1414           // No do as pairs
1415           __ stf(FloatRegisterImpl::S, src.first()-&gt;as_FloatRegister(), SP, reg2offset(dst.first()) + STACK_BIAS);
1416           __ stf(FloatRegisterImpl::S, src.second()-&gt;as_FloatRegister(), SP, reg2offset(dst.second()) + STACK_BIAS);
1417         } else {
1418           __ stf(FloatRegisterImpl::D, src.first()-&gt;as_FloatRegister(), SP, reg2offset(dst.first()) + STACK_BIAS);
1419         }
1420       }
1421     }
1422   } else {
1423     // reg to reg
1424     if (src.first()-&gt;is_Register()) {
1425       if (dst.first()-&gt;is_Register()) {
1426         // gpr -&gt; gpr
1427         __ mov(src.first()-&gt;as_Register(), dst.first()-&gt;as_Register());
1428         __ mov(src.second()-&gt;as_Register(), dst.second()-&gt;as_Register());
1429       } else {
1430         // gpr -&gt; fpr
1431         // ought to be able to do a single store
1432         __ stx(src.first()-&gt;as_Register(), FP, -8 + STACK_BIAS);
1433         __ stx(src.second()-&gt;as_Register(), FP, -4 + STACK_BIAS);
1434         // ought to be able to do a single load
1435         __ ldf(FloatRegisterImpl::S, FP, -8 + STACK_BIAS, dst.first()-&gt;as_FloatRegister());
1436         __ ldf(FloatRegisterImpl::S, FP, -4 + STACK_BIAS, dst.second()-&gt;as_FloatRegister());
1437       }
1438     } else if (dst.first()-&gt;is_Register()) {
1439       // fpr -&gt; gpr
1440       // ought to be able to do a single store
1441       __ stf(FloatRegisterImpl::D, src.first()-&gt;as_FloatRegister(), FP, -8 + STACK_BIAS);
1442       // ought to be able to do a single load
1443       // REMEMBER first() is low address not LSB
1444       __ ld(FP, -8 + STACK_BIAS, dst.first()-&gt;as_Register());
1445       if (dst.second()-&gt;is_Register()) {
1446         __ ld(FP, -4 + STACK_BIAS, dst.second()-&gt;as_Register());
1447       } else {
1448         __ ld(FP, -4 + STACK_BIAS, L4);
1449         __ st(L4, SP, reg2offset(dst.second()) + STACK_BIAS);
1450       }
1451     } else {
1452       // fpr -&gt; fpr
1453       // In theory these overlap but the ordering is such that this is likely a nop
1454       if ( src.first() != dst.first()) {
1455         __ fmov(FloatRegisterImpl::D, src.first()-&gt;as_FloatRegister(), dst.first()-&gt;as_FloatRegister());
1456       }
1457     }
1458   }
1459 }
1460 
1461 // Creates an inner frame if one hasn&#39;t already been created, and
1462 // saves a copy of the thread in L7_thread_cache
1463 static void create_inner_frame(MacroAssembler* masm, bool* already_created) {
1464   if (!*already_created) {
1465     __ save_frame(0);
1466     // Save thread in L7 (INNER FRAME); it crosses a bunch of VM calls below
1467     // Don&#39;t use save_thread because it smashes G2 and we merely want to save a
1468     // copy
1469     __ mov(G2_thread, L7_thread_cache);
1470     *already_created = true;
1471   }
1472 }
1473 
1474 
1475 static void save_or_restore_arguments(MacroAssembler* masm,
1476                                       const int stack_slots,
1477                                       const int total_in_args,
1478                                       const int arg_save_area,
1479                                       OopMap* map,
1480                                       VMRegPair* in_regs,
1481                                       BasicType* in_sig_bt) {
1482   // if map is non-NULL then the code should store the values,
1483   // otherwise it should load them.
1484   if (map != NULL) {
1485     // Fill in the map
1486     for (int i = 0; i &lt; total_in_args; i++) {
1487       if (in_sig_bt[i] == T_ARRAY) {
1488         if (in_regs[i].first()-&gt;is_stack()) {
1489           int offset_in_older_frame = in_regs[i].first()-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
1490           map-&gt;set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));
1491         } else if (in_regs[i].first()-&gt;is_Register()) {
1492           map-&gt;set_oop(in_regs[i].first());
1493         } else {
1494           ShouldNotReachHere();
1495         }
1496       }
1497     }
1498   }
1499 
1500   // Save or restore double word values
1501   int handle_index = 0;
1502   for (int i = 0; i &lt; total_in_args; i++) {
1503     int slot = handle_index + arg_save_area;
1504     int offset = slot * VMRegImpl::stack_slot_size;
1505     if (in_sig_bt[i] == T_LONG &amp;&amp; in_regs[i].first()-&gt;is_Register()) {
1506       const Register reg = in_regs[i].first()-&gt;as_Register();
1507       if (reg-&gt;is_global()) {
1508         handle_index += 2;
1509         assert(handle_index &lt;= stack_slots, &quot;overflow&quot;);
1510         if (map != NULL) {
1511           __ stx(reg, SP, offset + STACK_BIAS);
1512         } else {
1513           __ ldx(SP, offset + STACK_BIAS, reg);
1514         }
1515       }
1516     } else if (in_sig_bt[i] == T_DOUBLE &amp;&amp; in_regs[i].first()-&gt;is_FloatRegister()) {
1517       handle_index += 2;
1518       assert(handle_index &lt;= stack_slots, &quot;overflow&quot;);
1519       if (map != NULL) {
1520         __ stf(FloatRegisterImpl::D, in_regs[i].first()-&gt;as_FloatRegister(), SP, offset + STACK_BIAS);
1521       } else {
1522         __ ldf(FloatRegisterImpl::D, SP, offset + STACK_BIAS, in_regs[i].first()-&gt;as_FloatRegister());
1523       }
1524     }
1525   }
1526   // Save floats
1527   for (int i = 0; i &lt; total_in_args; i++) {
1528     int slot = handle_index + arg_save_area;
1529     int offset = slot * VMRegImpl::stack_slot_size;
1530     if (in_sig_bt[i] == T_FLOAT &amp;&amp; in_regs[i].first()-&gt;is_FloatRegister()) {
1531       handle_index++;
1532       assert(handle_index &lt;= stack_slots, &quot;overflow&quot;);
1533       if (map != NULL) {
1534         __ stf(FloatRegisterImpl::S, in_regs[i].first()-&gt;as_FloatRegister(), SP, offset + STACK_BIAS);
1535       } else {
1536         __ ldf(FloatRegisterImpl::S, SP, offset + STACK_BIAS, in_regs[i].first()-&gt;as_FloatRegister());
1537       }
1538     }
1539   }
1540 
1541 }
1542 
1543 
1544 // Check GCLocker::needs_gc and enter the runtime if it&#39;s true.  This
1545 // keeps a new JNI critical region from starting until a GC has been
1546 // forced.  Save down any oops in registers and describe them in an
1547 // OopMap.
1548 static void check_needs_gc_for_critical_native(MacroAssembler* masm,
1549                                                const int stack_slots,
1550                                                const int total_in_args,
1551                                                const int arg_save_area,
1552                                                OopMapSet* oop_maps,
1553                                                VMRegPair* in_regs,
1554                                                BasicType* in_sig_bt) {
1555   __ block_comment(&quot;check GCLocker::needs_gc&quot;);
1556   Label cont;
1557   AddressLiteral sync_state(GCLocker::needs_gc_address());
1558   __ load_bool_contents(sync_state, G3_scratch);
1559   __ cmp_zero_and_br(Assembler::equal, G3_scratch, cont);
1560   __ delayed()-&gt;nop();
1561 
1562   // Save down any values that are live in registers and call into the
1563   // runtime to halt for a GC
1564   OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
1565   save_or_restore_arguments(masm, stack_slots, total_in_args,
1566                             arg_save_area, map, in_regs, in_sig_bt);
1567 
1568   __ mov(G2_thread, L7_thread_cache);
1569 
1570   __ set_last_Java_frame(SP, noreg);
1571 
1572   __ block_comment(&quot;block_for_jni_critical&quot;);
1573   __ call(CAST_FROM_FN_PTR(address, SharedRuntime::block_for_jni_critical), relocInfo::runtime_call_type);
1574   __ delayed()-&gt;mov(L7_thread_cache, O0);
1575   oop_maps-&gt;add_gc_map( __ offset(), map);
1576 
1577   __ restore_thread(L7_thread_cache); // restore G2_thread
1578   __ reset_last_Java_frame();
1579 
1580   // Reload all the register arguments
1581   save_or_restore_arguments(masm, stack_slots, total_in_args,
1582                             arg_save_area, NULL, in_regs, in_sig_bt);
1583 
1584   __ bind(cont);
1585 #ifdef ASSERT
1586   if (StressCriticalJNINatives) {
1587     // Stress register saving
1588     OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
1589     save_or_restore_arguments(masm, stack_slots, total_in_args,
1590                               arg_save_area, map, in_regs, in_sig_bt);
1591     // Destroy argument registers
1592     for (int i = 0; i &lt; total_in_args; i++) {
1593       if (in_regs[i].first()-&gt;is_Register()) {
1594         const Register reg = in_regs[i].first()-&gt;as_Register();
1595         if (reg-&gt;is_global()) {
1596           __ mov(G0, reg);
1597         }
1598       } else if (in_regs[i].first()-&gt;is_FloatRegister()) {
1599         __ fneg(FloatRegisterImpl::D, in_regs[i].first()-&gt;as_FloatRegister(), in_regs[i].first()-&gt;as_FloatRegister());
1600       }
1601     }
1602 
1603     save_or_restore_arguments(masm, stack_slots, total_in_args,
1604                               arg_save_area, NULL, in_regs, in_sig_bt);
1605   }
1606 #endif
1607 }
1608 
1609 // Unpack an array argument into a pointer to the body and the length
1610 // if the array is non-null, otherwise pass 0 for both.
1611 static void unpack_array_argument(MacroAssembler* masm, VMRegPair reg, BasicType in_elem_type, VMRegPair body_arg, VMRegPair length_arg) {
1612   // Pass the length, ptr pair
1613   Label is_null, done;
1614   if (reg.first()-&gt;is_stack()) {
1615     VMRegPair tmp  = reg64_to_VMRegPair(L2);
1616     // Load the arg up from the stack
1617     move_ptr(masm, reg, tmp);
1618     reg = tmp;
1619   }
1620   __ cmp(reg.first()-&gt;as_Register(), G0);
1621   __ brx(Assembler::equal, false, Assembler::pt, is_null);
1622   __ delayed()-&gt;add(reg.first()-&gt;as_Register(), arrayOopDesc::base_offset_in_bytes(in_elem_type), L4);
1623   move_ptr(masm, reg64_to_VMRegPair(L4), body_arg);
1624   __ ld(reg.first()-&gt;as_Register(), arrayOopDesc::length_offset_in_bytes(), L4);
1625   move32_64(masm, reg64_to_VMRegPair(L4), length_arg);
1626   __ ba_short(done);
1627   __ bind(is_null);
1628   // Pass zeros
1629   move_ptr(masm, reg64_to_VMRegPair(G0), body_arg);
1630   move32_64(masm, reg64_to_VMRegPair(G0), length_arg);
1631   __ bind(done);
1632 }
1633 
1634 static void verify_oop_args(MacroAssembler* masm,
1635                             const methodHandle&amp; method,
1636                             const BasicType* sig_bt,
1637                             const VMRegPair* regs) {
1638   Register temp_reg = G5_method;  // not part of any compiled calling seq
1639   if (VerifyOops) {
1640     for (int i = 0; i &lt; method-&gt;size_of_parameters(); i++) {
1641       if (is_reference_type(sig_bt[i])) {
1642         VMReg r = regs[i].first();
1643         assert(r-&gt;is_valid(), &quot;bad oop arg&quot;);
1644         if (r-&gt;is_stack()) {
1645           RegisterOrConstant ld_off = reg2offset(r) + STACK_BIAS;
1646           ld_off = __ ensure_simm13_or_reg(ld_off, temp_reg);
1647           __ ld_ptr(SP, ld_off, temp_reg);
1648           __ verify_oop(temp_reg);
1649         } else {
1650           __ verify_oop(r-&gt;as_Register());
1651         }
1652       }
1653     }
1654   }
1655 }
1656 
1657 static void gen_special_dispatch(MacroAssembler* masm,
1658                                  const methodHandle&amp; method,
1659                                  const BasicType* sig_bt,
1660                                  const VMRegPair* regs) {
1661   verify_oop_args(masm, method, sig_bt, regs);
1662   vmIntrinsics::ID iid = method-&gt;intrinsic_id();
1663 
1664   // Now write the args into the outgoing interpreter space
1665   bool     has_receiver   = false;
1666   Register receiver_reg   = noreg;
1667   int      member_arg_pos = -1;
1668   Register member_reg     = noreg;
1669   int      ref_kind       = MethodHandles::signature_polymorphic_intrinsic_ref_kind(iid);
1670   if (ref_kind != 0) {
1671     member_arg_pos = method-&gt;size_of_parameters() - 1;  // trailing MemberName argument
1672     member_reg = G5_method;  // known to be free at this point
1673     has_receiver = MethodHandles::ref_kind_has_receiver(ref_kind);
1674   } else if (iid == vmIntrinsics::_invokeBasic) {
1675     has_receiver = true;
1676   } else {
1677     fatal(&quot;unexpected intrinsic id %d&quot;, iid);
1678   }
1679 
1680   if (member_reg != noreg) {
1681     // Load the member_arg into register, if necessary.
1682     SharedRuntime::check_member_name_argument_is_last_argument(method, sig_bt, regs);
1683     VMReg r = regs[member_arg_pos].first();
1684     if (r-&gt;is_stack()) {
1685       RegisterOrConstant ld_off = reg2offset(r) + STACK_BIAS;
1686       ld_off = __ ensure_simm13_or_reg(ld_off, member_reg);
1687       __ ld_ptr(SP, ld_off, member_reg);
1688     } else {
1689       // no data motion is needed
1690       member_reg = r-&gt;as_Register();
1691     }
1692   }
1693 
1694   if (has_receiver) {
1695     // Make sure the receiver is loaded into a register.
1696     assert(method-&gt;size_of_parameters() &gt; 0, &quot;oob&quot;);
1697     assert(sig_bt[0] == T_OBJECT, &quot;receiver argument must be an object&quot;);
1698     VMReg r = regs[0].first();
1699     assert(r-&gt;is_valid(), &quot;bad receiver arg&quot;);
1700     if (r-&gt;is_stack()) {
1701       // Porting note:  This assumes that compiled calling conventions always
1702       // pass the receiver oop in a register.  If this is not true on some
1703       // platform, pick a temp and load the receiver from stack.
1704       fatal(&quot;receiver always in a register&quot;);
1705       receiver_reg = G3_scratch;  // known to be free at this point
1706       RegisterOrConstant ld_off = reg2offset(r) + STACK_BIAS;
1707       ld_off = __ ensure_simm13_or_reg(ld_off, member_reg);
1708       __ ld_ptr(SP, ld_off, receiver_reg);
1709     } else {
1710       // no data motion is needed
1711       receiver_reg = r-&gt;as_Register();
1712     }
1713   }
1714 
1715   // Figure out which address we are really jumping to:
1716   MethodHandles::generate_method_handle_dispatch(masm, iid,
1717                                                  receiver_reg, member_reg, /*for_compiler_entry:*/ true);
1718 }
1719 
1720 // ---------------------------------------------------------------------------
1721 // Generate a native wrapper for a given method.  The method takes arguments
1722 // in the Java compiled code convention, marshals them to the native
1723 // convention (handlizes oops, etc), transitions to native, makes the call,
1724 // returns to java state (possibly blocking), unhandlizes any result and
1725 // returns.
1726 //
1727 // Critical native functions are a shorthand for the use of
1728 // GetPrimtiveArrayCritical and disallow the use of any other JNI
1729 // functions.  The wrapper is expected to unpack the arguments before
1730 // passing them to the callee and perform checks before and after the
1731 // native call to ensure that they GCLocker
1732 // lock_critical/unlock_critical semantics are followed.  Some other
1733 // parts of JNI setup are skipped like the tear down of the JNI handle
1734 // block and the check for pending exceptions it&#39;s impossible for them
1735 // to be thrown.
1736 //
1737 // They are roughly structured like this:
1738 //    if (GCLocker::needs_gc())
1739 //      SharedRuntime::block_for_jni_critical();
1740 //    tranistion to thread_in_native
1741 //    unpack arrray arguments and call native entry point
1742 //    check for safepoint in progress
1743 //    check if any thread suspend flags are set
1744 //      call into JVM and possible unlock the JNI critical
1745 //      if a GC was suppressed while in the critical native.
1746 //    transition back to thread_in_Java
1747 //    return to caller
1748 //
1749 nmethod* SharedRuntime::generate_native_wrapper(MacroAssembler* masm,
1750                                                 const methodHandle&amp; method,
1751                                                 int compile_id,
1752                                                 BasicType* in_sig_bt,
1753                                                 VMRegPair* in_regs,
1754                                                 BasicType ret_type,
1755                                                 address critical_entry) {
1756   if (method-&gt;is_method_handle_intrinsic()) {
1757     vmIntrinsics::ID iid = method-&gt;intrinsic_id();
1758     intptr_t start = (intptr_t)__ pc();
1759     int vep_offset = ((intptr_t)__ pc()) - start;
1760     gen_special_dispatch(masm,
1761                          method,
1762                          in_sig_bt,
1763                          in_regs);
1764     int frame_complete = ((intptr_t)__ pc()) - start;  // not complete, period
1765     __ flush();
1766     int stack_slots = SharedRuntime::out_preserve_stack_slots();  // no out slots at all, actually
1767     return nmethod::new_native_nmethod(method,
1768                                        compile_id,
1769                                        masm-&gt;code(),
1770                                        vep_offset,
1771                                        frame_complete,
1772                                        stack_slots / VMRegImpl::slots_per_word,
1773                                        in_ByteSize(-1),
1774                                        in_ByteSize(-1),
1775                                        (OopMapSet*)NULL);
1776   }
1777   bool is_critical_native = true;
1778   address native_func = critical_entry;
1779   if (native_func == NULL) {
1780     native_func = method-&gt;native_function();
1781     is_critical_native = false;
1782   }
1783   assert(native_func != NULL, &quot;must have function&quot;);
1784 
1785   // Native nmethod wrappers never take possesion of the oop arguments.
1786   // So the caller will gc the arguments. The only thing we need an
1787   // oopMap for is if the call is static
1788   //
1789   // An OopMap for lock (and class if static), and one for the VM call itself
1790   OopMapSet *oop_maps = new OopMapSet();
1791   intptr_t start = (intptr_t)__ pc();
1792 
1793   // First thing make an ic check to see if we should even be here
1794   {
1795     Label L;
1796     const Register temp_reg = G3_scratch;
1797     AddressLiteral ic_miss(SharedRuntime::get_ic_miss_stub());
1798     __ verify_oop(O0);
1799     __ load_klass(O0, temp_reg);
1800     __ cmp_and_brx_short(temp_reg, G5_inline_cache_reg, Assembler::equal, Assembler::pt, L);
1801 
1802     __ jump_to(ic_miss, temp_reg);
1803     __ delayed()-&gt;nop();
1804     __ align(CodeEntryAlignment);
1805     __ bind(L);
1806   }
1807 
1808   int vep_offset = ((intptr_t)__ pc()) - start;
1809 
1810 #ifdef COMPILER1
1811   if ((InlineObjectHash &amp;&amp; method-&gt;intrinsic_id() == vmIntrinsics::_hashCode) || (method-&gt;intrinsic_id() == vmIntrinsics::_identityHashCode)) {
1812     // Object.hashCode, System.identityHashCode can pull the hashCode from the
1813     // header word instead of doing a full VM transition once it&#39;s been computed.
1814     // Since hashCode is usually polymorphic at call sites we can&#39;t do this
1815     // optimization at the call site without a lot of work.
1816     Label slowCase;
1817     Label done;
1818     Register obj_reg              = O0;
1819     Register result               = O0;
1820     Register header               = G3_scratch;
1821     Register hash                 = G3_scratch; // overwrite header value with hash value
1822     Register mask                 = G1;         // to get hash field from header
1823 
1824     // Unlike for Object.hashCode, System.identityHashCode is static method and
1825     // gets object as argument instead of the receiver.
1826     if (method-&gt;intrinsic_id() == vmIntrinsics::_identityHashCode) {
1827       assert(method-&gt;is_static(), &quot;method should be static&quot;);
1828       // return 0 for null reference input
1829       __ br_null(obj_reg, false, Assembler::pn, done);
1830       __ delayed()-&gt;mov(obj_reg, hash);
1831     }
1832 
1833     // Read the header and build a mask to get its hash field.  Give up if the object is not unlocked.
1834     // We depend on hash_mask being at most 32 bits and avoid the use of
1835     // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
1836     // vm: see markWord.hpp.
1837     __ ld_ptr(obj_reg, oopDesc::mark_offset_in_bytes(), header);
1838     __ sethi(markWord::hash_mask, mask);
1839     __ btst(markWord::unlocked_value, header);
1840     __ br(Assembler::zero, false, Assembler::pn, slowCase);
1841     if (UseBiasedLocking) {
1842       // Check if biased and fall through to runtime if so
1843       __ delayed()-&gt;nop();
1844       __ btst(markWord::biased_lock_bit_in_place, header);
1845       __ br(Assembler::notZero, false, Assembler::pn, slowCase);
1846     }
1847     __ delayed()-&gt;or3(mask, markWord::hash_mask &amp; 0x3ff, mask);
1848 
1849     // Check for a valid (non-zero) hash code and get its value.
1850     __ srlx(header, markWord::hash_shift, hash);
1851     __ andcc(hash, mask, hash);
1852     __ br(Assembler::equal, false, Assembler::pn, slowCase);
1853     __ delayed()-&gt;nop();
1854 
1855     // leaf return.
1856     __ bind(done);
1857     __ retl();
1858     __ delayed()-&gt;mov(hash, result);
1859     __ bind(slowCase);
1860   }
1861 #endif // COMPILER1
1862 
1863 
1864   // We have received a description of where all the java arg are located
1865   // on entry to the wrapper. We need to convert these args to where
1866   // the jni function will expect them. To figure out where they go
1867   // we convert the java signature to a C signature by inserting
1868   // the hidden arguments as arg[0] and possibly arg[1] (static method)
1869 
1870   const int total_in_args = method-&gt;size_of_parameters();
1871   int total_c_args = total_in_args;
1872   int total_save_slots = 6 * VMRegImpl::slots_per_word;
1873   if (!is_critical_native) {
1874     total_c_args += 1;
1875     if (method-&gt;is_static()) {
1876       total_c_args++;
1877     }
1878   } else {
1879     for (int i = 0; i &lt; total_in_args; i++) {
1880       if (in_sig_bt[i] == T_ARRAY) {
1881         // These have to be saved and restored across the safepoint
1882         total_c_args++;
1883       }
1884     }
1885   }
1886 
1887   BasicType* out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_c_args);
1888   VMRegPair* out_regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_c_args);
1889   BasicType* in_elem_bt = NULL;
1890 
1891   int argc = 0;
1892   if (!is_critical_native) {
1893     out_sig_bt[argc++] = T_ADDRESS;
1894     if (method-&gt;is_static()) {
1895       out_sig_bt[argc++] = T_OBJECT;
1896     }
1897 
1898     for (int i = 0; i &lt; total_in_args ; i++ ) {
1899       out_sig_bt[argc++] = in_sig_bt[i];
1900     }
1901   } else {
1902     Thread* THREAD = Thread::current();
1903     in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);
1904     SignatureStream ss(method-&gt;signature());
1905     for (int i = 0; i &lt; total_in_args ; i++ ) {
1906       if (in_sig_bt[i] == T_ARRAY) {
1907         // Arrays are passed as int, elem* pair
1908         out_sig_bt[argc++] = T_INT;
1909         out_sig_bt[argc++] = T_ADDRESS;
1910         ss.skip_array_prefix(1);  // skip one &#39;[&#39;
1911         assert(ss.is_primitive(), &quot;primitive type expected&quot;);
1912         in_elem_bt[i] = ss.type();
1913       } else {
1914         out_sig_bt[argc++] = in_sig_bt[i];
1915         in_elem_bt[i] = T_VOID;
1916       }
1917       if (in_sig_bt[i] != T_VOID) {
1918         assert(in_sig_bt[i] == ss.type() ||
1919                in_sig_bt[i] == T_ARRAY, &quot;must match&quot;);
1920         ss.next();
1921       }
1922     }
1923   }
1924 
1925   // Now figure out where the args must be stored and how much stack space
1926   // they require (neglecting out_preserve_stack_slots but space for storing
1927   // the 1st six register arguments). It&#39;s weird see int_stk_helper.
1928   //
1929   int out_arg_slots;
1930   out_arg_slots = c_calling_convention(out_sig_bt, out_regs, NULL, total_c_args);
1931 
1932   if (is_critical_native) {
1933     // Critical natives may have to call out so they need a save area
1934     // for register arguments.
1935     int double_slots = 0;
1936     int single_slots = 0;
1937     for ( int i = 0; i &lt; total_in_args; i++) {
1938       if (in_regs[i].first()-&gt;is_Register()) {
1939         const Register reg = in_regs[i].first()-&gt;as_Register();
1940         switch (in_sig_bt[i]) {
1941           case T_ARRAY:
1942           case T_BOOLEAN:
1943           case T_BYTE:
1944           case T_SHORT:
1945           case T_CHAR:
1946           case T_INT:  assert(reg-&gt;is_in(), &quot;don&#39;t need to save these&quot;); break;
1947           case T_LONG: if (reg-&gt;is_global()) double_slots++; break;
1948           default:  ShouldNotReachHere();
1949         }
1950       } else if (in_regs[i].first()-&gt;is_FloatRegister()) {
1951         switch (in_sig_bt[i]) {
1952           case T_FLOAT:  single_slots++; break;
1953           case T_DOUBLE: double_slots++; break;
1954           default:  ShouldNotReachHere();
1955         }
1956       }
1957     }
1958     total_save_slots = double_slots * 2 + single_slots;
1959   }
1960 
1961   // Compute framesize for the wrapper.  We need to handlize all oops in
1962   // registers. We must create space for them here that is disjoint from
1963   // the windowed save area because we have no control over when we might
1964   // flush the window again and overwrite values that gc has since modified.
1965   // (The live window race)
1966   //
1967   // We always just allocate 6 word for storing down these object. This allow
1968   // us to simply record the base and use the Ireg number to decide which
1969   // slot to use. (Note that the reg number is the inbound number not the
1970   // outbound number).
1971   // We must shuffle args to match the native convention, and include var-args space.
1972 
1973   // Calculate the total number of stack slots we will need.
1974 
1975   // First count the abi requirement plus all of the outgoing args
1976   int stack_slots = SharedRuntime::out_preserve_stack_slots() + out_arg_slots;
1977 
1978   // Now the space for the inbound oop handle area
1979 
1980   int oop_handle_offset = align_up(stack_slots, 2);
1981   stack_slots += total_save_slots;
1982 
1983   // Now any space we need for handlizing a klass if static method
1984 
1985   int klass_slot_offset = 0;
1986   int klass_offset = -1;
1987   int lock_slot_offset = 0;
1988   bool is_static = false;
1989 
1990   if (method-&gt;is_static()) {
1991     klass_slot_offset = stack_slots;
1992     stack_slots += VMRegImpl::slots_per_word;
1993     klass_offset = klass_slot_offset * VMRegImpl::stack_slot_size;
1994     is_static = true;
1995   }
1996 
1997   // Plus a lock if needed
1998 
1999   if (method-&gt;is_synchronized()) {
2000     lock_slot_offset = stack_slots;
2001     stack_slots += VMRegImpl::slots_per_word;
2002   }
2003 
2004   // Now a place to save return value or as a temporary for any gpr -&gt; fpr moves
2005   stack_slots += 2;
2006 
2007   // Ok The space we have allocated will look like:
2008   //
2009   //
2010   // FP-&gt; |                     |
2011   //      |---------------------|
2012   //      | 2 slots for moves   |
2013   //      |---------------------|
2014   //      | lock box (if sync)  |
2015   //      |---------------------| &lt;- lock_slot_offset
2016   //      | klass (if static)   |
2017   //      |---------------------| &lt;- klass_slot_offset
2018   //      | oopHandle area      |
2019   //      |---------------------| &lt;- oop_handle_offset
2020   //      | outbound memory     |
2021   //      | based arguments     |
2022   //      |                     |
2023   //      |---------------------|
2024   //      | vararg area         |
2025   //      |---------------------|
2026   //      |                     |
2027   // SP-&gt; | out_preserved_slots |
2028   //
2029   //
2030 
2031 
2032   // Now compute actual number of stack words we need rounding to make
2033   // stack properly aligned.
2034   stack_slots = align_up(stack_slots, 2 * VMRegImpl::slots_per_word);
2035 
2036   int stack_size = stack_slots * VMRegImpl::stack_slot_size;
2037 
2038   // Generate stack overflow check before creating frame
2039   __ generate_stack_overflow_check(stack_size);
2040 
2041   // Generate a new frame for the wrapper.
2042   __ save(SP, -stack_size, SP);
2043 
2044   int frame_complete = ((intptr_t)__ pc()) - start;
2045 
2046   __ verify_thread();
2047 
2048   if (is_critical_native) {
2049     check_needs_gc_for_critical_native(masm, stack_slots,  total_in_args,
2050                                        oop_handle_offset, oop_maps, in_regs, in_sig_bt);
2051   }
2052 
2053   //
2054   // We immediately shuffle the arguments so that any vm call we have to
2055   // make from here on out (sync slow path, jvmti, etc.) we will have
2056   // captured the oops from our caller and have a valid oopMap for
2057   // them.
2058 
2059   // -----------------
2060   // The Grand Shuffle
2061   //
2062   // Natives require 1 or 2 extra arguments over the normal ones: the JNIEnv*
2063   // (derived from JavaThread* which is in L7_thread_cache) and, if static,
2064   // the class mirror instead of a receiver.  This pretty much guarantees that
2065   // register layout will not match.  We ignore these extra arguments during
2066   // the shuffle. The shuffle is described by the two calling convention
2067   // vectors we have in our possession. We simply walk the java vector to
2068   // get the source locations and the c vector to get the destinations.
2069   // Because we have a new window and the argument registers are completely
2070   // disjoint ( I0 -&gt; O1, I1 -&gt; O2, ...) we have nothing to worry about
2071   // here.
2072 
2073   // This is a trick. We double the stack slots so we can claim
2074   // the oops in the caller&#39;s frame. Since we are sure to have
2075   // more args than the caller doubling is enough to make
2076   // sure we can capture all the incoming oop args from the
2077   // caller.
2078   //
2079   OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
2080   // Record sp-based slot for receiver on stack for non-static methods
2081   int receiver_offset = -1;
2082 
2083   // We move the arguments backward because the floating point registers
2084   // destination will always be to a register with a greater or equal register
2085   // number or the stack.
2086 
2087 #ifdef ASSERT
2088   bool reg_destroyed[RegisterImpl::number_of_registers];
2089   bool freg_destroyed[FloatRegisterImpl::number_of_registers];
2090   for ( int r = 0 ; r &lt; RegisterImpl::number_of_registers ; r++ ) {
2091     reg_destroyed[r] = false;
2092   }
2093   for ( int f = 0 ; f &lt; FloatRegisterImpl::number_of_registers ; f++ ) {
2094     freg_destroyed[f] = false;
2095   }
2096 
2097 #endif /* ASSERT */
2098 
2099   for ( int i = total_in_args - 1, c_arg = total_c_args - 1; i &gt;= 0 ; i--, c_arg-- ) {
2100 
2101 #ifdef ASSERT
2102     if (in_regs[i].first()-&gt;is_Register()) {
2103       assert(!reg_destroyed[in_regs[i].first()-&gt;as_Register()-&gt;encoding()], &quot;ack!&quot;);
2104     } else if (in_regs[i].first()-&gt;is_FloatRegister()) {
2105       assert(!freg_destroyed[in_regs[i].first()-&gt;as_FloatRegister()-&gt;encoding(FloatRegisterImpl::S)], &quot;ack!&quot;);
2106     }
2107     if (out_regs[c_arg].first()-&gt;is_Register()) {
2108       reg_destroyed[out_regs[c_arg].first()-&gt;as_Register()-&gt;encoding()] = true;
2109     } else if (out_regs[c_arg].first()-&gt;is_FloatRegister()) {
2110       freg_destroyed[out_regs[c_arg].first()-&gt;as_FloatRegister()-&gt;encoding(FloatRegisterImpl::S)] = true;
2111     }
2112 #endif /* ASSERT */
2113 
2114     switch (in_sig_bt[i]) {
2115       case T_ARRAY:
2116         if (is_critical_native) {
2117           unpack_array_argument(masm, in_regs[i], in_elem_bt[i], out_regs[c_arg], out_regs[c_arg - 1]);
2118           c_arg--;
2119           break;
2120         }
2121       case T_OBJECT:
2122         assert(!is_critical_native, &quot;no oop arguments&quot;);
2123         object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],
2124                     ((i == 0) &amp;&amp; (!is_static)),
2125                     &amp;receiver_offset);
2126         break;
2127       case T_VOID:
2128         break;
2129 
2130       case T_FLOAT:
2131         float_move(masm, in_regs[i], out_regs[c_arg]);
2132         break;
2133 
2134       case T_DOUBLE:
2135         assert( i + 1 &lt; total_in_args &amp;&amp;
2136                 in_sig_bt[i + 1] == T_VOID &amp;&amp;
2137                 out_sig_bt[c_arg+1] == T_VOID, &quot;bad arg list&quot;);
2138         double_move(masm, in_regs[i], out_regs[c_arg]);
2139         break;
2140 
2141       case T_LONG :
2142         long_move(masm, in_regs[i], out_regs[c_arg]);
2143         break;
2144 
2145       case T_ADDRESS: assert(false, &quot;found T_ADDRESS in java args&quot;);
2146 
2147       default:
2148         move32_64(masm, in_regs[i], out_regs[c_arg]);
2149     }
2150   }
2151 
2152   // Pre-load a static method&#39;s oop into O1.  Used both by locking code and
2153   // the normal JNI call code.
2154   if (method-&gt;is_static() &amp;&amp; !is_critical_native) {
2155     __ set_oop_constant(JNIHandles::make_local(method-&gt;method_holder()-&gt;java_mirror()), O1);
2156 
2157     // Now handlize the static class mirror in O1.  It&#39;s known not-null.
2158     __ st_ptr(O1, SP, klass_offset + STACK_BIAS);
2159     map-&gt;set_oop(VMRegImpl::stack2reg(klass_slot_offset));
2160     __ add(SP, klass_offset + STACK_BIAS, O1);
2161   }
2162 
2163 
2164   const Register L6_handle = L6;
2165 
2166   if (method-&gt;is_synchronized()) {
2167     assert(!is_critical_native, &quot;unhandled&quot;);
2168     __ mov(O1, L6_handle);
2169   }
2170 
2171   // We have all of the arguments setup at this point. We MUST NOT touch any Oregs
2172   // except O6/O7. So if we must call out we must push a new frame. We immediately
2173   // push a new frame and flush the windows.
2174   intptr_t thepc = (intptr_t) __ pc();
2175   {
2176     address here = __ pc();
2177     // Call the next instruction
2178     __ call(here + 8, relocInfo::none);
2179     __ delayed()-&gt;nop();
2180   }
2181 
2182   // We use the same pc/oopMap repeatedly when we call out
2183   oop_maps-&gt;add_gc_map(thepc - start, map);
2184 
2185   // O7 now has the pc loaded that we will use when we finally call to native.
2186 
2187   // Save thread in L7; it crosses a bunch of VM calls below
2188   // Don&#39;t use save_thread because it smashes G2 and we merely
2189   // want to save a copy
2190   __ mov(G2_thread, L7_thread_cache);
2191 
2192 
2193   // If we create an inner frame once is plenty
2194   // when we create it we must also save G2_thread
2195   bool inner_frame_created = false;
2196 
2197   // dtrace method entry support
2198   {
2199     SkipIfEqual skip_if(
2200       masm, G3_scratch, &amp;DTraceMethodProbes, Assembler::zero);
2201     // create inner frame
2202     __ save_frame(0);
2203     __ mov(G2_thread, L7_thread_cache);
2204     __ set_metadata_constant(method(), O1);
2205     __ call_VM_leaf(L7_thread_cache,
2206          CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_entry),
2207          G2_thread, O1);
2208     __ restore();
2209   }
2210 
2211   // RedefineClasses() tracing support for obsolete method entry
2212   if (log_is_enabled(Trace, redefine, class, obsolete)) {
2213     // create inner frame
2214     __ save_frame(0);
2215     __ mov(G2_thread, L7_thread_cache);
2216     __ set_metadata_constant(method(), O1);
2217     __ call_VM_leaf(L7_thread_cache,
2218          CAST_FROM_FN_PTR(address, SharedRuntime::rc_trace_method_entry),
2219          G2_thread, O1);
2220     __ restore();
2221   }
2222 
2223   // We are in the jni frame unless saved_frame is true in which case
2224   // we are in one frame deeper (the &quot;inner&quot; frame). If we are in the
2225   // &quot;inner&quot; frames the args are in the Iregs and if the jni frame then
2226   // they are in the Oregs.
2227   // If we ever need to go to the VM (for locking, jvmti) then
2228   // we will always be in the &quot;inner&quot; frame.
2229 
2230   // Lock a synchronized method
2231   int lock_offset = -1;         // Set if locked
2232   if (method-&gt;is_synchronized()) {
2233     Register Roop = O1;
2234     const Register L3_box = L3;
2235 
2236     create_inner_frame(masm, &amp;inner_frame_created);
2237 
2238     __ ld_ptr(I1, 0, O1);
2239     Label done;
2240 
2241     lock_offset = (lock_slot_offset * VMRegImpl::stack_slot_size);
2242     __ add(FP, lock_offset+STACK_BIAS, L3_box);
2243 #ifdef ASSERT
2244     if (UseBiasedLocking) {
2245       // making the box point to itself will make it clear it went unused
2246       // but also be obviously invalid
2247       __ st_ptr(L3_box, L3_box, 0);
2248     }
2249 #endif // ASSERT
2250     //
2251     // Compiler_lock_object (Roop, Rmark, Rbox, Rscratch) -- kills Rmark, Rbox, Rscratch
2252     //
2253     __ compiler_lock_object(Roop, L1,    L3_box, L2);
2254     __ br(Assembler::equal, false, Assembler::pt, done);
2255     __ delayed() -&gt; add(FP, lock_offset+STACK_BIAS, L3_box);
2256 
2257 
2258     // None of the above fast optimizations worked so we have to get into the
2259     // slow case of monitor enter.  Inline a special case of call_VM that
2260     // disallows any pending_exception.
2261     __ mov(Roop, O0);            // Need oop in O0
2262     __ mov(L3_box, O1);
2263 
2264     // Record last_Java_sp, in case the VM code releases the JVM lock.
2265 
2266     __ set_last_Java_frame(FP, I7);
2267 
2268     // do the call
2269     __ call(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_locking_C), relocInfo::runtime_call_type);
2270     __ delayed()-&gt;mov(L7_thread_cache, O2);
2271 
2272     __ restore_thread(L7_thread_cache); // restore G2_thread
2273     __ reset_last_Java_frame();
2274 
2275 #ifdef ASSERT
2276     { Label L;
2277     __ ld_ptr(G2_thread, in_bytes(Thread::pending_exception_offset()), O0);
2278     __ br_null_short(O0, Assembler::pt, L);
2279     __ stop(&quot;no pending exception allowed on exit from IR::monitorenter&quot;);
2280     __ bind(L);
2281     }
2282 #endif
2283     __ bind(done);
2284   }
2285 
2286 
2287   // Finally just about ready to make the JNI call
2288 
2289   __ flushw();
2290   if (inner_frame_created) {
2291     __ restore();
2292   } else {
2293     // Store only what we need from this frame
2294     // QQQ I think that non-v9 (like we care) we don&#39;t need these saves
2295     // either as the flush traps and the current window goes too.
2296     __ st_ptr(FP, SP, FP-&gt;sp_offset_in_saved_window()*wordSize + STACK_BIAS);
2297     __ st_ptr(I7, SP, I7-&gt;sp_offset_in_saved_window()*wordSize + STACK_BIAS);
2298   }
2299 
2300   // get JNIEnv* which is first argument to native
2301   if (!is_critical_native) {
2302     __ add(G2_thread, in_bytes(JavaThread::jni_environment_offset()), O0);
2303   }
2304 
2305   // Use that pc we placed in O7 a while back as the current frame anchor
2306   __ set_last_Java_frame(SP, O7);
2307 
2308   // We flushed the windows ages ago now mark them as flushed before transitioning.
2309   __ set(JavaFrameAnchor::flushed, G3_scratch);
2310   __ st(G3_scratch, G2_thread, JavaThread::frame_anchor_offset() + JavaFrameAnchor::flags_offset());
2311 
2312   // Transition from _thread_in_Java to _thread_in_native.
2313   __ set(_thread_in_native, G3_scratch);
2314 
2315   AddressLiteral dest(native_func);
2316   __ relocate(relocInfo::runtime_call_type);
2317   __ jumpl_to(dest, O7, O7);
2318   __ delayed()-&gt;st(G3_scratch, G2_thread, JavaThread::thread_state_offset());
2319 
2320   __ restore_thread(L7_thread_cache); // restore G2_thread
2321 
2322   // Unpack native results.  For int-types, we do any needed sign-extension
2323   // and move things into I0.  The return value there will survive any VM
2324   // calls for blocking or unlocking.  An FP or OOP result (handle) is done
2325   // specially in the slow-path code.
2326   switch (ret_type) {
2327   case T_VOID:    break;        // Nothing to do!
2328   case T_FLOAT:   break;        // Got it where we want it (unless slow-path)
2329   case T_DOUBLE:  break;        // Got it where we want it (unless slow-path)
2330   // In 64 bits build result is in O0, in O0, O1 in 32bit build
2331   case T_LONG:
2332                   // Fall thru
2333   case T_OBJECT:                // Really a handle
2334   case T_ARRAY:
2335   case T_INT:
2336                   __ mov(O0, I0);
2337                   break;
2338   case T_BOOLEAN: __ subcc(G0, O0, G0); __ addc(G0, 0, I0); break; // !0 =&gt; true; 0 =&gt; false
2339   case T_BYTE   : __ sll(O0, 24, O0); __ sra(O0, 24, I0);   break;
2340   case T_CHAR   : __ sll(O0, 16, O0); __ srl(O0, 16, I0);   break; // cannot use and3, 0xFFFF too big as immediate value!
2341   case T_SHORT  : __ sll(O0, 16, O0); __ sra(O0, 16, I0);   break;
2342     break;                      // Cannot de-handlize until after reclaiming jvm_lock
2343   default:
2344     ShouldNotReachHere();
2345   }
2346 
2347   Label after_transition;
2348   // must we block?
2349 
2350   // Block, if necessary, before resuming in _thread_in_Java state.
2351   // In order for GC to work, don&#39;t clear the last_Java_sp until after blocking.
2352   { Label no_block;
2353 
2354     // Switch thread to &quot;native transition&quot; state before reading the synchronization state.
2355     // This additional state is necessary because reading and testing the synchronization
2356     // state is not atomic w.r.t. GC, as this scenario demonstrates:
2357     //     Java thread A, in _thread_in_native state, loads _not_synchronized and is preempted.
2358     //     VM thread changes sync state to synchronizing and suspends threads for GC.
2359     //     Thread A is resumed to finish this native method, but doesn&#39;t block here since it
2360     //     didn&#39;t see any synchronization is progress, and escapes.
2361     __ set(_thread_in_native_trans, G3_scratch);
2362     __ st(G3_scratch, G2_thread, JavaThread::thread_state_offset());
2363 
2364     // Force this write out before the read below
2365     __ membar(Assembler::StoreLoad);
2366 
2367     Label L;
2368     Address suspend_state(G2_thread, JavaThread::suspend_flags_offset());
2369     __ safepoint_poll(L, false, G2_thread, G3_scratch);
2370     __ delayed()-&gt;ld(suspend_state, G3_scratch);
2371     __ cmp_and_br_short(G3_scratch, 0, Assembler::equal, Assembler::pt, no_block);
2372     __ bind(L);
2373 
2374     // Block.  Save any potential method result value before the operation and
2375     // use a leaf call to leave the last_Java_frame setup undisturbed. Doing this
2376     // lets us share the oopMap we used when we went native rather the create
2377     // a distinct one for this pc
2378     //
2379     save_native_result(masm, ret_type, stack_slots);
2380     if (!is_critical_native) {
2381       __ call_VM_leaf(L7_thread_cache,
2382                       CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans),
2383                       G2_thread);
2384     } else {
2385       __ call_VM_leaf(L7_thread_cache,
2386                       CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition),
2387                       G2_thread);
2388     }
2389 
2390     // Restore any method result value
2391     restore_native_result(masm, ret_type, stack_slots);
2392 
2393     if (is_critical_native) {
2394       // The call above performed the transition to thread_in_Java so
2395       // skip the transition logic below.
2396       __ ba(after_transition);
2397       __ delayed()-&gt;nop();
2398     }
2399 
2400     __ bind(no_block);
2401   }
2402 
2403   // thread state is thread_in_native_trans. Any safepoint blocking has already
2404   // happened so we can now change state to _thread_in_Java.
2405   __ set(_thread_in_Java, G3_scratch);
2406   __ st(G3_scratch, G2_thread, JavaThread::thread_state_offset());
2407   __ bind(after_transition);
2408 
2409   Label no_reguard;
2410   __ ld(G2_thread, JavaThread::stack_guard_state_offset(), G3_scratch);
2411   __ cmp_and_br_short(G3_scratch, JavaThread::stack_guard_yellow_reserved_disabled, Assembler::notEqual, Assembler::pt, no_reguard);
2412 
2413     save_native_result(masm, ret_type, stack_slots);
2414   __ call(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));
2415   __ delayed()-&gt;nop();
2416 
2417   __ restore_thread(L7_thread_cache); // restore G2_thread
2418     restore_native_result(masm, ret_type, stack_slots);
2419 
2420   __ bind(no_reguard);
2421 
2422   // Handle possible exception (will unlock if necessary)
2423 
2424   // native result if any is live in freg or I0 (and I1 if long and 32bit vm)
2425 
2426   // Unlock
2427   if (method-&gt;is_synchronized()) {
2428     Label done;
2429     Register I2_ex_oop = I2;
2430     const Register L3_box = L3;
2431     // Get locked oop from the handle we passed to jni
2432     __ ld_ptr(L6_handle, 0, L4);
2433     __ add(SP, lock_offset+STACK_BIAS, L3_box);
2434     // Must save pending exception around the slow-path VM call.  Since it&#39;s a
2435     // leaf call, the pending exception (if any) can be kept in a register.
2436     __ ld_ptr(G2_thread, in_bytes(Thread::pending_exception_offset()), I2_ex_oop);
2437     // Now unlock
2438     //                       (Roop, Rmark, Rbox,   Rscratch)
2439     __ compiler_unlock_object(L4,   L1,    L3_box, L2);
2440     __ br(Assembler::equal, false, Assembler::pt, done);
2441     __ delayed()-&gt; add(SP, lock_offset+STACK_BIAS, L3_box);
2442 
2443     // save and restore any potential method result value around the unlocking
2444     // operation.  Will save in I0 (or stack for FP returns).
2445     save_native_result(masm, ret_type, stack_slots);
2446 
2447     // Must clear pending-exception before re-entering the VM.  Since this is
2448     // a leaf call, pending-exception-oop can be safely kept in a register.
2449     __ st_ptr(G0, G2_thread, in_bytes(Thread::pending_exception_offset()));
2450 
2451     // slow case of monitor enter.  Inline a special case of call_VM that
2452     // disallows any pending_exception.
2453     __ mov(L3_box, O1);
2454 
2455     // Pass in current thread pointer
2456     __ mov(G2_thread, O2);
2457 
2458     __ call(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C), relocInfo::runtime_call_type);
2459     __ delayed()-&gt;mov(L4, O0);              // Need oop in O0
2460 
2461     __ restore_thread(L7_thread_cache); // restore G2_thread
2462 
2463 #ifdef ASSERT
2464     { Label L;
2465     __ ld_ptr(G2_thread, in_bytes(Thread::pending_exception_offset()), O0);
2466     __ br_null_short(O0, Assembler::pt, L);
2467     __ stop(&quot;no pending exception allowed on exit from IR::monitorexit&quot;);
2468     __ bind(L);
2469     }
2470 #endif
2471     restore_native_result(masm, ret_type, stack_slots);
2472     // check_forward_pending_exception jump to forward_exception if any pending
2473     // exception is set.  The forward_exception routine expects to see the
2474     // exception in pending_exception and not in a register.  Kind of clumsy,
2475     // since all folks who branch to forward_exception must have tested
2476     // pending_exception first and hence have it in a register already.
2477     __ st_ptr(I2_ex_oop, G2_thread, in_bytes(Thread::pending_exception_offset()));
2478     __ bind(done);
2479   }
2480 
2481   // Tell dtrace about this method exit
2482   {
2483     SkipIfEqual skip_if(
2484       masm, G3_scratch, &amp;DTraceMethodProbes, Assembler::zero);
2485     save_native_result(masm, ret_type, stack_slots);
2486     __ set_metadata_constant(method(), O1);
2487     __ call_VM_leaf(L7_thread_cache,
2488        CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit),
2489        G2_thread, O1);
2490     restore_native_result(masm, ret_type, stack_slots);
2491   }
2492 
2493   // Clear &quot;last Java frame&quot; SP and PC.
2494   __ verify_thread(); // G2_thread must be correct
2495   __ reset_last_Java_frame();
2496 
2497   // Unbox oop result, e.g. JNIHandles::resolve value in I0.
2498   if (is_reference_type(ret_type)) {
2499     __ resolve_jobject(I0, G3_scratch);
2500   }
2501 
2502   if (CheckJNICalls) {
2503     // clear_pending_jni_exception_check
2504     __ st_ptr(G0, G2_thread, JavaThread::pending_jni_exception_check_fn_offset());
2505   }
2506 
2507   if (!is_critical_native) {
2508     // reset handle block
2509     __ ld_ptr(G2_thread, in_bytes(JavaThread::active_handles_offset()), L5);
2510     __ st(G0, L5, JNIHandleBlock::top_offset_in_bytes());
2511 
2512     __ ld_ptr(G2_thread, in_bytes(Thread::pending_exception_offset()), G3_scratch);
2513     check_forward_pending_exception(masm, G3_scratch);
2514   }
2515 
2516 
2517   // Return
2518 
2519   __ ret();
2520   __ delayed()-&gt;restore();
2521 
2522   __ flush();
2523 
2524   nmethod *nm = nmethod::new_native_nmethod(method,
2525                                             compile_id,
2526                                             masm-&gt;code(),
2527                                             vep_offset,
2528                                             frame_complete,
2529                                             stack_slots / VMRegImpl::slots_per_word,
2530                                             (is_static ? in_ByteSize(klass_offset) : in_ByteSize(receiver_offset)),
2531                                             in_ByteSize(lock_offset),
2532                                             oop_maps);
2533 
2534   if (is_critical_native) {
2535     nm-&gt;set_lazy_critical_native(true);
2536   }
2537   return nm;
2538 
2539 }
2540 
2541 // this function returns the adjust size (in number of words) to a c2i adapter
2542 // activation for use during deoptimization
2543 int Deoptimization::last_frame_adjust(int callee_parameters, int callee_locals) {
2544   assert(callee_locals &gt;= callee_parameters,
2545           &quot;test and remove; got more parms than locals&quot;);
2546   if (callee_locals &lt; callee_parameters)
2547     return 0;                   // No adjustment for negative locals
2548   int diff = (callee_locals - callee_parameters) * Interpreter::stackElementWords;
2549   return align_up(diff, WordsPerLong);
2550 }
2551 
2552 // &quot;Top of Stack&quot; slots that may be unused by the calling convention but must
2553 // otherwise be preserved.
2554 // On Intel these are not necessary and the value can be zero.
2555 // On Sparc this describes the words reserved for storing a register window
2556 // when an interrupt occurs.
2557 uint SharedRuntime::out_preserve_stack_slots() {
2558   return frame::register_save_words * VMRegImpl::slots_per_word;
2559 }
2560 
2561 static void gen_new_frame(MacroAssembler* masm, bool deopt) {
2562 //
2563 // Common out the new frame generation for deopt and uncommon trap
2564 //
2565   Register        G3pcs              = G3_scratch; // Array of new pcs (input)
2566   Register        Oreturn0           = O0;
2567   Register        Oreturn1           = O1;
2568   Register        O2UnrollBlock      = O2;
2569   Register        O3array            = O3;         // Array of frame sizes (input)
2570   Register        O4array_size       = O4;         // number of frames (input)
2571   Register        O7frame_size       = O7;         // number of frames (input)
2572 
2573   __ ld_ptr(O3array, 0, O7frame_size);
2574   __ sub(G0, O7frame_size, O7frame_size);
2575   __ save(SP, O7frame_size, SP);
2576   __ ld_ptr(G3pcs, 0, I7);                      // load frame&#39;s new pc
2577 
2578   #ifdef ASSERT
2579   // make sure that the frames are aligned properly
2580   #endif
2581 
2582   // Deopt needs to pass some extra live values from frame to frame
2583 
2584   if (deopt) {
2585     __ mov(Oreturn0-&gt;after_save(), Oreturn0);
2586     __ mov(Oreturn1-&gt;after_save(), Oreturn1);
2587   }
2588 
2589   __ mov(O4array_size-&gt;after_save(), O4array_size);
2590   __ sub(O4array_size, 1, O4array_size);
2591   __ mov(O3array-&gt;after_save(), O3array);
2592   __ mov(O2UnrollBlock-&gt;after_save(), O2UnrollBlock);
2593   __ add(G3pcs, wordSize, G3pcs);               // point to next pc value
2594 
2595   #ifdef ASSERT
2596   // trash registers to show a clear pattern in backtraces
2597   __ set(0xDEAD0000, I0);
2598   __ add(I0,  2, I1);
2599   __ add(I0,  4, I2);
2600   __ add(I0,  6, I3);
2601   __ add(I0,  8, I4);
2602   // Don&#39;t touch I5 could have valuable savedSP
2603   __ set(0xDEADBEEF, L0);
2604   __ mov(L0, L1);
2605   __ mov(L0, L2);
2606   __ mov(L0, L3);
2607   __ mov(L0, L4);
2608   __ mov(L0, L5);
2609 
2610   // trash the return value as there is nothing to return yet
2611   __ set(0xDEAD0001, O7);
2612   #endif
2613 
2614   __ mov(SP, O5_savedSP);
2615 }
2616 
2617 
2618 static void make_new_frames(MacroAssembler* masm, bool deopt) {
2619   //
2620   // loop through the UnrollBlock info and create new frames
2621   //
2622   Register        G3pcs              = G3_scratch;
2623   Register        Oreturn0           = O0;
2624   Register        Oreturn1           = O1;
2625   Register        O2UnrollBlock      = O2;
2626   Register        O3array            = O3;
2627   Register        O4array_size       = O4;
2628   Label           loop;
2629 
2630 #ifdef ASSERT
2631   // Compilers generate code that bang the stack by as much as the
2632   // interpreter would need. So this stack banging should never
2633   // trigger a fault. Verify that it does not on non product builds.
2634   if (UseStackBanging) {
2635     // Get total frame size for interpreted frames
2636     __ ld(O2UnrollBlock, Deoptimization::UnrollBlock::total_frame_sizes_offset_in_bytes(), O4);
2637     __ bang_stack_size(O4, O3, G3_scratch);
2638   }
2639 #endif
2640 
2641   __ ld(O2UnrollBlock, Deoptimization::UnrollBlock::number_of_frames_offset_in_bytes(), O4array_size);
2642   __ ld_ptr(O2UnrollBlock, Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes(), G3pcs);
2643   __ ld_ptr(O2UnrollBlock, Deoptimization::UnrollBlock::frame_sizes_offset_in_bytes(), O3array);
2644 
2645   // Adjust old interpreter frame to make space for new frame&#39;s extra java locals
2646   //
2647   // We capture the original sp for the transition frame only because it is needed in
2648   // order to properly calculate interpreter_sp_adjustment. Even though in real life
2649   // every interpreter frame captures a savedSP it is only needed at the transition
2650   // (fortunately). If we had to have it correct everywhere then we would need to
2651   // be told the sp_adjustment for each frame we create. If the frame size array
2652   // were to have twice the frame count entries then we could have pairs [sp_adjustment, frame_size]
2653   // for each frame we create and keep up the illusion every where.
2654   //
2655 
2656   __ ld(O2UnrollBlock, Deoptimization::UnrollBlock::caller_adjustment_offset_in_bytes(), O7);
2657   __ mov(SP, O5_savedSP);       // remember initial sender&#39;s original sp before adjustment
2658   __ sub(SP, O7, SP);
2659 
2660 #ifdef ASSERT
2661   // make sure that there is at least one entry in the array
2662   __ tst(O4array_size);
2663   __ breakpoint_trap(Assembler::zero, Assembler::icc);
2664 #endif
2665 
2666   // Now push the new interpreter frames
2667   __ bind(loop);
2668 
2669   // allocate a new frame, filling the registers
2670 
2671   gen_new_frame(masm, deopt);        // allocate an interpreter frame
2672 
2673   __ cmp_zero_and_br(Assembler::notZero, O4array_size, loop);
2674   __ delayed()-&gt;add(O3array, wordSize, O3array);
2675   __ ld_ptr(G3pcs, 0, O7);                      // load final frame new pc
2676 
2677 }
2678 
2679 //------------------------------generate_deopt_blob----------------------------
2680 // Ought to generate an ideal graph &amp; compile, but here&#39;s some SPARC ASM
2681 // instead.
2682 void SharedRuntime::generate_deopt_blob() {
2683   // allocate space for the code
2684   ResourceMark rm;
2685   // setup code generation tools
2686   int pad = VerifyThread ? 512 : 0;// Extra slop space for more verify code
2687 #ifdef ASSERT
2688   if (UseStackBanging) {
2689     pad += (JavaThread::stack_shadow_zone_size() / os::vm_page_size())*16 + 32;
2690   }
2691 #endif
2692 #if INCLUDE_JVMCI
2693   if (EnableJVMCI) {
2694     pad += 1000; // Increase the buffer size when compiling for JVMCI
2695   }
2696 #endif
2697   CodeBuffer buffer(&quot;deopt_blob&quot;, 2100+pad, 512);
2698   MacroAssembler* masm               = new MacroAssembler(&amp;buffer);
2699   FloatRegister   Freturn0           = F0;
2700   Register        Greturn1           = G1;
2701   Register        Oreturn0           = O0;
2702   Register        Oreturn1           = O1;
2703   Register        O2UnrollBlock      = O2;
2704   Register        L0deopt_mode       = L0;
2705   Register        G4deopt_mode       = G4_scratch;
2706   int             frame_size_words;
2707   Address         saved_Freturn0_addr(FP, -sizeof(double) + STACK_BIAS);
2708   Label           cont;
2709 
2710   OopMapSet *oop_maps = new OopMapSet();
2711 
2712   //
2713   // This is the entry point for code which is returning to a de-optimized
2714   // frame.
2715   // The steps taken by this frame are as follows:
2716   //   - push a dummy &quot;register_save&quot; and save the return values (O0, O1, F0/F1, G1)
2717   //     and all potentially live registers (at a pollpoint many registers can be live).
2718   //
2719   //   - call the C routine: Deoptimization::fetch_unroll_info (this function
2720   //     returns information about the number and size of interpreter frames
2721   //     which are equivalent to the frame which is being deoptimized)
2722   //   - deallocate the unpack frame, restoring only results values. Other
2723   //     volatile registers will now be captured in the vframeArray as needed.
2724   //   - deallocate the deoptimization frame
2725   //   - in a loop using the information returned in the previous step
2726   //     push new interpreter frames (take care to propagate the return
2727   //     values through each new frame pushed)
2728   //   - create a dummy &quot;unpack_frame&quot; and save the return values (O0, O1, F0)
2729   //   - call the C routine: Deoptimization::unpack_frames (this function
2730   //     lays out values on the interpreter frame which was just created)
2731   //   - deallocate the dummy unpack_frame
2732   //   - ensure that all the return values are correctly set and then do
2733   //     a return to the interpreter entry point
2734   //
2735   // Refer to the following methods for more information:
2736   //   - Deoptimization::fetch_unroll_info
2737   //   - Deoptimization::unpack_frames
2738 
2739   OopMap* map = NULL;
2740 
2741   int start = __ offset();
2742 
2743   // restore G2, the trampoline destroyed it
2744   __ get_thread();
2745 
2746   // On entry we have been called by the deoptimized nmethod with a call that
2747   // replaced the original call (or safepoint polling location) so the deoptimizing
2748   // pc is now in O7. Return values are still in the expected places
2749 
2750   map = RegisterSaver::save_live_registers(masm, 0, &amp;frame_size_words);
2751   __ ba(cont);
2752   __ delayed()-&gt;mov(Deoptimization::Unpack_deopt, L0deopt_mode);
2753 
2754 
2755 #if INCLUDE_JVMCI
2756   Label after_fetch_unroll_info_call;
2757   int implicit_exception_uncommon_trap_offset = 0;
2758   int uncommon_trap_offset = 0;
2759 
2760   if (EnableJVMCI) {
2761     masm-&gt;block_comment(&quot;BEGIN implicit_exception_uncommon_trap&quot;);
2762     implicit_exception_uncommon_trap_offset = __ offset() - start;
2763 
2764     __ ld_ptr(G2_thread, in_bytes(JavaThread::jvmci_implicit_exception_pc_offset()), O7);
2765     __ st_ptr(G0, Address(G2_thread, in_bytes(JavaThread::jvmci_implicit_exception_pc_offset())));
2766     __ add(O7, -8, O7);
2767 
2768     uncommon_trap_offset = __ offset() - start;
2769 
2770     // Save everything in sight.
2771     (void) RegisterSaver::save_live_registers(masm, 0, &amp;frame_size_words);
2772     __ set_last_Java_frame(SP, NULL);
2773 
2774     __ ld(G2_thread, in_bytes(JavaThread::pending_deoptimization_offset()), O1);
2775     __ sub(G0, 1, L1);
2776     __ st(L1, G2_thread, in_bytes(JavaThread::pending_deoptimization_offset()));
2777 
2778     __ mov((int32_t)Deoptimization::Unpack_reexecute, L0deopt_mode);
2779     __ mov(G2_thread, O0);
2780     __ mov(L0deopt_mode, O2);
2781     __ call(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap));
2782     __ delayed()-&gt;nop();
2783     oop_maps-&gt;add_gc_map( __ offset()-start, map-&gt;deep_copy());
2784     __ get_thread();
2785     __ add(O7, 8, O7);
2786     __ reset_last_Java_frame();
2787 
2788     __ ba(after_fetch_unroll_info_call);
2789     __ delayed()-&gt;nop(); // Delay slot
2790     masm-&gt;block_comment(&quot;END implicit_exception_uncommon_trap&quot;);
2791   } // EnableJVMCI
2792 #endif // INCLUDE_JVMCI
2793 
2794   int exception_offset = __ offset() - start;
2795 
2796   // restore G2, the trampoline destroyed it
2797   __ get_thread();
2798 
2799   // On entry we have been jumped to by the exception handler (or exception_blob
2800   // for server).  O0 contains the exception oop and O7 contains the original
2801   // exception pc.  So if we push a frame here it will look to the
2802   // stack walking code (fetch_unroll_info) just like a normal call so
2803   // state will be extracted normally.
2804 
2805   // save exception oop in JavaThread and fall through into the
2806   // exception_in_tls case since they are handled in same way except
2807   // for where the pending exception is kept.
2808   __ st_ptr(Oexception, G2_thread, JavaThread::exception_oop_offset());
2809 
2810   //
2811   // Vanilla deoptimization with an exception pending in exception_oop
2812   //
2813   int exception_in_tls_offset = __ offset() - start;
2814 
2815   // No need to update oop_map  as each call to save_live_registers will produce identical oopmap
2816   // Opens a new stack frame
2817   (void) RegisterSaver::save_live_registers(masm, 0, &amp;frame_size_words);
2818 
2819   // Restore G2_thread
2820   __ get_thread();
2821 
2822 #ifdef ASSERT
2823   {
2824     // verify that there is really an exception oop in exception_oop
2825     Label has_exception;
2826     __ ld_ptr(G2_thread, JavaThread::exception_oop_offset(), Oexception);
2827     __ br_notnull_short(Oexception, Assembler::pt, has_exception);
2828     __ stop(&quot;no exception in thread&quot;);
2829     __ bind(has_exception);
2830 
2831     // verify that there is no pending exception
2832     Label no_pending_exception;
2833     Address exception_addr(G2_thread, Thread::pending_exception_offset());
2834     __ ld_ptr(exception_addr, Oexception);
2835     __ br_null_short(Oexception, Assembler::pt, no_pending_exception);
2836     __ stop(&quot;must not have pending exception here&quot;);
2837     __ bind(no_pending_exception);
2838   }
2839 #endif
2840 
2841   __ ba(cont);
2842   __ delayed()-&gt;mov(Deoptimization::Unpack_exception, L0deopt_mode);;
2843 
2844   //
2845   // Reexecute entry, similar to c2 uncommon trap
2846   //
2847   int reexecute_offset = __ offset() - start;
2848 #if INCLUDE_JVMCI &amp;&amp; !defined(COMPILER1)
2849   if (EnableJVMCI &amp;&amp; UseJVMCICompiler) {
2850     // JVMCI does not use this kind of deoptimization
2851     __ should_not_reach_here();
2852   }
2853 #endif
2854   // No need to update oop_map  as each call to save_live_registers will produce identical oopmap
2855   (void) RegisterSaver::save_live_registers(masm, 0, &amp;frame_size_words);
2856 
2857   __ mov(Deoptimization::Unpack_reexecute, L0deopt_mode);
2858 
2859   __ bind(cont);
2860 
2861   __ set_last_Java_frame(SP, noreg);
2862 
2863   // do the call by hand so we can get the oopmap
2864 
2865   __ mov(G2_thread, L7_thread_cache);
2866   __ mov(L0deopt_mode, O1);
2867   __ call(CAST_FROM_FN_PTR(address, Deoptimization::fetch_unroll_info), relocInfo::runtime_call_type);
2868   __ delayed()-&gt;mov(G2_thread, O0);
2869 
2870   // Set an oopmap for the call site this describes all our saved volatile registers
2871 
2872   oop_maps-&gt;add_gc_map( __ offset()-start, map);
2873 
2874   __ mov(L7_thread_cache, G2_thread);
2875 
2876   __ reset_last_Java_frame();
2877 
2878 #if INCLUDE_JVMCI
2879   if (EnableJVMCI) {
2880     __ bind(after_fetch_unroll_info_call);
2881   }
2882 #endif
2883   // NOTE: we know that only O0/O1 will be reloaded by restore_result_registers
2884   // so this move will survive
2885 
2886   __ mov(L0deopt_mode, G4deopt_mode);
2887 
2888   __ mov(O0, O2UnrollBlock-&gt;after_save());
2889 
2890   RegisterSaver::restore_result_registers(masm);
2891 
2892   __ ld(O2UnrollBlock, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes(), G4deopt_mode);
2893   Label noException;
2894   __ cmp_and_br_short(G4deopt_mode, Deoptimization::Unpack_exception, Assembler::notEqual, Assembler::pt, noException);
2895 
2896   // Move the pending exception from exception_oop to Oexception so
2897   // the pending exception will be picked up the interpreter.
2898   __ ld_ptr(G2_thread, in_bytes(JavaThread::exception_oop_offset()), Oexception);
2899   __ st_ptr(G0, G2_thread, in_bytes(JavaThread::exception_oop_offset()));
2900   __ st_ptr(G0, G2_thread, in_bytes(JavaThread::exception_pc_offset()));
2901   __ bind(noException);
2902 
2903   // deallocate the deoptimization frame taking care to preserve the return values
2904   __ mov(Oreturn0,     Oreturn0-&gt;after_save());
2905   __ mov(Oreturn1,     Oreturn1-&gt;after_save());
2906   __ mov(O2UnrollBlock, O2UnrollBlock-&gt;after_save());
2907   __ restore();
2908 
2909   // Allocate new interpreter frame(s) and possible c2i adapter frame
2910 
2911   make_new_frames(masm, true);
2912 
2913   // push a dummy &quot;unpack_frame&quot; taking care of float return values and
2914   // call Deoptimization::unpack_frames to have the unpacker layout
2915   // information in the interpreter frames just created and then return
2916   // to the interpreter entry point
2917   __ save(SP, -frame_size_words*wordSize, SP);
2918   __ stf(FloatRegisterImpl::D, Freturn0, saved_Freturn0_addr);
2919   // LP64 uses g4 in set_last_Java_frame
2920   __ mov(G4deopt_mode, O1);
2921   __ set_last_Java_frame(SP, G0);
2922   __ call_VM_leaf(L7_thread_cache, CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames), G2_thread, O1);
2923   __ reset_last_Java_frame();
2924   __ ldf(FloatRegisterImpl::D, saved_Freturn0_addr, Freturn0);
2925 
2926   __ ret();
2927   __ delayed()-&gt;restore();
2928 
2929   masm-&gt;flush();
2930   _deopt_blob = DeoptimizationBlob::create(&amp;buffer, oop_maps, 0, exception_offset, reexecute_offset, frame_size_words);
2931   _deopt_blob-&gt;set_unpack_with_exception_in_tls_offset(exception_in_tls_offset);
2932 #if INCLUDE_JVMCI
2933   if (EnableJVMCI) {
2934     _deopt_blob-&gt;set_uncommon_trap_offset(uncommon_trap_offset);
2935     _deopt_blob-&gt;set_implicit_exception_uncommon_trap_offset(implicit_exception_uncommon_trap_offset);
2936   }
2937 #endif
2938 }
2939 
2940 #ifdef COMPILER2
2941 
2942 //------------------------------generate_uncommon_trap_blob--------------------
2943 // Ought to generate an ideal graph &amp; compile, but here&#39;s some SPARC ASM
2944 // instead.
2945 void SharedRuntime::generate_uncommon_trap_blob() {
2946   // allocate space for the code
2947   ResourceMark rm;
2948   // setup code generation tools
2949   int pad = VerifyThread ? 512 : 0;
2950 #ifdef ASSERT
2951   if (UseStackBanging) {
2952     pad += (JavaThread::stack_shadow_zone_size() / os::vm_page_size())*16 + 32;
2953   }
2954 #endif
2955   CodeBuffer buffer(&quot;uncommon_trap_blob&quot;, 2700+pad, 512);
2956   MacroAssembler* masm               = new MacroAssembler(&amp;buffer);
2957   Register        O2UnrollBlock      = O2;
2958   Register        O2klass_index      = O2;
2959 
2960   //
2961   // This is the entry point for all traps the compiler takes when it thinks
2962   // it cannot handle further execution of compilation code. The frame is
2963   // deoptimized in these cases and converted into interpreter frames for
2964   // execution
2965   // The steps taken by this frame are as follows:
2966   //   - push a fake &quot;unpack_frame&quot;
2967   //   - call the C routine Deoptimization::uncommon_trap (this function
2968   //     packs the current compiled frame into vframe arrays and returns
2969   //     information about the number and size of interpreter frames which
2970   //     are equivalent to the frame which is being deoptimized)
2971   //   - deallocate the &quot;unpack_frame&quot;
2972   //   - deallocate the deoptimization frame
2973   //   - in a loop using the information returned in the previous step
2974   //     push interpreter frames;
2975   //   - create a dummy &quot;unpack_frame&quot;
2976   //   - call the C routine: Deoptimization::unpack_frames (this function
2977   //     lays out values on the interpreter frame which was just created)
2978   //   - deallocate the dummy unpack_frame
2979   //   - return to the interpreter entry point
2980   //
2981   //  Refer to the following methods for more information:
2982   //   - Deoptimization::uncommon_trap
2983   //   - Deoptimization::unpack_frame
2984 
2985   // the unloaded class index is in O0 (first parameter to this blob)
2986 
2987   // push a dummy &quot;unpack_frame&quot;
2988   // and call Deoptimization::uncommon_trap to pack the compiled frame into
2989   // vframe array and return the UnrollBlock information
2990   __ save_frame(0);
2991   __ set_last_Java_frame(SP, noreg);
2992   __ mov(I0, O2klass_index);
2993   __ mov(Deoptimization::Unpack_uncommon_trap, O3); // exec mode
2994   __ call_VM_leaf(L7_thread_cache, CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap), G2_thread, O2klass_index, O3);
2995   __ reset_last_Java_frame();
2996   __ mov(O0, O2UnrollBlock-&gt;after_save());
2997   __ restore();
2998 
2999   // deallocate the deoptimized frame taking care to preserve the return values
3000   __ mov(O2UnrollBlock, O2UnrollBlock-&gt;after_save());
3001   __ restore();
3002 
3003 #ifdef ASSERT
3004   { Label L;
3005     __ ld(O2UnrollBlock, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes(), O1);
3006     __ cmp_and_br_short(O1, Deoptimization::Unpack_uncommon_trap, Assembler::equal, Assembler::pt, L);
3007     __ stop(&quot;SharedRuntime::generate_deopt_blob: expected Unpack_uncommon_trap&quot;);
3008     __ bind(L);
3009   }
3010 #endif
3011 
3012   // Allocate new interpreter frame(s) and possible c2i adapter frame
3013 
3014   make_new_frames(masm, false);
3015 
3016   // push a dummy &quot;unpack_frame&quot; taking care of float return values and
3017   // call Deoptimization::unpack_frames to have the unpacker layout
3018   // information in the interpreter frames just created and then return
3019   // to the interpreter entry point
3020   __ save_frame(0);
3021   __ set_last_Java_frame(SP, noreg);
3022   __ mov(Deoptimization::Unpack_uncommon_trap, O3); // indicate it is the uncommon trap case
3023   __ call_VM_leaf(L7_thread_cache, CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames), G2_thread, O3);
3024   __ reset_last_Java_frame();
3025   __ ret();
3026   __ delayed()-&gt;restore();
3027 
3028   masm-&gt;flush();
3029   _uncommon_trap_blob = UncommonTrapBlob::create(&amp;buffer, NULL, __ total_frame_size_in_bytes(0)/wordSize);
3030 }
3031 
3032 #endif // COMPILER2
3033 
3034 //------------------------------generate_handler_blob-------------------
3035 //
3036 // Generate a special Compile2Runtime blob that saves all registers, and sets
3037 // up an OopMap.
3038 //
3039 // This blob is jumped to (via a breakpoint and the signal handler) from a
3040 // safepoint in compiled code.  On entry to this blob, O7 contains the
3041 // address in the original nmethod at which we should resume normal execution.
3042 // Thus, this blob looks like a subroutine which must preserve lots of
3043 // registers and return normally.  Note that O7 is never register-allocated,
3044 // so it is guaranteed to be free here.
3045 //
3046 
3047 // The hardest part of what this blob must do is to save the 64-bit %o
3048 // registers in the 32-bit build.  A simple &#39;save&#39; turn the %o&#39;s to %i&#39;s and
3049 // an interrupt will chop off their heads.  Making space in the caller&#39;s frame
3050 // first will let us save the 64-bit %o&#39;s before save&#39;ing, but we cannot hand
3051 // the adjusted FP off to the GC stack-crawler: this will modify the caller&#39;s
3052 // SP and mess up HIS OopMaps.  So we first adjust the caller&#39;s SP, then save
3053 // the 64-bit %o&#39;s, then do a save, then fixup the caller&#39;s SP (our FP).
3054 // Tricky, tricky, tricky...
3055 
3056 SafepointBlob* SharedRuntime::generate_handler_blob(address call_ptr, int poll_type) {
3057   assert (StubRoutines::forward_exception_entry() != NULL, &quot;must be generated before&quot;);
3058 
3059   // allocate space for the code
3060   ResourceMark rm;
3061   // setup code generation tools
3062   // Measured 8/7/03 at 896 in 32bit debug build (no VerifyThread)
3063   // Measured 8/7/03 at 1080 in 32bit debug build (VerifyThread)
3064   CodeBuffer buffer(&quot;handler_blob&quot;, 1600, 512);
3065   MacroAssembler* masm                = new MacroAssembler(&amp;buffer);
3066   int             frame_size_words;
3067   OopMapSet *oop_maps = new OopMapSet();
3068   OopMap* map = NULL;
3069 
3070   int start = __ offset();
3071 
3072   bool cause_return = (poll_type == POLL_AT_RETURN);
3073   // If this causes a return before the processing, then do a &quot;restore&quot;
3074   if (cause_return) {
3075     __ restore();
3076   } else {
3077     // Make it look like we were called via the poll
3078     // so that frame constructor always sees a valid return address
3079     __ ld_ptr(Address(G2_thread, JavaThread::saved_exception_pc_offset()), O7);
3080     __ sub(O7, frame::pc_return_offset, O7);
3081   }
3082 
3083   map = RegisterSaver::save_live_registers(masm, 0, &amp;frame_size_words);
3084 
3085   // setup last_Java_sp (blows G4)
3086   __ set_last_Java_frame(SP, noreg);
3087 
3088   Register saved_O7 = O7-&gt;after_save();
3089   if (!cause_return &amp;&amp; SafepointMechanism::uses_thread_local_poll()) {
3090     // Keep a copy of the return pc in L0 to detect if it gets modified
3091     __ mov(saved_O7, L0);
3092     // Adjust and keep a copy of our npc saved by the signal handler
3093     __ ld_ptr(Address(G2_thread, JavaThread::saved_exception_npc_offset()), L1);
3094     __ sub(L1, frame::pc_return_offset, L1);
3095   }
3096 
3097   // call into the runtime to handle illegal instructions exception
3098   // Do not use call_VM_leaf, because we need to make a GC map at this call site.
3099   __ mov(G2_thread, O0);
3100   __ save_thread(L7_thread_cache);
3101   __ call(call_ptr);
3102   __ delayed()-&gt;nop();
3103 
3104   // Set an oopmap for the call site.
3105   // We need this not only for callee-saved registers, but also for volatile
3106   // registers that the compiler might be keeping live across a safepoint.
3107 
3108   oop_maps-&gt;add_gc_map( __ offset() - start, map);
3109 
3110   __ restore_thread(L7_thread_cache);
3111   // clear last_Java_sp
3112   __ reset_last_Java_frame();
3113 
3114   // Check for exceptions
3115   Label pending;
3116 
3117   __ ld_ptr(G2_thread, in_bytes(Thread::pending_exception_offset()), O1);
3118   __ br_notnull_short(O1, Assembler::pn, pending);
3119 
3120   if (!cause_return &amp;&amp; SafepointMechanism::uses_thread_local_poll()) {
3121     // If nobody modified our return pc then we must return to the npc which he saved in L1
3122     __ cmp(saved_O7, L0);
3123     __ movcc(Assembler::equal, false, Assembler::ptr_cc, L1, saved_O7);
3124   }
3125 
3126   RegisterSaver::restore_live_registers(masm);
3127 
3128   // We are back the the original state on entry and ready to go.
3129 
3130   __ retl();
3131   __ delayed()-&gt;nop();
3132 
3133   // Pending exception after the safepoint
3134 
3135   __ bind(pending);
3136 
3137   RegisterSaver::restore_live_registers(masm);
3138 
3139   // We are back the the original state on entry.
3140 
3141   // Tail-call forward_exception_entry, with the issuing PC in O7,
3142   // so it looks like the original nmethod called forward_exception_entry.
3143   __ set((intptr_t)StubRoutines::forward_exception_entry(), O0);
3144   __ JMP(O0, 0);
3145   __ delayed()-&gt;nop();
3146 
3147   // -------------
3148   // make sure all code is generated
3149   masm-&gt;flush();
3150 
3151   // return exception blob
3152   return SafepointBlob::create(&amp;buffer, oop_maps, frame_size_words);
3153 }
3154 
3155 //
3156 // generate_resolve_blob - call resolution (static/virtual/opt-virtual/ic-miss
3157 //
3158 // Generate a stub that calls into vm to find out the proper destination
3159 // of a java call. All the argument registers are live at this point
3160 // but since this is generic code we don&#39;t know what they are and the caller
3161 // must do any gc of the args.
3162 //
3163 RuntimeStub* SharedRuntime::generate_resolve_blob(address destination, const char* name) {
3164   assert (StubRoutines::forward_exception_entry() != NULL, &quot;must be generated before&quot;);
3165 
3166   // allocate space for the code
3167   ResourceMark rm;
3168   // setup code generation tools
3169   // Measured 8/7/03 at 896 in 32bit debug build (no VerifyThread)
3170   // Measured 8/7/03 at 1080 in 32bit debug build (VerifyThread)
3171   CodeBuffer buffer(name, 1600, 512);
3172   MacroAssembler* masm                = new MacroAssembler(&amp;buffer);
3173   int             frame_size_words;
3174   OopMapSet *oop_maps = new OopMapSet();
3175   OopMap* map = NULL;
3176 
3177   int start = __ offset();
3178 
3179   map = RegisterSaver::save_live_registers(masm, 0, &amp;frame_size_words);
3180 
3181   int frame_complete = __ offset();
3182 
3183   // setup last_Java_sp (blows G4)
3184   __ set_last_Java_frame(SP, noreg);
3185 
3186   // call into the runtime to handle illegal instructions exception
3187   // Do not use call_VM_leaf, because we need to make a GC map at this call site.
3188   __ mov(G2_thread, O0);
3189   __ save_thread(L7_thread_cache);
3190   __ call(destination, relocInfo::runtime_call_type);
3191   __ delayed()-&gt;nop();
3192 
3193   // O0 contains the address we are going to jump to assuming no exception got installed
3194 
3195   // Set an oopmap for the call site.
3196   // We need this not only for callee-saved registers, but also for volatile
3197   // registers that the compiler might be keeping live across a safepoint.
3198 
3199   oop_maps-&gt;add_gc_map( __ offset() - start, map);
3200 
3201   __ restore_thread(L7_thread_cache);
3202   // clear last_Java_sp
3203   __ reset_last_Java_frame();
3204 
3205   // Check for exceptions
3206   Label pending;
3207 
3208   __ ld_ptr(G2_thread, in_bytes(Thread::pending_exception_offset()), O1);
3209   __ br_notnull_short(O1, Assembler::pn, pending);
3210 
3211   // get the returned Method*
3212 
3213   __ get_vm_result_2(G5_method);
3214   __ stx(G5_method, SP, RegisterSaver::G5_offset()+STACK_BIAS);
3215 
3216   // O0 is where we want to jump, overwrite G3 which is saved and scratch
3217 
3218   __ stx(O0, SP, RegisterSaver::G3_offset()+STACK_BIAS);
3219 
3220   RegisterSaver::restore_live_registers(masm);
3221 
3222   // We are back the the original state on entry and ready to go.
3223 
3224   __ JMP(G3, 0);
3225   __ delayed()-&gt;nop();
3226 
3227   // Pending exception after the safepoint
3228 
3229   __ bind(pending);
3230 
3231   RegisterSaver::restore_live_registers(masm);
3232 
3233   // We are back the the original state on entry.
3234 
3235   // Tail-call forward_exception_entry, with the issuing PC in O7,
3236   // so it looks like the original nmethod called forward_exception_entry.
3237   __ set((intptr_t)StubRoutines::forward_exception_entry(), O0);
3238   __ JMP(O0, 0);
3239   __ delayed()-&gt;nop();
3240 
3241   // -------------
3242   // make sure all code is generated
3243   masm-&gt;flush();
3244 
3245   // return the  blob
3246   // frame_size_words or bytes??
3247   return RuntimeStub::new_runtime_stub(name, &amp;buffer, frame_complete, frame_size_words, oop_maps, true);
3248 }
    </pre>
  </body>
</html>